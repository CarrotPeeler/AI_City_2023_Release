[2023-08-29 17:35:04,952] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-29 17:35:05,003] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/training_output', lr=0.0007, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fb1122b7af0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00003281
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-08-29 17:35:10,054] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-29 17:35:10,054] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-29 17:35:10,205] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-29 17:35:10,205] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-29 17:35:10,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.40451812744140625 seconds
[2023-08-29 17:35:11,336] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-08-29 17:35:11,345] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-08-29 17:35:11,345] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-08-29 17:35:11,373] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-08-29 17:35:11,373] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-29 17:35:11,373] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-29 17:35:11,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 17:35:11,374] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-29 17:35:11,375] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-29 17:35:11,375] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-29 17:35:11,375] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-29 17:35:11,375] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb0c4ee35e0>
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-29 17:35:11,376] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-29 17:35:11,377] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0007, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-29 17:35:11,378] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-29 17:35:11,379] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-29 17:35:11,379] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.0007, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:28:47  lr: 0.000000  min_lr: 0.000000  loss: 2.7734 (2.7734)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 10.7953 (10.7953 -- 10.7953)  data: 6.8904 (6.8904 -- 6.8904)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:43  lr: 0.000001  min_lr: 0.000000  loss: 2.7730 (2.7730)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5932 (1.6020)  time: 0.6837 (0.4999 -- 2.0451)  data: 0.1014 (0.0003 -- 1.0669)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:11  lr: 0.000002  min_lr: 0.000000  loss: 2.7724 (2.7727)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4588 (1.5416)  time: 1.0274 (0.5058 -- 4.1384)  data: 0.2333 (0.0002 -- 1.4997)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 2.7724 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4179 (1.5246)  time: 0.7852 (0.4959 -- 3.3573)  data: 0.0014 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:19  lr: 0.000003  min_lr: 0.000000  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4507 (1.5166)  time: 0.9927 (0.5139 -- 5.0902)  data: 0.0541 (0.0004 -- 0.5424)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 2.7722 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4778 (1.5101)  time: 0.6720 (0.4937 -- 2.8732)  data: 0.0022 (0.0003 -- 0.0176)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 2.7718 (2.7724)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4890 (1.5103)  time: 0.9470 (0.5170 -- 3.8382)  data: 0.0190 (0.0002 -- 0.3199)  max mem: 16413
[2023-08-29 17:37:10,323] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:37:10,323] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:37:10,323] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-08-29 17:37:10,323] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 2.7715 (2.7722)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4395 (1.5001)  time: 0.8012 (0.5117 -- 2.0172)  data: 0.0411 (0.0003 -- 0.2745)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 2.7711 (2.7721)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3586 (1.4873)  time: 0.6985 (0.4871 -- 2.2150)  data: 0.0565 (0.0002 -- 0.8391)  max mem: 16413
Epoch: [0] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 2.7711 (2.7721)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3586 (1.4873)
Val:  [ 0/27]  eta: 0:01:16  loss: 2.7680 (2.7680)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 2.8506 (2.8506 -- 2.8506)  data: 2.4570 (2.4570 -- 2.4570)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7680 (2.7679)  acc1: 33.3333 (31.3131)  acc5: 77.7778 (76.7677)  time: 0.4588 (0.2015 -- 2.8506)  data: 0.2278 (0.0008 -- 2.4570)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7676 (2.7677)  acc1: 33.3333 (31.7460)  acc5: 77.7778 (79.3651)  time: 0.2222 (0.1677 -- 0.4869)  data: 0.0180 (0.0001 -- 0.3088)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7676 (2.7678)  acc1: 33.3333 (31.9502)  acc5: 77.7778 (77.5934)  time: 0.2074 (0.1641 -- 0.4869)  data: 0.0176 (0.0001 -- 0.3088)  max mem: 16413
Val: Total time: 0:00:08 (0.3076 s / it)
* Acc@1 30.913 Acc@5 78.423 loss 2.768
Accuracy of the network on the 482 val images: 30.91%
[2023-08-29 17:37:42,261] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-29 17:37:42,263] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 17:37:42,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 17:37:42,264] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 17:37:43,147] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 17:37:43,147] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.91%
Epoch: [1]  [  0/160]  eta: 0:19:26  lr: 0.000007  min_lr: 0.000000  loss: 2.7707 (2.7707)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7194 (1.7194)  time: 7.2890 (7.2890 -- 7.2890)  data: 6.7627 (6.7627 -- 6.7627)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:38  lr: 0.000007  min_lr: 0.000000  loss: 2.7701 (2.7699)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4836 (1.5159)  time: 0.8221 (0.5315 -- 2.7189)  data: 0.2181 (0.0009 -- 2.1963)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:02  lr: 0.000008  min_lr: 0.000000  loss: 2.7693 (2.7697)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5475 (1.5281)  time: 0.9001 (0.5268 -- 2.5334)  data: 0.2024 (0.0004 -- 1.9905)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:37  lr: 0.000009  min_lr: 0.000000  loss: 2.7667 (2.7687)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5234 (1.5380)  time: 0.8776 (0.5157 -- 3.8520)  data: 0.2489 (0.0004 -- 3.3154)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 2.7640 (2.7676)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5348 (1.5274)  time: 0.9105 (0.5304 -- 3.7816)  data: 0.2469 (0.0004 -- 2.2393)  max mem: 16413
[2023-08-29 17:39:14,465] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:39:14,465] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-08-29 17:39:14,466] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:39:14,466] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 2.7614 (2.7663)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5821 (1.5313)  time: 0.8359 (0.5297 -- 3.0869)  data: 0.2107 (0.0007 -- 2.4436)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 2.7558 (2.7648)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5106 (1.5252)  time: 0.8629 (0.5258 -- 2.7620)  data: 0.2591 (0.0003 -- 2.2261)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 2.7505 (2.7627)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5785 (1.5308)  time: 0.8114 (0.5310 -- 2.3852)  data: 0.2692 (0.0001 -- 1.8552)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 2.7491 (2.7609)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4988 (1.5338)  time: 0.7086 (0.4940 -- 2.8023)  data: 0.1521 (0.0002 -- 2.2850)  max mem: 16413
Epoch: [1] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 2.7491 (2.7610)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4988 (1.5338)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.7122 (2.7122)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.3747 (2.3747 -- 2.3747)  data: 2.1074 (2.1074 -- 2.1074)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.7192 (2.7205)  acc1: 33.3333 (35.3535)  acc5: 88.8889 (84.8485)  time: 0.4094 (0.2012 -- 2.3747)  data: 0.1962 (0.0007 -- 2.1074)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7166 (2.7178)  acc1: 33.3333 (39.6825)  acc5: 88.8889 (87.8307)  time: 0.2129 (0.1695 -- 0.3856)  data: 0.0121 (0.0001 -- 0.1881)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7213 (2.7201)  acc1: 42.8571 (40.6639)  acc5: 88.8889 (86.3071)  time: 0.1992 (0.1333 -- 0.3856)  data: 0.0118 (0.0001 -- 0.1881)  max mem: 16413
Val: Total time: 0:00:07 (0.2823 s / it)
* Acc@1 41.494 Acc@5 86.307 loss 2.719
Accuracy of the network on the 482 val images: 41.49%
[2023-08-29 17:40:12,160] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 17:40:12,162] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 17:40:12,162] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 17:40:12,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 17:40:13,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 17:40:13,420] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.49%
Epoch: [2]  [  0/160]  eta: 0:22:38  lr: 0.000013  min_lr: 0.000000  loss: 2.7620 (2.7620)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4733 (1.4733)  time: 8.4935 (8.4935 -- 8.4935)  data: 7.9561 (7.9561 -- 7.9561)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:44  lr: 0.000014  min_lr: 0.000000  loss: 2.7410 (2.7414)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5328 (1.5381)  time: 0.8093 (0.5243 -- 4.9986)  data: 0.2671 (0.0006 -- 4.4639)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:08  lr: 0.000015  min_lr: 0.000000  loss: 2.7292 (2.7374)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5583 (1.5540)  time: 0.9691 (0.5241 -- 4.9682)  data: 0.4262 (0.0003 -- 4.4231)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:37  lr: 0.000016  min_lr: 0.000000  loss: 2.7251 (2.7327)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5425 (1.5619)  time: 0.7668 (0.5259 -- 3.3952)  data: 0.2141 (0.0007 -- 2.8466)  max mem: 16413
[2023-08-29 17:41:17,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:41:17,092] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-08-29 17:41:17,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:41:17,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:15  lr: 0.000016  min_lr: 0.000000  loss: 2.7155 (2.7293)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5649 (1.5626)  time: 0.8763 (0.5251 -- 3.9766)  data: 0.3354 (0.0003 -- 3.4699)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 2.7075 (2.7260)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6170 (1.5772)  time: 0.8583 (0.5177 -- 2.8932)  data: 0.2088 (0.0005 -- 2.3593)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 2.7012 (2.7223)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6105 (1.5939)  time: 0.9469 (0.5371 -- 3.9543)  data: 0.3951 (0.0005 -- 3.4292)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 2.6854 (2.7159)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5727 (1.6043)  time: 0.7877 (0.5202 -- 3.0336)  data: 0.2353 (0.0003 -- 2.4860)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.6682 (2.7103)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6381 (1.6128)  time: 0.6710 (0.4937 -- 2.3583)  data: 0.1472 (0.0002 -- 1.8114)  max mem: 16413
Epoch: [2] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.6682 (2.7097)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6381 (1.6128)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.5477 (2.5477)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3553 (2.3553 -- 2.3553)  data: 2.1125 (2.1125 -- 2.1125)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.5920 (2.5828)  acc1: 44.4444 (41.4141)  acc5: 88.8889 (87.8788)  time: 0.4194 (0.2121 -- 2.3553)  data: 0.1937 (0.0009 -- 2.1125)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.5623 (2.5732)  acc1: 33.3333 (38.0952)  acc5: 88.8889 (89.9471)  time: 0.2151 (0.1697 -- 0.2688)  data: 0.0056 (0.0001 -- 0.0906)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.5694 (2.5801)  acc1: 33.3333 (38.1743)  acc5: 88.8889 (87.1369)  time: 0.1967 (0.1351 -- 0.2688)  data: 0.0050 (0.0001 -- 0.0906)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 40.249 Acc@5 86.929 loss 2.579
Accuracy of the network on the 482 val images: 40.25%
Max accuracy: 41.49%
Epoch: [3]  [  0/160]  eta: 0:17:40  lr: 0.000020  min_lr: 0.000000  loss: 2.6509 (2.6509)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9511 (1.9511)  time: 6.6271 (6.6271 -- 6.6271)  data: 6.0700 (6.0700 -- 6.0700)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:37  lr: 0.000021  min_lr: 0.000000  loss: 2.6784 (2.6745)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8118 (1.8018)  time: 0.8477 (0.5281 -- 3.2182)  data: 0.2679 (0.0005 -- 2.6868)  max mem: 16413
[2023-08-29 17:43:20,122] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:43:20,123] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-08-29 17:43:20,124] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:43:20,124] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:07  lr: 0.000021  min_lr: 0.000001  loss: 2.6479 (2.6609)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6273 (1.7169)  time: 0.9951 (0.5247 -- 5.0046)  data: 0.4466 (0.0004 -- 4.4871)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:39  lr: 0.000022  min_lr: 0.000001  loss: 2.6244 (2.6486)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7334 (1.7450)  time: 0.8701 (0.5227 -- 3.9054)  data: 0.3304 (0.0003 -- 3.3966)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.6239 (2.6428)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8514 (1.7871)  time: 0.8538 (0.5270 -- 2.8686)  data: 0.3134 (0.0001 -- 2.3610)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000001  loss: 2.6382 (2.6395)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8132 (1.7994)  time: 0.8180 (0.5238 -- 3.4163)  data: 0.1546 (0.0003 -- 1.6879)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000001  loss: 2.5808 (2.6304)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9244 (1.8442)  time: 0.9267 (0.5365 -- 2.8066)  data: 0.3016 (0.0004 -- 2.2929)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 2.5954 (2.6249)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9757 (1.8553)  time: 0.8994 (0.5217 -- 3.1318)  data: 0.2406 (0.0002 -- 2.6102)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 2.5963 (2.6199)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9745 (1.8784)  time: 0.8079 (0.4963 -- 3.3915)  data: 0.2924 (0.0002 -- 2.8924)  max mem: 16413
Epoch: [3] Total time: 0:02:23 (0.8997 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 2.5963 (2.6190)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9745 (1.8784)
Val:  [ 0/27]  eta: 0:01:06  loss: 2.3414 (2.3414)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.4525 (2.4525 -- 2.4525)  data: 2.2039 (2.2039 -- 2.2039)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.4247 (2.4046)  acc1: 33.3333 (41.4141)  acc5: 88.8889 (89.8990)  time: 0.4202 (0.1980 -- 2.4525)  data: 0.2012 (0.0006 -- 2.2039)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.3696 (2.3883)  acc1: 44.4444 (41.7989)  acc5: 88.8889 (89.4180)  time: 0.2136 (0.1701 -- 0.3308)  data: 0.0080 (0.0001 -- 0.1476)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.3783 (2.4018)  acc1: 44.4444 (40.6639)  acc5: 88.8889 (87.1369)  time: 0.1955 (0.1333 -- 0.3308)  data: 0.0078 (0.0001 -- 0.1476)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 41.494 Acc@5 86.722 loss 2.402
Accuracy of the network on the 482 val images: 41.49%
Max accuracy: 41.49%
[2023-08-29 17:45:20,158] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:45:20,158] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:45:20,159] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-08-29 17:45:20,159] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:15:13  lr: 0.000026  min_lr: 0.000001  loss: 2.5054 (2.5054)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0198 (2.0198)  time: 5.7064 (5.7064 -- 5.7064)  data: 4.6920 (4.6920 -- 4.6920)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:39  lr: 0.000027  min_lr: 0.000001  loss: 2.5494 (2.5306)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9863 (2.0667)  time: 0.9085 (0.5243 -- 3.2584)  data: 0.2419 (0.0008 -- 2.4097)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:01:56  lr: 0.000028  min_lr: 0.000001  loss: 2.5571 (2.5362)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0538 (2.1214)  time: 0.7980 (0.5394 -- 3.4492)  data: 0.2345 (0.0004 -- 2.9168)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:38  lr: 0.000029  min_lr: 0.000001  loss: 2.4969 (2.5202)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1540 (2.1418)  time: 1.0035 (0.5257 -- 4.0458)  data: 0.4592 (0.0005 -- 3.5300)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:14  lr: 0.000030  min_lr: 0.000001  loss: 2.5192 (2.5144)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3994 (2.2317)  time: 0.7787 (0.5288 -- 2.9681)  data: 0.2064 (0.0002 -- 2.4445)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000001  loss: 2.5496 (2.5107)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5917 (2.3091)  time: 0.8645 (0.5201 -- 3.4366)  data: 0.3191 (0.0005 -- 2.8902)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 2.4706 (2.5010)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6096 (2.3534)  time: 0.9008 (0.5389 -- 3.1894)  data: 0.0451 (0.0004 -- 0.4809)  max mem: 16413
[2023-08-29 17:47:11,227] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:47:11,227] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-08-29 17:47:11,230] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:47:11,231] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 2.5162 (2.5022)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4625 (2.3722)  time: 0.9739 (0.5220 -- 3.8429)  data: 0.3791 (0.0004 -- 3.3117)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.4986 (2.5024)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5824 (2.4159)  time: 0.6160 (0.4959 -- 1.6285)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [4] Total time: 0:02:21 (0.8829 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.4986 (2.5147)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5824 (2.4159)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.1403 (2.1403)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3391 (2.3391 -- 2.3391)  data: 2.1115 (2.1115 -- 2.1115)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.1877 (2.1969)  acc1: 33.3333 (40.4040)  acc5: 100.0000 (93.9394)  time: 0.4346 (0.1926 -- 2.3391)  data: 0.2135 (0.0006 -- 2.1115)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.1620 (2.1764)  acc1: 33.3333 (40.2116)  acc5: 100.0000 (93.1217)  time: 0.2186 (0.1697 -- 0.4484)  data: 0.0120 (0.0001 -- 0.2202)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.1999 (2.1989)  acc1: 44.4444 (40.6639)  acc5: 88.8889 (90.0415)  time: 0.2002 (0.1328 -- 0.4484)  data: 0.0115 (0.0001 -- 0.2202)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 43.361 Acc@5 90.249 loss 2.193
Accuracy of the network on the 482 val images: 43.36%
[2023-08-29 17:47:43,434] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 17:47:43,435] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 17:47:43,435] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 17:47:43,436] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 17:47:44,780] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 17:47:44,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.36%
Epoch: [5]  [  0/160]  eta: 0:18:24  lr: 0.000033  min_lr: 0.000001  loss: 2.3929 (2.3929)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9130 (2.9130)  time: 6.9056 (6.9056 -- 6.9056)  data: 5.8258 (5.8258 -- 5.8258)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:49  lr: 0.000033  min_lr: 0.000001  loss: 2.4022 (2.4522)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8211 (3.1732)  time: 0.9240 (0.5337 -- 5.0947)  data: 0.1868 (0.0004 -- 2.7269)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:02:04  lr: 0.000033  min_lr: 0.000001  loss: 2.4120 (2.4309)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9954 (3.1361)  time: 0.8552 (0.5229 -- 3.6706)  data: 0.0015 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:40  lr: 0.000033  min_lr: 0.000001  loss: 2.4640 (2.4359)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9443 (3.0888)  time: 0.9349 (0.5188 -- 2.7250)  data: 0.0014 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000001  loss: 2.3873 (2.4277)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1452 (3.1578)  time: 0.7916 (0.5279 -- 2.4721)  data: 0.0014 (0.0002 -- 0.0040)  max mem: 16413
[2023-08-29 17:49:14,696] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:49:14,696] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-08-29 17:49:14,699] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:49:14,699] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000001  loss: 2.4428 (2.4276)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2754 (3.2218)  time: 0.7792 (0.5405 -- 1.9841)  data: 0.0015 (0.0007 -- 0.0028)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.3738 (2.4239)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9995 (3.2237)  time: 0.9222 (0.5451 -- 3.5856)  data: 0.0025 (0.0003 -- 0.0155)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.4101 (2.4242)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3676 (3.2714)  time: 0.8508 (0.5404 -- 3.3385)  data: 0.0121 (0.0003 -- 0.2104)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.2938 (2.4149)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1816 (3.3068)  time: 0.6577 (0.4952 -- 2.5087)  data: 0.0007 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [5] Total time: 0:02:20 (0.8794 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.2938 (2.4179)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1816 (3.3068)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.9630 (1.9630)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4370 (2.4370 -- 2.4370)  data: 2.2020 (2.2020 -- 2.2020)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.0161 (2.0132)  acc1: 55.5556 (47.4747)  acc5: 100.0000 (93.9394)  time: 0.4271 (0.1962 -- 2.4370)  data: 0.2105 (0.0007 -- 2.2020)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.9580 (1.9900)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (94.1799)  time: 0.2117 (0.1703 -- 0.3069)  data: 0.0064 (0.0001 -- 0.1013)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0161 (2.0173)  acc1: 44.4444 (44.8133)  acc5: 88.8889 (92.9461)  time: 0.1961 (0.1331 -- 0.3069)  data: 0.0060 (0.0001 -- 0.1013)  max mem: 16413
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 48.548 Acc@5 92.116 loss 2.007
Accuracy of the network on the 482 val images: 48.55%
[2023-08-29 17:50:13,188] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 17:50:13,190] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 17:50:13,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 17:50:13,194] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 17:50:14,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 17:50:14,464] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 48.55%
Epoch: [6]  [  0/160]  eta: 0:17:22  lr: 0.000033  min_lr: 0.000001  loss: 2.5488 (2.5488)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1706 (4.1706)  time: 6.5180 (6.5180 -- 6.5180)  data: 5.8369 (5.8369 -- 5.8369)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:41  lr: 0.000033  min_lr: 0.000001  loss: 2.4484 (2.4670)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2949 (3.6627)  time: 0.8858 (0.5337 -- 2.3236)  data: 0.2377 (0.0005 -- 1.7958)  max mem: 16413
[2023-08-29 17:50:55,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[7.794570921304778e-07, 7.794570921304778e-07, 1.0392761228406371e-06, 1.0392761228406371e-06, 1.3857014971208495e-06, 1.3857014971208495e-06, 1.8476019961611325e-06, 1.8476019961611325e-06, 2.4634693282148435e-06, 2.4634693282148435e-06, 3.2846257709531246e-06, 3.2846257709531246e-06, 4.379501027937499e-06, 4.379501027937499e-06, 5.839334703916666e-06, 5.839334703916666e-06, 7.78577960522222e-06, 7.78577960522222e-06, 1.0381039473629629e-05, 1.0381039473629629e-05, 1.3841385964839504e-05, 1.3841385964839504e-05, 1.8455181286452672e-05, 1.8455181286452672e-05, 2.46069083819369e-05, 2.46069083819369e-05, 3.280921117591586e-05, 3.280921117591586e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 17:50:55,957] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=18.617485479044436, CurrSamplesPerSec=22.143657609460792, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:02:02  lr: 0.000033  min_lr: 0.000001  loss: 2.3869 (2.4102)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8199 (3.7234)  time: 0.8894 (0.5167 -- 4.1714)  data: 0.3205 (0.0003 -- 3.6479)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:39  lr: 0.000033  min_lr: 0.000001  loss: 2.3332 (2.3890)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7126 (3.6989)  time: 0.9234 (0.5214 -- 3.1633)  data: 0.3577 (0.0003 -- 2.6408)  max mem: 16413
[2023-08-29 17:51:17,211] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:51:17,211] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:51:17,252] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-08-29 17:51:17,252] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:14  lr: 0.000033  min_lr: 0.000001  loss: 2.3447 (2.3819)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4754 (3.6899)  time: 0.7242 (0.5240 -- 2.4639)  data: 0.1041 (0.0004 -- 1.9228)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000001  loss: 2.3781 (2.3802)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0671 (3.7745)  time: 0.9684 (0.5205 -- 3.9789)  data: 0.3057 (0.0004 -- 3.4431)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.2701 (2.3702)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7790 (3.7866)  time: 0.7660 (0.5276 -- 2.2425)  data: 0.0268 (0.0004 -- 0.5122)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.2870 (2.3594)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (3.8960)  time: 0.9795 (0.5212 -- 3.9181)  data: 0.0276 (0.0004 -- 0.5257)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.3100 (2.3524)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8499 (3.8872)  time: 0.6571 (0.4975 -- 2.3381)  data: 0.0008 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [6] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.3100 (2.3495)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8499 (3.8872)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.8077 (1.8077)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3244 (2.3244 -- 2.3244)  data: 2.1065 (2.1065 -- 2.1065)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.8451 (1.8728)  acc1: 44.4444 (42.4242)  acc5: 100.0000 (93.9394)  time: 0.4229 (0.2068 -- 2.3244)  data: 0.2053 (0.0006 -- 2.1065)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8351 (1.8506)  acc1: 44.4444 (40.7407)  acc5: 100.0000 (93.6508)  time: 0.2189 (0.1695 -- 0.3133)  data: 0.0136 (0.0001 -- 0.1184)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.8451 (1.8834)  acc1: 44.4444 (40.6639)  acc5: 88.8889 (93.3610)  time: 0.2002 (0.1324 -- 0.3133)  data: 0.0132 (0.0001 -- 0.1184)  max mem: 16413
Val: Total time: 0:00:07 (0.2847 s / it)
* Acc@1 46.266 Acc@5 92.739 loss 1.863
Accuracy of the network on the 482 val images: 46.27%
Max accuracy: 48.55%
Epoch: [7]  [  0/160]  eta: 0:19:46  lr: 0.000033  min_lr: 0.000001  loss: 2.0137 (2.0137)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8337 (3.8337)  time: 7.4183 (7.4183 -- 7.4183)  data: 6.8909 (6.8909 -- 6.8909)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:52  lr: 0.000033  min_lr: 0.000001  loss: 2.3357 (2.3428)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9194 (4.5136)  time: 0.9249 (0.5240 -- 3.8983)  data: 0.2501 (0.0003 -- 3.3542)  max mem: 16413
[2023-08-29 17:53:19,801] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:53:19,801] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-08-29 17:53:19,803] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:53:19,803] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:01:59  lr: 0.000033  min_lr: 0.000001  loss: 2.2228 (2.3124)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7829 (4.2162)  time: 0.7525 (0.5226 -- 1.6897)  data: 0.1131 (0.0003 -- 1.1638)  max mem: 16413
[2023-08-29 17:53:34,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1170
[2023-08-29 17:53:34,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1170
[2023-08-29 17:53:34,343] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-08-29 17:53:34,343] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-08-29 17:53:34,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
Epoch: [7]  [ 60/160]  eta: 0:01:35  lr: 0.000033  min_lr: 0.000001  loss: 2.4151 (2.3282)  loss_scale: 32768.0000 (42437.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1244 (4.3701)  time: 0.8581 (0.5235 -- 2.8705)  data: 0.1984 (0.0003 -- 2.3307)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:14  lr: 0.000033  min_lr: 0.000001  loss: 2.3779 (2.3385)  loss_scale: 32768.0000 (40049.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0539 (4.3252)  time: 0.8647 (0.5366 -- 2.7945)  data: 0.3035 (0.0003 -- 2.2793)  max mem: 16413
Epoch: [7]  [100/160]  eta: 0:00:54  lr: 0.000033  min_lr: 0.000001  loss: 2.2464 (2.3215)  loss_scale: 32768.0000 (38607.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2914 (4.4007)  time: 0.8248 (0.5080 -- 3.1242)  data: 0.1783 (0.0003 -- 2.6014)  max mem: 16413
[2023-08-29 17:54:16,524] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1221
[2023-08-29 17:54:16,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 17:54:16,524] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1221
[2023-08-29 17:54:16,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 17:54:16,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.2557 (2.3158)  loss_scale: 16384.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2939 (4.4105)  time: 0.8788 (0.5214 -- 2.4721)  data: 0.0881 (0.0003 -- 0.8483)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:17  lr: 0.000033  min_lr: 0.000001  loss: 2.3667 (2.3118)  loss_scale: 16384.0000 (32303.2057)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3451 (4.5365)  time: 0.8180 (0.5337 -- 2.7914)  data: 0.2125 (0.0003 -- 2.2467)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.3194 (2.3131)  loss_scale: 16384.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8566 (4.5552)  time: 0.7057 (0.4974 -- 3.4728)  data: 0.1757 (0.0002 -- 2.9365)  max mem: 16413
Epoch: [7] Total time: 0:02:19 (0.8714 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.3194 (2.3132)  loss_scale: 16384.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8566 (4.5552)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.7091 (1.7091)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4368 (2.4368 -- 2.4368)  data: 2.2153 (2.2153 -- 2.2153)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.7209 (1.7633)  acc1: 55.5556 (49.4949)  acc5: 100.0000 (93.9394)  time: 0.4210 (0.1970 -- 2.4368)  data: 0.2060 (0.0007 -- 2.2153)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.7209 (1.7472)  acc1: 44.4444 (48.1481)  acc5: 100.0000 (94.1799)  time: 0.2149 (0.1695 -- 0.2860)  data: 0.0081 (0.0001 -- 0.0783)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7683 (1.7839)  acc1: 44.4444 (47.3029)  acc5: 100.0000 (94.1909)  time: 0.2009 (0.1332 -- 0.2860)  data: 0.0078 (0.0001 -- 0.0783)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 52.075 Acc@5 93.361 loss 1.759
Accuracy of the network on the 482 val images: 52.07%
[2023-08-29 17:55:11,195] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 17:55:11,196] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 17:55:11,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 17:55:11,197] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 17:55:12,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 17:55:12,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 52.07%
Epoch: [8]  [  0/160]  eta: 0:19:18  lr: 0.000033  min_lr: 0.000001  loss: 2.1229 (2.1229)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2568 (8.2568)  time: 7.2427 (7.2427 -- 7.2427)  data: 5.7771 (5.7771 -- 5.7771)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:44  lr: 0.000033  min_lr: 0.000001  loss: 2.2127 (2.2465)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3540 (5.2655)  time: 0.8717 (0.5336 -- 2.9653)  data: 0.0928 (0.0007 -- 1.0544)  max mem: 16413
Epoch: [8]  [ 40/160]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000001  loss: 2.2049 (2.2153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1872 (5.3383)  time: 0.9149 (0.5124 -- 5.1686)  data: 0.0041 (0.0004 -- 0.0424)  max mem: 16413
Epoch: [8]  [ 60/160]  eta: 0:01:40  lr: 0.000033  min_lr: 0.000001  loss: 2.1773 (2.2317)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6728 (5.1315)  time: 0.9193 (0.5151 -- 3.7391)  data: 0.0012 (0.0003 -- 0.0030)  max mem: 16413
[2023-08-29 17:56:21,056] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:56:21,056] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 17:56:21,058] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:56:21,058] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [8]  [ 80/160]  eta: 0:01:17  lr: 0.000033  min_lr: 0.000001  loss: 2.2578 (2.2346)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0578 (5.0670)  time: 0.8374 (0.5318 -- 4.3700)  data: 0.0018 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000001  loss: 2.3118 (2.2521)  loss_scale: 32768.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1906 (5.0784)  time: 0.8253 (0.5329 -- 3.1029)  data: 0.0019 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.2065 (2.2593)  loss_scale: 32768.0000 (23289.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2109 (5.1349)  time: 0.7706 (0.5391 -- 2.2118)  data: 0.0015 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [8]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.3868 (2.2735)  loss_scale: 32768.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8394 (5.2663)  time: 0.9107 (0.5306 -- 2.6624)  data: 0.0025 (0.0003 -- 0.0108)  max mem: 16413
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.3561 (2.2794)  loss_scale: 32768.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5777 (5.2194)  time: 0.6597 (0.4962 -- 2.3231)  data: 0.0009 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [8] Total time: 0:02:20 (0.8805 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.3561 (2.2722)  loss_scale: 32768.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5777 (5.2194)
Val:  [ 0/27]  eta: 0:00:56  loss: 1.6177 (1.6177)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.0913 (2.0913 -- 2.0913)  data: 1.7954 (1.7954 -- 1.7954)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.6373 (1.6643)  acc1: 44.4444 (51.5152)  acc5: 100.0000 (93.9394)  time: 0.3907 (0.1983 -- 2.0913)  data: 0.1708 (0.0002 -- 1.7954)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6373 (1.6445)  acc1: 44.4444 (49.7355)  acc5: 100.0000 (94.1799)  time: 0.2253 (0.1729 -- 0.4699)  data: 0.0205 (0.0001 -- 0.2569)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6732 (1.6873)  acc1: 44.4444 (50.2075)  acc5: 88.8889 (93.7759)  time: 0.2107 (0.1333 -- 0.4699)  data: 0.0202 (0.0001 -- 0.2569)  max mem: 16413
Val: Total time: 0:00:07 (0.2811 s / it)
* Acc@1 55.187 Acc@5 93.568 loss 1.662
Accuracy of the network on the 482 val images: 55.19%
[2023-08-29 17:57:40,891] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 17:57:40,893] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 17:57:40,893] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 17:57:40,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 17:57:42,214] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 17:57:42,214] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 55.19%
Epoch: [9]  [  0/160]  eta: 0:16:09  lr: 0.000033  min_lr: 0.000001  loss: 2.1534 (2.1534)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1845 (4.1845)  time: 6.0598 (6.0598 -- 6.0598)  data: 5.2505 (5.2505 -- 5.2505)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:03:05  lr: 0.000033  min_lr: 0.000001  loss: 2.1871 (2.2290)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1668 (5.7100)  time: 1.0860 (0.5182 -- 5.1492)  data: 0.0016 (0.0004 -- 0.0055)  max mem: 16413
[2023-08-29 17:58:23,834] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:58:23,837] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 17:58:23,875] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-29 17:58:23,875] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 40/160]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000001  loss: 2.2090 (2.2406)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6528 (5.4720)  time: 0.7487 (0.5228 -- 3.1168)  data: 0.0015 (0.0003 -- 0.0058)  max mem: 16413
Epoch: [9]  [ 60/160]  eta: 0:01:43  lr: 0.000033  min_lr: 0.000001  loss: 2.1666 (2.2280)  loss_scale: 65536.0000 (45123.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6737 (5.3747)  time: 1.0288 (0.5212 -- 5.2880)  data: 0.0020 (0.0005 -- 0.0160)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000001  loss: 2.2978 (2.2324)  loss_scale: 65536.0000 (50163.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7433 (5.2499)  time: 0.6982 (0.5194 -- 2.8375)  data: 0.0013 (0.0002 -- 0.0024)  max mem: 16413
[2023-08-29 17:59:04,727] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1526
[2023-08-29 17:59:04,727] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1526
[2023-08-29 17:59:04,768] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-29 17:59:04,768] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-29 17:59:04,769] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000001  loss: 2.2897 (2.2346)  loss_scale: 32768.0000 (48340.9109)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8235 (5.3372)  time: 0.9234 (0.5210 -- 4.1477)  data: 0.0016 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [9]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.1804 (2.2291)  loss_scale: 32768.0000 (45766.8760)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6883 (5.2958)  time: 0.7751 (0.5282 -- 3.2330)  data: 0.0012 (0.0003 -- 0.0019)  max mem: 16413
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.0575 (2.2119)  loss_scale: 32768.0000 (43923.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9013 (5.2717)  time: 0.9929 (0.5180 -- 4.0947)  data: 0.0013 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.1794 (2.2122)  loss_scale: 32768.0000 (42598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9524 (5.3304)  time: 0.6202 (0.4964 -- 1.4080)  data: 0.0200 (0.0002 -- 0.3887)  max mem: 16413
Epoch: [9] Total time: 0:02:23 (0.8938 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.1794 (2.2171)  loss_scale: 32768.0000 (42598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9524 (5.3304)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.5628 (1.5628)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4292 (2.4292 -- 2.4292)  data: 2.1698 (2.1698 -- 2.1698)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5567 (1.5857)  acc1: 55.5556 (52.5253)  acc5: 100.0000 (95.9596)  time: 0.4170 (0.1966 -- 2.4292)  data: 0.1982 (0.0007 -- 2.1698)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5141 (1.5591)  acc1: 55.5556 (53.4392)  acc5: 100.0000 (96.2963)  time: 0.2131 (0.1689 -- 0.2695)  data: 0.0068 (0.0001 -- 0.0775)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5567 (1.6032)  acc1: 44.4444 (53.5270)  acc5: 100.0000 (95.4357)  time: 0.1964 (0.1335 -- 0.2695)  data: 0.0065 (0.0001 -- 0.0775)  max mem: 16413
Val: Total time: 0:00:07 (0.2847 s / it)
* Acc@1 57.261 Acc@5 93.983 loss 1.581
Accuracy of the network on the 482 val images: 57.26%
[2023-08-29 18:00:13,028] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:00:13,030] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:00:13,030] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:00:13,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:00:14,237] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:00:14,237] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 57.26%
Epoch: [10]  [  0/160]  eta: 0:25:44  lr: 0.000033  min_lr: 0.000001  loss: 2.4879 (2.4879)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7793 (3.7793)  time: 9.6557 (9.6557 -- 9.6557)  data: 9.1347 (9.1347 -- 9.1347)  max mem: 16413
Epoch: [10]  [ 20/160]  eta: 0:02:54  lr: 0.000033  min_lr: 0.000001  loss: 2.0949 (2.1789)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0296 (5.5214)  time: 0.8264 (0.5256 -- 4.4326)  data: 0.2806 (0.0002 -- 3.9049)  max mem: 16413
Epoch: [10]  [ 40/160]  eta: 0:02:08  lr: 0.000033  min_lr: 0.000001  loss: 2.2225 (2.2199)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2520 (5.6401)  time: 0.8915 (0.5209 -- 3.2019)  data: 0.2279 (0.0003 -- 1.9273)  max mem: 16413
[2023-08-29 18:01:08,836] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:01:08,837] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-29 18:01:08,837] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:01:08,838] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [ 60/160]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000001  loss: 2.0133 (2.1691)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3418 (5.6440)  time: 0.8183 (0.5173 -- 3.6529)  data: 0.0196 (0.0002 -- 0.3640)  max mem: 16413
[2023-08-29 18:01:19,527] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1665
[2023-08-29 18:01:19,527] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1665
[2023-08-29 18:01:19,527] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-29 18:01:19,527] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-29 18:01:19,527] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-29 18:01:29,881] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1678
[2023-08-29 18:01:29,881] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1678
[2023-08-29 18:01:29,881] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:01:29,881] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:01:29,881] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [10]  [ 80/160]  eta: 0:01:17  lr: 0.000033  min_lr: 0.000001  loss: 2.1007 (2.1489)  loss_scale: 32768.0000 (36206.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8525 (5.4715)  time: 0.9202 (0.5254 -- 3.7789)  data: 0.0012 (0.0005 -- 0.0032)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000001  loss: 2.3384 (2.1737)  loss_scale: 16384.0000 (32281.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9560 (5.5903)  time: 0.8344 (0.5216 -- 3.8491)  data: 0.0013 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [10]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.2098 (2.1739)  loss_scale: 16384.0000 (29653.6860)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3377 (5.5799)  time: 0.8103 (0.5343 -- 3.0810)  data: 0.0014 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.2567 (2.1917)  loss_scale: 16384.0000 (27771.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1183 (5.6924)  time: 0.7820 (0.5275 -- 2.6993)  data: 0.0015 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.2225 (2.2034)  loss_scale: 16384.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3577 (5.7823)  time: 0.6764 (0.4986 -- 2.1127)  data: 0.0007 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [10] Total time: 0:02:20 (0.8774 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.2225 (2.2026)  loss_scale: 16384.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3577 (5.7823)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.5244 (1.5244)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4406 (2.4406 -- 2.4406)  data: 2.2101 (2.2101 -- 2.2101)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5244 (1.5279)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (95.9596)  time: 0.4246 (0.1944 -- 2.4406)  data: 0.2046 (0.0005 -- 2.2101)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4153 (1.5151)  acc1: 55.5556 (56.0847)  acc5: 100.0000 (96.2963)  time: 0.2158 (0.1690 -- 0.3053)  data: 0.0073 (0.0001 -- 0.1014)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5477 (1.5614)  acc1: 55.5556 (56.0166)  acc5: 100.0000 (95.4357)  time: 0.1998 (0.1333 -- 0.3053)  data: 0.0069 (0.0001 -- 0.1014)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 57.469 Acc@5 93.983 loss 1.536
Accuracy of the network on the 482 val images: 57.47%
[2023-08-29 18:02:42,372] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:02:42,373] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:02:42,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:02:42,374] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:02:43,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:02:43,735] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 57.47%
Epoch: [11]  [  0/160]  eta: 0:21:20  lr: 0.000033  min_lr: 0.000001  loss: 2.3629 (2.3629)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1081 (6.1081)  time: 8.0021 (8.0021 -- 8.0021)  data: 5.7216 (5.7216 -- 5.7216)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:32  lr: 0.000033  min_lr: 0.000001  loss: 2.2583 (2.2874)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2400 (6.3648)  time: 0.7428 (0.5341 -- 2.6272)  data: 0.0926 (0.0004 -- 1.4009)  max mem: 16413
Epoch: [11]  [ 40/160]  eta: 0:02:02  lr: 0.000033  min_lr: 0.000001  loss: 2.1599 (2.2268)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5986 (6.2600)  time: 0.9427 (0.5337 -- 3.6089)  data: 0.0215 (0.0004 -- 0.4022)  max mem: 16413
[2023-08-29 18:03:30,159] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:03:30,159] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:03:30,162] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:03:30,162] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [ 60/160]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000001  loss: 2.2628 (2.2343)  loss_scale: 32768.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6924 (6.2810)  time: 0.9300 (0.5333 -- 4.1130)  data: 0.0016 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [11]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000001  loss: 2.2160 (2.2314)  loss_scale: 32768.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7163 (6.1928)  time: 0.8426 (0.5284 -- 3.6746)  data: 0.0011 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000001  loss: 2.2265 (2.2164)  loss_scale: 32768.0000 (25143.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1332 (6.2186)  time: 0.8541 (0.5289 -- 3.6938)  data: 0.0018 (0.0007 -- 0.0040)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.1812 (2.2116)  loss_scale: 32768.0000 (26403.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1797 (6.2071)  time: 0.8280 (0.5324 -- 3.6671)  data: 0.0018 (0.0006 -- 0.0057)  max mem: 16413
[2023-08-29 18:04:49,553] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1897
[2023-08-29 18:04:49,554] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1897
[2023-08-29 18:04:49,554] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:04:49,554] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:04:49,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.0542 (2.1907)  loss_scale: 32768.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7920 (6.1918)  time: 0.8687 (0.5277 -- 4.0374)  data: 0.0014 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.0715 (2.1791)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3022 (6.2227)  time: 0.7366 (0.4952 -- 3.4156)  data: 0.0008 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [11] Total time: 0:02:22 (0.8903 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.0715 (2.1631)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3022 (6.2227)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.4390 (1.4390)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3977 (2.3977 -- 2.3977)  data: 2.1755 (2.1755 -- 2.1755)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3803 (1.4544)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (96.9697)  time: 0.4201 (0.2001 -- 2.3977)  data: 0.2069 (0.0004 -- 2.1755)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3734 (1.4284)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (96.8254)  time: 0.2128 (0.1699 -- 0.3327)  data: 0.0104 (0.0001 -- 0.0903)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4873 (1.4884)  acc1: 55.5556 (56.0166)  acc5: 100.0000 (95.0207)  time: 0.2000 (0.1332 -- 0.3327)  data: 0.0101 (0.0001 -- 0.0903)  max mem: 16413
Val: Total time: 0:00:07 (0.2832 s / it)
* Acc@1 57.469 Acc@5 94.191 loss 1.460
Accuracy of the network on the 482 val images: 57.47%
Max accuracy: 57.47%
Epoch: [12]  [  0/160]  eta: 0:18:41  lr: 0.000033  min_lr: 0.000001  loss: 1.5205 (1.5205)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8259 (7.8259)  time: 7.0097 (7.0097 -- 7.0097)  data: 5.6431 (5.6431 -- 5.6431)  max mem: 16413
Epoch: [12]  [ 20/160]  eta: 0:02:36  lr: 0.000033  min_lr: 0.000001  loss: 2.0169 (2.0155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1881 (6.4947)  time: 0.8239 (0.5190 -- 2.1555)  data: 0.2192 (0.0005 -- 1.5526)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:01:56  lr: 0.000033  min_lr: 0.000001  loss: 2.1096 (2.0745)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7162 (6.7056)  time: 0.8141 (0.5319 -- 1.9342)  data: 0.1499 (0.0002 -- 1.3895)  max mem: 16413
Epoch: [12]  [ 60/160]  eta: 0:01:40  lr: 0.000033  min_lr: 0.000001  loss: 2.1286 (2.0853)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5236 (6.4437)  time: 1.0862 (0.5127 -- 3.7439)  data: 0.4445 (0.0003 -- 3.2026)  max mem: 16413
[2023-08-29 18:06:28,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=6, lr=[7.767021625604439e-07, 7.767021625604439e-07, 1.0356028834139252e-06, 1.0356028834139252e-06, 1.3808038445519004e-06, 1.3808038445519004e-06, 1.841071792735867e-06, 1.841071792735867e-06, 2.4547623903144893e-06, 2.4547623903144893e-06, 3.2730165204193195e-06, 3.2730165204193195e-06, 4.364022027225759e-06, 4.364022027225759e-06, 5.818696036301012e-06, 5.818696036301012e-06, 7.758261381734683e-06, 7.758261381734683e-06, 1.0344348508979577e-05, 1.0344348508979577e-05, 1.3792464678639437e-05, 1.3792464678639437e-05, 1.8389952904852582e-05, 1.8389952904852582e-05, 2.451993720647011e-05, 2.451993720647011e-05, 3.269324960862681e-05, 3.269324960862681e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 18:06:28,878] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=17.22077977243502, CurrSamplesPerSec=21.454615361990555, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:14  lr: 0.000033  min_lr: 0.000001  loss: 2.0761 (2.0862)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7795 (6.4118)  time: 0.6936 (0.5258 -- 2.1249)  data: 0.0967 (0.0002 -- 1.6005)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000001  loss: 2.3406 (2.1313)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0459 (6.4623)  time: 0.9439 (0.5255 -- 2.1870)  data: 0.1780 (0.0004 -- 1.6546)  max mem: 16413
[2023-08-29 18:06:52,007] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:06:52,007] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:06:52,007] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:06:52,007] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.1794 (2.1474)  loss_scale: 32768.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5249 (6.4440)  time: 0.8534 (0.5209 -- 3.2801)  data: 0.2250 (0.0001 -- 2.7526)  max mem: 16413
[2023-08-29 18:07:06,483] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2042
[2023-08-29 18:07:06,483] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2042
[2023-08-29 18:07:06,483] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:07:06,484] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:07:06,484] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [12]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 1.9790 (2.1247)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1687 (6.3530)  time: 0.8281 (0.5305 -- 3.2479)  data: 0.2759 (0.0002 -- 2.7053)  max mem: 16413
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.1920 (2.1338)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6530 (6.4163)  time: 0.6776 (0.4975 -- 2.3092)  data: 0.1074 (0.0002 -- 1.7583)  max mem: 16413
Epoch: [12] Total time: 0:02:20 (0.8805 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.1920 (2.1390)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6530 (6.4163)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.3925 (1.3925)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.5019 (2.5019 -- 2.5019)  data: 2.2766 (2.2766 -- 2.2766)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3412 (1.4139)  acc1: 44.4444 (52.5253)  acc5: 100.0000 (95.9596)  time: 0.4205 (0.1974 -- 2.5019)  data: 0.2080 (0.0005 -- 2.2766)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3333 (1.4006)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (95.7672)  time: 0.2131 (0.1701 -- 0.3566)  data: 0.0123 (0.0001 -- 0.1547)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4510 (1.4528)  acc1: 55.5556 (53.9419)  acc5: 100.0000 (95.0207)  time: 0.1968 (0.1331 -- 0.3566)  data: 0.0119 (0.0001 -- 0.1547)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 58.714 Acc@5 94.191 loss 1.422
Accuracy of the network on the 482 val images: 58.71%
[2023-08-29 18:07:42,693] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:07:42,695] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:07:42,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:07:42,695] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:07:44,089] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:07:44,090] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 58.71%
Epoch: [13]  [  0/160]  eta: 0:17:58  lr: 0.000033  min_lr: 0.000001  loss: 1.8819 (1.8819)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0125 (6.0125)  time: 6.7431 (6.7431 -- 6.7431)  data: 5.9874 (5.9874 -- 5.9874)  max mem: 16413
Epoch: [13]  [ 20/160]  eta: 0:02:37  lr: 0.000033  min_lr: 0.000001  loss: 2.1997 (2.2012)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3241 (6.5496)  time: 0.8437 (0.5347 -- 2.4331)  data: 0.1031 (0.0004 -- 0.8997)  max mem: 16413
Epoch: [13]  [ 40/160]  eta: 0:01:59  lr: 0.000033  min_lr: 0.000001  loss: 2.1807 (2.2066)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9326 (6.6388)  time: 0.8686 (0.5280 -- 3.1788)  data: 0.1954 (0.0003 -- 1.9783)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:34  lr: 0.000033  min_lr: 0.000001  loss: 2.1254 (2.1835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3513 (6.6271)  time: 0.8292 (0.5208 -- 2.4562)  data: 0.0876 (0.0007 -- 0.7796)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000001  loss: 2.2136 (2.1866)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8388 (6.7827)  time: 0.9842 (0.5268 -- 3.7241)  data: 0.0020 (0.0004 -- 0.0108)  max mem: 16413
[2023-08-29 18:09:09,363] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:09:09,363] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:09:09,363] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:09:09,363] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [100/160]  eta: 0:00:54  lr: 0.000033  min_lr: 0.000001  loss: 2.0487 (2.1692)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4450 (6.8906)  time: 0.7155 (0.5303 -- 2.1699)  data: 0.0019 (0.0006 -- 0.0045)  max mem: 16413
[2023-08-29 18:09:20,201] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2184
[2023-08-29 18:09:20,201] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2184
[2023-08-29 18:09:20,202] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:09:20,202] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:09:20,202] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.1429 (2.1540)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3124 (6.9588)  time: 0.9330 (0.5181 -- 3.1639)  data: 0.0018 (0.0004 -- 0.0070)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.1537 (2.1357)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5777 (6.9185)  time: 1.0223 (0.5274 -- 3.8864)  data: 0.0390 (0.0005 -- 0.7552)  max mem: 16413
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.0525 (2.1217)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5359 (7.0355)  time: 0.7422 (0.4978 -- 3.7710)  data: 0.0010 (0.0002 -- 0.0061)  max mem: 16413
Epoch: [13] Total time: 0:02:21 (0.8863 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.0525 (2.1300)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5359 (7.0355)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.3516 (1.3516)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4071 (2.4071 -- 2.4071)  data: 2.1788 (2.1788 -- 2.1788)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2874 (1.3615)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (95.9596)  time: 0.4180 (0.2029 -- 2.4071)  data: 0.1992 (0.0008 -- 2.1788)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2874 (1.3432)  acc1: 55.5556 (56.0847)  acc5: 100.0000 (95.2381)  time: 0.2158 (0.1696 -- 0.3435)  data: 0.0087 (0.0001 -- 0.1524)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3471 (1.3934)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (94.6058)  time: 0.2012 (0.1336 -- 0.3435)  data: 0.0084 (0.0001 -- 0.1524)  max mem: 16413
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 60.373 Acc@5 93.776 loss 1.363
Accuracy of the network on the 482 val images: 60.37%
[2023-08-29 18:10:13,695] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:10:13,697] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:10:13,697] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:10:13,697] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:10:14,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:10:14,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 60.37%
Epoch: [14]  [  0/160]  eta: 0:22:27  lr: 0.000033  min_lr: 0.000001  loss: 2.0370 (2.0370)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7576 (6.7576)  time: 8.4240 (8.4240 -- 8.4240)  data: 7.9101 (7.9101 -- 7.9101)  max mem: 16413
Epoch: [14]  [ 20/160]  eta: 0:02:41  lr: 0.000033  min_lr: 0.000001  loss: 2.0957 (2.0992)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7940 (8.4271)  time: 0.7911 (0.5197 -- 3.3013)  data: 0.1736 (0.0003 -- 2.7621)  max mem: 16413
Epoch: [14]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000001  loss: 2.0879 (2.0861)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9469 (7.7936)  time: 0.9669 (0.5262 -- 4.8429)  data: 0.0472 (0.0002 -- 0.8993)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000001  loss: 2.1169 (2.1017)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4682 (7.5695)  time: 0.8178 (0.5290 -- 3.9990)  data: 0.2654 (0.0004 -- 3.4716)  max mem: 16413
[2023-08-29 18:11:21,605] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2307
[2023-08-29 18:11:21,605] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2307
[2023-08-29 18:11:21,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 18:11:21,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 18:11:21,605] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [14]  [ 80/160]  eta: 0:01:18  lr: 0.000033  min_lr: 0.000001  loss: 2.1632 (2.1078)  loss_scale: 8192.0000 (14968.0988)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4360 (7.4068)  time: 0.9578 (0.5235 -- 3.7253)  data: 0.3145 (0.0003 -- 2.8711)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000001  loss: 2.0985 (2.1082)  loss_scale: 8192.0000 (13626.2970)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2347 (7.4533)  time: 0.7529 (0.5163 -- 2.8910)  data: 0.0022 (0.0004 -- 0.0154)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 1.9299 (2.0862)  loss_scale: 8192.0000 (12728.0661)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6223 (7.3636)  time: 0.8581 (0.5306 -- 2.9826)  data: 0.0017 (0.0007 -- 0.0064)  max mem: 16413
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.1354 (2.0904)  loss_scale: 8192.0000 (12084.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2805 (7.3724)  time: 0.8618 (0.5280 -- 4.1341)  data: 0.0014 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.0139 (2.0846)  loss_scale: 8192.0000 (11622.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1054 (7.2965)  time: 0.6521 (0.4994 -- 2.3141)  data: 0.0011 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [14] Total time: 0:02:21 (0.8817 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.0139 (2.1176)  loss_scale: 8192.0000 (11622.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1054 (7.2965)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.3018 (1.3018)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2650 (2.2650 -- 2.2650)  data: 2.0272 (2.0272 -- 2.0272)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.2973 (1.3432)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (94.9495)  time: 0.4093 (0.1939 -- 2.2650)  data: 0.1943 (0.0007 -- 2.0272)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2253 (1.3185)  acc1: 55.5556 (57.1429)  acc5: 100.0000 (94.7090)  time: 0.2299 (0.1693 -- 0.6328)  data: 0.0275 (0.0001 -- 0.4379)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3178 (1.3825)  acc1: 55.5556 (56.8465)  acc5: 88.8889 (94.1909)  time: 0.2158 (0.1329 -- 0.6328)  data: 0.0271 (0.0001 -- 0.4379)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 60.166 Acc@5 93.568 loss 1.348
Accuracy of the network on the 482 val images: 60.17%
Max accuracy: 60.37%
Epoch: [15]  [  0/160]  eta: 0:17:56  lr: 0.000033  min_lr: 0.000001  loss: 2.0182 (2.0182)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5434 (8.5434)  time: 6.7310 (6.7310 -- 6.7310)  data: 6.0578 (6.0578 -- 6.0578)  max mem: 16413
Epoch: [15]  [ 20/160]  eta: 0:02:50  lr: 0.000033  min_lr: 0.000001  loss: 2.1704 (2.1108)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8773 (7.0465)  time: 0.9395 (0.5313 -- 3.1545)  data: 0.2898 (0.0004 -- 2.1926)  max mem: 16413
[2023-08-29 18:13:23,005] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:13:23,005] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:13:23,005] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 18:13:23,005] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [15]  [ 40/160]  eta: 0:02:00  lr: 0.000033  min_lr: 0.000001  loss: 2.1281 (2.1039)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4455 (7.1038)  time: 0.7881 (0.5249 -- 2.4656)  data: 0.2186 (0.0004 -- 1.9292)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:36  lr: 0.000033  min_lr: 0.000001  loss: 2.1101 (2.1105)  loss_scale: 16384.0000 (11549.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0995 (7.1013)  time: 0.8706 (0.5279 -- 3.0014)  data: 0.3210 (0.0004 -- 2.4578)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000001  loss: 1.9909 (2.1006)  loss_scale: 16384.0000 (12743.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3037 (7.1471)  time: 0.9574 (0.5121 -- 4.0413)  data: 0.4091 (0.0006 -- 3.5195)  max mem: 16413
Epoch: [15]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000001  loss: 2.0910 (2.1086)  loss_scale: 16384.0000 (13464.0792)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3115 (7.2245)  time: 0.8278 (0.5161 -- 4.5218)  data: 0.2838 (0.0005 -- 3.9758)  max mem: 16413
Epoch: [15]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000001  loss: 2.0918 (2.1008)  loss_scale: 16384.0000 (13946.7107)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3848 (7.3677)  time: 0.8776 (0.5324 -- 2.8851)  data: 0.2757 (0.0004 -- 2.3449)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.1779 (2.1148)  loss_scale: 16384.0000 (14292.4255)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0654 (7.3520)  time: 0.8616 (0.5288 -- 4.0510)  data: 0.3077 (0.0003 -- 3.5441)  max mem: 16413
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.1828 (2.1297)  loss_scale: 16384.0000 (14540.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6916 (7.2942)  time: 0.6638 (0.4951 -- 2.0479)  data: 0.1453 (0.0002 -- 1.5329)  max mem: 16413
Epoch: [15] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.1828 (2.1044)  loss_scale: 16384.0000 (14540.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6916 (7.2942)
Val:  [ 0/27]  eta: 0:01:08  loss: 1.3327 (1.3327)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5352 (2.5352 -- 2.5352)  data: 2.2516 (2.2516 -- 2.2516)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2502 (1.3045)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (95.9596)  time: 0.4261 (0.2022 -- 2.5352)  data: 0.2058 (0.0007 -- 2.2516)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1627 (1.2826)  acc1: 55.5556 (58.2011)  acc5: 100.0000 (95.2381)  time: 0.2141 (0.1698 -- 0.3863)  data: 0.0102 (0.0001 -- 0.1883)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2974 (1.3339)  acc1: 55.5556 (58.5062)  acc5: 100.0000 (95.0207)  time: 0.1973 (0.1322 -- 0.3863)  data: 0.0098 (0.0001 -- 0.1883)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 61.618 Acc@5 94.191 loss 1.305
Accuracy of the network on the 482 val images: 61.62%
[2023-08-29 18:15:13,731] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:15:13,733] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:15:13,733] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:15:13,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:15:15,102] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:15:15,102] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 61.62%
Epoch: [16]  [  0/160]  eta: 0:23:15  lr: 0.000033  min_lr: 0.000001  loss: 2.0489 (2.0489)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9297 (11.9297)  time: 8.7236 (8.7236 -- 8.7236)  data: 8.1696 (8.1696 -- 8.1696)  max mem: 16413
[2023-08-29 18:15:25,987] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:15:25,987] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:15:25,988] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:15:25,988] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 20/160]  eta: 0:02:51  lr: 0.000033  min_lr: 0.000001  loss: 2.0192 (2.1114)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7426 (7.6936)  time: 0.8519 (0.5308 -- 3.5865)  data: 0.3039 (0.0003 -- 3.0613)  max mem: 16413
Epoch: [16]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000001  loss: 2.1115 (2.0882)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7805 (7.3912)  time: 0.8951 (0.5164 -- 3.0242)  data: 0.3475 (0.0007 -- 2.4896)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:35  lr: 0.000033  min_lr: 0.000001  loss: 2.0356 (2.0840)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8112 (7.2234)  time: 0.7165 (0.5300 -- 2.5868)  data: 0.1653 (0.0003 -- 2.0409)  max mem: 16413
[2023-08-29 18:16:17,496] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2626
[2023-08-29 18:16:17,496] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2626
[2023-08-29 18:16:17,496] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:16:17,496] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:16:17,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 80/160]  eta: 0:01:15  lr: 0.000033  min_lr: 0.000001  loss: 2.0410 (2.0870)  loss_scale: 16384.0000 (28924.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0369 (7.5406)  time: 0.9247 (0.5345 -- 3.1257)  data: 0.2485 (0.0007 -- 2.5995)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:54  lr: 0.000033  min_lr: 0.000001  loss: 2.1132 (2.0880)  loss_scale: 16384.0000 (26441.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9307 (7.7504)  time: 0.8015 (0.5324 -- 2.0495)  data: 0.1340 (0.0008 -- 1.2952)  max mem: 16413
Epoch: [16]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 2.1840 (2.1037)  loss_scale: 16384.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3761 (7.6992)  time: 0.8622 (0.5278 -- 3.8991)  data: 0.0513 (0.0007 -- 0.5052)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 2.1985 (2.1060)  loss_scale: 16384.0000 (23588.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9834 (7.5821)  time: 0.9249 (0.5308 -- 3.3133)  data: 0.0973 (0.0004 -- 0.8549)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 2.2000 (2.1148)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7942 (7.4760)  time: 0.6977 (0.4951 -- 3.2635)  data: 0.0010 (0.0002 -- 0.0072)  max mem: 16413
Epoch: [16] Total time: 0:02:21 (0.8857 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 2.2000 (2.1125)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7942 (7.4760)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.2735 (1.2735)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2976 (2.2976 -- 2.2976)  data: 2.0702 (2.0702 -- 2.0702)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1476 (1.2808)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4362 (0.2143 -- 2.2976)  data: 0.2111 (0.0010 -- 2.0702)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1476 (1.2647)  acc1: 55.5556 (58.7302)  acc5: 100.0000 (95.2381)  time: 0.2288 (0.1751 -- 0.4996)  data: 0.0174 (0.0001 -- 0.2377)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2511 (1.3117)  acc1: 55.5556 (58.9212)  acc5: 100.0000 (95.0207)  time: 0.2105 (0.1327 -- 0.4996)  data: 0.0170 (0.0001 -- 0.2377)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 62.448 Acc@5 94.398 loss 1.276
Accuracy of the network on the 482 val images: 62.45%
[2023-08-29 18:17:44,684] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:17:44,686] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:17:44,686] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:17:44,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:17:46,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:17:46,053] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.45%
Epoch: [17]  [  0/160]  eta: 0:21:47  lr: 0.000033  min_lr: 0.000001  loss: 2.5276 (2.5276)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4880 (5.4880)  time: 8.1723 (8.1723 -- 8.1723)  data: 4.5341 (4.5341 -- 4.5341)  max mem: 16413
Epoch: [17]  [ 20/160]  eta: 0:02:45  lr: 0.000033  min_lr: 0.000001  loss: 1.9338 (2.0263)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4373 (7.5300)  time: 0.8325 (0.5151 -- 3.6170)  data: 0.0985 (0.0006 -- 1.8111)  max mem: 16413
[2023-08-29 18:18:24,725] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:18:24,725] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:18:24,726] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:18:24,726] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 40/160]  eta: 0:02:04  lr: 0.000032  min_lr: 0.000001  loss: 1.9828 (2.0236)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2807 (7.1664)  time: 0.8816 (0.5254 -- 2.3050)  data: 0.1353 (0.0003 -- 1.5814)  max mem: 16413
Epoch: [17]  [ 60/160]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000001  loss: 1.9860 (2.0328)  loss_scale: 32768.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3621 (7.0958)  time: 0.8607 (0.5299 -- 2.6274)  data: 0.3115 (0.0004 -- 2.0978)  max mem: 16413
Epoch: [17]  [ 80/160]  eta: 0:01:17  lr: 0.000032  min_lr: 0.000001  loss: 2.0658 (2.0387)  loss_scale: 32768.0000 (25688.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8778 (7.2084)  time: 0.9237 (0.5334 -- 4.2147)  data: 0.1424 (0.0004 -- 1.1485)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:54  lr: 0.000032  min_lr: 0.000001  loss: 2.1024 (2.0512)  loss_scale: 32768.0000 (27090.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1863 (7.1737)  time: 0.7076 (0.5390 -- 2.1564)  data: 0.0516 (0.0003 -- 0.5059)  max mem: 16413
Epoch: [17]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000001  loss: 1.9192 (2.0449)  loss_scale: 32768.0000 (28028.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7107 (7.1134)  time: 1.0157 (0.5184 -- 3.9823)  data: 0.0016 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.8991 (2.0248)  loss_scale: 32768.0000 (28701.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7441 (7.0686)  time: 0.8556 (0.5212 -- 3.0048)  data: 0.0012 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.8777 (2.0199)  loss_scale: 32768.0000 (29184.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5693 (7.0070)  time: 0.6436 (0.4965 -- 1.9003)  data: 0.0010 (0.0002 -- 0.0063)  max mem: 16413
Epoch: [17] Total time: 0:02:20 (0.8795 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.8777 (2.0454)  loss_scale: 32768.0000 (29184.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5693 (7.0070)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.2191 (1.2191)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3272 (2.3272 -- 2.3272)  data: 2.1095 (2.1095 -- 2.1095)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2191 (1.2726)  acc1: 66.6667 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4168 (0.2055 -- 2.3272)  data: 0.2033 (0.0006 -- 2.1095)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1522 (1.2462)  acc1: 66.6667 (59.2593)  acc5: 100.0000 (95.7672)  time: 0.2204 (0.1702 -- 0.3606)  data: 0.0145 (0.0001 -- 0.1606)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2521 (1.3042)  acc1: 55.5556 (57.6763)  acc5: 100.0000 (94.1909)  time: 0.2065 (0.1330 -- 0.3606)  data: 0.0141 (0.0001 -- 0.1606)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 61.618 Acc@5 93.983 loss 1.268
Accuracy of the network on the 482 val images: 61.62%
Max accuracy: 62.45%
Epoch: [18]  [  0/160]  eta: 0:20:14  lr: 0.000032  min_lr: 0.000001  loss: 2.1521 (2.1521)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8764 (7.8764)  time: 7.5876 (7.5876 -- 7.5876)  data: 7.0498 (7.0498 -- 7.0498)  max mem: 16413
[2023-08-29 18:20:23,897] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:20:23,897] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:20:23,897] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-29 18:20:23,897] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-29 18:20:24,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2885
[2023-08-29 18:20:24,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2885
[2023-08-29 18:20:24,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-29 18:20:24,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-29 18:20:24,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 20/160]  eta: 0:02:45  lr: 0.000032  min_lr: 0.000001  loss: 2.0384 (2.0838)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9816 (7.3135)  time: 0.8633 (0.5102 -- 4.1592)  data: 0.3152 (0.0004 -- 3.6086)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:04  lr: 0.000032  min_lr: 0.000001  loss: 2.1664 (2.0868)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4506 (7.1423)  time: 0.8784 (0.5174 -- 3.9929)  data: 0.2635 (0.0002 -- 3.4678)  max mem: 16413
Epoch: [18]  [ 60/160]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000001  loss: 2.2386 (2.1296)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4000 (7.3599)  time: 0.8411 (0.5237 -- 2.2717)  data: 0.1066 (0.0003 -- 1.7298)  max mem: 16413
Epoch: [18]  [ 80/160]  eta: 0:01:14  lr: 0.000032  min_lr: 0.000001  loss: 1.9805 (2.0899)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3165 (7.4659)  time: 0.7856 (0.5209 -- 3.7112)  data: 0.0691 (0.0002 -- 1.1265)  max mem: 16413
[2023-08-29 18:21:41,631] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2971
[2023-08-29 18:21:41,632] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:21:41,632] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2971
[2023-08-29 18:21:41,632] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:21:41,632] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [18]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 2.1445 (2.0951)  loss_scale: 16384.0000 (31794.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8589 (7.4249)  time: 0.9021 (0.5321 -- 4.5671)  data: 0.0904 (0.0006 -- 1.3998)  max mem: 16413
[2023-08-29 18:22:02,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[7.700330990392231e-07, 7.700330990392231e-07, 1.0267107987189642e-06, 1.0267107987189642e-06, 1.3689477316252854e-06, 1.3689477316252854e-06, 1.8252636421670473e-06, 1.8252636421670473e-06, 2.4336848562227296e-06, 2.4336848562227296e-06, 3.244913141630306e-06, 3.244913141630306e-06, 4.326550855507075e-06, 4.326550855507075e-06, 5.768734474009433e-06, 5.768734474009433e-06, 7.691645965345912e-06, 7.691645965345912e-06, 1.0255527953794548e-05, 1.0255527953794548e-05, 1.3674037271726065e-05, 1.3674037271726065e-05, 1.8232049695634754e-05, 1.8232049695634754e-05, 2.4309399594179672e-05, 2.4309399594179672e-05, 3.241253279223956e-05, 3.241253279223956e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 18:22:02,920] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=17.320665374059022, CurrSamplesPerSec=22.980493929750388, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:35  lr: 0.000032  min_lr: 0.000001  loss: 2.1959 (2.1092)  loss_scale: 16384.0000 (29247.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7332 (7.5581)  time: 0.7926 (0.5165 -- 3.2027)  data: 0.0019 (0.0003 -- 0.0083)  max mem: 16413
Epoch: [18]  [140/160]  eta: 0:00:17  lr: 0.000032  min_lr: 0.000001  loss: 1.9596 (2.1060)  loss_scale: 16384.0000 (27422.8652)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8673 (7.4387)  time: 0.8257 (0.5319 -- 2.4851)  data: 0.0163 (0.0003 -- 0.2936)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.9541 (2.0953)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7197 (7.3471)  time: 0.7893 (0.4938 -- 3.1280)  data: 0.2616 (0.0001 -- 2.6231)  max mem: 16413
Epoch: [18] Total time: 0:02:20 (0.8788 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.9541 (2.0709)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7197 (7.3471)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.1775 (1.1775)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3745 (2.3745 -- 2.3745)  data: 2.1262 (2.1262 -- 2.1262)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0921 (1.2305)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4197 (0.1988 -- 2.3745)  data: 0.2076 (0.0005 -- 2.1262)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0921 (1.2134)  acc1: 55.5556 (61.3757)  acc5: 100.0000 (96.2963)  time: 0.2245 (0.1701 -- 0.5025)  data: 0.0242 (0.0001 -- 0.3228)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2467 (1.2698)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.4357)  time: 0.2105 (0.1328 -- 0.5025)  data: 0.0238 (0.0001 -- 0.3228)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 63.485 Acc@5 95.021 loss 1.229
Accuracy of the network on the 482 val images: 63.49%
[2023-08-29 18:22:43,124] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:22:43,125] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:22:43,125] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:22:43,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:22:44,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:22:44,669] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.49%
Epoch: [19]  [  0/160]  eta: 0:17:20  lr: 0.000032  min_lr: 0.000001  loss: 1.7683 (1.7683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0266 (6.0266)  time: 6.5045 (6.5045 -- 6.5045)  data: 5.9771 (5.9771 -- 5.9771)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:40  lr: 0.000032  min_lr: 0.000001  loss: 1.9330 (1.9406)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5353 (6.8033)  time: 0.8763 (0.5268 -- 3.3132)  data: 0.1625 (0.0004 -- 2.4355)  max mem: 16413
Epoch: [19]  [ 40/160]  eta: 0:01:56  lr: 0.000032  min_lr: 0.000001  loss: 2.0087 (1.9916)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5242 (7.3650)  time: 0.7827 (0.5269 -- 4.6139)  data: 0.0667 (0.0003 -- 0.7482)  max mem: 16413
[2023-08-29 18:23:44,112] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:23:44,112] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:23:44,112] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:23:44,112] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [ 60/160]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000001  loss: 2.1805 (2.0499)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3337 (7.1081)  time: 0.9879 (0.5288 -- 3.8605)  data: 0.3978 (0.0006 -- 3.3463)  max mem: 16413
[2023-08-29 18:23:51,030] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3108
[2023-08-29 18:23:51,031] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3108
[2023-08-29 18:23:51,031] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:23:51,031] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:23:51,031] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [ 80/160]  eta: 0:01:15  lr: 0.000032  min_lr: 0.000001  loss: 1.9342 (2.0223)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8981 (7.2859)  time: 0.8598 (0.5186 -- 3.0631)  data: 0.3134 (0.0002 -- 2.5183)  max mem: 16413
Epoch: [19]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 1.8898 (2.0154)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7121 (7.3990)  time: 0.8587 (0.5298 -- 3.7985)  data: 0.2877 (0.0008 -- 3.2776)  max mem: 16413
Epoch: [19]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 2.0549 (2.0264)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7809 (7.3848)  time: 0.8348 (0.5306 -- 2.5475)  data: 0.2215 (0.0002 -- 1.9947)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 2.1152 (2.0349)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5512 (7.3930)  time: 0.8846 (0.5372 -- 2.2699)  data: 0.2173 (0.0009 -- 1.7345)  max mem: 16413
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0832 (2.0326)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2367 (7.3260)  time: 0.7268 (0.4950 -- 2.0574)  data: 0.1620 (0.0002 -- 1.5260)  max mem: 16413
Epoch: [19] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0832 (2.0499)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2367 (7.3260)
[2023-08-29 18:25:06,055] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-08-29 18:25:06,056] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-08-29 18:25:06,056] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-08-29 18:25:06,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-08-29 18:25:07,041] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-08-29 18:25:07,041] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 1.1918 (1.1918)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3535 (2.3535 -- 2.3535)  data: 2.1407 (2.1407 -- 2.1407)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1648 (1.2341)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (94.9495)  time: 0.4330 (0.1895 -- 2.3535)  data: 0.2186 (0.0004 -- 2.1407)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1477 (1.2007)  acc1: 55.5556 (60.8466)  acc5: 100.0000 (95.7672)  time: 0.2258 (0.1703 -- 0.4944)  data: 0.0186 (0.0001 -- 0.2539)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2636 (1.2672)  acc1: 55.5556 (58.9212)  acc5: 100.0000 (94.6058)  time: 0.2113 (0.1330 -- 0.4944)  data: 0.0183 (0.0001 -- 0.2539)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 62.656 Acc@5 94.191 loss 1.222
Accuracy of the network on the 482 val images: 62.66%
Max accuracy: 63.49%
Epoch: [20]  [  0/160]  eta: 0:17:08  lr: 0.000032  min_lr: 0.000001  loss: 2.3571 (2.3571)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0938 (4.0937)  time: 6.4277 (6.4277 -- 6.4277)  data: 4.6218 (4.6218 -- 4.6218)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:37  lr: 0.000032  min_lr: 0.000001  loss: 1.9938 (2.0245)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6143 (7.4757)  time: 0.8570 (0.5258 -- 3.3621)  data: 0.1044 (0.0007 -- 1.5538)  max mem: 16413
[2023-08-29 18:25:54,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:25:54,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:25:54,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:25:54,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [ 40/160]  eta: 0:02:02  lr: 0.000032  min_lr: 0.000001  loss: 2.1447 (2.0733)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5076 (7.3638)  time: 0.9075 (0.5390 -- 4.5413)  data: 0.0279 (0.0005 -- 0.5325)  max mem: 16413
[2023-08-29 18:26:03,767] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3248
[2023-08-29 18:26:03,767] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3248
[2023-08-29 18:26:03,767] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:26:03,767] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:26:03,767] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [ 60/160]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000001  loss: 2.1200 (2.0732)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4950 (7.4174)  time: 0.8845 (0.5226 -- 3.2543)  data: 0.2506 (0.0007 -- 2.7143)  max mem: 16413
Epoch: [20]  [ 80/160]  eta: 0:01:15  lr: 0.000032  min_lr: 0.000001  loss: 2.0347 (2.0565)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3290 (7.2664)  time: 0.8671 (0.5295 -- 4.4678)  data: 0.3155 (0.0003 -- 3.9485)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 1.9989 (2.0503)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4423 (7.1699)  time: 0.8624 (0.5388 -- 2.8689)  data: 0.1419 (0.0011 -- 1.5574)  max mem: 16413
Epoch: [20]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 2.0527 (2.0457)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0455 (7.1771)  time: 0.8181 (0.5346 -- 3.5848)  data: 0.0351 (0.0003 -- 0.6782)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.9830 (2.0401)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2461 (7.2113)  time: 0.9259 (0.5366 -- 4.6516)  data: 0.0115 (0.0004 -- 0.1859)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.1207 (2.0493)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7443 (7.2706)  time: 0.6156 (0.4972 -- 1.3645)  data: 0.0611 (0.0002 -- 0.8613)  max mem: 16413
Epoch: [20] Total time: 0:02:20 (0.8792 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.1207 (2.0470)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7443 (7.2706)
Val:  [ 0/27]  eta: 0:00:57  loss: 1.2059 (1.2059)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1436 (2.1436 -- 2.1436)  data: 1.9325 (1.9325 -- 1.9325)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1240 (1.1978)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (96.9697)  time: 0.4155 (0.2008 -- 2.1436)  data: 0.1996 (0.0005 -- 1.9325)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0214 (1.1697)  acc1: 55.5556 (59.7884)  acc5: 100.0000 (96.8254)  time: 0.2326 (0.1695 -- 0.4773)  data: 0.0255 (0.0001 -- 0.2532)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1594 (1.2165)  acc1: 55.5556 (58.9212)  acc5: 100.0000 (95.4357)  time: 0.2182 (0.1335 -- 0.4773)  data: 0.0253 (0.0001 -- 0.2532)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 63.900 Acc@5 94.606 loss 1.173
Accuracy of the network on the 482 val images: 63.90%
[2023-08-29 18:27:43,408] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:27:43,410] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:27:43,410] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:27:43,410] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:27:44,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:27:44,872] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.90%
Epoch: [21]  [  0/160]  eta: 0:24:47  lr: 0.000032  min_lr: 0.000001  loss: 2.1147 (2.1147)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1246 (10.1246)  time: 9.2965 (9.2965 -- 9.2965)  data: 8.7939 (8.7939 -- 8.7939)  max mem: 16413
[2023-08-29 18:28:08,709] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:28:08,710] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:28:08,710] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:28:08,710] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 20/160]  eta: 0:02:50  lr: 0.000032  min_lr: 0.000001  loss: 2.0478 (2.0782)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0787 (7.4877)  time: 0.8126 (0.5241 -- 3.5517)  data: 0.2652 (0.0004 -- 3.0297)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:07  lr: 0.000032  min_lr: 0.000001  loss: 1.9794 (2.0429)  loss_scale: 32768.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8055 (7.1375)  time: 0.9036 (0.5176 -- 3.9217)  data: 0.3645 (0.0002 -- 3.4139)  max mem: 16413
[2023-08-29 18:28:29,002] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3401
[2023-08-29 18:28:29,003] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:28:29,003] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3401
[2023-08-29 18:28:29,003] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:28:29,004] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [ 60/160]  eta: 0:01:35  lr: 0.000032  min_lr: 0.000001  loss: 2.2035 (2.0592)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7163 (7.3302)  time: 0.7181 (0.5042 -- 2.7652)  data: 0.1647 (0.0002 -- 2.2422)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:16  lr: 0.000032  min_lr: 0.000001  loss: 1.9249 (2.0342)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7507 (7.4008)  time: 0.9931 (0.5288 -- 4.5900)  data: 0.3590 (0.0003 -- 4.0764)  max mem: 16413
[2023-08-29 18:29:04,919] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3444
[2023-08-29 18:29:04,919] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3444
[2023-08-29 18:29:04,919] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 18:29:04,919] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 18:29:04,919] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [21]  [100/160]  eta: 0:00:54  lr: 0.000032  min_lr: 0.000001  loss: 2.0455 (2.0319)  loss_scale: 8192.0000 (18898.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4729 (7.4610)  time: 0.7297 (0.5190 -- 3.1074)  data: 0.1793 (0.0004 -- 2.5473)  max mem: 16413
Epoch: [21]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 2.0287 (2.0293)  loss_scale: 8192.0000 (17128.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7200 (7.5290)  time: 0.9728 (0.5202 -- 4.1731)  data: 0.4208 (0.0004 -- 3.6518)  max mem: 16413
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.8703 (2.0207)  loss_scale: 8192.0000 (15861.1064)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3942 (7.7397)  time: 0.7543 (0.5346 -- 2.0003)  data: 0.1289 (0.0003 -- 1.0351)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0499 (2.0233)  loss_scale: 8192.0000 (14950.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0635 (7.6992)  time: 0.7143 (0.4973 -- 3.3584)  data: 0.0352 (0.0002 -- 0.6875)  max mem: 16413
Epoch: [21] Total time: 0:02:20 (0.8754 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0499 (2.0228)  loss_scale: 8192.0000 (14950.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0635 (7.6992)
Val:  [ 0/27]  eta: 0:00:58  loss: 1.1581 (1.1581)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1554 (2.1554 -- 2.1554)  data: 1.9413 (1.9413 -- 1.9413)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0790 (1.1560)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4183 (0.1891 -- 2.1554)  data: 0.2094 (0.0006 -- 1.9413)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0229 (1.1259)  acc1: 55.5556 (61.3757)  acc5: 100.0000 (96.2963)  time: 0.2337 (0.1700 -- 0.4705)  data: 0.0321 (0.0001 -- 0.2375)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1486 (1.1763)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.4357)  time: 0.2219 (0.1335 -- 0.4705)  data: 0.0319 (0.0001 -- 0.2375)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 63.900 Acc@5 94.813 loss 1.140
Accuracy of the network on the 482 val images: 63.90%
[2023-08-29 18:30:12,808] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:30:12,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:30:12,810] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:30:12,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:30:14,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:30:14,266] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.90%
Epoch: [22]  [  0/160]  eta: 0:22:18  lr: 0.000032  min_lr: 0.000001  loss: 1.8292 (1.8292)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5293 (4.5293)  time: 8.3639 (8.3639 -- 8.3639)  data: 7.8305 (7.8305 -- 7.8305)  max mem: 16413
Epoch: [22]  [ 20/160]  eta: 0:02:52  lr: 0.000032  min_lr: 0.000001  loss: 2.0772 (1.9826)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6374 (7.7985)  time: 0.8767 (0.5249 -- 2.7994)  data: 0.1546 (0.0001 -- 1.8606)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:01  lr: 0.000032  min_lr: 0.000001  loss: 2.1038 (2.0500)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1866 (7.9432)  time: 0.7834 (0.5188 -- 3.4850)  data: 0.0020 (0.0004 -- 0.0153)  max mem: 16413
[2023-08-29 18:31:08,055] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:31:08,055] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:31:08,055] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 18:31:08,055] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [22]  [ 60/160]  eta: 0:01:39  lr: 0.000032  min_lr: 0.000001  loss: 2.2591 (2.1025)  loss_scale: 8192.0000 (9266.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3288 (7.8060)  time: 0.9445 (0.5296 -- 3.3539)  data: 0.0195 (0.0001 -- 0.3681)  max mem: 16413
Epoch: [22]  [ 80/160]  eta: 0:01:15  lr: 0.000032  min_lr: 0.000001  loss: 2.0614 (2.0841)  loss_scale: 16384.0000 (11023.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4295 (7.8993)  time: 0.7749 (0.5254 -- 3.3346)  data: 0.0017 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:54  lr: 0.000032  min_lr: 0.000001  loss: 2.1078 (2.0755)  loss_scale: 16384.0000 (12085.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2694 (7.7525)  time: 0.7981 (0.5350 -- 2.5829)  data: 0.0015 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [22]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 2.0755 (2.0674)  loss_scale: 16384.0000 (12795.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9419 (7.6177)  time: 0.9199 (0.5171 -- 4.1119)  data: 0.0815 (0.0004 -- 1.5950)  max mem: 16413
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 2.0052 (2.0674)  loss_scale: 16384.0000 (13304.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0815 (7.6789)  time: 0.9539 (0.5305 -- 2.5752)  data: 0.0825 (0.0005 -- 0.8739)  max mem: 16413
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0389 (2.0620)  loss_scale: 16384.0000 (13670.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5286 (7.5769)  time: 0.7205 (0.4952 -- 2.5752)  data: 0.0145 (0.0002 -- 0.2628)  max mem: 16413
Epoch: [22] Total time: 0:02:21 (0.8829 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0389 (2.0353)  loss_scale: 16384.0000 (13670.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5286 (7.5769)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.1926 (1.1926)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3955 (2.3955 -- 2.3955)  data: 2.1779 (2.1779 -- 2.1779)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1139 (1.1603)  acc1: 66.6667 (58.5859)  acc5: 100.0000 (96.9697)  time: 0.4234 (0.2072 -- 2.3955)  data: 0.1993 (0.0008 -- 2.1779)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0434 (1.1237)  acc1: 66.6667 (60.8466)  acc5: 100.0000 (96.8254)  time: 0.2268 (0.1694 -- 0.5127)  data: 0.0176 (0.0001 -- 0.3347)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0997 (1.1720)  acc1: 55.5556 (61.4108)  acc5: 100.0000 (95.4357)  time: 0.2078 (0.1325 -- 0.5127)  data: 0.0171 (0.0001 -- 0.3347)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 64.315 Acc@5 94.813 loss 1.132
Accuracy of the network on the 482 val images: 64.32%
[2023-08-29 18:32:43,459] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:32:43,460] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:32:43,460] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:32:43,460] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:32:44,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:32:44,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.32%
Epoch: [23]  [  0/160]  eta: 0:23:34  lr: 0.000032  min_lr: 0.000001  loss: 2.2766 (2.2766)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5947 (8.5947)  time: 8.8424 (8.8424 -- 8.8424)  data: 8.3010 (8.3010 -- 8.3010)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:44  lr: 0.000032  min_lr: 0.000001  loss: 2.0068 (2.0182)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1323 (7.3240)  time: 0.7896 (0.5145 -- 5.0799)  data: 0.2438 (0.0004 -- 4.5582)  max mem: 16413
[2023-08-29 18:33:10,023] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:33:10,023] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:33:10,024] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:33:10,024] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [ 40/160]  eta: 0:02:04  lr: 0.000032  min_lr: 0.000001  loss: 1.9832 (2.0019)  loss_scale: 32768.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4664 (7.6618)  time: 0.8977 (0.5315 -- 3.2846)  data: 0.3478 (0.0002 -- 2.7415)  max mem: 16413
Epoch: [23]  [ 60/160]  eta: 0:01:43  lr: 0.000032  min_lr: 0.000001  loss: 2.2144 (2.0685)  loss_scale: 32768.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9505 (7.7775)  time: 1.0298 (0.5210 -- 4.3158)  data: 0.4851 (0.0003 -- 3.8109)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:17  lr: 0.000032  min_lr: 0.000001  loss: 2.1278 (2.0671)  loss_scale: 32768.0000 (28520.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9377 (7.5157)  time: 0.7757 (0.5201 -- 3.1948)  data: 0.2349 (0.0003 -- 2.6673)  max mem: 16413
[2023-08-29 18:34:13,516] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3771
[2023-08-29 18:34:13,516] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3771
[2023-08-29 18:34:13,517] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:34:13,517] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:34:13,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [100/160]  eta: 0:00:57  lr: 0.000032  min_lr: 0.000001  loss: 2.0762 (2.0596)  loss_scale: 16384.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7154 (7.5698)  time: 0.8850 (0.5248 -- 3.2811)  data: 0.3383 (0.0001 -- 2.7614)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 2.0431 (2.0611)  loss_scale: 16384.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4797 (7.5652)  time: 0.7341 (0.5288 -- 1.7824)  data: 0.1405 (0.0007 -- 1.2412)  max mem: 16413
Epoch: [23]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.9739 (2.0566)  loss_scale: 16384.0000 (24517.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8744 (7.7131)  time: 0.8623 (0.5153 -- 3.5735)  data: 0.1622 (0.0005 -- 1.8829)  max mem: 16413
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.9718 (2.0551)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5023 (7.7605)  time: 0.7518 (0.4971 -- 2.5588)  data: 0.0143 (0.0002 -- 0.1782)  max mem: 16413
Epoch: [23] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.9718 (2.0545)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5023 (7.7605)
Val:  [ 0/27]  eta: 0:00:59  loss: 1.1864 (1.1864)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2158 (2.2158 -- 2.2158)  data: 2.0057 (2.0057 -- 2.0057)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1333 (1.1533)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4297 (0.2048 -- 2.2158)  data: 0.2152 (0.0007 -- 2.0057)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0602 (1.1031)  acc1: 55.5556 (61.3757)  acc5: 100.0000 (96.8254)  time: 0.2293 (0.1691 -- 0.5629)  data: 0.0247 (0.0001 -- 0.3499)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0989 (1.1563)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.4357)  time: 0.2139 (0.1336 -- 0.5629)  data: 0.0243 (0.0001 -- 0.3499)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 64.108 Acc@5 94.813 loss 1.124
Accuracy of the network on the 482 val images: 64.11%
Max accuracy: 64.32%
Epoch: [24]  [  0/160]  eta: 0:20:49  lr: 0.000032  min_lr: 0.000001  loss: 1.7091 (1.7091)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9009 (6.9009)  time: 7.8098 (7.8098 -- 7.8098)  data: 6.3595 (6.3595 -- 6.3595)  max mem: 16413
Epoch: [24]  [ 20/160]  eta: 0:02:45  lr: 0.000032  min_lr: 0.000001  loss: 2.0641 (1.9755)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4897 (8.4092)  time: 0.8491 (0.5294 -- 3.9082)  data: 0.1496 (0.0004 -- 1.5640)  max mem: 16413
Epoch: [24]  [ 40/160]  eta: 0:02:00  lr: 0.000032  min_lr: 0.000001  loss: 1.9095 (1.9492)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8636 (7.8401)  time: 0.8170 (0.5297 -- 2.3499)  data: 0.2344 (0.0003 -- 1.8198)  max mem: 16413
[2023-08-29 18:36:17,567] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:36:17,567] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:36:17,568] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:36:17,568] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 60/160]  eta: 0:01:41  lr: 0.000032  min_lr: 0.000001  loss: 1.9200 (1.9476)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0736 (7.7945)  time: 1.0441 (0.5261 -- 5.3318)  data: 0.4493 (0.0004 -- 4.8054)  max mem: 16413
[2023-08-29 18:36:29,070] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3915
[2023-08-29 18:36:29,070] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3915
[2023-08-29 18:36:29,070] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:36:29,070] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:36:29,071] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [ 80/160]  eta: 0:01:18  lr: 0.000032  min_lr: 0.000001  loss: 2.0317 (1.9761)  loss_scale: 32768.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1629 (7.8872)  time: 0.8628 (0.5167 -- 3.9120)  data: 0.3233 (0.0004 -- 3.3778)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:57  lr: 0.000032  min_lr: 0.000001  loss: 1.7301 (1.9376)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9305 (7.9639)  time: 0.8879 (0.5119 -- 4.2347)  data: 0.3396 (0.0003 -- 3.7052)  max mem: 16413
Epoch: [24]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000001  loss: 1.9260 (1.9364)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0381 (8.0856)  time: 0.7756 (0.5277 -- 3.5521)  data: 0.2237 (0.0003 -- 3.0138)  max mem: 16413
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 2.0566 (1.9561)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9087 (8.1179)  time: 0.8814 (0.5299 -- 2.8348)  data: 0.3227 (0.0003 -- 2.2820)  max mem: 16413
[2023-08-29 18:37:37,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[7.595174613802405e-07, 7.595174613802405e-07, 1.0126899485069874e-06, 1.0126899485069874e-06, 1.3502532646759832e-06, 1.3502532646759832e-06, 1.8003376862346443e-06, 1.8003376862346443e-06, 2.400450248312859e-06, 2.400450248312859e-06, 3.2006003310838117e-06, 3.2006003310838117e-06, 4.267467108111749e-06, 4.267467108111749e-06, 5.689956144148999e-06, 5.689956144148999e-06, 7.586608192198665e-06, 7.586608192198665e-06, 1.011547758959822e-05, 1.011547758959822e-05, 1.3487303452797627e-05, 1.3487303452797627e-05, 1.7983071270396835e-05, 1.7983071270396835e-05, 2.3977428360529113e-05, 2.3977428360529113e-05, 3.1969904480705487e-05, 3.1969904480705487e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 18:37:37,757] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=17.666215394169893, CurrSamplesPerSec=24.5823736525437, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0392 (1.9644)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2129 (8.0728)  time: 0.7371 (0.4963 -- 2.7593)  data: 0.2159 (0.0002 -- 2.1967)  max mem: 16413
Epoch: [24] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0392 (1.9747)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2129 (8.0728)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.1719 (1.1719)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4441 (2.4441 -- 2.4441)  data: 2.1953 (2.1953 -- 2.1953)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1659 (1.1127)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4162 (0.1992 -- 2.4441)  data: 0.2004 (0.0007 -- 2.1953)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0206 (1.0802)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (96.2963)  time: 0.2166 (0.1697 -- 0.4631)  data: 0.0137 (0.0001 -- 0.2626)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0721 (1.1276)  acc1: 55.5556 (60.5809)  acc5: 100.0000 (95.0207)  time: 0.2014 (0.1326 -- 0.4631)  data: 0.0135 (0.0001 -- 0.2626)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 64.523 Acc@5 94.606 loss 1.090
Accuracy of the network on the 482 val images: 64.52%
[2023-08-29 18:37:45,602] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:37:45,603] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:37:45,604] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:37:45,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:37:47,011] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:37:47,012] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.52%
Epoch: [25]  [  0/160]  eta: 0:20:44  lr: 0.000032  min_lr: 0.000001  loss: 2.1631 (2.1631)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1258 (6.1258)  time: 7.7767 (7.7767 -- 7.7767)  data: 7.2458 (7.2458 -- 7.2458)  max mem: 16413
[2023-08-29 18:37:57,888] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4005
[2023-08-29 18:37:57,889] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 18:37:57,889] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4005
[2023-08-29 18:37:57,889] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 18:37:57,889] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [25]  [ 20/160]  eta: 0:02:55  lr: 0.000032  min_lr: 0.000001  loss: 2.0306 (2.1015)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1396 (7.9504)  time: 0.9263 (0.5169 -- 4.4005)  data: 0.3606 (0.0004 -- 3.8630)  max mem: 16413
Epoch: [25]  [ 40/160]  eta: 0:02:06  lr: 0.000032  min_lr: 0.000001  loss: 1.9901 (2.0395)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7739 (7.7249)  time: 0.8468 (0.5163 -- 2.7475)  data: 0.2376 (0.0002 -- 2.2246)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:38  lr: 0.000032  min_lr: 0.000001  loss: 2.1735 (2.0776)  loss_scale: 8192.0000 (8863.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4902 (7.6726)  time: 0.8348 (0.5406 -- 2.6318)  data: 0.0619 (0.0003 -- 1.1860)  max mem: 16413
Epoch: [25]  [ 80/160]  eta: 0:01:17  lr: 0.000032  min_lr: 0.000001  loss: 1.9072 (2.0555)  loss_scale: 8192.0000 (8697.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9918 (7.5535)  time: 0.9086 (0.5263 -- 2.5235)  data: 0.1845 (0.0002 -- 1.9764)  max mem: 16413
Epoch: [25]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 1.9365 (2.0408)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4241 (7.5842)  time: 0.7813 (0.5365 -- 2.7913)  data: 0.2077 (0.0001 -- 2.2430)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 1.9511 (2.0237)  loss_scale: 8192.0000 (8530.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2735 (7.5069)  time: 0.8456 (0.5300 -- 2.3809)  data: 0.2708 (0.0004 -- 1.8485)  max mem: 16413
[2023-08-29 18:39:50,021] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:39:50,022] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 18:39:50,022] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:39:50,022] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [25]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 2.0673 (2.0251)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7153 (7.6236)  time: 0.8409 (0.5345 -- 2.6191)  data: 0.0738 (0.0005 -- 1.1797)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0731 (2.0214)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9611 (7.7376)  time: 0.7458 (0.4991 -- 1.7159)  data: 0.1281 (0.0002 -- 1.1797)  max mem: 16413
Epoch: [25] Total time: 0:02:20 (0.8794 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0731 (2.0244)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9611 (7.7376)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.0236 (1.0236)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4378 (2.4378 -- 2.4378)  data: 2.1872 (2.1872 -- 2.1872)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0046 (1.1032)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (94.9495)  time: 0.4245 (0.2034 -- 2.4378)  data: 0.2039 (0.0008 -- 2.1872)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9872 (1.0691)  acc1: 66.6667 (62.4339)  acc5: 100.0000 (95.7672)  time: 0.2250 (0.1694 -- 0.5171)  data: 0.0191 (0.0001 -- 0.3231)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0714 (1.1248)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (94.6058)  time: 0.2090 (0.1323 -- 0.5171)  data: 0.0188 (0.0001 -- 0.3231)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 64.315 Acc@5 94.398 loss 1.084
Accuracy of the network on the 482 val images: 64.32%
Max accuracy: 64.52%
Epoch: [26]  [  0/160]  eta: 0:21:14  lr: 0.000032  min_lr: 0.000001  loss: 1.3867 (1.3867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9507 (5.9507)  time: 7.9650 (7.9650 -- 7.9650)  data: 5.3877 (5.3877 -- 5.3877)  max mem: 16413
[2023-08-29 18:40:24,167] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4161
[2023-08-29 18:40:24,167] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 18:40:24,167] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4161
[2023-08-29 18:40:24,167] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 18:40:24,167] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [26]  [ 20/160]  eta: 0:02:37  lr: 0.000032  min_lr: 0.000001  loss: 2.1385 (2.1193)  loss_scale: 8192.0000 (8582.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1612 (7.1791)  time: 0.7851 (0.5361 -- 2.5151)  data: 0.0139 (0.0011 -- 0.2195)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:08  lr: 0.000032  min_lr: 0.000001  loss: 1.9947 (2.0714)  loss_scale: 8192.0000 (8391.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3826 (7.0705)  time: 1.0103 (0.5299 -- 2.6698)  data: 0.0272 (0.0002 -- 0.5124)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:40  lr: 0.000032  min_lr: 0.000001  loss: 2.1082 (2.0712)  loss_scale: 8192.0000 (8326.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7246 (6.9930)  time: 0.8605 (0.5316 -- 3.7749)  data: 0.0019 (0.0003 -- 0.0092)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:17  lr: 0.000032  min_lr: 0.000001  loss: 2.0362 (2.0519)  loss_scale: 8192.0000 (8293.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9243 (7.3086)  time: 0.8597 (0.5356 -- 2.7929)  data: 0.0023 (0.0004 -- 0.0167)  max mem: 16413
Epoch: [26]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 2.1212 (2.0475)  loss_scale: 8192.0000 (8273.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7910 (7.3583)  time: 0.7807 (0.5378 -- 2.8711)  data: 0.0022 (0.0003 -- 0.0135)  max mem: 16413
Epoch: [26]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000001  loss: 2.1266 (2.0469)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8114 (7.3258)  time: 1.0075 (0.5415 -- 3.7774)  data: 0.0026 (0.0002 -- 0.0136)  max mem: 16413
[2023-08-29 18:42:16,659] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:42:16,659] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 18:42:16,659] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:42:16,659] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 2.0259 (2.0391)  loss_scale: 16384.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9586 (7.3413)  time: 0.6571 (0.5238 -- 2.0482)  data: 0.0278 (0.0004 -- 0.5156)  max mem: 16413
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0934 (2.0414)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3260 (7.3671)  time: 0.7711 (0.4949 -- 3.9542)  data: 0.0986 (0.0002 -- 1.9135)  max mem: 16413
Epoch: [26] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0934 (2.0217)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3260 (7.3671)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.0601 (1.0601)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4503 (2.4503 -- 2.4503)  data: 2.2213 (2.2213 -- 2.2213)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8735 (1.0812)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4347 (0.1944 -- 2.4503)  data: 0.2216 (0.0004 -- 2.2213)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8735 (1.0460)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (95.2381)  time: 0.2241 (0.1695 -- 0.4384)  data: 0.0241 (0.0001 -- 0.2635)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0293 (1.0885)  acc1: 66.6667 (64.7303)  acc5: 100.0000 (94.6058)  time: 0.2091 (0.1324 -- 0.4384)  data: 0.0239 (0.0001 -- 0.2635)  max mem: 16413
Val: Total time: 0:00:07 (0.2932 s / it)
* Acc@1 66.183 Acc@5 94.606 loss 1.055
Accuracy of the network on the 482 val images: 66.18%
[2023-08-29 18:42:45,660] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:42:45,662] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:42:45,662] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:42:45,662] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:42:46,983] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:42:46,984] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.18%
Epoch: [27]  [  0/160]  eta: 0:22:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0796 (2.0796)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5083 (4.5083)  time: 8.2513 (8.2513 -- 8.2513)  data: 5.1640 (5.1640 -- 5.1640)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:52  lr: 0.000032  min_lr: 0.000001  loss: 1.9007 (1.9521)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3406 (7.6001)  time: 0.8812 (0.5262 -- 4.3761)  data: 0.0523 (0.0004 -- 0.5605)  max mem: 16413
Epoch: [27]  [ 40/160]  eta: 0:02:08  lr: 0.000032  min_lr: 0.000001  loss: 1.9205 (1.9510)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9912 (7.4711)  time: 0.9045 (0.5184 -- 3.4452)  data: 0.1547 (0.0003 -- 1.8813)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:39  lr: 0.000032  min_lr: 0.000001  loss: 2.1323 (1.9630)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0420 (7.6048)  time: 0.8354 (0.5252 -- 3.4769)  data: 0.0329 (0.0003 -- 0.6329)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:17  lr: 0.000032  min_lr: 0.000001  loss: 2.0189 (1.9710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3984 (7.5024)  time: 0.9025 (0.5339 -- 3.1025)  data: 0.0012 (0.0003 -- 0.0024)  max mem: 16413
[2023-08-29 18:44:18,830] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:44:18,830] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:44:18,830] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:44:18,831] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 2.0986 (1.9948)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3078 (7.5095)  time: 0.7105 (0.5255 -- 2.8440)  data: 0.1285 (0.0009 -- 2.1950)  max mem: 16413
[2023-08-29 18:44:32,728] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4434
[2023-08-29 18:44:32,728] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4434
[2023-08-29 18:44:32,729] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:44:32,729] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:44:32,729] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 1.9633 (1.9953)  loss_scale: 32768.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5170 (7.6437)  time: 0.8863 (0.5225 -- 2.5861)  data: 0.1825 (0.0002 -- 2.0579)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 2.0904 (1.9910)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8745 (7.5717)  time: 0.8503 (0.5449 -- 2.3716)  data: 0.0505 (0.0001 -- 0.5042)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0718 (1.9982)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7222 (7.6580)  time: 0.7093 (0.4949 -- 1.5992)  data: 0.0496 (0.0001 -- 0.9707)  max mem: 16413
Epoch: [27] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0718 (2.0082)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7222 (7.6580)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.9852 (0.9852)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2863 (2.2863 -- 2.2863)  data: 2.0404 (2.0404 -- 2.0404)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9410 (1.1182)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4081 (0.2050 -- 2.2863)  data: 0.1870 (0.0008 -- 2.0404)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9171 (1.0717)  acc1: 66.6667 (62.4339)  acc5: 100.0000 (96.2963)  time: 0.2186 (0.1697 -- 0.3085)  data: 0.0096 (0.0001 -- 0.1052)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0708 (1.1216)  acc1: 55.5556 (60.5809)  acc5: 100.0000 (95.0207)  time: 0.2002 (0.1362 -- 0.3085)  data: 0.0091 (0.0001 -- 0.1052)  max mem: 16413
Val: Total time: 0:00:07 (0.2833 s / it)
* Acc@1 65.145 Acc@5 94.813 loss 1.079
Accuracy of the network on the 482 val images: 65.15%
Max accuracy: 66.18%
Epoch: [28]  [  0/160]  eta: 0:16:57  lr: 0.000032  min_lr: 0.000001  loss: 2.0430 (2.0430)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2948 (10.2948)  time: 6.3570 (6.3570 -- 6.3570)  data: 5.0540 (5.0540 -- 5.0540)  max mem: 16413
Epoch: [28]  [ 20/160]  eta: 0:02:43  lr: 0.000032  min_lr: 0.000001  loss: 1.8267 (1.8748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9688 (7.7448)  time: 0.9060 (0.5203 -- 2.8042)  data: 0.2299 (0.0008 -- 2.2770)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:02:07  lr: 0.000032  min_lr: 0.000001  loss: 2.0878 (1.9473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4628 (8.0665)  time: 0.9546 (0.5085 -- 3.7325)  data: 0.0881 (0.0003 -- 0.8864)  max mem: 16413
Epoch: [28]  [ 60/160]  eta: 0:01:38  lr: 0.000032  min_lr: 0.000001  loss: 1.8708 (1.9297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1732 (7.7927)  time: 0.8158 (0.5212 -- 3.4106)  data: 0.1773 (0.0002 -- 2.5043)  max mem: 16413
Epoch: [28]  [ 80/160]  eta: 0:01:16  lr: 0.000032  min_lr: 0.000001  loss: 1.9564 (1.9453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2407 (7.8944)  time: 0.8972 (0.5151 -- 3.3917)  data: 0.3492 (0.0005 -- 2.8622)  max mem: 16413
[2023-08-29 18:46:35,659] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:46:35,659] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:46:35,660] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:46:35,661] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 2.0258 (1.9718)  loss_scale: 32768.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7152 (7.9652)  time: 0.8151 (0.5289 -- 2.3301)  data: 0.2029 (0.0003 -- 1.7607)  max mem: 16413
[2023-08-29 18:47:07,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4600
[2023-08-29 18:47:07,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4600
[2023-08-29 18:47:07,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:47:07,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:47:07,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 1.8891 (1.9669)  loss_scale: 32768.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2148 (8.0184)  time: 0.8425 (0.5248 -- 3.4220)  data: 0.2798 (0.0004 -- 2.8615)  max mem: 16413
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 2.0079 (1.9676)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7551 (8.0828)  time: 0.8088 (0.5285 -- 2.4036)  data: 0.2569 (0.0005 -- 1.8582)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.9529 (1.9648)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3965 (8.1422)  time: 0.6730 (0.4980 -- 2.1507)  data: 0.0998 (0.0002 -- 1.6311)  max mem: 16413
Epoch: [28] Total time: 0:02:20 (0.8756 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.9529 (1.9912)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3965 (8.1422)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.9663 (0.9663)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5307 (2.5307 -- 2.5307)  data: 2.2687 (2.2687 -- 2.2687)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9698 (1.0949)  acc1: 55.5556 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4245 (0.1917 -- 2.5307)  data: 0.2075 (0.0004 -- 2.2687)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9646 (1.0442)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.2963)  time: 0.2107 (0.1693 -- 0.3357)  data: 0.0082 (0.0001 -- 0.1295)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0036 (1.0935)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (95.4357)  time: 0.1951 (0.1362 -- 0.3357)  data: 0.0079 (0.0001 -- 0.1295)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 66.598 Acc@5 95.228 loss 1.046
Accuracy of the network on the 482 val images: 66.60%
[2023-08-29 18:47:44,072] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:47:44,074] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:47:44,074] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:47:44,074] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:47:45,477] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:47:45,477] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.60%
Epoch: [29]  [  0/160]  eta: 0:20:19  lr: 0.000032  min_lr: 0.000001  loss: 2.5449 (2.5449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4696 (7.4696)  time: 7.6216 (7.6216 -- 7.6216)  data: 7.0829 (7.0829 -- 7.0829)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:34  lr: 0.000032  min_lr: 0.000001  loss: 2.1517 (2.1335)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0781 (7.7006)  time: 0.7758 (0.5395 -- 2.3921)  data: 0.1612 (0.0007 -- 1.8173)  max mem: 16413
Epoch: [29]  [ 40/160]  eta: 0:02:09  lr: 0.000032  min_lr: 0.000001  loss: 1.8919 (2.0304)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5087 (7.6110)  time: 1.0540 (0.5260 -- 4.0816)  data: 0.4573 (0.0007 -- 3.5370)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:38  lr: 0.000032  min_lr: 0.000001  loss: 1.9091 (2.0526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3711 (8.0180)  time: 0.8015 (0.5174 -- 3.6828)  data: 0.2525 (0.0004 -- 3.1487)  max mem: 16413
Epoch: [29]  [ 80/160]  eta: 0:01:16  lr: 0.000032  min_lr: 0.000001  loss: 2.0105 (2.0325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1907 (8.0497)  time: 0.8825 (0.5347 -- 3.8507)  data: 0.3347 (0.0005 -- 3.3072)  max mem: 16413
[2023-08-29 18:49:11,710] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:49:11,711] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:49:11,712] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:49:11,712] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:49:20,719] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4740
[2023-08-29 18:49:20,719] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4740
[2023-08-29 18:49:20,719] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:49:20,719] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:49:20,719] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [100/160]  eta: 0:00:56  lr: 0.000032  min_lr: 0.000001  loss: 1.9049 (2.0096)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3473 (7.9174)  time: 0.8664 (0.5355 -- 3.7737)  data: 0.3105 (0.0002 -- 3.2405)  max mem: 16413
Epoch: [29]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000001  loss: 1.9622 (1.9983)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4729 (7.8922)  time: 0.8570 (0.5228 -- 3.0640)  data: 0.3076 (0.0003 -- 2.5318)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.9319 (1.9850)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7530 (7.9425)  time: 0.8139 (0.5352 -- 3.2060)  data: 0.2611 (0.0003 -- 2.6857)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 2.0298 (1.9928)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3511 (7.9002)  time: 0.6904 (0.4954 -- 2.3946)  data: 0.1229 (0.0002 -- 1.8282)  max mem: 16413
Epoch: [29] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 2.0298 (1.9750)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3511 (7.9002)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.0686 (1.0686)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5058 (2.5058 -- 2.5058)  data: 2.2387 (2.2387 -- 2.2387)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9899 (1.0565)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (94.9495)  time: 0.4379 (0.2062 -- 2.5058)  data: 0.2155 (0.0007 -- 2.2387)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9285 (1.0134)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (95.7672)  time: 0.2190 (0.1688 -- 0.3486)  data: 0.0130 (0.0001 -- 0.1241)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9610 (1.0554)  acc1: 66.6667 (62.6556)  acc5: 100.0000 (95.0207)  time: 0.2038 (0.1330 -- 0.3486)  data: 0.0127 (0.0001 -- 0.1241)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 67.427 Acc@5 95.021 loss 1.018
Accuracy of the network on the 482 val images: 67.43%
[2023-08-29 18:50:15,321] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 18:50:15,323] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 18:50:15,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 18:50:15,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 18:50:16,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 18:50:16,713] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.43%
Epoch: [30]  [  0/160]  eta: 0:16:24  lr: 0.000032  min_lr: 0.000001  loss: 1.7599 (1.7599)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7830 (7.7830)  time: 6.1547 (6.1547 -- 6.1547)  data: 5.6244 (5.6244 -- 5.6244)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:03:03  lr: 0.000031  min_lr: 0.000001  loss: 1.8558 (1.9436)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5191 (8.4677)  time: 1.0722 (0.5185 -- 4.6826)  data: 0.3808 (0.0003 -- 4.1560)  max mem: 16413
Epoch: [30]  [ 40/160]  eta: 0:02:08  lr: 0.000031  min_lr: 0.000001  loss: 2.0554 (1.9542)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8379 (7.8427)  time: 0.8155 (0.5319 -- 4.0209)  data: 0.2668 (0.0003 -- 3.4900)  max mem: 16413
Epoch: [30]  [ 60/160]  eta: 0:01:44  lr: 0.000031  min_lr: 0.000001  loss: 1.9324 (1.9344)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5181 (7.8771)  time: 0.9870 (0.5045 -- 4.7644)  data: 0.1032 (0.0003 -- 1.2452)  max mem: 16413
[2023-08-29 18:51:27,677] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:51:27,677] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:51:27,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:51:27,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:51:29,863] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4873
[2023-08-29 18:51:29,863] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4873
[2023-08-29 18:51:29,863] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:51:29,863] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:51:29,863] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [ 80/160]  eta: 0:01:17  lr: 0.000031  min_lr: 0.000001  loss: 1.7061 (1.8993)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8271 (7.8823)  time: 0.7347 (0.5147 -- 3.0368)  data: 0.0010 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:57  lr: 0.000031  min_lr: 0.000001  loss: 2.0089 (1.9194)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2510 (7.8472)  time: 0.8856 (0.5311 -- 3.6418)  data: 0.0020 (0.0005 -- 0.0107)  max mem: 16413
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000031  min_lr: 0.000001  loss: 2.0644 (1.9319)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5600 (8.0622)  time: 0.9049 (0.5423 -- 3.4397)  data: 0.0014 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 2.0199 (1.9495)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0738 (7.9665)  time: 0.8232 (0.5298 -- 3.0803)  data: 0.0024 (0.0004 -- 0.0132)  max mem: 16413
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.9732 (1.9593)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7861 (7.9845)  time: 0.7001 (0.4959 -- 1.9220)  data: 0.0007 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [30] Total time: 0:02:23 (0.8983 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.9732 (1.9439)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7861 (7.9845)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.9802 (0.9802)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4193 (2.4193 -- 2.4193)  data: 2.2067 (2.2067 -- 2.2067)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9119 (1.0519)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (94.9495)  time: 0.4385 (0.2040 -- 2.4193)  data: 0.2219 (0.0005 -- 2.2067)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8797 (1.0136)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (95.2381)  time: 0.2259 (0.1699 -- 0.4650)  data: 0.0195 (0.0001 -- 0.2241)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9877 (1.0629)  acc1: 55.5556 (62.2407)  acc5: 100.0000 (94.6058)  time: 0.2113 (0.1336 -- 0.4650)  data: 0.0192 (0.0001 -- 0.2241)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 67.012 Acc@5 94.813 loss 1.015
Accuracy of the network on the 482 val images: 67.01%
Max accuracy: 67.43%
Epoch: [31]  [  0/160]  eta: 0:20:54  lr: 0.000031  min_lr: 0.000001  loss: 1.2402 (1.2402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4112 (8.4112)  time: 7.8421 (7.8421 -- 7.8421)  data: 7.2958 (7.2958 -- 7.2958)  max mem: 16413
Epoch: [31]  [ 20/160]  eta: 0:02:43  lr: 0.000031  min_lr: 0.000001  loss: 2.0695 (1.9503)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0259 (9.1753)  time: 0.8367 (0.5300 -- 2.6044)  data: 0.1056 (0.0002 -- 2.0801)  max mem: 16413
[2023-08-29 18:53:28,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[7.452617764618246e-07, 7.452617764618246e-07, 9.936823686157662e-07, 9.936823686157662e-07, 1.3249098248210217e-06, 1.3249098248210217e-06, 1.7665464330946955e-06, 1.7665464330946955e-06, 2.3553952441262605e-06, 2.3553952441262605e-06, 3.1405269921683477e-06, 3.1405269921683477e-06, 4.18736932289113e-06, 4.18736932289113e-06, 5.583159097188173e-06, 5.583159097188173e-06, 7.444212129584231e-06, 7.444212129584231e-06, 9.925616172778976e-06, 9.925616172778976e-06, 1.3234154897038634e-05, 1.3234154897038634e-05, 1.7645539862718177e-05, 1.7645539862718177e-05, 2.352738648362424e-05, 2.352738648362424e-05, 3.1369848644832316e-05, 3.1369848644832316e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 18:53:28,812] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=17.592530802777322, CurrSamplesPerSec=21.917869685918024, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:01:59  lr: 0.000031  min_lr: 0.000001  loss: 2.0451 (2.0028)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1968 (8.5413)  time: 0.8171 (0.5216 -- 2.1156)  data: 0.0016 (0.0006 -- 0.0052)  max mem: 16413
[2023-08-29 18:53:32,212] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:53:32,213] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:53:32,212] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:53:32,213] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:53:34,344] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5005
[2023-08-29 18:53:34,344] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5005
[2023-08-29 18:53:34,345] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:53:34,345] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:53:34,345] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [ 60/160]  eta: 0:01:37  lr: 0.000031  min_lr: 0.000001  loss: 1.8827 (1.9809)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7763 (8.0815)  time: 0.9160 (0.5273 -- 2.2579)  data: 0.1437 (0.0003 -- 1.4930)  max mem: 16413
Epoch: [31]  [ 80/160]  eta: 0:01:14  lr: 0.000031  min_lr: 0.000001  loss: 2.0476 (1.9786)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9388 (8.0821)  time: 0.8333 (0.5267 -- 2.0777)  data: 0.1561 (0.0007 -- 1.5526)  max mem: 16413
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000001  loss: 2.1340 (2.0012)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5915 (8.0516)  time: 0.8540 (0.5349 -- 4.0695)  data: 0.2544 (0.0001 -- 3.5435)  max mem: 16413
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 2.0025 (2.0026)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5904 (8.1668)  time: 0.8823 (0.5280 -- 1.9328)  data: 0.2045 (0.0007 -- 1.3677)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:17  lr: 0.000031  min_lr: 0.000001  loss: 1.8748 (1.9865)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7087 (8.1480)  time: 0.8009 (0.5308 -- 2.7058)  data: 0.0687 (0.0004 -- 0.7738)  max mem: 16413
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 2.0354 (1.9824)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8858 (8.0529)  time: 0.7263 (0.4955 -- 2.5002)  data: 0.0008 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [31] Total time: 0:02:20 (0.8795 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 2.0354 (1.9817)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8858 (8.0529)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.0209 (1.0209)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2478 (2.2478 -- 2.2478)  data: 2.0413 (2.0413 -- 2.0413)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.0209 (1.0716)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4108 (0.1931 -- 2.2478)  data: 0.1941 (0.0006 -- 2.0413)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8784 (1.0232)  acc1: 66.6667 (61.9048)  acc5: 100.0000 (96.2963)  time: 0.2325 (0.1693 -- 0.6262)  data: 0.0258 (0.0001 -- 0.4176)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0096 (1.0665)  acc1: 66.6667 (60.1660)  acc5: 100.0000 (95.4357)  time: 0.2141 (0.1336 -- 0.6262)  data: 0.0253 (0.0001 -- 0.4176)  max mem: 16413
Val: Total time: 0:00:07 (0.2921 s / it)
* Acc@1 66.390 Acc@5 95.228 loss 1.016
Accuracy of the network on the 482 val images: 66.39%
Max accuracy: 67.43%
Epoch: [32]  [  0/160]  eta: 0:23:29  lr: 0.000031  min_lr: 0.000001  loss: 2.4811 (2.4811)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7904 (5.7904)  time: 8.8115 (8.8115 -- 8.8115)  data: 7.2123 (7.2123 -- 7.2123)  max mem: 16413
[2023-08-29 18:55:36,530] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:55:36,530] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:55:36,531] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:55:36,531] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:55:41,582] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5136
[2023-08-29 18:55:41,582] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5136
[2023-08-29 18:55:41,624] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:55:41,624] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:55:41,624] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 20/160]  eta: 0:02:57  lr: 0.000031  min_lr: 0.000001  loss: 1.9531 (1.9786)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1771 (7.2785)  time: 0.8935 (0.5132 -- 4.5349)  data: 0.2262 (0.0003 -- 2.3166)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:06  lr: 0.000031  min_lr: 0.000001  loss: 2.0191 (2.0069)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5919 (7.7271)  time: 0.8206 (0.5279 -- 3.4401)  data: 0.1483 (0.0002 -- 1.4513)  max mem: 16413
Epoch: [32]  [ 60/160]  eta: 0:01:41  lr: 0.000031  min_lr: 0.000001  loss: 1.9367 (1.9859)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4427 (7.8427)  time: 0.9522 (0.5262 -- 5.9277)  data: 0.1203 (0.0002 -- 2.1953)  max mem: 16413
Epoch: [32]  [ 80/160]  eta: 0:01:17  lr: 0.000031  min_lr: 0.000001  loss: 1.9822 (1.9759)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4730 (7.9386)  time: 0.8372 (0.5254 -- 3.5127)  data: 0.0015 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000001  loss: 2.1359 (1.9907)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2005 (8.0508)  time: 0.7779 (0.5190 -- 2.8389)  data: 0.0016 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [32]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 2.0957 (2.0111)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7562 (8.1814)  time: 0.8183 (0.5241 -- 1.7193)  data: 0.1909 (0.0004 -- 0.9469)  max mem: 16413
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.8918 (2.0082)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3410 (8.2496)  time: 0.9407 (0.5142 -- 3.3670)  data: 0.3564 (0.0004 -- 2.8542)  max mem: 16413
[2023-08-29 18:57:31,454] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:57:31,454] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 18:57:31,455] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 18:57:31,455] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 2.0725 (2.0033)  loss_scale: 32768.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3718 (8.1529)  time: 0.7236 (0.4932 -- 2.4845)  data: 0.1926 (0.0001 -- 1.9349)  max mem: 16413
Epoch: [32] Total time: 0:02:23 (0.8972 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 2.0725 (1.9726)  loss_scale: 32768.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3718 (8.1529)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.8478 (0.8478)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3893 (2.3893 -- 2.3893)  data: 2.1490 (2.1490 -- 2.1490)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8478 (1.0417)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (94.9495)  time: 0.4172 (0.1964 -- 2.3893)  data: 0.1964 (0.0006 -- 2.1490)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8212 (1.0006)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (95.2381)  time: 0.2220 (0.1685 -- 0.4796)  data: 0.0144 (0.0001 -- 0.2684)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0142 (1.0497)  acc1: 66.6667 (62.2407)  acc5: 100.0000 (94.6058)  time: 0.2062 (0.1327 -- 0.4796)  data: 0.0141 (0.0001 -- 0.2684)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 67.427 Acc@5 95.021 loss 0.995
Accuracy of the network on the 482 val images: 67.43%
Max accuracy: 67.43%
Epoch: [33]  [  0/160]  eta: 0:21:50  lr: 0.000031  min_lr: 0.000001  loss: 2.1571 (2.1571)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1644 (8.1644)  time: 8.1930 (8.1930 -- 8.1930)  data: 5.9243 (5.9243 -- 5.9243)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:46  lr: 0.000031  min_lr: 0.000001  loss: 2.0515 (2.0056)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6239 (8.2618)  time: 0.8381 (0.5256 -- 2.5538)  data: 0.0018 (0.0003 -- 0.0051)  max mem: 16413
[2023-08-29 18:58:20,780] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5307
[2023-08-29 18:58:20,780] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5307
[2023-08-29 18:58:20,780] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:58:20,780] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 18:58:20,780] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [ 40/160]  eta: 0:02:04  lr: 0.000031  min_lr: 0.000001  loss: 1.8682 (1.9571)  loss_scale: 16384.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0460 (8.3361)  time: 0.8799 (0.5256 -- 4.0534)  data: 0.0028 (0.0001 -- 0.0250)  max mem: 16413
Epoch: [33]  [ 60/160]  eta: 0:01:37  lr: 0.000031  min_lr: 0.000001  loss: 2.1378 (1.9834)  loss_scale: 16384.0000 (23635.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6259 (8.5360)  time: 0.8501 (0.5388 -- 3.3204)  data: 0.0021 (0.0005 -- 0.0143)  max mem: 16413
Epoch: [33]  [ 80/160]  eta: 0:01:14  lr: 0.000031  min_lr: 0.000001  loss: 1.9718 (1.9864)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1712 (8.5204)  time: 0.8157 (0.5336 -- 2.3737)  data: 0.0013 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [33]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000001  loss: 1.8643 (1.9789)  loss_scale: 16384.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6035 (8.4558)  time: 0.9334 (0.5274 -- 3.6751)  data: 0.0027 (0.0006 -- 0.0149)  max mem: 16413
Epoch: [33]  [120/160]  eta: 0:00:37  lr: 0.000031  min_lr: 0.000001  loss: 2.0177 (1.9731)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2499 (8.4922)  time: 0.8732 (0.5226 -- 3.3582)  data: 0.0012 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:17  lr: 0.000031  min_lr: 0.000001  loss: 1.8966 (1.9542)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6694 (8.3581)  time: 0.7064 (0.5412 -- 2.2236)  data: 0.0015 (0.0003 -- 0.0036)  max mem: 16413
[2023-08-29 19:00:07,167] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:00:07,168] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:00:07,168] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:00:07,168] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.9717 (1.9457)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6504 (8.4490)  time: 0.7311 (0.4964 -- 4.0422)  data: 0.0009 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [33] Total time: 0:02:20 (0.8767 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.9717 (1.9404)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6504 (8.4490)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.8628 (0.8628)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2229 (2.2229 -- 2.2229)  data: 2.0215 (2.0215 -- 2.0215)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8628 (1.0425)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4127 (0.2045 -- 2.2229)  data: 0.1884 (0.0005 -- 2.0215)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8373 (0.9875)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (96.2963)  time: 0.2284 (0.1700 -- 0.3733)  data: 0.0189 (0.0001 -- 0.1951)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9663 (1.0307)  acc1: 66.6667 (62.6556)  acc5: 100.0000 (95.4357)  time: 0.2076 (0.1333 -- 0.3733)  data: 0.0186 (0.0001 -- 0.1951)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 68.257 Acc@5 95.228 loss 0.982
Accuracy of the network on the 482 val images: 68.26%
[2023-08-29 19:00:16,542] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:00:16,544] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:00:16,544] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:00:16,544] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:00:17,952] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:00:17,952] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.26%
Epoch: [34]  [  0/160]  eta: 0:17:17  lr: 0.000031  min_lr: 0.000001  loss: 1.7809 (1.7809)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5637 (9.5637)  time: 6.4827 (6.4827 -- 6.4827)  data: 5.0904 (5.0904 -- 5.0904)  max mem: 16413
[2023-08-29 19:00:34,196] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5449
[2023-08-29 19:00:34,196] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5449
[2023-08-29 19:00:34,196] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:00:34,196] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:00:34,196] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 20/160]  eta: 0:02:57  lr: 0.000031  min_lr: 0.000001  loss: 1.9881 (1.9907)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3635 (9.0501)  time: 1.0080 (0.5221 -- 3.9551)  data: 0.0455 (0.0004 -- 0.8828)  max mem: 16413
Epoch: [34]  [ 40/160]  eta: 0:02:06  lr: 0.000031  min_lr: 0.000001  loss: 1.9922 (1.9692)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6361 (8.9151)  time: 0.8350 (0.5228 -- 3.1215)  data: 0.0015 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [34]  [ 60/160]  eta: 0:01:38  lr: 0.000031  min_lr: 0.000001  loss: 2.0206 (1.9520)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6856 (8.3810)  time: 0.8296 (0.5255 -- 2.7084)  data: 0.0019 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [34]  [ 80/160]  eta: 0:01:15  lr: 0.000031  min_lr: 0.000001  loss: 1.9830 (1.9719)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2297 (8.3233)  time: 0.8434 (0.5325 -- 2.0633)  data: 0.0705 (0.0007 -- 1.3752)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000001  loss: 2.0111 (1.9683)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3838 (8.3357)  time: 0.9047 (0.5205 -- 3.4078)  data: 0.3570 (0.0003 -- 2.8848)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.8139 (1.9370)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9138 (8.3090)  time: 0.8482 (0.5239 -- 3.1771)  data: 0.2977 (0.0003 -- 2.6291)  max mem: 16413
[2023-08-29 19:02:22,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:02:22,794] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:02:22,794] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:02:22,795] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.9397 (1.9280)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4057 (8.1863)  time: 0.8980 (0.5268 -- 3.0805)  data: 0.3471 (0.0002 -- 2.5619)  max mem: 16413
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 2.0780 (1.9345)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6323 (8.2684)  time: 0.6930 (0.4969 -- 3.0805)  data: 0.1681 (0.0002 -- 2.5619)  max mem: 16413
Epoch: [34] Total time: 0:02:20 (0.8792 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 2.0780 (1.9670)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6323 (8.2684)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.8225 (0.8225)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3405 (2.3405 -- 2.3405)  data: 2.0953 (2.0953 -- 2.0953)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8225 (1.0287)  acc1: 55.5556 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4147 (0.2056 -- 2.3405)  data: 0.1932 (0.0007 -- 2.0953)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8089 (0.9781)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.2963)  time: 0.2231 (0.1693 -- 0.3830)  data: 0.0150 (0.0001 -- 0.1379)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9265 (1.0207)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (95.4357)  time: 0.2066 (0.1327 -- 0.3830)  data: 0.0147 (0.0001 -- 0.1379)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 69.087 Acc@5 95.228 loss 0.970
Accuracy of the network on the 482 val images: 69.09%
[2023-08-29 19:02:46,423] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:02:46,425] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:02:46,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:02:46,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:02:47,619] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:02:47,619] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.09%
[2023-08-29 19:02:55,145] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5600
[2023-08-29 19:02:55,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:02:55,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5600
[2023-08-29 19:02:55,146] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 19:02:55,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [35]  [  0/160]  eta: 0:20:04  lr: 0.000031  min_lr: 0.000001  loss: 1.6976 (1.6976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8050 (8.8050)  time: 7.5264 (7.5264 -- 7.5264)  data: 7.0129 (7.0129 -- 7.0129)  max mem: 16413
Epoch: [35]  [ 20/160]  eta: 0:02:38  lr: 0.000031  min_lr: 0.000001  loss: 1.8603 (1.8217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0178 (7.5373)  time: 0.8131 (0.5176 -- 3.7916)  data: 0.2374 (0.0009 -- 2.8727)  max mem: 16413
Epoch: [35]  [ 40/160]  eta: 0:02:07  lr: 0.000031  min_lr: 0.000001  loss: 2.0437 (1.9458)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2877 (8.0786)  time: 0.9957 (0.5245 -- 3.6788)  data: 0.3541 (0.0002 -- 3.1342)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:37  lr: 0.000031  min_lr: 0.000001  loss: 1.9755 (1.9377)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5840 (7.9758)  time: 0.8011 (0.5292 -- 3.1215)  data: 0.2053 (0.0003 -- 2.6085)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:16  lr: 0.000031  min_lr: 0.000001  loss: 1.7443 (1.9217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5220 (8.2096)  time: 0.8848 (0.5289 -- 2.4000)  data: 0.1975 (0.0007 -- 1.8792)  max mem: 16413
Epoch: [35]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000001  loss: 2.1129 (1.9427)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1006 (8.2504)  time: 0.8602 (0.5384 -- 2.3571)  data: 0.1650 (0.0005 -- 1.8365)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:37  lr: 0.000031  min_lr: 0.000001  loss: 1.9529 (1.9333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2222 (8.1361)  time: 0.8888 (0.5334 -- 3.1113)  data: 0.3362 (0.0002 -- 2.6030)  max mem: 16413
[2023-08-29 19:04:46,783] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:04:46,783] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:04:46,784] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:04:46,784] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.9977 (1.9417)  loss_scale: 32768.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2138 (8.0787)  time: 0.8110 (0.5249 -- 2.6917)  data: 0.1319 (0.0004 -- 2.1583)  max mem: 16413
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.9426 (1.9476)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8788 (8.1964)  time: 0.6900 (0.4978 -- 3.1644)  data: 0.1726 (0.0002 -- 2.6398)  max mem: 16413
Epoch: [35] Total time: 0:02:21 (0.8874 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.9426 (1.9365)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8788 (8.1964)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7545 (0.7545)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3830 (2.3830 -- 2.3830)  data: 2.1777 (2.1777 -- 2.1777)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8807 (1.0308)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4176 (0.1915 -- 2.3830)  data: 0.1992 (0.0007 -- 2.1777)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8807 (0.9759)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (96.2963)  time: 0.2214 (0.1699 -- 0.3344)  data: 0.0124 (0.0001 -- 0.1363)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9299 (1.0264)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (95.4357)  time: 0.2027 (0.1331 -- 0.3344)  data: 0.0119 (0.0001 -- 0.1363)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 68.672 Acc@5 94.813 loss 0.970
Accuracy of the network on the 482 val images: 68.67%
Max accuracy: 69.09%
Epoch: [36]  [  0/160]  eta: 0:19:50  lr: 0.000031  min_lr: 0.000001  loss: 2.0195 (2.0195)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5334 (6.5334)  time: 7.4375 (7.4375 -- 7.4375)  data: 6.8719 (6.8719 -- 6.8719)  max mem: 16413
[2023-08-29 19:05:38,740] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5776
[2023-08-29 19:05:38,740] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:05:38,741] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 19:05:38,740] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5776
[2023-08-29 19:05:38,741] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [36]  [ 20/160]  eta: 0:02:36  lr: 0.000031  min_lr: 0.000001  loss: 1.7008 (1.7543)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3859 (7.4130)  time: 0.8035 (0.5233 -- 2.9338)  data: 0.2054 (0.0003 -- 2.3956)  max mem: 16413
Epoch: [36]  [ 40/160]  eta: 0:02:01  lr: 0.000031  min_lr: 0.000001  loss: 2.0042 (1.8541)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5291 (7.9469)  time: 0.8920 (0.5272 -- 4.1714)  data: 0.3376 (0.0003 -- 3.6082)  max mem: 16413
Epoch: [36]  [ 60/160]  eta: 0:01:35  lr: 0.000031  min_lr: 0.000001  loss: 1.8433 (1.8523)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9519 (7.9129)  time: 0.8546 (0.5261 -- 2.9171)  data: 0.1452 (0.0004 -- 1.3409)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:14  lr: 0.000031  min_lr: 0.000001  loss: 2.0125 (1.8986)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4076 (7.8589)  time: 0.8694 (0.5265 -- 3.2270)  data: 0.0900 (0.0004 -- 1.7723)  max mem: 16413
Epoch: [36]  [100/160]  eta: 0:00:57  lr: 0.000031  min_lr: 0.000001  loss: 2.1089 (1.9347)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8817 (8.0528)  time: 1.0077 (0.5168 -- 3.9079)  data: 0.0015 (0.0004 -- 0.0064)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 2.1202 (1.9715)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4103 (8.1029)  time: 0.7147 (0.5175 -- 3.0088)  data: 0.0014 (0.0001 -- 0.0040)  max mem: 16413
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.7781 (1.9640)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7797 (8.0717)  time: 0.9465 (0.5225 -- 3.7644)  data: 0.0015 (0.0002 -- 0.0039)  max mem: 16413
[2023-08-29 19:07:32,246] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:07:32,246] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:07:32,247] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:07:32,247] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 2.0456 (1.9737)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7761 (8.0053)  time: 0.6613 (0.4957 -- 3.2851)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [36] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 2.0456 (1.9562)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7761 (8.0053)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7744 (0.7744)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3801 (2.3801 -- 2.3801)  data: 2.1720 (2.1720 -- 2.1720)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7744 (1.0278)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4181 (0.1917 -- 2.3801)  data: 0.2062 (0.0005 -- 2.1720)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8775 (0.9754)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2169 (0.1700 -- 0.3480)  data: 0.0118 (0.0001 -- 0.1367)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9606 (1.0226)  acc1: 66.6667 (62.2407)  acc5: 100.0000 (95.0207)  time: 0.2034 (0.1329 -- 0.3480)  data: 0.0110 (0.0001 -- 0.1367)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 68.465 Acc@5 95.228 loss 0.969
Accuracy of the network on the 482 val images: 68.46%
Max accuracy: 69.09%
[2023-08-29 19:07:54,793] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5920
[2023-08-29 19:07:54,793] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5920
[2023-08-29 19:07:54,793] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:07:54,793] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:07:54,793] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [37]  [  0/160]  eta: 0:20:04  lr: 0.000031  min_lr: 0.000001  loss: 2.3459 (2.3459)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9529 (8.9529)  time: 7.5303 (7.5303 -- 7.5303)  data: 6.5991 (6.5991 -- 6.5991)  max mem: 16413
Epoch: [37]  [ 20/160]  eta: 0:02:42  lr: 0.000031  min_lr: 0.000001  loss: 1.9479 (1.9905)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3721 (6.8933)  time: 0.8456 (0.5164 -- 3.3835)  data: 0.1171 (0.0002 -- 1.1643)  max mem: 16413
Epoch: [37]  [ 40/160]  eta: 0:02:05  lr: 0.000031  min_lr: 0.000001  loss: 1.9706 (1.9732)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0776 (7.3755)  time: 0.9276 (0.5231 -- 3.3774)  data: 0.2999 (0.0006 -- 2.8549)  max mem: 16413
Epoch: [37]  [ 60/160]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000001  loss: 1.9857 (1.9887)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3592 (7.5456)  time: 0.8838 (0.5276 -- 3.6565)  data: 0.2808 (0.0004 -- 2.0034)  max mem: 16413
[2023-08-29 19:09:03,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=31, lr=[7.274104590747888e-07, 7.274104590747888e-07, 9.698806120997184e-07, 9.698806120997184e-07, 1.2931741494662913e-06, 1.2931741494662913e-06, 1.7242321992883884e-06, 1.7242321992883884e-06, 2.2989762657178512e-06, 2.2989762657178512e-06, 3.0653016876238016e-06, 3.0653016876238016e-06, 4.087068916831735e-06, 4.087068916831735e-06, 5.449425222442314e-06, 5.449425222442314e-06, 7.265900296589752e-06, 7.265900296589752e-06, 9.687867062119669e-06, 9.687867062119669e-06, 1.2917156082826226e-05, 1.2917156082826226e-05, 1.7222874777101632e-05, 1.7222874777101632e-05, 2.2963833036135512e-05, 2.2963833036135512e-05, 3.061844404818068e-05, 3.061844404818068e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 19:09:03,822] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=17.52444898071958, CurrSamplesPerSec=22.54180061572527, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:17  lr: 0.000031  min_lr: 0.000001  loss: 1.9321 (1.9702)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4726 (7.6188)  time: 0.8669 (0.5284 -- 3.3932)  data: 0.3010 (0.0003 -- 2.8599)  max mem: 16413
Epoch: [37]  [100/160]  eta: 0:00:57  lr: 0.000031  min_lr: 0.000001  loss: 1.9715 (1.9768)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6991 (7.6447)  time: 0.9574 (0.5270 -- 4.4775)  data: 0.4106 (0.0004 -- 3.9557)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:37  lr: 0.000031  min_lr: 0.000001  loss: 1.9917 (1.9660)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9258 (7.8613)  time: 0.8051 (0.5234 -- 3.0074)  data: 0.2550 (0.0002 -- 2.4684)  max mem: 16413
[2023-08-29 19:09:50,382] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:09:50,382] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:09:50,382] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:09:50,382] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.8629 (1.9515)  loss_scale: 32768.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8377 (7.7963)  time: 0.8262 (0.5336 -- 5.4952)  data: 0.2809 (0.0004 -- 4.9819)  max mem: 16413
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.9323 (1.9498)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8573 (7.7934)  time: 0.7207 (0.4972 -- 3.2290)  data: 0.1954 (0.0002 -- 2.6992)  max mem: 16413
Epoch: [37] Total time: 0:02:23 (0.8980 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.9323 (1.9578)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8573 (7.7934)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7627 (0.7627)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3851 (2.3851 -- 2.3851)  data: 2.1583 (2.1583 -- 2.1583)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7627 (1.0024)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (93.9394)  time: 0.4151 (0.1979 -- 2.3851)  data: 0.2009 (0.0005 -- 2.1583)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7711 (0.9553)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (95.2381)  time: 0.2155 (0.1714 -- 0.3188)  data: 0.0099 (0.0001 -- 0.1143)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9416 (1.0051)  acc1: 66.6667 (65.1452)  acc5: 100.0000 (94.6058)  time: 0.1984 (0.1410 -- 0.3188)  data: 0.0096 (0.0001 -- 0.1143)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 68.257 Acc@5 95.021 loss 0.952
Accuracy of the network on the 482 val images: 68.26%
Max accuracy: 69.09%
Epoch: [38]  [  0/160]  eta: 0:16:40  lr: 0.000031  min_lr: 0.000001  loss: 2.0310 (2.0310)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2516 (10.2516)  time: 6.2503 (6.2503 -- 6.2503)  data: 5.4728 (5.4728 -- 5.4728)  max mem: 16413
[2023-08-29 19:10:41,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6100
[2023-08-29 19:10:41,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:10:41,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6100
[2023-08-29 19:10:41,794] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:10:41,794] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [ 20/160]  eta: 0:02:34  lr: 0.000031  min_lr: 0.000001  loss: 2.0031 (1.9809)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7050 (7.6610)  time: 0.8445 (0.5312 -- 2.5883)  data: 0.2003 (0.0007 -- 1.9959)  max mem: 16413
Epoch: [38]  [ 40/160]  eta: 0:02:00  lr: 0.000031  min_lr: 0.000001  loss: 1.8867 (1.9688)  loss_scale: 16384.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1891 (8.0951)  time: 0.9076 (0.5326 -- 3.1041)  data: 0.3546 (0.0007 -- 2.5442)  max mem: 16413
Epoch: [38]  [ 60/160]  eta: 0:01:36  lr: 0.000031  min_lr: 0.000001  loss: 1.8319 (1.9376)  loss_scale: 16384.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8844 (8.1655)  time: 0.8892 (0.5136 -- 4.6061)  data: 0.2853 (0.0003 -- 4.0958)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:15  lr: 0.000030  min_lr: 0.000001  loss: 1.9251 (1.9447)  loss_scale: 16384.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7414 (8.1214)  time: 0.8605 (0.5318 -- 3.3917)  data: 0.1348 (0.0003 -- 1.3226)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000001  loss: 2.0478 (1.9753)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3452 (8.0891)  time: 0.9271 (0.5263 -- 3.5742)  data: 0.1413 (0.0003 -- 1.3768)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 2.0606 (1.9876)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8629 (8.2348)  time: 0.8468 (0.5226 -- 3.3354)  data: 0.2919 (0.0001 -- 2.7951)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.8970 (1.9806)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1187 (8.3340)  time: 0.8717 (0.5300 -- 3.9860)  data: 0.3247 (0.0003 -- 3.4728)  max mem: 16413
[2023-08-29 19:12:34,997] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:12:34,997] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:12:34,997] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:12:34,997] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9728 (1.9784)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1874 (8.2009)  time: 0.6509 (0.4963 -- 2.7618)  data: 0.1291 (0.0002 -- 2.2429)  max mem: 16413
Epoch: [38] Total time: 0:02:21 (0.8857 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9728 (1.9555)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1874 (8.2009)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7521 (0.7521)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4015 (2.4015 -- 2.4015)  data: 2.1777 (2.1777 -- 2.1777)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7521 (0.9878)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (94.9495)  time: 0.4171 (0.1951 -- 2.4015)  data: 0.1991 (0.0005 -- 2.1777)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8153 (0.9454)  acc1: 66.6667 (65.6085)  acc5: 100.0000 (95.7672)  time: 0.2221 (0.1690 -- 0.5339)  data: 0.0179 (0.0001 -- 0.3422)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9269 (0.9929)  acc1: 66.6667 (65.5602)  acc5: 100.0000 (95.4357)  time: 0.2065 (0.1336 -- 0.5339)  data: 0.0175 (0.0001 -- 0.3422)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 71.162 Acc@5 95.228 loss 0.939
Accuracy of the network on the 482 val images: 71.16%
[2023-08-29 19:12:48,209] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:12:48,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:12:48,211] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:12:48,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:12:49,671] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:12:49,671] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.16%
Epoch: [39]  [  0/160]  eta: 0:18:50  lr: 0.000030  min_lr: 0.000001  loss: 1.9821 (1.9821)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8427 (5.8427)  time: 7.0681 (7.0681 -- 7.0681)  data: 3.9574 (3.9574 -- 3.9574)  max mem: 16413
Epoch: [39]  [ 20/160]  eta: 0:02:42  lr: 0.000030  min_lr: 0.000001  loss: 1.9748 (1.9654)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9803 (7.7367)  time: 0.8639 (0.5206 -- 3.5978)  data: 0.1712 (0.0002 -- 3.0704)  max mem: 16413
[2023-08-29 19:13:30,280] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6278
[2023-08-29 19:13:30,280] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6278
[2023-08-29 19:13:30,280] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:13:30,280] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:13:30,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [ 40/160]  eta: 0:02:02  lr: 0.000030  min_lr: 0.000001  loss: 1.9295 (1.9632)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5546 (7.5126)  time: 0.8690 (0.5231 -- 4.8220)  data: 0.3126 (0.0008 -- 4.2912)  max mem: 16413
Epoch: [39]  [ 60/160]  eta: 0:01:34  lr: 0.000030  min_lr: 0.000001  loss: 2.0595 (1.9767)  loss_scale: 16384.0000 (26590.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6352 (7.8040)  time: 0.7911 (0.5270 -- 2.4608)  data: 0.2436 (0.0004 -- 1.9388)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:13  lr: 0.000030  min_lr: 0.000001  loss: 2.1218 (2.0071)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6516 (7.6949)  time: 0.8222 (0.5308 -- 2.5511)  data: 0.1469 (0.0002 -- 2.0173)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:54  lr: 0.000030  min_lr: 0.000001  loss: 1.8894 (1.9972)  loss_scale: 16384.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4048 (7.8728)  time: 0.8922 (0.5290 -- 3.1682)  data: 0.2034 (0.0006 -- 2.2038)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:35  lr: 0.000030  min_lr: 0.000001  loss: 1.9002 (1.9824)  loss_scale: 16384.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2895 (7.8633)  time: 0.8098 (0.5264 -- 3.2321)  data: 0.1873 (0.0002 -- 2.7114)  max mem: 16413
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 2.0329 (1.9833)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4503 (7.8633)  time: 0.9965 (0.5368 -- 4.8524)  data: 0.4389 (0.0003 -- 4.3094)  max mem: 16413
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9930 (1.9684)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2542 (7.8586)  time: 0.6501 (0.4968 -- 1.7402)  data: 0.1259 (0.0002 -- 1.2084)  max mem: 16413
Epoch: [39] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9930 (1.9589)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2542 (7.8586)
[2023-08-29 19:15:10,088] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-08-29 19:15:10,090] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-08-29 19:15:10,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-08-29 19:15:10,095] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-08-29 19:15:11,118] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-08-29 19:15:11,118] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6905 (0.6905)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3444 (2.3444 -- 2.3444)  data: 2.1136 (2.1136 -- 2.1136)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7121 (0.9876)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4232 (0.2049 -- 2.3444)  data: 0.2029 (0.0006 -- 2.1136)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8358 (0.9505)  acc1: 66.6667 (69.8413)  acc5: 100.0000 (95.7672)  time: 0.2247 (0.1700 -- 0.3367)  data: 0.0130 (0.0001 -- 0.1374)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9498 (1.0003)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (95.4357)  time: 0.2083 (0.1326 -- 0.3367)  data: 0.0121 (0.0001 -- 0.1374)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 71.992 Acc@5 95.228 loss 0.940
Accuracy of the network on the 482 val images: 71.99%
[2023-08-29 19:15:18,946] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:15:18,948] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:15:18,948] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:15:18,948] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:15:20,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:15:20,200] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.99%
Epoch: [40]  [  0/160]  eta: 0:18:11  lr: 0.000030  min_lr: 0.000001  loss: 1.5591 (1.5591)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3957 (9.3957)  time: 6.8235 (6.8235 -- 6.8235)  data: 4.9143 (4.9143 -- 4.9143)  max mem: 16413
[2023-08-29 19:15:31,095] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:15:31,095] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:15:31,095] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:15:31,095] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [ 20/160]  eta: 0:02:39  lr: 0.000030  min_lr: 0.000001  loss: 1.9790 (1.9337)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0538 (6.9890)  time: 0.8550 (0.5259 -- 4.7876)  data: 0.0441 (0.0005 -- 0.6784)  max mem: 16413
[2023-08-29 19:15:57,479] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6434
[2023-08-29 19:15:57,479] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:15:57,479] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6434
[2023-08-29 19:15:57,479] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:15:57,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [40]  [ 40/160]  eta: 0:01:58  lr: 0.000030  min_lr: 0.000001  loss: 2.0055 (1.9457)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2971 (7.5189)  time: 0.8318 (0.5294 -- 2.4777)  data: 0.0325 (0.0003 -- 0.6069)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:36  lr: 0.000030  min_lr: 0.000001  loss: 1.8551 (1.9183)  loss_scale: 16384.0000 (23635.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0552 (7.5899)  time: 0.9073 (0.5126 -- 4.1865)  data: 0.0012 (0.0003 -- 0.0018)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:15  lr: 0.000030  min_lr: 0.000001  loss: 1.9251 (1.9108)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9530 (7.7347)  time: 0.8951 (0.5283 -- 2.8560)  data: 0.0020 (0.0007 -- 0.0058)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000001  loss: 2.0386 (1.9177)  loss_scale: 16384.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7038 (7.8447)  time: 0.8357 (0.5302 -- 2.6977)  data: 0.1057 (0.0004 -- 1.7974)  max mem: 16413
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 2.0317 (1.9328)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7836 (8.0331)  time: 0.8713 (0.5294 -- 4.2500)  data: 0.0305 (0.0002 -- 0.5862)  max mem: 16413
Epoch: [40]  [140/160]  eta: 0:00:17  lr: 0.000030  min_lr: 0.000001  loss: 2.0404 (1.9512)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3499 (7.9993)  time: 0.7677 (0.5343 -- 2.4666)  data: 0.1380 (0.0004 -- 1.6471)  max mem: 16413
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9630 (1.9349)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4022 (8.0858)  time: 0.7633 (0.4961 -- 3.5197)  data: 0.0210 (0.0002 -- 0.4064)  max mem: 16413
Epoch: [40] Total time: 0:02:20 (0.8803 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9630 (1.9366)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4022 (8.0858)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6676 (0.6676)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3682 (2.3682 -- 2.3682)  data: 2.1008 (2.1008 -- 2.1008)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7256 (0.9816)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4196 (0.1950 -- 2.3682)  data: 0.1986 (0.0006 -- 2.1008)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7347 (0.9341)  acc1: 66.6667 (69.8413)  acc5: 100.0000 (96.2963)  time: 0.2129 (0.1718 -- 0.3174)  data: 0.0044 (0.0001 -- 0.0743)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9139 (0.9834)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (95.8506)  time: 0.1987 (0.1334 -- 0.3174)  data: 0.0041 (0.0001 -- 0.0743)  max mem: 16413
Val: Total time: 0:00:07 (0.2821 s / it)
* Acc@1 72.614 Acc@5 95.436 loss 0.928
Accuracy of the network on the 482 val images: 72.61%
[2023-08-29 19:17:48,895] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:17:48,896] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:17:48,897] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:17:48,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:17:50,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:17:50,087] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 72.61%
Epoch: [41]  [  0/160]  eta: 0:22:18  lr: 0.000030  min_lr: 0.000001  loss: 2.3157 (2.3157)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5713 (7.5713)  time: 8.3683 (8.3683 -- 8.3683)  data: 7.8391 (7.8391 -- 7.8391)  max mem: 16413
[2023-08-29 19:18:00,115] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:18:00,115] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:18:00,118] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:18:00,118] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [41]  [ 20/160]  eta: 0:02:40  lr: 0.000030  min_lr: 0.000001  loss: 1.9299 (1.9733)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2035 (7.9629)  time: 0.7823 (0.5313 -- 2.9299)  data: 0.0998 (0.0006 -- 1.3215)  max mem: 16413
[2023-08-29 19:18:15,188] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6582
[2023-08-29 19:18:15,188] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6582
[2023-08-29 19:18:15,189] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:18:15,189] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:18:15,189] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [ 40/160]  eta: 0:02:05  lr: 0.000030  min_lr: 0.000001  loss: 2.0093 (1.9795)  loss_scale: 16384.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1741 (8.0303)  time: 0.9401 (0.5361 -- 3.2256)  data: 0.3896 (0.0004 -- 2.6993)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:35  lr: 0.000030  min_lr: 0.000001  loss: 1.8939 (1.9673)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4728 (7.5943)  time: 0.7588 (0.5293 -- 3.3023)  data: 0.1051 (0.0003 -- 1.2725)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:15  lr: 0.000030  min_lr: 0.000001  loss: 2.0145 (1.9612)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4082 (7.5838)  time: 0.9155 (0.5425 -- 3.8345)  data: 0.1244 (0.0004 -- 1.4979)  max mem: 16413
Epoch: [41]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000001  loss: 1.9815 (1.9430)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1694 (7.5210)  time: 0.8617 (0.5250 -- 2.6027)  data: 0.0618 (0.0004 -- 1.1859)  max mem: 16413
Epoch: [41]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 1.8605 (1.9301)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2032 (7.5652)  time: 0.8762 (0.5307 -- 4.0352)  data: 0.0015 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.8824 (1.9237)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7672 (7.7775)  time: 0.8427 (0.5393 -- 3.8972)  data: 0.0091 (0.0002 -- 0.1534)  max mem: 16413
[2023-08-29 19:20:06,997] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:20:06,997] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:20:06,997] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:20:06,997] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9858 (1.9345)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7786 (7.8252)  time: 0.7068 (0.4973 -- 2.6288)  data: 0.0286 (0.0002 -- 0.5535)  max mem: 16413
Epoch: [41] Total time: 0:02:21 (0.8844 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9858 (1.9348)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7786 (7.8252)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.7238 (0.7238)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4994 (2.4994 -- 2.4994)  data: 2.2473 (2.2473 -- 2.2473)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7238 (0.9628)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4259 (0.1916 -- 2.4994)  data: 0.2111 (0.0008 -- 2.2473)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7358 (0.9252)  acc1: 77.7778 (71.4286)  acc5: 100.0000 (95.7672)  time: 0.2147 (0.1706 -- 0.2934)  data: 0.0095 (0.0001 -- 0.1107)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8717 (0.9661)  acc1: 66.6667 (69.2946)  acc5: 100.0000 (95.4357)  time: 0.1998 (0.1327 -- 0.2934)  data: 0.0092 (0.0001 -- 0.1107)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 73.651 Acc@5 95.228 loss 0.912
Accuracy of the network on the 482 val images: 73.65%
[2023-08-29 19:20:19,380] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:20:19,382] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:20:19,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:20:19,382] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:20:20,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:20:20,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.65%
Epoch: [42]  [  0/160]  eta: 0:19:58  lr: 0.000030  min_lr: 0.000001  loss: 2.1859 (2.1859)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3055 (8.3055)  time: 7.4875 (7.4875 -- 7.4875)  data: 6.0897 (6.0897 -- 6.0897)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:37  lr: 0.000030  min_lr: 0.000001  loss: 1.9396 (1.9690)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2191 (7.6795)  time: 0.8034 (0.5236 -- 3.3393)  data: 0.1231 (0.0003 -- 1.3903)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:01  lr: 0.000030  min_lr: 0.000001  loss: 1.9108 (1.9206)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4134 (7.9498)  time: 0.9021 (0.5233 -- 4.4079)  data: 0.0768 (0.0003 -- 1.2971)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:36  lr: 0.000030  min_lr: 0.000001  loss: 1.9172 (1.9282)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7701 (7.9397)  time: 0.8554 (0.5333 -- 2.3971)  data: 0.0574 (0.0004 -- 0.7463)  max mem: 16413
[2023-08-29 19:21:21,198] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6783
[2023-08-29 19:21:21,198] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6783
[2023-08-29 19:21:21,198] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:21:21,198] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:21:21,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [42]  [ 80/160]  eta: 0:01:14  lr: 0.000030  min_lr: 0.000001  loss: 2.0183 (1.9474)  loss_scale: 16384.0000 (29127.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4669 (7.8982)  time: 0.8580 (0.5331 -- 2.3982)  data: 0.0990 (0.0003 -- 1.8647)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000001  loss: 1.8415 (1.9381)  loss_scale: 16384.0000 (26603.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5056 (7.8240)  time: 0.8872 (0.5358 -- 2.5401)  data: 0.2201 (0.0003 -- 2.0178)  max mem: 16413
Epoch: [42]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 1.7561 (1.9202)  loss_scale: 16384.0000 (24914.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7226 (7.6326)  time: 0.7970 (0.5324 -- 2.1420)  data: 0.0822 (0.0004 -- 0.8190)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.7633 (1.9070)  loss_scale: 16384.0000 (23704.5106)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9320 (7.7456)  time: 0.9281 (0.5439 -- 2.9920)  data: 0.1168 (0.0003 -- 1.5957)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9038 (1.9007)  loss_scale: 16384.0000 (22835.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9806 (7.7310)  time: 0.7579 (0.4975 -- 3.3064)  data: 0.2425 (0.0001 -- 2.8113)  max mem: 16413
Epoch: [42] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9038 (1.9128)  loss_scale: 16384.0000 (22835.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9806 (7.7310)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.6962 (0.6962)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.6471 (2.6471 -- 2.6471)  data: 2.4119 (2.4119 -- 2.4119)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6962 (0.9716)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (96.9697)  time: 0.4314 (0.1902 -- 2.6471)  data: 0.2201 (0.0003 -- 2.4119)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7887 (0.9052)  acc1: 77.7778 (70.8995)  acc5: 100.0000 (96.8254)  time: 0.2069 (0.1693 -- 0.2640)  data: 0.0044 (0.0001 -- 0.0693)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8218 (0.9481)  acc1: 66.6667 (68.8797)  acc5: 100.0000 (96.2656)  time: 0.1931 (0.1329 -- 0.2640)  data: 0.0042 (0.0001 -- 0.0693)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 73.651 Acc@5 96.473 loss 0.897
Accuracy of the network on the 482 val images: 73.65%
Max accuracy: 73.65%
Epoch: [43]  [  0/160]  eta: 0:17:16  lr: 0.000030  min_lr: 0.000001  loss: 2.1554 (2.1554)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0982 (7.0982)  time: 6.4782 (6.4782 -- 6.4782)  data: 5.1702 (5.1702 -- 5.1702)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:46  lr: 0.000030  min_lr: 0.000001  loss: 1.9763 (1.9605)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1465 (7.6932)  time: 0.9284 (0.5243 -- 3.1658)  data: 0.1158 (0.0003 -- 1.0439)  max mem: 16413
[2023-08-29 19:23:25,754] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:23:25,754] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:23:25,755] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:23:25,755] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [43]  [ 40/160]  eta: 0:02:01  lr: 0.000030  min_lr: 0.000001  loss: 1.9008 (1.8917)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4188 (7.9404)  time: 0.8291 (0.5253 -- 3.3944)  data: 0.0520 (0.0003 -- 0.9962)  max mem: 16413
[2023-08-29 19:23:46,171] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6933
[2023-08-29 19:23:46,171] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6933
[2023-08-29 19:23:46,172] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:23:46,172] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:23:46,172] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [43]  [ 60/160]  eta: 0:01:39  lr: 0.000030  min_lr: 0.000001  loss: 1.8760 (1.9046)  loss_scale: 32768.0000 (22024.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0584 (8.0265)  time: 0.9506 (0.5087 -- 4.6883)  data: 0.0583 (0.0004 -- 1.1372)  max mem: 16413
Epoch: [43]  [ 80/160]  eta: 0:01:15  lr: 0.000030  min_lr: 0.000001  loss: 1.9095 (1.8895)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2979 (8.0293)  time: 0.7794 (0.5471 -- 2.7550)  data: 0.0016 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [43]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000001  loss: 1.8730 (1.9012)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5891 (7.9747)  time: 0.8341 (0.5239 -- 2.6164)  data: 0.0435 (0.0002 -- 0.8330)  max mem: 16413
[2023-08-29 19:24:39,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=37, lr=[7.061443489529621e-07, 7.061443489529621e-07, 9.415257986039495e-07, 9.415257986039495e-07, 1.2553677314719327e-06, 1.2553677314719327e-06, 1.6738236419625768e-06, 1.6738236419625768e-06, 2.2317648559501025e-06, 2.2317648559501025e-06, 2.975686474600137e-06, 2.975686474600137e-06, 3.967581966133515e-06, 3.967581966133515e-06, 5.29010928817802e-06, 5.29010928817802e-06, 7.053479050904028e-06, 7.053479050904028e-06, 9.404638734538703e-06, 9.404638734538703e-06, 1.253951831271827e-05, 1.253951831271827e-05, 1.6719357750291028e-05, 1.6719357750291028e-05, 2.2292477000388037e-05, 2.2292477000388037e-05, 2.972330266718405e-05, 2.972330266718405e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 19:24:39,069] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=17.513010252113272, CurrSamplesPerSec=22.711966373130995, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:35  lr: 0.000030  min_lr: 0.000001  loss: 1.7953 (1.8777)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8533 (8.0335)  time: 0.7713 (0.5226 -- 2.8670)  data: 0.0314 (0.0003 -- 0.5625)  max mem: 16413
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.8412 (1.8874)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5104 (8.1010)  time: 0.9530 (0.5236 -- 3.2555)  data: 0.2330 (0.0004 -- 1.8271)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9528 (1.8892)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6006 (8.0361)  time: 0.6715 (0.4940 -- 2.6631)  data: 0.1204 (0.0002 -- 1.5852)  max mem: 16413
Epoch: [43] Total time: 0:02:20 (0.8768 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9528 (1.9134)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6006 (8.0361)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.7059 (0.7059)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4386 (2.4386 -- 2.4386)  data: 2.2228 (2.2228 -- 2.2228)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7059 (0.9727)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (95.9596)  time: 0.4171 (0.2003 -- 2.4386)  data: 0.2032 (0.0007 -- 2.2228)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7868 (0.9102)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (96.2963)  time: 0.2183 (0.1722 -- 0.4448)  data: 0.0138 (0.0001 -- 0.2589)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8570 (0.9542)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (95.8506)  time: 0.2024 (0.1326 -- 0.4448)  data: 0.0134 (0.0001 -- 0.2589)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 71.992 Acc@5 96.680 loss 0.899
Accuracy of the network on the 482 val images: 71.99%
Max accuracy: 73.65%
Epoch: [44]  [  0/160]  eta: 0:21:11  lr: 0.000030  min_lr: 0.000001  loss: 1.7713 (1.7713)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5723 (8.5723)  time: 7.9484 (7.9484 -- 7.9484)  data: 7.3936 (7.3936 -- 7.3936)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:02:48  lr: 0.000030  min_lr: 0.000001  loss: 1.8982 (1.8509)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8091 (7.7045)  time: 0.8673 (0.5267 -- 3.8184)  data: 0.2628 (0.0005 -- 3.2927)  max mem: 16413
[2023-08-29 19:25:45,735] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:25:45,735] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:25:45,737] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:25:45,737] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:25:53,124] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7069
[2023-08-29 19:25:53,125] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:25:53,124] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7069
[2023-08-29 19:25:53,125] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:25:53,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [44]  [ 40/160]  eta: 0:02:04  lr: 0.000030  min_lr: 0.000001  loss: 1.7974 (1.8310)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9091 (7.5134)  time: 0.8573 (0.5224 -- 3.3525)  data: 0.3143 (0.0002 -- 2.8296)  max mem: 16413
Epoch: [44]  [ 60/160]  eta: 0:01:38  lr: 0.000030  min_lr: 0.000001  loss: 1.9927 (1.8791)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8720 (7.6009)  time: 0.8816 (0.5419 -- 3.6293)  data: 0.2647 (0.0002 -- 3.0984)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:15  lr: 0.000030  min_lr: 0.000001  loss: 1.9894 (1.8944)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2272 (7.7554)  time: 0.8208 (0.5220 -- 3.3454)  data: 0.1742 (0.0004 -- 2.8056)  max mem: 16413
Epoch: [44]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000001  loss: 1.8101 (1.8754)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4767 (7.6538)  time: 0.9627 (0.5341 -- 3.2960)  data: 0.0616 (0.0001 -- 0.9023)  max mem: 16413
Epoch: [44]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 1.8840 (1.8871)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3448 (7.8171)  time: 0.8058 (0.5368 -- 3.8706)  data: 0.0016 (0.0003 -- 0.0111)  max mem: 16413
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.9229 (1.8978)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3496 (7.9573)  time: 1.0250 (0.5024 -- 5.6086)  data: 0.0016 (0.0002 -- 0.0092)  max mem: 16413
[2023-08-29 19:27:41,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:27:41,887] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:27:41,888] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:27:41,929] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 2.0490 (1.9047)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8603 (7.9920)  time: 0.5621 (0.4958 -- 1.0174)  data: 0.0013 (0.0001 -- 0.0141)  max mem: 16413
Epoch: [44] Total time: 0:02:23 (0.8943 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 2.0490 (1.9233)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8603 (7.9920)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6585 (0.6585)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4024 (2.4024 -- 2.4024)  data: 2.1902 (2.1902 -- 2.1902)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6585 (0.9595)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (95.9596)  time: 0.4409 (0.1925 -- 2.4024)  data: 0.2178 (0.0007 -- 2.1902)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7488 (0.8991)  acc1: 66.6667 (71.4286)  acc5: 100.0000 (96.2963)  time: 0.2238 (0.1696 -- 0.4234)  data: 0.0153 (0.0001 -- 0.1945)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8516 (0.9504)  acc1: 66.6667 (68.4647)  acc5: 100.0000 (95.8506)  time: 0.2072 (0.1328 -- 0.4234)  data: 0.0149 (0.0001 -- 0.1945)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 74.274 Acc@5 96.058 loss 0.892
Accuracy of the network on the 482 val images: 74.27%
[2023-08-29 19:27:50,305] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:27:50,307] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:27:50,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:27:50,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:27:51,512] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:27:51,513] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.27%
Epoch: [45]  [  0/160]  eta: 0:20:02  lr: 0.000030  min_lr: 0.000001  loss: 2.0441 (2.0441)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2698 (9.2698)  time: 7.5144 (7.5144 -- 7.5144)  data: 5.1273 (5.1273 -- 5.1273)  max mem: 16413
[2023-08-29 19:28:13,751] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7218
[2023-08-29 19:28:13,751] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7218
[2023-08-29 19:28:13,751] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:28:13,751] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:28:13,751] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [45]  [ 20/160]  eta: 0:02:35  lr: 0.000030  min_lr: 0.000001  loss: 1.9066 (1.9292)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4967 (8.1539)  time: 0.7914 (0.5074 -- 2.6612)  data: 0.0012 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [45]  [ 40/160]  eta: 0:01:55  lr: 0.000029  min_lr: 0.000001  loss: 1.9777 (1.9556)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9692 (8.7857)  time: 0.7987 (0.5214 -- 2.7812)  data: 0.0483 (0.0005 -- 0.5252)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:35  lr: 0.000029  min_lr: 0.000001  loss: 1.9225 (1.9343)  loss_scale: 16384.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7043 (8.5805)  time: 0.9410 (0.5353 -- 3.1781)  data: 0.0628 (0.0009 -- 0.6519)  max mem: 16413
Epoch: [45]  [ 80/160]  eta: 0:01:13  lr: 0.000029  min_lr: 0.000001  loss: 1.9119 (1.9166)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1211 (8.3896)  time: 0.8100 (0.5142 -- 3.1016)  data: 0.0247 (0.0004 -- 0.4624)  max mem: 16413
Epoch: [45]  [100/160]  eta: 0:00:57  lr: 0.000029  min_lr: 0.000001  loss: 2.0535 (1.9397)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8802 (8.4440)  time: 1.0958 (0.5200 -- 4.4857)  data: 0.0019 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 1.8668 (1.9361)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0310 (8.3982)  time: 0.8550 (0.5100 -- 4.0497)  data: 0.0009 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.9208 (1.9256)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0379 (8.2647)  time: 0.7261 (0.5207 -- 2.2996)  data: 0.0015 (0.0003 -- 0.0034)  max mem: 16413
[2023-08-29 19:30:05,345] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:30:05,345] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:30:05,345] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:30:05,345] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:30:09,582] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7354
[2023-08-29 19:30:09,582] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7354
[2023-08-29 19:30:09,582] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:30:09,582] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:30:09,582] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.9262 (1.9249)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1830 (8.3188)  time: 0.6963 (0.4854 -- 2.4471)  data: 0.0327 (0.0002 -- 0.4668)  max mem: 16413
Epoch: [45] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.9262 (1.8931)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1830 (8.3188)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6672 (0.6672)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4018 (2.4018 -- 2.4018)  data: 2.1917 (2.1917 -- 2.1917)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6672 (0.9315)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (96.9697)  time: 0.4193 (0.2030 -- 2.4018)  data: 0.2003 (0.0007 -- 2.1917)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7461 (0.8897)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (96.2963)  time: 0.2184 (0.1694 -- 0.3841)  data: 0.0098 (0.0001 -- 0.1547)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8289 (0.9314)  acc1: 66.6667 (70.9544)  acc5: 100.0000 (95.8506)  time: 0.2011 (0.1330 -- 0.3841)  data: 0.0095 (0.0001 -- 0.1547)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 74.689 Acc@5 96.266 loss 0.877
Accuracy of the network on the 482 val images: 74.69%
[2023-08-29 19:30:20,642] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:30:20,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:30:20,643] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:30:20,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:30:22,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:30:22,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.69%
Epoch: [46]  [  0/160]  eta: 0:17:15  lr: 0.000029  min_lr: 0.000001  loss: 2.3776 (2.3776)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8224 (6.8224)  time: 6.4735 (6.4735 -- 6.4735)  data: 5.9410 (5.9410 -- 5.9410)  max mem: 16413
Epoch: [46]  [ 20/160]  eta: 0:02:48  lr: 0.000029  min_lr: 0.000001  loss: 1.8854 (1.9180)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5548 (7.4973)  time: 0.9423 (0.5211 -- 3.6359)  data: 0.3642 (0.0004 -- 3.0594)  max mem: 16413
Epoch: [46]  [ 40/160]  eta: 0:02:01  lr: 0.000029  min_lr: 0.000001  loss: 1.7915 (1.9131)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8341 (7.8622)  time: 0.8134 (0.5259 -- 2.6205)  data: 0.2621 (0.0003 -- 2.1009)  max mem: 16413
Epoch: [46]  [ 60/160]  eta: 0:01:36  lr: 0.000029  min_lr: 0.000001  loss: 2.0380 (1.9400)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3224 (8.0862)  time: 0.8647 (0.5413 -- 2.3379)  data: 0.2652 (0.0002 -- 1.8019)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:13  lr: 0.000029  min_lr: 0.000001  loss: 1.8224 (1.9082)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8374 (8.0971)  time: 0.7785 (0.5266 -- 2.5722)  data: 0.1652 (0.0005 -- 1.9768)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000001  loss: 1.7744 (1.8835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5646 (8.0256)  time: 0.9241 (0.5209 -- 3.6925)  data: 0.1452 (0.0004 -- 1.4157)  max mem: 16413
Epoch: [46]  [120/160]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000001  loss: 2.1197 (1.9108)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6463 (7.9933)  time: 0.8522 (0.5245 -- 2.3359)  data: 0.0757 (0.0005 -- 1.2030)  max mem: 16413
[2023-08-29 19:32:14,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:32:14,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:32:14,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:32:14,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:32:29,435] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7499
[2023-08-29 19:32:29,435] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7499
[2023-08-29 19:32:29,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:32:29,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:32:29,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.8874 (1.9144)  loss_scale: 32768.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3055 (7.9524)  time: 0.8976 (0.5205 -- 3.9039)  data: 0.0382 (0.0004 -- 0.7331)  max mem: 16413
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.9909 (1.9179)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4672 (8.0611)  time: 0.7238 (0.4973 -- 2.8420)  data: 0.0008 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [46] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.9909 (1.9228)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4672 (8.0611)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.7155 (0.7155)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3414 (2.3414 -- 2.3414)  data: 2.1011 (2.1011 -- 2.1011)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7155 (0.9286)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (96.9697)  time: 0.4076 (0.1964 -- 2.3414)  data: 0.1967 (0.0005 -- 2.1011)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7423 (0.8842)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.8254)  time: 0.2210 (0.1686 -- 0.4848)  data: 0.0180 (0.0001 -- 0.2936)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8398 (0.9277)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (96.2656)  time: 0.2081 (0.1329 -- 0.4848)  data: 0.0177 (0.0001 -- 0.2936)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 75.104 Acc@5 96.680 loss 0.876
Accuracy of the network on the 482 val images: 75.10%
[2023-08-29 19:32:51,715] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:32:51,717] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:32:51,717] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:32:51,717] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:32:53,081] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:32:53,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.10%
Epoch: [47]  [  0/160]  eta: 0:19:08  lr: 0.000029  min_lr: 0.000001  loss: 1.7926 (1.7926)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8764 (8.8764)  time: 7.1756 (7.1756 -- 7.1756)  data: 5.6464 (5.6464 -- 5.6464)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:43  lr: 0.000029  min_lr: 0.000001  loss: 2.0060 (1.9350)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4004 (8.6930)  time: 0.8651 (0.5260 -- 4.2896)  data: 0.3184 (0.0004 -- 3.7620)  max mem: 16413
Epoch: [47]  [ 40/160]  eta: 0:02:02  lr: 0.000029  min_lr: 0.000001  loss: 1.8695 (1.8747)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6432 (8.6229)  time: 0.8666 (0.5249 -- 3.2392)  data: 0.2741 (0.0003 -- 2.7072)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:40  lr: 0.000029  min_lr: 0.000001  loss: 1.8299 (1.8710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6650 (8.2694)  time: 0.9847 (0.5188 -- 4.2519)  data: 0.0052 (0.0002 -- 0.0672)  max mem: 16413
Epoch: [47]  [ 80/160]  eta: 0:01:16  lr: 0.000029  min_lr: 0.000001  loss: 1.8362 (1.8737)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9982 (8.0619)  time: 0.8191 (0.5301 -- 3.2788)  data: 0.1400 (0.0002 -- 2.7698)  max mem: 16413
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000029  min_lr: 0.000001  loss: 1.8303 (1.8700)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2999 (7.9617)  time: 0.8735 (0.5290 -- 4.3688)  data: 0.3238 (0.0004 -- 3.8416)  max mem: 16413
[2023-08-29 19:34:36,526] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:34:36,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:34:36,527] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:34:36,527] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [47]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 1.9475 (1.8864)  loss_scale: 32768.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6279 (7.9718)  time: 0.9046 (0.5232 -- 4.2135)  data: 0.3566 (0.0002 -- 3.6909)  max mem: 16413
[2023-08-29 19:35:00,723] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7659
[2023-08-29 19:35:00,723] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7659
[2023-08-29 19:35:00,723] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:35:00,723] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:35:00,723] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 2.0015 (1.8931)  loss_scale: 32768.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4631 (7.9124)  time: 0.7316 (0.5159 -- 2.7482)  data: 0.1760 (0.0005 -- 2.2080)  max mem: 16413
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.9121 (1.8802)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4263 (8.0533)  time: 0.7127 (0.4950 -- 3.3922)  data: 0.1879 (0.0002 -- 2.8735)  max mem: 16413
Epoch: [47] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.9121 (1.8914)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4263 (8.0533)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6770 (0.6770)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3270 (2.3270 -- 2.3270)  data: 2.0698 (2.0698 -- 2.0698)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6828 (0.9514)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (95.9596)  time: 0.4272 (0.2047 -- 2.3270)  data: 0.2048 (0.0007 -- 2.0698)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6828 (0.8896)  acc1: 77.7778 (71.9577)  acc5: 100.0000 (96.2963)  time: 0.2241 (0.1706 -- 0.3871)  data: 0.0152 (0.0001 -- 0.1723)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8197 (0.9346)  acc1: 66.6667 (69.7095)  acc5: 100.0000 (95.8506)  time: 0.2077 (0.1327 -- 0.3871)  data: 0.0149 (0.0001 -- 0.1723)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 74.481 Acc@5 96.058 loss 0.873
Accuracy of the network on the 482 val images: 74.48%
Max accuracy: 75.10%
Epoch: [48]  [  0/160]  eta: 0:18:24  lr: 0.000029  min_lr: 0.000001  loss: 1.6852 (1.6852)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6021 (6.6021)  time: 6.9062 (6.9062 -- 6.9062)  data: 6.3367 (6.3367 -- 6.3367)  max mem: 16413
Epoch: [48]  [ 20/160]  eta: 0:02:43  lr: 0.000029  min_lr: 0.000001  loss: 2.0052 (1.9887)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6926 (8.1324)  time: 0.8802 (0.5351 -- 2.7277)  data: 0.2229 (0.0001 -- 2.1714)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:00  lr: 0.000029  min_lr: 0.000001  loss: 1.7230 (1.8883)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8861 (8.3098)  time: 0.8394 (0.5276 -- 2.7435)  data: 0.1120 (0.0007 -- 2.2070)  max mem: 16413
Epoch: [48]  [ 60/160]  eta: 0:01:40  lr: 0.000029  min_lr: 0.000001  loss: 1.9253 (1.9071)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8195 (8.3571)  time: 1.0082 (0.5196 -- 4.8577)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:16  lr: 0.000029  min_lr: 0.000001  loss: 1.8760 (1.9126)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6278 (8.2976)  time: 0.8129 (0.5380 -- 3.5232)  data: 0.0023 (0.0004 -- 0.0131)  max mem: 16413
Epoch: [48]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000001  loss: 1.9002 (1.8971)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8705 (8.4239)  time: 0.8128 (0.5414 -- 2.5178)  data: 0.0016 (0.0003 -- 0.0031)  max mem: 16413
[2023-08-29 19:37:03,562] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:37:03,562] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:37:03,563] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:37:03,563] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [48]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 1.9066 (1.9022)  loss_scale: 32768.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2784 (8.3992)  time: 0.9522 (0.5292 -- 3.9066)  data: 0.2058 (0.0001 -- 3.3769)  max mem: 16413
Epoch: [48]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.9641 (1.9105)  loss_scale: 32768.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0233 (8.3230)  time: 0.6998 (0.5304 -- 2.1975)  data: 0.1506 (0.0006 -- 1.6689)  max mem: 16413
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.9974 (1.8999)  loss_scale: 32768.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7421 (8.2882)  time: 0.7447 (0.4977 -- 1.9829)  data: 0.1016 (0.0002 -- 1.4472)  max mem: 16413
Epoch: [48] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.9974 (1.8915)  loss_scale: 32768.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7421 (8.2882)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6459 (0.6459)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3223 (2.3223 -- 2.3223)  data: 2.1008 (2.1008 -- 2.1008)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6570 (0.9219)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (94.9495)  time: 0.4108 (0.2035 -- 2.3223)  data: 0.1927 (0.0004 -- 2.1008)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7612 (0.8741)  acc1: 66.6667 (72.4868)  acc5: 100.0000 (95.7672)  time: 0.2272 (0.1703 -- 0.5602)  data: 0.0193 (0.0001 -- 0.3645)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8499 (0.9248)  acc1: 66.6667 (69.7095)  acc5: 100.0000 (95.4357)  time: 0.2116 (0.1332 -- 0.5602)  data: 0.0190 (0.0001 -- 0.3645)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 74.066 Acc@5 95.643 loss 0.868
Accuracy of the network on the 482 val images: 74.07%
Max accuracy: 75.10%
Epoch: [49]  [  0/160]  eta: 0:20:05  lr: 0.000029  min_lr: 0.000001  loss: 2.4688 (2.4688)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3393 (12.3393)  time: 7.5349 (7.5349 -- 7.5349)  data: 6.9966 (6.9966 -- 6.9966)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:49  lr: 0.000029  min_lr: 0.000001  loss: 1.8894 (1.9542)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0892 (8.5190)  time: 0.8954 (0.5321 -- 3.5619)  data: 0.3499 (0.0002 -- 3.0320)  max mem: 16413
[2023-08-29 19:38:33,058] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7877
[2023-08-29 19:38:33,058] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7877
[2023-08-29 19:38:33,058] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:38:33,058] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:38:33,058] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [49]  [ 40/160]  eta: 0:02:13  lr: 0.000029  min_lr: 0.000001  loss: 1.9883 (1.9592)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6032 (7.9955)  time: 1.0042 (0.5265 -- 3.8534)  data: 0.4560 (0.0003 -- 3.3266)  max mem: 16413
Epoch: [49]  [ 60/160]  eta: 0:01:41  lr: 0.000029  min_lr: 0.000001  loss: 1.8736 (1.9446)  loss_scale: 16384.0000 (26321.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0217 (7.7230)  time: 0.8292 (0.5134 -- 4.9201)  data: 0.2938 (0.0002 -- 4.4040)  max mem: 16413
Epoch: [49]  [ 80/160]  eta: 0:01:19  lr: 0.000029  min_lr: 0.000001  loss: 1.9997 (1.9404)  loss_scale: 16384.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8121 (7.8590)  time: 0.9434 (0.5223 -- 4.3464)  data: 0.3897 (0.0003 -- 3.8378)  max mem: 16413
Epoch: [49]  [100/160]  eta: 0:00:58  lr: 0.000029  min_lr: 0.000001  loss: 2.0211 (1.9392)  loss_scale: 16384.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7787 (7.8173)  time: 0.8583 (0.5152 -- 4.0553)  data: 0.3209 (0.0003 -- 3.5373)  max mem: 16413
Epoch: [49]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 2.0393 (1.9381)  loss_scale: 16384.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3370 (7.7664)  time: 0.8290 (0.5089 -- 3.9499)  data: 0.2900 (0.0001 -- 3.4187)  max mem: 16413
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.9300 (1.9239)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3642 (7.7671)  time: 0.7944 (0.5301 -- 3.6754)  data: 0.2340 (0.0006 -- 3.1499)  max mem: 16413
[2023-08-29 19:40:16,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[6.816788788070302e-07, 6.816788788070302e-07, 9.08905171742707e-07, 9.08905171742707e-07, 1.2118735623236092e-06, 1.2118735623236092e-06, 1.615831416431479e-06, 1.615831416431479e-06, 2.154441888575305e-06, 2.154441888575305e-06, 2.872589184767074e-06, 2.872589184767074e-06, 3.830118913022765e-06, 3.830118913022765e-06, 5.1068252173636866e-06, 5.1068252173636866e-06, 6.809100289818249e-06, 6.809100289818249e-06, 9.078800386424332e-06, 9.078800386424332e-06, 1.210506718189911e-05, 1.210506718189911e-05, 1.6140089575865478e-05, 1.6140089575865478e-05, 2.1520119434487305e-05, 2.1520119434487305e-05, 2.8693492579316408e-05, 2.8693492579316408e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 19:40:16,399] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=17.544710142956347, CurrSamplesPerSec=24.75910616305942, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.8122 (1.9086)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4347 (7.8676)  time: 0.7135 (0.4965 -- 4.4551)  data: 0.1978 (0.0001 -- 3.9416)  max mem: 16413
Epoch: [49] Total time: 0:02:24 (0.9023 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.8122 (1.9200)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4347 (7.8676)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6604 (0.6604)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3319 (2.3319 -- 2.3319)  data: 2.1200 (2.1200 -- 2.1200)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6604 (0.9192)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4241 (0.1915 -- 2.3319)  data: 0.2079 (0.0007 -- 2.1200)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6855 (0.8580)  acc1: 66.6667 (71.9577)  acc5: 100.0000 (96.2963)  time: 0.2239 (0.1695 -- 0.3901)  data: 0.0189 (0.0001 -- 0.2085)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8030 (0.9089)  acc1: 66.6667 (69.2946)  acc5: 100.0000 (95.8506)  time: 0.2108 (0.1330 -- 0.3901)  data: 0.0185 (0.0001 -- 0.2085)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 74.274 Acc@5 95.851 loss 0.858
Accuracy of the network on the 482 val images: 74.27%
Max accuracy: 75.10%
Epoch: [50]  [  0/160]  eta: 0:19:21  lr: 0.000029  min_lr: 0.000001  loss: 2.1509 (2.1509)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7487 (7.7487)  time: 7.2610 (7.2610 -- 7.2610)  data: 5.9553 (5.9553 -- 5.9553)  max mem: 16413
[2023-08-29 19:40:34,744] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:40:34,745] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:40:34,745] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:40:34,745] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [50]  [ 20/160]  eta: 0:02:36  lr: 0.000029  min_lr: 0.000001  loss: 1.6591 (1.7436)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7319 (6.8375)  time: 0.8138 (0.5300 -- 3.3591)  data: 0.2021 (0.0004 -- 1.9915)  max mem: 16413
Epoch: [50]  [ 40/160]  eta: 0:02:02  lr: 0.000029  min_lr: 0.000001  loss: 1.6827 (1.7615)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1587 (7.1430)  time: 0.9173 (0.5332 -- 2.4548)  data: 0.3011 (0.0003 -- 1.7839)  max mem: 16413
[2023-08-29 19:41:22,647] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8058
[2023-08-29 19:41:22,647] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8058
[2023-08-29 19:41:22,647] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:41:22,647] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:41:22,648] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [50]  [ 60/160]  eta: 0:01:37  lr: 0.000029  min_lr: 0.000001  loss: 1.8685 (1.7829)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3555 (7.3517)  time: 0.8844 (0.5219 -- 2.4478)  data: 0.2630 (0.0003 -- 1.9259)  max mem: 16413
Epoch: [50]  [ 80/160]  eta: 0:01:16  lr: 0.000029  min_lr: 0.000001  loss: 2.0089 (1.8132)  loss_scale: 16384.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6763 (7.6007)  time: 0.8895 (0.5508 -- 2.9490)  data: 0.2536 (0.0003 -- 2.4113)  max mem: 16413
Epoch: [50]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000001  loss: 1.9038 (1.8330)  loss_scale: 16384.0000 (24819.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0806 (7.8535)  time: 0.8124 (0.5171 -- 3.5268)  data: 0.2655 (0.0003 -- 2.9825)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000001  loss: 1.8104 (1.8385)  loss_scale: 16384.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4039 (7.9662)  time: 0.8598 (0.5202 -- 4.4007)  data: 0.3147 (0.0003 -- 3.9036)  max mem: 16413
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.9208 (1.8572)  loss_scale: 16384.0000 (22426.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0216 (8.1131)  time: 0.8739 (0.5313 -- 2.8273)  data: 0.3244 (0.0003 -- 2.2941)  max mem: 16413
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.8733 (1.8581)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0234 (8.2440)  time: 0.6638 (0.4977 -- 2.6601)  data: 0.1446 (0.0001 -- 2.1217)  max mem: 16413
Epoch: [50] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.8733 (1.8891)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0234 (8.2440)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.6273 (0.6273)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2398 (2.2398 -- 2.2398)  data: 2.0301 (2.0301 -- 2.0301)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6625 (0.9002)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4066 (0.1968 -- 2.2398)  data: 0.1925 (0.0008 -- 2.0301)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6924 (0.8606)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.2963)  time: 0.2278 (0.1696 -- 0.5116)  data: 0.0236 (0.0001 -- 0.3285)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8199 (0.9052)  acc1: 66.6667 (73.0290)  acc5: 100.0000 (96.2656)  time: 0.2137 (0.1329 -- 0.5116)  data: 0.0233 (0.0001 -- 0.3285)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 76.556 Acc@5 96.266 loss 0.854
Accuracy of the network on the 482 val images: 76.56%
[2023-08-29 19:42:53,004] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:42:53,005] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:42:53,005] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:42:53,005] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:42:54,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:42:54,382] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.56%
Epoch: [51]  [  0/160]  eta: 0:19:53  lr: 0.000029  min_lr: 0.000001  loss: 2.2250 (2.2250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1004 (8.1004)  time: 7.4603 (7.4603 -- 7.4603)  data: 6.8925 (6.8925 -- 6.8925)  max mem: 16413
Epoch: [51]  [ 20/160]  eta: 0:02:49  lr: 0.000028  min_lr: 0.000001  loss: 1.8656 (1.8936)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9581 (8.4274)  time: 0.8972 (0.5347 -- 2.5207)  data: 0.1019 (0.0003 -- 1.9994)  max mem: 16413
[2023-08-29 19:43:26,299] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:43:26,300] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:43:26,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:43:26,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [51]  [ 40/160]  eta: 0:02:03  lr: 0.000028  min_lr: 0.000001  loss: 1.7555 (1.8715)  loss_scale: 32768.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6099 (8.1925)  time: 0.8396 (0.5220 -- 3.0367)  data: 0.0897 (0.0003 -- 1.7210)  max mem: 16413
[2023-08-29 19:43:37,114] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8201
[2023-08-29 19:43:37,114] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8201
[2023-08-29 19:43:37,155] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:43:37,155] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:43:37,155] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [51]  [ 60/160]  eta: 0:01:36  lr: 0.000028  min_lr: 0.000001  loss: 1.7892 (1.8752)  loss_scale: 16384.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6212 (8.3955)  time: 0.8270 (0.5247 -- 3.7071)  data: 0.2470 (0.0002 -- 3.1574)  max mem: 16413
Epoch: [51]  [ 80/160]  eta: 0:01:14  lr: 0.000028  min_lr: 0.000001  loss: 1.8255 (1.8784)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8603 (8.2023)  time: 0.8576 (0.5397 -- 2.1852)  data: 0.3021 (0.0003 -- 1.6557)  max mem: 16413
Epoch: [51]  [100/160]  eta: 0:00:54  lr: 0.000028  min_lr: 0.000001  loss: 1.8775 (1.8886)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2079 (8.1155)  time: 0.8019 (0.5328 -- 2.1793)  data: 0.2544 (0.0006 -- 1.6429)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000001  loss: 1.9582 (1.8995)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5811 (8.0355)  time: 0.8906 (0.5339 -- 3.0567)  data: 0.3269 (0.0008 -- 2.5070)  max mem: 16413
Epoch: [51]  [140/160]  eta: 0:00:17  lr: 0.000028  min_lr: 0.000001  loss: 1.8570 (1.9021)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1409 (8.1400)  time: 0.7998 (0.5264 -- 2.7319)  data: 0.2531 (0.0002 -- 2.1984)  max mem: 16413
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.9100 (1.9016)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1686 (8.0751)  time: 0.7614 (0.4949 -- 3.0518)  data: 0.2301 (0.0002 -- 2.4934)  max mem: 16413
Epoch: [51] Total time: 0:02:20 (0.8779 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.9100 (1.9020)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1686 (8.0751)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.6229 (0.6229)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4279 (2.4279 -- 2.4279)  data: 2.2024 (2.2024 -- 2.2024)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6467 (0.9112)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4191 (0.1931 -- 2.4279)  data: 0.2071 (0.0006 -- 2.2024)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6467 (0.8542)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (96.2963)  time: 0.2175 (0.1710 -- 0.3882)  data: 0.0134 (0.0001 -- 0.1888)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8147 (0.9077)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (95.4357)  time: 0.2029 (0.1338 -- 0.3882)  data: 0.0131 (0.0001 -- 0.1888)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 75.934 Acc@5 96.058 loss 0.854
Accuracy of the network on the 482 val images: 75.93%
Max accuracy: 76.56%
Epoch: [52]  [  0/160]  eta: 0:17:34  lr: 0.000028  min_lr: 0.000001  loss: 1.8337 (1.8337)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9328 (8.9328)  time: 6.5900 (6.5900 -- 6.5900)  data: 6.0431 (6.0431 -- 6.0431)  max mem: 16413
[2023-08-29 19:45:37,166] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:45:37,166] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:45:37,166] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:45:37,166] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [52]  [ 20/160]  eta: 0:02:31  lr: 0.000028  min_lr: 0.000001  loss: 1.6971 (1.7547)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9273 (7.3479)  time: 0.8048 (0.5196 -- 3.1187)  data: 0.0557 (0.0002 -- 0.7005)  max mem: 16413
[2023-08-29 19:46:04,706] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8359
[2023-08-29 19:46:04,706] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:46:04,706] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 19:46:04,706] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8359
[2023-08-29 19:46:04,706] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [52]  [ 40/160]  eta: 0:02:04  lr: 0.000028  min_lr: 0.000001  loss: 1.8943 (1.7995)  loss_scale: 32768.0000 (27972.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8400 (7.5185)  time: 0.9962 (0.5250 -- 3.5914)  data: 0.0022 (0.0005 -- 0.0088)  max mem: 16413
Epoch: [52]  [ 60/160]  eta: 0:01:34  lr: 0.000028  min_lr: 0.000001  loss: 1.9755 (1.8691)  loss_scale: 16384.0000 (24173.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3649 (7.7861)  time: 0.7624 (0.5165 -- 3.2911)  data: 0.0017 (0.0002 -- 0.0072)  max mem: 16413
Epoch: [52]  [ 80/160]  eta: 0:01:16  lr: 0.000028  min_lr: 0.000001  loss: 1.8958 (1.8826)  loss_scale: 16384.0000 (22249.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2288 (8.1238)  time: 0.9858 (0.5252 -- 3.3750)  data: 0.0275 (0.0003 -- 0.5283)  max mem: 16413
Epoch: [52]  [100/160]  eta: 0:00:56  lr: 0.000028  min_lr: 0.000001  loss: 1.9718 (1.8921)  loss_scale: 16384.0000 (21088.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7497 (8.0408)  time: 0.8536 (0.5289 -- 2.2187)  data: 0.0024 (0.0002 -- 0.0154)  max mem: 16413
Epoch: [52]  [120/160]  eta: 0:00:37  lr: 0.000028  min_lr: 0.000001  loss: 1.8830 (1.8976)  loss_scale: 16384.0000 (20310.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5047 (8.0009)  time: 0.9258 (0.5109 -- 2.9285)  data: 0.0024 (0.0003 -- 0.0160)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000001  loss: 1.9425 (1.8980)  loss_scale: 16384.0000 (19753.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6152 (7.9592)  time: 0.8487 (0.5279 -- 2.6134)  data: 0.0017 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.9712 (1.8989)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2525 (7.9588)  time: 0.6374 (0.4956 -- 2.8898)  data: 0.0007 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [52] Total time: 0:02:22 (0.8897 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.9712 (1.9142)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2525 (7.9588)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6432 (0.6432)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3467 (2.3467 -- 2.3467)  data: 2.1162 (2.1162 -- 2.1162)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6506 (0.8945)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4084 (0.1974 -- 2.3467)  data: 0.1940 (0.0005 -- 2.1162)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7063 (0.8456)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (96.2963)  time: 0.2235 (0.1689 -- 0.3758)  data: 0.0162 (0.0001 -- 0.1587)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7793 (0.8971)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.8506)  time: 0.2092 (0.1326 -- 0.3758)  data: 0.0160 (0.0001 -- 0.1587)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 77.386 Acc@5 96.266 loss 0.844
Accuracy of the network on the 482 val images: 77.39%
[2023-08-29 19:47:52,802] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 19:47:52,804] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 19:47:52,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 19:47:52,804] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 19:47:54,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 19:47:54,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.39%
Epoch: [53]  [  0/160]  eta: 0:22:52  lr: 0.000028  min_lr: 0.000001  loss: 2.0770 (2.0770)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1777 (4.1777)  time: 8.5752 (8.5752 -- 8.5752)  data: 8.0519 (8.0519 -- 8.0519)  max mem: 16413
[2023-08-29 19:48:09,643] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:48:09,644] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:48:09,644] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:48:09,644] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:48:12,924] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8494
[2023-08-29 19:48:12,924] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8494
[2023-08-29 19:48:12,924] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:48:12,924] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:48:12,925] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [53]  [ 20/160]  eta: 0:02:49  lr: 0.000028  min_lr: 0.000001  loss: 1.9195 (1.8676)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0387 (7.7393)  time: 0.8426 (0.5230 -- 3.9595)  data: 0.2950 (0.0003 -- 3.4298)  max mem: 16413
Epoch: [53]  [ 40/160]  eta: 0:02:13  lr: 0.000028  min_lr: 0.000001  loss: 1.8358 (1.8544)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9523 (8.2934)  time: 1.0145 (0.5117 -- 4.9591)  data: 0.4717 (0.0003 -- 4.4207)  max mem: 16413
Epoch: [53]  [ 60/160]  eta: 0:01:40  lr: 0.000028  min_lr: 0.000001  loss: 2.0270 (1.9329)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1495 (8.5640)  time: 0.7758 (0.5252 -- 3.6152)  data: 0.2285 (0.0002 -- 3.0889)  max mem: 16413
Epoch: [53]  [ 80/160]  eta: 0:01:14  lr: 0.000028  min_lr: 0.000001  loss: 1.7977 (1.9014)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9172 (8.4943)  time: 0.6981 (0.5224 -- 2.6546)  data: 0.1468 (0.0008 -- 2.1070)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000001  loss: 2.0566 (1.9256)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1188 (8.4500)  time: 0.9188 (0.5430 -- 2.3061)  data: 0.2662 (0.0004 -- 1.5047)  max mem: 16413
Epoch: [53]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000001  loss: 1.9014 (1.9168)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4385 (8.2140)  time: 0.8712 (0.5241 -- 2.5875)  data: 0.1327 (0.0006 -- 1.6195)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000001  loss: 1.7889 (1.9057)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8539 (8.2318)  time: 0.8182 (0.5246 -- 2.4381)  data: 0.0773 (0.0003 -- 1.5163)  max mem: 16413
[2023-08-29 19:50:03,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:50:03,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:50:03,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:50:03,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:50:16,614] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8639
[2023-08-29 19:50:16,614] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8639
[2023-08-29 19:50:16,614] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:50:16,614] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:50:16,614] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.9379 (1.9023)  loss_scale: 32768.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3881 (8.3265)  time: 0.7834 (0.4820 -- 4.3405)  data: 0.0061 (0.0002 -- 0.0845)  max mem: 16413
Epoch: [53] Total time: 0:02:22 (0.8901 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.9379 (1.8809)  loss_scale: 32768.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3881 (8.3265)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.6461 (0.6461)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2710 (2.2710 -- 2.2710)  data: 2.0089 (2.0089 -- 2.0089)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6461 (0.9007)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (94.9495)  time: 0.4059 (0.1987 -- 2.2710)  data: 0.1881 (0.0007 -- 2.0089)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6176 (0.8411)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (95.7672)  time: 0.2264 (0.1692 -- 0.5562)  data: 0.0213 (0.0001 -- 0.3629)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7422 (0.8891)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.4357)  time: 0.2107 (0.1328 -- 0.5562)  data: 0.0207 (0.0001 -- 0.3629)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 75.934 Acc@5 96.058 loss 0.843
Accuracy of the network on the 482 val images: 75.93%
Max accuracy: 77.39%
Epoch: [54]  [  0/160]  eta: 0:20:10  lr: 0.000028  min_lr: 0.000001  loss: 2.4097 (2.4097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3881 (8.3881)  time: 7.5684 (7.5684 -- 7.5684)  data: 7.0057 (7.0057 -- 7.0057)  max mem: 16413
Epoch: [54]  [ 20/160]  eta: 0:02:40  lr: 0.000028  min_lr: 0.000001  loss: 1.8836 (1.8497)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2144 (8.5915)  time: 0.8241 (0.5251 -- 2.7945)  data: 0.2811 (0.0004 -- 2.2592)  max mem: 16413
[2023-08-29 19:51:04,051] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8677
[2023-08-29 19:51:04,052] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 19:51:04,052] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-29 19:51:04,051] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8677
[2023-08-29 19:51:04,055] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [54]  [ 40/160]  eta: 0:02:05  lr: 0.000028  min_lr: 0.000001  loss: 1.7779 (1.8614)  loss_scale: 16384.0000 (15584.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4407 (8.3155)  time: 0.9401 (0.5271 -- 4.6075)  data: 0.3874 (0.0006 -- 4.0670)  max mem: 16413
Epoch: [54]  [ 60/160]  eta: 0:01:41  lr: 0.000028  min_lr: 0.000001  loss: 1.8545 (1.8576)  loss_scale: 8192.0000 (13160.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0863 (8.4028)  time: 0.9537 (0.5242 -- 4.9346)  data: 0.4048 (0.0003 -- 4.4234)  max mem: 16413
Epoch: [54]  [ 80/160]  eta: 0:01:16  lr: 0.000028  min_lr: 0.000001  loss: 2.0501 (1.8956)  loss_scale: 8192.0000 (11934.0247)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3817 (8.3671)  time: 0.7879 (0.5251 -- 3.2345)  data: 0.2407 (0.0003 -- 2.7268)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:57  lr: 0.000028  min_lr: 0.000001  loss: 1.9154 (1.8926)  loss_scale: 8192.0000 (11193.0297)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7024 (8.4402)  time: 0.9270 (0.5026 -- 4.4653)  data: 0.3825 (0.0003 -- 3.9577)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:37  lr: 0.000028  min_lr: 0.000001  loss: 1.9306 (1.8933)  loss_scale: 8192.0000 (10696.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6597 (8.3925)  time: 0.8697 (0.5316 -- 3.3407)  data: 0.3157 (0.0002 -- 2.7781)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000001  loss: 1.8949 (1.8915)  loss_scale: 8192.0000 (10341.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9551 (8.3431)  time: 0.7779 (0.5335 -- 2.9189)  data: 0.2211 (0.0003 -- 2.3331)  max mem: 16413
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.7994 (1.8857)  loss_scale: 8192.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9554 (8.3251)  time: 0.6990 (0.4945 -- 3.8869)  data: 0.1842 (0.0002 -- 3.3581)  max mem: 16413
Epoch: [54] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.7994 (1.8849)  loss_scale: 8192.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9554 (8.3251)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6546 (0.6546)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3398 (2.3398 -- 2.3398)  data: 2.1250 (2.1250 -- 2.1250)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6546 (0.8890)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (94.9495)  time: 0.4218 (0.1943 -- 2.3398)  data: 0.2003 (0.0007 -- 2.1250)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6631 (0.8303)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (95.7672)  time: 0.2224 (0.1697 -- 0.3719)  data: 0.0118 (0.0001 -- 0.1550)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7984 (0.8824)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (95.0207)  time: 0.2029 (0.1328 -- 0.3719)  data: 0.0115 (0.0001 -- 0.1550)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 76.349 Acc@5 96.058 loss 0.836
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 77.39%
Epoch: [55]  [  0/160]  eta: 0:22:21  lr: 0.000028  min_lr: 0.000001  loss: 1.5452 (1.5452)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5790 (10.5790)  time: 8.3848 (8.3848 -- 8.3848)  data: 6.8512 (6.8512 -- 6.8512)  max mem: 16413
[2023-08-29 19:53:06,430] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:53:06,430] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 19:53:06,430] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:53:06,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [55]  [ 20/160]  eta: 0:02:39  lr: 0.000028  min_lr: 0.000001  loss: 1.9024 (1.9546)  loss_scale: 16384.0000 (14043.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7351 (8.7089)  time: 0.7784 (0.5157 -- 3.0051)  data: 0.1932 (0.0002 -- 2.1042)  max mem: 16413
Epoch: [55]  [ 40/160]  eta: 0:01:59  lr: 0.000028  min_lr: 0.000001  loss: 1.9632 (1.9813)  loss_scale: 16384.0000 (15185.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7015 (8.3242)  time: 0.8466 (0.5374 -- 3.2740)  data: 0.2944 (0.0004 -- 2.7116)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:36  lr: 0.000028  min_lr: 0.000001  loss: 1.9071 (1.9530)  loss_scale: 16384.0000 (15578.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1735 (8.2025)  time: 0.9092 (0.5177 -- 4.2532)  data: 0.3629 (0.0004 -- 3.7385)  max mem: 16413
Epoch: [55]  [ 80/160]  eta: 0:01:13  lr: 0.000028  min_lr: 0.000001  loss: 1.8174 (1.9227)  loss_scale: 16384.0000 (15777.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6616 (8.2571)  time: 0.7887 (0.5285 -- 3.1665)  data: 0.1446 (0.0003 -- 2.6070)  max mem: 16413
Epoch: [55]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000001  loss: 1.9019 (1.9113)  loss_scale: 16384.0000 (15897.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1427 (8.4334)  time: 0.9466 (0.5390 -- 3.4443)  data: 0.2987 (0.0002 -- 2.9150)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000001  loss: 1.9362 (1.9107)  loss_scale: 16384.0000 (15977.7851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5132 (8.3586)  time: 0.8075 (0.5210 -- 3.5609)  data: 0.0021 (0.0004 -- 0.0162)  max mem: 16413
[2023-08-29 19:54:58,354] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:54:58,354] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:54:58,354] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:54:58,354] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000001  loss: 2.0373 (1.9217)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7350 (8.2610)  time: 0.9261 (0.5210 -- 5.4417)  data: 0.0013 (0.0002 -- 0.0057)  max mem: 16413
[2023-08-29 19:55:15,795] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8958
[2023-08-29 19:55:15,795] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8958
[2023-08-29 19:55:15,795] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:55:15,795] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:55:15,795] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.9251 (1.9312)  loss_scale: 32768.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2281 (8.2960)  time: 0.6765 (0.4897 -- 1.9687)  data: 0.0011 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [55] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.9251 (1.9042)  loss_scale: 32768.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2281 (8.2960)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.6501 (0.6501)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5864 (2.5864 -- 2.5864)  data: 2.3347 (2.3347 -- 2.3347)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6736 (0.9038)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (94.9495)  time: 0.4329 (0.2052 -- 2.5864)  data: 0.2134 (0.0009 -- 2.3347)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6683 (0.8329)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (95.2381)  time: 0.2079 (0.1699 -- 0.2322)  data: 0.0012 (0.0001 -- 0.0066)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7866 (0.8889)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (94.6058)  time: 0.1919 (0.1330 -- 0.2322)  data: 0.0009 (0.0001 -- 0.0066)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 76.141 Acc@5 95.021 loss 0.843
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 77.39%
Epoch: [56]  [  0/160]  eta: 0:18:23  lr: 0.000028  min_lr: 0.000001  loss: 2.1099 (2.1099)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1398 (5.1398)  time: 6.8973 (6.8973 -- 6.8973)  data: 6.3069 (6.3069 -- 6.3069)  max mem: 16413
Epoch: [56]  [ 20/160]  eta: 0:02:38  lr: 0.000028  min_lr: 0.000001  loss: 1.9596 (1.9810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6799 (8.5749)  time: 0.8404 (0.5263 -- 3.7980)  data: 0.2534 (0.0002 -- 3.2421)  max mem: 16413
[2023-08-29 19:56:04,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=50, lr=[6.542618919201115e-07, 6.542618919201115e-07, 8.723491892268153e-07, 8.723491892268153e-07, 1.1631322523024204e-06, 1.1631322523024204e-06, 1.5508430030698938e-06, 1.5508430030698938e-06, 2.0677906707598586e-06, 2.0677906707598586e-06, 2.757054227679811e-06, 2.757054227679811e-06, 3.676072303573082e-06, 3.676072303573082e-06, 4.901429738097442e-06, 4.901429738097442e-06, 6.53523965079659e-06, 6.53523965079659e-06, 8.713652867728786e-06, 8.713652867728786e-06, 1.1618203823638382e-05, 1.1618203823638382e-05, 1.5490938431517843e-05, 1.5490938431517843e-05, 2.0654584575357124e-05, 2.0654584575357124e-05, 2.7539446100476165e-05, 2.7539446100476165e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 19:56:04,439] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=17.690728613375526, CurrSamplesPerSec=21.872179915277954, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:03  lr: 0.000028  min_lr: 0.000001  loss: 1.6817 (1.8814)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1502 (8.4583)  time: 0.9286 (0.5287 -- 3.7647)  data: 0.3839 (0.0006 -- 3.2459)  max mem: 16413
Epoch: [56]  [ 60/160]  eta: 0:01:37  lr: 0.000028  min_lr: 0.000001  loss: 1.8829 (1.8614)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6950 (8.4097)  time: 0.8746 (0.5183 -- 2.8992)  data: 0.0438 (0.0003 -- 0.8447)  max mem: 16413
Epoch: [56]  [ 80/160]  eta: 0:01:15  lr: 0.000027  min_lr: 0.000001  loss: 1.9533 (1.8855)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7509 (8.6346)  time: 0.8532 (0.5244 -- 2.7928)  data: 0.1519 (0.0007 -- 1.5538)  max mem: 16413
Epoch: [56]  [100/160]  eta: 0:00:56  lr: 0.000027  min_lr: 0.000001  loss: 1.7530 (1.8468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4794 (8.6359)  time: 0.8993 (0.5206 -- 3.0757)  data: 0.0319 (0.0001 -- 0.6125)  max mem: 16413
Epoch: [56]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000001  loss: 1.9766 (1.8606)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9085 (8.5623)  time: 0.8375 (0.5365 -- 2.6967)  data: 0.1308 (0.0003 -- 1.3805)  max mem: 16413
[2023-08-29 19:57:21,464] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:57:21,464] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:57:21,465] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:57:21,465] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:57:31,820] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9099
[2023-08-29 19:57:31,820] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9099
[2023-08-29 19:57:31,820] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:57:31,820] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 19:57:31,820] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 1.8306 (1.8584)  loss_scale: 32768.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4848 (8.4221)  time: 0.8328 (0.5244 -- 3.0415)  data: 0.0597 (0.0002 -- 1.1709)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.8944 (1.8681)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8605 (8.4092)  time: 0.6936 (0.4983 -- 3.0067)  data: 0.0010 (0.0001 -- 0.0064)  max mem: 16413
Epoch: [56] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.8944 (1.8618)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8605 (8.4092)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.5615 (0.5615)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5469 (2.5469 -- 2.5469)  data: 2.2996 (2.2996 -- 2.2996)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5888 (0.8920)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4457 (0.1939 -- 2.5469)  data: 0.2311 (0.0005 -- 2.2996)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6753 (0.8186)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (96.2963)  time: 0.2166 (0.1696 -- 0.4520)  data: 0.0137 (0.0001 -- 0.2329)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7549 (0.8786)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (95.4357)  time: 0.2043 (0.1328 -- 0.4520)  data: 0.0134 (0.0001 -- 0.2329)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 77.178 Acc@5 95.851 loss 0.833
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 77.39%
Epoch: [57]  [  0/160]  eta: 0:19:54  lr: 0.000027  min_lr: 0.000001  loss: 2.1023 (2.1023)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8563 (8.8563)  time: 7.4640 (7.4640 -- 7.4640)  data: 6.9297 (6.9297 -- 6.9297)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:43  lr: 0.000027  min_lr: 0.000001  loss: 2.0021 (1.9526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7436 (8.6058)  time: 0.8515 (0.5294 -- 2.6702)  data: 0.3009 (0.0003 -- 2.1365)  max mem: 16413
Epoch: [57]  [ 40/160]  eta: 0:02:15  lr: 0.000027  min_lr: 0.000001  loss: 1.7010 (1.8342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4651 (8.3675)  time: 1.0971 (0.5228 -- 4.3860)  data: 0.5499 (0.0003 -- 3.8301)  max mem: 16413
Epoch: [57]  [ 60/160]  eta: 0:01:37  lr: 0.000027  min_lr: 0.000001  loss: 1.8020 (1.8289)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1352 (8.0609)  time: 0.6400 (0.5344 -- 1.6586)  data: 0.0901 (0.0002 -- 1.1466)  max mem: 16413
Epoch: [57]  [ 80/160]  eta: 0:01:17  lr: 0.000027  min_lr: 0.000001  loss: 1.7908 (1.8365)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8933 (8.3910)  time: 0.9772 (0.5243 -- 4.1448)  data: 0.4275 (0.0003 -- 3.6153)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:56  lr: 0.000027  min_lr: 0.000001  loss: 1.9911 (1.8519)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4074 (8.3011)  time: 0.8326 (0.5199 -- 3.5709)  data: 0.2878 (0.0002 -- 3.0577)  max mem: 16413
[2023-08-29 19:59:36,823] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:59:36,823] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 19:59:36,824] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 19:59:36,825] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [120/160]  eta: 0:00:38  lr: 0.000027  min_lr: 0.000001  loss: 1.8874 (1.8480)  loss_scale: 32768.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1886 (8.2140)  time: 1.0236 (0.5258 -- 4.6283)  data: 0.4858 (0.0004 -- 4.0988)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 2.0174 (1.8624)  loss_scale: 32768.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6184 (8.2105)  time: 0.7331 (0.5173 -- 3.3972)  data: 0.1884 (0.0002 -- 2.8691)  max mem: 16413
[2023-08-29 20:00:11,344] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9269
[2023-08-29 20:00:11,344] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9269
[2023-08-29 20:00:11,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:00:11,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:00:11,345] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.9981 (1.8699)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8614 (8.2706)  time: 0.6793 (0.4960 -- 2.8369)  data: 0.1583 (0.0001 -- 2.3213)  max mem: 16413
Epoch: [57] Total time: 0:02:23 (0.8976 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.9981 (1.9030)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8614 (8.2706)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5395 (0.5395)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4377 (2.4377 -- 2.4377)  data: 2.1881 (2.1881 -- 2.1881)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5900 (0.8762)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4271 (0.2009 -- 2.4377)  data: 0.2088 (0.0004 -- 2.1881)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5900 (0.8025)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (96.2963)  time: 0.2157 (0.1720 -- 0.3535)  data: 0.0064 (0.0001 -- 0.1000)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7366 (0.8650)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (95.8506)  time: 0.2015 (0.1325 -- 0.3535)  data: 0.0061 (0.0001 -- 0.1000)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 76.556 Acc@5 96.058 loss 0.819
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 77.39%
Epoch: [58]  [  0/160]  eta: 0:25:44  lr: 0.000027  min_lr: 0.000001  loss: 1.8369 (1.8369)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5702 (8.5702)  time: 9.6509 (9.6509 -- 9.6509)  data: 6.1241 (6.1241 -- 6.1241)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:49  lr: 0.000027  min_lr: 0.000001  loss: 1.6143 (1.7464)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2522 (7.8679)  time: 0.7858 (0.5325 -- 3.4080)  data: 0.0016 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [58]  [ 40/160]  eta: 0:02:08  lr: 0.000027  min_lr: 0.000001  loss: 1.9322 (1.8141)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6361 (7.9765)  time: 0.9207 (0.5278 -- 4.5301)  data: 0.1046 (0.0002 -- 0.9207)  max mem: 16413
Epoch: [58]  [ 60/160]  eta: 0:01:41  lr: 0.000027  min_lr: 0.000001  loss: 1.9208 (1.8212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0107 (8.0775)  time: 0.9091 (0.5378 -- 5.9641)  data: 0.0765 (0.0001 -- 1.1240)  max mem: 16413
Epoch: [58]  [ 80/160]  eta: 0:01:19  lr: 0.000027  min_lr: 0.000001  loss: 1.8418 (1.8278)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7033 (8.5025)  time: 0.9191 (0.5235 -- 3.7192)  data: 0.0014 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:57  lr: 0.000027  min_lr: 0.000001  loss: 2.1206 (1.8713)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0519 (8.4882)  time: 0.8493 (0.5158 -- 4.0184)  data: 0.0011 (0.0002 -- 0.0026)  max mem: 16413
[2023-08-29 20:02:15,879] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:02:15,879] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:02:15,879] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:02:15,880] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [58]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000001  loss: 1.9859 (1.9058)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8944 (8.3735)  time: 0.7933 (0.5179 -- 2.9166)  data: 0.0015 (0.0003 -- 0.0058)  max mem: 16413
[2023-08-29 20:02:22,066] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9407
[2023-08-29 20:02:22,067] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9407
[2023-08-29 20:02:22,067] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:02:22,067] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:02:22,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 1.9598 (1.9107)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5052 (8.2170)  time: 0.7990 (0.5256 -- 2.9437)  data: 0.0367 (0.0006 -- 0.7041)  max mem: 16413
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.9312 (1.9087)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9202 (8.1320)  time: 0.7260 (0.4954 -- 2.1342)  data: 0.1174 (0.0002 -- 1.2285)  max mem: 16413
Epoch: [58] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.9312 (1.9036)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9202 (8.1320)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.6087 (0.6087)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2090 (2.2090 -- 2.2090)  data: 1.9889 (1.9889 -- 1.9889)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6087 (0.8699)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4041 (0.1939 -- 2.2090)  data: 0.1925 (0.0004 -- 1.9889)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6028 (0.8055)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2259 (0.1693 -- 0.3507)  data: 0.0197 (0.0001 -- 0.1638)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7667 (0.8623)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.8506)  time: 0.2118 (0.1331 -- 0.3507)  data: 0.0195 (0.0001 -- 0.1638)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 76.971 Acc@5 96.266 loss 0.815
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 77.39%
Epoch: [59]  [  0/160]  eta: 0:18:55  lr: 0.000027  min_lr: 0.000001  loss: 1.7604 (1.7604)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0891 (8.0891)  time: 7.0961 (7.0961 -- 7.0961)  data: 6.5323 (6.5323 -- 6.5323)  max mem: 16413
Epoch: [59]  [ 20/160]  eta: 0:02:33  lr: 0.000027  min_lr: 0.000001  loss: 1.7244 (1.8568)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4804 (7.6905)  time: 0.7948 (0.5382 -- 3.0816)  data: 0.2230 (0.0005 -- 2.5341)  max mem: 16413
Epoch: [59]  [ 40/160]  eta: 0:02:05  lr: 0.000027  min_lr: 0.000001  loss: 2.0054 (1.9275)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6605 (7.9102)  time: 0.9864 (0.5207 -- 3.5979)  data: 0.4459 (0.0004 -- 3.0716)  max mem: 16413
Epoch: [59]  [ 60/160]  eta: 0:01:35  lr: 0.000027  min_lr: 0.000001  loss: 1.9454 (1.9183)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3187 (7.8995)  time: 0.7858 (0.5347 -- 2.6469)  data: 0.1670 (0.0006 -- 2.1019)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:15  lr: 0.000027  min_lr: 0.000001  loss: 1.6855 (1.8554)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6514 (8.0620)  time: 0.8844 (0.5288 -- 4.7895)  data: 0.0179 (0.0001 -- 0.3343)  max mem: 16413
[2023-08-29 20:04:25,074] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:04:25,074] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:04:25,075] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:04:25,075] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000027  min_lr: 0.000001  loss: 1.9562 (1.8890)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6624 (8.0639)  time: 0.9831 (0.5040 -- 4.3394)  data: 0.0014 (0.0002 -- 0.0067)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000001  loss: 1.9235 (1.8745)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9080 (8.0842)  time: 0.7701 (0.5290 -- 3.2127)  data: 0.0013 (0.0001 -- 0.0029)  max mem: 16413
Epoch: [59]  [140/160]  eta: 0:00:17  lr: 0.000027  min_lr: 0.000001  loss: 1.7987 (1.8735)  loss_scale: 32768.0000 (21612.9362)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9085 (8.0602)  time: 0.7491 (0.5301 -- 2.7038)  data: 0.0017 (0.0004 -- 0.0067)  max mem: 16413
[2023-08-29 20:05:02,744] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9582
[2023-08-29 20:05:02,744] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9582
[2023-08-29 20:05:02,745] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:05:02,745] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 20:05:02,745] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.7534 (1.8663)  loss_scale: 16384.0000 (21094.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9372 (8.0509)  time: 0.7232 (0.4958 -- 1.9643)  data: 0.0602 (0.0002 -- 0.4953)  max mem: 16413
Epoch: [59] Total time: 0:02:20 (0.8756 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.7534 (1.8542)  loss_scale: 16384.0000 (21094.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9372 (8.0509)
[2023-08-29 20:05:15,283] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-08-29 20:05:15,284] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-08-29 20:05:15,284] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-08-29 20:05:15,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-08-29 20:05:16,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-08-29 20:05:16,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.5495 (0.5495)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3503 (2.3503 -- 2.3503)  data: 2.1377 (2.1377 -- 2.1377)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5938 (0.8622)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4305 (0.2044 -- 2.3503)  data: 0.2116 (0.0005 -- 2.1377)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6325 (0.7946)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.2963)  time: 0.2303 (0.1683 -- 0.4649)  data: 0.0240 (0.0001 -- 0.2867)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7051 (0.8525)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.8506)  time: 0.2146 (0.1319 -- 0.4649)  data: 0.0238 (0.0001 -- 0.2867)  max mem: 16413
Val: Total time: 0:00:07 (0.2940 s / it)
* Acc@1 78.008 Acc@5 96.266 loss 0.804
Accuracy of the network on the 482 val images: 78.01%
[2023-08-29 20:05:24,258] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 20:05:24,259] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 20:05:24,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 20:05:24,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 20:05:25,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 20:05:25,401] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.01%
Epoch: [60]  [  0/160]  eta: 0:24:15  lr: 0.000027  min_lr: 0.000001  loss: 2.1371 (2.1371)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5820 (12.5820)  time: 9.0946 (9.0946 -- 9.0946)  data: 8.5736 (8.5736 -- 8.5736)  max mem: 16413
Epoch: [60]  [ 20/160]  eta: 0:02:40  lr: 0.000027  min_lr: 0.000001  loss: 1.7115 (1.8152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3423 (8.6284)  time: 0.7455 (0.5305 -- 2.7396)  data: 0.2009 (0.0003 -- 2.2065)  max mem: 16413
Epoch: [60]  [ 40/160]  eta: 0:02:01  lr: 0.000027  min_lr: 0.000001  loss: 1.8381 (1.8504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1996 (8.2005)  time: 0.8784 (0.5215 -- 2.9068)  data: 0.2456 (0.0004 -- 1.8258)  max mem: 16413
Epoch: [60]  [ 60/160]  eta: 0:01:34  lr: 0.000027  min_lr: 0.000001  loss: 1.8577 (1.8586)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6251 (8.4052)  time: 0.8026 (0.5381 -- 2.3763)  data: 0.2152 (0.0004 -- 1.8227)  max mem: 16413
[2023-08-29 20:06:38,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9678
[2023-08-29 20:06:38,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9678
[2023-08-29 20:06:38,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:06:38,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:06:38,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [60]  [ 80/160]  eta: 0:01:14  lr: 0.000027  min_lr: 0.000001  loss: 1.8554 (1.8806)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3530 (8.2139)  time: 0.9149 (0.5195 -- 2.7735)  data: 0.1155 (0.0003 -- 1.4140)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:54  lr: 0.000027  min_lr: 0.000001  loss: 1.7955 (1.8830)  loss_scale: 8192.0000 (14518.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7236 (8.2675)  time: 0.8205 (0.5289 -- 2.7074)  data: 0.1367 (0.0006 -- 1.3713)  max mem: 16413
Epoch: [60]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000001  loss: 1.9303 (1.8803)  loss_scale: 8192.0000 (13472.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6515 (8.3406)  time: 0.8997 (0.5337 -- 3.0403)  data: 0.2321 (0.0004 -- 2.5212)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 1.9182 (1.8967)  loss_scale: 8192.0000 (12723.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8573 (8.3919)  time: 0.9202 (0.5238 -- 4.4156)  data: 0.0462 (0.0004 -- 0.4793)  max mem: 16413
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.7920 (1.8896)  loss_scale: 8192.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9027 (8.3136)  time: 0.6898 (0.4945 -- 2.4205)  data: 0.0008 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [60] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.7920 (1.8750)  loss_scale: 8192.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9027 (8.3136)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4954 (0.4954)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3417 (2.3417 -- 2.3417)  data: 2.1400 (2.1400 -- 2.1400)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5566 (0.8501)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (94.9495)  time: 0.4281 (0.2018 -- 2.3417)  data: 0.2111 (0.0004 -- 2.1400)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5889 (0.7831)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (95.7672)  time: 0.2259 (0.1697 -- 0.4019)  data: 0.0175 (0.0001 -- 0.1716)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7392 (0.8454)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (95.4357)  time: 0.2094 (0.1321 -- 0.4019)  data: 0.0171 (0.0001 -- 0.1716)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 76.556 Acc@5 96.266 loss 0.796
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 78.01%
Epoch: [61]  [  0/160]  eta: 0:18:55  lr: 0.000027  min_lr: 0.000001  loss: 2.1753 (2.1753)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0322 (7.0322)  time: 7.0955 (7.0955 -- 7.0955)  data: 6.5312 (6.5312 -- 6.5312)  max mem: 16413
Epoch: [61]  [ 20/160]  eta: 0:02:51  lr: 0.000027  min_lr: 0.000001  loss: 1.8660 (1.8249)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8422 (8.5346)  time: 0.9314 (0.5219 -- 4.5070)  data: 0.1111 (0.0002 -- 2.1777)  max mem: 16413
Epoch: [61]  [ 40/160]  eta: 0:02:10  lr: 0.000027  min_lr: 0.000001  loss: 1.8108 (1.8178)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5873 (8.3966)  time: 0.9486 (0.5284 -- 3.9701)  data: 0.0014 (0.0002 -- 0.0039)  max mem: 16413
[2023-08-29 20:08:43,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:08:43,888] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 20:08:43,888] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:08:43,888] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [61]  [ 60/160]  eta: 0:01:38  lr: 0.000027  min_lr: 0.000001  loss: 1.7677 (1.8319)  loss_scale: 16384.0000 (10072.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0628 (8.4921)  time: 0.7833 (0.5223 -- 3.7948)  data: 0.0022 (0.0002 -- 0.0159)  max mem: 16413
Epoch: [61]  [ 80/160]  eta: 0:01:19  lr: 0.000026  min_lr: 0.000001  loss: 1.7644 (1.8105)  loss_scale: 16384.0000 (11630.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8954 (8.3422)  time: 0.9910 (0.5298 -- 4.2897)  data: 0.0012 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000001  loss: 1.8533 (1.8281)  loss_scale: 16384.0000 (12571.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9888 (8.3679)  time: 0.6975 (0.5297 -- 3.0052)  data: 0.0020 (0.0003 -- 0.0165)  max mem: 16413
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000001  loss: 1.8164 (1.8359)  loss_scale: 16384.0000 (13201.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6180 (8.3437)  time: 0.9368 (0.5252 -- 3.9898)  data: 0.0016 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.8581 (1.8464)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5419 (8.3735)  time: 0.8416 (0.5226 -- 3.9404)  data: 0.0016 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.9183 (1.8565)  loss_scale: 16384.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9681 (8.3385)  time: 0.6536 (0.4971 -- 2.6077)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [61] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.9183 (1.8766)  loss_scale: 16384.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9681 (8.3385)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4703 (0.4703)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3884 (2.3884 -- 2.3884)  data: 2.1673 (2.1673 -- 2.1673)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5616 (0.8261)  acc1: 88.8889 (74.7475)  acc5: 100.0000 (94.9495)  time: 0.4288 (0.1925 -- 2.3884)  data: 0.2120 (0.0008 -- 2.1673)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6340 (0.7718)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (95.7672)  time: 0.2190 (0.1712 -- 0.4054)  data: 0.0107 (0.0001 -- 0.1537)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7286 (0.8344)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.0207)  time: 0.2032 (0.1335 -- 0.4054)  data: 0.0104 (0.0001 -- 0.1537)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 77.801 Acc@5 95.851 loss 0.783
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 78.01%
Epoch: [62]  [  0/160]  eta: 0:20:06  lr: 0.000026  min_lr: 0.000001  loss: 1.8498 (1.8498)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5327 (5.5327)  time: 7.5383 (7.5383 -- 7.5383)  data: 7.0098 (7.0098 -- 7.0098)  max mem: 16413
[2023-08-29 20:10:43,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:10:43,864] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:10:43,865] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:10:43,866] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [62]  [ 20/160]  eta: 0:02:36  lr: 0.000026  min_lr: 0.000001  loss: 1.9761 (1.9357)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7717 (7.8623)  time: 0.7969 (0.5280 -- 2.8194)  data: 0.2511 (0.0006 -- 2.2935)  max mem: 16413
[2023-08-29 20:10:56,828] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9949
[2023-08-29 20:10:56,828] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9949
[2023-08-29 20:10:56,828] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:10:56,828] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:10:56,829] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 20:11:04,394] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9957
[2023-08-29 20:11:04,394] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9957
[2023-08-29 20:11:04,394] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:11:04,394] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:11:04,394] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [62]  [ 40/160]  eta: 0:02:06  lr: 0.000026  min_lr: 0.000001  loss: 1.7501 (1.8716)  loss_scale: 16384.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1003 (8.3011)  time: 0.9802 (0.5267 -- 3.7616)  data: 0.4299 (0.0005 -- 3.2278)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:38  lr: 0.000026  min_lr: 0.000001  loss: 1.9450 (1.9020)  loss_scale: 8192.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0701 (8.5417)  time: 0.8455 (0.5076 -- 3.9940)  data: 0.2950 (0.0004 -- 3.4466)  max mem: 16413
[2023-08-29 20:11:39,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=57, lr=[6.241711314135408e-07, 6.241711314135408e-07, 8.322281752180544e-07, 8.322281752180544e-07, 1.109637566957406e-06, 1.109637566957406e-06, 1.479516755943208e-06, 1.479516755943208e-06, 1.972689007924277e-06, 1.972689007924277e-06, 2.630252010565703e-06, 2.630252010565703e-06, 3.5070026807542706e-06, 3.5070026807542706e-06, 4.676003574339028e-06, 4.676003574339028e-06, 6.234671432452036e-06, 6.234671432452036e-06, 8.312895243269383e-06, 8.312895243269383e-06, 1.1083860324359176e-05, 1.1083860324359176e-05, 1.4778480432478902e-05, 1.4778480432478902e-05, 1.9704640576638537e-05, 1.9704640576638537e-05, 2.6272854102184714e-05, 2.6272854102184714e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 20:11:39,760] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=17.639340506859657, CurrSamplesPerSec=22.374991331252833, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:18  lr: 0.000026  min_lr: 0.000001  loss: 1.8008 (1.8764)  loss_scale: 8192.0000 (14765.8272)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8336 (8.3982)  time: 0.9672 (0.5329 -- 4.9840)  data: 0.2847 (0.0003 -- 2.0932)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000001  loss: 1.8176 (1.8665)  loss_scale: 8192.0000 (13464.0792)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1076 (8.4570)  time: 0.8254 (0.5040 -- 4.3676)  data: 0.0012 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000001  loss: 1.9847 (1.8806)  loss_scale: 8192.0000 (12592.6612)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8884 (8.5572)  time: 0.8368 (0.5252 -- 3.8304)  data: 0.0015 (0.0001 -- 0.0040)  max mem: 16413
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.8034 (1.8780)  loss_scale: 8192.0000 (11968.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7369 (8.4910)  time: 0.7391 (0.5290 -- 3.3403)  data: 0.0018 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.9228 (1.8741)  loss_scale: 8192.0000 (11520.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6187 (8.4952)  time: 0.6648 (0.4968 -- 1.6919)  data: 0.0286 (0.0002 -- 0.2901)  max mem: 16413
Epoch: [62] Total time: 0:02:20 (0.8758 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.9228 (1.8497)  loss_scale: 8192.0000 (11520.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6187 (8.4952)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3843 (0.3843)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4046 (2.4046 -- 2.4046)  data: 2.1802 (2.1802 -- 2.1802)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5523 (0.8288)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (94.9495)  time: 0.4230 (0.1985 -- 2.4046)  data: 0.2036 (0.0008 -- 2.1802)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5845 (0.7730)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.7672)  time: 0.2197 (0.1682 -- 0.2794)  data: 0.0088 (0.0001 -- 0.0739)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6825 (0.8346)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.0207)  time: 0.2041 (0.1335 -- 0.2794)  data: 0.0084 (0.0001 -- 0.0739)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 77.386 Acc@5 96.058 loss 0.781
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 78.01%
Epoch: [63]  [  0/160]  eta: 0:21:45  lr: 0.000026  min_lr: 0.000001  loss: 2.0394 (2.0394)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3444 (5.3444)  time: 8.1569 (8.1569 -- 8.1569)  data: 6.3243 (6.3243 -- 6.3243)  max mem: 16413
[2023-08-29 20:13:04,920] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:13:04,921] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 20:13:04,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:13:04,923] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [63]  [ 20/160]  eta: 0:03:05  lr: 0.000026  min_lr: 0.000001  loss: 1.8903 (1.7897)  loss_scale: 16384.0000 (14043.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5345 (7.7345)  time: 0.9857 (0.5191 -- 4.7486)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:02:07  lr: 0.000026  min_lr: 0.000001  loss: 1.8787 (1.8541)  loss_scale: 16384.0000 (15185.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2724 (7.9829)  time: 0.7820 (0.5254 -- 3.4637)  data: 0.0020 (0.0005 -- 0.0066)  max mem: 16413
Epoch: [63]  [ 60/160]  eta: 0:01:39  lr: 0.000026  min_lr: 0.000001  loss: 1.9084 (1.8570)  loss_scale: 16384.0000 (15578.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7990 (7.9272)  time: 0.8738 (0.5261 -- 4.0727)  data: 0.0016 (0.0004 -- 0.0057)  max mem: 16413
Epoch: [63]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000001  loss: 1.7729 (1.8512)  loss_scale: 16384.0000 (15777.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4345 (8.1867)  time: 0.7906 (0.5346 -- 3.4709)  data: 0.0017 (0.0004 -- 0.0060)  max mem: 16413
Epoch: [63]  [100/160]  eta: 0:00:57  lr: 0.000026  min_lr: 0.000001  loss: 1.8996 (1.8649)  loss_scale: 16384.0000 (15897.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9519 (8.1889)  time: 1.0036 (0.5289 -- 4.1536)  data: 0.0014 (0.0002 -- 0.0037)  max mem: 16413
[2023-08-29 20:14:46,550] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10199
[2023-08-29 20:14:46,550] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10199
[2023-08-29 20:14:46,550] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:14:46,550] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:14:46,551] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [63]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000001  loss: 1.8565 (1.8753)  loss_scale: 16384.0000 (15842.3802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5460 (8.1739)  time: 0.8447 (0.5054 -- 4.8224)  data: 0.0014 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.9502 (1.8814)  loss_scale: 8192.0000 (14757.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1631 (8.2686)  time: 0.8163 (0.5240 -- 3.6873)  data: 0.0017 (0.0003 -- 0.0062)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.8487 (1.8751)  loss_scale: 8192.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0228 (8.4334)  time: 0.6670 (0.4955 -- 2.4114)  data: 0.0009 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [63] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.8487 (1.8800)  loss_scale: 8192.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0228 (8.4334)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4502 (0.4502)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4634 (2.4634 -- 2.4634)  data: 2.1676 (2.1676 -- 2.1676)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5791 (0.8323)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (94.9495)  time: 0.4331 (0.2031 -- 2.4634)  data: 0.1979 (0.0006 -- 2.1676)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5791 (0.7704)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.7672)  time: 0.2155 (0.1721 -- 0.2775)  data: 0.0007 (0.0001 -- 0.0014)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7074 (0.8341)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.0207)  time: 0.1937 (0.1326 -- 0.2394)  data: 0.0004 (0.0001 -- 0.0011)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 77.178 Acc@5 95.851 loss 0.788
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 78.01%
Epoch: [64]  [  0/160]  eta: 0:18:36  lr: 0.000026  min_lr: 0.000001  loss: 1.6968 (1.6968)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9327 (9.9327)  time: 6.9812 (6.9812 -- 6.9812)  data: 6.4329 (6.4329 -- 6.4329)  max mem: 16413
Epoch: [64]  [ 20/160]  eta: 0:02:41  lr: 0.000026  min_lr: 0.000001  loss: 1.8903 (1.9331)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3091 (8.3518)  time: 0.8590 (0.5247 -- 3.8585)  data: 0.3158 (0.0006 -- 3.3342)  max mem: 16413
Epoch: [64]  [ 40/160]  eta: 0:02:01  lr: 0.000026  min_lr: 0.000001  loss: 1.8461 (1.9000)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0958 (8.2329)  time: 0.8737 (0.5243 -- 2.7387)  data: 0.3039 (0.0005 -- 2.1847)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:39  lr: 0.000026  min_lr: 0.000001  loss: 1.7974 (1.8567)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4079 (8.2822)  time: 0.9453 (0.5165 -- 2.6773)  data: 0.3572 (0.0005 -- 2.1367)  max mem: 16413
Epoch: [64]  [ 80/160]  eta: 0:01:14  lr: 0.000026  min_lr: 0.000001  loss: 1.8139 (1.8489)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0997 (8.4651)  time: 0.7402 (0.5290 -- 2.5592)  data: 0.1852 (0.0001 -- 2.0201)  max mem: 16413
[2023-08-29 20:16:45,711] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:16:45,711] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 20:16:45,712] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:16:45,712] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [64]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000001  loss: 1.9495 (1.8680)  loss_scale: 16384.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4085 (8.4381)  time: 0.9548 (0.5225 -- 4.6888)  data: 0.3853 (0.0005 -- 4.1658)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 2.0057 (1.8861)  loss_scale: 16384.0000 (10426.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8608 (8.3615)  time: 0.8102 (0.5366 -- 3.2775)  data: 0.2605 (0.0002 -- 2.7634)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.7335 (1.8699)  loss_scale: 16384.0000 (11271.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9560 (8.1843)  time: 0.8459 (0.5320 -- 3.5414)  data: 0.2959 (0.0004 -- 2.9840)  max mem: 16413
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.8819 (1.8634)  loss_scale: 16384.0000 (11878.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4347 (8.2345)  time: 0.6688 (0.4992 -- 2.5796)  data: 0.0645 (0.0002 -- 0.6876)  max mem: 16413
Epoch: [64] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.8819 (1.8691)  loss_scale: 16384.0000 (11878.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4347 (8.2345)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4426 (0.4426)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3973 (2.3973 -- 2.3973)  data: 2.1949 (2.1949 -- 2.1949)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5412 (0.8306)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (94.9495)  time: 0.4385 (0.2024 -- 2.3973)  data: 0.2248 (0.0006 -- 2.1949)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5782 (0.7719)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2270 (0.1697 -- 0.4895)  data: 0.0219 (0.0001 -- 0.2661)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6862 (0.8314)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.0207)  time: 0.2118 (0.1327 -- 0.4895)  data: 0.0216 (0.0001 -- 0.2661)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 78.631 Acc@5 95.851 loss 0.786
Accuracy of the network on the 482 val images: 78.63%
[2023-08-29 20:17:52,344] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 20:17:52,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 20:17:52,346] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 20:17:52,346] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 20:17:53,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 20:17:53,853] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.63%
Epoch: [65]  [  0/160]  eta: 0:18:21  lr: 0.000026  min_lr: 0.000001  loss: 1.7601 (1.7601)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4016 (9.4016)  time: 6.8831 (6.8831 -- 6.8831)  data: 5.1947 (5.1947 -- 5.1947)  max mem: 16413
Epoch: [65]  [ 20/160]  eta: 0:02:39  lr: 0.000026  min_lr: 0.000001  loss: 1.7209 (1.8072)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4285 (8.1263)  time: 0.8520 (0.5256 -- 4.6363)  data: 0.0158 (0.0006 -- 0.2825)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:03  lr: 0.000026  min_lr: 0.000001  loss: 1.5893 (1.7051)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3661 (7.9875)  time: 0.9088 (0.5291 -- 3.0505)  data: 0.0394 (0.0002 -- 0.7507)  max mem: 16413
[2023-08-29 20:18:50,183] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:18:50,183] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:18:50,184] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:18:50,184] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [65]  [ 60/160]  eta: 0:01:38  lr: 0.000026  min_lr: 0.000001  loss: 1.8184 (1.7226)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1123 (7.9659)  time: 0.9011 (0.5219 -- 2.8873)  data: 0.0020 (0.0003 -- 0.0049)  max mem: 16413
[2023-08-29 20:19:07,556] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10478
[2023-08-29 20:19:07,556] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10478
[2023-08-29 20:19:07,556] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:19:07,556] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:19:07,556] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [65]  [ 80/160]  eta: 0:01:14  lr: 0.000026  min_lr: 0.000001  loss: 1.9906 (1.7734)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7849 (8.0309)  time: 0.7860 (0.5198 -- 2.4761)  data: 0.0191 (0.0005 -- 0.3366)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000001  loss: 1.7359 (1.7736)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5419 (8.0521)  time: 0.9502 (0.5308 -- 3.8147)  data: 0.0270 (0.0009 -- 0.4941)  max mem: 16413
Epoch: [65]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 1.8931 (1.7973)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4793 (8.1010)  time: 0.7867 (0.5286 -- 2.2361)  data: 0.0024 (0.0004 -- 0.0144)  max mem: 16413
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.7139 (1.7957)  loss_scale: 16384.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6070 (8.3291)  time: 0.8336 (0.5419 -- 2.9625)  data: 0.0109 (0.0002 -- 0.1651)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.9664 (1.8084)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2265 (8.3098)  time: 0.7846 (0.4949 -- 3.4271)  data: 0.0009 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [65] Total time: 0:02:22 (0.8901 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.9664 (1.8440)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2265 (8.3098)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.4088 (0.4088)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4910 (2.4910 -- 2.4910)  data: 2.2317 (2.2317 -- 2.2317)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5611 (0.8117)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4331 (0.1935 -- 2.4910)  data: 0.2174 (0.0005 -- 2.2317)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5563 (0.7385)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2220 (0.1693 -- 0.3889)  data: 0.0177 (0.0001 -- 0.1901)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6575 (0.8061)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2063 (0.1324 -- 0.3889)  data: 0.0173 (0.0001 -- 0.1901)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 78.838 Acc@5 96.266 loss 0.763
Accuracy of the network on the 482 val images: 78.84%
[2023-08-29 20:20:24,190] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 20:20:24,191] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 20:20:24,191] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 20:20:24,191] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 20:20:25,575] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 20:20:25,575] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.84%
Epoch: [66]  [  0/160]  eta: 0:16:22  lr: 0.000026  min_lr: 0.000001  loss: 1.2798 (1.2798)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3106 (9.3106)  time: 6.1413 (6.1413 -- 6.1413)  data: 5.1111 (5.1111 -- 5.1111)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:41  lr: 0.000025  min_lr: 0.000001  loss: 1.8593 (1.8179)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0839 (7.8142)  time: 0.9038 (0.5347 -- 2.4587)  data: 0.3564 (0.0007 -- 1.9271)  max mem: 16413
Epoch: [66]  [ 40/160]  eta: 0:01:58  lr: 0.000025  min_lr: 0.000001  loss: 1.7428 (1.7943)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0154 (8.1423)  time: 0.8133 (0.5237 -- 3.0982)  data: 0.2664 (0.0004 -- 2.5717)  max mem: 16413
[2023-08-29 20:21:13,695] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:21:13,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:21:13,696] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:21:13,697] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:21:21,579] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10616
[2023-08-29 20:21:21,580] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:21:21,579] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10616
[2023-08-29 20:21:21,580] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:21:21,580] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [ 60/160]  eta: 0:01:41  lr: 0.000025  min_lr: 0.000001  loss: 1.7518 (1.7951)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3262 (8.0678)  time: 1.0573 (0.5210 -- 4.2204)  data: 0.5066 (0.0003 -- 3.6519)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:15  lr: 0.000025  min_lr: 0.000001  loss: 2.0472 (1.8420)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0949 (8.1740)  time: 0.7653 (0.5275 -- 2.7114)  data: 0.2115 (0.0001 -- 2.1875)  max mem: 16413
Epoch: [66]  [100/160]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000001  loss: 1.7027 (1.8344)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6500 (8.0855)  time: 0.9580 (0.5180 -- 4.0335)  data: 0.4129 (0.0003 -- 3.4904)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000001  loss: 1.9477 (1.8511)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6717 (8.1376)  time: 0.8482 (0.5095 -- 4.3459)  data: 0.3050 (0.0003 -- 3.8435)  max mem: 16413
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 1.8526 (1.8449)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2612 (8.2191)  time: 0.9634 (0.5089 -- 3.9090)  data: 0.4271 (0.0003 -- 3.4023)  max mem: 16413
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.7304 (1.8329)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1343 (8.2318)  time: 0.5864 (0.4955 -- 1.8037)  data: 0.0641 (0.0001 -- 1.2708)  max mem: 16413
Epoch: [66] Total time: 0:02:23 (0.8975 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.7304 (1.8644)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1343 (8.2318)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4050 (0.4050)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3077 (2.3077 -- 2.3077)  data: 2.0846 (2.0846 -- 2.0846)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5877 (0.8296)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (94.9495)  time: 0.4195 (0.1946 -- 2.3077)  data: 0.2108 (0.0008 -- 2.0846)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5894 (0.7552)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2201 (0.1690 -- 0.4266)  data: 0.0190 (0.0001 -- 0.2208)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7191 (0.8184)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (94.6058)  time: 0.2058 (0.1320 -- 0.4266)  data: 0.0186 (0.0001 -- 0.2208)  max mem: 16413
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 78.838 Acc@5 95.643 loss 0.775
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 78.84%
Epoch: [67]  [  0/160]  eta: 0:17:56  lr: 0.000025  min_lr: 0.000001  loss: 1.9717 (1.9717)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8471 (4.8471)  time: 6.7298 (6.7298 -- 6.7298)  data: 5.9666 (5.9666 -- 5.9666)  max mem: 16413
Epoch: [67]  [ 20/160]  eta: 0:02:44  lr: 0.000025  min_lr: 0.000001  loss: 1.7868 (1.9002)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9717 (8.0061)  time: 0.8956 (0.5270 -- 4.0158)  data: 0.3410 (0.0006 -- 3.4825)  max mem: 16413
[2023-08-29 20:23:27,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:23:27,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:23:27,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:23:27,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [67]  [ 40/160]  eta: 0:02:08  lr: 0.000025  min_lr: 0.000001  loss: 1.7132 (1.8430)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8853 (7.8806)  time: 0.9657 (0.5218 -- 3.7302)  data: 0.4158 (0.0003 -- 3.1621)  max mem: 16413
[2023-08-29 20:23:50,748] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10775
[2023-08-29 20:23:50,748] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10775
[2023-08-29 20:23:50,748] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:23:50,748] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:23:50,749] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [67]  [ 60/160]  eta: 0:01:37  lr: 0.000025  min_lr: 0.000001  loss: 1.9024 (1.8362)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5838 (7.8213)  time: 0.7703 (0.5245 -- 3.3888)  data: 0.2217 (0.0003 -- 2.8436)  max mem: 16413
Epoch: [67]  [ 80/160]  eta: 0:01:17  lr: 0.000025  min_lr: 0.000001  loss: 1.8799 (1.8470)  loss_scale: 16384.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8899 (8.1137)  time: 0.9804 (0.5196 -- 4.3754)  data: 0.4349 (0.0003 -- 3.8685)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000001  loss: 1.8822 (1.8595)  loss_scale: 16384.0000 (21250.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6779 (8.3086)  time: 0.8064 (0.5243 -- 3.1467)  data: 0.2584 (0.0004 -- 2.6213)  max mem: 16413
Epoch: [67]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000001  loss: 1.7182 (1.8431)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2763 (8.2788)  time: 0.9200 (0.5223 -- 3.9034)  data: 0.3749 (0.0003 -- 3.3794)  max mem: 16413
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 1.8279 (1.8416)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6235 (8.3210)  time: 0.7778 (0.5195 -- 3.5206)  data: 0.2346 (0.0002 -- 2.9618)  max mem: 16413
[2023-08-29 20:25:13,570] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10871
[2023-08-29 20:25:13,570] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10871
[2023-08-29 20:25:13,570] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:25:13,570] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:25:13,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.9472 (1.8473)  loss_scale: 16384.0000 (18995.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9647 (8.3461)  time: 0.6640 (0.4958 -- 1.6569)  data: 0.1022 (0.0002 -- 1.0887)  max mem: 16413
Epoch: [67] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.9472 (1.8468)  loss_scale: 16384.0000 (18995.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9647 (8.3461)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4248 (0.4248)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4357 (2.4357 -- 2.4357)  data: 2.2301 (2.2301 -- 2.2301)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5242 (0.8218)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (94.9495)  time: 0.4183 (0.1976 -- 2.4357)  data: 0.2037 (0.0004 -- 2.2301)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5362 (0.7484)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2220 (0.1687 -- 0.6018)  data: 0.0216 (0.0001 -- 0.4193)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6561 (0.8176)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.0207)  time: 0.2054 (0.1327 -- 0.6018)  data: 0.0214 (0.0001 -- 0.4193)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 78.838 Acc@5 95.643 loss 0.765
Accuracy of the network on the 482 val images: 78.84%
[2023-08-29 20:25:26,761] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 20:25:26,763] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 20:25:26,763] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 20:25:26,763] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 20:25:27,899] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 20:25:27,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.84%
Epoch: [68]  [  0/160]  eta: 0:21:37  lr: 0.000025  min_lr: 0.000001  loss: 2.1159 (2.1159)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6835 (6.6835)  time: 8.1122 (8.1122 -- 8.1122)  data: 7.6025 (7.6025 -- 7.6025)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:45  lr: 0.000025  min_lr: 0.000001  loss: 1.6278 (1.7523)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9666 (8.1485)  time: 0.8368 (0.5342 -- 3.4273)  data: 0.2403 (0.0003 -- 2.8878)  max mem: 16413
Epoch: [68]  [ 40/160]  eta: 0:02:09  lr: 0.000025  min_lr: 0.000001  loss: 1.7447 (1.7641)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1209 (8.6291)  time: 0.9767 (0.5337 -- 4.4630)  data: 0.0756 (0.0004 -- 1.2340)  max mem: 16413
Epoch: [68]  [ 60/160]  eta: 0:01:38  lr: 0.000025  min_lr: 0.000001  loss: 1.7921 (1.7620)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3796 (8.2859)  time: 0.7963 (0.5124 -- 4.4500)  data: 0.0016 (0.0004 -- 0.0077)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:16  lr: 0.000025  min_lr: 0.000001  loss: 1.9556 (1.8205)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9964 (8.3868)  time: 0.8725 (0.5058 -- 3.2569)  data: 0.0016 (0.0005 -- 0.0043)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000001  loss: 1.8518 (1.8259)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0106 (8.7897)  time: 0.8375 (0.5264 -- 2.2359)  data: 0.0813 (0.0002 -- 0.9786)  max mem: 16413
[2023-08-29 20:27:17,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=62, lr=[5.917114266174307e-07, 5.917114266174307e-07, 7.889485688232409e-07, 7.889485688232409e-07, 1.0519314250976546e-06, 1.0519314250976546e-06, 1.4025752334635394e-06, 1.4025752334635394e-06, 1.8701003112847194e-06, 1.8701003112847194e-06, 2.493467081712959e-06, 2.493467081712959e-06, 3.3246227756172787e-06, 3.3246227756172787e-06, 4.432830367489705e-06, 4.432830367489705e-06, 5.910440489986273e-06, 5.910440489986273e-06, 7.880587319981698e-06, 7.880587319981698e-06, 1.0507449759975597e-05, 1.0507449759975597e-05, 1.4009933013300795e-05, 1.4009933013300795e-05, 1.867991068440106e-05, 1.867991068440106e-05, 2.4906547579201415e-05, 2.4906547579201415e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 20:27:17,788] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=17.639536063688436, CurrSamplesPerSec=21.523609553210495, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-08-29 20:27:18,836] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:27:18,837] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 20:27:18,842] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:27:18,843] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [68]  [120/160]  eta: 0:00:36  lr: 0.000025  min_lr: 0.000001  loss: 1.8945 (1.8355)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3618 (8.6283)  time: 0.8212 (0.5481 -- 1.9641)  data: 0.1061 (0.0003 -- 0.8784)  max mem: 16413
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 1.9813 (1.8658)  loss_scale: 16384.0000 (9412.0851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4360 (8.7189)  time: 0.9355 (0.5333 -- 3.7532)  data: 0.0698 (0.0005 -- 1.3547)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.8448 (1.8630)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6871 (8.6582)  time: 0.6253 (0.4945 -- 1.8083)  data: 0.0011 (0.0002 -- 0.0059)  max mem: 16413
Epoch: [68] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.8448 (1.8634)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6871 (8.6582)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.4631 (0.4631)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1614 (2.1614 -- 2.1614)  data: 1.9466 (1.9466 -- 1.9466)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5472 (0.8327)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (94.9495)  time: 0.4079 (0.1879 -- 2.1614)  data: 0.1920 (0.0007 -- 1.9466)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5954 (0.7538)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (95.7672)  time: 0.2256 (0.1723 -- 0.3931)  data: 0.0135 (0.0001 -- 0.1561)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6723 (0.8176)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (94.6058)  time: 0.2101 (0.1327 -- 0.3931)  data: 0.0133 (0.0001 -- 0.1561)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 79.253 Acc@5 95.228 loss 0.777
Accuracy of the network on the 482 val images: 79.25%
[2023-08-29 20:27:57,412] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 20:27:57,414] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 20:27:57,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 20:27:57,414] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 20:27:58,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 20:27:58,579] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.25%
Epoch: [69]  [  0/160]  eta: 0:19:39  lr: 0.000025  min_lr: 0.000001  loss: 2.4043 (2.4043)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9503 (6.9503)  time: 7.3713 (7.3713 -- 7.3713)  data: 5.8137 (5.8137 -- 5.8137)  max mem: 16413
Epoch: [69]  [ 20/160]  eta: 0:02:50  lr: 0.000025  min_lr: 0.000001  loss: 1.8932 (1.9099)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0661 (8.2356)  time: 0.9071 (0.5390 -- 3.5346)  data: 0.2288 (0.0009 -- 3.0004)  max mem: 16413
Epoch: [69]  [ 40/160]  eta: 0:02:10  lr: 0.000025  min_lr: 0.000001  loss: 1.8007 (1.8795)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1498 (8.4272)  time: 0.9547 (0.5360 -- 3.5056)  data: 0.0756 (0.0003 -- 1.0718)  max mem: 16413
Epoch: [69]  [ 60/160]  eta: 0:01:37  lr: 0.000025  min_lr: 0.000001  loss: 2.0462 (1.9227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2250 (8.3347)  time: 0.7423 (0.5299 -- 2.5877)  data: 0.0016 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:18  lr: 0.000025  min_lr: 0.000001  loss: 1.9126 (1.9177)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1145 (8.5746)  time: 0.9921 (0.5248 -- 4.6620)  data: 0.0053 (0.0004 -- 0.0774)  max mem: 16413
[2023-08-29 20:29:23,551] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:29:23,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:29:23,554] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:29:23,593] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:29:24,689] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11130
[2023-08-29 20:29:24,689] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11130
[2023-08-29 20:29:24,690] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:29:24,690] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:29:24,690] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [69]  [100/160]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000001  loss: 1.9422 (1.9075)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9376 (8.5920)  time: 0.7456 (0.5346 -- 1.8644)  data: 0.0022 (0.0003 -- 0.0077)  max mem: 16413
Epoch: [69]  [120/160]  eta: 0:00:36  lr: 0.000025  min_lr: 0.000001  loss: 1.8562 (1.9147)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4412 (8.5509)  time: 0.8794 (0.5291 -- 3.2485)  data: 0.0023 (0.0005 -- 0.0213)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 1.8285 (1.9050)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2577 (8.5198)  time: 0.8505 (0.5352 -- 3.6899)  data: 0.0018 (0.0001 -- 0.0058)  max mem: 16413
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.7641 (1.8867)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2088 (8.4274)  time: 0.6984 (0.4943 -- 2.4988)  data: 0.0008 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [69] Total time: 0:02:22 (0.8893 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.7641 (1.8669)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2088 (8.4274)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4180 (0.4180)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3012 (2.3012 -- 2.3012)  data: 2.0849 (2.0849 -- 2.0849)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5152 (0.8046)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (94.9495)  time: 0.4058 (0.1940 -- 2.3012)  data: 0.1905 (0.0007 -- 2.0849)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5522 (0.7276)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (95.7672)  time: 0.2203 (0.1714 -- 0.3173)  data: 0.0115 (0.0001 -- 0.1452)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6494 (0.7925)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.0207)  time: 0.2047 (0.1334 -- 0.3173)  data: 0.0113 (0.0001 -- 0.1452)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 80.290 Acc@5 95.851 loss 0.754
Accuracy of the network on the 482 val images: 80.29%
[2023-08-29 20:30:28,610] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 20:30:28,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 20:30:28,612] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 20:30:28,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 20:30:30,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 20:30:30,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.29%
Epoch: [70]  [  0/160]  eta: 0:17:10  lr: 0.000025  min_lr: 0.000001  loss: 1.8041 (1.8041)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2694 (8.2694)  time: 6.4388 (6.4388 -- 6.4388)  data: 5.8899 (5.8899 -- 5.8899)  max mem: 16413
Epoch: [70]  [ 20/160]  eta: 0:02:50  lr: 0.000025  min_lr: 0.000001  loss: 1.8124 (1.8429)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0029 (9.0330)  time: 0.9566 (0.5269 -- 3.9931)  data: 0.3586 (0.0005 -- 3.4638)  max mem: 16413
Epoch: [70]  [ 40/160]  eta: 0:01:59  lr: 0.000025  min_lr: 0.000001  loss: 1.6737 (1.8016)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7994 (8.9443)  time: 0.7556 (0.5184 -- 2.8570)  data: 0.2034 (0.0002 -- 2.3076)  max mem: 16413
[2023-08-29 20:31:29,745] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:31:29,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:31:29,745] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:31:29,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [70]  [ 60/160]  eta: 0:01:38  lr: 0.000025  min_lr: 0.000001  loss: 1.8090 (1.7897)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2148 (8.7729)  time: 0.9700 (0.5156 -- 3.6576)  data: 0.4273 (0.0004 -- 3.1247)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:14  lr: 0.000025  min_lr: 0.000001  loss: 1.9213 (1.8369)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8318 (8.9184)  time: 0.7850 (0.5255 -- 2.1501)  data: 0.0949 (0.0003 -- 1.3429)  max mem: 16413
[2023-08-29 20:31:51,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11286
[2023-08-29 20:31:51,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11286
[2023-08-29 20:31:51,204] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:31:51,204] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:31:51,204] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [70]  [100/160]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000001  loss: 2.0250 (1.8706)  loss_scale: 16384.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2709 (8.7409)  time: 0.9308 (0.5291 -- 3.0955)  data: 0.1331 (0.0004 -- 1.9709)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000001  loss: 1.8406 (1.8733)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7357 (8.5824)  time: 0.7981 (0.5385 -- 2.6819)  data: 0.2209 (0.0005 -- 2.1680)  max mem: 16413
Epoch: [70]  [140/160]  eta: 0:00:17  lr: 0.000024  min_lr: 0.000001  loss: 1.8628 (1.8764)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1010 (8.5946)  time: 0.8022 (0.5264 -- 3.2420)  data: 0.0688 (0.0003 -- 0.9900)  max mem: 16413
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.9268 (1.8807)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4901 (8.6972)  time: 0.7189 (0.4972 -- 1.7295)  data: 0.1762 (0.0002 -- 1.2060)  max mem: 16413
Epoch: [70] Total time: 0:02:20 (0.8767 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.9268 (1.8547)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4901 (8.6972)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3526 (0.3526)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3767 (2.3767 -- 2.3767)  data: 2.1255 (2.1255 -- 2.1255)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5065 (0.7959)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4149 (0.1985 -- 2.3767)  data: 0.1996 (0.0007 -- 2.1255)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5583 (0.7307)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2204 (0.1685 -- 0.3773)  data: 0.0159 (0.0001 -- 0.1784)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6409 (0.7959)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.0207)  time: 0.2047 (0.1331 -- 0.3773)  data: 0.0144 (0.0001 -- 0.1784)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 79.876 Acc@5 95.851 loss 0.746
Accuracy of the network on the 482 val images: 79.88%
Max accuracy: 80.29%
Epoch: [71]  [  0/160]  eta: 0:18:38  lr: 0.000024  min_lr: 0.000001  loss: 1.3853 (1.3853)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6157 (8.6157)  time: 6.9918 (6.9918 -- 6.9918)  data: 6.3143 (6.3143 -- 6.3143)  max mem: 16413
Epoch: [71]  [ 20/160]  eta: 0:02:48  lr: 0.000024  min_lr: 0.000001  loss: 1.8435 (1.7688)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8551 (8.7786)  time: 0.9167 (0.5289 -- 3.3936)  data: 0.3547 (0.0006 -- 2.8561)  max mem: 16413
Epoch: [71]  [ 40/160]  eta: 0:02:07  lr: 0.000024  min_lr: 0.000001  loss: 1.7381 (1.7599)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6542 (8.1962)  time: 0.9117 (0.5118 -- 4.4736)  data: 0.0613 (0.0003 -- 1.1951)  max mem: 16413
[2023-08-29 20:33:52,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:33:52,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:33:52,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:33:52,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [71]  [ 60/160]  eta: 0:01:40  lr: 0.000024  min_lr: 0.000001  loss: 1.9770 (1.8041)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7708 (8.4320)  time: 0.8947 (0.5246 -- 5.1761)  data: 0.0016 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:15  lr: 0.000024  min_lr: 0.000001  loss: 1.5678 (1.7697)  loss_scale: 32768.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5840 (8.1608)  time: 0.7492 (0.5214 -- 2.5445)  data: 0.1797 (0.0004 -- 1.9981)  max mem: 16413
[2023-08-29 20:34:30,721] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11459
[2023-08-29 20:34:30,721] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11459
[2023-08-29 20:34:30,721] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:34:30,721] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:34:30,721] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [71]  [100/160]  eta: 0:00:55  lr: 0.000024  min_lr: 0.000001  loss: 1.9524 (1.7822)  loss_scale: 32768.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2941 (8.1546)  time: 0.8288 (0.5354 -- 2.5045)  data: 0.2664 (0.0006 -- 1.9605)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000001  loss: 1.9008 (1.8025)  loss_scale: 16384.0000 (22341.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7825 (8.0788)  time: 1.0511 (0.5325 -- 4.2120)  data: 0.5006 (0.0002 -- 3.6779)  max mem: 16413
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 2.0722 (1.8320)  loss_scale: 16384.0000 (21496.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5179 (8.1311)  time: 0.7498 (0.5299 -- 3.4521)  data: 0.1981 (0.0005 -- 2.9414)  max mem: 16413
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.8724 (1.8451)  loss_scale: 16384.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2591 (8.1429)  time: 0.6956 (0.4942 -- 3.6344)  data: 0.1713 (0.0002 -- 3.1210)  max mem: 16413
Epoch: [71] Total time: 0:02:22 (0.8902 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.8724 (1.8262)  loss_scale: 16384.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2591 (8.1429)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4035 (0.4035)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2885 (2.2885 -- 2.2885)  data: 2.0564 (2.0564 -- 2.0564)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5362 (0.8088)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4129 (0.1887 -- 2.2885)  data: 0.1950 (0.0008 -- 2.0564)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5426 (0.7397)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2254 (0.1689 -- 0.3648)  data: 0.0178 (0.0001 -- 0.1428)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6787 (0.7968)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.0207)  time: 0.2106 (0.1329 -- 0.3648)  data: 0.0174 (0.0001 -- 0.1428)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 79.253 Acc@5 95.851 loss 0.751
Accuracy of the network on the 482 val images: 79.25%
Max accuracy: 80.29%
Epoch: [72]  [  0/160]  eta: 0:17:41  lr: 0.000024  min_lr: 0.000001  loss: 1.4485 (1.4485)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4010 (6.4010)  time: 6.6329 (6.6329 -- 6.6329)  data: 5.1671 (5.1671 -- 5.1671)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:02:37  lr: 0.000024  min_lr: 0.000001  loss: 1.7825 (1.7790)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1947 (8.7296)  time: 0.8466 (0.5296 -- 2.7977)  data: 0.2958 (0.0009 -- 2.2851)  max mem: 16413
Epoch: [72]  [ 40/160]  eta: 0:02:01  lr: 0.000024  min_lr: 0.000001  loss: 1.8455 (1.8076)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0599 (9.2135)  time: 0.8921 (0.5373 -- 3.1773)  data: 0.2966 (0.0003 -- 2.6580)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:36  lr: 0.000024  min_lr: 0.000001  loss: 1.7005 (1.7838)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4789 (9.1335)  time: 0.8817 (0.5346 -- 3.3858)  data: 0.3295 (0.0005 -- 2.8364)  max mem: 16413
[2023-08-29 20:36:35,961] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:36:35,961] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:36:35,961] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:36:35,961] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:36:40,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11593
[2023-08-29 20:36:40,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:36:40,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11593
[2023-08-29 20:36:40,985] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:36:40,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [72]  [ 80/160]  eta: 0:01:15  lr: 0.000024  min_lr: 0.000001  loss: 1.8207 (1.8019)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9587 (8.8739)  time: 0.8724 (0.5266 -- 2.6321)  data: 0.1634 (0.0003 -- 1.7909)  max mem: 16413
Epoch: [72]  [100/160]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000001  loss: 1.6937 (1.7987)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4134 (8.5812)  time: 0.8989 (0.5266 -- 4.1842)  data: 0.3538 (0.0004 -- 3.6684)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000001  loss: 1.8296 (1.7943)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2410 (8.4132)  time: 0.8172 (0.5348 -- 4.0579)  data: 0.2645 (0.0003 -- 3.5065)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 1.8542 (1.8043)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0830 (8.4209)  time: 0.9601 (0.5210 -- 5.3322)  data: 0.4130 (0.0005 -- 4.8141)  max mem: 16413
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.7762 (1.8022)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4290 (8.4928)  time: 0.5977 (0.4954 -- 1.3492)  data: 0.0777 (0.0002 -- 0.8572)  max mem: 16413
Epoch: [72] Total time: 0:02:21 (0.8843 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.7762 (1.8228)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4290 (8.4928)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4554 (0.4554)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4194 (2.4194 -- 2.4194)  data: 2.1821 (2.1821 -- 2.1821)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5423 (0.8202)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4204 (0.2007 -- 2.4194)  data: 0.1995 (0.0007 -- 2.1821)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5551 (0.7510)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2162 (0.1710 -- 0.2969)  data: 0.0062 (0.0001 -- 0.1083)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6572 (0.8089)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.0207)  time: 0.1967 (0.1329 -- 0.2969)  data: 0.0059 (0.0001 -- 0.1083)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 79.046 Acc@5 95.851 loss 0.763
Accuracy of the network on the 482 val images: 79.05%
Max accuracy: 80.29%
Epoch: [73]  [  0/160]  eta: 0:20:58  lr: 0.000024  min_lr: 0.000001  loss: 2.3011 (2.3011)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2630 (8.2630)  time: 7.8675 (7.8675 -- 7.8675)  data: 6.2261 (6.2261 -- 6.2261)  max mem: 16413
Epoch: [73]  [ 20/160]  eta: 0:02:52  lr: 0.000024  min_lr: 0.000001  loss: 1.9269 (1.8552)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5704 (7.9539)  time: 0.9013 (0.5249 -- 3.6073)  data: 0.0228 (0.0003 -- 0.4277)  max mem: 16413
Epoch: [73]  [ 40/160]  eta: 0:02:10  lr: 0.000024  min_lr: 0.000001  loss: 1.7145 (1.7777)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6390 (7.9167)  time: 0.9326 (0.5147 -- 3.6070)  data: 0.0013 (0.0003 -- 0.0052)  max mem: 16413
[2023-08-29 20:38:43,658] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:38:43,658] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:38:43,658] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:38:43,658] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [73]  [ 60/160]  eta: 0:01:40  lr: 0.000024  min_lr: 0.000001  loss: 1.7786 (1.8044)  loss_scale: 32768.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5668 (8.3174)  time: 0.8500 (0.5257 -- 4.5446)  data: 0.0012 (0.0003 -- 0.0023)  max mem: 16413
[2023-08-29 20:39:08,387] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11749
[2023-08-29 20:39:08,387] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11749
[2023-08-29 20:39:08,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:39:08,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:39:08,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [73]  [ 80/160]  eta: 0:01:17  lr: 0.000024  min_lr: 0.000001  loss: 1.8353 (1.7989)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1397 (8.3283)  time: 0.8590 (0.5050 -- 4.6567)  data: 0.0015 (0.0003 -- 0.0086)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000001  loss: 1.9058 (1.8103)  loss_scale: 16384.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4901 (8.1048)  time: 0.7896 (0.5341 -- 3.0864)  data: 0.0015 (0.0007 -- 0.0044)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000001  loss: 1.7432 (1.7996)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9213 (8.0551)  time: 0.9178 (0.5303 -- 3.2627)  data: 0.0189 (0.0003 -- 0.3489)  max mem: 16413
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 1.7231 (1.8007)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1083 (8.1485)  time: 0.9555 (0.5204 -- 4.5477)  data: 0.0544 (0.0001 -- 1.0457)  max mem: 16413
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.7316 (1.8028)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6126 (8.1212)  time: 0.6334 (0.4958 -- 2.8840)  data: 0.0008 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [73] Total time: 0:02:24 (0.9008 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.7316 (1.8412)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6126 (8.1212)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3519 (0.3519)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4208 (2.4208 -- 2.4208)  data: 2.2176 (2.2176 -- 2.2176)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4939 (0.7870)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (95.9596)  time: 0.4242 (0.2070 -- 2.4208)  data: 0.2037 (0.0006 -- 2.2176)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5832 (0.7255)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2238 (0.1697 -- 0.4459)  data: 0.0136 (0.0001 -- 0.2455)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7002 (0.7801)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.0207)  time: 0.2078 (0.1335 -- 0.4459)  data: 0.0133 (0.0001 -- 0.2455)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 82.365 Acc@5 95.851 loss 0.733
Accuracy of the network on the 482 val images: 82.37%
[2023-08-29 20:40:29,898] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 20:40:29,899] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 20:40:29,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 20:40:29,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 20:40:31,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 20:40:31,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [74]  [  0/160]  eta: 0:19:30  lr: 0.000024  min_lr: 0.000001  loss: 1.8203 (1.8203)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9315 (5.9315)  time: 7.3174 (7.3174 -- 7.3174)  data: 6.7545 (6.7545 -- 6.7545)  max mem: 16413
Epoch: [74]  [ 20/160]  eta: 0:02:43  lr: 0.000024  min_lr: 0.000001  loss: 1.8297 (1.7919)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1548 (8.7643)  time: 0.8626 (0.5279 -- 3.5897)  data: 0.3089 (0.0008 -- 3.0200)  max mem: 16413
[2023-08-29 20:41:10,833] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:41:10,833] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:41:10,833] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:41:10,833] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [74]  [ 40/160]  eta: 0:02:00  lr: 0.000024  min_lr: 0.000001  loss: 1.7254 (1.7627)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2166 (8.5252)  time: 0.8344 (0.5337 -- 3.5883)  data: 0.2488 (0.0002 -- 3.0617)  max mem: 16413
[2023-08-29 20:41:16,218] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11883
[2023-08-29 20:41:16,218] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11883
[2023-08-29 20:41:16,218] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:41:16,218] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:41:16,218] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [74]  [ 60/160]  eta: 0:01:36  lr: 0.000024  min_lr: 0.000001  loss: 1.9010 (1.8027)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3135 (8.5163)  time: 0.8817 (0.5266 -- 4.7025)  data: 0.1025 (0.0006 -- 2.0193)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:15  lr: 0.000024  min_lr: 0.000001  loss: 2.0114 (1.8310)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8877 (8.4716)  time: 0.8721 (0.5367 -- 2.7464)  data: 0.0484 (0.0009 -- 0.9300)  max mem: 16413
Epoch: [74]  [100/160]  eta: 0:00:54  lr: 0.000024  min_lr: 0.000001  loss: 1.8904 (1.8383)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2104 (8.4576)  time: 0.7771 (0.5369 -- 2.9741)  data: 0.0093 (0.0003 -- 0.1070)  max mem: 16413
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000001  loss: 1.9560 (1.8493)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6366 (8.3246)  time: 0.8863 (0.5290 -- 3.3739)  data: 0.1028 (0.0006 -- 1.8946)  max mem: 16413
[2023-08-29 20:42:23,148] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11964
[2023-08-29 20:42:23,148] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:42:23,148] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-29 20:42:23,148] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11964
[2023-08-29 20:42:23,148] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [74]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 1.9217 (1.8639)  loss_scale: 8192.0000 (15977.3050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8394 (8.3371)  time: 0.8639 (0.5274 -- 4.6634)  data: 0.0642 (0.0004 -- 1.2581)  max mem: 16413
[2023-08-29 20:42:51,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=69, lr=[5.572116050490015e-07, 5.572116050490015e-07, 7.42948806732002e-07, 7.42948806732002e-07, 9.905984089760026e-07, 9.905984089760026e-07, 1.3207978786346702e-06, 1.3207978786346702e-06, 1.7610638381795603e-06, 1.7610638381795603e-06, 2.348085117572747e-06, 2.348085117572747e-06, 3.1307801567636626e-06, 3.1307801567636626e-06, 4.17437354235155e-06, 4.17437354235155e-06, 5.565831389802067e-06, 5.565831389802067e-06, 7.4211085197360895e-06, 7.4211085197360895e-06, 9.89481135964812e-06, 9.89481135964812e-06, 1.3193081812864159e-05, 1.3193081812864159e-05, 1.7590775750485546e-05, 1.7590775750485546e-05, 2.345436766731406e-05, 2.345436766731406e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 20:42:51,790] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=17.616543528428583, CurrSamplesPerSec=24.544959255628868, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.6423 (1.8513)  loss_scale: 8192.0000 (15052.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7728 (8.3970)  time: 0.7046 (0.4970 -- 2.8029)  data: 0.1264 (0.0002 -- 2.2580)  max mem: 16413
Epoch: [74] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.6423 (1.8537)  loss_scale: 8192.0000 (15052.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7728 (8.3970)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3667 (0.3667)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4453 (2.4453 -- 2.4453)  data: 2.1889 (2.1889 -- 2.1889)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5195 (0.7962)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (94.9495)  time: 0.4261 (0.1883 -- 2.4453)  data: 0.2108 (0.0004 -- 2.1889)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5590 (0.7250)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (95.2381)  time: 0.2197 (0.1699 -- 0.3503)  data: 0.0154 (0.0001 -- 0.1202)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6390 (0.7869)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (94.1909)  time: 0.2059 (0.1329 -- 0.3503)  data: 0.0151 (0.0001 -- 0.1202)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 81.328 Acc@5 95.436 loss 0.740
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 82.37%
Epoch: [75]  [  0/160]  eta: 0:21:54  lr: 0.000023  min_lr: 0.000001  loss: 2.0150 (2.0150)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5586 (4.5586)  time: 8.2172 (8.2172 -- 8.2172)  data: 4.9667 (4.9667 -- 4.9667)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:02:51  lr: 0.000023  min_lr: 0.000001  loss: 1.7901 (1.7916)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2584 (8.2954)  time: 0.8754 (0.5169 -- 3.4707)  data: 0.0262 (0.0004 -- 0.4968)  max mem: 16413
Epoch: [75]  [ 40/160]  eta: 0:02:06  lr: 0.000023  min_lr: 0.000001  loss: 1.9692 (1.8496)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0048 (8.3788)  time: 0.8769 (0.5130 -- 3.9362)  data: 0.0578 (0.0002 -- 0.6302)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000001  loss: 1.8812 (1.8411)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5123 (8.4446)  time: 0.8689 (0.5287 -- 3.1106)  data: 0.0013 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [75]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000001  loss: 1.8138 (1.8338)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0483 (8.5189)  time: 0.9512 (0.5209 -- 3.8446)  data: 0.0015 (0.0003 -- 0.0075)  max mem: 16413
[2023-08-29 20:44:30,401] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:44:30,402] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 20:44:30,402] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:44:30,403] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [75]  [100/160]  eta: 0:00:57  lr: 0.000023  min_lr: 0.000001  loss: 1.7496 (1.8076)  loss_scale: 8192.0000 (8840.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8608 (8.4855)  time: 0.8499 (0.5152 -- 4.5632)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.8780 (1.8070)  loss_scale: 16384.0000 (10087.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8319 (8.4317)  time: 0.8207 (0.5367 -- 2.8452)  data: 0.0041 (0.0009 -- 0.0169)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.7983 (1.8075)  loss_scale: 16384.0000 (10980.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3309 (8.5145)  time: 0.8822 (0.5186 -- 4.5987)  data: 0.0021 (0.0004 -- 0.0074)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7186 (1.7996)  loss_scale: 16384.0000 (11622.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6496 (8.5169)  time: 0.6789 (0.4940 -- 2.6517)  data: 0.0010 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [75] Total time: 0:02:23 (0.8987 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7186 (1.8087)  loss_scale: 16384.0000 (11622.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6496 (8.5169)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3325 (0.3325)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4201 (2.4201 -- 2.4201)  data: 2.1418 (2.1418 -- 2.1418)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5169 (0.7817)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (95.9596)  time: 0.4340 (0.1980 -- 2.4201)  data: 0.2137 (0.0006 -- 2.1418)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5499 (0.7161)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2238 (0.1691 -- 0.3869)  data: 0.0170 (0.0001 -- 0.1745)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6183 (0.7672)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.0207)  time: 0.2077 (0.1327 -- 0.3869)  data: 0.0154 (0.0001 -- 0.1745)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 82.365 Acc@5 95.851 loss 0.717
Accuracy of the network on the 482 val images: 82.37%
[2023-08-29 20:45:31,309] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 20:45:31,311] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 20:45:31,311] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 20:45:31,311] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 20:45:32,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 20:45:32,444] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [76]  [  0/160]  eta: 0:17:30  lr: 0.000023  min_lr: 0.000001  loss: 1.9926 (1.9926)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.0952 (14.0952)  time: 6.5675 (6.5675 -- 6.5675)  data: 3.8048 (3.8048 -- 3.8048)  max mem: 16413
Epoch: [76]  [ 20/160]  eta: 0:02:48  lr: 0.000023  min_lr: 0.000001  loss: 1.9344 (1.8818)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7333 (9.1412)  time: 0.9331 (0.5298 -- 3.4399)  data: 0.3393 (0.0003 -- 2.8652)  max mem: 16413
Epoch: [76]  [ 40/160]  eta: 0:02:02  lr: 0.000023  min_lr: 0.000001  loss: 1.6289 (1.7390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2965 (9.0638)  time: 0.8249 (0.5312 -- 3.3955)  data: 0.1988 (0.0002 -- 2.8655)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 1.7953 (1.7852)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1644 (8.7695)  time: 0.8546 (0.5287 -- 2.7686)  data: 0.3016 (0.0004 -- 2.2319)  max mem: 16413
[2023-08-29 20:46:31,815] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:46:31,815] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:46:31,815] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:46:31,815] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:46:38,115] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12229
[2023-08-29 20:46:38,116] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:46:38,116] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12229
[2023-08-29 20:46:38,116] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 20:46:38,116] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [76]  [ 80/160]  eta: 0:01:13  lr: 0.000023  min_lr: 0.000001  loss: 1.8909 (1.8156)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3333 (8.4408)  time: 0.8005 (0.5278 -- 2.0697)  data: 0.1724 (0.0005 -- 1.5391)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.8949 (1.8280)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1491 (8.4845)  time: 0.9282 (0.5274 -- 2.1311)  data: 0.1665 (0.0007 -- 1.5595)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.9440 (1.8433)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9192 (8.7331)  time: 1.0030 (0.5191 -- 4.3532)  data: 0.1120 (0.0004 -- 1.3966)  max mem: 16413
Epoch: [76]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9877 (1.8469)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3627 (8.7540)  time: 0.7169 (0.5351 -- 2.5425)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7700 (1.8359)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7218 (8.7698)  time: 0.6761 (0.4977 -- 2.1272)  data: 0.0162 (0.0002 -- 0.3067)  max mem: 16413
Epoch: [76] Total time: 0:02:20 (0.8800 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7700 (1.8337)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7218 (8.7698)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3440 (0.3440)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3836 (2.3836 -- 2.3836)  data: 2.1668 (2.1668 -- 2.1668)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5013 (0.7891)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4182 (0.1981 -- 2.3836)  data: 0.1987 (0.0007 -- 2.1668)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5171 (0.7048)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2193 (0.1710 -- 0.3874)  data: 0.0096 (0.0001 -- 0.1693)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6407 (0.7642)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.0207)  time: 0.2017 (0.1330 -- 0.3874)  data: 0.0091 (0.0001 -- 0.1693)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 81.743 Acc@5 96.058 loss 0.722
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 82.37%
Epoch: [77]  [  0/160]  eta: 0:21:54  lr: 0.000023  min_lr: 0.000001  loss: 1.9522 (1.9522)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3059 (8.3059)  time: 8.2187 (8.2187 -- 8.2187)  data: 7.1241 (7.1241 -- 7.1241)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:47  lr: 0.000023  min_lr: 0.000001  loss: 1.8894 (1.8888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0757 (8.4843)  time: 0.8482 (0.5273 -- 4.1751)  data: 0.2924 (0.0004 -- 3.6327)  max mem: 16413
[2023-08-29 20:48:40,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:48:40,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:48:40,794] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:48:40,794] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [77]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 1.8748 (1.8462)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6216 (8.2366)  time: 0.9153 (0.5219 -- 3.1867)  data: 0.3720 (0.0001 -- 2.6597)  max mem: 16413
[2023-08-29 20:48:58,121] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12376
[2023-08-29 20:48:58,121] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12376
[2023-08-29 20:48:58,121] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:48:58,121] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:48:58,121] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [77]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 1.6498 (1.7769)  loss_scale: 32768.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7879 (8.0762)  time: 0.7914 (0.5298 -- 3.0534)  data: 0.2069 (0.0002 -- 2.0725)  max mem: 16413
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 1.6456 (1.7792)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8414 (8.4567)  time: 0.9038 (0.5367 -- 5.4534)  data: 0.1797 (0.0003 -- 1.8730)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.7093 (1.7712)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0238 (8.5841)  time: 0.8140 (0.5303 -- 4.4156)  data: 0.0020 (0.0004 -- 0.0121)  max mem: 16413
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.6821 (1.7639)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8759 (8.7160)  time: 0.8013 (0.5528 -- 2.9202)  data: 0.0051 (0.0003 -- 0.0669)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 1.8199 (1.7695)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1401 (8.7025)  time: 0.7696 (0.5284 -- 3.1529)  data: 0.0016 (0.0002 -- 0.0042)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.8731 (1.7869)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2934 (8.6462)  time: 0.7656 (0.4948 -- 2.2874)  data: 0.1581 (0.0002 -- 1.7600)  max mem: 16413
Epoch: [77] Total time: 0:02:19 (0.8743 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.8731 (1.8235)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2934 (8.6462)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3312 (0.3312)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4737 (2.4737 -- 2.4737)  data: 2.2548 (2.2548 -- 2.2548)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4841 (0.7838)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4368 (0.1961 -- 2.4737)  data: 0.2241 (0.0005 -- 2.2548)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5553 (0.7115)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2172 (0.1697 -- 0.4541)  data: 0.0138 (0.0001 -- 0.1996)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6288 (0.7643)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.0207)  time: 0.2044 (0.1331 -- 0.4541)  data: 0.0136 (0.0001 -- 0.1996)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 81.120 Acc@5 96.266 loss 0.717
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 82.37%
Epoch: [78]  [  0/160]  eta: 0:21:05  lr: 0.000023  min_lr: 0.000001  loss: 1.6886 (1.6886)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5167 (6.5167)  time: 7.9088 (7.9088 -- 7.9088)  data: 7.3454 (7.3454 -- 7.3454)  max mem: 16413
[2023-08-29 20:50:44,759] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12491
[2023-08-29 20:50:44,759] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12491
[2023-08-29 20:50:44,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:50:44,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:50:44,760] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [78]  [ 20/160]  eta: 0:02:35  lr: 0.000023  min_lr: 0.000001  loss: 1.7368 (1.7519)  loss_scale: 8192.0000 (12483.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7072 (8.8486)  time: 0.7706 (0.5364 -- 2.3672)  data: 0.1848 (0.0010 -- 1.8009)  max mem: 16413
Epoch: [78]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 2.0414 (1.8565)  loss_scale: 8192.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1046 (9.3539)  time: 0.9114 (0.5329 -- 3.0084)  data: 0.3338 (0.0002 -- 2.4770)  max mem: 16413
Epoch: [78]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 1.8290 (1.8525)  loss_scale: 8192.0000 (9669.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8663 (9.2203)  time: 0.8313 (0.5202 -- 2.2593)  data: 0.0848 (0.0003 -- 1.0464)  max mem: 16413
Epoch: [78]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 1.9342 (1.8589)  loss_scale: 8192.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4064 (9.1136)  time: 0.8999 (0.5265 -- 2.8222)  data: 0.3520 (0.0004 -- 2.3019)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.7684 (1.8560)  loss_scale: 8192.0000 (9084.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6547 (9.0684)  time: 0.8573 (0.5251 -- 2.7016)  data: 0.3021 (0.0004 -- 2.1697)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.8606 (1.8586)  loss_scale: 8192.0000 (8936.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8109 (8.9069)  time: 0.9094 (0.5218 -- 3.1717)  data: 0.3571 (0.0003 -- 2.6332)  max mem: 16413
[2023-08-29 20:52:37,878] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:52:37,878] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:52:37,878] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 20:52:37,878] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.7826 (1.8469)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7887 (8.9791)  time: 0.8805 (0.5378 -- 4.7281)  data: 0.3340 (0.0005 -- 4.2053)  max mem: 16413
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8803 (1.8392)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2515 (9.0091)  time: 0.6835 (0.4947 -- 3.3569)  data: 0.1618 (0.0002 -- 2.8420)  max mem: 16413
Epoch: [78] Total time: 0:02:22 (0.8893 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.8803 (1.8442)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2515 (9.0091)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3681 (0.3681)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3682 (2.3682 -- 2.3682)  data: 2.1532 (2.1532 -- 2.1532)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4954 (0.7780)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4539 (0.2098 -- 2.3682)  data: 0.2370 (0.0009 -- 2.1532)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5574 (0.7003)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2299 (0.1689 -- 0.6465)  data: 0.0269 (0.0001 -- 0.4431)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6755 (0.7560)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.4357)  time: 0.2133 (0.1325 -- 0.6465)  data: 0.0266 (0.0001 -- 0.4431)  max mem: 16413
Val: Total time: 0:00:07 (0.2944 s / it)
* Acc@1 81.328 Acc@5 96.473 loss 0.717
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 82.37%
Epoch: [79]  [  0/160]  eta: 0:24:39  lr: 0.000022  min_lr: 0.000001  loss: 1.9058 (1.9058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7920 (6.7920)  time: 9.2446 (9.2446 -- 9.2446)  data: 8.7067 (8.7067 -- 8.7067)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:44  lr: 0.000022  min_lr: 0.000001  loss: 1.8504 (1.8583)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9776 (8.4577)  time: 0.7751 (0.5243 -- 3.2525)  data: 0.2302 (0.0003 -- 2.7396)  max mem: 16413
Epoch: [79]  [ 40/160]  eta: 0:02:02  lr: 0.000022  min_lr: 0.000001  loss: 1.9719 (1.9059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0312 (8.7603)  time: 0.8594 (0.5256 -- 2.5834)  data: 0.1425 (0.0003 -- 1.0122)  max mem: 16413
[2023-08-29 20:53:43,339] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12681
[2023-08-29 20:53:43,339] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:53:43,339] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12681
[2023-08-29 20:53:43,339] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 20:53:43,339] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [79]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 1.9322 (1.9160)  loss_scale: 8192.0000 (13698.0984)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8325 (9.1301)  time: 0.8732 (0.5225 -- 3.9617)  data: 0.0012 (0.0003 -- 0.0022)  max mem: 16413
Epoch: [79]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 1.9203 (1.9052)  loss_scale: 8192.0000 (12338.5679)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1016 (8.9880)  time: 0.8204 (0.5322 -- 3.1422)  data: 0.0020 (0.0005 -- 0.0066)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000001  loss: 1.8568 (1.8980)  loss_scale: 8192.0000 (11517.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5664 (8.9312)  time: 0.9501 (0.5282 -- 3.3519)  data: 0.0017 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.5322 (1.8456)  loss_scale: 8192.0000 (10967.8017)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0470 (8.8580)  time: 0.8037 (0.5214 -- 3.7891)  data: 0.0019 (0.0002 -- 0.0060)  max mem: 16413
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.7823 (1.8321)  loss_scale: 8192.0000 (10574.0709)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2421 (8.8081)  time: 0.8770 (0.5223 -- 4.7180)  data: 0.0021 (0.0002 -- 0.0147)  max mem: 16413
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.5465 (1.8105)  loss_scale: 8192.0000 (10291.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6778 (8.7753)  time: 0.7008 (0.4949 -- 3.3791)  data: 0.0008 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [79] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.5465 (1.8351)  loss_scale: 8192.0000 (10291.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6778 (8.7753)
[2023-08-29 20:55:20,890] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-08-29 20:55:20,892] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-08-29 20:55:20,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-08-29 20:55:20,892] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-08-29 20:55:21,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-08-29 20:55:21,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3636 (0.3636)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3464 (2.3464 -- 2.3464)  data: 2.1477 (2.1477 -- 2.1477)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5494 (0.7782)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (96.9697)  time: 0.4152 (0.2006 -- 2.3464)  data: 0.2011 (0.0010 -- 2.1477)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5412 (0.7035)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.8254)  time: 0.2274 (0.1688 -- 0.6048)  data: 0.0244 (0.0001 -- 0.4216)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6169 (0.7533)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.2081 (0.1325 -- 0.6048)  data: 0.0233 (0.0001 -- 0.4216)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 80.290 Acc@5 96.680 loss 0.709
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 82.37%
Epoch: [80]  [  0/160]  eta: 0:19:16  lr: 0.000022  min_lr: 0.000001  loss: 2.0812 (2.0812)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3098 (12.3098)  time: 7.2305 (7.2305 -- 7.2305)  data: 6.0180 (6.0180 -- 6.0180)  max mem: 16413
[2023-08-29 20:55:45,724] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:55:45,724] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:55:45,724] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 20:55:45,724] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [80]  [ 20/160]  eta: 0:02:34  lr: 0.000022  min_lr: 0.000001  loss: 1.8401 (1.8680)  loss_scale: 16384.0000 (12483.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9461 (9.1797)  time: 0.7993 (0.5168 -- 2.2073)  data: 0.0728 (0.0001 -- 1.0394)  max mem: 16413
Epoch: [80]  [ 40/160]  eta: 0:02:00  lr: 0.000022  min_lr: 0.000001  loss: 1.7511 (1.8313)  loss_scale: 16384.0000 (14385.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5900 (9.2911)  time: 0.8925 (0.5251 -- 2.8148)  data: 0.1688 (0.0001 -- 2.2510)  max mem: 16413
Epoch: [80]  [ 60/160]  eta: 0:01:35  lr: 0.000022  min_lr: 0.000001  loss: 1.7371 (1.8283)  loss_scale: 16384.0000 (15041.0492)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0173 (8.9719)  time: 0.8478 (0.5437 -- 2.1253)  data: 0.0797 (0.0003 -- 1.5501)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:17  lr: 0.000022  min_lr: 0.000001  loss: 1.9108 (1.8402)  loss_scale: 16384.0000 (15372.6420)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5642 (8.7016)  time: 1.0354 (0.5249 -- 3.7689)  data: 0.2913 (0.0003 -- 3.2633)  max mem: 16413
Epoch: [80]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9024 (1.8475)  loss_scale: 16384.0000 (15572.9109)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5407 (8.5361)  time: 0.7048 (0.5217 -- 2.0801)  data: 0.1519 (0.0001 -- 1.5435)  max mem: 16413
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9036 (1.8616)  loss_scale: 16384.0000 (15706.9752)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6946 (8.9318)  time: 0.8645 (0.5249 -- 3.8661)  data: 0.3128 (0.0003 -- 3.3239)  max mem: 16413
[2023-08-29 20:57:34,621] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:57:34,621] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 20:57:34,662] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 20:57:34,662] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [80]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 1.5066 (1.8279)  loss_scale: 16384.0000 (16151.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3953 (8.8933)  time: 0.7997 (0.5318 -- 3.1541)  data: 0.2461 (0.0001 -- 2.6306)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8692 (1.8426)  loss_scale: 32768.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6794 (8.8835)  time: 0.7135 (0.5021 -- 3.2515)  data: 0.1772 (0.0002 -- 2.7304)  max mem: 16413
Epoch: [80] Total time: 0:02:19 (0.8740 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.8692 (1.8390)  loss_scale: 32768.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6794 (8.8835)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3123 (0.3123)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2989 (2.2989 -- 2.2989)  data: 2.0950 (2.0950 -- 2.0950)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4832 (0.7655)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4218 (0.2034 -- 2.2989)  data: 0.2051 (0.0005 -- 2.0950)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4843 (0.6865)  acc1: 88.8889 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2266 (0.1693 -- 0.4490)  data: 0.0207 (0.0001 -- 0.2492)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6162 (0.7419)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.0207)  time: 0.2084 (0.1334 -- 0.4490)  data: 0.0199 (0.0001 -- 0.2492)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 81.120 Acc@5 96.266 loss 0.703
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 82.37%
Epoch: [81]  [  0/160]  eta: 0:22:27  lr: 0.000022  min_lr: 0.000001  loss: 2.2150 (2.2150)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6731 (7.6731)  time: 8.4218 (8.4218 -- 8.4218)  data: 7.8910 (7.8910 -- 7.8910)  max mem: 16413
Epoch: [81]  [ 20/160]  eta: 0:02:49  lr: 0.000022  min_lr: 0.000001  loss: 1.9082 (1.8827)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6131 (8.3825)  time: 0.8464 (0.5240 -- 2.6106)  data: 0.0755 (0.0003 -- 1.3578)  max mem: 16413
[2023-08-29 20:58:39,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=73, lr=[5.210211612813597e-07, 5.210211612813597e-07, 6.946948817084797e-07, 6.946948817084797e-07, 9.262598422779729e-07, 9.262598422779729e-07, 1.2350131230372973e-06, 1.2350131230372973e-06, 1.6466841640497297e-06, 1.6466841640497297e-06, 2.1955788853996396e-06, 2.1955788853996396e-06, 2.927438513866186e-06, 2.927438513866186e-06, 3.903251351821581e-06, 3.903251351821581e-06, 5.204335135762108e-06, 5.204335135762108e-06, 6.939113514349478e-06, 6.939113514349478e-06, 9.252151352465971e-06, 9.252151352465971e-06, 1.2336201803287961e-05, 1.2336201803287961e-05, 1.6448269071050613e-05, 1.6448269071050613e-05, 2.1931025428067486e-05, 2.1931025428067486e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 20:58:39,449] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=17.58331838813258, CurrSamplesPerSec=22.260524973828264, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:04  lr: 0.000022  min_lr: 0.000001  loss: 1.5424 (1.7823)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2221 (8.5627)  time: 0.8655 (0.5258 -- 3.2849)  data: 0.0092 (0.0003 -- 0.1521)  max mem: 16413
[2023-08-29 20:58:45,058] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13006
[2023-08-29 20:58:45,058] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13006
[2023-08-29 20:58:45,073] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:58:45,073] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 20:58:45,073] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [81]  [ 60/160]  eta: 0:01:39  lr: 0.000022  min_lr: 0.000001  loss: 1.9903 (1.8454)  loss_scale: 16384.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9087 (8.4369)  time: 0.9101 (0.5189 -- 5.1785)  data: 0.2688 (0.0002 -- 4.6403)  max mem: 16413
Epoch: [81]  [ 80/160]  eta: 0:01:17  lr: 0.000022  min_lr: 0.000001  loss: 1.8606 (1.8311)  loss_scale: 16384.0000 (25688.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5526 (8.2897)  time: 0.8625 (0.5245 -- 2.7528)  data: 0.3095 (0.0002 -- 2.2091)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000001  loss: 1.7945 (1.8273)  loss_scale: 16384.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9117 (8.2478)  time: 0.8799 (0.5352 -- 2.5280)  data: 0.2316 (0.0006 -- 1.9798)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.6274 (1.8085)  loss_scale: 16384.0000 (22612.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8178 (8.1969)  time: 0.7217 (0.5153 -- 2.1280)  data: 0.0285 (0.0002 -- 0.3900)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.8421 (1.8116)  loss_scale: 16384.0000 (21729.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3595 (8.2440)  time: 0.9156 (0.5315 -- 4.0462)  data: 0.0114 (0.0004 -- 0.2021)  max mem: 16413
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.7326 (1.7952)  loss_scale: 16384.0000 (21094.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5708 (8.2818)  time: 0.7934 (0.4971 -- 4.0452)  data: 0.0174 (0.0002 -- 0.3350)  max mem: 16413
Epoch: [81] Total time: 0:02:23 (0.8987 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.7326 (1.7958)  loss_scale: 16384.0000 (21094.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5708 (8.2818)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3692 (0.3692)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4974 (2.4974 -- 2.4974)  data: 2.2761 (2.2761 -- 2.2761)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4937 (0.7786)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (94.9495)  time: 0.4253 (0.2003 -- 2.4974)  data: 0.2104 (0.0009 -- 2.2761)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5290 (0.6938)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (95.2381)  time: 0.2127 (0.1704 -- 0.3050)  data: 0.0080 (0.0001 -- 0.1177)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6186 (0.7543)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (94.1909)  time: 0.1964 (0.1332 -- 0.3050)  data: 0.0071 (0.0001 -- 0.1177)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 81.120 Acc@5 95.643 loss 0.724
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 82.37%
Epoch: [82]  [  0/160]  eta: 0:21:29  lr: 0.000022  min_lr: 0.000001  loss: 2.0444 (2.0444)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3163 (12.3163)  time: 8.0622 (8.0622 -- 8.0622)  data: 6.2529 (6.2529 -- 6.2529)  max mem: 16413
[2023-08-29 21:00:48,335] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:00:48,335] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:00:48,335] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:00:48,335] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [82]  [ 20/160]  eta: 0:02:40  lr: 0.000022  min_lr: 0.000001  loss: 1.8679 (1.8855)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1719 (8.7653)  time: 0.7980 (0.5287 -- 2.3018)  data: 0.1207 (0.0004 -- 0.8308)  max mem: 16413
[2023-08-29 21:01:07,271] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13153
[2023-08-29 21:01:07,271] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13153
[2023-08-29 21:01:07,272] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:01:07,272] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:01:07,272] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [82]  [ 40/160]  eta: 0:02:11  lr: 0.000022  min_lr: 0.000001  loss: 1.7649 (1.8405)  loss_scale: 32768.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9602 (8.9012)  time: 1.0474 (0.5163 -- 4.7613)  data: 0.5038 (0.0004 -- 4.2173)  max mem: 16413
Epoch: [82]  [ 60/160]  eta: 0:01:38  lr: 0.000022  min_lr: 0.000001  loss: 1.8064 (1.8276)  loss_scale: 16384.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0431 (8.9271)  time: 0.7611 (0.5176 -- 2.7112)  data: 0.2184 (0.0007 -- 2.1846)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:13  lr: 0.000022  min_lr: 0.000001  loss: 1.8390 (1.8507)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3843 (8.7773)  time: 0.7361 (0.5323 -- 3.3968)  data: 0.1799 (0.0003 -- 2.8303)  max mem: 16413
Epoch: [82]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.0287 (1.8702)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3922 (8.6264)  time: 0.9020 (0.5247 -- 3.8329)  data: 0.0540 (0.0004 -- 0.7203)  max mem: 16413
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.7776 (1.8511)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9205 (8.7038)  time: 0.8906 (0.5305 -- 3.5851)  data: 0.2638 (0.0007 -- 3.0651)  max mem: 16413
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.6372 (1.8360)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9768 (8.6476)  time: 0.9365 (0.5332 -- 3.8494)  data: 0.1130 (0.0001 -- 2.2308)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.7051 (1.8251)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2901 (8.6671)  time: 0.7948 (0.4949 -- 3.8494)  data: 0.0007 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [82] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.7051 (1.8281)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2901 (8.6671)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3440 (0.3440)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4488 (2.4488 -- 2.4488)  data: 2.2165 (2.2165 -- 2.2165)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4740 (0.7678)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4308 (0.2005 -- 2.4488)  data: 0.2168 (0.0005 -- 2.2165)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5404 (0.6899)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2144 (0.1693 -- 0.3601)  data: 0.0106 (0.0001 -- 0.1582)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5962 (0.7455)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.0207)  time: 0.1999 (0.1331 -- 0.3601)  data: 0.0103 (0.0001 -- 0.1582)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 80.290 Acc@5 96.266 loss 0.713
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 82.37%
Epoch: [83]  [  0/160]  eta: 0:22:18  lr: 0.000021  min_lr: 0.000001  loss: 1.8802 (1.8802)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5460 (7.5460)  time: 8.3634 (8.3634 -- 8.3634)  data: 7.8285 (7.8285 -- 7.8285)  max mem: 16413
[2023-08-29 21:03:07,831] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:03:07,831] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:03:07,872] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:03:07,872] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:03:08,404] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13283
[2023-08-29 21:03:08,404] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13283
[2023-08-29 21:03:08,405] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:03:08,405] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:03:08,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [83]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000001  loss: 1.7574 (1.8100)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1738 (8.7949)  time: 0.8521 (0.5216 -- 3.9291)  data: 0.3059 (0.0007 -- 3.3899)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:01:57  lr: 0.000021  min_lr: 0.000001  loss: 1.7333 (1.7401)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2976 (8.5463)  time: 0.7371 (0.5339 -- 1.5842)  data: 0.1343 (0.0002 -- 0.9900)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000001  loss: 1.9195 (1.7931)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2562 (8.4650)  time: 0.9631 (0.5264 -- 2.9369)  data: 0.0855 (0.0006 -- 1.4604)  max mem: 16413
Epoch: [83]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000001  loss: 1.8659 (1.7961)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5587 (8.4996)  time: 0.8266 (0.5266 -- 3.4148)  data: 0.0971 (0.0005 -- 1.0529)  max mem: 16413
Epoch: [83]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000001  loss: 2.0004 (1.8364)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4407 (8.4451)  time: 0.8346 (0.5223 -- 2.2369)  data: 0.1645 (0.0002 -- 1.4501)  max mem: 16413
Epoch: [83]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.6066 (1.8100)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9881 (8.4795)  time: 0.9012 (0.5359 -- 3.6669)  data: 0.3488 (0.0006 -- 3.1253)  max mem: 16413
[2023-08-29 21:05:00,408] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:05:00,408] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:05:00,409] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:05:00,409] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:05:01,543] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13414
[2023-08-29 21:05:01,543] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13414
[2023-08-29 21:05:01,543] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:05:01,543] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:05:01,543] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 21:05:02,087] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13415
[2023-08-29 21:05:02,087] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13415
[2023-08-29 21:05:02,087] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:05:02,087] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:05:02,087] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.6109 (1.7926)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0187 (8.5065)  time: 0.9415 (0.5240 -- 3.7995)  data: 0.3955 (0.0004 -- 3.2849)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.9432 (1.8063)  loss_scale: 8192.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0685 (8.6456)  time: 0.6720 (0.4941 -- 3.0372)  data: 0.1388 (0.0002 -- 2.5105)  max mem: 16413
Epoch: [83] Total time: 0:02:22 (0.8903 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.9432 (1.8289)  loss_scale: 8192.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0685 (8.6456)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3953 (0.3953)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4136 (2.4136 -- 2.4136)  data: 2.1905 (2.1905 -- 2.1905)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4938 (0.7625)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4235 (0.2040 -- 2.4136)  data: 0.2045 (0.0005 -- 2.1905)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5470 (0.6944)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (95.7672)  time: 0.2199 (0.1696 -- 0.2794)  data: 0.0107 (0.0001 -- 0.0980)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5953 (0.7505)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (94.6058)  time: 0.2043 (0.1326 -- 0.2794)  data: 0.0105 (0.0001 -- 0.0980)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 79.461 Acc@5 96.058 loss 0.722
Accuracy of the network on the 482 val images: 79.46%
Max accuracy: 82.37%
Epoch: [84]  [  0/160]  eta: 0:20:11  lr: 0.000021  min_lr: 0.000001  loss: 2.2143 (2.2143)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1540 (8.1540)  time: 7.5737 (7.5737 -- 7.5737)  data: 7.0421 (7.0421 -- 7.0421)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000001  loss: 1.5813 (1.6802)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5749 (8.0178)  time: 0.8916 (0.5196 -- 4.2717)  data: 0.3396 (0.0003 -- 3.7288)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:03  lr: 0.000021  min_lr: 0.000001  loss: 1.8051 (1.7553)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2654 (8.0623)  time: 0.8454 (0.5302 -- 3.2747)  data: 0.2766 (0.0004 -- 2.7084)  max mem: 16413
Epoch: [84]  [ 60/160]  eta: 0:01:36  lr: 0.000021  min_lr: 0.000001  loss: 1.8977 (1.8057)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3193 (8.2825)  time: 0.8240 (0.5256 -- 3.8699)  data: 0.2758 (0.0004 -- 3.3495)  max mem: 16413
Epoch: [84]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000001  loss: 1.6952 (1.7880)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7725 (8.2999)  time: 0.9308 (0.5325 -- 4.3167)  data: 0.3823 (0.0004 -- 3.7943)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000001  loss: 1.7389 (1.7760)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5550 (8.1806)  time: 0.8074 (0.5221 -- 3.3581)  data: 0.2599 (0.0004 -- 2.8271)  max mem: 16413
[2023-08-29 21:07:07,035] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:07:07,035] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:07:07,035] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 21:07:07,035] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [84]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000001  loss: 1.9213 (1.7922)  loss_scale: 16384.0000 (9342.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5235 (8.1746)  time: 0.9259 (0.5223 -- 3.2276)  data: 0.3757 (0.0003 -- 2.7078)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.7144 (1.7893)  loss_scale: 16384.0000 (10341.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9551 (8.5061)  time: 0.8045 (0.5297 -- 3.6668)  data: 0.2444 (0.0003 -- 3.1415)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.7915 (1.7822)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4071 (8.4012)  time: 0.7353 (0.4965 -- 3.3640)  data: 0.2156 (0.0002 -- 2.8428)  max mem: 16413
Epoch: [84] Total time: 0:02:22 (0.8897 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.7915 (1.8150)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4071 (8.4012)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3386 (0.3386)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2928 (2.2928 -- 2.2928)  data: 2.0371 (2.0371 -- 2.0371)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5102 (0.7747)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4066 (0.1848 -- 2.2928)  data: 0.1900 (0.0007 -- 2.0371)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5102 (0.6857)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2250 (0.1713 -- 0.4084)  data: 0.0176 (0.0001 -- 0.1657)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6046 (0.7443)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (94.6058)  time: 0.2110 (0.1331 -- 0.4084)  data: 0.0174 (0.0001 -- 0.1657)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 80.290 Acc@5 95.851 loss 0.711
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 82.37%
Epoch: [85]  [  0/160]  eta: 0:23:58  lr: 0.000021  min_lr: 0.000000  loss: 1.6457 (1.6457)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8182 (8.8182)  time: 8.9924 (8.9924 -- 8.9924)  data: 6.2080 (6.2080 -- 6.2080)  max mem: 16413
Epoch: [85]  [ 20/160]  eta: 0:02:32  lr: 0.000021  min_lr: 0.000000  loss: 1.8236 (1.7798)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9531 (8.7330)  time: 0.6979 (0.5232 -- 1.8831)  data: 0.0021 (0.0004 -- 0.0165)  max mem: 16413
Epoch: [85]  [ 40/160]  eta: 0:02:01  lr: 0.000021  min_lr: 0.000000  loss: 1.8633 (1.7781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9130 (8.4850)  time: 0.9321 (0.5292 -- 2.6462)  data: 0.0029 (0.0002 -- 0.0236)  max mem: 16413
Epoch: [85]  [ 60/160]  eta: 0:01:34  lr: 0.000021  min_lr: 0.000000  loss: 1.5140 (1.7224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1201 (8.1948)  time: 0.7933 (0.5152 -- 2.5602)  data: 0.0918 (0.0005 -- 1.8019)  max mem: 16413
[2023-08-29 21:09:07,560] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:09:07,560] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:09:07,565] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:09:07,566] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [85]  [ 80/160]  eta: 0:01:17  lr: 0.000021  min_lr: 0.000000  loss: 1.9057 (1.7717)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5850 (8.1586)  time: 1.0256 (0.5268 -- 3.4161)  data: 0.3257 (0.0004 -- 2.8718)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 1.6401 (1.7464)  loss_scale: 32768.0000 (21088.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2991 (8.2436)  time: 0.8726 (0.5201 -- 4.4682)  data: 0.3310 (0.0004 -- 3.9478)  max mem: 16413
[2023-08-29 21:09:35,259] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13702
[2023-08-29 21:09:35,259] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:09:35,259] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13702
[2023-08-29 21:09:35,259] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:09:35,259] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 1.7274 (1.7577)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6429 (8.4594)  time: 0.8815 (0.5139 -- 3.6822)  data: 0.3311 (0.0002 -- 3.1239)  max mem: 16413
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.7284 (1.7655)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8611 (8.4904)  time: 0.8150 (0.5260 -- 3.5931)  data: 0.2628 (0.0004 -- 3.0397)  max mem: 16413
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9053 (1.7817)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5037 (8.6655)  time: 0.7119 (0.4949 -- 2.4131)  data: 0.0534 (0.0001 -- 1.0546)  max mem: 16413
Epoch: [85] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9053 (1.8208)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5037 (8.6655)
Val:  [ 0/27]  eta: 0:01:14  loss: 0.3176 (0.3176)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7417 (2.7417 -- 2.7417)  data: 2.5247 (2.5247 -- 2.5247)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5043 (0.7577)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4436 (0.2014 -- 2.7417)  data: 0.2305 (0.0003 -- 2.5247)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5559 (0.6893)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (95.7672)  time: 0.2064 (0.1701 -- 0.2387)  data: 0.0007 (0.0001 -- 0.0020)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6379 (0.7453)  acc1: 88.8889 (80.4979)  acc5: 100.0000 (94.6058)  time: 0.1910 (0.1329 -- 0.2387)  data: 0.0004 (0.0001 -- 0.0014)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 82.158 Acc@5 95.851 loss 0.712
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.37%
Epoch: [86]  [  0/160]  eta: 0:20:29  lr: 0.000021  min_lr: 0.000000  loss: 1.9391 (1.9391)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4929 (7.4929)  time: 7.6818 (7.6818 -- 7.6818)  data: 7.1391 (7.1391 -- 7.1391)  max mem: 16413
Epoch: [86]  [ 20/160]  eta: 0:02:44  lr: 0.000021  min_lr: 0.000000  loss: 1.9115 (1.8554)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8351 (8.8929)  time: 0.8508 (0.5295 -- 3.8179)  data: 0.2994 (0.0004 -- 3.2441)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:05  lr: 0.000021  min_lr: 0.000000  loss: 1.7180 (1.7976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1606 (8.6435)  time: 0.9078 (0.5212 -- 3.3075)  data: 0.2799 (0.0004 -- 2.7946)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:40  lr: 0.000021  min_lr: 0.000000  loss: 1.7438 (1.7915)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3844 (9.0393)  time: 0.9264 (0.5090 -- 3.2255)  data: 0.0782 (0.0002 -- 1.1591)  max mem: 16413
[2023-08-29 21:11:38,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:11:38,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:11:38,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:11:38,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [86]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000000  loss: 1.8990 (1.8199)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6347 (8.9974)  time: 0.8191 (0.5354 -- 2.2615)  data: 0.2017 (0.0005 -- 1.7271)  max mem: 16413
[2023-08-29 21:11:54,678] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13848
[2023-08-29 21:11:54,678] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13848
[2023-08-29 21:11:54,678] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:11:54,679] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:11:54,679] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [86]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 1.7895 (1.8127)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6384 (8.9995)  time: 0.8268 (0.5252 -- 3.2758)  data: 0.2152 (0.0002 -- 2.7246)  max mem: 16413
Epoch: [86]  [120/160]  eta: 0:00:38  lr: 0.000021  min_lr: 0.000000  loss: 1.9420 (1.8351)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7929 (8.9866)  time: 1.0381 (0.5189 -- 5.0269)  data: 0.4994 (0.0002 -- 4.5259)  max mem: 16413
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.6966 (1.8225)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1069 (8.9681)  time: 0.8199 (0.5201 -- 3.3335)  data: 0.2784 (0.0003 -- 2.8264)  max mem: 16413
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7882 (1.8120)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1729 (8.9088)  time: 0.6646 (0.4948 -- 3.5410)  data: 0.1520 (0.0002 -- 3.0303)  max mem: 16413
Epoch: [86] Total time: 0:02:24 (0.9017 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7882 (1.7829)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1729 (8.9088)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2976 (0.2976)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3664 (2.3664 -- 2.3664)  data: 2.1524 (2.1524 -- 2.1524)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5396 (0.7836)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (94.9495)  time: 0.4379 (0.1931 -- 2.3664)  data: 0.2286 (0.0004 -- 2.1524)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5402 (0.6918)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (95.2381)  time: 0.2246 (0.1700 -- 0.5531)  data: 0.0228 (0.0001 -- 0.3526)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5925 (0.7614)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (94.1909)  time: 0.2115 (0.1329 -- 0.5531)  data: 0.0225 (0.0001 -- 0.3526)  max mem: 16413
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 81.535 Acc@5 95.851 loss 0.721
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.37%
Epoch: [87]  [  0/160]  eta: 0:20:14  lr: 0.000020  min_lr: 0.000000  loss: 1.9396 (1.9396)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9357 (8.9357)  time: 7.5887 (7.5887 -- 7.5887)  data: 7.0341 (7.0341 -- 7.0341)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:45  lr: 0.000020  min_lr: 0.000000  loss: 1.7517 (1.7704)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7922 (9.0996)  time: 0.8626 (0.5263 -- 3.9057)  data: 0.2608 (0.0003 -- 3.3501)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7152 (1.7555)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7693 (8.7048)  time: 0.8185 (0.5298 -- 2.9478)  data: 0.2678 (0.0004 -- 2.3921)  max mem: 16413
[2023-08-29 21:13:55,814] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:13:55,814] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:13:55,815] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:13:55,815] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [87]  [ 60/160]  eta: 0:01:34  lr: 0.000020  min_lr: 0.000000  loss: 1.7289 (1.7554)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7744 (8.8235)  time: 0.8125 (0.5291 -- 2.4914)  data: 0.2521 (0.0002 -- 1.9511)  max mem: 16413
[2023-08-29 21:14:16,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=80, lr=[4.83506716448184e-07, 4.83506716448184e-07, 6.44675621930912e-07, 6.44675621930912e-07, 8.595674959078828e-07, 8.595674959078828e-07, 1.1460899945438437e-06, 1.1460899945438437e-06, 1.528119992725125e-06, 1.528119992725125e-06, 2.0374933236334998e-06, 2.0374933236334998e-06, 2.716657764844666e-06, 2.716657764844666e-06, 3.622210353126222e-06, 3.622210353126222e-06, 4.829613804168296e-06, 4.829613804168296e-06, 6.439485072224394e-06, 6.439485072224394e-06, 8.585980096299192e-06, 8.585980096299192e-06, 1.1447973461732256e-05, 1.1447973461732256e-05, 1.526396461564301e-05, 1.526396461564301e-05, 2.0351952820857345e-05, 2.0351952820857345e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 21:14:16,117] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=17.696393477608776, CurrSamplesPerSec=20.93397839873826, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:13  lr: 0.000020  min_lr: 0.000000  loss: 2.0981 (1.8322)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5590 (8.6325)  time: 0.8671 (0.5334 -- 2.4000)  data: 0.1382 (0.0005 -- 1.6409)  max mem: 16413
Epoch: [87]  [100/160]  eta: 0:00:54  lr: 0.000020  min_lr: 0.000000  loss: 1.7321 (1.8134)  loss_scale: 32768.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5015 (8.6042)  time: 0.8746 (0.5419 -- 3.2332)  data: 0.0381 (0.0005 -- 0.6458)  max mem: 16413
[2023-08-29 21:14:43,559] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14030
[2023-08-29 21:14:43,559] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:14:43,559] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14030
[2023-08-29 21:14:43,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 21:14:43,560] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [87]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8233 (1.8185)  loss_scale: 16384.0000 (23560.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5902 (8.4975)  time: 0.8668 (0.5376 -- 1.9298)  data: 0.1912 (0.0004 -- 1.3844)  max mem: 16413
Epoch: [87]  [140/160]  eta: 0:00:17  lr: 0.000020  min_lr: 0.000000  loss: 1.8629 (1.8304)  loss_scale: 16384.0000 (22542.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7381 (8.5040)  time: 0.8212 (0.5391 -- 1.7330)  data: 0.2161 (0.0007 -- 1.1792)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7697 (1.8313)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9075 (8.4576)  time: 0.7843 (0.4964 -- 2.8651)  data: 0.2312 (0.0002 -- 2.3393)  max mem: 16413
Epoch: [87] Total time: 0:02:20 (0.8755 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7697 (1.8015)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9075 (8.4576)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3147 (0.3147)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4400 (2.4400 -- 2.4400)  data: 2.1863 (2.1863 -- 2.1863)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4627 (0.7636)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4318 (0.1865 -- 2.4400)  data: 0.2112 (0.0007 -- 2.1863)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5044 (0.6764)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2188 (0.1685 -- 0.3349)  data: 0.0104 (0.0001 -- 0.1234)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6376 (0.7401)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.4357)  time: 0.2013 (0.1326 -- 0.3349)  data: 0.0100 (0.0001 -- 0.1234)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 82.573 Acc@5 96.680 loss 0.702
Accuracy of the network on the 482 val images: 82.57%
[2023-08-29 21:15:29,727] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 21:15:29,729] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 21:15:29,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 21:15:29,729] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 21:15:31,099] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 21:15:31,099] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.57%
Epoch: [88]  [  0/160]  eta: 0:17:17  lr: 0.000020  min_lr: 0.000000  loss: 1.9683 (1.9683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4687 (4.4687)  time: 6.4871 (6.4871 -- 6.4871)  data: 5.9456 (5.9456 -- 5.9456)  max mem: 16413
Epoch: [88]  [ 20/160]  eta: 0:02:46  lr: 0.000020  min_lr: 0.000000  loss: 2.0039 (1.9041)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6056 (8.8015)  time: 0.9220 (0.5283 -- 4.2077)  data: 0.3726 (0.0006 -- 3.6572)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:01:57  lr: 0.000020  min_lr: 0.000000  loss: 1.7052 (1.7757)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9088 (9.0632)  time: 0.7631 (0.5268 -- 2.5938)  data: 0.1956 (0.0002 -- 2.0765)  max mem: 16413
Epoch: [88]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 1.8392 (1.7902)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1389 (8.8657)  time: 0.9129 (0.5375 -- 3.1222)  data: 0.3541 (0.0009 -- 2.5986)  max mem: 16413
[2023-08-29 21:16:45,942] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:16:45,942] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:16:45,943] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:16:45,943] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [88]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000000  loss: 1.8738 (1.7890)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7012 (8.7318)  time: 0.8477 (0.5176 -- 3.1383)  data: 0.1633 (0.0006 -- 2.1900)  max mem: 16413
[2023-08-29 21:16:56,987] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14172
[2023-08-29 21:16:56,987] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14172
[2023-08-29 21:16:56,988] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:16:56,988] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:16:56,988] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [88]  [100/160]  eta: 0:00:54  lr: 0.000020  min_lr: 0.000000  loss: 1.8901 (1.8003)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4309 (8.8494)  time: 0.8585 (0.5310 -- 2.6560)  data: 0.0716 (0.0003 -- 1.3939)  max mem: 16413
Epoch: [88]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8801 (1.8106)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3437 (8.8286)  time: 0.9215 (0.5172 -- 2.6263)  data: 0.2697 (0.0004 -- 2.0617)  max mem: 16413
Epoch: [88]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.7324 (1.7952)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5494 (8.8930)  time: 0.8136 (0.5244 -- 2.2140)  data: 0.1386 (0.0003 -- 1.4847)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.8008 (1.7912)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0353 (8.9447)  time: 0.6741 (0.4969 -- 2.3235)  data: 0.1537 (0.0002 -- 1.8036)  max mem: 16413
Epoch: [88] Total time: 0:02:20 (0.8765 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.8008 (1.8023)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0353 (8.9447)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3717 (0.3717)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4186 (2.4186 -- 2.4186)  data: 2.1886 (2.1886 -- 2.1886)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4462 (0.7780)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4271 (0.2006 -- 2.4186)  data: 0.2014 (0.0007 -- 2.1886)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4462 (0.6839)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2243 (0.1695 -- 0.4367)  data: 0.0145 (0.0001 -- 0.2603)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5955 (0.7485)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.0207)  time: 0.2051 (0.1330 -- 0.4367)  data: 0.0134 (0.0001 -- 0.2603)  max mem: 16413
Val: Total time: 0:00:07 (0.2926 s / it)
* Acc@1 80.913 Acc@5 96.058 loss 0.715
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 82.57%
Epoch: [89]  [  0/160]  eta: 0:23:26  lr: 0.000020  min_lr: 0.000000  loss: 1.8905 (1.8905)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7627 (10.7627)  time: 8.7877 (8.7877 -- 8.7877)  data: 8.2854 (8.2854 -- 8.2854)  max mem: 16413
Epoch: [89]  [ 20/160]  eta: 0:02:56  lr: 0.000020  min_lr: 0.000000  loss: 1.7418 (1.8047)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1778 (9.0381)  time: 0.8830 (0.5363 -- 4.6568)  data: 0.3314 (0.0002 -- 4.1240)  max mem: 16413
Epoch: [89]  [ 40/160]  eta: 0:02:09  lr: 0.000020  min_lr: 0.000000  loss: 1.8526 (1.8124)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6305 (9.0129)  time: 0.8820 (0.5139 -- 3.1225)  data: 0.3331 (0.0002 -- 2.5740)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 1.7607 (1.8160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4108 (8.7414)  time: 0.7169 (0.5232 -- 3.3568)  data: 0.1648 (0.0002 -- 2.8115)  max mem: 16413
[2023-08-29 21:18:58,228] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:18:58,228] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:18:58,228] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:18:58,228] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [89]  [ 80/160]  eta: 0:01:17  lr: 0.000020  min_lr: 0.000000  loss: 1.7330 (1.8180)  loss_scale: 32768.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1709 (8.7076)  time: 1.0030 (0.5298 -- 4.8923)  data: 0.4455 (0.0003 -- 4.3724)  max mem: 16413
[2023-08-29 21:19:23,915] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14328
[2023-08-29 21:19:23,915] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14328
[2023-08-29 21:19:23,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:19:23,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:19:23,916] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [89]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.7276 (1.8085)  loss_scale: 16384.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0966 (8.9464)  time: 0.7606 (0.5298 -- 2.2517)  data: 0.1666 (0.0006 -- 1.7444)  max mem: 16413
Epoch: [89]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.6599 (1.7800)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7569 (9.0048)  time: 0.8352 (0.5327 -- 2.7389)  data: 0.0846 (0.0003 -- 0.6495)  max mem: 16413
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 2.0107 (1.8127)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1647 (8.9015)  time: 0.9847 (0.5290 -- 4.7451)  data: 0.4021 (0.0004 -- 4.2228)  max mem: 16413
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7951 (1.8085)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3743 (8.9500)  time: 0.5715 (0.4961 -- 1.2270)  data: 0.0495 (0.0002 -- 0.7095)  max mem: 16413
Epoch: [89] Total time: 0:02:21 (0.8816 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7951 (1.8421)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3743 (8.9500)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3663 (0.3663)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4161 (2.4161 -- 2.4161)  data: 2.1879 (2.1879 -- 2.1879)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4510 (0.7653)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4245 (0.1972 -- 2.4161)  data: 0.2104 (0.0008 -- 2.1879)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5074 (0.6810)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2212 (0.1692 -- 0.3411)  data: 0.0203 (0.0001 -- 0.1410)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5613 (0.7438)  acc1: 85.7143 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2055 (0.1327 -- 0.3411)  data: 0.0201 (0.0001 -- 0.1410)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 81.535 Acc@5 96.888 loss 0.711
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.57%
Epoch: [90]  [  0/160]  eta: 0:19:38  lr: 0.000020  min_lr: 0.000000  loss: 2.1400 (2.1400)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0772 (11.0772)  time: 7.3684 (7.3684 -- 7.3684)  data: 6.8295 (6.8295 -- 6.8295)  max mem: 16413
[2023-08-29 21:20:43,961] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14410
[2023-08-29 21:20:43,961] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14410
[2023-08-29 21:20:43,961] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:20:43,961] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:20:43,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [90]  [ 20/160]  eta: 0:02:42  lr: 0.000020  min_lr: 0.000000  loss: 1.8411 (1.8259)  loss_scale: 8192.0000 (12092.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7410 (8.4423)  time: 0.8541 (0.5339 -- 2.7997)  data: 0.2219 (0.0006 -- 2.2394)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:04  lr: 0.000020  min_lr: 0.000000  loss: 1.9645 (1.8921)  loss_scale: 8192.0000 (10190.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7952 (8.7261)  time: 0.9114 (0.5270 -- 2.4253)  data: 0.2492 (0.0005 -- 1.9080)  max mem: 16413
Epoch: [90]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000000  loss: 1.9261 (1.9145)  loss_scale: 8192.0000 (9534.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2386 (8.8955)  time: 0.8194 (0.5299 -- 3.8047)  data: 0.2424 (0.0002 -- 3.2342)  max mem: 16413
Epoch: [90]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000000  loss: 1.8596 (1.9123)  loss_scale: 8192.0000 (9203.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8599 (8.9227)  time: 0.8117 (0.5284 -- 2.5554)  data: 0.0928 (0.0004 -- 0.6769)  max mem: 16413
Epoch: [90]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.8351 (1.8890)  loss_scale: 8192.0000 (9003.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8937 (8.7419)  time: 0.9265 (0.5334 -- 2.3675)  data: 0.2021 (0.0002 -- 1.5866)  max mem: 16413
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.7277 (1.8660)  loss_scale: 8192.0000 (8869.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7675 (8.8394)  time: 0.8173 (0.5391 -- 2.8606)  data: 0.2173 (0.0004 -- 2.3355)  max mem: 16413
[2023-08-29 21:22:34,686] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:22:34,686] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:22:34,687] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 21:22:34,687] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [90]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.7767 (1.8563)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3046 (8.7548)  time: 0.8460 (0.5211 -- 3.3996)  data: 0.3021 (0.0004 -- 2.8648)  max mem: 16413
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9789 (1.8618)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0209 (8.9136)  time: 0.6334 (0.4984 -- 1.4823)  data: 0.0963 (0.0002 -- 0.9530)  max mem: 16413
Epoch: [90] Total time: 0:02:19 (0.8702 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.9789 (1.8469)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0209 (8.9136)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3132 (0.3132)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4542 (2.4542 -- 2.4542)  data: 2.2149 (2.2149 -- 2.2149)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4869 (0.7716)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4355 (0.1926 -- 2.4542)  data: 0.2185 (0.0006 -- 2.2149)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5294 (0.6861)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2242 (0.1692 -- 0.3870)  data: 0.0195 (0.0001 -- 0.1986)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6403 (0.7493)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.0207)  time: 0.2087 (0.1325 -- 0.3870)  data: 0.0191 (0.0001 -- 0.1986)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 80.290 Acc@5 96.473 loss 0.716
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 82.57%
Epoch: [91]  [  0/160]  eta: 0:20:51  lr: 0.000019  min_lr: 0.000000  loss: 2.1475 (2.1475)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4932 (8.4932)  time: 7.8199 (7.8199 -- 7.8199)  data: 6.2169 (6.2169 -- 6.2169)  max mem: 16413
[2023-08-29 21:23:14,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14572
[2023-08-29 21:23:14,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14572
[2023-08-29 21:23:14,956] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:23:14,956] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:23:14,956] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [91]  [ 20/160]  eta: 0:02:46  lr: 0.000019  min_lr: 0.000000  loss: 1.8306 (1.7613)  loss_scale: 16384.0000 (12873.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0671 (8.4499)  time: 0.8546 (0.5208 -- 5.6834)  data: 0.1294 (0.0004 -- 2.5552)  max mem: 16413
Epoch: [91]  [ 40/160]  eta: 0:02:07  lr: 0.000019  min_lr: 0.000000  loss: 1.8772 (1.7937)  loss_scale: 8192.0000 (10589.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1090 (8.5213)  time: 0.9388 (0.5168 -- 4.0946)  data: 0.0345 (0.0004 -- 0.3428)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:37  lr: 0.000019  min_lr: 0.000000  loss: 2.0502 (1.8557)  loss_scale: 8192.0000 (9803.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5370 (8.5082)  time: 0.7980 (0.5253 -- 3.4728)  data: 0.0015 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [91]  [ 80/160]  eta: 0:01:17  lr: 0.000019  min_lr: 0.000000  loss: 1.6638 (1.8224)  loss_scale: 8192.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8804 (8.4969)  time: 0.9431 (0.5246 -- 4.8240)  data: 0.0013 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [91]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.7851 (1.8161)  loss_scale: 8192.0000 (9165.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0391 (8.6155)  time: 0.7054 (0.5157 -- 3.7540)  data: 0.0019 (0.0003 -- 0.0053)  max mem: 16413
Epoch: [91]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 1.5448 (1.7841)  loss_scale: 8192.0000 (9004.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9766 (8.5357)  time: 1.0337 (0.5233 -- 4.8296)  data: 0.0015 (0.0003 -- 0.0064)  max mem: 16413
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.6999 (1.7704)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3413 (8.5797)  time: 0.8123 (0.5138 -- 3.6227)  data: 0.0012 (0.0005 -- 0.0035)  max mem: 16413
[2023-08-29 21:25:06,442] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:25:06,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 21:25:06,442] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:25:06,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.5498 (1.7543)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1580 (8.7065)  time: 0.6854 (0.4959 -- 2.3685)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [91] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.5498 (1.7807)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1580 (8.7065)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3390 (0.3390)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4146 (2.4146 -- 2.4146)  data: 2.2022 (2.2022 -- 2.2022)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4929 (0.7517)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4209 (0.2094 -- 2.4146)  data: 0.2011 (0.0006 -- 2.2022)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5269 (0.6634)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2191 (0.1687 -- 0.3986)  data: 0.0111 (0.0001 -- 0.2099)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5616 (0.7269)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (95.8506)  time: 0.2023 (0.1330 -- 0.3986)  data: 0.0109 (0.0001 -- 0.2099)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 82.365 Acc@5 96.680 loss 0.697
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.57%
Epoch: [92]  [  0/160]  eta: 0:20:14  lr: 0.000019  min_lr: 0.000000  loss: 1.8132 (1.8132)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.2424 (13.2424)  time: 7.5923 (7.5923 -- 7.5923)  data: 6.9273 (6.9273 -- 6.9273)  max mem: 16413
Epoch: [92]  [ 20/160]  eta: 0:02:48  lr: 0.000019  min_lr: 0.000000  loss: 1.8926 (1.8445)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7906 (8.8003)  time: 0.8852 (0.5320 -- 2.8273)  data: 0.0830 (0.0003 -- 1.1297)  max mem: 16413
Epoch: [92]  [ 40/160]  eta: 0:02:04  lr: 0.000019  min_lr: 0.000000  loss: 1.7829 (1.7838)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4323 (8.7296)  time: 0.8675 (0.5143 -- 2.2933)  data: 0.0864 (0.0003 -- 1.7056)  max mem: 16413
[2023-08-29 21:26:21,821] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14774
[2023-08-29 21:26:21,821] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:26:21,821] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14774
[2023-08-29 21:26:21,821] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-29 21:26:21,821] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [92]  [ 60/160]  eta: 0:01:41  lr: 0.000019  min_lr: 0.000000  loss: 1.5929 (1.7713)  loss_scale: 16384.0000 (15443.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3750 (8.5965)  time: 0.9556 (0.5223 -- 3.1353)  data: 0.0755 (0.0003 -- 1.4894)  max mem: 16413
Epoch: [92]  [ 80/160]  eta: 0:01:15  lr: 0.000019  min_lr: 0.000000  loss: 1.9050 (1.8211)  loss_scale: 8192.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6008 (8.6120)  time: 0.7444 (0.5296 -- 2.2402)  data: 0.0016 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [92]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 1.7726 (1.8051)  loss_scale: 8192.0000 (12571.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0462 (8.8797)  time: 0.9226 (0.5147 -- 3.6849)  data: 0.0173 (0.0005 -- 0.3170)  max mem: 16413
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.7005 (1.7908)  loss_scale: 8192.0000 (11847.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6038 (8.9800)  time: 0.7347 (0.5284 -- 2.8644)  data: 0.0033 (0.0008 -- 0.0194)  max mem: 16413
Epoch: [92]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8155 (1.7908)  loss_scale: 8192.0000 (11329.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4295 (9.0544)  time: 0.9041 (0.5349 -- 2.6554)  data: 0.3231 (0.0009 -- 2.1255)  max mem: 16413
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.7104 (1.7926)  loss_scale: 8192.0000 (10956.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7877 (8.9956)  time: 0.7008 (0.4970 -- 2.8131)  data: 0.1579 (0.0002 -- 2.2978)  max mem: 16413
Epoch: [92] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.7104 (1.8023)  loss_scale: 8192.0000 (10956.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7877 (8.9956)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3354 (0.3354)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4396 (2.4396 -- 2.4396)  data: 2.2238 (2.2238 -- 2.2238)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4947 (0.7593)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4266 (0.1987 -- 2.4396)  data: 0.2123 (0.0008 -- 2.2238)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4947 (0.6751)  acc1: 88.8889 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2178 (0.1712 -- 0.3119)  data: 0.0105 (0.0001 -- 0.1024)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5691 (0.7433)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.0207)  time: 0.2029 (0.1328 -- 0.3119)  data: 0.0103 (0.0001 -- 0.1024)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 80.083 Acc@5 96.473 loss 0.708
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 82.57%
Epoch: [93]  [  0/160]  eta: 0:18:05  lr: 0.000019  min_lr: 0.000000  loss: 1.3750 (1.3750)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6412 (4.6412)  time: 6.7868 (6.7868 -- 6.7868)  data: 5.6424 (5.6424 -- 5.6424)  max mem: 16413
Epoch: [93]  [ 20/160]  eta: 0:02:41  lr: 0.000019  min_lr: 0.000000  loss: 1.8187 (1.7867)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9387 (8.6242)  time: 0.8751 (0.5258 -- 4.1093)  data: 0.1132 (0.0004 -- 1.1127)  max mem: 16413
[2023-08-29 21:28:21,058] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:28:21,058] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:28:21,059] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 21:28:21,059] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [93]  [ 40/160]  eta: 0:02:09  lr: 0.000019  min_lr: 0.000000  loss: 1.6254 (1.7582)  loss_scale: 16384.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2221 (8.4534)  time: 0.9934 (0.5211 -- 3.8296)  data: 0.1704 (0.0005 -- 0.9935)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:39  lr: 0.000019  min_lr: 0.000000  loss: 1.8163 (1.7630)  loss_scale: 16384.0000 (13295.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7906 (8.6357)  time: 0.8373 (0.5241 -- 3.7915)  data: 0.1630 (0.0002 -- 1.6514)  max mem: 16413
Epoch: [93]  [ 80/160]  eta: 0:01:17  lr: 0.000019  min_lr: 0.000000  loss: 1.8306 (1.7722)  loss_scale: 16384.0000 (14057.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7427 (8.7596)  time: 0.8888 (0.5114 -- 3.5631)  data: 0.0087 (0.0002 -- 0.1529)  max mem: 16413
Epoch: [93]  [100/160]  eta: 0:00:57  lr: 0.000019  min_lr: 0.000000  loss: 1.7164 (1.7684)  loss_scale: 16384.0000 (14518.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8951 (8.8149)  time: 0.8833 (0.5266 -- 3.9523)  data: 0.0021 (0.0004 -- 0.0160)  max mem: 16413
[2023-08-29 21:29:45,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=86, lr=[4.4504830425070076e-07, 4.4504830425070076e-07, 5.933977390009343e-07, 5.933977390009343e-07, 7.911969853345792e-07, 7.911969853345792e-07, 1.0549293137794389e-06, 1.0549293137794389e-06, 1.4065724183725851e-06, 1.4065724183725851e-06, 1.8754298911634468e-06, 1.8754298911634468e-06, 2.5005731882179293e-06, 2.5005731882179293e-06, 3.3340975842905722e-06, 3.3340975842905722e-06, 4.445463445720763e-06, 4.445463445720763e-06, 5.9272845942943505e-06, 5.9272845942943505e-06, 7.903046125725801e-06, 7.903046125725801e-06, 1.0537394834301068e-05, 1.0537394834301068e-05, 1.4049859779068091e-05, 1.4049859779068091e-05, 1.8733146372090787e-05, 1.8733146372090787e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 21:29:45,673] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=17.68980519341948, CurrSamplesPerSec=22.45243589577736, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [93]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8478 (1.7851)  loss_scale: 16384.0000 (14826.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7293 (8.7381)  time: 0.7372 (0.5331 -- 2.6254)  data: 0.0017 (0.0003 -- 0.0116)  max mem: 16413
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 2.0328 (1.7958)  loss_scale: 16384.0000 (15047.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7697 (8.8658)  time: 0.8612 (0.5391 -- 4.2304)  data: 0.0014 (0.0003 -- 0.0026)  max mem: 16413
[2023-08-29 21:30:12,449] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:30:12,449] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:30:12,449] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:30:12,449] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.6448 (1.7830)  loss_scale: 16384.0000 (16128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6446 (8.7875)  time: 0.7281 (0.4964 -- 2.7170)  data: 0.0854 (0.0002 -- 0.8995)  max mem: 16413
Epoch: [93] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.6448 (1.8110)  loss_scale: 16384.0000 (16128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6446 (8.7875)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3731 (0.3731)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3335 (2.3335 -- 2.3335)  data: 2.1237 (2.1237 -- 2.1237)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4550 (0.7547)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4156 (0.1969 -- 2.3335)  data: 0.2025 (0.0004 -- 2.1237)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4579 (0.6700)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2250 (0.1690 -- 0.4708)  data: 0.0197 (0.0001 -- 0.2872)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5677 (0.7346)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.4357)  time: 0.2083 (0.1361 -- 0.4708)  data: 0.0195 (0.0001 -- 0.2872)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 80.913 Acc@5 96.680 loss 0.699
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 82.57%
Epoch: [94]  [  0/160]  eta: 0:20:35  lr: 0.000019  min_lr: 0.000000  loss: 2.0502 (2.0502)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4207 (8.4207)  time: 7.7225 (7.7225 -- 7.7225)  data: 7.1855 (7.1855 -- 7.1855)  max mem: 16413
[2023-08-29 21:30:33,594] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15041
[2023-08-29 21:30:33,594] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15041
[2023-08-29 21:30:33,594] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:30:33,594] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:30:33,594] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [94]  [ 20/160]  eta: 0:02:44  lr: 0.000019  min_lr: 0.000000  loss: 1.5779 (1.7277)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0216 (8.3470)  time: 0.8480 (0.5236 -- 3.6912)  data: 0.1424 (0.0007 -- 1.8386)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:01:57  lr: 0.000019  min_lr: 0.000000  loss: 1.8926 (1.7459)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7436 (8.1829)  time: 0.7765 (0.5369 -- 1.9529)  data: 0.1182 (0.0001 -- 1.4239)  max mem: 16413
Epoch: [94]  [ 60/160]  eta: 0:01:37  lr: 0.000019  min_lr: 0.000000  loss: 1.8374 (1.7644)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8343 (8.2496)  time: 0.9629 (0.5175 -- 3.6919)  data: 0.0216 (0.0005 -- 0.3907)  max mem: 16413
Epoch: [94]  [ 80/160]  eta: 0:01:15  lr: 0.000019  min_lr: 0.000000  loss: 1.7148 (1.7575)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7999 (8.4924)  time: 0.8687 (0.5301 -- 3.1068)  data: 0.2652 (0.0004 -- 2.5886)  max mem: 16413
Epoch: [94]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.8741 (1.7777)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0676 (8.4738)  time: 0.7984 (0.5240 -- 2.3380)  data: 0.2396 (0.0006 -- 1.7872)  max mem: 16413
Epoch: [94]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.8206 (1.7973)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5575 (8.5877)  time: 0.8174 (0.5272 -- 2.7063)  data: 0.2438 (0.0004 -- 2.1329)  max mem: 16413
[2023-08-29 21:32:22,726] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:32:22,726] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:32:22,727] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:32:22,727] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [94]  [140/160]  eta: 0:00:17  lr: 0.000018  min_lr: 0.000000  loss: 1.9957 (1.8260)  loss_scale: 32768.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1457 (8.6710)  time: 0.8667 (0.5348 -- 2.0766)  data: 0.2605 (0.0007 -- 1.5519)  max mem: 16413
[2023-08-29 21:32:40,418] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15190
[2023-08-29 21:32:40,418] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15190
[2023-08-29 21:32:40,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:32:40,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:32:40,418] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8067 (1.8218)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8348 (8.6752)  time: 0.7355 (0.4991 -- 2.2446)  data: 0.1268 (0.0002 -- 1.7384)  max mem: 16413
Epoch: [94] Total time: 0:02:19 (0.8747 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8067 (1.8015)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8348 (8.6752)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3656 (0.3656)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4174 (2.4174 -- 2.4174)  data: 2.2065 (2.2065 -- 2.2065)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4615 (0.7520)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4242 (0.1945 -- 2.4174)  data: 0.2033 (0.0007 -- 2.2065)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5078 (0.6741)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2136 (0.1695 -- 0.2639)  data: 0.0039 (0.0001 -- 0.0291)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5530 (0.7365)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.4357)  time: 0.1923 (0.1328 -- 0.2318)  data: 0.0026 (0.0001 -- 0.0291)  max mem: 16413
Val: Total time: 0:00:07 (0.2843 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.703
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.57%
Epoch: [95]  [  0/160]  eta: 0:20:20  lr: 0.000018  min_lr: 0.000000  loss: 1.7504 (1.7504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7206 (6.7206)  time: 7.6288 (7.6288 -- 7.6288)  data: 6.8480 (6.8480 -- 6.8480)  max mem: 16413
Epoch: [95]  [ 20/160]  eta: 0:02:47  lr: 0.000018  min_lr: 0.000000  loss: 2.0153 (1.9138)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6372 (7.9763)  time: 0.8785 (0.5218 -- 4.0072)  data: 0.1625 (0.0005 -- 2.5032)  max mem: 16413
Epoch: [95]  [ 40/160]  eta: 0:02:05  lr: 0.000018  min_lr: 0.000000  loss: 1.7932 (1.8650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4286 (8.3577)  time: 0.8795 (0.5254 -- 4.9069)  data: 0.0022 (0.0003 -- 0.0163)  max mem: 16413
[2023-08-29 21:33:44,055] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15251
[2023-08-29 21:33:44,055] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15251
[2023-08-29 21:33:44,055] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:33:44,055] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:33:44,055] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [95]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.8031 (1.8505)  loss_scale: 8192.0000 (15041.0492)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2116 (8.5284)  time: 0.7985 (0.5356 -- 3.0092)  data: 0.0359 (0.0008 -- 0.3526)  max mem: 16413
Epoch: [95]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 1.7993 (1.8410)  loss_scale: 8192.0000 (13349.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9987 (8.4273)  time: 0.9035 (0.5277 -- 2.8178)  data: 0.1481 (0.0005 -- 2.3033)  max mem: 16413
Epoch: [95]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.7841 (1.8345)  loss_scale: 8192.0000 (12328.5545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5561 (8.4481)  time: 0.8292 (0.5379 -- 2.7198)  data: 0.2322 (0.0004 -- 2.1749)  max mem: 16413
Epoch: [95]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7499 (1.8204)  loss_scale: 8192.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8294 (8.5106)  time: 0.8232 (0.5240 -- 3.2286)  data: 0.2469 (0.0004 -- 2.7040)  max mem: 16413
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.7191 (1.8192)  loss_scale: 8192.0000 (11155.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1432 (8.4911)  time: 0.8596 (0.5308 -- 2.2177)  data: 0.2502 (0.0004 -- 1.4668)  max mem: 16413
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8163 (1.8196)  loss_scale: 8192.0000 (10803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5502 (8.5142)  time: 0.6928 (0.4973 -- 2.5580)  data: 0.1352 (0.0002 -- 1.9971)  max mem: 16413
Epoch: [95] Total time: 0:02:20 (0.8762 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8163 (1.8158)  loss_scale: 8192.0000 (10803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5502 (8.5142)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3246 (0.3246)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4537 (2.4537 -- 2.4537)  data: 2.1625 (2.1625 -- 2.1625)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4572 (0.7530)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4229 (0.1994 -- 2.4537)  data: 0.1988 (0.0007 -- 2.1625)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4572 (0.6565)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (95.7672)  time: 0.2169 (0.1700 -- 0.4155)  data: 0.0129 (0.0001 -- 0.2309)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5526 (0.7229)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (95.0207)  time: 0.2003 (0.1328 -- 0.4155)  data: 0.0126 (0.0001 -- 0.2309)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.689
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.57%
Epoch: [96]  [  0/160]  eta: 0:18:03  lr: 0.000018  min_lr: 0.000000  loss: 1.6336 (1.6336)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3547 (8.3547)  time: 6.7713 (6.7713 -- 6.7713)  data: 5.8045 (5.8045 -- 5.8045)  max mem: 16413
[2023-08-29 21:35:44,497] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:35:44,498] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:35:44,498] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 21:35:44,498] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [96]  [ 20/160]  eta: 0:02:35  lr: 0.000018  min_lr: 0.000000  loss: 1.7905 (1.8881)  loss_scale: 8192.0000 (8582.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7810 (8.8093)  time: 0.8310 (0.5200 -- 3.7325)  data: 0.2745 (0.0003 -- 3.1722)  max mem: 16413
Epoch: [96]  [ 40/160]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000000  loss: 1.6846 (1.7884)  loss_scale: 16384.0000 (12387.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3029 (8.6315)  time: 0.9637 (0.5211 -- 4.2793)  data: 0.3679 (0.0005 -- 3.7392)  max mem: 16413
Epoch: [96]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7503 (1.7702)  loss_scale: 16384.0000 (13698.0984)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0250 (8.6976)  time: 0.8200 (0.5160 -- 2.7025)  data: 0.1513 (0.0003 -- 2.1571)  max mem: 16413
Epoch: [96]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 1.7408 (1.7764)  loss_scale: 16384.0000 (14361.2840)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2269 (8.6627)  time: 0.8893 (0.5229 -- 2.4007)  data: 0.2075 (0.0003 -- 1.8690)  max mem: 16413
[2023-08-29 21:36:50,300] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15455
[2023-08-29 21:36:50,301] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15455
[2023-08-29 21:36:50,301] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:36:50,301] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:36:50,301] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [96]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.6538 (1.7757)  loss_scale: 16384.0000 (14275.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4938 (8.6787)  time: 0.8642 (0.5291 -- 2.0957)  data: 0.1377 (0.0003 -- 1.5487)  max mem: 16413
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7744 (1.7610)  loss_scale: 8192.0000 (13269.6860)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3803 (8.7690)  time: 0.8716 (0.5310 -- 3.2330)  data: 0.0754 (0.0007 -- 1.4752)  max mem: 16413
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.7912 (1.7669)  loss_scale: 8192.0000 (12549.4468)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4977 (8.7864)  time: 0.9130 (0.5290 -- 3.5503)  data: 0.0115 (0.0004 -- 0.2045)  max mem: 16413
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9235 (1.7855)  loss_scale: 8192.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5540 (8.7879)  time: 0.6792 (0.4944 -- 3.5632)  data: 0.0009 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [96] Total time: 0:02:22 (0.8931 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9235 (1.7901)  loss_scale: 8192.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5540 (8.7879)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3308 (0.3308)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4926 (2.4926 -- 2.4926)  data: 2.2208 (2.2208 -- 2.2208)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4464 (0.7495)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4260 (0.2005 -- 2.4926)  data: 0.2035 (0.0007 -- 2.2208)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4594 (0.6623)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.2963)  time: 0.2178 (0.1720 -- 0.4022)  data: 0.0119 (0.0001 -- 0.2164)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5387 (0.7257)  acc1: 85.7143 (82.5726)  acc5: 100.0000 (95.8506)  time: 0.1990 (0.1328 -- 0.4022)  data: 0.0112 (0.0001 -- 0.2164)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 82.573 Acc@5 96.888 loss 0.690
Accuracy of the network on the 482 val images: 82.57%
[2023-08-29 21:37:51,894] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 21:37:51,896] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 21:37:51,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 21:37:51,896] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 21:37:53,323] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 21:37:53,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.57%
Epoch: [97]  [  0/160]  eta: 0:20:56  lr: 0.000018  min_lr: 0.000000  loss: 2.0369 (2.0369)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.9723 (14.9723)  time: 7.8554 (7.8554 -- 7.8554)  data: 7.3239 (7.3239 -- 7.3239)  max mem: 16413
Epoch: [97]  [ 20/160]  eta: 0:02:35  lr: 0.000018  min_lr: 0.000000  loss: 1.7636 (1.7741)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3416 (9.1768)  time: 0.7707 (0.5136 -- 3.8750)  data: 0.1839 (0.0004 -- 3.3466)  max mem: 16413
Epoch: [97]  [ 40/160]  eta: 0:01:59  lr: 0.000018  min_lr: 0.000000  loss: 1.9537 (1.8313)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2459 (8.6914)  time: 0.8763 (0.5304 -- 2.7670)  data: 0.0889 (0.0003 -- 1.1649)  max mem: 16413
Epoch: [97]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7951 (1.8192)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7853 (8.7777)  time: 0.9177 (0.5082 -- 3.6873)  data: 0.2754 (0.0003 -- 3.1830)  max mem: 16413
[2023-08-29 21:38:54,796] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:38:54,796] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 21:38:54,796] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:38:54,796] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [97]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 1.8530 (1.8075)  loss_scale: 16384.0000 (9911.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5798 (8.6015)  time: 0.8508 (0.5213 -- 4.2852)  data: 0.3049 (0.0003 -- 3.7656)  max mem: 16413
Epoch: [97]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.7605 (1.7898)  loss_scale: 16384.0000 (11193.0297)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4527 (8.5542)  time: 0.8978 (0.5204 -- 3.4948)  data: 0.3488 (0.0003 -- 2.9722)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.8692 (1.7960)  loss_scale: 16384.0000 (12051.0413)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7854 (8.5968)  time: 0.8733 (0.5244 -- 4.1436)  data: 0.3259 (0.0003 -- 3.6242)  max mem: 16413
Epoch: [97]  [140/160]  eta: 0:00:17  lr: 0.000018  min_lr: 0.000000  loss: 1.7926 (1.8062)  loss_scale: 16384.0000 (12665.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7599 (8.6108)  time: 0.7490 (0.5244 -- 2.6883)  data: 0.1653 (0.0004 -- 1.7441)  max mem: 16413
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7696 (1.8014)  loss_scale: 16384.0000 (13107.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7785 (8.7308)  time: 0.7132 (0.4975 -- 2.9657)  data: 0.1334 (0.0002 -- 2.4289)  max mem: 16413
Epoch: [97] Total time: 0:02:20 (0.8773 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7696 (1.7970)  loss_scale: 16384.0000 (13107.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7785 (8.7308)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3129 (0.3129)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5224 (2.5224 -- 2.5224)  data: 2.2279 (2.2279 -- 2.2279)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4425 (0.7556)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4256 (0.1919 -- 2.5224)  data: 0.2033 (0.0006 -- 2.2279)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4705 (0.6628)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (95.7672)  time: 0.2123 (0.1694 -- 0.2631)  data: 0.0048 (0.0001 -- 0.0841)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5457 (0.7301)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.0207)  time: 0.1959 (0.1360 -- 0.2631)  data: 0.0046 (0.0001 -- 0.0841)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.691
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.57%
Epoch: [98]  [  0/160]  eta: 0:18:59  lr: 0.000018  min_lr: 0.000000  loss: 1.1425 (1.1425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5758 (10.5758)  time: 7.1211 (7.1211 -- 7.1211)  data: 6.5892 (6.5892 -- 6.5892)  max mem: 16413
Epoch: [98]  [ 20/160]  eta: 0:02:43  lr: 0.000018  min_lr: 0.000000  loss: 1.8114 (1.7671)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2846 (7.8623)  time: 0.8691 (0.5150 -- 4.0429)  data: 0.3135 (0.0008 -- 3.5008)  max mem: 16413
[2023-08-29 21:41:00,249] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:41:00,250] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:41:00,250] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:41:00,250] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [98]  [ 40/160]  eta: 0:02:06  lr: 0.000018  min_lr: 0.000000  loss: 1.8390 (1.8128)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5646 (8.1488)  time: 0.9304 (0.5117 -- 5.6310)  data: 0.2672 (0.0004 -- 2.8881)  max mem: 16413
Epoch: [98]  [ 60/160]  eta: 0:01:39  lr: 0.000018  min_lr: 0.000000  loss: 1.8503 (1.7891)  loss_scale: 32768.0000 (24173.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8022 (8.4164)  time: 0.8714 (0.5259 -- 5.5655)  data: 0.3211 (0.0001 -- 5.0572)  max mem: 16413
[2023-08-29 21:41:34,172] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15754
[2023-08-29 21:41:34,172] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15754
[2023-08-29 21:41:34,172] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:41:34,172] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:41:34,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [98]  [ 80/160]  eta: 0:01:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7227 (1.7996)  loss_scale: 32768.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8300 (8.3188)  time: 0.9545 (0.5273 -- 4.1011)  data: 0.4018 (0.0004 -- 3.5613)  max mem: 16413
Epoch: [98]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000000  loss: 1.6873 (1.7708)  loss_scale: 16384.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0154 (8.5227)  time: 0.7887 (0.5177 -- 3.7561)  data: 0.2322 (0.0002 -- 3.2480)  max mem: 16413
Epoch: [98]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.8664 (1.7755)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9406 (8.4834)  time: 0.9032 (0.5318 -- 3.8386)  data: 0.3549 (0.0004 -- 3.3020)  max mem: 16413
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.8172 (1.7865)  loss_scale: 16384.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8996 (8.5324)  time: 0.7222 (0.5157 -- 2.8659)  data: 0.1720 (0.0004 -- 2.3386)  max mem: 16413
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.9094 (1.7942)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6107 (8.6322)  time: 0.7331 (0.4976 -- 2.3205)  data: 0.2081 (0.0002 -- 1.7986)  max mem: 16413
Epoch: [98] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.9094 (1.7940)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6107 (8.6322)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2860 (0.2860)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4211 (2.4211 -- 2.4211)  data: 2.1912 (2.1912 -- 2.1912)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4505 (0.7498)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4263 (0.2137 -- 2.4211)  data: 0.2048 (0.0004 -- 2.1912)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4520 (0.6585)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2177 (0.1689 -- 0.3049)  data: 0.0096 (0.0001 -- 0.1217)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5448 (0.7285)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (95.0207)  time: 0.2001 (0.1329 -- 0.3049)  data: 0.0093 (0.0001 -- 0.1217)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 82.158 Acc@5 96.473 loss 0.684
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.57%
Epoch: [99]  [  0/160]  eta: 0:18:08  lr: 0.000017  min_lr: 0.000000  loss: 2.1369 (2.1369)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1304 (7.1304)  time: 6.8034 (6.8034 -- 6.8034)  data: 5.7182 (5.7182 -- 5.7182)  max mem: 16413
Epoch: [99]  [ 20/160]  eta: 0:02:34  lr: 0.000017  min_lr: 0.000000  loss: 1.8090 (1.7583)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5089 (9.4331)  time: 0.8187 (0.5309 -- 2.4157)  data: 0.1632 (0.0004 -- 1.2700)  max mem: 16413
Epoch: [99]  [ 40/160]  eta: 0:02:08  lr: 0.000017  min_lr: 0.000000  loss: 1.8321 (1.8016)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1950 (9.0892)  time: 1.0336 (0.5292 -- 3.7736)  data: 0.1938 (0.0005 -- 3.2100)  max mem: 16413
[2023-08-29 21:43:36,881] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:43:36,881] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:43:36,881] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:43:36,881] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [99]  [ 60/160]  eta: 0:01:36  lr: 0.000017  min_lr: 0.000000  loss: 1.8665 (1.8240)  loss_scale: 32768.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9184 (8.7992)  time: 0.7364 (0.5266 -- 2.6737)  data: 0.0016 (0.0002 -- 0.0044)  max mem: 16413
[2023-08-29 21:43:56,375] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15907
[2023-08-29 21:43:56,375] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15907
[2023-08-29 21:43:56,375] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:43:56,375] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:43:56,376] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [99]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 1.7586 (1.7781)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9162 (8.8106)  time: 0.9182 (0.5287 -- 3.2493)  data: 0.0784 (0.0003 -- 0.7149)  max mem: 16413
[2023-08-29 21:44:14,365] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15928
[2023-08-29 21:44:14,365] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15928
[2023-08-29 21:44:14,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:44:14,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:44:14,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [99]  [100/160]  eta: 0:00:54  lr: 0.000017  min_lr: 0.000000  loss: 1.8580 (1.7740)  loss_scale: 8192.0000 (19222.8119)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8032 (8.9286)  time: 0.7662 (0.5298 -- 2.1590)  data: 0.1807 (0.0004 -- 1.6441)  max mem: 16413
Epoch: [99]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.7258 (1.7657)  loss_scale: 8192.0000 (17399.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9238 (8.8653)  time: 0.8341 (0.5250 -- 2.7847)  data: 0.2112 (0.0003 -- 2.2298)  max mem: 16413
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7437 (1.7573)  loss_scale: 8192.0000 (16093.5035)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4353 (8.7573)  time: 0.9947 (0.5138 -- 3.8035)  data: 0.0921 (0.0003 -- 1.8187)  max mem: 16413
[2023-08-29 21:45:11,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=93, lr=[4.0603552109092804e-07, 4.0603552109092804e-07, 5.413806947879041e-07, 5.413806947879041e-07, 7.218409263838721e-07, 7.218409263838721e-07, 9.624545685118294e-07, 9.624545685118294e-07, 1.2832727580157725e-06, 1.2832727580157725e-06, 1.71103034402103e-06, 1.71103034402103e-06, 2.2813737920280403e-06, 2.2813737920280403e-06, 3.0418317227040533e-06, 3.0418317227040533e-06, 4.055775630272071e-06, 4.055775630272071e-06, 5.407700840362762e-06, 5.407700840362762e-06, 7.210267787150349e-06, 7.210267787150349e-06, 9.613690382867132e-06, 9.613690382867132e-06, 1.2818253843822842e-05, 1.2818253843822842e-05, 1.7091005125097124e-05, 1.7091005125097124e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 21:45:11,992] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=17.74426341740832, CurrSamplesPerSec=23.907954775401787, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.7893 (1.7523)  loss_scale: 8192.0000 (15155.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2962 (8.7986)  time: 0.6141 (0.4965 -- 2.3139)  data: 0.0902 (0.0002 -- 1.7906)  max mem: 16413
Epoch: [99] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.7893 (1.7796)  loss_scale: 8192.0000 (15155.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2962 (8.7986)
[2023-08-29 21:45:11,997] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-08-29 21:45:11,999] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-08-29 21:45:12,000] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-08-29 21:45:12,001] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-08-29 21:45:13,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-08-29 21:45:13,053] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2947 (0.2947)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1555 (2.1555 -- 2.1555)  data: 1.9306 (1.9306 -- 1.9306)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4666 (0.7410)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4396 (0.2023 -- 2.1555)  data: 0.2166 (0.0009 -- 1.9306)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4460 (0.6460)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2342 (0.1695 -- 0.4385)  data: 0.0241 (0.0001 -- 0.2212)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5442 (0.7174)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.4357)  time: 0.2140 (0.1359 -- 0.4385)  data: 0.0227 (0.0001 -- 0.2212)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 81.535 Acc@5 96.888 loss 0.687
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.57%
Epoch: [100]  [  0/160]  eta: 0:19:43  lr: 0.000017  min_lr: 0.000000  loss: 2.0503 (2.0503)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5190 (7.5190)  time: 7.3955 (7.3955 -- 7.3955)  data: 5.6808 (5.6808 -- 5.6808)  max mem: 16413
Epoch: [100]  [ 20/160]  eta: 0:02:44  lr: 0.000017  min_lr: 0.000000  loss: 1.6584 (1.7806)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0825 (8.3352)  time: 0.8614 (0.5346 -- 3.7773)  data: 0.0016 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [100]  [ 40/160]  eta: 0:02:12  lr: 0.000017  min_lr: 0.000000  loss: 1.5809 (1.7312)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7498 (8.5464)  time: 1.0323 (0.5200 -- 4.6532)  data: 0.0019 (0.0007 -- 0.0101)  max mem: 16413
[2023-08-29 21:46:20,693] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:46:20,693] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 21:46:20,695] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:46:20,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [100]  [ 60/160]  eta: 0:01:40  lr: 0.000017  min_lr: 0.000000  loss: 1.8319 (1.7773)  loss_scale: 8192.0000 (8729.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9461 (8.4519)  time: 0.8065 (0.5220 -- 4.0207)  data: 0.0018 (0.0003 -- 0.0065)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:19  lr: 0.000017  min_lr: 0.000000  loss: 1.8925 (1.7793)  loss_scale: 16384.0000 (10619.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6611 (8.8006)  time: 0.9590 (0.5150 -- 4.5889)  data: 0.0013 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [100]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.8079 (1.7933)  loss_scale: 16384.0000 (11760.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9007 (8.7775)  time: 0.6691 (0.5200 -- 2.9991)  data: 0.0015 (0.0004 -- 0.0097)  max mem: 16413
Epoch: [100]  [120/160]  eta: 0:00:38  lr: 0.000017  min_lr: 0.000000  loss: 1.7993 (1.7973)  loss_scale: 16384.0000 (12524.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5653 (8.8097)  time: 1.0619 (0.5402 -- 6.1925)  data: 0.0019 (0.0006 -- 0.0076)  max mem: 16413
Epoch: [100]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.8738 (1.7991)  loss_scale: 16384.0000 (13072.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2156 (8.6900)  time: 0.7714 (0.5097 -- 4.0342)  data: 0.0014 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.9130 (1.8120)  loss_scale: 16384.0000 (13465.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5522 (8.6926)  time: 0.6350 (0.4951 -- 2.5565)  data: 0.0006 (0.0002 -- 0.0012)  max mem: 16413
Epoch: [100] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.9130 (1.8182)  loss_scale: 16384.0000 (13465.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5522 (8.6926)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3193 (0.3193)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4151 (2.4151 -- 2.4151)  data: 2.1990 (2.1990 -- 2.1990)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4337 (0.7307)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (95.9596)  time: 0.4165 (0.1868 -- 2.4151)  data: 0.2008 (0.0003 -- 2.1990)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4337 (0.6409)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (95.7672)  time: 0.2185 (0.1706 -- 0.4639)  data: 0.0144 (0.0001 -- 0.2754)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5580 (0.7036)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (95.0207)  time: 0.2050 (0.1330 -- 0.4639)  data: 0.0141 (0.0001 -- 0.2754)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 82.988 Acc@5 96.680 loss 0.678
Accuracy of the network on the 482 val images: 82.99%
[2023-08-29 21:47:51,652] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 21:47:51,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 21:47:51,654] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 21:47:51,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 21:47:53,058] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 21:47:53,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.99%
Epoch: [101]  [  0/160]  eta: 0:19:46  lr: 0.000017  min_lr: 0.000000  loss: 1.5535 (1.5535)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.4421 (13.4421)  time: 7.4157 (7.4157 -- 7.4157)  data: 6.8822 (6.8822 -- 6.8822)  max mem: 16413
Epoch: [101]  [ 20/160]  eta: 0:02:43  lr: 0.000017  min_lr: 0.000000  loss: 1.8774 (1.7881)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3447 (9.2100)  time: 0.8535 (0.5182 -- 2.4482)  data: 0.1439 (0.0005 -- 1.3644)  max mem: 16413
[2023-08-29 21:48:20,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:48:20,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:48:20,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:48:20,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:48:23,287] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16187
[2023-08-29 21:48:23,287] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:48:23,287] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16187
[2023-08-29 21:48:23,287] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:48:23,287] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [101]  [ 40/160]  eta: 0:02:02  lr: 0.000017  min_lr: 0.000000  loss: 1.7741 (1.7607)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5691 (8.6197)  time: 0.8762 (0.5181 -- 2.7901)  data: 0.1250 (0.0004 -- 1.6739)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:39  lr: 0.000017  min_lr: 0.000000  loss: 1.7421 (1.7495)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5836 (8.4423)  time: 0.9462 (0.5262 -- 3.2887)  data: 0.0576 (0.0004 -- 1.1214)  max mem: 16413
Epoch: [101]  [ 80/160]  eta: 0:01:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7409 (1.7427)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5214 (8.4767)  time: 0.9303 (0.5089 -- 4.8210)  data: 0.0013 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [101]  [100/160]  eta: 0:00:57  lr: 0.000017  min_lr: 0.000000  loss: 1.7964 (1.7624)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0309 (8.5512)  time: 0.8238 (0.5186 -- 3.4198)  data: 0.0014 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [101]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.6353 (1.7526)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1082 (8.6879)  time: 0.8827 (0.5191 -- 4.4034)  data: 0.0016 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7649 (1.7554)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4770 (8.7284)  time: 0.8630 (0.5063 -- 4.0992)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
[2023-08-29 21:50:14,051] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:50:14,051] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:50:14,051] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:50:14,052] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.7232 (1.7517)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2551 (8.6637)  time: 0.6047 (0.4968 -- 2.2372)  data: 0.0007 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [101] Total time: 0:02:22 (0.8907 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.7232 (1.7945)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2551 (8.6637)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2685 (0.2685)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5331 (2.5331 -- 2.5331)  data: 2.2718 (2.2718 -- 2.2718)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4351 (0.7427)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4210 (0.1961 -- 2.5331)  data: 0.2076 (0.0007 -- 2.2718)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4375 (0.6484)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2107 (0.1698 -- 0.3540)  data: 0.0092 (0.0001 -- 0.1679)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5408 (0.7153)  acc1: 85.7143 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.1964 (0.1328 -- 0.3540)  data: 0.0089 (0.0001 -- 0.1679)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 81.743 Acc@5 97.095 loss 0.679
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 82.99%
Epoch: [102]  [  0/160]  eta: 0:22:07  lr: 0.000017  min_lr: 0.000000  loss: 1.9014 (1.9014)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9889 (8.9889)  time: 8.2958 (8.2958 -- 8.2958)  data: 7.7833 (7.7833 -- 7.7833)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:44  lr: 0.000017  min_lr: 0.000000  loss: 1.6280 (1.7677)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1346 (8.5429)  time: 0.8159 (0.5189 -- 4.6641)  data: 0.2705 (0.0002 -- 4.1482)  max mem: 16413
Epoch: [102]  [ 40/160]  eta: 0:02:02  lr: 0.000016  min_lr: 0.000000  loss: 1.8000 (1.7921)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0862 (8.5256)  time: 0.8537 (0.5272 -- 2.8029)  data: 0.3004 (0.0008 -- 2.2615)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:37  lr: 0.000016  min_lr: 0.000000  loss: 1.7559 (1.7769)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5497 (8.5275)  time: 0.8818 (0.5374 -- 2.1907)  data: 0.2926 (0.0004 -- 1.6684)  max mem: 16413
[2023-08-29 21:51:23,751] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16382
[2023-08-29 21:51:23,751] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:51:23,751] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16382
[2023-08-29 21:51:23,751] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:51:23,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [102]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000000  loss: 1.8274 (1.7913)  loss_scale: 16384.0000 (28924.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7717 (8.6236)  time: 0.9234 (0.5359 -- 3.8925)  data: 0.3706 (0.0004 -- 3.3435)  max mem: 16413
Epoch: [102]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.7302 (1.7941)  loss_scale: 16384.0000 (26441.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5702 (8.5076)  time: 0.9017 (0.5185 -- 3.1434)  data: 0.3598 (0.0004 -- 2.5959)  max mem: 16413
Epoch: [102]  [120/160]  eta: 0:00:37  lr: 0.000016  min_lr: 0.000000  loss: 1.8637 (1.8087)  loss_scale: 16384.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7100 (8.5908)  time: 0.8286 (0.5263 -- 3.0262)  data: 0.2243 (0.0003 -- 2.4962)  max mem: 16413
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.8215 (1.8009)  loss_scale: 16384.0000 (23588.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5986 (8.4697)  time: 0.8789 (0.5308 -- 3.1754)  data: 0.3300 (0.0005 -- 2.6618)  max mem: 16413
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7932 (1.7977)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3201 (8.4704)  time: 0.6575 (0.5005 -- 3.1931)  data: 0.1349 (0.0002 -- 2.6814)  max mem: 16413
Epoch: [102] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7932 (1.7876)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3201 (8.4704)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2977 (0.2977)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1638 (2.1638 -- 2.1638)  data: 1.9648 (1.9648 -- 1.9648)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4396 (0.7321)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4019 (0.1993 -- 2.1638)  data: 0.1889 (0.0006 -- 1.9648)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4396 (0.6502)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2289 (0.1703 -- 0.4530)  data: 0.0235 (0.0001 -- 0.2454)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5588 (0.7108)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2131 (0.1328 -- 0.4530)  data: 0.0232 (0.0001 -- 0.2454)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 81.328 Acc@5 97.095 loss 0.680
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 82.99%
Epoch: [103]  [  0/160]  eta: 0:19:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7035 (1.7035)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2214 (12.2214)  time: 7.3518 (7.3518 -- 7.3518)  data: 5.7556 (5.7556 -- 5.7556)  max mem: 16413
Epoch: [103]  [ 20/160]  eta: 0:02:45  lr: 0.000016  min_lr: 0.000000  loss: 1.8850 (1.8277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5790 (8.9258)  time: 0.8720 (0.5248 -- 2.6982)  data: 0.1548 (0.0008 -- 1.8330)  max mem: 16413
[2023-08-29 21:53:26,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:53:26,142] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:53:26,143] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:53:26,143] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:53:27,491] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16512
[2023-08-29 21:53:27,491] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16512
[2023-08-29 21:53:27,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:53:27,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:53:27,492] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [103]  [ 40/160]  eta: 0:01:58  lr: 0.000016  min_lr: 0.000000  loss: 1.9767 (1.8415)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2590 (8.3698)  time: 0.7808 (0.5348 -- 1.7846)  data: 0.1094 (0.0003 -- 1.2520)  max mem: 16413
Epoch: [103]  [ 60/160]  eta: 0:01:40  lr: 0.000016  min_lr: 0.000000  loss: 1.8517 (1.8300)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4234 (8.4008)  time: 1.0560 (0.5300 -- 4.4279)  data: 0.0591 (0.0002 -- 0.7537)  max mem: 16413
Epoch: [103]  [ 80/160]  eta: 0:01:17  lr: 0.000016  min_lr: 0.000000  loss: 1.6633 (1.7954)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2559 (8.3996)  time: 0.8360 (0.5134 -- 3.4900)  data: 0.0014 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.9110 (1.8095)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2249 (8.4436)  time: 0.8784 (0.5258 -- 3.6659)  data: 0.0013 (0.0003 -- 0.0059)  max mem: 16413
Epoch: [103]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.8617 (1.8056)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6470 (8.5782)  time: 0.7569 (0.5336 -- 3.1213)  data: 0.0018 (0.0004 -- 0.0056)  max mem: 16413
Epoch: [103]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.8559 (1.8027)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7663 (8.5895)  time: 0.8980 (0.5301 -- 3.6467)  data: 0.0022 (0.0009 -- 0.0063)  max mem: 16413
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7579 (1.8006)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1296 (8.5556)  time: 0.7505 (0.4976 -- 2.5396)  data: 0.0007 (0.0003 -- 0.0018)  max mem: 16413
Epoch: [103] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7579 (1.7715)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1296 (8.5556)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2703 (0.2703)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4720 (2.4720 -- 2.4720)  data: 2.2584 (2.2584 -- 2.2584)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4488 (0.7359)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4342 (0.1912 -- 2.4720)  data: 0.2250 (0.0006 -- 2.2584)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4352 (0.6435)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2194 (0.1698 -- 0.4129)  data: 0.0176 (0.0001 -- 0.2072)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5488 (0.7120)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.4357)  time: 0.2062 (0.1328 -- 0.4129)  data: 0.0173 (0.0001 -- 0.2072)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 81.743 Acc@5 96.888 loss 0.677
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 82.99%
Epoch: [104]  [  0/160]  eta: 0:19:34  lr: 0.000016  min_lr: 0.000000  loss: 2.0648 (2.0648)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9061 (10.9061)  time: 7.3420 (7.3420 -- 7.3420)  data: 6.7678 (6.7678 -- 6.7678)  max mem: 16413
[2023-08-29 21:55:31,852] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:55:31,852] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:55:31,852] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:55:31,852] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:55:47,563] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16657
[2023-08-29 21:55:47,563] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16657
[2023-08-29 21:55:47,563] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:55:47,563] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:55:47,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [104]  [ 20/160]  eta: 0:02:51  lr: 0.000016  min_lr: 0.000000  loss: 1.6989 (1.7677)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0037 (8.6908)  time: 0.9210 (0.5226 -- 4.5147)  data: 0.1021 (0.0009 -- 1.4135)  max mem: 16413
Epoch: [104]  [ 40/160]  eta: 0:02:09  lr: 0.000016  min_lr: 0.000000  loss: 1.7147 (1.7559)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7926 (8.4787)  time: 0.9284 (0.5204 -- 3.1899)  data: 0.0023 (0.0005 -- 0.0168)  max mem: 16413
Epoch: [104]  [ 60/160]  eta: 0:01:43  lr: 0.000016  min_lr: 0.000000  loss: 1.8010 (1.7810)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1136 (8.6282)  time: 0.9349 (0.5277 -- 4.1756)  data: 0.0019 (0.0006 -- 0.0078)  max mem: 16413
Epoch: [104]  [ 80/160]  eta: 0:01:17  lr: 0.000016  min_lr: 0.000000  loss: 1.8612 (1.7591)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0658 (8.3305)  time: 0.7789 (0.5125 -- 3.9152)  data: 0.0016 (0.0003 -- 0.0082)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.7022 (1.7511)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3135 (8.3307)  time: 0.8418 (0.5100 -- 4.0452)  data: 0.0022 (0.0003 -- 0.0144)  max mem: 16413
Epoch: [104]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 2.0560 (1.7845)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5359 (8.3736)  time: 0.7057 (0.5332 -- 2.2809)  data: 0.0018 (0.0003 -- 0.0068)  max mem: 16413
Epoch: [104]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.7440 (1.7877)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9115 (8.4995)  time: 0.9088 (0.5315 -- 2.5365)  data: 0.0854 (0.0004 -- 1.2812)  max mem: 16413
[2023-08-29 21:57:36,850] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:57:36,850] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:57:36,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:57:36,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 21:57:43,687] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16797
[2023-08-29 21:57:43,687] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16797
[2023-08-29 21:57:43,687] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:57:43,687] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 21:57:43,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7370 (1.7810)  loss_scale: 32768.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1742 (8.5202)  time: 0.7563 (0.4869 -- 2.5939)  data: 0.0695 (0.0002 -- 0.8606)  max mem: 16413
Epoch: [104] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7370 (1.7792)  loss_scale: 32768.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1742 (8.5202)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2898 (0.2898)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3035 (2.3035 -- 2.3035)  data: 2.0793 (2.0793 -- 2.0793)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4628 (0.7327)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4012 (0.1878 -- 2.3035)  data: 0.1904 (0.0002 -- 2.0793)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4565 (0.6308)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.2963)  time: 0.2242 (0.1710 -- 0.4778)  data: 0.0190 (0.0002 -- 0.2929)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5278 (0.7014)  acc1: 85.7143 (80.9129)  acc5: 100.0000 (95.4357)  time: 0.2117 (0.1329 -- 0.4778)  data: 0.0187 (0.0001 -- 0.2929)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 82.573 Acc@5 96.266 loss 0.668
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [105]  [  0/160]  eta: 0:21:25  lr: 0.000016  min_lr: 0.000000  loss: 1.5951 (1.5951)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1668 (13.1668)  time: 8.0337 (8.0337 -- 8.0337)  data: 7.4800 (7.4800 -- 7.4800)  max mem: 16413
[2023-08-29 21:58:04,061] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16806
[2023-08-29 21:58:04,061] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16806
[2023-08-29 21:58:04,062] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:58:04,062] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 21:58:04,062] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [105]  [ 20/160]  eta: 0:02:46  lr: 0.000016  min_lr: 0.000000  loss: 1.6654 (1.5911)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6025 (8.4922)  time: 0.8456 (0.5160 -- 3.7244)  data: 0.3000 (0.0003 -- 3.2027)  max mem: 16413
Epoch: [105]  [ 40/160]  eta: 0:02:06  lr: 0.000016  min_lr: 0.000000  loss: 1.8610 (1.7440)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1641 (9.3956)  time: 0.9167 (0.5318 -- 3.5623)  data: 0.0703 (0.0002 -- 1.3814)  max mem: 16413
Epoch: [105]  [ 60/160]  eta: 0:01:37  lr: 0.000016  min_lr: 0.000000  loss: 1.7938 (1.7510)  loss_scale: 8192.0000 (8997.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5009 (9.1485)  time: 0.7998 (0.5357 -- 3.4315)  data: 0.0177 (0.0002 -- 0.3197)  max mem: 16413
Epoch: [105]  [ 80/160]  eta: 0:01:15  lr: 0.000016  min_lr: 0.000000  loss: 2.0409 (1.7915)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4154 (9.2751)  time: 0.8741 (0.5276 -- 3.6730)  data: 0.0022 (0.0003 -- 0.0137)  max mem: 16413
Epoch: [105]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.7587 (1.7728)  loss_scale: 8192.0000 (8678.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6620 (9.1585)  time: 0.9306 (0.5130 -- 4.2567)  data: 0.0020 (0.0002 -- 0.0108)  max mem: 16413
Epoch: [105]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.6133 (1.7688)  loss_scale: 8192.0000 (8598.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9428 (9.0827)  time: 0.7886 (0.5280 -- 3.1122)  data: 0.0017 (0.0002 -- 0.0046)  max mem: 16413
[2023-08-29 21:59:56,846] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:59:56,846] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 21:59:56,847] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 21:59:56,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [105]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.6870 (1.7656)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0705 (8.9743)  time: 0.9266 (0.5255 -- 3.3401)  data: 0.0011 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9150 (1.7665)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3930 (9.0155)  time: 0.6395 (0.4975 -- 2.9225)  data: 0.0011 (0.0002 -- 0.0144)  max mem: 16413
Epoch: [105] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9150 (1.7713)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3930 (9.0155)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2450 (0.2450)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3294 (2.3294 -- 2.3294)  data: 2.0994 (2.0994 -- 2.0994)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4364 (0.7473)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4106 (0.2041 -- 2.3294)  data: 0.1921 (0.0007 -- 2.0994)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4364 (0.6445)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2159 (0.1690 -- 0.3699)  data: 0.0096 (0.0001 -- 0.1623)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5466 (0.7101)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (95.8506)  time: 0.1996 (0.1329 -- 0.3699)  data: 0.0092 (0.0001 -- 0.1623)  max mem: 16413
Val: Total time: 0:00:07 (0.2846 s / it)
* Acc@1 83.610 Acc@5 97.095 loss 0.673
Accuracy of the network on the 482 val images: 83.61%
[2023-08-29 22:00:22,547] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 22:00:22,549] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 22:00:22,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 22:00:22,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 22:00:23,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 22:00:23,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.61%
Epoch: [106]  [  0/160]  eta: 0:15:48  lr: 0.000016  min_lr: 0.000000  loss: 1.2080 (1.2080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8230 (13.8230)  time: 5.9292 (5.9292 -- 5.9292)  data: 5.3821 (5.3821 -- 5.3821)  max mem: 16413
Epoch: [106]  [ 20/160]  eta: 0:02:45  lr: 0.000015  min_lr: 0.000000  loss: 1.8221 (1.7605)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4039 (8.9526)  time: 0.9459 (0.5136 -- 3.6328)  data: 0.0017 (0.0004 -- 0.0039)  max mem: 16413
[2023-08-29 22:01:04,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=99, lr=[3.6686357933160477e-07, 3.6686357933160477e-07, 4.891514391088064e-07, 4.891514391088064e-07, 6.522019188117419e-07, 6.522019188117419e-07, 8.696025584156558e-07, 8.696025584156558e-07, 1.159470077887541e-06, 1.159470077887541e-06, 1.5459601038500549e-06, 1.5459601038500549e-06, 2.0612801384667395e-06, 2.0612801384667395e-06, 2.748373517955653e-06, 2.748373517955653e-06, 3.6644980239408706e-06, 3.6644980239408706e-06, 4.885997365254494e-06, 4.885997365254494e-06, 6.514663153672659e-06, 6.514663153672659e-06, 8.686217538230212e-06, 8.686217538230212e-06, 1.158162338430695e-05, 1.158162338430695e-05, 1.5442164512409265e-05, 1.5442164512409265e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 22:01:04,174] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=17.61200206811615, CurrSamplesPerSec=22.175023185423914, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:01:59  lr: 0.000015  min_lr: 0.000000  loss: 1.9221 (1.8391)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4339 (8.6423)  time: 0.8058 (0.5176 -- 4.8583)  data: 0.0020 (0.0001 -- 0.0117)  max mem: 16413
Epoch: [106]  [ 60/160]  eta: 0:01:39  lr: 0.000015  min_lr: 0.000000  loss: 1.7934 (1.8191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7747 (8.5430)  time: 0.9869 (0.5331 -- 3.8772)  data: 0.0014 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:13  lr: 0.000015  min_lr: 0.000000  loss: 1.6709 (1.7849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1458 (8.5952)  time: 0.6969 (0.5286 -- 3.4362)  data: 0.0018 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.7991 (1.7617)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4975 (8.6640)  time: 0.9553 (0.5314 -- 4.5757)  data: 0.0015 (0.0004 -- 0.0034)  max mem: 16413
[2023-08-29 22:02:01,098] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:02:01,098] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:02:01,098] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:02:01,098] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [106]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.6417 (1.7444)  loss_scale: 32768.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7124 (8.5485)  time: 0.8748 (0.5280 -- 2.5744)  data: 0.0081 (0.0004 -- 0.1334)  max mem: 16413
[2023-08-29 22:02:18,495] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17084
[2023-08-29 22:02:18,495] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17084
[2023-08-29 22:02:18,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:02:18,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:02:18,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.9230 (1.7614)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9501 (8.5081)  time: 0.8995 (0.5146 -- 2.7471)  data: 0.0724 (0.0001 -- 1.4042)  max mem: 16413
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.9219 (1.7740)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3705 (8.5408)  time: 0.6402 (0.4944 -- 1.7647)  data: 0.1078 (0.0001 -- 1.2083)  max mem: 16413
Epoch: [106] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.9219 (1.7880)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3705 (8.5408)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2607 (0.2607)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4511 (2.4511 -- 2.4511)  data: 2.1767 (2.1767 -- 2.1767)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4485 (0.7580)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4288 (0.2122 -- 2.4511)  data: 0.2017 (0.0005 -- 2.1767)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4759 (0.6632)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2197 (0.1743 -- 0.3025)  data: 0.0082 (0.0001 -- 0.1186)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5515 (0.7283)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (95.4357)  time: 0.2033 (0.1371 -- 0.3025)  data: 0.0079 (0.0001 -- 0.1186)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 82.988 Acc@5 96.680 loss 0.686
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 83.61%
Epoch: [107]  [  0/160]  eta: 0:20:29  lr: 0.000015  min_lr: 0.000000  loss: 1.4829 (1.4829)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7382 (5.7382)  time: 7.6830 (7.6830 -- 7.6830)  data: 7.1098 (7.1098 -- 7.1098)  max mem: 16413
[2023-08-29 22:03:02,465] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17123
[2023-08-29 22:03:02,465] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17123
[2023-08-29 22:03:02,465] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:03:02,465] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:03:02,465] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [107]  [ 20/160]  eta: 0:02:43  lr: 0.000015  min_lr: 0.000000  loss: 1.7694 (1.7764)  loss_scale: 8192.0000 (9362.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9464 (8.3540)  time: 0.8457 (0.5239 -- 4.5205)  data: 0.2809 (0.0004 -- 4.0182)  max mem: 16413
Epoch: [107]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000000  loss: 1.7392 (1.8244)  loss_scale: 8192.0000 (8791.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1497 (8.6869)  time: 0.9201 (0.5260 -- 3.9715)  data: 0.3702 (0.0003 -- 3.4332)  max mem: 16413
[2023-08-29 22:03:46,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17175
[2023-08-29 22:03:46,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17175
[2023-08-29 22:03:46,204] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-29 22:03:46,204] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-29 22:03:46,204] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [107]  [ 60/160]  eta: 0:01:35  lr: 0.000015  min_lr: 0.000000  loss: 1.7880 (1.8276)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3010 (8.8483)  time: 0.7669 (0.5216 -- 3.1102)  data: 0.2101 (0.0004 -- 2.5787)  max mem: 16413
Epoch: [107]  [ 80/160]  eta: 0:01:14  lr: 0.000015  min_lr: 0.000000  loss: 1.7140 (1.7979)  loss_scale: 4096.0000 (7180.6420)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0919 (8.7616)  time: 0.8526 (0.5322 -- 2.6782)  data: 0.2384 (0.0004 -- 2.1703)  max mem: 16413
Epoch: [107]  [100/160]  eta: 0:00:54  lr: 0.000015  min_lr: 0.000000  loss: 1.7405 (1.8160)  loss_scale: 4096.0000 (6569.8218)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6029 (8.5752)  time: 0.8257 (0.5275 -- 2.5926)  data: 0.1897 (0.0004 -- 2.0657)  max mem: 16413
Epoch: [107]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.6218 (1.7933)  loss_scale: 4096.0000 (6160.9256)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2787 (8.5570)  time: 0.9269 (0.5170 -- 3.0798)  data: 0.0015 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [107]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.8722 (1.8015)  loss_scale: 4096.0000 (5868.0284)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3507 (8.5991)  time: 0.8449 (0.5288 -- 3.2150)  data: 0.0013 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.7421 (1.7847)  loss_scale: 4096.0000 (5657.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0707 (8.7092)  time: 0.6770 (0.4961 -- 2.2551)  data: 0.0168 (0.0002 -- 0.3202)  max mem: 16413
Epoch: [107] Total time: 0:02:20 (0.8775 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.7421 (1.7701)  loss_scale: 4096.0000 (5657.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0707 (8.7092)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2708 (0.2708)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3606 (2.3606 -- 2.3606)  data: 2.1194 (2.1194 -- 2.1194)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4279 (0.7375)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4166 (0.2053 -- 2.3606)  data: 0.1976 (0.0006 -- 2.1194)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4681 (0.6493)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2217 (0.1693 -- 0.4849)  data: 0.0180 (0.0001 -- 0.3025)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5441 (0.7146)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.2656)  time: 0.2045 (0.1333 -- 0.4849)  data: 0.0177 (0.0001 -- 0.3025)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 83.195 Acc@5 97.303 loss 0.675
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 83.61%
Epoch: [108]  [  0/160]  eta: 0:19:01  lr: 0.000015  min_lr: 0.000000  loss: 1.8706 (1.8706)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5879 (7.5879)  time: 7.1357 (7.1357 -- 7.1357)  data: 5.6996 (5.6996 -- 5.6996)  max mem: 16413
Epoch: [108]  [ 20/160]  eta: 0:02:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8733 (1.8148)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5056 (9.8680)  time: 0.8227 (0.5288 -- 2.7952)  data: 0.1381 (0.0004 -- 2.2641)  max mem: 16413
[2023-08-29 22:05:50,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:05:50,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-29 22:05:50,303] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:05:50,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [108]  [ 40/160]  eta: 0:02:04  lr: 0.000015  min_lr: 0.000000  loss: 1.8929 (1.8457)  loss_scale: 8192.0000 (5794.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9376 (9.4524)  time: 0.9474 (0.5216 -- 4.0790)  data: 0.1235 (0.0002 -- 1.3018)  max mem: 16413
Epoch: [108]  [ 60/160]  eta: 0:01:39  lr: 0.000015  min_lr: 0.000000  loss: 1.8174 (1.8392)  loss_scale: 8192.0000 (6580.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5933 (9.1590)  time: 0.9116 (0.5290 -- 4.0493)  data: 0.0020 (0.0003 -- 0.0108)  max mem: 16413
Epoch: [108]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 1.7143 (1.8268)  loss_scale: 8192.0000 (6978.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0233 (9.1022)  time: 0.8194 (0.5240 -- 2.4000)  data: 0.0018 (0.0003 -- 0.0103)  max mem: 16413
Epoch: [108]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.5753 (1.8002)  loss_scale: 8192.0000 (7218.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8905 (8.9253)  time: 0.8368 (0.5208 -- 2.2842)  data: 0.1292 (0.0003 -- 1.7217)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8740 (1.8107)  loss_scale: 8192.0000 (7379.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3326 (8.9110)  time: 0.9696 (0.5225 -- 4.3807)  data: 0.4213 (0.0002 -- 3.8107)  max mem: 16413
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.9357 (1.8325)  loss_scale: 8192.0000 (7494.8085)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2961 (8.9777)  time: 0.7916 (0.5302 -- 2.9393)  data: 0.2415 (0.0002 -- 2.4108)  max mem: 16413
[2023-08-29 22:07:39,170] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:07:39,170] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:07:39,170] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:07:39,171] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.7010 (1.8246)  loss_scale: 8192.0000 (7987.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7035 (8.8937)  time: 0.6402 (0.4961 -- 2.1567)  data: 0.1133 (0.0002 -- 1.6048)  max mem: 16413
Epoch: [108] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.7010 (1.7952)  loss_scale: 8192.0000 (7987.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7035 (8.8937)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2705 (0.2705)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4359 (2.4359 -- 2.4359)  data: 2.1834 (2.1834 -- 2.1834)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4860 (0.7292)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4245 (0.2034 -- 2.4359)  data: 0.2012 (0.0005 -- 2.1834)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4860 (0.6461)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2147 (0.1698 -- 0.2764)  data: 0.0056 (0.0001 -- 0.0787)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5411 (0.7076)  acc1: 85.7143 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.1985 (0.1337 -- 0.2764)  data: 0.0053 (0.0001 -- 0.0787)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 81.535 Acc@5 97.510 loss 0.670
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 83.61%
Epoch: [109]  [  0/160]  eta: 0:17:29  lr: 0.000015  min_lr: 0.000000  loss: 2.2255 (2.2255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1220 (9.1220)  time: 6.5566 (6.5566 -- 6.5566)  data: 5.5628 (5.5628 -- 5.5628)  max mem: 16413
Epoch: [109]  [ 20/160]  eta: 0:02:46  lr: 0.000015  min_lr: 0.000000  loss: 1.7303 (1.8160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5207 (8.9445)  time: 0.9199 (0.5331 -- 3.3549)  data: 0.0409 (0.0006 -- 0.7729)  max mem: 16413
Epoch: [109]  [ 40/160]  eta: 0:02:00  lr: 0.000015  min_lr: 0.000000  loss: 1.9786 (1.8512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2663 (8.4449)  time: 0.8177 (0.5352 -- 2.8517)  data: 0.1037 (0.0003 -- 1.2599)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8528 (1.8389)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2525 (8.3725)  time: 0.9045 (0.5228 -- 2.8501)  data: 0.0898 (0.0002 -- 1.2098)  max mem: 16413
Epoch: [109]  [ 80/160]  eta: 0:01:15  lr: 0.000015  min_lr: 0.000000  loss: 1.9125 (1.8484)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9420 (8.5656)  time: 0.8520 (0.5267 -- 2.3851)  data: 0.2388 (0.0003 -- 1.8282)  max mem: 16413
Epoch: [109]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.8965 (1.8359)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3093 (8.6927)  time: 0.8509 (0.5319 -- 3.6712)  data: 0.1309 (0.0004 -- 1.7074)  max mem: 16413
[2023-08-29 22:09:41,412] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:09:41,412] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:09:41,414] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:09:41,415] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.8021 (1.8309)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1647 (8.5837)  time: 0.8753 (0.5252 -- 2.0971)  data: 0.2454 (0.0004 -- 1.5717)  max mem: 16413
[2023-08-29 22:09:56,420] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17578
[2023-08-29 22:09:56,420] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:09:56,420] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17578
[2023-08-29 22:09:56,420] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:09:56,421] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.8915 (1.8371)  loss_scale: 32768.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6412 (8.5091)  time: 0.8115 (0.5255 -- 2.0636)  data: 0.0801 (0.0008 -- 1.0147)  max mem: 16413
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7395 (1.8295)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6610 (8.5064)  time: 0.7603 (0.4974 -- 2.8545)  data: 0.0772 (0.0002 -- 0.9301)  max mem: 16413
Epoch: [109] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7395 (1.7820)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6610 (8.5064)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2597 (0.2597)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4106 (2.4106 -- 2.4106)  data: 2.1998 (2.1998 -- 2.1998)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4704 (0.7345)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (95.9596)  time: 0.4134 (0.2032 -- 2.4106)  data: 0.2013 (0.0006 -- 2.1998)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4410 (0.6412)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (95.7672)  time: 0.2149 (0.1693 -- 0.3996)  data: 0.0126 (0.0001 -- 0.1612)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5598 (0.7016)  acc1: 85.7143 (81.7427)  acc5: 100.0000 (95.8506)  time: 0.2011 (0.1332 -- 0.3996)  data: 0.0123 (0.0001 -- 0.1612)  max mem: 16413
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.663
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 83.61%
Epoch: [110]  [  0/160]  eta: 0:21:06  lr: 0.000014  min_lr: 0.000000  loss: 2.0850 (2.0850)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3767 (8.3767)  time: 7.9158 (7.9158 -- 7.9158)  data: 4.4359 (4.4359 -- 4.4359)  max mem: 16413
Epoch: [110]  [ 20/160]  eta: 0:02:54  lr: 0.000014  min_lr: 0.000000  loss: 1.7892 (1.8524)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8186 (9.4009)  time: 0.9150 (0.5183 -- 4.0584)  data: 0.0024 (0.0002 -- 0.0157)  max mem: 16413
Epoch: [110]  [ 40/160]  eta: 0:02:11  lr: 0.000014  min_lr: 0.000000  loss: 1.7693 (1.8551)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0429 (9.1108)  time: 0.9378 (0.5204 -- 3.3575)  data: 0.0014 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [110]  [ 60/160]  eta: 0:01:39  lr: 0.000014  min_lr: 0.000000  loss: 1.7192 (1.8404)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9274 (8.5756)  time: 0.7799 (0.5253 -- 3.4707)  data: 0.0010 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [110]  [ 80/160]  eta: 0:01:19  lr: 0.000014  min_lr: 0.000000  loss: 1.8193 (1.8343)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9479 (8.7573)  time: 0.9826 (0.5277 -- 4.9754)  data: 0.0026 (0.0004 -- 0.0169)  max mem: 16413
[2023-08-29 22:11:40,693] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17681
[2023-08-29 22:11:40,693] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17681
[2023-08-29 22:11:40,693] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:11:40,693] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:11:40,693] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [110]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 1.4197 (1.7976)  loss_scale: 8192.0000 (14761.8218)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9385 (8.7119)  time: 0.7729 (0.5228 -- 3.1715)  data: 0.0012 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [110]  [120/160]  eta: 0:00:38  lr: 0.000014  min_lr: 0.000000  loss: 1.6638 (1.7873)  loss_scale: 8192.0000 (13675.9008)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5586 (8.6905)  time: 1.0123 (0.5227 -- 4.4923)  data: 0.0010 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.7361 (1.7820)  loss_scale: 8192.0000 (12898.0426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5838 (8.7201)  time: 0.7791 (0.5212 -- 2.9856)  data: 0.0018 (0.0003 -- 0.0076)  max mem: 16413
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.9679 (1.7933)  loss_scale: 8192.0000 (12339.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0090 (8.6282)  time: 0.6202 (0.4950 -- 2.4856)  data: 0.0009 (0.0002 -- 0.0059)  max mem: 16413
Epoch: [110] Total time: 0:02:23 (0.8960 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.9679 (1.7788)  loss_scale: 8192.0000 (12339.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0090 (8.6282)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2591 (0.2591)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3532 (2.3532 -- 2.3532)  data: 2.1127 (2.1127 -- 2.1127)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4440 (0.7220)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4216 (0.2035 -- 2.3532)  data: 0.1980 (0.0007 -- 2.1127)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4684 (0.6510)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2220 (0.1714 -- 0.3745)  data: 0.0125 (0.0001 -- 0.1807)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5342 (0.7074)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (97.0954)  time: 0.2051 (0.1327 -- 0.3745)  data: 0.0122 (0.0001 -- 0.1807)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 82.988 Acc@5 97.718 loss 0.668
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 83.61%
Epoch: [111]  [  0/160]  eta: 0:19:57  lr: 0.000014  min_lr: 0.000000  loss: 1.6279 (1.6279)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0690 (8.0690)  time: 7.4837 (7.4837 -- 7.4837)  data: 6.4720 (6.4720 -- 6.4720)  max mem: 16413
Epoch: [111]  [ 20/160]  eta: 0:02:52  lr: 0.000014  min_lr: 0.000000  loss: 1.8451 (1.7626)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2748 (9.3451)  time: 0.9185 (0.5159 -- 4.1131)  data: 0.3518 (0.0003 -- 3.6006)  max mem: 16413
Epoch: [111]  [ 40/160]  eta: 0:02:04  lr: 0.000014  min_lr: 0.000000  loss: 1.6481 (1.7096)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8505 (8.9631)  time: 0.8364 (0.5242 -- 3.7101)  data: 0.2871 (0.0002 -- 3.1937)  max mem: 16413
[2023-08-29 22:13:41,526] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:13:41,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:13:41,529] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:13:41,529] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [111]  [ 60/160]  eta: 0:01:36  lr: 0.000014  min_lr: 0.000000  loss: 1.5904 (1.7031)  loss_scale: 16384.0000 (9669.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8108 (9.0276)  time: 0.8153 (0.5211 -- 3.1513)  data: 0.2496 (0.0001 -- 2.5567)  max mem: 16413
Epoch: [111]  [ 80/160]  eta: 0:01:13  lr: 0.000014  min_lr: 0.000000  loss: 1.8765 (1.7296)  loss_scale: 16384.0000 (11327.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3044 (8.6878)  time: 0.7785 (0.5372 -- 2.5899)  data: 0.1824 (0.0002 -- 2.0634)  max mem: 16413
Epoch: [111]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 1.8028 (1.7402)  loss_scale: 16384.0000 (12328.5545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9523 (8.7146)  time: 0.9926 (0.5206 -- 3.9506)  data: 0.0825 (0.0009 -- 1.4500)  max mem: 16413
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.8136 (1.7384)  loss_scale: 16384.0000 (12998.8760)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0067 (8.8302)  time: 0.7807 (0.5341 -- 2.4341)  data: 0.0275 (0.0003 -- 0.4259)  max mem: 16413
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.8475 (1.7465)  loss_scale: 16384.0000 (13479.0355)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4928 (8.8222)  time: 0.9329 (0.5459 -- 3.4862)  data: 0.2922 (0.0003 -- 1.9994)  max mem: 16413
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7734 (1.7457)  loss_scale: 16384.0000 (13824.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0312 (8.8599)  time: 0.7098 (0.4971 -- 2.8157)  data: 0.0416 (0.0002 -- 0.8210)  max mem: 16413
Epoch: [111] Total time: 0:02:22 (0.8891 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7734 (1.7737)  loss_scale: 16384.0000 (13824.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0312 (8.8599)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2519 (0.2519)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4071 (2.4071 -- 2.4071)  data: 2.1594 (2.1594 -- 2.1594)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4665 (0.7262)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4425 (0.2008 -- 2.4071)  data: 0.2279 (0.0007 -- 2.1594)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4665 (0.6304)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2201 (0.1690 -- 0.5521)  data: 0.0217 (0.0001 -- 0.3387)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5383 (0.6964)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (95.8506)  time: 0.2042 (0.1325 -- 0.5521)  data: 0.0214 (0.0001 -- 0.3387)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 83.610 Acc@5 96.888 loss 0.663
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 83.61%
Epoch: [112]  [  0/160]  eta: 0:22:05  lr: 0.000014  min_lr: 0.000000  loss: 1.7330 (1.7330)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5481 (7.5481)  time: 8.2866 (8.2866 -- 8.2866)  data: 7.7584 (7.7584 -- 7.7584)  max mem: 16413
[2023-08-29 22:15:45,362] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:15:45,362] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:15:45,365] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:15:45,365] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [112]  [ 20/160]  eta: 0:02:48  lr: 0.000014  min_lr: 0.000000  loss: 1.8676 (1.7598)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7391 (9.0239)  time: 0.8519 (0.5372 -- 2.6429)  data: 0.0778 (0.0003 -- 1.0632)  max mem: 16413
Epoch: [112]  [ 40/160]  eta: 0:01:57  lr: 0.000014  min_lr: 0.000000  loss: 1.6941 (1.7654)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0488 (8.8491)  time: 0.7469 (0.5195 -- 2.3747)  data: 0.0020 (0.0002 -- 0.0070)  max mem: 16413
Epoch: [112]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000000  loss: 1.8106 (1.7791)  loss_scale: 32768.0000 (27933.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3619 (8.7935)  time: 0.9462 (0.5250 -- 3.4659)  data: 0.0019 (0.0003 -- 0.0059)  max mem: 16413
[2023-08-29 22:16:21,434] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17982
[2023-08-29 22:16:21,434] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17982
[2023-08-29 22:16:21,434] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:16:21,434] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:16:21,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 22:16:35,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=105, lr=[3.27929303664586e-07, 3.27929303664586e-07, 4.372390715527813e-07, 4.372390715527813e-07, 5.829854287370417e-07, 5.829854287370417e-07, 7.773139049827223e-07, 7.773139049827223e-07, 1.036418539976963e-06, 1.036418539976963e-06, 1.3818913866359508e-06, 1.3818913866359508e-06, 1.8425218488479344e-06, 1.8425218488479344e-06, 2.4566957984639127e-06, 2.4566957984639127e-06, 3.2755943979518833e-06, 3.2755943979518833e-06, 4.367459197269178e-06, 4.367459197269178e-06, 5.823278929692237e-06, 5.823278929692237e-06, 7.764371906256316e-06, 7.764371906256316e-06, 1.0352495875008422e-05, 1.0352495875008422e-05, 1.3803327833344562e-05, 1.3803327833344562e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 22:16:35,368] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=17.544081182272176, CurrSamplesPerSec=20.871960747235686, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [112]  [ 80/160]  eta: 0:01:13  lr: 0.000014  min_lr: 0.000000  loss: 1.8185 (1.7806)  loss_scale: 16384.0000 (25283.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7858 (8.8748)  time: 0.7779 (0.5264 -- 2.8786)  data: 0.0017 (0.0003 -- 0.0065)  max mem: 16413
[2023-08-29 22:16:45,800] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18009
[2023-08-29 22:16:45,800] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18009
[2023-08-29 22:16:45,800] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:16:45,800] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:16:45,800] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [112]  [100/160]  eta: 0:00:54  lr: 0.000014  min_lr: 0.000000  loss: 1.9658 (1.7977)  loss_scale: 8192.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5945 (8.9922)  time: 0.8749 (0.5186 -- 3.2687)  data: 0.0228 (0.0002 -- 0.4287)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.5831 (1.7699)  loss_scale: 8192.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3086 (9.0353)  time: 0.8764 (0.5244 -- 2.4092)  data: 0.0019 (0.0003 -- 0.0091)  max mem: 16413
Epoch: [112]  [140/160]  eta: 0:00:17  lr: 0.000014  min_lr: 0.000000  loss: 1.7880 (1.7682)  loss_scale: 8192.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9606 (9.2523)  time: 0.7960 (0.5294 -- 2.2857)  data: 0.1928 (0.0005 -- 1.5104)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.9081 (1.7757)  loss_scale: 8192.0000 (17254.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2608 (9.1610)  time: 0.7378 (0.4957 -- 2.3596)  data: 0.2127 (0.0003 -- 1.8641)  max mem: 16413
Epoch: [112] Total time: 0:02:19 (0.8745 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.9081 (1.7624)  loss_scale: 8192.0000 (17254.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2608 (9.1610)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2675 (0.2675)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3360 (2.3360 -- 2.3360)  data: 2.1079 (2.1079 -- 2.1079)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4412 (0.7128)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4578 (0.2006 -- 2.3360)  data: 0.2400 (0.0007 -- 2.1079)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4412 (0.6295)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2305 (0.1693 -- 0.7336)  data: 0.0267 (0.0001 -- 0.5168)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5219 (0.6908)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (95.8506)  time: 0.2144 (0.1328 -- 0.7336)  data: 0.0264 (0.0001 -- 0.5168)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 83.402 Acc@5 96.888 loss 0.659
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 83.61%
Epoch: [113]  [  0/160]  eta: 0:19:20  lr: 0.000014  min_lr: 0.000000  loss: 1.0460 (1.0460)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5846 (6.5846)  time: 7.2521 (7.2521 -- 7.2521)  data: 6.5302 (6.5302 -- 6.5302)  max mem: 16413
Epoch: [113]  [ 20/160]  eta: 0:02:39  lr: 0.000014  min_lr: 0.000000  loss: 1.9244 (1.8825)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8426 (9.6131)  time: 0.8330 (0.5274 -- 3.7538)  data: 0.2801 (0.0004 -- 3.2070)  max mem: 16413
Epoch: [113]  [ 40/160]  eta: 0:02:05  lr: 0.000014  min_lr: 0.000000  loss: 1.7846 (1.8107)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3769 (9.4068)  time: 0.9476 (0.5268 -- 3.7596)  data: 0.3816 (0.0004 -- 3.2459)  max mem: 16413
[2023-08-29 22:18:46,615] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:18:46,615] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:18:46,615] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:18:46,616] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [113]  [ 60/160]  eta: 0:01:36  lr: 0.000014  min_lr: 0.000000  loss: 1.6996 (1.8162)  loss_scale: 8192.0000 (8594.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4442 (9.3483)  time: 0.7927 (0.5370 -- 3.3928)  data: 0.1981 (0.0005 -- 2.8336)  max mem: 16413
Epoch: [113]  [ 80/160]  eta: 0:01:14  lr: 0.000014  min_lr: 0.000000  loss: 1.8617 (1.8422)  loss_scale: 16384.0000 (10518.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5538 (9.3715)  time: 0.8358 (0.5243 -- 2.0768)  data: 0.1992 (0.0007 -- 1.5156)  max mem: 16413
Epoch: [113]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 1.8303 (1.8368)  loss_scale: 16384.0000 (11679.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6697 (9.3531)  time: 0.9133 (0.5214 -- 3.8524)  data: 0.0534 (0.0003 -- 1.0375)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.5614 (1.8086)  loss_scale: 16384.0000 (12457.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1881 (9.3379)  time: 0.8253 (0.5304 -- 2.2152)  data: 0.0016 (0.0003 -- 0.0054)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.7971 (1.8124)  loss_scale: 16384.0000 (13014.2411)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0262 (9.3456)  time: 0.9604 (0.5196 -- 3.2816)  data: 0.0015 (0.0002 -- 0.0035)  max mem: 16413
[2023-08-29 22:20:08,716] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18235
[2023-08-29 22:20:08,716] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:20:08,716] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18235
[2023-08-29 22:20:08,716] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-29 22:20:08,716] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.9179 (1.8329)  loss_scale: 16384.0000 (13158.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0529 (9.2391)  time: 0.6405 (0.4965 -- 1.7162)  data: 0.0008 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [113] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.9179 (1.7983)  loss_scale: 16384.0000 (13158.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0529 (9.2391)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3028 (0.3028)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4408 (2.4408 -- 2.4408)  data: 2.1649 (2.1649 -- 2.1649)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4450 (0.7158)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4243 (0.2033 -- 2.4408)  data: 0.1983 (0.0004 -- 2.1649)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4533 (0.6294)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.2963)  time: 0.2209 (0.1707 -- 0.3721)  data: 0.0094 (0.0001 -- 0.1692)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5562 (0.6912)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (96.6805)  time: 0.2022 (0.1336 -- 0.3721)  data: 0.0091 (0.0001 -- 0.1692)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.662
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.61%
Epoch: [114]  [  0/160]  eta: 0:19:37  lr: 0.000013  min_lr: 0.000000  loss: 1.7719 (1.7719)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2209 (12.2209)  time: 7.3587 (7.3587 -- 7.3587)  data: 4.1550 (4.1550 -- 4.1550)  max mem: 16413
Epoch: [114]  [ 20/160]  eta: 0:02:36  lr: 0.000013  min_lr: 0.000000  loss: 1.7479 (1.7494)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7890 (9.5352)  time: 0.8085 (0.5260 -- 1.8151)  data: 0.1128 (0.0004 -- 1.1435)  max mem: 16413
Epoch: [114]  [ 40/160]  eta: 0:02:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7864 (1.7971)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1984 (9.4947)  time: 0.8749 (0.5414 -- 3.0880)  data: 0.0653 (0.0006 -- 0.7940)  max mem: 16413
Epoch: [114]  [ 60/160]  eta: 0:01:35  lr: 0.000013  min_lr: 0.000000  loss: 1.6250 (1.7515)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8422 (9.4303)  time: 0.8739 (0.5399 -- 3.9045)  data: 0.0071 (0.0003 -- 0.1083)  max mem: 16413
Epoch: [114]  [ 80/160]  eta: 0:01:14  lr: 0.000013  min_lr: 0.000000  loss: 1.8421 (1.7667)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9416 (9.7105)  time: 0.8621 (0.5270 -- 4.3872)  data: 0.0366 (0.0003 -- 0.5814)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.7958 (1.7697)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2699 (9.5928)  time: 0.8672 (0.5261 -- 3.3179)  data: 0.2689 (0.0004 -- 2.7712)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.6229 (1.7612)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0949 (9.5044)  time: 0.8585 (0.5445 -- 3.5759)  data: 0.1285 (0.0008 -- 2.0461)  max mem: 16413
[2023-08-29 22:22:11,356] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:22:11,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:22:11,356] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:22:11,357] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8792 (1.7800)  loss_scale: 16384.0000 (9179.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6593 (9.3993)  time: 0.8583 (0.5304 -- 4.6979)  data: 0.3069 (0.0002 -- 4.1923)  max mem: 16413
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.9345 (1.7883)  loss_scale: 16384.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3874 (9.4119)  time: 0.6892 (0.4954 -- 3.4095)  data: 0.1699 (0.0002 -- 2.8920)  max mem: 16413
Epoch: [114] Total time: 0:02:20 (0.8794 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.9345 (1.7781)  loss_scale: 16384.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3874 (9.4119)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2719 (0.2719)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4285 (2.4285 -- 2.4285)  data: 2.1974 (2.1974 -- 2.1974)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4586 (0.7172)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4191 (0.2011 -- 2.4285)  data: 0.2017 (0.0007 -- 2.1974)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4883 (0.6235)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2159 (0.1695 -- 0.3492)  data: 0.0107 (0.0001 -- 0.1456)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5285 (0.6874)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.1995 (0.1328 -- 0.3492)  data: 0.0102 (0.0001 -- 0.1456)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 84.025 Acc@5 97.303 loss 0.653
Accuracy of the network on the 482 val images: 84.02%
[2023-08-29 22:22:47,155] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 22:22:47,157] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 22:22:47,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 22:22:47,157] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 22:22:48,545] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 22:22:48,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.02%
Epoch: [115]  [  0/160]  eta: 0:23:16  lr: 0.000013  min_lr: 0.000000  loss: 1.7405 (1.7405)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4830 (8.4830)  time: 8.7265 (8.7265 -- 8.7265)  data: 6.3516 (6.3516 -- 6.3516)  max mem: 16413
Epoch: [115]  [ 20/160]  eta: 0:02:41  lr: 0.000013  min_lr: 0.000000  loss: 1.7433 (1.7349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3632 (8.9292)  time: 0.7775 (0.5272 -- 3.3120)  data: 0.2241 (0.0008 -- 2.7744)  max mem: 16413
Epoch: [115]  [ 40/160]  eta: 0:02:04  lr: 0.000013  min_lr: 0.000000  loss: 1.6047 (1.7167)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1314 (9.3752)  time: 0.9167 (0.5246 -- 4.0554)  data: 0.3232 (0.0002 -- 3.4992)  max mem: 16413
Epoch: [115]  [ 60/160]  eta: 0:01:35  lr: 0.000013  min_lr: 0.000000  loss: 1.6446 (1.7067)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3770 (9.3850)  time: 0.7915 (0.5300 -- 1.9629)  data: 0.1086 (0.0003 -- 1.4153)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:14  lr: 0.000013  min_lr: 0.000000  loss: 1.8087 (1.7355)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9021 (9.2602)  time: 0.8658 (0.5266 -- 3.4810)  data: 0.0716 (0.0002 -- 0.9420)  max mem: 16413
[2023-08-29 22:24:15,522] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:24:15,522] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:24:15,522] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:24:15,522] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:24:17,202] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18495
[2023-08-29 22:24:17,202] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18495
[2023-08-29 22:24:17,202] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:24:17,202] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:24:17,203] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [115]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.8034 (1.7436)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9364 (9.2722)  time: 0.8658 (0.5212 -- 3.0844)  data: 0.1499 (0.0003 -- 2.5572)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.9499 (1.7548)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1352 (9.2510)  time: 0.9297 (0.5162 -- 3.6729)  data: 0.3022 (0.0003 -- 3.1189)  max mem: 16413
Epoch: [115]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.7744 (1.7564)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7411 (9.2636)  time: 0.8123 (0.5285 -- 3.2980)  data: 0.2693 (0.0002 -- 2.7656)  max mem: 16413
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8142 (1.7595)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5573 (9.1779)  time: 0.7122 (0.4937 -- 2.8910)  data: 0.1849 (0.0001 -- 2.3524)  max mem: 16413
Epoch: [115] Total time: 0:02:21 (0.8852 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8142 (1.7575)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5573 (9.1779)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2545 (0.2545)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5222 (2.5222 -- 2.5222)  data: 2.2657 (2.2657 -- 2.2657)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4294 (0.7065)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4300 (0.2049 -- 2.5222)  data: 0.2074 (0.0009 -- 2.2657)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4592 (0.6171)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.2963)  time: 0.2135 (0.1694 -- 0.2984)  data: 0.0061 (0.0001 -- 0.1021)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5199 (0.6833)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.1954 (0.1329 -- 0.2984)  data: 0.0055 (0.0001 -- 0.1021)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 84.025 Acc@5 97.303 loss 0.652
Accuracy of the network on the 482 val images: 84.02%
[2023-08-29 22:25:17,972] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 22:25:17,973] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 22:25:17,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 22:25:17,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 22:25:19,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 22:25:19,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.02%
Epoch: [116]  [  0/160]  eta: 0:21:20  lr: 0.000013  min_lr: 0.000000  loss: 1.9495 (1.9495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5970 (9.5970)  time: 8.0009 (8.0009 -- 8.0009)  data: 7.4799 (7.4799 -- 7.4799)  max mem: 16413
Epoch: [116]  [ 20/160]  eta: 0:02:44  lr: 0.000013  min_lr: 0.000000  loss: 1.6224 (1.7489)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9989 (9.1048)  time: 0.8349 (0.5243 -- 3.1575)  data: 0.2449 (0.0002 -- 2.6309)  max mem: 16413
Epoch: [116]  [ 40/160]  eta: 0:02:07  lr: 0.000013  min_lr: 0.000000  loss: 1.7536 (1.7776)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5361 (9.1351)  time: 0.9369 (0.5349 -- 4.6358)  data: 0.3859 (0.0004 -- 4.0765)  max mem: 16413
Epoch: [116]  [ 60/160]  eta: 0:01:38  lr: 0.000013  min_lr: 0.000000  loss: 1.7959 (1.7731)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8482 (8.9022)  time: 0.8429 (0.5222 -- 2.2246)  data: 0.2969 (0.0002 -- 1.7033)  max mem: 16413
[2023-08-29 22:26:22,339] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:26:22,340] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:26:22,340] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:26:22,340] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:26:30,064] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18633
[2023-08-29 22:26:30,064] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18633
[2023-08-29 22:26:30,064] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:26:30,064] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:26:30,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [ 80/160]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000000  loss: 1.7226 (1.7575)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0418 (8.7167)  time: 0.8861 (0.5168 -- 3.0539)  data: 0.3503 (0.0002 -- 2.5299)  max mem: 16413
Epoch: [116]  [100/160]  eta: 0:00:57  lr: 0.000013  min_lr: 0.000000  loss: 1.7853 (1.7609)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5756 (8.5958)  time: 0.8967 (0.5244 -- 4.1085)  data: 0.3433 (0.0005 -- 3.5872)  max mem: 16413
Epoch: [116]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.4836 (1.7376)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8330 (8.5982)  time: 0.7934 (0.5257 -- 3.0114)  data: 0.2402 (0.0005 -- 2.4849)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8022 (1.7510)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5117 (8.6794)  time: 0.8458 (0.5221 -- 2.4193)  data: 0.2992 (0.0001 -- 1.8830)  max mem: 16413
[2023-08-29 22:27:37,743] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18712
[2023-08-29 22:27:37,743] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18712
[2023-08-29 22:27:37,743] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:27:37,743] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:27:37,743] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.6582 (1.7432)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9529 (8.6114)  time: 0.7735 (0.4837 -- 2.5202)  data: 0.2591 (0.0002 -- 2.0073)  max mem: 16413
Epoch: [116] Total time: 0:02:22 (0.8929 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.6582 (1.7526)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9529 (8.6114)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2625 (0.2625)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5933 (2.5933 -- 2.5933)  data: 2.3158 (2.3158 -- 2.3158)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4697 (0.7257)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4391 (0.2097 -- 2.5933)  data: 0.2118 (0.0007 -- 2.3158)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4394 (0.6260)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2092 (0.1689 -- 0.2409)  data: 0.0019 (0.0001 -- 0.0210)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5250 (0.6963)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (97.0954)  time: 0.1910 (0.1328 -- 0.2342)  data: 0.0015 (0.0001 -- 0.0210)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 82.365 Acc@5 97.510 loss 0.662
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 84.02%
Epoch: [117]  [  0/160]  eta: 0:16:33  lr: 0.000013  min_lr: 0.000000  loss: 1.4537 (1.4537)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7552 (7.7552)  time: 6.2076 (6.2076 -- 6.2076)  data: 5.6453 (5.6453 -- 5.6453)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:38  lr: 0.000013  min_lr: 0.000000  loss: 1.7614 (1.6702)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5759 (8.5747)  time: 0.8789 (0.5308 -- 2.4732)  data: 0.0765 (0.0002 -- 0.9548)  max mem: 16413
Epoch: [117]  [ 40/160]  eta: 0:02:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7696 (1.7096)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8429 (8.5394)  time: 0.8658 (0.5302 -- 2.8614)  data: 0.0982 (0.0003 -- 1.1759)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000000  loss: 1.8814 (1.7829)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3776 (8.8385)  time: 0.9076 (0.5252 -- 3.0355)  data: 0.0913 (0.0005 -- 0.8267)  max mem: 16413
Epoch: [117]  [ 80/160]  eta: 0:01:14  lr: 0.000013  min_lr: 0.000000  loss: 1.8688 (1.7920)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9043 (8.6220)  time: 0.8236 (0.5255 -- 3.3979)  data: 0.0016 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [117]  [100/160]  eta: 0:00:57  lr: 0.000012  min_lr: 0.000000  loss: 1.6775 (1.7809)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9340 (8.6517)  time: 1.0298 (0.5069 -- 4.3520)  data: 0.0011 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [117]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.7283 (1.7748)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2480 (8.6466)  time: 0.7435 (0.5152 -- 3.9545)  data: 0.0012 (0.0001 -- 0.0051)  max mem: 16413
[2023-08-29 22:29:44,243] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:29:44,243] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:29:44,243] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:29:44,243] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8003 (1.7802)  loss_scale: 16384.0000 (9353.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9736 (8.6542)  time: 0.9057 (0.5352 -- 3.1010)  data: 0.0017 (0.0001 -- 0.0038)  max mem: 16413
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7874 (1.7760)  loss_scale: 16384.0000 (10188.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2125 (8.7028)  time: 0.7356 (0.4954 -- 4.3845)  data: 0.0007 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [117] Total time: 0:02:23 (0.8966 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7874 (1.7756)  loss_scale: 16384.0000 (10188.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2125 (8.7028)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2637 (0.2637)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4224 (2.4224 -- 2.4224)  data: 2.2002 (2.2002 -- 2.2002)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4383 (0.7107)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4296 (0.1998 -- 2.4224)  data: 0.2144 (0.0005 -- 2.2002)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4481 (0.6261)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2210 (0.1696 -- 0.3698)  data: 0.0151 (0.0001 -- 0.1498)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5240 (0.6903)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (97.0954)  time: 0.2054 (0.1332 -- 0.3698)  data: 0.0149 (0.0001 -- 0.1498)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 82.573 Acc@5 97.510 loss 0.655
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.02%
Epoch: [118]  [  0/160]  eta: 0:24:56  lr: 0.000012  min_lr: 0.000000  loss: 1.4275 (1.4275)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7013 (9.7013)  time: 9.3509 (9.3509 -- 9.3509)  data: 8.8067 (8.8067 -- 8.8067)  max mem: 16413
Epoch: [118]  [ 20/160]  eta: 0:02:42  lr: 0.000012  min_lr: 0.000000  loss: 1.7517 (1.7803)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3703 (8.9584)  time: 0.7543 (0.5239 -- 2.8370)  data: 0.2112 (0.0003 -- 2.3113)  max mem: 16413
[2023-08-29 22:30:52,884] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18908
[2023-08-29 22:30:52,884] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18908
[2023-08-29 22:30:52,884] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:30:52,884] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:30:52,884] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [118]  [ 40/160]  eta: 0:02:02  lr: 0.000012  min_lr: 0.000000  loss: 1.7095 (1.7783)  loss_scale: 8192.0000 (13786.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4361 (8.7560)  time: 0.8648 (0.5295 -- 2.4898)  data: 0.2506 (0.0003 -- 1.7943)  max mem: 16413
Epoch: [118]  [ 60/160]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 1.7268 (1.7723)  loss_scale: 8192.0000 (11952.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3470 (9.0256)  time: 0.9011 (0.5182 -- 3.2509)  data: 0.3277 (0.0003 -- 2.3833)  max mem: 16413
Epoch: [118]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 1.9806 (1.7935)  loss_scale: 8192.0000 (11023.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1575 (8.9307)  time: 0.8506 (0.5207 -- 3.3667)  data: 0.1953 (0.0002 -- 2.7877)  max mem: 16413
Epoch: [118]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.6409 (1.7765)  loss_scale: 8192.0000 (10463.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8072 (8.9474)  time: 0.9312 (0.5213 -- 3.9733)  data: 0.3781 (0.0004 -- 3.4457)  max mem: 16413
[2023-08-29 22:32:13,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=111, lr=[2.8962711114581125e-07, 2.8962711114581125e-07, 3.861694815277483e-07, 3.861694815277483e-07, 5.148926420369978e-07, 5.148926420369978e-07, 6.865235227159971e-07, 6.865235227159971e-07, 9.153646969546627e-07, 9.153646969546627e-07, 1.220486262606217e-06, 1.220486262606217e-06, 1.6273150168082894e-06, 1.6273150168082894e-06, 2.1697533557443855e-06, 2.1697533557443855e-06, 2.8930044743258477e-06, 2.8930044743258477e-06, 3.8573392991011305e-06, 3.8573392991011305e-06, 5.143119065468174e-06, 5.143119065468174e-06, 6.857492087290898e-06, 6.857492087290898e-06, 9.14332278305453e-06, 9.14332278305453e-06, 1.2191097044072708e-05, 1.2191097044072708e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 22:32:13,017] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=17.569823069340526, CurrSamplesPerSec=23.50171459309232, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.8382 (1.7623)  loss_scale: 8192.0000 (10087.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1903 (9.0852)  time: 0.8399 (0.5229 -- 3.5441)  data: 0.2594 (0.0002 -- 3.0280)  max mem: 16413
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.7521 (1.7628)  loss_scale: 8192.0000 (9818.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2673 (9.0627)  time: 0.9171 (0.5229 -- 3.0336)  data: 0.3280 (0.0002 -- 2.5002)  max mem: 16413
[2023-08-29 22:32:42,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:32:42,981] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:32:42,981] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:32:42,981] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7214 (1.7451)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6701 (8.9824)  time: 0.7239 (0.4960 -- 2.3558)  data: 0.0918 (0.0001 -- 1.8214)  max mem: 16413
Epoch: [118] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7214 (1.7623)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6701 (8.9824)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2509 (0.2509)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4607 (2.4607 -- 2.4607)  data: 2.2031 (2.2031 -- 2.2031)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4528 (0.7122)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4208 (0.1935 -- 2.4607)  data: 0.2012 (0.0007 -- 2.2031)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4528 (0.6314)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2144 (0.1686 -- 0.2703)  data: 0.0062 (0.0001 -- 0.0742)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5324 (0.6946)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.6805)  time: 0.1972 (0.1329 -- 0.2703)  data: 0.0059 (0.0001 -- 0.0742)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.655
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.02%
Epoch: [119]  [  0/160]  eta: 0:22:54  lr: 0.000012  min_lr: 0.000000  loss: 1.7205 (1.7205)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.4565 (13.4565)  time: 8.5891 (8.5891 -- 8.5891)  data: 6.9232 (6.9232 -- 6.9232)  max mem: 16413
Epoch: [119]  [ 20/160]  eta: 0:02:35  lr: 0.000012  min_lr: 0.000000  loss: 1.8041 (1.7806)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4706 (8.7794)  time: 0.7372 (0.5294 -- 2.9135)  data: 0.1166 (0.0004 -- 1.2963)  max mem: 16413
Epoch: [119]  [ 40/160]  eta: 0:02:02  lr: 0.000012  min_lr: 0.000000  loss: 1.9105 (1.8477)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7193 (9.3985)  time: 0.9324 (0.5237 -- 3.5153)  data: 0.3175 (0.0003 -- 2.9857)  max mem: 16413
Epoch: [119]  [ 60/160]  eta: 0:01:38  lr: 0.000012  min_lr: 0.000000  loss: 1.8959 (1.8343)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6682 (9.1248)  time: 0.9161 (0.5268 -- 4.0872)  data: 0.0905 (0.0003 -- 1.4502)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 1.9116 (1.8442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4799 (9.0262)  time: 0.8622 (0.5146 -- 3.8483)  data: 0.0143 (0.0004 -- 0.2630)  max mem: 16413
Epoch: [119]  [100/160]  eta: 0:00:57  lr: 0.000012  min_lr: 0.000000  loss: 1.6188 (1.8140)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1942 (8.9090)  time: 0.9359 (0.5222 -- 3.8028)  data: 0.0020 (0.0003 -- 0.0145)  max mem: 16413
Epoch: [119]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.6167 (1.7913)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0667 (9.0944)  time: 0.7997 (0.5223 -- 3.5129)  data: 0.0012 (0.0002 -- 0.0024)  max mem: 16413
[2023-08-29 22:34:48,341] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:34:48,341] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:34:48,342] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:34:48,342] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8662 (1.8045)  loss_scale: 32768.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8024 (9.0115)  time: 0.8034 (0.5296 -- 2.6033)  data: 0.0016 (0.0006 -- 0.0037)  max mem: 16413
[2023-08-29 22:35:13,167] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19199
[2023-08-29 22:35:13,167] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19199
[2023-08-29 22:35:13,168] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:35:13,168] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:35:13,168] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7936 (1.7966)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3482 (8.9833)  time: 0.6774 (0.4844 -- 2.6893)  data: 0.0008 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [119] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7936 (1.7871)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3482 (8.9833)
[2023-08-29 22:35:13,171] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-08-29 22:35:13,173] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-08-29 22:35:13,173] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-08-29 22:35:13,173] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-08-29 22:35:14,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-08-29 22:35:14,120] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2695 (0.2695)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4571 (2.4571 -- 2.4571)  data: 2.2292 (2.2292 -- 2.2292)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4379 (0.6983)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4185 (0.2007 -- 2.4571)  data: 0.2038 (0.0007 -- 2.2292)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4379 (0.6236)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2223 (0.1693 -- 0.6060)  data: 0.0215 (0.0001 -- 0.4129)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5146 (0.6914)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.2071 (0.1333 -- 0.6060)  data: 0.0211 (0.0001 -- 0.4129)  max mem: 16413
Val: Total time: 0:00:07 (0.2923 s / it)
* Acc@1 81.950 Acc@5 97.303 loss 0.663
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 84.02%
Epoch: [120]  [  0/160]  eta: 0:17:25  lr: 0.000012  min_lr: 0.000000  loss: 1.8853 (1.8853)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8996 (9.8996)  time: 6.5359 (6.5359 -- 6.5359)  data: 5.4539 (5.4539 -- 5.4539)  max mem: 16413
Epoch: [120]  [ 20/160]  eta: 0:02:37  lr: 0.000012  min_lr: 0.000000  loss: 1.9085 (1.8748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6411 (8.6995)  time: 0.8537 (0.5314 -- 3.3065)  data: 0.0013 (0.0005 -- 0.0025)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:07  lr: 0.000012  min_lr: 0.000000  loss: 1.8193 (1.8403)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3147 (8.7184)  time: 0.9900 (0.5233 -- 3.6736)  data: 0.3887 (0.0005 -- 3.1440)  max mem: 16413
Epoch: [120]  [ 60/160]  eta: 0:01:38  lr: 0.000012  min_lr: 0.000000  loss: 1.8344 (1.8263)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4529 (9.1150)  time: 0.8330 (0.5082 -- 4.6930)  data: 0.2846 (0.0003 -- 4.1478)  max mem: 16413
Epoch: [120]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 1.5047 (1.7766)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6815 (9.0715)  time: 0.8048 (0.5360 -- 2.1833)  data: 0.0118 (0.0004 -- 0.2001)  max mem: 16413
Epoch: [120]  [100/160]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 1.9234 (1.8018)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5340 (8.9646)  time: 0.8554 (0.5253 -- 2.3480)  data: 0.1277 (0.0002 -- 1.8001)  max mem: 16413
Epoch: [120]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.7071 (1.7949)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5200 (8.8906)  time: 0.9053 (0.5217 -- 2.3310)  data: 0.0447 (0.0003 -- 0.4698)  max mem: 16413
[2023-08-29 22:37:19,995] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:37:19,995] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:37:19,995] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:37:19,995] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:37:22,728] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19333
[2023-08-29 22:37:22,728] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19333
[2023-08-29 22:37:22,729] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:37:22,729] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:37:22,729] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [120]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.7112 (1.7828)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8159 (8.8544)  time: 0.7987 (0.5181 -- 3.3843)  data: 0.0261 (0.0002 -- 0.4977)  max mem: 16413
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.9323 (1.7872)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6132 (8.9106)  time: 0.7088 (0.4961 -- 2.2845)  data: 0.0240 (0.0003 -- 0.4625)  max mem: 16413
Epoch: [120] Total time: 0:02:21 (0.8815 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.9323 (1.7808)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6132 (8.9106)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2946 (0.2946)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3285 (2.3285 -- 2.3285)  data: 2.1220 (2.1220 -- 2.1220)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4297 (0.6963)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4158 (0.1959 -- 2.3285)  data: 0.2045 (0.0007 -- 2.1220)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4761 (0.6207)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2252 (0.1687 -- 0.5004)  data: 0.0217 (0.0001 -- 0.3050)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5129 (0.6847)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.6805)  time: 0.2108 (0.1328 -- 0.5004)  data: 0.0215 (0.0001 -- 0.3050)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.661
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.02%
Epoch: [121]  [  0/160]  eta: 0:22:20  lr: 0.000012  min_lr: 0.000000  loss: 2.3209 (2.3209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9791 (7.9791)  time: 8.3759 (8.3759 -- 8.3759)  data: 6.6407 (6.6407 -- 6.6407)  max mem: 16413
Epoch: [121]  [ 20/160]  eta: 0:02:47  lr: 0.000012  min_lr: 0.000000  loss: 1.8356 (1.7525)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4579 (8.9249)  time: 0.8347 (0.5415 -- 2.4910)  data: 0.0380 (0.0008 -- 0.6916)  max mem: 16413
Epoch: [121]  [ 40/160]  eta: 0:02:12  lr: 0.000012  min_lr: 0.000000  loss: 1.4917 (1.6819)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2115 (8.4821)  time: 1.0129 (0.5392 -- 3.0520)  data: 0.0018 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [121]  [ 60/160]  eta: 0:01:38  lr: 0.000012  min_lr: 0.000000  loss: 1.8346 (1.7283)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9720 (8.4549)  time: 0.7448 (0.5265 -- 3.8423)  data: 0.0017 (0.0003 -- 0.0090)  max mem: 16413
Epoch: [121]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000000  loss: 1.6314 (1.7325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2570 (8.6396)  time: 0.8484 (0.5182 -- 3.3106)  data: 0.0014 (0.0002 -- 0.0036)  max mem: 16413
[2023-08-29 22:39:17,576] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19452
[2023-08-29 22:39:17,576] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19452
[2023-08-29 22:39:17,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:39:17,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:39:17,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [121]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.5849 (1.7358)  loss_scale: 16384.0000 (15654.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3417 (8.7344)  time: 0.8562 (0.5222 -- 3.3977)  data: 0.0686 (0.0003 -- 1.3431)  max mem: 16413
Epoch: [121]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.8103 (1.7561)  loss_scale: 8192.0000 (14420.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9131 (8.7354)  time: 0.8862 (0.5158 -- 3.5274)  data: 0.0017 (0.0008 -- 0.0039)  max mem: 16413
Epoch: [121]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8625 (1.7633)  loss_scale: 8192.0000 (13537.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9670 (8.8483)  time: 0.8385 (0.5143 -- 3.8878)  data: 0.0016 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.6884 (1.7549)  loss_scale: 8192.0000 (12902.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3705 (8.8591)  time: 0.6510 (0.4968 -- 1.6276)  data: 0.0634 (0.0002 -- 0.5888)  max mem: 16413
Epoch: [121] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.6884 (1.7622)  loss_scale: 8192.0000 (12902.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3705 (8.8591)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2908 (0.2908)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3806 (2.3806 -- 2.3806)  data: 2.0997 (2.0997 -- 2.0997)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4331 (0.6989)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4157 (0.1956 -- 2.3806)  data: 0.1923 (0.0008 -- 2.0997)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4331 (0.6192)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2193 (0.1696 -- 0.3957)  data: 0.0115 (0.0001 -- 0.2124)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5252 (0.6841)  acc1: 88.8889 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2028 (0.1337 -- 0.3957)  data: 0.0112 (0.0001 -- 0.2124)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 81.743 Acc@5 96.888 loss 0.664
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 84.02%
Epoch: [122]  [  0/160]  eta: 0:21:17  lr: 0.000011  min_lr: 0.000000  loss: 2.0133 (2.0133)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0441 (7.0441)  time: 7.9832 (7.9832 -- 7.9832)  data: 7.3945 (7.3945 -- 7.3945)  max mem: 16413
Epoch: [122]  [ 20/160]  eta: 0:02:42  lr: 0.000011  min_lr: 0.000000  loss: 1.7699 (1.7695)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8262 (8.5126)  time: 0.8205 (0.5294 -- 3.3759)  data: 0.2685 (0.0003 -- 2.8296)  max mem: 16413
Epoch: [122]  [ 40/160]  eta: 0:02:08  lr: 0.000011  min_lr: 0.000000  loss: 1.8214 (1.8175)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7744 (9.1213)  time: 0.9757 (0.5387 -- 3.8940)  data: 0.4254 (0.0004 -- 3.3765)  max mem: 16413
Epoch: [122]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000000  loss: 1.8270 (1.8257)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8024 (8.7392)  time: 0.7823 (0.5187 -- 3.1772)  data: 0.2386 (0.0002 -- 2.6393)  max mem: 16413
[2023-08-29 22:41:20,050] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:41:20,050] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:41:20,051] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:41:20,051] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [122]  [ 80/160]  eta: 0:01:18  lr: 0.000011  min_lr: 0.000000  loss: 1.7797 (1.8243)  loss_scale: 16384.0000 (10214.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2073 (8.8896)  time: 1.0113 (0.5332 -- 5.5215)  data: 0.4573 (0.0002 -- 4.9866)  max mem: 16413
Epoch: [122]  [100/160]  eta: 0:00:57  lr: 0.000011  min_lr: 0.000000  loss: 1.8018 (1.8042)  loss_scale: 16384.0000 (11436.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0287 (8.7276)  time: 0.8340 (0.5110 -- 4.3335)  data: 0.2932 (0.0003 -- 3.8201)  max mem: 16413
Epoch: [122]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.7795 (1.8065)  loss_scale: 16384.0000 (12254.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4853 (8.7330)  time: 0.8855 (0.5336 -- 3.1019)  data: 0.3312 (0.0002 -- 2.5366)  max mem: 16413
Epoch: [122]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.5613 (1.7862)  loss_scale: 16384.0000 (12839.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5391 (8.6745)  time: 0.7044 (0.5296 -- 2.5296)  data: 0.1528 (0.0005 -- 1.9991)  max mem: 16413
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.5937 (1.7702)  loss_scale: 16384.0000 (13260.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6238 (8.7614)  time: 0.7256 (0.4956 -- 2.2107)  data: 0.1340 (0.0002 -- 1.6619)  max mem: 16413
Epoch: [122] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.5937 (1.7392)  loss_scale: 16384.0000 (13260.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6238 (8.7614)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2565 (0.2565)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4091 (2.4091 -- 2.4091)  data: 2.1714 (2.1714 -- 2.1714)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4698 (0.6973)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4307 (0.2039 -- 2.4091)  data: 0.1997 (0.0011 -- 2.1714)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4698 (0.6142)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.2963)  time: 0.2163 (0.1698 -- 0.2949)  data: 0.0015 (0.0001 -- 0.0094)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5236 (0.6749)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (95.4357)  time: 0.1963 (0.1331 -- 0.2949)  data: 0.0011 (0.0001 -- 0.0094)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 82.988 Acc@5 96.473 loss 0.649
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [123]  [  0/160]  eta: 0:19:09  lr: 0.000011  min_lr: 0.000000  loss: 0.9260 (0.9260)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8450 (7.8450)  time: 7.1871 (7.1871 -- 7.1871)  data: 6.6072 (6.6072 -- 6.6072)  max mem: 16413
Epoch: [123]  [ 20/160]  eta: 0:02:42  lr: 0.000011  min_lr: 0.000000  loss: 1.8934 (1.7919)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7660 (9.2835)  time: 0.8595 (0.5425 -- 4.0225)  data: 0.0869 (0.0007 -- 1.2268)  max mem: 16413
[2023-08-29 22:43:22,760] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:43:22,760] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:43:22,761] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:43:22,761] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [123]  [ 40/160]  eta: 0:01:59  lr: 0.000011  min_lr: 0.000000  loss: 1.7339 (1.7879)  loss_scale: 32768.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7292 (8.8122)  time: 0.8309 (0.5258 -- 2.4813)  data: 0.1730 (0.0009 -- 1.9452)  max mem: 16413
[2023-08-29 22:43:35,042] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19724
[2023-08-29 22:43:35,042] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19724
[2023-08-29 22:43:35,042] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:43:35,042] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:43:35,042] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [123]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000000  loss: 1.9352 (1.8230)  loss_scale: 16384.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9524 (8.6517)  time: 0.9095 (0.5221 -- 2.5444)  data: 0.1380 (0.0002 -- 1.4259)  max mem: 16413
Epoch: [123]  [ 80/160]  eta: 0:01:15  lr: 0.000011  min_lr: 0.000000  loss: 1.8418 (1.7951)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8820 (8.5337)  time: 0.8844 (0.5196 -- 3.9164)  data: 0.3319 (0.0003 -- 3.3725)  max mem: 16413
Epoch: [123]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.9258 (1.8149)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6071 (8.6860)  time: 0.9078 (0.5288 -- 3.2385)  data: 0.3242 (0.0006 -- 2.7032)  max mem: 16413
Epoch: [123]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.9155 (1.8196)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0925 (8.7759)  time: 0.7766 (0.5325 -- 2.2599)  data: 0.1540 (0.0003 -- 1.7434)  max mem: 16413
Epoch: [123]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.7801 (1.8051)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8333 (8.8187)  time: 0.9007 (0.5276 -- 3.1639)  data: 0.3404 (0.0003 -- 2.6239)  max mem: 16413
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7752 (1.7909)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2682 (8.9156)  time: 0.6996 (0.4984 -- 2.1791)  data: 0.0833 (0.0002 -- 1.6503)  max mem: 16413
Epoch: [123] Total time: 0:02:20 (0.8779 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7752 (1.7808)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2682 (8.9156)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2316 (0.2316)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3952 (2.3952 -- 2.3952)  data: 2.1689 (2.1689 -- 2.1689)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4713 (0.7185)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4250 (0.2074 -- 2.3952)  data: 0.2056 (0.0004 -- 2.1689)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4713 (0.6212)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.2963)  time: 0.2210 (0.1702 -- 0.3768)  data: 0.0155 (0.0001 -- 0.1912)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5037 (0.6889)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (95.4357)  time: 0.2052 (0.1329 -- 0.3768)  data: 0.0153 (0.0001 -- 0.1912)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 83.195 Acc@5 96.680 loss 0.657
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 84.02%
Epoch: [124]  [  0/160]  eta: 0:22:48  lr: 0.000011  min_lr: 0.000000  loss: 2.3392 (2.3392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9762 (7.9762)  time: 8.5535 (8.5535 -- 8.5535)  data: 6.1789 (6.1789 -- 6.1789)  max mem: 16413
[2023-08-29 22:45:37,003] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:45:37,004] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:45:37,004] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:45:37,004] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 22:45:42,918] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19857
[2023-08-29 22:45:42,918] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19857
[2023-08-29 22:45:42,918] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:45:42,918] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 22:45:42,918] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [124]  [ 20/160]  eta: 0:02:55  lr: 0.000011  min_lr: 0.000000  loss: 1.7374 (1.7817)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1256 (8.2431)  time: 0.8871 (0.5155 -- 4.2292)  data: 0.0016 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [124]  [ 40/160]  eta: 0:02:09  lr: 0.000011  min_lr: 0.000000  loss: 1.8478 (1.7829)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7606 (8.6213)  time: 0.9037 (0.5164 -- 3.1110)  data: 0.0425 (0.0002 -- 0.8211)  max mem: 16413
Epoch: [124]  [ 60/160]  eta: 0:01:41  lr: 0.000011  min_lr: 0.000000  loss: 1.8984 (1.7904)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1808 (8.6081)  time: 0.8640 (0.5096 -- 4.5515)  data: 0.2860 (0.0002 -- 3.2062)  max mem: 16413
Epoch: [124]  [ 80/160]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000000  loss: 1.8240 (1.7808)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4776 (8.6780)  time: 0.8464 (0.5192 -- 3.2412)  data: 0.2641 (0.0002 -- 2.6903)  max mem: 16413
Epoch: [124]  [100/160]  eta: 0:00:57  lr: 0.000011  min_lr: 0.000000  loss: 1.8365 (1.7883)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3896 (8.7293)  time: 0.8947 (0.5242 -- 5.1010)  data: 0.3307 (0.0004 -- 4.5803)  max mem: 16413
[2023-08-29 22:47:00,784] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19945
[2023-08-29 22:47:00,784] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:47:00,784] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19945
[2023-08-29 22:47:00,784] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:47:00,784] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [124]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.8913 (1.7889)  loss_scale: 8192.0000 (15842.3802)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2675 (8.7229)  time: 0.8075 (0.5083 -- 3.8549)  data: 0.2610 (0.0003 -- 3.3203)  max mem: 16413
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.7761 (1.7842)  loss_scale: 8192.0000 (14757.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5934 (8.8287)  time: 0.8463 (0.5214 -- 4.5472)  data: 0.3013 (0.0004 -- 3.9887)  max mem: 16413
[2023-08-29 22:47:41,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=117, lr=[2.5234501562042043e-07, 2.5234501562042043e-07, 3.364600208272272e-07, 3.364600208272272e-07, 4.486133611029696e-07, 4.486133611029696e-07, 5.981511481372928e-07, 5.981511481372928e-07, 7.975348641830571e-07, 7.975348641830571e-07, 1.0633798189107429e-06, 1.0633798189107429e-06, 1.417839758547657e-06, 1.417839758547657e-06, 1.890453011396876e-06, 1.890453011396876e-06, 2.5206040151958348e-06, 2.5206040151958348e-06, 3.3608053535944464e-06, 3.3608053535944464e-06, 4.4810738047925954e-06, 4.4810738047925954e-06, 5.974765073056793e-06, 5.974765073056793e-06, 7.966353430742392e-06, 7.966353430742392e-06, 1.0621804574323188e-05, 1.0621804574323188e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 22:47:41,440] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=17.575313321847517, CurrSamplesPerSec=24.610067158395534, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.6635 (1.7698)  loss_scale: 8192.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1688 (8.8742)  time: 0.7090 (0.4944 -- 2.8594)  data: 0.1862 (0.0002 -- 2.3260)  max mem: 16413
Epoch: [124] Total time: 0:02:23 (0.8950 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.6635 (1.7338)  loss_scale: 8192.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1688 (8.8742)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2255 (0.2255)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4077 (2.4077 -- 2.4077)  data: 2.1553 (2.1553 -- 2.1553)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4504 (0.6949)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4368 (0.1948 -- 2.4077)  data: 0.2170 (0.0006 -- 2.1553)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4504 (0.6150)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2208 (0.1702 -- 0.4357)  data: 0.0165 (0.0001 -- 0.2013)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5083 (0.6797)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (95.8506)  time: 0.2048 (0.1328 -- 0.4357)  data: 0.0157 (0.0001 -- 0.2013)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 83.402 Acc@5 96.888 loss 0.649
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.02%
Epoch: [125]  [  0/160]  eta: 0:17:48  lr: 0.000011  min_lr: 0.000000  loss: 2.1048 (2.1048)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2006 (9.2006)  time: 6.6805 (6.6805 -- 6.6805)  data: 5.5315 (5.5315 -- 5.5315)  max mem: 16413
Epoch: [125]  [ 20/160]  eta: 0:02:53  lr: 0.000011  min_lr: 0.000000  loss: 1.8294 (1.8532)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0455 (9.3654)  time: 0.9677 (0.5274 -- 4.6673)  data: 0.0014 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [125]  [ 40/160]  eta: 0:02:06  lr: 0.000011  min_lr: 0.000000  loss: 1.8641 (1.7994)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9798 (8.9875)  time: 0.8595 (0.5259 -- 3.8220)  data: 0.0013 (0.0005 -- 0.0039)  max mem: 16413
Epoch: [125]  [ 60/160]  eta: 0:01:42  lr: 0.000011  min_lr: 0.000000  loss: 1.7594 (1.7968)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8656 (8.7198)  time: 0.9503 (0.5165 -- 4.0251)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
[2023-08-29 22:49:03,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:49:03,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:49:03,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:49:03,429] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [125]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 1.9427 (1.8312)  loss_scale: 8192.0000 (8899.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3174 (9.0318)  time: 0.7594 (0.5217 -- 3.4986)  data: 0.0012 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [125]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 1.8462 (1.8312)  loss_scale: 16384.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5818 (8.9588)  time: 0.8641 (0.5278 -- 3.4219)  data: 0.0015 (0.0006 -- 0.0033)  max mem: 16413
[2023-08-29 22:49:39,302] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20118
[2023-08-29 22:49:39,302] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20118
[2023-08-29 22:49:39,303] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:49:39,303] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:49:39,303] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [125]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.5396 (1.7879)  loss_scale: 16384.0000 (11170.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6535 (8.9868)  time: 0.9193 (0.5357 -- 2.4922)  data: 0.0022 (0.0005 -- 0.0076)  max mem: 16413
Epoch: [125]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.8352 (1.7856)  loss_scale: 8192.0000 (10748.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6115 (9.0426)  time: 0.8630 (0.5124 -- 2.6983)  data: 0.0012 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7325 (1.7860)  loss_scale: 8192.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9836 (8.9505)  time: 0.6877 (0.4975 -- 2.9504)  data: 0.0009 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [125] Total time: 0:02:23 (0.8973 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7325 (1.7820)  loss_scale: 8192.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9836 (8.9505)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2572 (0.2572)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5176 (2.5176 -- 2.5176)  data: 2.2843 (2.2843 -- 2.2843)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4246 (0.6947)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4477 (0.1925 -- 2.5176)  data: 0.2291 (0.0007 -- 2.2843)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4246 (0.6109)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2155 (0.1722 -- 0.4623)  data: 0.0119 (0.0001 -- 0.2247)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5114 (0.6756)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2005 (0.1333 -- 0.4623)  data: 0.0116 (0.0001 -- 0.2247)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 83.195 Acc@5 97.303 loss 0.659
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 84.02%
Epoch: [126]  [  0/160]  eta: 0:23:27  lr: 0.000010  min_lr: 0.000000  loss: 1.7187 (1.7187)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9811 (7.9811)  time: 8.7999 (8.7999 -- 8.7999)  data: 8.2596 (8.2596 -- 8.2596)  max mem: 16413
Epoch: [126]  [ 20/160]  eta: 0:03:02  lr: 0.000010  min_lr: 0.000000  loss: 1.6026 (1.6828)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0806 (9.4712)  time: 0.9273 (0.5246 -- 5.2328)  data: 0.3833 (0.0004 -- 4.6880)  max mem: 16413
Epoch: [126]  [ 40/160]  eta: 0:02:11  lr: 0.000010  min_lr: 0.000000  loss: 1.7743 (1.6889)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2173 (9.2320)  time: 0.8870 (0.5222 -- 3.6331)  data: 0.3416 (0.0004 -- 3.0977)  max mem: 16413
Epoch: [126]  [ 60/160]  eta: 0:01:39  lr: 0.000010  min_lr: 0.000000  loss: 1.6473 (1.6881)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3181 (8.7359)  time: 0.7815 (0.5215 -- 2.9827)  data: 0.2341 (0.0003 -- 2.4710)  max mem: 16413
Epoch: [126]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 1.9511 (1.7347)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8498 (8.8028)  time: 0.8214 (0.5338 -- 2.9325)  data: 0.1529 (0.0002 -- 1.5162)  max mem: 16413
[2023-08-29 22:51:44,515] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:51:44,516] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:51:44,516] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:51:44,517] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [126]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 1.7608 (1.7284)  loss_scale: 16384.0000 (9327.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9508 (8.6632)  time: 0.9211 (0.5263 -- 2.9021)  data: 0.0015 (0.0003 -- 0.0051)  max mem: 16413
Epoch: [126]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.7912 (1.7506)  loss_scale: 16384.0000 (10493.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1335 (8.7687)  time: 0.8192 (0.5295 -- 3.2407)  data: 0.0015 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [126]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.9080 (1.7667)  loss_scale: 16384.0000 (11329.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9409 (8.7486)  time: 0.8325 (0.5254 -- 2.4016)  data: 0.0252 (0.0003 -- 0.4749)  max mem: 16413
[2023-08-29 22:52:42,278] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20316
[2023-08-29 22:52:42,278] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20316
[2023-08-29 22:52:42,278] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:52:42,278] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:52:42,279] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7207 (1.7718)  loss_scale: 16384.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3661 (8.7438)  time: 0.8537 (0.4818 -- 3.7088)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [126] Total time: 0:02:24 (0.9003 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7207 (1.7589)  loss_scale: 16384.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3661 (8.7438)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2480 (0.2480)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6141 (2.6141 -- 2.6141)  data: 2.3876 (2.3876 -- 2.3876)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4341 (0.7006)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4361 (0.1968 -- 2.6141)  data: 0.2226 (0.0005 -- 2.3876)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4507 (0.6184)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2105 (0.1691 -- 0.2802)  data: 0.0075 (0.0001 -- 0.0853)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5225 (0.6832)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (95.4357)  time: 0.1962 (0.1332 -- 0.2802)  data: 0.0072 (0.0001 -- 0.0853)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 83.402 Acc@5 96.680 loss 0.658
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.02%
Epoch: [127]  [  0/160]  eta: 0:20:29  lr: 0.000010  min_lr: 0.000000  loss: 2.1050 (2.1050)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8454 (6.8454)  time: 7.6851 (7.6851 -- 7.6851)  data: 7.1627 (7.1627 -- 7.1627)  max mem: 16413
Epoch: [127]  [ 20/160]  eta: 0:02:39  lr: 0.000010  min_lr: 0.000000  loss: 1.9605 (1.8804)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9080 (8.3536)  time: 0.8142 (0.5230 -- 2.7039)  data: 0.1860 (0.0002 -- 2.1477)  max mem: 16413
Epoch: [127]  [ 40/160]  eta: 0:02:10  lr: 0.000010  min_lr: 0.000000  loss: 1.4619 (1.7228)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5375 (8.7311)  time: 1.0284 (0.5361 -- 5.0037)  data: 0.4740 (0.0003 -- 4.4637)  max mem: 16413
Epoch: [127]  [ 60/160]  eta: 0:01:38  lr: 0.000010  min_lr: 0.000000  loss: 1.7626 (1.7497)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0362 (8.7199)  time: 0.7675 (0.5283 -- 3.8295)  data: 0.2147 (0.0001 -- 3.2950)  max mem: 16413
Epoch: [127]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000000  loss: 1.7624 (1.7661)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8502 (8.7737)  time: 0.9134 (0.5222 -- 2.9885)  data: 0.3652 (0.0004 -- 2.4681)  max mem: 16413
Epoch: [127]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.7415 (1.7638)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6945 (8.7709)  time: 0.7524 (0.5301 -- 2.7989)  data: 0.2040 (0.0003 -- 2.2616)  max mem: 16413
Epoch: [127]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.7766 (1.7633)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9997 (8.8486)  time: 0.9952 (0.5124 -- 5.2230)  data: 0.4486 (0.0003 -- 4.6986)  max mem: 16413
[2023-08-29 22:54:48,677] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:54:48,677] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:54:48,677] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:54:48,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [127]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.7426 (1.7635)  loss_scale: 16384.0000 (9121.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2497 (8.8189)  time: 0.8337 (0.5128 -- 3.4827)  data: 0.2880 (0.0004 -- 2.9732)  max mem: 16413
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7852 (1.7758)  loss_scale: 16384.0000 (9984.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0535 (8.9061)  time: 0.5951 (0.4963 -- 1.7457)  data: 0.0630 (0.0002 -- 1.2222)  max mem: 16413
Epoch: [127] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7852 (1.7823)  loss_scale: 16384.0000 (9984.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0535 (8.9061)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2618 (0.2618)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5131 (2.5131 -- 2.5131)  data: 2.2917 (2.2917 -- 2.2917)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4530 (0.6958)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4254 (0.1928 -- 2.5131)  data: 0.2192 (0.0006 -- 2.2917)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4530 (0.6124)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.2963)  time: 0.2149 (0.1704 -- 0.4006)  data: 0.0188 (0.0001 -- 0.2125)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5140 (0.6766)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.2656)  time: 0.2039 (0.1325 -- 0.4006)  data: 0.0186 (0.0001 -- 0.2125)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.653
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [128]  [  0/160]  eta: 0:21:20  lr: 0.000010  min_lr: 0.000000  loss: 1.9802 (1.9802)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3460 (7.3460)  time: 8.0055 (8.0055 -- 8.0055)  data: 7.4643 (7.4643 -- 7.4643)  max mem: 16413
Epoch: [128]  [ 20/160]  eta: 0:02:47  lr: 0.000010  min_lr: 0.000000  loss: 1.6777 (1.7063)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3908 (8.2004)  time: 0.8595 (0.5269 -- 3.9918)  data: 0.3092 (0.0003 -- 3.4655)  max mem: 16413
Epoch: [128]  [ 40/160]  eta: 0:02:12  lr: 0.000010  min_lr: 0.000000  loss: 1.7742 (1.7644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2178 (8.6325)  time: 1.0014 (0.5309 -- 3.7785)  data: 0.4508 (0.0005 -- 3.2444)  max mem: 16413
[2023-08-29 22:56:17,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20533
[2023-08-29 22:56:17,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:56:17,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20533
[2023-08-29 22:56:17,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:56:17,167] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [128]  [ 60/160]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000000  loss: 1.7443 (1.7598)  loss_scale: 16384.0000 (15309.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7517 (8.4269)  time: 0.8021 (0.5142 -- 3.8909)  data: 0.2586 (0.0002 -- 3.3620)  max mem: 16413
Epoch: [128]  [ 80/160]  eta: 0:01:19  lr: 0.000010  min_lr: 0.000000  loss: 1.6558 (1.7385)  loss_scale: 8192.0000 (13552.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7259 (8.5701)  time: 0.9691 (0.5211 -- 4.5669)  data: 0.4199 (0.0002 -- 4.0490)  max mem: 16413
Epoch: [128]  [100/160]  eta: 0:00:57  lr: 0.000010  min_lr: 0.000000  loss: 1.9781 (1.7662)  loss_scale: 8192.0000 (12490.7723)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5065 (8.6514)  time: 0.7729 (0.5343 -- 2.8259)  data: 0.2203 (0.0004 -- 2.2841)  max mem: 16413
Epoch: [128]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.8148 (1.7896)  loss_scale: 8192.0000 (11780.2314)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8799 (8.7007)  time: 0.9165 (0.5189 -- 3.7732)  data: 0.3655 (0.0003 -- 3.2309)  max mem: 16413
Epoch: [128]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.7800 (1.7832)  loss_scale: 8192.0000 (11271.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2255 (8.6472)  time: 0.7495 (0.5217 -- 2.6638)  data: 0.1973 (0.0002 -- 2.1340)  max mem: 16413
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.5634 (1.7747)  loss_scale: 8192.0000 (10905.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4712 (8.6734)  time: 0.7130 (0.4962 -- 3.2282)  data: 0.1958 (0.0002 -- 2.6895)  max mem: 16413
Epoch: [128] Total time: 0:02:23 (0.8949 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.5634 (1.7679)  loss_scale: 8192.0000 (10905.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4712 (8.6734)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2728 (0.2728)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4386 (2.4386 -- 2.4386)  data: 2.1802 (2.1802 -- 2.1802)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4559 (0.7007)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4273 (0.2088 -- 2.4386)  data: 0.2026 (0.0007 -- 2.1802)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4559 (0.6176)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2181 (0.1691 -- 0.2542)  data: 0.0082 (0.0001 -- 0.0623)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5285 (0.6827)  acc1: 85.7143 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.1986 (0.1332 -- 0.2542)  data: 0.0080 (0.0001 -- 0.0623)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 81.535 Acc@5 96.888 loss 0.667
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 84.02%
Epoch: [129]  [  0/160]  eta: 0:19:07  lr: 0.000010  min_lr: 0.000000  loss: 0.9857 (0.9857)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0784 (12.0784)  time: 7.1691 (7.1691 -- 7.1691)  data: 6.5952 (6.5952 -- 6.5952)  max mem: 16413
Epoch: [129]  [ 20/160]  eta: 0:02:36  lr: 0.000010  min_lr: 0.000000  loss: 1.8492 (1.8469)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8498 (8.8907)  time: 0.8181 (0.5205 -- 3.1950)  data: 0.2528 (0.0004 -- 2.6655)  max mem: 16413
[2023-08-29 22:58:17,624] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:58:17,624] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 22:58:17,625] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 22:58:17,625] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [129]  [ 40/160]  eta: 0:02:09  lr: 0.000010  min_lr: 0.000000  loss: 1.8772 (1.8541)  loss_scale: 16384.0000 (11988.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8263 (8.9843)  time: 1.0294 (0.5328 -- 3.1072)  data: 0.2763 (0.0005 -- 2.5749)  max mem: 16413
Epoch: [129]  [ 60/160]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000000  loss: 1.7560 (1.8336)  loss_scale: 16384.0000 (13429.5082)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5719 (8.9968)  time: 0.8619 (0.5208 -- 3.3987)  data: 0.2133 (0.0004 -- 2.8931)  max mem: 16413
Epoch: [129]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000000  loss: 1.7754 (1.8220)  loss_scale: 16384.0000 (14159.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6358 (8.9167)  time: 0.8595 (0.5252 -- 2.3601)  data: 0.1697 (0.0002 -- 1.6928)  max mem: 16413
Epoch: [129]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.7495 (1.8119)  loss_scale: 16384.0000 (14599.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6000 (8.9598)  time: 0.8520 (0.5250 -- 2.7957)  data: 0.2077 (0.0003 -- 2.2555)  max mem: 16413
[2023-08-29 22:59:29,508] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20742
[2023-08-29 22:59:29,508] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20742
[2023-08-29 22:59:29,549] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:59:29,549] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 22:59:29,549] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [129]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.8874 (1.8297)  loss_scale: 8192.0000 (13608.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9674 (9.0330)  time: 0.8582 (0.5349 -- 2.7259)  data: 0.2915 (0.0002 -- 2.1807)  max mem: 16413
Epoch: [129]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.7317 (1.8258)  loss_scale: 8192.0000 (12839.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4152 (8.9488)  time: 0.8504 (0.5303 -- 2.3999)  data: 0.3009 (0.0002 -- 1.8737)  max mem: 16413
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7920 (1.8117)  loss_scale: 8192.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2682 (8.9028)  time: 0.6680 (0.4965 -- 1.2868)  data: 0.1491 (0.0001 -- 0.7750)  max mem: 16413
Epoch: [129] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7920 (1.8045)  loss_scale: 8192.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2682 (8.9028)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2656 (0.2656)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4563 (2.4563 -- 2.4563)  data: 2.2490 (2.2490 -- 2.2490)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4471 (0.7031)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4138 (0.1931 -- 2.4563)  data: 0.2056 (0.0008 -- 2.2490)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4475 (0.6222)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2152 (0.1698 -- 0.3177)  data: 0.0120 (0.0001 -- 0.1330)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5269 (0.6803)  acc1: 85.7143 (81.3278)  acc5: 100.0000 (97.0954)  time: 0.2017 (0.1326 -- 0.3177)  data: 0.0116 (0.0001 -- 0.1330)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 81.950 Acc@5 97.303 loss 0.666
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 84.02%
Epoch: [130]  [  0/160]  eta: 0:15:35  lr: 0.000009  min_lr: 0.000000  loss: 1.8461 (1.8461)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5943 (10.5943)  time: 5.8470 (5.8470 -- 5.8470)  data: 5.3173 (5.3173 -- 5.3173)  max mem: 16413
Epoch: [130]  [ 20/160]  eta: 0:02:43  lr: 0.000009  min_lr: 0.000000  loss: 1.9313 (1.8040)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3394 (8.5632)  time: 0.9367 (0.5208 -- 3.8325)  data: 0.2362 (0.0003 -- 3.3010)  max mem: 16413
Epoch: [130]  [ 40/160]  eta: 0:02:12  lr: 0.000009  min_lr: 0.000000  loss: 1.8794 (1.8322)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1941 (8.5443)  time: 1.0301 (0.5299 -- 4.2435)  data: 0.4733 (0.0003 -- 3.6952)  max mem: 16413
Epoch: [130]  [ 60/160]  eta: 0:01:42  lr: 0.000009  min_lr: 0.000000  loss: 1.7674 (1.7751)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5623 (8.4755)  time: 0.8530 (0.5213 -- 3.8072)  data: 0.3032 (0.0001 -- 3.2450)  max mem: 16413
[2023-08-29 23:01:31,242] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:01:31,243] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:01:31,243] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:01:31,243] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [130]  [ 80/160]  eta: 0:01:17  lr: 0.000009  min_lr: 0.000000  loss: 1.6906 (1.7619)  loss_scale: 8192.0000 (9203.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1394 (8.4485)  time: 0.8263 (0.5108 -- 3.9236)  data: 0.2726 (0.0001 -- 3.3886)  max mem: 16413
[2023-08-29 23:01:55,977] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20896
[2023-08-29 23:01:55,977] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20896
[2023-08-29 23:01:55,978] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:01:55,978] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:01:55,979] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [130]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.8310 (1.7817)  loss_scale: 16384.0000 (10219.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9407 (8.5653)  time: 0.8298 (0.5170 -- 4.5829)  data: 0.2777 (0.0004 -- 4.0682)  max mem: 16413
Epoch: [130]  [120/160]  eta: 0:00:38  lr: 0.000009  min_lr: 0.000000  loss: 1.7911 (1.7902)  loss_scale: 8192.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2215 (8.6930)  time: 1.0073 (0.5204 -- 4.2457)  data: 0.4567 (0.0002 -- 3.7289)  max mem: 16413
Epoch: [130]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.5964 (1.7742)  loss_scale: 8192.0000 (9644.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0375 (8.7521)  time: 0.8094 (0.5145 -- 3.9961)  data: 0.2673 (0.0002 -- 3.4407)  max mem: 16413
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.6035 (1.7600)  loss_scale: 8192.0000 (9472.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4656 (8.7907)  time: 0.6177 (0.4965 -- 2.5305)  data: 0.1014 (0.0002 -- 2.0180)  max mem: 16413
Epoch: [130] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.6035 (1.7932)  loss_scale: 8192.0000 (9472.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4656 (8.7907)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2741 (0.2741)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5455 (2.5455 -- 2.5455)  data: 2.3294 (2.3294 -- 2.3294)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4464 (0.7032)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4382 (0.2084 -- 2.5455)  data: 0.2223 (0.0007 -- 2.3294)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4567 (0.6182)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2132 (0.1699 -- 0.2906)  data: 0.0090 (0.0001 -- 0.0901)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5148 (0.6813)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (95.8506)  time: 0.1973 (0.1326 -- 0.2906)  data: 0.0086 (0.0001 -- 0.0901)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 82.988 Acc@5 96.680 loss 0.664
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [131]  [  0/160]  eta: 0:22:02  lr: 0.000009  min_lr: 0.000000  loss: 1.2614 (1.2614)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2772 (7.2772)  time: 8.2683 (8.2683 -- 8.2683)  data: 7.7266 (7.7266 -- 7.7266)  max mem: 16413
Epoch: [131]  [ 20/160]  eta: 0:02:48  lr: 0.000009  min_lr: 0.000000  loss: 1.7440 (1.7134)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8783 (8.5907)  time: 0.8471 (0.5287 -- 3.1548)  data: 0.2390 (0.0003 -- 2.6376)  max mem: 16413
[2023-08-29 23:03:36,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=122, lr=[2.1646069701450603e-07, 2.1646069701450603e-07, 2.8861426268600806e-07, 2.8861426268600806e-07, 3.848190169146774e-07, 3.848190169146774e-07, 5.130920225529032e-07, 5.130920225529032e-07, 6.841226967372042e-07, 6.841226967372042e-07, 9.121635956496057e-07, 9.121635956496057e-07, 1.2162181275328075e-06, 1.2162181275328075e-06, 1.6216241700437434e-06, 1.6216241700437434e-06, 2.1621655600583244e-06, 2.1621655600583244e-06, 2.8828874134110993e-06, 2.8828874134110993e-06, 3.8438498845481324e-06, 3.8438498845481324e-06, 5.1251331793975105e-06, 5.1251331793975105e-06, 6.833510905863347e-06, 6.833510905863347e-06, 9.111347874484462e-06, 9.111347874484462e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 23:03:36,556] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=17.64407186863963, CurrSamplesPerSec=24.002758343105643, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [131]  [ 40/160]  eta: 0:02:14  lr: 0.000009  min_lr: 0.000000  loss: 1.8328 (1.7850)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3653 (8.8510)  time: 1.0339 (0.5121 -- 4.5367)  data: 0.4863 (0.0003 -- 4.0265)  max mem: 16413
Epoch: [131]  [ 60/160]  eta: 0:01:40  lr: 0.000009  min_lr: 0.000000  loss: 1.8001 (1.7780)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2316 (9.0352)  time: 0.7787 (0.5268 -- 2.8477)  data: 0.2264 (0.0002 -- 2.3121)  max mem: 16413
[2023-08-29 23:03:59,924] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:03:59,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:03:59,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:03:59,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:04:00,464] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21026
[2023-08-29 23:04:00,464] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21026
[2023-08-29 23:04:00,464] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:04:00,464] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:04:00,464] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [131]  [ 80/160]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000000  loss: 1.6549 (1.7720)  loss_scale: 8192.0000 (8293.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7955 (8.9614)  time: 0.9255 (0.5193 -- 3.7322)  data: 0.3819 (0.0004 -- 3.2150)  max mem: 16413
Epoch: [131]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.7606 (1.7743)  loss_scale: 8192.0000 (8273.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2021 (8.8837)  time: 0.7243 (0.5149 -- 3.9969)  data: 0.1751 (0.0003 -- 3.4798)  max mem: 16413
Epoch: [131]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.7228 (1.7597)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5690 (8.9174)  time: 0.9199 (0.5244 -- 3.8987)  data: 0.3684 (0.0003 -- 3.3211)  max mem: 16413
Epoch: [131]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.6311 (1.7565)  loss_scale: 8192.0000 (8250.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2443 (8.8611)  time: 0.8282 (0.5380 -- 3.4338)  data: 0.2756 (0.0002 -- 2.8993)  max mem: 16413
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.6292 (1.7374)  loss_scale: 8192.0000 (8243.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5363 (8.9643)  time: 0.7183 (0.4963 -- 3.2862)  data: 0.1976 (0.0002 -- 2.7614)  max mem: 16413
Epoch: [131] Total time: 0:02:23 (0.8955 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.6292 (1.7572)  loss_scale: 8192.0000 (8243.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5363 (8.9643)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2619 (0.2619)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3326 (2.3326 -- 2.3326)  data: 2.0540 (2.0540 -- 2.0540)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4290 (0.7026)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4109 (0.2039 -- 2.3326)  data: 0.1879 (0.0008 -- 2.0540)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4548 (0.6223)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2234 (0.1691 -- 0.5127)  data: 0.0173 (0.0001 -- 0.3224)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5210 (0.6832)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (95.8506)  time: 0.2073 (0.1323 -- 0.5127)  data: 0.0169 (0.0001 -- 0.3224)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 82.988 Acc@5 96.680 loss 0.655
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [132]  [  0/160]  eta: 0:17:46  lr: 0.000009  min_lr: 0.000000  loss: 1.6176 (1.6176)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.2059 (13.2059)  time: 6.6670 (6.6670 -- 6.6670)  data: 4.1947 (4.1947 -- 4.1947)  max mem: 16413
Epoch: [132]  [ 20/160]  eta: 0:02:43  lr: 0.000009  min_lr: 0.000000  loss: 1.7731 (1.7409)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8016 (9.2432)  time: 0.8916 (0.5281 -- 2.0830)  data: 0.2838 (0.0009 -- 1.5429)  max mem: 16413
[2023-08-29 23:06:05,470] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:06:05,471] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:06:05,471] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:06:05,471] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [132]  [ 40/160]  eta: 0:02:05  lr: 0.000009  min_lr: 0.000000  loss: 1.8316 (1.7938)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1610 (8.8497)  time: 0.9159 (0.5244 -- 3.9515)  data: 0.3695 (0.0006 -- 3.4277)  max mem: 16413
Epoch: [132]  [ 60/160]  eta: 0:01:42  lr: 0.000009  min_lr: 0.000000  loss: 1.7081 (1.7790)  loss_scale: 16384.0000 (11683.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8095 (8.9867)  time: 0.9879 (0.5184 -- 4.1596)  data: 0.4466 (0.0003 -- 3.6521)  max mem: 16413
Epoch: [132]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 1.9752 (1.8010)  loss_scale: 16384.0000 (12844.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8325 (8.8906)  time: 0.7225 (0.5204 -- 2.6584)  data: 0.1739 (0.0001 -- 2.1329)  max mem: 16413
Epoch: [132]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.7545 (1.7850)  loss_scale: 16384.0000 (13545.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8481 (8.8163)  time: 0.8484 (0.5353 -- 2.6612)  data: 0.2297 (0.0004 -- 2.1189)  max mem: 16413
Epoch: [132]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.7082 (1.7779)  loss_scale: 16384.0000 (14014.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0572 (8.8388)  time: 0.8787 (0.5318 -- 2.9659)  data: 0.2679 (0.0005 -- 2.4037)  max mem: 16413
Epoch: [132]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.9013 (1.7971)  loss_scale: 16384.0000 (14350.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2857 (8.8973)  time: 0.7824 (0.5299 -- 2.5429)  data: 0.0759 (0.0002 -- 0.9409)  max mem: 16413
[2023-08-29 23:07:39,240] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21266
[2023-08-29 23:07:39,240] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21266
[2023-08-29 23:07:39,240] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:07:39,241] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-29 23:07:39,240] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8011 (1.8019)  loss_scale: 8192.0000 (13875.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0867 (8.9195)  time: 0.7712 (0.4961 -- 3.8465)  data: 0.0011 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [132] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.8011 (1.7660)  loss_scale: 8192.0000 (13875.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0867 (8.9195)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2439 (0.2439)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4702 (2.4702 -- 2.4702)  data: 2.2099 (2.2099 -- 2.2099)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4569 (0.7137)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4232 (0.1976 -- 2.4702)  data: 0.2024 (0.0006 -- 2.2099)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4569 (0.6195)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.2963)  time: 0.2135 (0.1697 -- 0.3055)  data: 0.0073 (0.0001 -- 0.0929)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5238 (0.6826)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (95.8506)  time: 0.1980 (0.1326 -- 0.3055)  data: 0.0070 (0.0001 -- 0.0929)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 82.988 Acc@5 96.888 loss 0.651
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [133]  [  0/160]  eta: 0:16:11  lr: 0.000009  min_lr: 0.000000  loss: 1.9675 (1.9675)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9615 (9.9615)  time: 6.0709 (6.0709 -- 6.0709)  data: 5.5401 (5.5401 -- 5.5401)  max mem: 16413
Epoch: [133]  [ 20/160]  eta: 0:02:43  lr: 0.000009  min_lr: 0.000000  loss: 1.6015 (1.6081)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5285 (8.1713)  time: 0.9227 (0.5337 -- 3.2507)  data: 0.3354 (0.0007 -- 2.7276)  max mem: 16413
Epoch: [133]  [ 40/160]  eta: 0:02:06  lr: 0.000009  min_lr: 0.000000  loss: 1.5643 (1.5834)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7491 (8.2699)  time: 0.9332 (0.5312 -- 2.8427)  data: 0.0900 (0.0002 -- 1.2272)  max mem: 16413
Epoch: [133]  [ 60/160]  eta: 0:01:37  lr: 0.000009  min_lr: 0.000000  loss: 1.9207 (1.6886)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1262 (8.4796)  time: 0.8011 (0.5278 -- 2.3615)  data: 0.1556 (0.0005 -- 1.8192)  max mem: 16413
Epoch: [133]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 1.9497 (1.7451)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8596 (8.5892)  time: 0.9109 (0.5200 -- 3.1023)  data: 0.0037 (0.0004 -- 0.0365)  max mem: 16413
Epoch: [133]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.6919 (1.7435)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9657 (8.5066)  time: 0.8433 (0.5364 -- 3.5357)  data: 0.0014 (0.0007 -- 0.0026)  max mem: 16413
[2023-08-29 23:09:43,408] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:09:43,408] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:09:43,409] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:09:43,409] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [133]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.7692 (1.7531)  loss_scale: 8192.0000 (8598.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4614 (8.5754)  time: 0.9634 (0.5222 -- 3.2047)  data: 0.3008 (0.0001 -- 2.6988)  max mem: 16413
Epoch: [133]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7005 (1.7429)  loss_scale: 16384.0000 (9702.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8218 (8.5424)  time: 0.7487 (0.5324 -- 2.4926)  data: 0.0013 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7657 (1.7381)  loss_scale: 16384.0000 (10496.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8585 (8.6275)  time: 0.7132 (0.4975 -- 1.8267)  data: 0.0008 (0.0003 -- 0.0017)  max mem: 16413
Epoch: [133] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7657 (1.7669)  loss_scale: 16384.0000 (10496.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8585 (8.6275)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2424 (0.2424)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3217 (2.3217 -- 2.3217)  data: 2.0705 (2.0705 -- 2.0705)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3973 (0.6993)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4093 (0.1927 -- 2.3217)  data: 0.1910 (0.0002 -- 2.0705)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4229 (0.6137)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2236 (0.1740 -- 0.4089)  data: 0.0160 (0.0001 -- 0.1907)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5154 (0.6725)  acc1: 88.8889 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2094 (0.1328 -- 0.4089)  data: 0.0158 (0.0001 -- 0.1907)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 82.988 Acc@5 97.303 loss 0.642
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [134]  [  0/160]  eta: 0:20:20  lr: 0.000008  min_lr: 0.000000  loss: 2.3130 (2.3130)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7712 (12.7712)  time: 7.6292 (7.6292 -- 7.6292)  data: 5.5295 (5.5295 -- 5.5295)  max mem: 16413
Epoch: [134]  [ 20/160]  eta: 0:02:41  lr: 0.000008  min_lr: 0.000000  loss: 1.7723 (1.8028)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0100 (9.2396)  time: 0.8278 (0.5234 -- 3.0057)  data: 0.1773 (0.0007 -- 2.1681)  max mem: 16413
Epoch: [134]  [ 40/160]  eta: 0:01:59  lr: 0.000008  min_lr: 0.000000  loss: 1.7549 (1.7655)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0307 (8.8259)  time: 0.8325 (0.5250 -- 2.9487)  data: 0.0014 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [134]  [ 60/160]  eta: 0:01:40  lr: 0.000008  min_lr: 0.000000  loss: 1.6852 (1.7457)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8654 (8.5759)  time: 1.0168 (0.5143 -- 4.1003)  data: 0.0009 (0.0003 -- 0.0022)  max mem: 16413
Epoch: [134]  [ 80/160]  eta: 0:01:13  lr: 0.000008  min_lr: 0.000000  loss: 1.6910 (1.7347)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4939 (8.4994)  time: 0.6869 (0.5306 -- 1.8617)  data: 0.0029 (0.0001 -- 0.0145)  max mem: 16413
[2023-08-29 23:11:42,661] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:11:42,661] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:11:42,663] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:11:42,663] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:11:43,213] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21524
[2023-08-29 23:11:43,213] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21524
[2023-08-29 23:11:43,213] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:11:43,213] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:11:43,214] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.7357 (1.7287)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6411 (8.7338)  time: 0.9691 (0.5305 -- 3.3610)  data: 0.0014 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [134]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.7168 (1.7108)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7069 (9.0635)  time: 0.8055 (0.5266 -- 3.6587)  data: 0.0019 (0.0003 -- 0.0157)  max mem: 16413
Epoch: [134]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.9025 (1.7375)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5255 (9.0166)  time: 0.9532 (0.5283 -- 3.9865)  data: 0.0017 (0.0005 -- 0.0086)  max mem: 16413
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.5984 (1.7291)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7564 (8.9550)  time: 0.6984 (0.4947 -- 3.1127)  data: 0.0005 (0.0002 -- 0.0012)  max mem: 16413
Epoch: [134] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.5984 (1.7285)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7564 (8.9550)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2334 (0.2334)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3890 (2.3890 -- 2.3890)  data: 2.1587 (2.1587 -- 2.1587)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3905 (0.7010)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4201 (0.2026 -- 2.3890)  data: 0.2038 (0.0006 -- 2.1587)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4243 (0.6189)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2215 (0.1698 -- 0.4627)  data: 0.0174 (0.0001 -- 0.2612)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5220 (0.6793)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (95.8506)  time: 0.2063 (0.1330 -- 0.4627)  data: 0.0171 (0.0001 -- 0.2612)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 82.988 Acc@5 96.888 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [135]  [  0/160]  eta: 0:20:49  lr: 0.000008  min_lr: 0.000000  loss: 1.4098 (1.4098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3860 (7.3860)  time: 7.8115 (7.8115 -- 7.8115)  data: 5.8502 (5.8502 -- 5.8502)  max mem: 16413
Epoch: [135]  [ 20/160]  eta: 0:02:43  lr: 0.000008  min_lr: 0.000000  loss: 1.7111 (1.7371)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8548 (10.3812)  time: 0.8394 (0.5166 -- 2.6179)  data: 0.2566 (0.0002 -- 2.0821)  max mem: 16413
Epoch: [135]  [ 40/160]  eta: 0:02:03  lr: 0.000008  min_lr: 0.000000  loss: 1.6781 (1.7573)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8085 (9.7731)  time: 0.8836 (0.5221 -- 3.3198)  data: 0.3395 (0.0003 -- 2.7968)  max mem: 16413
[2023-08-29 23:13:48,754] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:13:48,754] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:13:48,758] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:13:48,758] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:13:53,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21659
[2023-08-29 23:13:53,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21659
[2023-08-29 23:13:53,034] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:13:53,034] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:13:53,034] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [135]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000000  loss: 1.6688 (1.7485)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0478 (9.4423)  time: 0.8835 (0.5177 -- 2.6989)  data: 0.3376 (0.0003 -- 2.1903)  max mem: 16413
Epoch: [135]  [ 80/160]  eta: 0:01:15  lr: 0.000008  min_lr: 0.000000  loss: 1.9129 (1.7672)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5395 (9.3215)  time: 0.8305 (0.5304 -- 2.2090)  data: 0.1557 (0.0004 -- 1.6790)  max mem: 16413
Epoch: [135]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 1.6956 (1.7600)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3286 (9.2646)  time: 0.8712 (0.5295 -- 2.5681)  data: 0.3264 (0.0003 -- 2.0340)  max mem: 16413
Epoch: [135]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.7967 (1.7648)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3242 (9.1524)  time: 0.9305 (0.5300 -- 2.3167)  data: 0.3922 (0.0002 -- 1.7882)  max mem: 16413
Epoch: [135]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.5451 (1.7454)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9020 (9.0794)  time: 0.8289 (0.5149 -- 2.6384)  data: 0.2882 (0.0001 -- 2.1190)  max mem: 16413
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7988 (1.7532)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8079 (8.9625)  time: 0.6330 (0.4977 -- 2.7065)  data: 0.1099 (0.0002 -- 2.1656)  max mem: 16413
Epoch: [135] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7988 (1.7650)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8079 (8.9625)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2369 (0.2369)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3007 (2.3007 -- 2.3007)  data: 2.0542 (2.0542 -- 2.0542)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4164 (0.6961)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4117 (0.2002 -- 2.3007)  data: 0.1921 (0.0009 -- 2.0542)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4395 (0.6202)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2245 (0.1691 -- 0.4212)  data: 0.0188 (0.0001 -- 0.2231)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5190 (0.6786)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2075 (0.1334 -- 0.4212)  data: 0.0185 (0.0001 -- 0.2231)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 83.402 Acc@5 97.095 loss 0.643
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.02%
Epoch: [136]  [  0/160]  eta: 0:18:30  lr: 0.000008  min_lr: 0.000000  loss: 2.2793 (2.2793)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6783 (12.6783)  time: 6.9382 (6.9382 -- 6.9382)  data: 6.3458 (6.3458 -- 6.3458)  max mem: 16413
Epoch: [136]  [ 20/160]  eta: 0:02:44  lr: 0.000008  min_lr: 0.000000  loss: 1.7885 (1.7802)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1798 (9.1045)  time: 0.8867 (0.5391 -- 2.7256)  data: 0.1763 (0.0008 -- 1.5563)  max mem: 16413
[2023-08-29 23:15:55,468] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:15:55,468] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:15:55,471] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:15:55,471] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:16:03,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21797
[2023-08-29 23:16:03,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21797
[2023-08-29 23:16:03,691] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:16:03,691] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:16:03,691] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [136]  [ 40/160]  eta: 0:01:59  lr: 0.000008  min_lr: 0.000000  loss: 1.7398 (1.7818)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2208 (8.2804)  time: 0.8092 (0.5358 -- 3.3132)  data: 0.1374 (0.0004 -- 1.1001)  max mem: 16413
Epoch: [136]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000000  loss: 1.9197 (1.8200)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8395 (8.2520)  time: 0.9744 (0.5241 -- 3.3851)  data: 0.3304 (0.0005 -- 2.8756)  max mem: 16413
Epoch: [136]  [ 80/160]  eta: 0:01:18  lr: 0.000008  min_lr: 0.000000  loss: 1.6036 (1.7805)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9917 (8.2649)  time: 0.9321 (0.5393 -- 4.4355)  data: 0.2970 (0.0001 -- 3.8978)  max mem: 16413
Epoch: [136]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.7139 (1.7533)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7902 (8.3825)  time: 0.7805 (0.5321 -- 2.6988)  data: 0.1893 (0.0003 -- 2.1336)  max mem: 16413
Epoch: [136]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9027 (1.7802)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9587 (8.6577)  time: 0.9341 (0.5072 -- 3.9128)  data: 0.3853 (0.0003 -- 3.4068)  max mem: 16413
Epoch: [136]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7041 (1.7677)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1674 (8.7739)  time: 0.6976 (0.5360 -- 2.1725)  data: 0.1440 (0.0004 -- 1.6137)  max mem: 16413
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7287 (1.7705)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0093 (8.8345)  time: 0.6859 (0.4952 -- 2.4346)  data: 0.1535 (0.0002 -- 1.8903)  max mem: 16413
Epoch: [136] Total time: 0:02:20 (0.8777 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7287 (1.7667)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0093 (8.8345)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2345 (0.2345)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4002 (2.4002 -- 2.4002)  data: 2.1030 (2.1030 -- 2.1030)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4166 (0.6998)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4186 (0.1993 -- 2.4002)  data: 0.1921 (0.0007 -- 2.1030)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4244 (0.6193)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2206 (0.1692 -- 0.4075)  data: 0.0115 (0.0001 -- 0.2172)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5146 (0.6795)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.2044 (0.1327 -- 0.4075)  data: 0.0112 (0.0001 -- 0.2172)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 83.610 Acc@5 97.095 loss 0.644
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.02%
Epoch: [137]  [  0/160]  eta: 0:18:28  lr: 0.000008  min_lr: 0.000000  loss: 2.1300 (2.1300)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1785 (9.1785)  time: 6.9284 (6.9284 -- 6.9284)  data: 6.3934 (6.3934 -- 6.3934)  max mem: 16413
[2023-08-29 23:18:03,544] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:18:03,544] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:18:03,545] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:18:03,546] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:18:08,997] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21931
[2023-08-29 23:18:08,997] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21931
[2023-08-29 23:18:08,997] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:18:08,997] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:18:08,997] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [137]  [ 20/160]  eta: 0:02:42  lr: 0.000008  min_lr: 0.000000  loss: 1.7101 (1.8285)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3219 (8.9011)  time: 0.8744 (0.5370 -- 3.6955)  data: 0.2990 (0.0005 -- 3.1578)  max mem: 16413
Epoch: [137]  [ 40/160]  eta: 0:02:04  lr: 0.000008  min_lr: 0.000000  loss: 1.6878 (1.8136)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3052 (8.5550)  time: 0.9128 (0.5275 -- 2.8472)  data: 0.3657 (0.0003 -- 2.3010)  max mem: 16413
Epoch: [137]  [ 60/160]  eta: 0:01:37  lr: 0.000008  min_lr: 0.000000  loss: 1.7206 (1.8030)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6373 (8.8211)  time: 0.8512 (0.5362 -- 3.3025)  data: 0.1858 (0.0002 -- 2.7968)  max mem: 16413
[2023-08-29 23:18:54,122] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21983
[2023-08-29 23:18:54,122] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21983
[2023-08-29 23:18:54,163] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:18:54,163] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:18:54,163] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-29 23:19:07,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=129, lr=[1.823376753128754e-07, 1.823376753128754e-07, 2.431169004171672e-07, 2.431169004171672e-07, 3.2415586722288963e-07, 3.2415586722288963e-07, 4.3220782296385284e-07, 4.3220782296385284e-07, 5.762770972851372e-07, 5.762770972851372e-07, 7.683694630468495e-07, 7.683694630468495e-07, 1.0244926173957993e-06, 1.0244926173957993e-06, 1.3659901565277324e-06, 1.3659901565277324e-06, 1.8213202087036433e-06, 1.8213202087036433e-06, 2.428426944938191e-06, 2.428426944938191e-06, 3.237902593250921e-06, 3.237902593250921e-06, 4.317203457667895e-06, 4.317203457667895e-06, 5.756271276890527e-06, 5.756271276890527e-06, 7.675028369187369e-06, 7.675028369187369e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 23:19:07,638] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=17.688028040656704, CurrSamplesPerSec=21.56147797314178, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [137]  [ 80/160]  eta: 0:01:17  lr: 0.000008  min_lr: 0.000000  loss: 1.7104 (1.7851)  loss_scale: 8192.0000 (15574.9136)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6432 (8.4477)  time: 0.9582 (0.5311 -- 3.9760)  data: 0.3230 (0.0005 -- 3.4485)  max mem: 16413
Epoch: [137]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.8598 (1.7969)  loss_scale: 8192.0000 (14112.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8575 (8.5910)  time: 0.7922 (0.5286 -- 3.2509)  data: 0.2247 (0.0006 -- 2.7144)  max mem: 16413
Epoch: [137]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.8848 (1.7971)  loss_scale: 8192.0000 (13134.2810)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4047 (8.6832)  time: 0.8655 (0.5321 -- 3.5711)  data: 0.2663 (0.0004 -- 3.0233)  max mem: 16413
[2023-08-29 23:19:56,257] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22052
[2023-08-29 23:19:56,257] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22052
[2023-08-29 23:19:56,257] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-29 23:19:56,257] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-29 23:19:56,257] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [137]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7577 (1.7863)  loss_scale: 8192.0000 (12171.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8252 (8.6207)  time: 0.9063 (0.5153 -- 3.3575)  data: 0.0022 (0.0003 -- 0.0158)  max mem: 16413
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.9512 (1.7993)  loss_scale: 4096.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2745 (8.6189)  time: 0.6373 (0.4978 -- 2.8600)  data: 0.0008 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [137] Total time: 0:02:22 (0.8899 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.9512 (1.7648)  loss_scale: 4096.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2745 (8.6189)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2548 (0.2548)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3074 (2.3074 -- 2.3074)  data: 2.0798 (2.0798 -- 2.0798)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4175 (0.7000)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4152 (0.2109 -- 2.3074)  data: 0.1909 (0.0006 -- 2.0798)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4523 (0.6230)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2198 (0.1687 -- 0.3509)  data: 0.0092 (0.0001 -- 0.1616)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5109 (0.6862)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (96.6805)  time: 0.2028 (0.1328 -- 0.3509)  data: 0.0085 (0.0001 -- 0.1616)  max mem: 16413
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.652
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [138]  [  0/160]  eta: 0:23:18  lr: 0.000008  min_lr: 0.000000  loss: 1.9519 (1.9519)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0715 (7.0715)  time: 8.7411 (8.7411 -- 8.7411)  data: 8.2279 (8.2279 -- 8.2279)  max mem: 16413
Epoch: [138]  [ 20/160]  eta: 0:02:47  lr: 0.000008  min_lr: 0.000000  loss: 1.7027 (1.7247)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5975 (8.5522)  time: 0.8175 (0.5223 -- 3.5894)  data: 0.2651 (0.0004 -- 3.0515)  max mem: 16413
Epoch: [138]  [ 40/160]  eta: 0:02:05  lr: 0.000008  min_lr: 0.000000  loss: 1.6921 (1.6770)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3624 (8.4224)  time: 0.8834 (0.5237 -- 2.8464)  data: 0.3395 (0.0003 -- 2.3084)  max mem: 16413
Epoch: [138]  [ 60/160]  eta: 0:01:41  lr: 0.000007  min_lr: 0.000000  loss: 1.4697 (1.6623)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7403 (8.8657)  time: 0.9656 (0.5239 -- 4.3686)  data: 0.4237 (0.0004 -- 3.8430)  max mem: 16413
Epoch: [138]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.6193 (1.6658)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6806 (8.8846)  time: 0.7920 (0.5287 -- 3.1354)  data: 0.2478 (0.0003 -- 2.6182)  max mem: 16413
Epoch: [138]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.7935 (1.7018)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6541 (9.0023)  time: 0.7597 (0.5277 -- 2.3694)  data: 0.2155 (0.0007 -- 1.8495)  max mem: 16413
[2023-08-29 23:21:56,653] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:21:56,653] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-29 23:21:56,657] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:21:56,658] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [138]  [120/160]  eta: 0:00:35  lr: 0.000007  min_lr: 0.000000  loss: 1.6771 (1.7087)  loss_scale: 8192.0000 (4773.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3009 (9.1927)  time: 0.7837 (0.5378 -- 1.8674)  data: 0.2140 (0.0007 -- 1.3348)  max mem: 16413
Epoch: [138]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.5708 (1.6929)  loss_scale: 8192.0000 (5257.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1345 (9.1254)  time: 0.9996 (0.5364 -- 2.9499)  data: 0.4344 (0.0006 -- 2.4138)  max mem: 16413
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7598 (1.6908)  loss_scale: 8192.0000 (5606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3100 (9.1089)  time: 0.6740 (0.4964 -- 2.0996)  data: 0.1321 (0.0002 -- 1.5828)  max mem: 16413
Epoch: [138] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7598 (1.7349)  loss_scale: 8192.0000 (5606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3100 (9.1089)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2349 (0.2349)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4385 (2.4385 -- 2.4385)  data: 2.1956 (2.1956 -- 2.1956)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3881 (0.7032)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4145 (0.1998 -- 2.4385)  data: 0.2006 (0.0005 -- 2.1956)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4384 (0.6216)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1713 -- 0.5114)  data: 0.0156 (0.0001 -- 0.2980)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5140 (0.6833)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (96.6805)  time: 0.2041 (0.1330 -- 0.5114)  data: 0.0153 (0.0001 -- 0.2980)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.650
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 84.02%
Epoch: [139]  [  0/160]  eta: 0:19:55  lr: 0.000007  min_lr: 0.000000  loss: 1.3726 (1.3726)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8016 (10.8016)  time: 7.4714 (7.4714 -- 7.4714)  data: 5.7181 (5.7181 -- 5.7181)  max mem: 16413
Epoch: [139]  [ 20/160]  eta: 0:02:41  lr: 0.000007  min_lr: 0.000000  loss: 1.7505 (1.6789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9612 (9.0941)  time: 0.8389 (0.5288 -- 3.5154)  data: 0.0168 (0.0003 -- 0.1926)  max mem: 16413
Epoch: [139]  [ 40/160]  eta: 0:02:09  lr: 0.000007  min_lr: 0.000000  loss: 1.9408 (1.7855)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6831 (9.1489)  time: 0.9967 (0.5249 -- 3.8924)  data: 0.1633 (0.0003 -- 1.9899)  max mem: 16413
Epoch: [139]  [ 60/160]  eta: 0:01:35  lr: 0.000007  min_lr: 0.000000  loss: 1.6003 (1.7651)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5542 (9.4432)  time: 0.7023 (0.5263 -- 2.6470)  data: 0.1367 (0.0004 -- 2.1065)  max mem: 16413
[2023-08-29 23:23:58,400] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:23:58,401] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:23:58,401] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:23:58,401] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [139]  [ 80/160]  eta: 0:01:14  lr: 0.000007  min_lr: 0.000000  loss: 1.6644 (1.7443)  loss_scale: 16384.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5885 (9.2740)  time: 0.8765 (0.5408 -- 2.1495)  data: 0.2716 (0.0003 -- 1.6131)  max mem: 16413
[2023-08-29 23:24:17,385] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22330
[2023-08-29 23:24:17,385] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22330
[2023-08-29 23:24:17,385] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:24:17,385] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:24:17,385] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [139]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.8485 (1.7709)  loss_scale: 8192.0000 (9895.2871)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9138 (9.0965)  time: 0.8915 (0.5073 -- 2.7022)  data: 0.1514 (0.0004 -- 1.5208)  max mem: 16413
Epoch: [139]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.7095 (1.7688)  loss_scale: 8192.0000 (9613.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2693 (9.2159)  time: 0.9019 (0.5273 -- 2.6416)  data: 0.0015 (0.0002 -- 0.0053)  max mem: 16413
Epoch: [139]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.7851 (1.7689)  loss_scale: 8192.0000 (9412.0851)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0464 (9.3279)  time: 0.8660 (0.5245 -- 3.0499)  data: 0.0014 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8664 (1.7777)  loss_scale: 8192.0000 (9267.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9419 (9.2914)  time: 0.7383 (0.4943 -- 2.6837)  data: 0.0010 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [139] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8664 (1.7919)  loss_scale: 8192.0000 (9267.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9419 (9.2914)
[2023-08-29 23:25:13,861] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-08-29 23:25:13,863] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-08-29 23:25:13,863] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-08-29 23:25:13,863] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-08-29 23:25:14,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-08-29 23:25:14,730] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2413 (0.2413)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5161 (2.5161 -- 2.5161)  data: 2.2533 (2.2533 -- 2.2533)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4119 (0.7015)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4415 (0.1930 -- 2.5161)  data: 0.2172 (0.0007 -- 2.2533)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4573 (0.6204)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2203 (0.1686 -- 0.3682)  data: 0.0125 (0.0001 -- 0.1164)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5205 (0.6800)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.2033 (0.1329 -- 0.3682)  data: 0.0117 (0.0001 -- 0.1164)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.649
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.02%
Epoch: [140]  [  0/160]  eta: 0:16:19  lr: 0.000007  min_lr: 0.000000  loss: 1.9242 (1.9242)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.4883 (15.4883)  time: 6.1203 (6.1203 -- 6.1203)  data: 5.5858 (5.5858 -- 5.5858)  max mem: 16413
Epoch: [140]  [ 20/160]  eta: 0:02:53  lr: 0.000007  min_lr: 0.000000  loss: 1.9817 (1.8638)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9470 (8.9349)  time: 0.9968 (0.5238 -- 4.3774)  data: 0.0745 (0.0002 -- 0.9740)  max mem: 16413
Epoch: [140]  [ 40/160]  eta: 0:02:06  lr: 0.000007  min_lr: 0.000000  loss: 1.7808 (1.8304)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6152 (9.0127)  time: 0.8662 (0.5254 -- 4.3989)  data: 0.0016 (0.0006 -- 0.0035)  max mem: 16413
[2023-08-29 23:26:24,613] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:26:24,613] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:26:24,614] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:26:24,614] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [140]  [ 60/160]  eta: 0:01:42  lr: 0.000007  min_lr: 0.000000  loss: 1.9322 (1.8587)  loss_scale: 8192.0000 (8460.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4921 (8.9161)  time: 0.9548 (0.5262 -- 3.9870)  data: 0.0013 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [140]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.6276 (1.8126)  loss_scale: 16384.0000 (10416.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9181 (9.0130)  time: 0.7565 (0.5253 -- 2.6306)  data: 0.0013 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [140]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.6677 (1.7948)  loss_scale: 16384.0000 (11598.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8320 (9.2406)  time: 0.8863 (0.5260 -- 4.0889)  data: 0.0017 (0.0003 -- 0.0087)  max mem: 16413
Epoch: [140]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 2.0358 (1.8316)  loss_scale: 16384.0000 (12389.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6276 (9.2279)  time: 0.8259 (0.5291 -- 3.2799)  data: 0.0019 (0.0002 -- 0.0123)  max mem: 16413
Epoch: [140]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.7733 (1.8250)  loss_scale: 16384.0000 (12956.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6398 (9.2628)  time: 0.8074 (0.5235 -- 3.1805)  data: 0.0625 (0.0004 -- 0.7499)  max mem: 16413
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7550 (1.8248)  loss_scale: 16384.0000 (13363.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9075 (9.2159)  time: 0.7200 (0.4987 -- 3.4046)  data: 0.1439 (0.0002 -- 2.8255)  max mem: 16413
Epoch: [140] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7550 (1.8037)  loss_scale: 16384.0000 (13363.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9075 (9.2159)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2450 (0.2450)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2713 (2.2713 -- 2.2713)  data: 2.0338 (2.0338 -- 2.0338)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4202 (0.6934)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4089 (0.2101 -- 2.2713)  data: 0.1864 (0.0003 -- 2.0338)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4696 (0.6140)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2233 (0.1713 -- 0.3906)  data: 0.0127 (0.0001 -- 0.1635)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5138 (0.6765)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.2656)  time: 0.2064 (0.1326 -- 0.3906)  data: 0.0122 (0.0001 -- 0.1635)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 83.402 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.02%
Epoch: [141]  [  0/160]  eta: 0:26:32  lr: 0.000007  min_lr: 0.000000  loss: 1.3712 (1.3712)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6335 (9.6335)  time: 9.9529 (9.9529 -- 9.9529)  data: 6.0477 (6.0477 -- 6.0477)  max mem: 16413
Epoch: [141]  [ 20/160]  eta: 0:02:54  lr: 0.000007  min_lr: 0.000000  loss: 1.8343 (1.7959)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6625 (8.6751)  time: 0.8077 (0.5205 -- 3.9918)  data: 0.0016 (0.0002 -- 0.0062)  max mem: 16413
[2023-08-29 23:28:24,909] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:28:24,910] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:28:24,909] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:28:24,910] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:28:30,980] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22596
[2023-08-29 23:28:30,980] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22596
[2023-08-29 23:28:30,980] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:28:30,980] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:28:30,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [141]  [ 40/160]  eta: 0:02:03  lr: 0.000007  min_lr: 0.000000  loss: 1.7042 (1.7643)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6086 (8.8822)  time: 0.8054 (0.5231 -- 3.1842)  data: 0.0104 (0.0005 -- 0.1592)  max mem: 16413
Epoch: [141]  [ 60/160]  eta: 0:01:40  lr: 0.000007  min_lr: 0.000000  loss: 1.7111 (1.7547)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8133 (9.0928)  time: 0.9470 (0.5271 -- 2.8681)  data: 0.2087 (0.0006 -- 1.8600)  max mem: 16413
Epoch: [141]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000000  loss: 1.7419 (1.7698)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5898 (9.1754)  time: 0.7397 (0.5283 -- 2.5390)  data: 0.1648 (0.0004 -- 1.9958)  max mem: 16413
Epoch: [141]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.8264 (1.7816)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6171 (9.0808)  time: 0.9884 (0.5307 -- 4.0292)  data: 0.4279 (0.0005 -- 3.5075)  max mem: 16413
Epoch: [141]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.6910 (1.7629)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4600 (9.0620)  time: 0.8380 (0.5149 -- 3.6477)  data: 0.0473 (0.0003 -- 0.9192)  max mem: 16413
Epoch: [141]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.6734 (1.7318)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8105 (9.1480)  time: 0.8956 (0.5187 -- 2.8737)  data: 0.0017 (0.0005 -- 0.0035)  max mem: 16413
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7084 (1.7255)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4483 (9.1197)  time: 0.7152 (0.4958 -- 2.8072)  data: 0.0007 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [141] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7084 (1.7480)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4483 (9.1197)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2387 (0.2387)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2809 (2.2809 -- 2.2809)  data: 2.0619 (2.0619 -- 2.0619)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4419 (0.7089)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4072 (0.1961 -- 2.2809)  data: 0.1913 (0.0007 -- 2.0619)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4503 (0.6225)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2278 (0.1684 -- 0.5927)  data: 0.0213 (0.0001 -- 0.3800)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5090 (0.6874)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2123 (0.1333 -- 0.5927)  data: 0.0209 (0.0001 -- 0.3800)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 83.402 Acc@5 97.095 loss 0.655
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.02%
Epoch: [142]  [  0/160]  eta: 0:17:43  lr: 0.000007  min_lr: 0.000000  loss: 2.1429 (2.1429)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8661 (13.8661)  time: 6.6455 (6.6455 -- 6.6455)  data: 5.7852 (5.7852 -- 5.7852)  max mem: 16413
[2023-08-29 23:30:32,812] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:30:32,812] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:30:32,814] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:30:32,814] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [142]  [ 20/160]  eta: 0:02:44  lr: 0.000007  min_lr: 0.000000  loss: 1.7975 (1.8316)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3919 (8.4244)  time: 0.9015 (0.5213 -- 2.6939)  data: 0.0348 (0.0002 -- 0.3346)  max mem: 16413
[2023-08-29 23:30:49,053] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22744
[2023-08-29 23:30:49,053] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22744
[2023-08-29 23:30:49,053] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:30:49,053] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:30:49,054] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 23:30:51,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22746
[2023-08-29 23:30:51,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22746
[2023-08-29 23:30:51,665] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:30:51,665] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:30:51,665] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [142]  [ 40/160]  eta: 0:02:01  lr: 0.000007  min_lr: 0.000000  loss: 1.9572 (1.8623)  loss_scale: 8192.0000 (20979.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6684 (8.5225)  time: 0.8458 (0.5206 -- 2.6500)  data: 0.1095 (0.0004 -- 2.1064)  max mem: 16413
Epoch: [142]  [ 60/160]  eta: 0:01:38  lr: 0.000007  min_lr: 0.000000  loss: 1.7523 (1.8322)  loss_scale: 8192.0000 (16786.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1290 (8.6640)  time: 0.9332 (0.5381 -- 2.3171)  data: 0.2547 (0.0003 -- 1.7844)  max mem: 16413
Epoch: [142]  [ 80/160]  eta: 0:01:17  lr: 0.000007  min_lr: 0.000000  loss: 1.9755 (1.8473)  loss_scale: 8192.0000 (14664.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9018 (8.8416)  time: 0.8904 (0.5036 -- 2.8128)  data: 0.1158 (0.0005 -- 2.2938)  max mem: 16413
Epoch: [142]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.8771 (1.8409)  loss_scale: 8192.0000 (13382.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1145 (8.8774)  time: 0.7860 (0.5323 -- 1.9153)  data: 0.1789 (0.0001 -- 1.3715)  max mem: 16413
Epoch: [142]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.5836 (1.8038)  loss_scale: 8192.0000 (12524.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8151 (8.9384)  time: 0.8109 (0.5403 -- 2.1843)  data: 0.0126 (0.0003 -- 0.2180)  max mem: 16413
Epoch: [142]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8026 (1.8048)  loss_scale: 8192.0000 (11910.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3117 (9.0378)  time: 0.9901 (0.5376 -- 4.3115)  data: 0.0020 (0.0003 -- 0.0063)  max mem: 16413
[2023-08-29 23:32:41,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:32:41,793] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:32:41,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:32:41,793] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6053 (1.7845)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2056 (9.1506)  time: 0.6142 (0.4958 -- 2.4581)  data: 0.0004 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [142] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6053 (1.7784)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2056 (9.1506)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2519 (0.2519)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3514 (2.3514 -- 2.3514)  data: 2.1203 (2.1203 -- 2.1203)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3987 (0.7042)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4150 (0.1990 -- 2.3514)  data: 0.1964 (0.0006 -- 2.1203)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4627 (0.6225)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2256 (0.1710 -- 0.5762)  data: 0.0216 (0.0001 -- 0.3873)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5101 (0.6825)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (97.0954)  time: 0.2086 (0.1330 -- 0.5762)  data: 0.0206 (0.0001 -- 0.3873)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 82.988 Acc@5 97.303 loss 0.654
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [143]  [  0/160]  eta: 0:22:28  lr: 0.000006  min_lr: 0.000000  loss: 1.5261 (1.5261)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0636 (9.0636)  time: 8.4292 (8.4292 -- 8.4292)  data: 6.8262 (6.8262 -- 6.8262)  max mem: 16413
Epoch: [143]  [ 20/160]  eta: 0:02:45  lr: 0.000006  min_lr: 0.000000  loss: 1.6996 (1.7307)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1438 (8.5107)  time: 0.8230 (0.5144 -- 3.5744)  data: 0.0797 (0.0006 -- 1.5648)  max mem: 16413
Epoch: [143]  [ 40/160]  eta: 0:02:04  lr: 0.000006  min_lr: 0.000000  loss: 1.9120 (1.7813)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9501 (8.6026)  time: 0.8898 (0.5308 -- 3.0649)  data: 0.0018 (0.0002 -- 0.0079)  max mem: 16413
Epoch: [143]  [ 60/160]  eta: 0:01:38  lr: 0.000006  min_lr: 0.000000  loss: 1.5915 (1.7144)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5703 (8.5819)  time: 0.8580 (0.5192 -- 3.6141)  data: 0.0016 (0.0006 -- 0.0037)  max mem: 16413
Epoch: [143]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 1.7826 (1.7450)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5078 (8.5789)  time: 0.8724 (0.5168 -- 3.1520)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [143]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7661 (1.7573)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6113 (8.6325)  time: 0.7941 (0.5272 -- 2.9146)  data: 0.0019 (0.0006 -- 0.0036)  max mem: 16413
[2023-08-29 23:34:41,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=134, lr=[1.5032162798168748e-07, 1.5032162798168748e-07, 2.0042883730891664e-07, 2.0042883730891664e-07, 2.6723844974522217e-07, 2.6723844974522217e-07, 3.5631793299362954e-07, 3.5631793299362954e-07, 4.7509057732483944e-07, 4.7509057732483944e-07, 6.334541030997859e-07, 6.334541030997859e-07, 8.446054707997145e-07, 8.446054707997145e-07, 1.1261406277329526e-06, 1.1261406277329526e-06, 1.5015208369772703e-06, 1.5015208369772703e-06, 2.0020277826363604e-06, 2.0020277826363604e-06, 2.6693703768484804e-06, 2.6693703768484804e-06, 3.5591605024646406e-06, 3.5591605024646406e-06, 4.745547336619521e-06, 4.745547336619521e-06, 6.327396448826028e-06, 6.327396448826028e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 23:34:41,876] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=17.639237874546062, CurrSamplesPerSec=3.646403335368143, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [143]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.7897 (1.7600)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3146 (8.6397)  time: 0.8769 (0.5274 -- 3.3033)  data: 0.0438 (0.0002 -- 0.8462)  max mem: 16413
[2023-08-29 23:34:44,099] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:34:44,099] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:34:44,099] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:34:44,099] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:34:48,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23007
[2023-08-29 23:34:48,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23007
[2023-08-29 23:34:48,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:34:48,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:34:48,147] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [143]  [140/160]  eta: 0:00:17  lr: 0.000006  min_lr: 0.000000  loss: 1.9015 (1.7782)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3046 (8.6340)  time: 0.7258 (0.5301 -- 2.3603)  data: 0.0021 (0.0003 -- 0.0070)  max mem: 16413
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7651 (1.7780)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1372 (8.6313)  time: 0.7713 (0.4963 -- 2.1532)  data: 0.0831 (0.0002 -- 1.6431)  max mem: 16413
Epoch: [143] Total time: 0:02:20 (0.8758 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7651 (1.7739)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1372 (8.6313)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2579 (0.2579)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4133 (2.4133 -- 2.4133)  data: 2.1729 (2.1729 -- 2.1729)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4082 (0.6972)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4292 (0.1974 -- 2.4133)  data: 0.2085 (0.0003 -- 2.1729)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4729 (0.6154)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2203 (0.1717 -- 0.3539)  data: 0.0144 (0.0001 -- 0.1642)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5113 (0.6729)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.2035 (0.1331 -- 0.3539)  data: 0.0140 (0.0001 -- 0.1642)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 84.440 Acc@5 97.095 loss 0.646
Accuracy of the network on the 482 val images: 84.44%
[2023-08-29 23:35:19,617] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-29 23:35:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-29 23:35:19,618] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-29 23:35:19,619] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-29 23:35:21,045] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-29 23:35:21,045] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.44%
Epoch: [144]  [  0/160]  eta: 0:20:10  lr: 0.000006  min_lr: 0.000000  loss: 0.8870 (0.8870)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2441 (9.2441)  time: 7.5640 (7.5640 -- 7.5640)  data: 7.0298 (7.0298 -- 7.0298)  max mem: 16413
Epoch: [144]  [ 20/160]  eta: 0:02:38  lr: 0.000006  min_lr: 0.000000  loss: 1.6651 (1.7464)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7129 (8.8351)  time: 0.8086 (0.5325 -- 2.5092)  data: 0.2333 (0.0010 -- 1.9994)  max mem: 16413
Epoch: [144]  [ 40/160]  eta: 0:01:58  lr: 0.000006  min_lr: 0.000000  loss: 1.8068 (1.7347)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4727 (8.9316)  time: 0.8331 (0.5284 -- 3.7264)  data: 0.2439 (0.0003 -- 3.1637)  max mem: 16413
Epoch: [144]  [ 60/160]  eta: 0:01:36  lr: 0.000006  min_lr: 0.000000  loss: 1.9171 (1.7884)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1155 (9.0440)  time: 0.9357 (0.5304 -- 1.8460)  data: 0.1708 (0.0006 -- 1.3182)  max mem: 16413
Epoch: [144]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000000  loss: 1.7953 (1.7478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9179 (9.2596)  time: 0.8400 (0.5200 -- 3.1428)  data: 0.1734 (0.0001 -- 2.5864)  max mem: 16413
[2023-08-29 23:36:51,492] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:36:51,492] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:36:51,492] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:36:51,493] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [144]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 1.8263 (1.7689)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6019 (9.1191)  time: 0.9796 (0.5231 -- 3.8578)  data: 0.4380 (0.0004 -- 3.3046)  max mem: 16413
[2023-08-29 23:37:04,299] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23149
[2023-08-29 23:37:04,299] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23149
[2023-08-29 23:37:04,299] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:37:04,299] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:37:04,299] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [144]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.7969 (1.7746)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2762 (9.0590)  time: 0.8505 (0.5210 -- 3.8240)  data: 0.3038 (0.0004 -- 3.2916)  max mem: 16413
Epoch: [144]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.6221 (1.7541)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1217 (9.0978)  time: 0.9887 (0.5069 -- 3.9396)  data: 0.4463 (0.0002 -- 3.4314)  max mem: 16413
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6686 (1.7403)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5573 (9.0644)  time: 0.5215 (0.4990 -- 0.5865)  data: 0.0006 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [144] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6686 (1.7387)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5573 (9.0644)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2479 (0.2479)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3883 (2.3883 -- 2.3883)  data: 2.1560 (2.1560 -- 2.1560)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4003 (0.6990)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4228 (0.2050 -- 2.3883)  data: 0.2030 (0.0008 -- 2.1560)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4792 (0.6217)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2207 (0.1688 -- 0.3843)  data: 0.0134 (0.0001 -- 0.1878)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5073 (0.6786)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.2656)  time: 0.2028 (0.1325 -- 0.3843)  data: 0.0131 (0.0001 -- 0.1878)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 84.232 Acc@5 97.303 loss 0.650
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.44%
Epoch: [145]  [  0/160]  eta: 0:17:58  lr: 0.000006  min_lr: 0.000000  loss: 1.4880 (1.4880)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3953 (7.3953)  time: 6.7379 (6.7379 -- 6.7379)  data: 5.6312 (5.6312 -- 5.6312)  max mem: 16413
Epoch: [145]  [ 20/160]  eta: 0:03:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7351 (1.7760)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8569 (9.3868)  time: 1.0148 (0.5307 -- 3.7315)  data: 0.1072 (0.0008 -- 1.3629)  max mem: 16413
Epoch: [145]  [ 40/160]  eta: 0:02:10  lr: 0.000006  min_lr: 0.000000  loss: 1.8456 (1.7765)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6464 (9.1802)  time: 0.8789 (0.5295 -- 3.5761)  data: 0.0022 (0.0002 -- 0.0142)  max mem: 16413
Epoch: [145]  [ 60/160]  eta: 0:01:40  lr: 0.000006  min_lr: 0.000000  loss: 1.6845 (1.7538)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8887 (8.8362)  time: 0.8462 (0.5149 -- 2.3939)  data: 0.0020 (0.0002 -- 0.0164)  max mem: 16413
[2023-08-29 23:39:06,045] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:39:06,045] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:39:06,045] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:39:06,045] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [145]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 1.9079 (1.7868)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6595 (8.9524)  time: 0.8047 (0.5387 -- 2.0916)  data: 0.0017 (0.0004 -- 0.0036)  max mem: 16413
[2023-08-29 23:39:12,187] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23285
[2023-08-29 23:39:12,187] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23285
[2023-08-29 23:39:12,187] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:39:12,187] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:39:12,187] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [145]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.6420 (1.7523)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3587 (8.8701)  time: 0.8066 (0.5036 -- 2.6895)  data: 0.0748 (0.0002 -- 1.4640)  max mem: 16413
Epoch: [145]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.8803 (1.7698)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2222 (9.0193)  time: 0.9243 (0.5371 -- 2.7073)  data: 0.2346 (0.0007 -- 2.1724)  max mem: 16413
Epoch: [145]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7133 (1.7697)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4735 (8.9711)  time: 0.9283 (0.5298 -- 4.7786)  data: 0.3384 (0.0004 -- 4.2759)  max mem: 16413
[2023-08-29 23:40:05,812] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23343
[2023-08-29 23:40:05,812] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23343
[2023-08-29 23:40:05,812] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:40:05,812] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:40:05,812] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6714 (1.7608)  loss_scale: 8192.0000 (16230.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7837 (8.9699)  time: 0.6874 (0.4946 -- 2.8385)  data: 0.1718 (0.0002 -- 2.3298)  max mem: 16413
Epoch: [145] Total time: 0:02:24 (0.9002 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6714 (1.7688)  loss_scale: 8192.0000 (16230.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7837 (8.9699)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2687 (0.2687)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4266 (2.4266 -- 2.4266)  data: 2.1750 (2.1750 -- 2.1750)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4081 (0.6857)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4192 (0.2060 -- 2.4266)  data: 0.1992 (0.0006 -- 2.1750)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4896 (0.6114)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2198 (0.1693 -- 0.3261)  data: 0.0110 (0.0001 -- 0.1492)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5037 (0.6687)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (97.0954)  time: 0.2039 (0.1325 -- 0.3261)  data: 0.0104 (0.0001 -- 0.1492)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 83.402 Acc@5 97.510 loss 0.644
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.44%
Epoch: [146]  [  0/160]  eta: 0:19:47  lr: 0.000006  min_lr: 0.000000  loss: 1.8284 (1.8284)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5499 (6.5499)  time: 7.4249 (7.4249 -- 7.4249)  data: 6.8780 (6.8780 -- 6.8780)  max mem: 16413
Epoch: [146]  [ 20/160]  eta: 0:03:01  lr: 0.000006  min_lr: 0.000000  loss: 1.6470 (1.6759)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7788 (8.3279)  time: 0.9868 (0.5031 -- 5.2234)  data: 0.4317 (0.0003 -- 4.6959)  max mem: 16413
Epoch: [146]  [ 40/160]  eta: 0:02:11  lr: 0.000006  min_lr: 0.000000  loss: 1.6794 (1.6665)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4943 (8.5840)  time: 0.8915 (0.5047 -- 4.3605)  data: 0.3531 (0.0001 -- 3.8482)  max mem: 16413
Epoch: [146]  [ 60/160]  eta: 0:01:40  lr: 0.000006  min_lr: 0.000000  loss: 1.6419 (1.6635)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5152 (8.5380)  time: 0.8133 (0.5387 -- 3.7683)  data: 0.2586 (0.0002 -- 3.2102)  max mem: 16413
Epoch: [146]  [ 80/160]  eta: 0:01:19  lr: 0.000006  min_lr: 0.000000  loss: 1.7198 (1.6836)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0419 (8.6346)  time: 0.9403 (0.5380 -- 3.2120)  data: 0.3839 (0.0005 -- 2.6759)  max mem: 16413
Epoch: [146]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 1.9233 (1.7348)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1928 (8.6824)  time: 0.7804 (0.5258 -- 2.3201)  data: 0.2327 (0.0001 -- 1.7708)  max mem: 16413
[2023-08-29 23:42:08,556] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:42:08,556] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:42:08,559] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:42:08,559] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [146]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.6752 (1.7451)  loss_scale: 8192.0000 (8801.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5434 (8.6090)  time: 0.8224 (0.5272 -- 2.1822)  data: 0.2691 (0.0004 -- 1.6261)  max mem: 16413
Epoch: [146]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.6699 (1.7267)  loss_scale: 16384.0000 (9876.8794)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8955 (8.7604)  time: 0.8595 (0.5230 -- 3.7071)  data: 0.3139 (0.0005 -- 3.1805)  max mem: 16413
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7021 (1.7399)  loss_scale: 16384.0000 (10649.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7912 (8.8259)  time: 0.7090 (0.4970 -- 4.1539)  data: 0.1881 (0.0002 -- 3.6443)  max mem: 16413
Epoch: [146] Total time: 0:02:22 (0.8935 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7021 (1.7383)  loss_scale: 16384.0000 (10649.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7912 (8.8259)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2556 (0.2556)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4318 (2.4318 -- 2.4318)  data: 2.2065 (2.2065 -- 2.2065)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4349 (0.6930)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4254 (0.1915 -- 2.4318)  data: 0.2100 (0.0006 -- 2.2065)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4707 (0.6104)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2226 (0.1715 -- 0.4121)  data: 0.0165 (0.0001 -- 0.2236)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5028 (0.6715)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2079 (0.1325 -- 0.4121)  data: 0.0163 (0.0001 -- 0.2236)  max mem: 16413
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 84.025 Acc@5 97.303 loss 0.643
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.44%
Epoch: [147]  [  0/160]  eta: 0:20:51  lr: 0.000006  min_lr: 0.000000  loss: 1.4788 (1.4788)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1229 (9.1229)  time: 7.8213 (7.8213 -- 7.8213)  data: 7.2850 (7.2850 -- 7.2850)  max mem: 16413
Epoch: [147]  [ 20/160]  eta: 0:02:53  lr: 0.000006  min_lr: 0.000000  loss: 1.6759 (1.6880)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0983 (8.1909)  time: 0.9103 (0.5317 -- 5.4740)  data: 0.3544 (0.0005 -- 4.9567)  max mem: 16413
Epoch: [147]  [ 40/160]  eta: 0:02:11  lr: 0.000006  min_lr: 0.000000  loss: 1.8052 (1.7623)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8214 (8.8372)  time: 0.9467 (0.5352 -- 3.4215)  data: 0.3965 (0.0003 -- 2.8525)  max mem: 16413
Epoch: [147]  [ 60/160]  eta: 0:01:38  lr: 0.000006  min_lr: 0.000000  loss: 1.7634 (1.7332)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4720 (8.8636)  time: 0.7507 (0.5210 -- 2.9510)  data: 0.2064 (0.0003 -- 2.4424)  max mem: 16413
[2023-08-29 23:44:14,780] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:44:14,781] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:44:14,785] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:44:14,785] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [147]  [ 80/160]  eta: 0:01:20  lr: 0.000006  min_lr: 0.000000  loss: 1.7869 (1.7601)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2604 (8.8411)  time: 1.0513 (0.5025 -- 4.9013)  data: 0.5075 (0.0003 -- 4.3865)  max mem: 16413
[2023-08-29 23:44:18,061] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23606
[2023-08-29 23:44:18,061] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23606
[2023-08-29 23:44:18,061] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:44:18,061] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:44:18,061] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [147]  [100/160]  eta: 0:00:57  lr: 0.000006  min_lr: 0.000000  loss: 1.9717 (1.7923)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8142 (8.8500)  time: 0.7803 (0.5135 -- 3.1194)  data: 0.2404 (0.0002 -- 2.6057)  max mem: 16413
[2023-08-29 23:44:44,272] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23636
[2023-08-29 23:44:44,273] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23636
[2023-08-29 23:44:44,273] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:44:44,273] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:44:44,273] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [147]  [120/160]  eta: 0:00:38  lr: 0.000006  min_lr: 0.000000  loss: 1.8462 (1.7873)  loss_scale: 16384.0000 (16857.9174)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7093 (8.8541)  time: 0.9338 (0.5196 -- 3.5629)  data: 0.3945 (0.0002 -- 3.0307)  max mem: 16413
Epoch: [147]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6493 (1.7742)  loss_scale: 8192.0000 (15628.7092)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1966 (8.8615)  time: 0.8124 (0.5130 -- 3.9144)  data: 0.2714 (0.0003 -- 3.4100)  max mem: 16413
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8612 (1.7807)  loss_scale: 8192.0000 (14745.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9515 (9.0335)  time: 0.5878 (0.4971 -- 1.1967)  data: 0.0695 (0.0002 -- 0.6599)  max mem: 16413
Epoch: [147] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8612 (1.7597)  loss_scale: 8192.0000 (14745.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9515 (9.0335)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2646 (0.2646)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1572 (2.1572 -- 2.1572)  data: 1.9053 (1.9053 -- 1.9053)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4224 (0.6849)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4004 (0.2051 -- 2.1572)  data: 0.1770 (0.0008 -- 1.9053)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4720 (0.6097)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2246 (0.1737 -- 0.4595)  data: 0.0144 (0.0001 -- 0.2423)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5030 (0.6717)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (96.2656)  time: 0.2075 (0.1330 -- 0.4595)  data: 0.0140 (0.0001 -- 0.2423)  max mem: 16413
Val: Total time: 0:00:07 (0.2829 s / it)
* Acc@1 84.025 Acc@5 97.095 loss 0.645
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.44%
Epoch: [148]  [  0/160]  eta: 0:21:01  lr: 0.000005  min_lr: 0.000000  loss: 1.5598 (1.5598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7725 (8.7725)  time: 7.8873 (7.8873 -- 7.8873)  data: 7.0685 (7.0685 -- 7.0685)  max mem: 16413
Epoch: [148]  [ 20/160]  eta: 0:02:41  lr: 0.000005  min_lr: 0.000000  loss: 1.8240 (1.7592)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0960 (9.1978)  time: 0.8173 (0.5242 -- 3.9298)  data: 0.2730 (0.0004 -- 3.4151)  max mem: 16413
Epoch: [148]  [ 40/160]  eta: 0:02:05  lr: 0.000005  min_lr: 0.000000  loss: 1.6501 (1.7310)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0697 (8.6289)  time: 0.9326 (0.5269 -- 3.7562)  data: 0.3847 (0.0004 -- 3.2158)  max mem: 16413
Epoch: [148]  [ 60/160]  eta: 0:01:38  lr: 0.000005  min_lr: 0.000000  loss: 1.7209 (1.7269)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2285 (8.5526)  time: 0.8453 (0.5169 -- 2.3256)  data: 0.3005 (0.0003 -- 1.8082)  max mem: 16413
Epoch: [148]  [ 80/160]  eta: 0:01:17  lr: 0.000005  min_lr: 0.000000  loss: 1.6579 (1.7246)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7438 (8.4833)  time: 0.9552 (0.5270 -- 2.8273)  data: 0.3089 (0.0002 -- 2.3031)  max mem: 16413
[2023-08-29 23:46:46,106] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:46:46,107] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:46:46,107] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:46:46,107] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [148]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.7587 (1.7353)  loss_scale: 16384.0000 (9489.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2654 (8.5411)  time: 0.6945 (0.5352 -- 2.6793)  data: 0.1332 (0.0005 -- 2.1553)  max mem: 16413
Epoch: [148]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.6574 (1.7274)  loss_scale: 16384.0000 (10629.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0898 (8.7737)  time: 0.9707 (0.5114 -- 4.1449)  data: 0.0553 (0.0004 -- 0.6258)  max mem: 16413
Epoch: [148]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.7461 (1.7368)  loss_scale: 16384.0000 (11445.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6728 (8.7392)  time: 0.7709 (0.5294 -- 3.2908)  data: 0.1120 (0.0002 -- 1.6439)  max mem: 16413
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7784 (1.7461)  loss_scale: 16384.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4072 (8.7469)  time: 0.7316 (0.4967 -- 2.1717)  data: 0.1829 (0.0002 -- 1.2918)  max mem: 16413
Epoch: [148] Total time: 0:02:21 (0.8861 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7784 (1.7636)  loss_scale: 16384.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4072 (8.7469)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2558 (0.2558)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3413 (2.3413 -- 2.3413)  data: 2.1249 (2.1249 -- 2.1249)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4257 (0.6867)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4214 (0.2058 -- 2.3413)  data: 0.2028 (0.0010 -- 2.1249)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4638 (0.6151)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2186 (0.1704 -- 0.3240)  data: 0.0093 (0.0001 -- 0.0946)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5031 (0.6761)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2024 (0.1331 -- 0.3240)  data: 0.0090 (0.0001 -- 0.0946)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.648
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.44%
Epoch: [149]  [  0/160]  eta: 0:17:04  lr: 0.000005  min_lr: 0.000000  loss: 1.4679 (1.4679)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2021 (10.2021)  time: 6.4034 (6.4034 -- 6.4034)  data: 5.8512 (5.8512 -- 5.8512)  max mem: 16413
Epoch: [149]  [ 20/160]  eta: 0:02:49  lr: 0.000005  min_lr: 0.000000  loss: 1.6443 (1.6961)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7098 (8.9155)  time: 0.9498 (0.5334 -- 3.5957)  data: 0.0379 (0.0004 -- 0.4280)  max mem: 16413
Epoch: [149]  [ 40/160]  eta: 0:02:04  lr: 0.000005  min_lr: 0.000000  loss: 1.7934 (1.7356)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0863 (8.9561)  time: 0.8647 (0.5280 -- 2.5808)  data: 0.1188 (0.0002 -- 2.0113)  max mem: 16413
[2023-08-29 23:48:49,131] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:48:49,132] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:48:49,134] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:48:49,135] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:48:54,246] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23899
[2023-08-29 23:48:54,246] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23899
[2023-08-29 23:48:54,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:48:54,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:48:54,246] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [149]  [ 60/160]  eta: 0:01:39  lr: 0.000005  min_lr: 0.000000  loss: 1.8509 (1.7675)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8703 (8.8150)  time: 0.9060 (0.5299 -- 3.0101)  data: 0.0660 (0.0005 -- 1.2965)  max mem: 16413
Epoch: [149]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 1.7273 (1.7603)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3948 (8.9576)  time: 0.8365 (0.5245 -- 3.6146)  data: 0.0015 (0.0006 -- 0.0036)  max mem: 16413
Epoch: [149]  [100/160]  eta: 0:00:57  lr: 0.000005  min_lr: 0.000000  loss: 1.5925 (1.7547)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4505 (8.9907)  time: 0.9860 (0.5323 -- 4.1057)  data: 0.0014 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [149]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.8487 (1.7613)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1441 (8.9265)  time: 0.7032 (0.5264 -- 2.6979)  data: 0.0015 (0.0005 -- 0.0053)  max mem: 16413
Epoch: [149]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6269 (1.7494)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6000 (9.1532)  time: 0.9014 (0.5260 -- 4.4805)  data: 0.0012 (0.0003 -- 0.0028)  max mem: 16413
[2023-08-29 23:50:17,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=141, lr=[1.207368881416918e-07, 1.207368881416918e-07, 1.6098251752225574e-07, 1.6098251752225574e-07, 2.14643356696341e-07, 2.14643356696341e-07, 2.86191142261788e-07, 2.86191142261788e-07, 3.81588189682384e-07, 3.81588189682384e-07, 5.087842529098454e-07, 5.087842529098454e-07, 6.783790038797937e-07, 6.783790038797937e-07, 9.045053385063917e-07, 9.045053385063917e-07, 1.2060071180085222e-06, 1.2060071180085222e-06, 1.6080094906780296e-06, 1.6080094906780296e-06, 2.144012654237373e-06, 2.144012654237373e-06, 2.858683538983164e-06, 2.858683538983164e-06, 3.8115780519775516e-06, 3.8115780519775516e-06, 5.082104069303402e-06, 5.082104069303402e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-29 23:50:17,287] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=17.686369380550726, CurrSamplesPerSec=24.677735410189566, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8355 (1.7616)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6833 (9.1136)  time: 0.7237 (0.4965 -- 4.1627)  data: 0.0007 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [149] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8355 (1.7840)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6833 (9.1136)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.2588 (0.2588)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1413 (2.1413 -- 2.1413)  data: 1.9227 (1.9227 -- 1.9227)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4183 (0.6812)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4185 (0.1964 -- 2.1413)  data: 0.2101 (0.0006 -- 1.9227)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4749 (0.6138)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2354 (0.1719 -- 0.6029)  data: 0.0314 (0.0001 -- 0.3790)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4991 (0.6724)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (97.0954)  time: 0.2226 (0.1336 -- 0.6029)  data: 0.0311 (0.0001 -- 0.3790)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.645
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.44%
Epoch: [150]  [  0/160]  eta: 0:22:27  lr: 0.000005  min_lr: 0.000000  loss: 2.1675 (2.1675)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9494 (4.9494)  time: 8.4216 (8.4216 -- 8.4216)  data: 7.8757 (7.8757 -- 7.8757)  max mem: 16413
Epoch: [150]  [ 20/160]  eta: 0:02:43  lr: 0.000005  min_lr: 0.000000  loss: 1.7752 (1.8587)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8266 (8.0354)  time: 0.8087 (0.5271 -- 2.6934)  data: 0.2669 (0.0006 -- 2.1616)  max mem: 16413
[2023-08-29 23:50:57,390] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:50:57,390] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:50:57,390] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:50:57,390] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [150]  [ 40/160]  eta: 0:02:07  lr: 0.000005  min_lr: 0.000000  loss: 1.7104 (1.7645)  loss_scale: 32768.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9040 (8.7593)  time: 0.9531 (0.5397 -- 3.4510)  data: 0.3047 (0.0005 -- 2.9106)  max mem: 16413
[2023-08-29 23:51:16,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24049
[2023-08-29 23:51:16,525] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:51:16,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-29 23:51:16,528] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24049
[2023-08-29 23:51:16,529] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [150]  [ 60/160]  eta: 0:01:40  lr: 0.000005  min_lr: 0.000000  loss: 1.8391 (1.7638)  loss_scale: 16384.0000 (22024.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8994 (8.6360)  time: 0.8680 (0.5107 -- 4.2896)  data: 0.3264 (0.0003 -- 3.7514)  max mem: 16413
Epoch: [150]  [ 80/160]  eta: 0:01:18  lr: 0.000005  min_lr: 0.000000  loss: 1.5872 (1.7496)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1898 (8.7080)  time: 0.9002 (0.5251 -- 4.7097)  data: 0.3562 (0.0004 -- 4.1801)  max mem: 16413
Epoch: [150]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.9224 (1.7731)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3266 (8.8979)  time: 0.7938 (0.5260 -- 3.5815)  data: 0.2435 (0.0005 -- 3.0248)  max mem: 16413
Epoch: [150]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.6626 (1.7544)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8453 (8.8614)  time: 0.9140 (0.5114 -- 3.4828)  data: 0.3654 (0.0002 -- 2.9259)  max mem: 16413
Epoch: [150]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8027 (1.7669)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1259 (8.7191)  time: 0.8406 (0.5138 -- 4.2058)  data: 0.2873 (0.0003 -- 3.6713)  max mem: 16413
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8082 (1.7669)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2150 (8.7306)  time: 0.6428 (0.4967 -- 2.9981)  data: 0.1235 (0.0002 -- 2.4608)  max mem: 16413
Epoch: [150] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8082 (1.7511)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2150 (8.7306)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2563 (0.2563)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5001 (2.5001 -- 2.5001)  data: 2.2178 (2.2178 -- 2.2178)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4297 (0.6837)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4278 (0.1983 -- 2.5001)  data: 0.2028 (0.0008 -- 2.2178)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4764 (0.6148)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2142 (0.1691 -- 0.2567)  data: 0.0046 (0.0001 -- 0.0695)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5113 (0.6750)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.1974 (0.1329 -- 0.2567)  data: 0.0043 (0.0001 -- 0.0695)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 84.025 Acc@5 97.303 loss 0.645
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.44%
Epoch: [151]  [  0/160]  eta: 0:17:09  lr: 0.000005  min_lr: 0.000000  loss: 1.2650 (1.2650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5671 (7.5671)  time: 6.4321 (6.4321 -- 6.4321)  data: 5.8525 (5.8525 -- 5.8525)  max mem: 16413
[2023-08-29 23:53:05,563] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24165
[2023-08-29 23:53:05,563] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24165
[2023-08-29 23:53:05,564] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:53:05,564] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-29 23:53:05,564] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [151]  [ 20/160]  eta: 0:02:42  lr: 0.000005  min_lr: 0.000000  loss: 1.7747 (1.6930)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4003 (9.2505)  time: 0.8950 (0.5184 -- 2.0541)  data: 0.2487 (0.0005 -- 1.3879)  max mem: 16413
Epoch: [151]  [ 40/160]  eta: 0:02:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8017 (1.7116)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9871 (9.5117)  time: 0.8460 (0.5204 -- 2.6235)  data: 0.2499 (0.0005 -- 2.1065)  max mem: 16413
Epoch: [151]  [ 60/160]  eta: 0:01:34  lr: 0.000005  min_lr: 0.000000  loss: 1.6830 (1.6990)  loss_scale: 8192.0000 (8863.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2454 (9.2048)  time: 0.8155 (0.5267 -- 2.6001)  data: 0.1791 (0.0002 -- 2.0778)  max mem: 16413
Epoch: [151]  [ 80/160]  eta: 0:01:14  lr: 0.000005  min_lr: 0.000000  loss: 1.7292 (1.7058)  loss_scale: 8192.0000 (8697.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9494 (9.1664)  time: 0.8933 (0.5343 -- 3.3867)  data: 0.2885 (0.0003 -- 2.8468)  max mem: 16413
Epoch: [151]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.7254 (1.7057)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5455 (9.2126)  time: 0.8586 (0.5212 -- 2.4004)  data: 0.1964 (0.0006 -- 1.8777)  max mem: 16413
Epoch: [151]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.8582 (1.7296)  loss_scale: 8192.0000 (8530.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6167 (9.0782)  time: 0.9933 (0.5216 -- 2.6454)  data: 0.4443 (0.0004 -- 2.0892)  max mem: 16413
[2023-08-29 23:54:56,479] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:54:56,479] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-29 23:54:56,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:54:56,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [151]  [140/160]  eta: 0:00:17  lr: 0.000005  min_lr: 0.000000  loss: 1.4900 (1.7086)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7333 (9.1742)  time: 0.7203 (0.5204 -- 3.1480)  data: 0.1741 (0.0003 -- 2.6077)  max mem: 16413
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.9513 (1.7369)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1576 (9.1883)  time: 0.7816 (0.4977 -- 5.4961)  data: 0.2654 (0.0002 -- 4.9526)  max mem: 16413
Epoch: [151] Total time: 0:02:22 (0.8875 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.9513 (1.7506)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1576 (9.1883)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2450 (0.2450)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4082 (2.4082 -- 2.4082)  data: 2.1622 (2.1622 -- 2.1622)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4353 (0.6906)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4234 (0.1932 -- 2.4082)  data: 0.2031 (0.0009 -- 2.1622)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4705 (0.6149)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2191 (0.1689 -- 0.3159)  data: 0.0112 (0.0001 -- 0.1198)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4984 (0.6776)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (97.0954)  time: 0.2037 (0.1329 -- 0.3159)  data: 0.0109 (0.0001 -- 0.1198)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 83.402 Acc@5 97.303 loss 0.646
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.44%
Epoch: [152]  [  0/160]  eta: 0:17:26  lr: 0.000005  min_lr: 0.000000  loss: 1.8302 (1.8302)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8673 (8.8673)  time: 6.5375 (6.5375 -- 6.5375)  data: 5.9689 (5.9689 -- 5.9689)  max mem: 16413
Epoch: [152]  [ 20/160]  eta: 0:02:32  lr: 0.000005  min_lr: 0.000000  loss: 1.7496 (1.7466)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1748 (8.4134)  time: 0.8155 (0.5265 -- 2.3486)  data: 0.2614 (0.0010 -- 1.8128)  max mem: 16413
Epoch: [152]  [ 40/160]  eta: 0:01:59  lr: 0.000005  min_lr: 0.000000  loss: 1.8591 (1.7968)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9734 (8.5262)  time: 0.8940 (0.5186 -- 2.7782)  data: 0.1741 (0.0002 -- 2.0832)  max mem: 16413
Epoch: [152]  [ 60/160]  eta: 0:01:40  lr: 0.000005  min_lr: 0.000000  loss: 1.8297 (1.7728)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9480 (8.8402)  time: 1.0277 (0.5298 -- 2.7932)  data: 0.2738 (0.0003 -- 1.8875)  max mem: 16413
Epoch: [152]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 1.7249 (1.7579)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7077 (8.8573)  time: 0.7968 (0.5203 -- 4.2101)  data: 0.2420 (0.0003 -- 3.7004)  max mem: 16413
Epoch: [152]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.9024 (1.7662)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6007 (8.8748)  time: 0.9118 (0.5250 -- 4.1729)  data: 0.3469 (0.0002 -- 3.6482)  max mem: 16413
[2023-08-29 23:57:01,767] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:57:01,768] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:57:01,768] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:57:01,768] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:57:02,906] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24424
[2023-08-29 23:57:02,906] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24424
[2023-08-29 23:57:02,907] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:57:02,907] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:57:02,907] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [152]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.6799 (1.7590)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7860 (8.7841)  time: 0.7709 (0.5337 -- 2.9354)  data: 0.0813 (0.0003 -- 1.6006)  max mem: 16413
Epoch: [152]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.7449 (1.7534)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8562 (8.8813)  time: 0.8310 (0.5431 -- 2.8139)  data: 0.0034 (0.0003 -- 0.0344)  max mem: 16413
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7569 (1.7567)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8974 (8.8744)  time: 0.7528 (0.4969 -- 4.1058)  data: 0.0007 (0.0002 -- 0.0014)  max mem: 16413
Epoch: [152] Total time: 0:02:22 (0.8877 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7569 (1.7476)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8974 (8.8744)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2364 (0.2364)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3555 (2.3555 -- 2.3555)  data: 2.1008 (2.1008 -- 2.1008)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4483 (0.6885)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4208 (0.2122 -- 2.3555)  data: 0.1935 (0.0007 -- 2.1008)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4486 (0.6098)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2218 (0.1699 -- 0.3041)  data: 0.0093 (0.0001 -- 0.1059)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4956 (0.6734)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2022 (0.1330 -- 0.3041)  data: 0.0085 (0.0001 -- 0.1059)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.643
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.44%
Epoch: [153]  [  0/160]  eta: 0:22:19  lr: 0.000005  min_lr: 0.000000  loss: 1.4739 (1.4739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1380 (8.1380)  time: 8.3748 (8.3748 -- 8.3748)  data: 7.0354 (7.0354 -- 7.0354)  max mem: 16413
Epoch: [153]  [ 20/160]  eta: 0:02:58  lr: 0.000005  min_lr: 0.000000  loss: 1.7798 (1.7329)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9488 (9.2144)  time: 0.9198 (0.5214 -- 4.2459)  data: 0.3681 (0.0003 -- 3.7266)  max mem: 16413
Epoch: [153]  [ 40/160]  eta: 0:02:13  lr: 0.000004  min_lr: 0.000000  loss: 1.8170 (1.7760)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3194 (9.3347)  time: 0.9411 (0.5066 -- 5.1665)  data: 0.4124 (0.0001 -- 4.6669)  max mem: 16413
Epoch: [153]  [ 60/160]  eta: 0:01:42  lr: 0.000004  min_lr: 0.000000  loss: 1.8236 (1.7964)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0927 (9.0264)  time: 0.8587 (0.5241 -- 4.1081)  data: 0.3176 (0.0003 -- 3.5541)  max mem: 16413
[2023-08-29 23:59:07,891] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:59:07,892] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-29 23:59:07,892] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-29 23:59:07,892] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [153]  [ 80/160]  eta: 0:01:19  lr: 0.000004  min_lr: 0.000000  loss: 1.7889 (1.7911)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2394 (9.0334)  time: 0.8640 (0.5272 -- 3.7404)  data: 0.3121 (0.0004 -- 3.2266)  max mem: 16413
[2023-08-29 23:59:19,001] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24567
[2023-08-29 23:59:19,001] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24567
[2023-08-29 23:59:19,002] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:59:19,002] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-29 23:59:19,002] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [153]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.8924 (1.8102)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8905 (9.1893)  time: 0.7948 (0.5070 -- 2.7917)  data: 0.2446 (0.0005 -- 2.2607)  max mem: 16413
Epoch: [153]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.7531 (1.7981)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7249 (9.1088)  time: 0.8577 (0.5272 -- 2.2737)  data: 0.3079 (0.0003 -- 1.7584)  max mem: 16413
Epoch: [153]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8305 (1.7933)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5780 (9.1128)  time: 0.9296 (0.5229 -- 3.3595)  data: 0.3873 (0.0003 -- 2.8428)  max mem: 16413
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7471 (1.7833)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1168 (9.1269)  time: 0.7563 (0.4967 -- 3.3595)  data: 0.2275 (0.0002 -- 2.8428)  max mem: 16413
Epoch: [153] Total time: 0:02:23 (0.8968 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7471 (1.7934)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1168 (9.1269)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2530 (0.2530)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4430 (2.4430 -- 2.4430)  data: 2.2126 (2.2126 -- 2.2126)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4280 (0.6752)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4227 (0.1984 -- 2.4430)  data: 0.2036 (0.0008 -- 2.2126)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4555 (0.6030)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2196 (0.1692 -- 0.4792)  data: 0.0162 (0.0001 -- 0.2950)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4972 (0.6639)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.2022 (0.1323 -- 0.4792)  data: 0.0159 (0.0001 -- 0.2950)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 84.855 Acc@5 97.510 loss 0.644
Accuracy of the network on the 482 val images: 84.85%
[2023-08-30 00:00:26,313] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 00:00:26,315] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 00:00:26,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 00:00:26,315] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 00:00:27,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 00:00:27,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.85%
Epoch: [154]  [  0/160]  eta: 0:22:14  lr: 0.000004  min_lr: 0.000000  loss: 1.3960 (1.3960)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6875 (8.6875)  time: 8.3435 (8.3435 -- 8.3435)  data: 4.8636 (4.8636 -- 4.8636)  max mem: 16413
Epoch: [154]  [ 20/160]  eta: 0:02:52  lr: 0.000004  min_lr: 0.000000  loss: 1.8421 (1.7893)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7362 (8.7513)  time: 0.8802 (0.5241 -- 3.5486)  data: 0.0325 (0.0004 -- 0.3444)  max mem: 16413
Epoch: [154]  [ 40/160]  eta: 0:02:03  lr: 0.000004  min_lr: 0.000000  loss: 1.7785 (1.8180)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5975 (9.3778)  time: 0.8169 (0.5173 -- 3.5253)  data: 0.0013 (0.0003 -- 0.0033)  max mem: 16413
[2023-08-30 00:01:22,943] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:01:22,943] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:01:22,943] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:01:22,943] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:01:27,140] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24699
[2023-08-30 00:01:27,140] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:01:27,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 00:01:27,140] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24699
[2023-08-30 00:01:27,142] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [154]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.7207 (1.8023)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3633 (9.3255)  time: 0.8827 (0.5194 -- 3.0888)  data: 0.2778 (0.0003 -- 2.5584)  max mem: 16413
Epoch: [154]  [ 80/160]  eta: 0:01:13  lr: 0.000004  min_lr: 0.000000  loss: 1.7534 (1.7928)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0461 (9.3280)  time: 0.7368 (0.5279 -- 2.0699)  data: 0.1844 (0.0006 -- 1.5397)  max mem: 16413
Epoch: [154]  [100/160]  eta: 0:00:54  lr: 0.000004  min_lr: 0.000000  loss: 1.8370 (1.7909)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7567 (9.2902)  time: 0.8665 (0.5287 -- 3.3744)  data: 0.3141 (0.0003 -- 2.8624)  max mem: 16413
Epoch: [154]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.8946 (1.8101)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0555 (9.1256)  time: 0.9880 (0.5425 -- 3.9037)  data: 0.3812 (0.0004 -- 3.3414)  max mem: 16413
[2023-08-30 00:02:29,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24774
[2023-08-30 00:02:29,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:02:29,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24774
[2023-08-30 00:02:29,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:02:29,193] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [154]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8133 (1.8122)  loss_scale: 16384.0000 (16325.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3210 (9.1931)  time: 0.7713 (0.5224 -- 2.3839)  data: 0.1730 (0.0002 -- 1.1930)  max mem: 16413
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7384 (1.7927)  loss_scale: 8192.0000 (15360.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2722 (9.1406)  time: 0.7296 (0.4970 -- 3.4146)  data: 0.1077 (0.0002 -- 1.2723)  max mem: 16413
Epoch: [154] Total time: 0:02:21 (0.8828 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7384 (1.7714)  loss_scale: 8192.0000 (15360.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2722 (9.1406)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2492 (0.2492)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3858 (2.3858 -- 2.3858)  data: 2.1382 (2.1382 -- 2.1382)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4286 (0.6739)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4162 (0.1922 -- 2.3858)  data: 0.1954 (0.0004 -- 2.1382)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4483 (0.6006)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2225 (0.1705 -- 0.4488)  data: 0.0139 (0.0001 -- 0.2632)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5036 (0.6607)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2020 (0.1336 -- 0.4488)  data: 0.0136 (0.0001 -- 0.2632)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.640
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.85%
Epoch: [155]  [  0/160]  eta: 0:21:05  lr: 0.000004  min_lr: 0.000000  loss: 1.4650 (1.4650)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.4273 (12.4273)  time: 7.9081 (7.9081 -- 7.9081)  data: 7.3844 (7.3844 -- 7.3844)  max mem: 16413
Epoch: [155]  [ 20/160]  eta: 0:02:53  lr: 0.000004  min_lr: 0.000000  loss: 1.8106 (1.7556)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3035 (9.4648)  time: 0.9044 (0.5265 -- 4.2965)  data: 0.2527 (0.0003 -- 2.1981)  max mem: 16413
Epoch: [155]  [ 40/160]  eta: 0:02:07  lr: 0.000004  min_lr: 0.000000  loss: 1.6970 (1.7303)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6747 (9.0255)  time: 0.8790 (0.5335 -- 3.5929)  data: 0.0279 (0.0005 -- 0.5240)  max mem: 16413
Epoch: [155]  [ 60/160]  eta: 0:01:37  lr: 0.000004  min_lr: 0.000000  loss: 1.8274 (1.7691)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0768 (9.3534)  time: 0.7882 (0.5151 -- 4.3146)  data: 0.0109 (0.0004 -- 0.1185)  max mem: 16413
Epoch: [155]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.7254 (1.7574)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2315 (9.1466)  time: 0.8976 (0.5272 -- 3.7446)  data: 0.0014 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [155]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.8481 (1.7570)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4709 (9.0904)  time: 0.8826 (0.5267 -- 3.1321)  data: 0.0955 (0.0003 -- 1.2743)  max mem: 16413
[2023-08-30 00:04:33,421] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:04:33,422] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 00:04:33,426] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:04:33,426] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [155]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7113 (1.7369)  loss_scale: 16384.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6110 (8.9406)  time: 0.8008 (0.5197 -- 2.5631)  data: 0.1395 (0.0002 -- 2.0365)  max mem: 16413
Epoch: [155]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.6824 (1.7347)  loss_scale: 16384.0000 (10399.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9967 (8.8668)  time: 0.8435 (0.5285 -- 3.1909)  data: 0.1761 (0.0003 -- 2.6691)  max mem: 16413
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.9271 (1.7522)  loss_scale: 16384.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0624 (8.8253)  time: 0.6866 (0.4968 -- 2.3584)  data: 0.0009 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [155] Total time: 0:02:21 (0.8813 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.9271 (1.7705)  loss_scale: 16384.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0624 (8.8253)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2521 (0.2521)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4038 (2.4038 -- 2.4038)  data: 2.1519 (2.1519 -- 2.1519)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4411 (0.6748)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4201 (0.2039 -- 2.4038)  data: 0.1969 (0.0008 -- 2.1519)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4411 (0.6006)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2176 (0.1698 -- 0.3057)  data: 0.0096 (0.0001 -- 0.0924)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5028 (0.6623)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2013 (0.1337 -- 0.3057)  data: 0.0092 (0.0001 -- 0.0924)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 84.025 Acc@5 97.303 loss 0.639
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.85%
Epoch: [156]  [  0/160]  eta: 0:20:58  lr: 0.000004  min_lr: 0.000000  loss: 2.0818 (2.0818)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6170 (7.6170)  time: 7.8646 (7.8646 -- 7.8646)  data: 5.2763 (5.2763 -- 5.2763)  max mem: 16413
Epoch: [156]  [ 20/160]  eta: 0:02:50  lr: 0.000004  min_lr: 0.000000  loss: 1.7919 (1.8820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5132 (9.9085)  time: 0.8840 (0.5303 -- 3.6756)  data: 0.3087 (0.0006 -- 3.1521)  max mem: 16413
[2023-08-30 00:06:04,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=147, lr=[9.388315896672492e-08, 9.388315896672492e-08, 1.2517754528896656e-07, 1.2517754528896656e-07, 1.6690339371862208e-07, 1.6690339371862208e-07, 2.225378582914961e-07, 2.225378582914961e-07, 2.967171443886615e-07, 2.967171443886615e-07, 3.95622859184882e-07, 3.95622859184882e-07, 5.274971455798426e-07, 5.274971455798426e-07, 7.033295274397902e-07, 7.033295274397902e-07, 9.377727032530536e-07, 9.377727032530536e-07, 1.2503636043374048e-06, 1.2503636043374048e-06, 1.667151472449873e-06, 1.667151472449873e-06, 2.222868629933164e-06, 2.222868629933164e-06, 2.963824839910885e-06, 2.963824839910885e-06, 3.951766453214514e-06, 3.951766453214514e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 00:06:04,843] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=17.748556675746663, CurrSamplesPerSec=20.623168898345817, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [156]  [ 40/160]  eta: 0:01:56  lr: 0.000004  min_lr: 0.000000  loss: 1.7554 (1.7917)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5576 (9.4948)  time: 0.7171 (0.5351 -- 2.2138)  data: 0.1347 (0.0001 -- 1.6648)  max mem: 16413
Epoch: [156]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7593 (1.7977)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9712 (9.4084)  time: 0.9377 (0.5290 -- 2.6834)  data: 0.3532 (0.0006 -- 2.1438)  max mem: 16413
[2023-08-30 00:06:32,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:06:32,477] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:06:32,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:06:32,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [156]  [ 80/160]  eta: 0:01:14  lr: 0.000004  min_lr: 0.000000  loss: 1.6986 (1.7769)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5483 (9.4204)  time: 0.8626 (0.5268 -- 3.6819)  data: 0.0120 (0.0002 -- 0.1129)  max mem: 16413
[2023-08-30 00:06:52,336] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25052
[2023-08-30 00:06:52,337] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:06:52,337] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25052
[2023-08-30 00:06:52,340] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:06:52,337] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [156]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.7305 (1.7604)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1850 (9.5109)  time: 0.9101 (0.5275 -- 3.5236)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [156]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7074 (1.7499)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4655 (9.3409)  time: 0.8792 (0.5238 -- 4.0717)  data: 0.0065 (0.0004 -- 0.0997)  max mem: 16413
Epoch: [156]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7977 (1.7655)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2690 (9.2128)  time: 0.9134 (0.5207 -- 4.6389)  data: 0.3010 (0.0003 -- 4.1349)  max mem: 16413
[2023-08-30 00:07:46,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25117
[2023-08-30 00:07:46,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25117
[2023-08-30 00:07:46,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:07:46,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:07:46,870] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8696 (1.7596)  loss_scale: 16384.0000 (18380.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0378 (9.1573)  time: 0.6398 (0.4864 -- 2.5514)  data: 0.1139 (0.0002 -- 2.0422)  max mem: 16413
Epoch: [156] Total time: 0:02:22 (0.8891 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8696 (1.7476)  loss_scale: 16384.0000 (18380.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0378 (9.1573)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2446 (0.2446)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1580 (2.1580 -- 2.1580)  data: 1.9242 (1.9242 -- 1.9242)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4474 (0.6764)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4248 (0.1992 -- 2.1580)  data: 0.1997 (0.0005 -- 1.9242)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4474 (0.6003)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2277 (0.1698 -- 0.4968)  data: 0.0161 (0.0001 -- 0.2591)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4984 (0.6628)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2095 (0.1334 -- 0.4968)  data: 0.0159 (0.0001 -- 0.2591)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.637
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.85%
Epoch: [157]  [  0/160]  eta: 0:20:07  lr: 0.000004  min_lr: 0.000000  loss: 1.2584 (1.2584)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4720 (10.4720)  time: 7.5474 (7.5474 -- 7.5474)  data: 7.0035 (7.0035 -- 7.0035)  max mem: 16413
Epoch: [157]  [ 20/160]  eta: 0:02:49  lr: 0.000004  min_lr: 0.000000  loss: 1.8465 (1.7152)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1364 (9.5770)  time: 0.8922 (0.5168 -- 4.9713)  data: 0.2947 (0.0005 -- 3.3282)  max mem: 16413
Epoch: [157]  [ 40/160]  eta: 0:02:13  lr: 0.000004  min_lr: 0.000000  loss: 1.7596 (1.7204)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7619 (9.2304)  time: 1.0033 (0.5065 -- 4.0095)  data: 0.0706 (0.0003 -- 0.9931)  max mem: 16413
Epoch: [157]  [ 60/160]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000000  loss: 1.6875 (1.7204)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3998 (9.1512)  time: 0.7490 (0.5311 -- 3.0353)  data: 0.0015 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [157]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.7570 (1.7309)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7682 (8.9561)  time: 0.8633 (0.5125 -- 3.9613)  data: 0.0504 (0.0003 -- 0.6567)  max mem: 16413
Epoch: [157]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.9750 (1.7730)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4588 (8.9529)  time: 0.8006 (0.5339 -- 3.0856)  data: 0.1530 (0.0002 -- 1.6394)  max mem: 16413
Epoch: [157]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.8085 (1.7824)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9049 (8.9756)  time: 0.8298 (0.5343 -- 2.1182)  data: 0.0615 (0.0003 -- 0.7703)  max mem: 16413
[2023-08-30 00:09:51,258] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:09:51,258] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 00:09:51,261] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:09:51,262] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [157]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.6633 (1.7677)  loss_scale: 16384.0000 (9063.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5127 (9.0365)  time: 0.9271 (0.5305 -- 3.1425)  data: 0.2835 (0.0002 -- 2.6189)  max mem: 16413
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.5045 (1.7456)  loss_scale: 16384.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9129 (9.0535)  time: 0.7022 (0.4958 -- 2.4454)  data: 0.1395 (0.0001 -- 1.9007)  max mem: 16413
Epoch: [157] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.5045 (1.7811)  loss_scale: 16384.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9129 (9.0535)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2405 (0.2405)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3807 (2.3807 -- 2.3807)  data: 2.1621 (2.1621 -- 2.1621)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4524 (0.6771)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4085 (0.2020 -- 2.3807)  data: 0.1978 (0.0006 -- 2.1621)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4484 (0.6000)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2255 (0.1699 -- 0.6508)  data: 0.0241 (0.0001 -- 0.4658)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4957 (0.6643)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.2114 (0.1325 -- 0.6508)  data: 0.0236 (0.0001 -- 0.4658)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.640
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.85%
Epoch: [158]  [  0/160]  eta: 0:19:53  lr: 0.000004  min_lr: 0.000000  loss: 1.9961 (1.9961)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3744 (8.3744)  time: 7.4575 (7.4575 -- 7.4575)  data: 6.9491 (6.9491 -- 6.9491)  max mem: 16413
Epoch: [158]  [ 20/160]  eta: 0:02:46  lr: 0.000004  min_lr: 0.000000  loss: 1.7830 (1.8449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6061 (9.0487)  time: 0.8760 (0.5292 -- 3.8477)  data: 0.3007 (0.0008 -- 3.3200)  max mem: 16413
Epoch: [158]  [ 40/160]  eta: 0:02:03  lr: 0.000004  min_lr: 0.000000  loss: 1.6835 (1.7623)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3100 (8.6454)  time: 0.8627 (0.5219 -- 2.7926)  data: 0.1644 (0.0004 -- 1.5309)  max mem: 16413
[2023-08-30 00:11:09,847] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25323
[2023-08-30 00:11:09,847] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25323
[2023-08-30 00:11:09,848] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:11:09,848] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:11:09,848] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [158]  [ 60/160]  eta: 0:01:35  lr: 0.000004  min_lr: 0.000000  loss: 1.8189 (1.7812)  loss_scale: 8192.0000 (13966.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8693 (8.5311)  time: 0.7925 (0.5189 -- 3.0915)  data: 0.1600 (0.0002 -- 2.2134)  max mem: 16413
Epoch: [158]  [ 80/160]  eta: 0:01:17  lr: 0.000004  min_lr: 0.000000  loss: 1.8133 (1.7848)  loss_scale: 8192.0000 (12540.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6249 (8.6641)  time: 1.0017 (0.5271 -- 4.0781)  data: 0.2323 (0.0003 -- 3.0236)  max mem: 16413
Epoch: [158]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.5542 (1.7612)  loss_scale: 8192.0000 (11679.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0339 (8.6507)  time: 0.7539 (0.5273 -- 2.0972)  data: 0.0015 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [158]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7037 (1.7510)  loss_scale: 8192.0000 (11103.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7180 (8.7634)  time: 0.8162 (0.5210 -- 3.6973)  data: 0.1288 (0.0003 -- 1.0854)  max mem: 16413
Epoch: [158]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.6790 (1.7449)  loss_scale: 8192.0000 (10690.2695)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1311 (8.8165)  time: 1.0344 (0.5410 -- 3.9241)  data: 0.4338 (0.0005 -- 3.4040)  max mem: 16413
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7729 (1.7473)  loss_scale: 8192.0000 (10393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3854 (8.7807)  time: 0.6228 (0.4970 -- 2.0283)  data: 0.1058 (0.0002 -- 1.5340)  max mem: 16413
Epoch: [158] Total time: 0:02:22 (0.8884 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7729 (1.7348)  loss_scale: 8192.0000 (10393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3854 (8.7807)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2334 (0.2334)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4701 (2.4701 -- 2.4701)  data: 2.1996 (2.1996 -- 2.1996)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4319 (0.6704)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4208 (0.2078 -- 2.4701)  data: 0.2010 (0.0005 -- 2.1996)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4319 (0.5997)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2181 (0.1704 -- 0.3249)  data: 0.0122 (0.0001 -- 0.1200)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4963 (0.6596)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2017 (0.1333 -- 0.3249)  data: 0.0119 (0.0001 -- 0.1200)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 84.025 Acc@5 97.510 loss 0.636
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.85%
Epoch: [159]  [  0/160]  eta: 0:20:01  lr: 0.000003  min_lr: 0.000000  loss: 2.1719 (2.1719)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1821 (13.1821)  time: 7.5088 (7.5088 -- 7.5088)  data: 6.9751 (6.9751 -- 6.9751)  max mem: 16413
[2023-08-30 00:13:13,189] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:13:13,189] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 00:13:13,192] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:13:13,192] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [159]  [ 20/160]  eta: 0:02:46  lr: 0.000003  min_lr: 0.000000  loss: 1.6890 (1.7639)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9623 (10.4635)  time: 0.8748 (0.5260 -- 3.8471)  data: 0.3202 (0.0002 -- 3.3186)  max mem: 16413
Epoch: [159]  [ 40/160]  eta: 0:02:09  lr: 0.000003  min_lr: 0.000000  loss: 1.7295 (1.7913)  loss_scale: 16384.0000 (13986.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4380 (9.5376)  time: 0.9581 (0.5296 -- 4.0670)  data: 0.4086 (0.0004 -- 3.5500)  max mem: 16413
Epoch: [159]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000000  loss: 1.6516 (1.7403)  loss_scale: 16384.0000 (14772.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4919 (9.2585)  time: 0.7530 (0.5210 -- 3.2151)  data: 0.2014 (0.0004 -- 2.6908)  max mem: 16413
Epoch: [159]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.6156 (1.7275)  loss_scale: 16384.0000 (15170.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5650 (9.2503)  time: 0.9033 (0.5191 -- 4.8922)  data: 0.3514 (0.0004 -- 4.3673)  max mem: 16413
Epoch: [159]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.6999 (1.7262)  loss_scale: 16384.0000 (15410.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3513 (8.9504)  time: 0.8325 (0.5332 -- 3.0145)  data: 0.2832 (0.0004 -- 2.4765)  max mem: 16413
Epoch: [159]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7667 (1.7316)  loss_scale: 16384.0000 (15571.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1381 (8.8968)  time: 0.8932 (0.5278 -- 4.9098)  data: 0.3342 (0.0003 -- 4.3894)  max mem: 16413
[2023-08-30 00:15:02,746] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:15:02,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:15:02,748] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:15:02,749] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [159]  [140/160]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 1.8605 (1.7442)  loss_scale: 16384.0000 (15803.0071)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1029 (8.9057)  time: 0.7512 (0.5205 -- 2.6817)  data: 0.0575 (0.0005 -- 0.9217)  max mem: 16413
[2023-08-30 00:15:06,821] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25584
[2023-08-30 00:15:06,821] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:15:06,821] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25584
[2023-08-30 00:15:06,822] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 00:15:06,822] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8259 (1.7470)  loss_scale: 16384.0000 (16179.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4880 (8.9119)  time: 0.6971 (0.5005 -- 2.4518)  data: 0.0010 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [159] Total time: 0:02:20 (0.8765 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8259 (1.7482)  loss_scale: 16384.0000 (16179.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4880 (8.9119)
[2023-08-30 00:15:16,144] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-08-30 00:15:16,146] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-08-30 00:15:16,146] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-08-30 00:15:16,146] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-08-30 00:15:17,147] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-08-30 00:15:17,147] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2321 (0.2321)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5155 (2.5155 -- 2.5155)  data: 2.2604 (2.2604 -- 2.2604)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4358 (0.6755)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4431 (0.2065 -- 2.5155)  data: 0.2235 (0.0009 -- 2.2604)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4358 (0.6007)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2196 (0.1709 -- 0.3722)  data: 0.0135 (0.0001 -- 0.1800)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4949 (0.6613)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2029 (0.1326 -- 0.3722)  data: 0.0129 (0.0001 -- 0.1800)  max mem: 16413
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.638
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.85%
Epoch: [160]  [  0/160]  eta: 0:23:50  lr: 0.000003  min_lr: 0.000000  loss: 1.9562 (1.9562)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2663 (8.2663)  time: 8.9401 (8.9401 -- 8.9401)  data: 6.7621 (6.7621 -- 6.7621)  max mem: 16413
Epoch: [160]  [ 20/160]  eta: 0:02:46  lr: 0.000003  min_lr: 0.000000  loss: 1.7968 (1.7696)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7271 (10.8372)  time: 0.8007 (0.5331 -- 3.7970)  data: 0.2498 (0.0006 -- 3.2531)  max mem: 16413
Epoch: [160]  [ 40/160]  eta: 0:02:11  lr: 0.000003  min_lr: 0.000000  loss: 1.6586 (1.7213)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1733 (10.0815)  time: 1.0017 (0.5251 -- 4.5176)  data: 0.4541 (0.0002 -- 4.0064)  max mem: 16413
Epoch: [160]  [ 60/160]  eta: 0:01:38  lr: 0.000003  min_lr: 0.000000  loss: 1.9265 (1.7404)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5229 (9.6643)  time: 0.7652 (0.5066 -- 2.7441)  data: 0.1977 (0.0005 -- 2.1937)  max mem: 16413
Epoch: [160]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000000  loss: 1.6199 (1.7033)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9231 (9.5893)  time: 0.9053 (0.5131 -- 3.5851)  data: 0.3615 (0.0003 -- 3.0549)  max mem: 16413
Epoch: [160]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.6443 (1.7101)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5133 (9.4355)  time: 0.7589 (0.5264 -- 1.8858)  data: 0.0532 (0.0001 -- 1.0082)  max mem: 16413
[2023-08-30 00:17:10,864] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:17:10,864] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:17:10,865] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:17:10,865] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:17:11,985] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25715
[2023-08-30 00:17:11,985] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25715
[2023-08-30 00:17:11,985] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:17:11,985] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:17:11,986] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [160]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7440 (1.7208)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0572 (9.3803)  time: 0.9050 (0.5374 -- 2.5598)  data: 0.1465 (0.0005 -- 1.5664)  max mem: 16413
[2023-08-30 00:17:29,668] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25735
[2023-08-30 00:17:29,668] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25735
[2023-08-30 00:17:29,668] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:17:29,668] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:17:29,668] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [160]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.9131 (1.7335)  loss_scale: 16384.0000 (16267.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4905 (9.4365)  time: 0.7815 (0.5324 -- 2.7123)  data: 0.0328 (0.0004 -- 0.3196)  max mem: 16413
Epoch: [160]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7200 (1.7289)  loss_scale: 8192.0000 (15308.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4899 (9.5860)  time: 0.7137 (0.4975 -- 2.3450)  data: 0.1636 (0.0002 -- 1.6573)  max mem: 16413
Epoch: [160] Total time: 0:02:21 (0.8815 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7200 (1.7467)  loss_scale: 8192.0000 (15308.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4899 (9.5860)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2294 (0.2294)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2739 (2.2739 -- 2.2739)  data: 2.0505 (2.0505 -- 2.0505)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4270 (0.6767)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4374 (0.1959 -- 2.2739)  data: 0.2171 (0.0007 -- 2.0505)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4270 (0.6015)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2378 (0.1694 -- 0.5622)  data: 0.0332 (0.0001 -- 0.3284)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4940 (0.6619)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.2204 (0.1326 -- 0.5622)  data: 0.0329 (0.0001 -- 0.3284)  max mem: 16413
Val: Total time: 0:00:08 (0.2971 s / it)
* Acc@1 83.610 Acc@5 97.510 loss 0.637
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.85%
Epoch: [161]  [  0/160]  eta: 0:19:23  lr: 0.000003  min_lr: 0.000000  loss: 2.5873 (2.5873)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9103 (6.9103)  time: 7.2747 (7.2747 -- 7.2747)  data: 6.7389 (6.7389 -- 6.7389)  max mem: 16413
Epoch: [161]  [ 20/160]  eta: 0:02:40  lr: 0.000003  min_lr: 0.000000  loss: 1.6176 (1.6156)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0378 (8.4389)  time: 0.8432 (0.5268 -- 2.0241)  data: 0.2858 (0.0007 -- 1.4974)  max mem: 16413
Epoch: [161]  [ 40/160]  eta: 0:02:02  lr: 0.000003  min_lr: 0.000000  loss: 1.6151 (1.6392)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9426 (8.6226)  time: 0.8932 (0.5312 -- 3.3209)  data: 0.3450 (0.0003 -- 2.7805)  max mem: 16413
Epoch: [161]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000000  loss: 1.7032 (1.6879)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0389 (8.5165)  time: 0.8776 (0.5326 -- 3.0915)  data: 0.3216 (0.0004 -- 2.5561)  max mem: 16413
Epoch: [161]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000000  loss: 1.6865 (1.7142)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8876 (8.7277)  time: 0.9625 (0.5282 -- 4.1209)  data: 0.4022 (0.0008 -- 3.6107)  max mem: 16413
Epoch: [161]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.8473 (1.7336)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6891 (8.7571)  time: 0.7457 (0.5367 -- 2.0359)  data: 0.1196 (0.0001 -- 1.4957)  max mem: 16413
[2023-08-30 00:19:32,319] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:19:32,320] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 00:19:32,320] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:19:32,320] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [161]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.6766 (1.7266)  loss_scale: 16384.0000 (9342.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7378 (8.7324)  time: 0.8553 (0.5330 -- 2.7680)  data: 0.2247 (0.0002 -- 2.2499)  max mem: 16413
Epoch: [161]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.6502 (1.7080)  loss_scale: 16384.0000 (10341.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5243 (8.6909)  time: 0.8313 (0.5251 -- 2.5976)  data: 0.1806 (0.0002 -- 2.0620)  max mem: 16413
Epoch: [161]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7495 (1.7088)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1894 (8.7864)  time: 0.7796 (0.4995 -- 3.9177)  data: 0.0150 (0.0002 -- 0.2877)  max mem: 16413
Epoch: [161] Total time: 0:02:22 (0.8906 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7495 (1.7472)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1894 (8.7864)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2387 (0.2387)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1668 (2.1668 -- 2.1668)  data: 1.9638 (1.9638 -- 1.9638)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4244 (0.6784)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4158 (0.2026 -- 2.1668)  data: 0.2027 (0.0007 -- 1.9638)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4273 (0.5996)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2329 (0.1697 -- 0.4666)  data: 0.0277 (0.0001 -- 0.2754)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4933 (0.6618)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.2164 (0.1327 -- 0.4666)  data: 0.0274 (0.0001 -- 0.2754)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 84.025 Acc@5 97.303 loss 0.640
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.85%
Epoch: [162]  [  0/160]  eta: 0:18:27  lr: 0.000003  min_lr: 0.000000  loss: 1.6811 (1.6811)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4503 (7.4503)  time: 6.9240 (6.9240 -- 6.9240)  data: 5.2268 (5.2268 -- 5.2268)  max mem: 16413
Epoch: [162]  [ 20/160]  eta: 0:02:44  lr: 0.000003  min_lr: 0.000000  loss: 1.7393 (1.7746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8578 (8.8700)  time: 0.8886 (0.5242 -- 3.2394)  data: 0.0021 (0.0005 -- 0.0079)  max mem: 16413
Epoch: [162]  [ 40/160]  eta: 0:01:58  lr: 0.000003  min_lr: 0.000000  loss: 1.6349 (1.7096)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7862 (8.7704)  time: 0.7854 (0.5158 -- 3.9140)  data: 0.0473 (0.0001 -- 0.8152)  max mem: 16413
Epoch: [162]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000000  loss: 1.5383 (1.6886)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5691 (8.8096)  time: 1.0361 (0.5202 -- 3.4376)  data: 0.1890 (0.0003 -- 1.6861)  max mem: 16413
[2023-08-30 00:21:26,140] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25981
[2023-08-30 00:21:26,140] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25981
[2023-08-30 00:21:26,140] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:21:26,140] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:21:26,140] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 00:21:35,997] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25993
[2023-08-30 00:21:35,997] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 00:21:35,997] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25993
[2023-08-30 00:21:35,997] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 00:21:35,998] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2023-08-30 00:21:41,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=155, lr=[7.003247759169534e-08, 7.003247759169534e-08, 9.337663678892713e-08, 9.337663678892713e-08, 1.2450218238523618e-07, 1.2450218238523618e-07, 1.6600290984698156e-07, 1.6600290984698156e-07, 2.2133721312930873e-07, 2.2133721312930873e-07, 2.951162841724117e-07, 2.951162841724117e-07, 3.9348837889654886e-07, 3.9348837889654886e-07, 5.246511718620652e-07, 5.246511718620652e-07, 6.995348958160869e-07, 6.995348958160869e-07, 9.327131944214492e-07, 9.327131944214492e-07, 1.2436175925619322e-06, 1.2436175925619322e-06, 1.6581567900825764e-06, 1.6581567900825764e-06, 2.210875720110102e-06, 2.210875720110102e-06, 2.9478342934801357e-06, 2.9478342934801357e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 00:21:41,401] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=17.7741820624938, CurrSamplesPerSec=22.952042154128883, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [162]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.7564 (1.7180)  loss_scale: 8192.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5605 (8.6426)  time: 0.8186 (0.5118 -- 2.6958)  data: 0.1270 (0.0002 -- 2.1586)  max mem: 16413
Epoch: [162]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.7688 (1.7154)  loss_scale: 4096.0000 (12004.1188)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4565 (8.6275)  time: 0.8527 (0.5296 -- 2.5767)  data: 0.1755 (0.0005 -- 2.0189)  max mem: 16413
Epoch: [162]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7253 (1.7133)  loss_scale: 4096.0000 (10696.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1661 (8.5927)  time: 0.8506 (0.5227 -- 2.9289)  data: 0.2053 (0.0003 -- 2.3845)  max mem: 16413
Epoch: [162]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7012 (1.7080)  loss_scale: 4096.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3545 (8.7281)  time: 0.8648 (0.5394 -- 3.6195)  data: 0.3050 (0.0005 -- 3.1004)  max mem: 16413
Epoch: [162]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6095 (1.7062)  loss_scale: 4096.0000 (9088.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7320 (8.7318)  time: 0.6853 (0.4959 -- 2.4601)  data: 0.1548 (0.0002 -- 1.9425)  max mem: 16413
Epoch: [162] Total time: 0:02:20 (0.8806 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6095 (1.7422)  loss_scale: 4096.0000 (9088.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7320 (8.7318)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2351 (0.2351)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3889 (2.3889 -- 2.3889)  data: 2.1542 (2.1542 -- 2.1542)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4286 (0.6747)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4136 (0.2053 -- 2.3889)  data: 0.1969 (0.0009 -- 2.1542)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4286 (0.5990)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2244 (0.1696 -- 0.5699)  data: 0.0193 (0.0001 -- 0.3715)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4923 (0.6601)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.2656)  time: 0.2079 (0.1334 -- 0.5699)  data: 0.0190 (0.0001 -- 0.3715)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 84.232 Acc@5 97.095 loss 0.637
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.85%
Epoch: [163]  [  0/160]  eta: 0:14:20  lr: 0.000003  min_lr: 0.000000  loss: 0.9909 (0.9909)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7506 (8.7506)  time: 5.3812 (5.3812 -- 5.3812)  data: 4.8340 (4.8340 -- 4.8340)  max mem: 16413
Epoch: [163]  [ 20/160]  eta: 0:02:51  lr: 0.000003  min_lr: 0.000000  loss: 1.8126 (1.7622)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5592 (8.9699)  time: 1.0201 (0.5228 -- 3.6856)  data: 0.4630 (0.0004 -- 3.1336)  max mem: 16413
Epoch: [163]  [ 40/160]  eta: 0:02:06  lr: 0.000003  min_lr: 0.000000  loss: 1.8087 (1.7530)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2223 (9.1513)  time: 0.8708 (0.5169 -- 4.4265)  data: 0.3288 (0.0003 -- 3.8944)  max mem: 16413
[2023-08-30 00:23:40,591] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:23:40,591] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-30 00:23:40,592] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:23:40,592] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [163]  [ 60/160]  eta: 0:01:41  lr: 0.000003  min_lr: 0.000000  loss: 1.7739 (1.7340)  loss_scale: 8192.0000 (5371.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6261 (8.9439)  time: 0.9231 (0.5078 -- 3.6218)  data: 0.3779 (0.0004 -- 3.1125)  max mem: 16413
Epoch: [163]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.5843 (1.7293)  loss_scale: 8192.0000 (6068.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1595 (8.7996)  time: 0.7651 (0.5354 -- 3.3877)  data: 0.2037 (0.0007 -- 2.8599)  max mem: 16413
Epoch: [163]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.7627 (1.7360)  loss_scale: 8192.0000 (6488.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8720 (8.9398)  time: 0.8413 (0.5224 -- 3.3770)  data: 0.2859 (0.0005 -- 2.8459)  max mem: 16413
Epoch: [163]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7057 (1.7295)  loss_scale: 8192.0000 (6770.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3635 (8.7395)  time: 0.7984 (0.5324 -- 2.0823)  data: 0.1005 (0.0004 -- 1.5279)  max mem: 16413
Epoch: [163]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.6828 (1.7326)  loss_scale: 8192.0000 (6971.9149)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4606 (8.8616)  time: 0.9881 (0.5348 -- 2.8830)  data: 0.3172 (0.0009 -- 2.3391)  max mem: 16413
Epoch: [163]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6643 (1.7316)  loss_scale: 8192.0000 (7116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9018 (8.7874)  time: 0.7297 (0.4958 -- 2.0085)  data: 0.0932 (0.0002 -- 0.8677)  max mem: 16413
Epoch: [163] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6643 (1.7451)  loss_scale: 8192.0000 (7116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9018 (8.7874)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2405 (0.2405)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1559 (2.1559 -- 2.1559)  data: 1.9172 (1.9172 -- 1.9172)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4343 (0.6733)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.3997 (0.2057 -- 2.1559)  data: 0.1832 (0.0006 -- 1.9172)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4344 (0.5990)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2295 (0.1705 -- 0.3823)  data: 0.0246 (0.0001 -- 0.1999)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4953 (0.6598)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.2656)  time: 0.2142 (0.1332 -- 0.3823)  data: 0.0243 (0.0001 -- 0.1999)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 84.025 Acc@5 97.095 loss 0.638
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.85%
Epoch: [164]  [  0/160]  eta: 0:19:26  lr: 0.000003  min_lr: 0.000000  loss: 2.0566 (2.0566)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0643 (7.0643)  time: 7.2920 (7.2920 -- 7.2920)  data: 6.5252 (6.5252 -- 6.5252)  max mem: 16413
[2023-08-30 00:25:41,052] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:25:41,052] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 00:25:41,052] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:25:41,052] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [164]  [ 20/160]  eta: 0:02:37  lr: 0.000003  min_lr: 0.000000  loss: 1.8210 (1.7761)  loss_scale: 16384.0000 (12483.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6583 (8.6649)  time: 0.8183 (0.5327 -- 2.2972)  data: 0.0288 (0.0004 -- 0.5525)  max mem: 16413
Epoch: [164]  [ 40/160]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6699 (1.7515)  loss_scale: 16384.0000 (14385.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7366 (8.9784)  time: 0.8823 (0.5141 -- 3.4603)  data: 0.2862 (0.0003 -- 2.9463)  max mem: 16413
Epoch: [164]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000000  loss: 1.8109 (1.7504)  loss_scale: 16384.0000 (15041.0492)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5709 (8.9821)  time: 0.9166 (0.5204 -- 4.0053)  data: 0.3456 (0.0006 -- 3.4604)  max mem: 16413
[2023-08-30 00:26:36,169] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26314
[2023-08-30 00:26:36,169] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26314
[2023-08-30 00:26:36,169] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:26:36,169] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:26:36,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [164]  [ 80/160]  eta: 0:01:14  lr: 0.000003  min_lr: 0.000000  loss: 1.8768 (1.7704)  loss_scale: 16384.0000 (14664.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1137 (9.1492)  time: 0.8028 (0.5158 -- 3.3452)  data: 0.2514 (0.0004 -- 2.7767)  max mem: 16413
Epoch: [164]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.7078 (1.7562)  loss_scale: 8192.0000 (13382.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4682 (9.4114)  time: 1.0025 (0.5219 -- 4.4345)  data: 0.4471 (0.0003 -- 3.9190)  max mem: 16413
Epoch: [164]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 1.7842 (1.7522)  loss_scale: 8192.0000 (12524.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1698 (9.3252)  time: 0.8142 (0.5270 -- 3.6661)  data: 0.2607 (0.0006 -- 3.1286)  max mem: 16413
Epoch: [164]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7668 (1.7596)  loss_scale: 8192.0000 (11910.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2937 (9.1945)  time: 1.0178 (0.5413 -- 3.1723)  data: 0.4687 (0.0006 -- 2.6512)  max mem: 16413
Epoch: [164]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6264 (1.7485)  loss_scale: 8192.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2390 (9.1924)  time: 0.7257 (0.4951 -- 3.1723)  data: 0.2049 (0.0002 -- 2.6512)  max mem: 16413
Epoch: [164] Total time: 0:02:23 (0.8984 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6264 (1.7554)  loss_scale: 8192.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2390 (9.1924)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2370 (0.2370)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4758 (2.4758 -- 2.4758)  data: 2.2061 (2.2061 -- 2.2061)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4312 (0.6749)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4174 (0.1959 -- 2.4758)  data: 0.2017 (0.0008 -- 2.2061)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4312 (0.5991)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2155 (0.1693 -- 0.4340)  data: 0.0118 (0.0001 -- 0.2202)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4893 (0.6608)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.2006 (0.1327 -- 0.4340)  data: 0.0114 (0.0001 -- 0.2202)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 83.817 Acc@5 97.303 loss 0.638
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.85%
Epoch: [165]  [  0/160]  eta: 0:21:03  lr: 0.000003  min_lr: 0.000000  loss: 2.4344 (2.4344)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9117 (6.9117)  time: 7.8986 (7.8986 -- 7.8986)  data: 6.8004 (6.8004 -- 6.8004)  max mem: 16413
Epoch: [165]  [ 20/160]  eta: 0:02:38  lr: 0.000003  min_lr: 0.000000  loss: 1.7390 (1.8077)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2386 (9.6306)  time: 0.7965 (0.5295 -- 2.9857)  data: 0.0427 (0.0009 -- 0.5237)  max mem: 16413
Epoch: [165]  [ 40/160]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6345 (1.7606)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5289 (8.9069)  time: 0.8731 (0.5371 -- 2.7112)  data: 0.0567 (0.0007 -- 0.9398)  max mem: 16413
[2023-08-30 00:28:38,223] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:28:38,223] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 00:28:38,223] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:28:38,223] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [165]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000000  loss: 1.7282 (1.7490)  loss_scale: 16384.0000 (10609.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0078 (8.6032)  time: 1.0097 (0.5233 -- 4.8028)  data: 0.0013 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [165]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.5710 (1.7270)  loss_scale: 16384.0000 (12035.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9439 (8.8549)  time: 0.8152 (0.5312 -- 3.1612)  data: 0.0015 (0.0001 -- 0.0030)  max mem: 16413
Epoch: [165]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8701 (1.7424)  loss_scale: 16384.0000 (12896.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0691 (9.0221)  time: 0.7739 (0.5298 -- 3.3681)  data: 0.0017 (0.0004 -- 0.0041)  max mem: 16413
Epoch: [165]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7259 (1.7474)  loss_scale: 16384.0000 (13472.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7032 (9.0295)  time: 0.8848 (0.5271 -- 2.6926)  data: 0.0016 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [165]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8006 (1.7542)  loss_scale: 16384.0000 (13885.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7996 (8.9421)  time: 0.8227 (0.5311 -- 3.4248)  data: 0.0018 (0.0007 -- 0.0053)  max mem: 16413
Epoch: [165]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8365 (1.7451)  loss_scale: 16384.0000 (14182.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8199 (8.9025)  time: 0.7747 (0.4958 -- 3.1247)  data: 0.0011 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [165] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8365 (1.7791)  loss_scale: 16384.0000 (14182.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8199 (8.9025)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2367 (0.2367)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2543 (2.2543 -- 2.2543)  data: 2.0456 (2.0456 -- 2.0456)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4312 (0.6740)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4270 (0.1901 -- 2.2543)  data: 0.2151 (0.0006 -- 2.0456)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4312 (0.5992)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2275 (0.1698 -- 0.5185)  data: 0.0217 (0.0001 -- 0.3081)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4901 (0.6598)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.2145 (0.1332 -- 0.5185)  data: 0.0214 (0.0001 -- 0.3081)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.637
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.85%
Epoch: [166]  [  0/160]  eta: 0:16:59  lr: 0.000002  min_lr: 0.000000  loss: 1.8289 (1.8289)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5238 (7.5238)  time: 6.3732 (6.3732 -- 6.3732)  data: 5.8281 (5.8281 -- 5.8281)  max mem: 16413
[2023-08-30 00:30:42,228] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:30:42,229] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:30:42,230] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:30:42,230] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [166]  [ 20/160]  eta: 0:02:48  lr: 0.000002  min_lr: 0.000000  loss: 1.8762 (1.8546)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2994 (9.1799)  time: 0.9475 (0.5368 -- 3.1915)  data: 0.1747 (0.0002 -- 2.6164)  max mem: 16413
Epoch: [166]  [ 40/160]  eta: 0:01:58  lr: 0.000002  min_lr: 0.000000  loss: 1.8487 (1.8465)  loss_scale: 32768.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3859 (8.8818)  time: 0.7653 (0.5354 -- 2.1728)  data: 0.1383 (0.0006 -- 1.6513)  max mem: 16413
[2023-08-30 00:31:25,444] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26619
[2023-08-30 00:31:25,444] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26619
[2023-08-30 00:31:25,445] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:31:25,445] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:31:25,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [166]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000000  loss: 1.6135 (1.7641)  loss_scale: 32768.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3350 (8.7109)  time: 1.0314 (0.5220 -- 4.0574)  data: 0.0014 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [166]  [ 80/160]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5417 (1.7373)  loss_scale: 16384.0000 (26093.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2087 (8.7474)  time: 0.8972 (0.5367 -- 4.0277)  data: 0.0020 (0.0004 -- 0.0115)  max mem: 16413
Epoch: [166]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8280 (1.7645)  loss_scale: 16384.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6459 (8.7526)  time: 0.7445 (0.5355 -- 2.2528)  data: 0.0027 (0.0005 -- 0.0147)  max mem: 16413
Epoch: [166]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7928 (1.7607)  loss_scale: 16384.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2033 (8.7958)  time: 0.8678 (0.5224 -- 4.0259)  data: 0.0018 (0.0006 -- 0.0040)  max mem: 16413
Epoch: [166]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7456 (1.7589)  loss_scale: 16384.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2481 (8.9498)  time: 0.9576 (0.5351 -- 4.3793)  data: 0.2431 (0.0002 -- 3.8372)  max mem: 16413
Epoch: [166]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7494 (1.7595)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0834 (8.9256)  time: 0.6583 (0.4977 -- 1.9081)  data: 0.1342 (0.0002 -- 1.4153)  max mem: 16413
Epoch: [166] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7494 (1.7658)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0834 (8.9256)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2351 (0.2351)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2324 (2.2324 -- 2.2324)  data: 2.0095 (2.0095 -- 2.0095)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4300 (0.6739)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4104 (0.2030 -- 2.2324)  data: 0.1912 (0.0007 -- 2.0095)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4300 (0.5979)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2259 (0.1696 -- 0.4487)  data: 0.0183 (0.0001 -- 0.2515)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4898 (0.6587)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (96.6805)  time: 0.2091 (0.1333 -- 0.4487)  data: 0.0180 (0.0001 -- 0.2515)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 84.440 Acc@5 97.303 loss 0.635
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 84.85%
Epoch: [167]  [  0/160]  eta: 0:21:14  lr: 0.000002  min_lr: 0.000000  loss: 2.1042 (2.1042)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0450 (7.0450)  time: 7.9677 (7.9677 -- 7.9677)  data: 7.1591 (7.1591 -- 7.1591)  max mem: 16413
Epoch: [167]  [ 20/160]  eta: 0:02:52  lr: 0.000002  min_lr: 0.000000  loss: 1.7234 (1.7588)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8483 (8.7685)  time: 0.8933 (0.5286 -- 3.4919)  data: 0.3441 (0.0004 -- 2.9491)  max mem: 16413
[2023-08-30 00:33:28,951] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:33:28,951] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:33:28,951] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:33:28,951] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [167]  [ 40/160]  eta: 0:01:59  lr: 0.000002  min_lr: 0.000000  loss: 1.6427 (1.7041)  loss_scale: 32768.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0565 (8.4848)  time: 0.7473 (0.5138 -- 2.8452)  data: 0.2059 (0.0001 -- 2.3148)  max mem: 16413
[2023-08-30 00:33:47,316] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26771
[2023-08-30 00:33:47,316] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26771
[2023-08-30 00:33:47,317] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:33:47,317] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:33:47,317] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [167]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7398 (1.7246)  loss_scale: 16384.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8303 (8.6649)  time: 0.9254 (0.5182 -- 2.6139)  data: 0.3790 (0.0006 -- 2.0825)  max mem: 16413
Epoch: [167]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.8144 (1.7254)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4529 (8.7360)  time: 0.8794 (0.5257 -- 3.6514)  data: 0.3258 (0.0002 -- 3.0908)  max mem: 16413
Epoch: [167]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.9131 (1.7487)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2990 (8.8476)  time: 0.9346 (0.5309 -- 3.9836)  data: 0.3851 (0.0005 -- 3.4663)  max mem: 16413
Epoch: [167]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9280 (1.7524)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5031 (8.8851)  time: 0.8616 (0.5301 -- 2.3405)  data: 0.3122 (0.0004 -- 1.8130)  max mem: 16413
Epoch: [167]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7472 (1.7608)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5406 (8.9565)  time: 0.8109 (0.5278 -- 2.9955)  data: 0.2622 (0.0003 -- 2.4625)  max mem: 16413
Epoch: [167]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7650 (1.7630)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5738 (8.9637)  time: 0.7224 (0.4961 -- 4.1771)  data: 0.2052 (0.0002 -- 3.6629)  max mem: 16413
Epoch: [167] Total time: 0:02:22 (0.8908 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7650 (1.7595)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5738 (8.9637)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2413 (0.2413)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4458 (2.4458 -- 2.4458)  data: 2.1833 (2.1833 -- 2.1833)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4234 (0.6721)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4242 (0.2021 -- 2.4458)  data: 0.2085 (0.0008 -- 2.1833)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4354 (0.5992)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2213 (0.1691 -- 0.4215)  data: 0.0177 (0.0001 -- 0.2407)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4934 (0.6594)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2062 (0.1328 -- 0.4215)  data: 0.0173 (0.0001 -- 0.2407)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.636
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 84.85%
Epoch: [168]  [  0/160]  eta: 0:22:20  lr: 0.000002  min_lr: 0.000000  loss: 2.0850 (2.0850)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3769 (7.3769)  time: 8.3772 (8.3772 -- 8.3772)  data: 5.7954 (5.7954 -- 5.7954)  max mem: 16413
[2023-08-30 00:35:52,433] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:35:52,434] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:35:52,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:35:52,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [168]  [ 20/160]  eta: 0:02:50  lr: 0.000002  min_lr: 0.000000  loss: 1.7423 (1.7426)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0546 (8.7382)  time: 0.8570 (0.5231 -- 3.7759)  data: 0.1934 (0.0004 -- 2.9493)  max mem: 16413
[2023-08-30 00:35:59,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26909
[2023-08-30 00:35:59,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26909
[2023-08-30 00:35:59,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:35:59,605] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 00:35:59,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [168]  [ 40/160]  eta: 0:01:59  lr: 0.000002  min_lr: 0.000000  loss: 1.6705 (1.7672)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8150 (8.6550)  time: 0.7647 (0.5438 -- 2.6244)  data: 0.0020 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [168]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8666 (1.7748)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7876 (8.8181)  time: 0.9112 (0.5256 -- 3.7956)  data: 0.0015 (0.0006 -- 0.0026)  max mem: 16413
Epoch: [168]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000000  loss: 1.7613 (1.7737)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9039 (8.7254)  time: 0.9589 (0.5301 -- 3.5308)  data: 0.0020 (0.0004 -- 0.0146)  max mem: 16413
Epoch: [168]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.6494 (1.7645)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2647 (8.7858)  time: 0.7865 (0.5369 -- 3.0097)  data: 0.0013 (0.0003 -- 0.0024)  max mem: 16413
[2023-08-30 00:37:14,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=159, lr=[4.9426459286668474e-08, 4.9426459286668474e-08, 6.590194571555796e-08, 6.590194571555796e-08, 8.786926095407729e-08, 8.786926095407729e-08, 1.1715901460543638e-07, 1.1715901460543638e-07, 1.5621201947391518e-07, 1.5621201947391518e-07, 2.082826926318869e-07, 2.082826926318869e-07, 2.7771025684251587e-07, 2.7771025684251587e-07, 3.7028034245668785e-07, 3.7028034245668785e-07, 4.937071232755838e-07, 4.937071232755838e-07, 6.582761643674451e-07, 6.582761643674451e-07, 8.777015524899267e-07, 8.777015524899267e-07, 1.1702687366532355e-06, 1.1702687366532355e-06, 1.5603583155376474e-06, 1.5603583155376474e-06, 2.0804777540501967e-06, 2.0804777540501967e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 00:37:14,669] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=17.782822768617677, CurrSamplesPerSec=21.28684919270798, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [168]  [120/160]  eta: 0:00:35  lr: 0.000002  min_lr: 0.000000  loss: 1.6901 (1.7615)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2795 (8.9498)  time: 0.7222 (0.5352 -- 2.5763)  data: 0.0281 (0.0006 -- 0.5261)  max mem: 16413
Epoch: [168]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5675 (1.7399)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6869 (8.9125)  time: 0.9647 (0.5375 -- 4.2340)  data: 0.2640 (0.0004 -- 3.6937)  max mem: 16413
[2023-08-30 00:37:46,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:37:46,863] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:37:46,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:37:46,863] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [168]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8529 (1.7442)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9832 (8.8133)  time: 0.6628 (0.5016 -- 3.1419)  data: 0.1319 (0.0002 -- 2.6160)  max mem: 16413
Epoch: [168] Total time: 0:02:20 (0.8778 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8529 (1.7293)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9832 (8.8133)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2423 (0.2423)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4222 (2.4222 -- 2.4222)  data: 2.1612 (2.1612 -- 2.1612)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4214 (0.6726)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4232 (0.2090 -- 2.4222)  data: 0.1974 (0.0006 -- 2.1612)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4433 (0.6008)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2191 (0.1701 -- 0.3581)  data: 0.0093 (0.0001 -- 0.1731)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4952 (0.6617)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (95.8506)  time: 0.2018 (0.1330 -- 0.3581)  data: 0.0090 (0.0001 -- 0.1731)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 84.855 Acc@5 96.888 loss 0.637
Accuracy of the network on the 482 val images: 84.85%
[2023-08-30 00:37:55,180] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 00:37:55,182] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 00:37:55,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 00:37:55,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 00:37:56,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 00:37:56,423] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.85%
Epoch: [169]  [  0/160]  eta: 0:21:58  lr: 0.000002  min_lr: 0.000000  loss: 1.6386 (1.6386)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9772 (7.9772)  time: 8.2436 (8.2436 -- 8.2436)  data: 7.6799 (7.6799 -- 7.6799)  max mem: 16413
Epoch: [169]  [ 20/160]  eta: 0:02:40  lr: 0.000002  min_lr: 0.000000  loss: 1.8153 (1.8190)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3332 (8.1818)  time: 0.7882 (0.5235 -- 3.7463)  data: 0.2372 (0.0003 -- 3.2148)  max mem: 16413
Epoch: [169]  [ 40/160]  eta: 0:02:05  lr: 0.000002  min_lr: 0.000000  loss: 1.6240 (1.7055)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0960 (7.8624)  time: 0.9509 (0.5345 -- 3.6691)  data: 0.4001 (0.0008 -- 3.1327)  max mem: 16413
Epoch: [169]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9437 (1.7620)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9366 (8.0904)  time: 0.8181 (0.5247 -- 2.3127)  data: 0.2690 (0.0004 -- 1.7621)  max mem: 16413
[2023-08-30 00:38:58,657] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27104
[2023-08-30 00:38:58,657] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:38:58,657] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27104
[2023-08-30 00:38:58,657] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:38:58,657] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 00:39:03,688] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27108
[2023-08-30 00:39:03,689] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:39:03,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 00:39:03,697] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27108
[2023-08-30 00:39:03,698] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:39:13,104] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27117
[2023-08-30 00:39:13,104] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27117
[2023-08-30 00:39:13,104] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 00:39:13,104] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 00:39:13,104] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [169]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000000  loss: 1.7187 (1.7698)  loss_scale: 8192.0000 (27812.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5229 (8.2473)  time: 0.9425 (0.5048 -- 4.8027)  data: 0.4061 (0.0004 -- 4.2624)  max mem: 16413
Epoch: [169]  [100/160]  eta: 0:00:57  lr: 0.000002  min_lr: 0.000000  loss: 1.7384 (1.7708)  loss_scale: 4096.0000 (23116.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9106 (8.3777)  time: 0.8874 (0.5316 -- 3.8391)  data: 0.3322 (0.0003 -- 3.3177)  max mem: 16413
Epoch: [169]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8297 (1.7827)  loss_scale: 4096.0000 (19972.2314)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1808 (8.4874)  time: 0.6611 (0.5260 -- 1.9640)  data: 0.1077 (0.0002 -- 1.3895)  max mem: 16413
Epoch: [169]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7175 (1.7784)  loss_scale: 4096.0000 (17720.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6379 (8.4338)  time: 1.0497 (0.5313 -- 5.2278)  data: 0.5011 (0.0004 -- 4.7023)  max mem: 16413
Epoch: [169]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7578 (1.7803)  loss_scale: 4096.0000 (16102.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1736 (8.4459)  time: 0.6380 (0.4961 -- 2.8709)  data: 0.1176 (0.0002 -- 2.3374)  max mem: 16413
Epoch: [169] Total time: 0:02:22 (0.8906 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7578 (1.7522)  loss_scale: 4096.0000 (16102.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1736 (8.4459)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2338 (0.2338)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4246 (2.4246 -- 2.4246)  data: 2.1818 (2.1818 -- 2.1818)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4281 (0.6727)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4191 (0.1991 -- 2.4246)  data: 0.2007 (0.0004 -- 2.1818)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4372 (0.5996)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2173 (0.1684 -- 0.3268)  data: 0.0080 (0.0001 -- 0.1312)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4928 (0.6616)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.1998 (0.1322 -- 0.3268)  data: 0.0070 (0.0001 -- 0.1312)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.637
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 84.85%
Epoch: [170]  [  0/160]  eta: 0:17:52  lr: 0.000002  min_lr: 0.000000  loss: 1.9638 (1.9638)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7326 (11.7326)  time: 6.7049 (6.7049 -- 6.7049)  data: 4.9903 (4.9903 -- 4.9903)  max mem: 16413
Epoch: [170]  [ 20/160]  eta: 0:02:32  lr: 0.000002  min_lr: 0.000000  loss: 1.6291 (1.6654)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8510 (9.4386)  time: 0.8116 (0.5307 -- 3.0636)  data: 0.2030 (0.0004 -- 2.0555)  max mem: 16413
Epoch: [170]  [ 40/160]  eta: 0:02:02  lr: 0.000002  min_lr: 0.000000  loss: 1.5220 (1.6003)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7891 (8.9604)  time: 0.9375 (0.5313 -- 2.6554)  data: 0.0069 (0.0002 -- 0.1013)  max mem: 16413
[2023-08-30 00:41:12,186] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:41:12,186] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-30 00:41:12,187] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:41:12,187] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [170]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9087 (1.7092)  loss_scale: 8192.0000 (5103.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0851 (8.8491)  time: 0.8775 (0.5266 -- 2.8692)  data: 0.0414 (0.0004 -- 0.8026)  max mem: 16413
Epoch: [170]  [ 80/160]  eta: 0:01:13  lr: 0.000002  min_lr: 0.000000  loss: 1.8875 (1.7410)  loss_scale: 8192.0000 (5865.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8868 (8.9952)  time: 0.7769 (0.5248 -- 2.8074)  data: 0.0014 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [170]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.7614 (1.7443)  loss_scale: 8192.0000 (6326.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3988 (9.0973)  time: 0.9402 (0.5278 -- 3.3366)  data: 0.0259 (0.0006 -- 0.4847)  max mem: 16413
Epoch: [170]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7869 (1.7412)  loss_scale: 8192.0000 (6634.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5239 (9.3253)  time: 0.7751 (0.5211 -- 2.6530)  data: 0.0013 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [170]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7350 (1.7413)  loss_scale: 8192.0000 (6855.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5954 (9.3561)  time: 0.9035 (0.5193 -- 2.9623)  data: 0.0019 (0.0003 -- 0.0096)  max mem: 16413
Epoch: [170]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6677 (1.7371)  loss_scale: 8192.0000 (7014.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0961 (9.2341)  time: 0.6897 (0.4974 -- 2.2685)  data: 0.0106 (0.0003 -- 0.1902)  max mem: 16413
Epoch: [170] Total time: 0:02:19 (0.8721 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6677 (1.7402)  loss_scale: 8192.0000 (7014.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0961 (9.2341)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2341 (0.2341)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4327 (2.4327 -- 2.4327)  data: 2.1758 (2.1758 -- 2.1758)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4218 (0.6711)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4211 (0.1952 -- 2.4327)  data: 0.2050 (0.0007 -- 2.1758)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4294 (0.5973)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2163 (0.1695 -- 0.3000)  data: 0.0108 (0.0001 -- 0.1022)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4924 (0.6592)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2013 (0.1329 -- 0.3000)  data: 0.0106 (0.0001 -- 0.1022)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 84.647 Acc@5 97.510 loss 0.636
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 84.85%
Epoch: [171]  [  0/160]  eta: 0:26:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6998 (1.6998)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3124 (8.3124)  time: 9.8661 (9.8661 -- 9.8661)  data: 9.3425 (9.3425 -- 9.3425)  max mem: 16413
[2023-08-30 00:43:12,590] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:43:12,590] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:43:12,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 00:43:12,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [171]  [ 20/160]  eta: 0:02:51  lr: 0.000002  min_lr: 0.000000  loss: 1.6781 (1.6831)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2197 (8.6386)  time: 0.7910 (0.5256 -- 4.5623)  data: 0.2493 (0.0007 -- 4.0450)  max mem: 16413
Epoch: [171]  [ 40/160]  eta: 0:01:58  lr: 0.000002  min_lr: 0.000000  loss: 1.9134 (1.7517)  loss_scale: 16384.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1304 (8.5916)  time: 0.7475 (0.5356 -- 2.2786)  data: 0.1715 (0.0002 -- 1.7410)  max mem: 16413
Epoch: [171]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.5781 (1.7118)  loss_scale: 16384.0000 (14503.8689)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0308 (8.8096)  time: 0.9115 (0.5315 -- 3.3873)  data: 0.2094 (0.0006 -- 2.6166)  max mem: 16413
Epoch: [171]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.5769 (1.7078)  loss_scale: 16384.0000 (14968.0988)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2765 (8.7448)  time: 0.9125 (0.5299 -- 3.0084)  data: 0.3638 (0.0002 -- 2.4644)  max mem: 16413
Epoch: [171]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.6190 (1.7187)  loss_scale: 16384.0000 (15248.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0912 (8.7815)  time: 0.9057 (0.5077 -- 3.4822)  data: 0.3650 (0.0002 -- 2.9457)  max mem: 16413
Epoch: [171]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7384 (1.7278)  loss_scale: 16384.0000 (15436.1653)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5084 (8.8782)  time: 0.8873 (0.5335 -- 3.9061)  data: 0.3321 (0.0007 -- 3.3552)  max mem: 16413
Epoch: [171]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6118 (1.7199)  loss_scale: 16384.0000 (15570.6099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8168 (8.8932)  time: 0.8001 (0.5319 -- 2.9778)  data: 0.2533 (0.0003 -- 2.4492)  max mem: 16413
[2023-08-30 00:45:04,292] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:45:04,292] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:45:04,292] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:45:04,292] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 00:45:10,508] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27509
[2023-08-30 00:45:10,509] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 00:45:10,509] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 00:45:10,509] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27509
[2023-08-30 00:45:10,509] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [171]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7571 (1.7217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9637 (8.9539)  time: 0.6795 (0.4971 -- 2.9474)  data: 0.1551 (0.0001 -- 2.4350)  max mem: 16413
Epoch: [171] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7571 (1.7404)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9637 (8.9539)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2299 (0.2299)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4134 (2.4134 -- 2.4134)  data: 2.1455 (2.1455 -- 2.1455)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4317 (0.6706)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4162 (0.1975 -- 2.4134)  data: 0.1969 (0.0005 -- 2.1455)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4317 (0.5974)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2185 (0.1699 -- 0.4630)  data: 0.0142 (0.0001 -- 0.2576)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4917 (0.6593)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2003 (0.1332 -- 0.4630)  data: 0.0133 (0.0001 -- 0.2576)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.634
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.85%
Epoch: [172]  [  0/160]  eta: 0:22:58  lr: 0.000002  min_lr: 0.000000  loss: 2.0691 (2.0691)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1067 (11.1067)  time: 8.6156 (8.6156 -- 8.6156)  data: 7.8296 (7.8296 -- 7.8296)  max mem: 16413
Epoch: [172]  [ 20/160]  eta: 0:02:56  lr: 0.000002  min_lr: 0.000000  loss: 1.8073 (1.8229)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2899 (10.1889)  time: 0.8899 (0.5376 -- 4.1970)  data: 0.1093 (0.0002 -- 1.5716)  max mem: 16413
Epoch: [172]  [ 40/160]  eta: 0:02:14  lr: 0.000002  min_lr: 0.000000  loss: 1.7585 (1.7629)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6124 (9.4536)  time: 0.9728 (0.5217 -- 4.9927)  data: 0.0012 (0.0002 -- 0.0075)  max mem: 16413
Epoch: [172]  [ 60/160]  eta: 0:01:41  lr: 0.000002  min_lr: 0.000000  loss: 1.7711 (1.7703)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1534 (9.2043)  time: 0.7971 (0.5282 -- 3.4110)  data: 0.0015 (0.0005 -- 0.0024)  max mem: 16413
Epoch: [172]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.9397 (1.7863)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8599 (9.4287)  time: 0.7490 (0.5273 -- 2.2706)  data: 0.1127 (0.0002 -- 1.7469)  max mem: 16413
Epoch: [172]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.6816 (1.7860)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6973 (9.2694)  time: 0.9205 (0.5144 -- 2.0187)  data: 0.3747 (0.0004 -- 1.4575)  max mem: 16413
[2023-08-30 00:47:15,804] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27638
[2023-08-30 00:47:15,804] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27638
[2023-08-30 00:47:15,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:47:15,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 00:47:15,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [172]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9763 (1.8170)  loss_scale: 16384.0000 (16180.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6318 (9.4179)  time: 0.8818 (0.5261 -- 4.4905)  data: 0.3325 (0.0003 -- 3.9697)  max mem: 16413
Epoch: [172]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.4288 (1.7886)  loss_scale: 8192.0000 (15047.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3265 (9.5195)  time: 0.9199 (0.5358 -- 3.2718)  data: 0.3650 (0.0003 -- 2.7222)  max mem: 16413
Epoch: [172]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7534 (1.7828)  loss_scale: 8192.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8598 (9.5049)  time: 0.6519 (0.4957 -- 1.7691)  data: 0.1301 (0.0001 -- 1.2727)  max mem: 16413
Epoch: [172] Total time: 0:02:23 (0.8982 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7534 (1.7474)  loss_scale: 8192.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8598 (9.5049)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2328 (0.2328)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4403 (2.4403 -- 2.4403)  data: 2.2028 (2.2028 -- 2.2028)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4352 (0.6693)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4206 (0.2010 -- 2.4403)  data: 0.2013 (0.0007 -- 2.2028)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4352 (0.5956)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2161 (0.1714 -- 0.3642)  data: 0.0093 (0.0001 -- 0.1712)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4911 (0.6576)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2002 (0.1325 -- 0.3642)  data: 0.0090 (0.0001 -- 0.1712)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.634
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 84.85%
Epoch: [173]  [  0/160]  eta: 0:21:02  lr: 0.000002  min_lr: 0.000000  loss: 1.9558 (1.9558)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5445 (7.5445)  time: 7.8886 (7.8886 -- 7.8886)  data: 5.7670 (5.7670 -- 5.7670)  max mem: 16413
Epoch: [173]  [ 20/160]  eta: 0:02:53  lr: 0.000002  min_lr: 0.000000  loss: 1.9386 (1.8732)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0932 (8.4295)  time: 0.9106 (0.5226 -- 3.1483)  data: 0.0023 (0.0004 -- 0.0076)  max mem: 16413
Epoch: [173]  [ 40/160]  eta: 0:02:08  lr: 0.000002  min_lr: 0.000000  loss: 1.7890 (1.8229)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7251 (8.7534)  time: 0.8836 (0.5270 -- 3.5114)  data: 0.0021 (0.0004 -- 0.0143)  max mem: 16413
Epoch: [173]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.4154 (1.7560)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5650 (8.7608)  time: 0.8155 (0.5304 -- 2.9455)  data: 0.0019 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [173]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.6153 (1.7373)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9880 (8.4719)  time: 0.8534 (0.5294 -- 3.5328)  data: 0.0103 (0.0004 -- 0.1194)  max mem: 16413
[2023-08-30 00:49:17,013] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:49:17,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 00:49:17,013] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 00:49:17,014] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [173]  [100/160]  eta: 0:00:57  lr: 0.000002  min_lr: 0.000000  loss: 1.6899 (1.7234)  loss_scale: 16384.0000 (9327.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8518 (8.4825)  time: 0.9833 (0.5123 -- 4.8860)  data: 0.0014 (0.0004 -- 0.0068)  max mem: 16413
Epoch: [173]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8266 (1.7256)  loss_scale: 16384.0000 (10493.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5492 (8.6381)  time: 0.7640 (0.5380 -- 3.4673)  data: 0.0018 (0.0004 -- 0.0032)  max mem: 16413
configs/vit_b_k710.sh: line 43: 608394 Killed                  OMP_NUM_THREADS=1 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node=${GPUS_PER_NODE} --master_port 12320 --nnodes=1 --node_rank=0 --master_addr=localhost run_class_finetuning.py --model vit_base_patch16_224 --data_set AI-City-Track-3 --nb_classes 16 --data_path ${DATA_PATH} --finetune ${MODEL_PATH} --log_dir ${OUTPUT_DIR} --output_dir ${OUTPUT_DIR} --batch_size 6 --input_size 224 --short_side_size 224 --save_ckpt_freq 20 --num_frames 16 --sampling_rate 4 --num_sample 2 --num_workers 8 --opt adamw --lr 7e-4 --drop_path 0.1 --head_drop_rate 0.0 --layer_decay 0.75 --opt_betas 0.9 0.999 --warmup_epochs 5 --epochs 200 --test_num_segment 5 --test_num_crop 3 --dist_eval --enable_deepspeed
