[2023-10-23 16:32:17,409] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-23 16:32:17,460] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast/annotations/Dashboard', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, img_diff_json_path=None, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sample_mode='normal', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=10, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f4487287f10>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
[2023-10-23 16:32:22,744] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-10-23 16:32:22,744] [INFO] [comm.py:631:init_distributed] cdb=None
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 648
Number of training training per epoch = 54
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-10-23 16:32:22,802] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-10-23 16:32:22,803] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-10-23 16:32:22,910] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.4172074794769287 seconds
[2023-10-23 16:32:24,028] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-10-23 16:32:24,038] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-10-23 16:32:24,038] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-10-23 16:32:24,063] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-10-23 16:32:24,064] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-23 16:32:24,064] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-10-23 16:32:24,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 16:32:24,065] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-10-23 16:32:24,066] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-23 16:32:24,066] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-23 16:32:24,066] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-10-23 16:32:24,066] [INFO] [config.py:964:print]   amp_params ................... False
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f438a867b20>
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-10-23 16:32:24,067] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   dump_state ................... False
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-10-23 16:32:24,068] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-10-23 16:32:24,069] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   pld_params ................... False
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   world_size ................... 2
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-10-23 16:32:24,070] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-23 16:32:24,071] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-10-23 16:32:24,071] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 270
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [ 0/54]  eta: 0:11:27  lr: 0.000000  min_lr: 0.000000  loss: 2.7734 (2.7734)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 12.7397 (12.7398 -- 12.7398)  data: 6.4884 (6.4884 -- 6.4884)  max mem: 16413
Epoch: [0]  [20/54]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000000  loss: 2.7728 (2.7728)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4462 (1.5050)  time: 0.5713 (0.4975 -- 1.2336)  data: 0.0016 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [0]  [40/54]  eta: 0:00:15  lr: 0.000007  min_lr: 0.000000  loss: 2.7726 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3325 (1.4477)  time: 1.0444 (0.5201 -- 4.5884)  data: 0.0934 (0.0005 -- 1.1785)  max mem: 16413
Epoch: [0]  [53/54]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.7721 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4323 (1.4794)  time: 0.6762 (0.4793 -- 3.9455)  data: 0.0330 (0.0002 -- 0.6433)  max mem: 16413
Epoch: [0] Total time: 0:00:51 (0.9518 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.7721 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4323 (1.4794)
Val:  [ 0/10]  eta: 0:00:26  loss: 2.7697 (2.7697)  acc1: 22.2222 (22.2222)  acc5: 66.6667 (66.6667)  time: 2.6810 (2.6810 -- 2.6810)  data: 2.4023 (2.4023 -- 2.4023)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.7697 (2.7699)  acc1: 44.4444 (42.6829)  acc5: 66.6667 (68.2927)  time: 0.4098 (0.0409 -- 2.6810)  data: 0.2403 (0.0001 -- 2.4023)  max mem: 16413
Val: Total time: 0:00:04 (0.4099 s / it)
* Acc@1 37.805 Acc@5 64.634 loss 2.770
Accuracy of the network on the 163 val images: 37.80%
[2023-10-23 16:33:19,634] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-10-23 16:33:19,638] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:33:19,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:33:19,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:33:20,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:33:20,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.80%
Epoch: [1]  [ 0/54]  eta: 0:06:54  lr: 0.000009  min_lr: 0.000000  loss: 2.7709 (2.7709)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5293 (1.5293)  time: 7.6786 (7.6786 -- 7.6786)  data: 7.1594 (7.1594 -- 7.1594)  max mem: 16413
Epoch: [1]  [20/54]  eta: 0:00:40  lr: 0.000013  min_lr: 0.000000  loss: 2.7711 (2.7711)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4207 (1.4404)  time: 0.8543 (0.5122 -- 3.1134)  data: 0.3114 (0.0005 -- 2.6004)  max mem: 16413
Epoch: [1]  [40/54]  eta: 0:00:13  lr: 0.000016  min_lr: 0.000000  loss: 2.7694 (2.7702)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4697 (1.4853)  time: 0.7757 (0.5118 -- 2.1354)  data: 0.2460 (0.0003 -- 1.5908)  max mem: 16413
Epoch: [1]  [53/54]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.7681 (2.7698)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4441 (1.4849)  time: 0.6816 (0.4881 -- 1.6267)  data: 0.1396 (0.0002 -- 1.1048)  max mem: 16413
Epoch: [1] Total time: 0:00:49 (0.9097 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.7681 (2.7698)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4441 (1.4849)
Val:  [ 0/10]  eta: 0:00:21  loss: 2.7572 (2.7572)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 2.1048 (2.1048 -- 2.1048)  data: 1.9377 (1.9377 -- 1.9377)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.7550 (2.7548)  acc1: 44.4444 (47.5610)  acc5: 88.8889 (89.0244)  time: 0.3480 (0.0223 -- 2.1048)  data: 0.1939 (0.0001 -- 1.9377)  max mem: 16413
Val: Total time: 0:00:03 (0.3481 s / it)
* Acc@1 41.463 Acc@5 84.756 loss 2.757
Accuracy of the network on the 163 val images: 41.46%
[2023-10-23 16:34:13,173] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:34:13,175] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:34:13,175] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:34:13,175] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:34:14,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:34:14,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.46%
Epoch: [2]  [ 0/54]  eta: 0:08:44  lr: 0.000019  min_lr: 0.000000  loss: 2.7692 (2.7692)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6142 (1.6142)  time: 9.7204 (9.7204 -- 9.7204)  data: 6.9137 (6.9137 -- 6.9137)  max mem: 16413
[2023-10-23 16:34:39,602] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:34:39,602] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-10-23 16:34:39,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:34:39,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [2]  [20/54]  eta: 0:00:40  lr: 0.000022  min_lr: 0.000001  loss: 2.7615 (2.7623)  loss_scale: 128.0000 (134.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5912 (1.5983)  time: 0.7675 (0.5161 -- 3.1112)  data: 0.0304 (0.0005 -- 0.5844)  max mem: 16413
Epoch: [2]  [40/54]  eta: 0:00:14  lr: 0.000026  min_lr: 0.000001  loss: 2.7582 (2.7595)  loss_scale: 256.0000 (193.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4833 (1.5388)  time: 0.8532 (0.5182 -- 3.0316)  data: 0.2156 (0.0004 -- 2.4837)  max mem: 16413
Epoch: [2]  [53/54]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 2.7500 (2.7565)  loss_scale: 256.0000 (208.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5037 (1.5414)  time: 0.7367 (0.4878 -- 3.0316)  data: 0.2201 (0.0001 -- 2.4837)  max mem: 16413
Epoch: [2] Total time: 0:00:49 (0.9184 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 2.7500 (2.7557)  loss_scale: 256.0000 (208.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5037 (1.5414)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.7072 (2.7072)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0828 (2.0828 -- 2.0828)  data: 1.9054 (1.9054 -- 1.9054)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.7072 (2.7060)  acc1: 55.5556 (52.4390)  acc5: 100.0000 (90.2439)  time: 0.3457 (0.0219 -- 2.0828)  data: 0.1906 (0.0001 -- 1.9054)  max mem: 16413
Val: Total time: 0:00:03 (0.3458 s / it)
* Acc@1 45.122 Acc@5 90.244 loss 2.712
Accuracy of the network on the 163 val images: 45.12%
[2023-10-23 16:35:07,595] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:35:07,597] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:35:07,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:35:07,597] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:35:08,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:35:08,993] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.12%
Epoch: [3]  [ 0/54]  eta: 0:06:33  lr: 0.000028  min_lr: 0.000001  loss: 2.7161 (2.7161)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7126 (1.7126)  time: 7.2925 (7.2925 -- 7.2925)  data: 6.6985 (6.6985 -- 6.6985)  max mem: 16413
Epoch: [3]  [20/54]  eta: 0:00:38  lr: 0.000032  min_lr: 0.000001  loss: 2.7367 (2.7296)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5460 (1.5760)  time: 0.8378 (0.5158 -- 3.5987)  data: 0.1394 (0.0004 -- 1.3979)  max mem: 16413
Epoch: [3]  [40/54]  eta: 0:00:14  lr: 0.000035  min_lr: 0.000001  loss: 2.7095 (2.7203)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5770 (1.5879)  time: 0.9549 (0.5172 -- 3.2410)  data: 0.1625 (0.0007 -- 2.0243)  max mem: 16413
Epoch: [3]  [53/54]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 2.7021 (2.7120)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6237 (1.6143)  time: 0.6664 (0.4896 -- 2.0016)  data: 0.0112 (0.0001 -- 0.1927)  max mem: 16413
Epoch: [3] Total time: 0:00:51 (0.9496 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 2.7021 (2.7148)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6237 (1.6143)
Val:  [ 0/10]  eta: 0:00:19  loss: 2.6003 (2.6003)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9424 (1.9424 -- 1.9424)  data: 1.7347 (1.7347 -- 1.7347)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.6003 (2.5984)  acc1: 55.5556 (57.3171)  acc5: 88.8889 (90.2439)  time: 0.3310 (0.0222 -- 1.9424)  data: 0.1735 (0.0001 -- 1.7347)  max mem: 16413
Val: Total time: 0:00:03 (0.3311 s / it)
* Acc@1 48.780 Acc@5 91.463 loss 2.612
Accuracy of the network on the 163 val images: 48.78%
[2023-10-23 16:36:03,656] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:36:03,658] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:36:03,658] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:36:03,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:36:05,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:36:05,222] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 48.78%
Epoch: [4]  [ 0/54]  eta: 0:06:55  lr: 0.000038  min_lr: 0.000001  loss: 2.6955 (2.6955)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4282 (1.4282)  time: 7.6865 (7.6865 -- 7.6865)  data: 7.1727 (7.1727 -- 7.1727)  max mem: 16413
Epoch: [4]  [20/54]  eta: 0:00:40  lr: 0.000041  min_lr: 0.000001  loss: 2.6864 (2.6851)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6437 (1.6565)  time: 0.8763 (0.5101 -- 4.3573)  data: 0.3043 (0.0004 -- 3.8246)  max mem: 16413
[2023-10-23 16:36:47,305] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:36:47,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-10-23 16:36:47,307] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:36:47,307] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [4]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 2.6405 (2.6610)  loss_scale: 256.0000 (262.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7459 (1.6977)  time: 0.8436 (0.5188 -- 3.2640)  data: 0.1717 (0.0006 -- 1.3116)  max mem: 16413
Epoch: [4]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.6087 (2.6462)  loss_scale: 512.0000 (322.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7950 (1.7189)  time: 0.6976 (0.4898 -- 1.9540)  data: 0.1394 (0.0001 -- 1.4643)  max mem: 16413
Epoch: [4] Total time: 0:00:51 (0.9451 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.6087 (2.6470)  loss_scale: 512.0000 (322.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7950 (1.7189)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.4685 (2.4685)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.0764 (2.0764 -- 2.0764)  data: 1.9093 (1.9093 -- 1.9093)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.4651 (2.4505)  acc1: 44.4444 (47.5610)  acc5: 88.8889 (90.2439)  time: 0.3477 (0.0220 -- 2.0764)  data: 0.1910 (0.0001 -- 1.9093)  max mem: 16413
Val: Total time: 0:00:03 (0.3478 s / it)
* Acc@1 46.951 Acc@5 90.244 loss 2.474
Accuracy of the network on the 163 val images: 46.95%
Max accuracy: 48.78%
Epoch: [5]  [ 0/54]  eta: 0:05:48  lr: 0.000047  min_lr: 0.000001  loss: 2.6178 (2.6178)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5579 (1.5579)  time: 6.4558 (6.4558 -- 6.4558)  data: 5.8998 (5.8998 -- 5.8998)  max mem: 16413
Epoch: [5]  [20/54]  eta: 0:00:42  lr: 0.000047  min_lr: 0.000001  loss: 2.6057 (2.5954)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8761 (1.8507)  time: 0.9869 (0.5221 -- 5.3604)  data: 0.4399 (0.0007 -- 4.8381)  max mem: 16413
Epoch: [5]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.5424 (2.5748)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9544 (1.9132)  time: 0.8822 (0.5313 -- 3.1689)  data: 0.3401 (0.0003 -- 2.6573)  max mem: 16413
Epoch: [5]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.5642 (2.5745)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9121 (1.9324)  time: 0.6495 (0.4930 -- 3.1689)  data: 0.1335 (0.0001 -- 2.6573)  max mem: 16413
Epoch: [5] Total time: 0:00:50 (0.9336 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.5642 (2.5646)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9121 (1.9324)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.2975 (2.2975)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0887 (2.0887 -- 2.0887)  data: 1.9156 (1.9156 -- 1.9156)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.2975 (2.2882)  acc1: 44.4444 (50.0000)  acc5: 100.0000 (92.6829)  time: 0.3463 (0.0219 -- 2.0887)  data: 0.1916 (0.0001 -- 1.9156)  max mem: 16413
Val: Total time: 0:00:03 (0.3464 s / it)
* Acc@1 50.000 Acc@5 92.073 loss 2.307
Accuracy of the network on the 163 val images: 50.00%
[2023-10-23 16:37:53,626] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:37:53,628] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:37:53,628] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:37:53,628] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:37:55,057] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:37:55,057] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.00%
Epoch: [6]  [ 0/54]  eta: 0:05:29  lr: 0.000047  min_lr: 0.000001  loss: 2.6231 (2.6231)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2076 (2.2076)  time: 6.1007 (6.1007 -- 6.1007)  data: 5.5711 (5.5711 -- 5.5711)  max mem: 16413
Epoch: [6]  [20/54]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000001  loss: 2.5098 (2.5272)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1454 (2.1176)  time: 0.8773 (0.5256 -- 2.2611)  data: 0.3362 (0.0004 -- 1.7279)  max mem: 16413
Epoch: [6]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.4489 (2.4965)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1744 (2.1604)  time: 0.9184 (0.5122 -- 5.3516)  data: 0.3800 (0.0003 -- 4.8238)  max mem: 16413
Epoch: [6]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4460 (2.4970)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0467 (2.1725)  time: 0.7028 (0.4917 -- 2.8024)  data: 0.1918 (0.0001 -- 2.2972)  max mem: 16413
Epoch: [6] Total time: 0:00:52 (0.9698 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4460 (2.4816)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0467 (2.1725)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.1506 (2.1506)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0490 (2.0490 -- 2.0490)  data: 1.8761 (1.8761 -- 1.8761)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.1506 (2.1513)  acc1: 44.4444 (50.0000)  acc5: 100.0000 (91.4634)  time: 0.3426 (0.0222 -- 2.0490)  data: 0.1877 (0.0001 -- 1.8761)  max mem: 16413
Val: Total time: 0:00:03 (0.3427 s / it)
* Acc@1 51.220 Acc@5 93.293 loss 2.165
Accuracy of the network on the 163 val images: 51.22%
[2023-10-23 16:38:50,887] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:38:50,889] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:38:50,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:38:50,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:38:52,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:38:52,316] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 51.22%
Epoch: [7]  [ 0/54]  eta: 0:06:35  lr: 0.000047  min_lr: 0.000001  loss: 2.4211 (2.4211)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5218 (2.5218)  time: 7.3249 (7.3249 -- 7.3249)  data: 5.8017 (5.8017 -- 5.8017)  max mem: 16413
[2023-10-23 16:39:03,013] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:39:03,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-10-23 16:39:03,016] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:39:03,017] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [7]  [20/54]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000001  loss: 2.3935 (2.4200)  loss_scale: 1024.0000 (877.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3027 (2.3025)  time: 0.8740 (0.5250 -- 3.2301)  data: 0.2568 (0.0003 -- 2.6983)  max mem: 16413
Epoch: [7]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3447 (2.3947)  loss_scale: 1024.0000 (949.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3573 (2.3873)  time: 0.8566 (0.5188 -- 2.7068)  data: 0.3192 (0.0002 -- 2.1850)  max mem: 16413
Epoch: [7]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4201 (2.4012)  loss_scale: 1024.0000 (967.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4159 (2.4510)  time: 0.6988 (0.4916 -- 2.5987)  data: 0.1687 (0.0002 -- 2.0732)  max mem: 16413
Epoch: [7] Total time: 0:00:50 (0.9281 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4201 (2.4160)  loss_scale: 1024.0000 (967.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4159 (2.4510)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.0617 (2.0617)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0539 (2.0539 -- 2.0539)  data: 1.8821 (1.8821 -- 1.8821)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.0617 (2.0444)  acc1: 55.5556 (51.2195)  acc5: 88.8889 (91.4634)  time: 0.3421 (0.0220 -- 2.0539)  data: 0.1883 (0.0001 -- 1.8821)  max mem: 16413
Val: Total time: 0:00:03 (0.3422 s / it)
* Acc@1 47.561 Acc@5 93.902 loss 2.066
Accuracy of the network on the 163 val images: 47.56%
Max accuracy: 51.22%
Epoch: [8]  [ 0/54]  eta: 0:07:37  lr: 0.000047  min_lr: 0.000001  loss: 2.5834 (2.5834)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2798 (3.2798)  time: 8.4811 (8.4811 -- 8.4811)  data: 7.4060 (7.4060 -- 7.4060)  max mem: 16413
Epoch: [8]  [20/54]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000001  loss: 2.4007 (2.4020)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0062 (3.1001)  time: 0.8024 (0.5265 -- 2.5668)  data: 0.2268 (0.0003 -- 2.0133)  max mem: 16413
Epoch: [8]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3346 (2.3727)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8782 (3.1298)  time: 0.8535 (0.5181 -- 3.0281)  data: 0.3159 (0.0004 -- 2.4882)  max mem: 16413
Epoch: [8]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.2925 (2.3686)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9331 (3.1111)  time: 0.7187 (0.4936 -- 3.0281)  data: 0.1975 (0.0001 -- 2.4882)  max mem: 16413
Epoch: [8] Total time: 0:00:49 (0.9142 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.2925 (2.3587)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9331 (3.1111)
Val:  [ 0/10]  eta: 0:00:21  loss: 1.9362 (1.9362)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.1290 (2.1290 -- 2.1290)  data: 1.9449 (1.9449 -- 1.9449)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.8911 (1.9032)  acc1: 55.5556 (56.0976)  acc5: 100.0000 (93.9024)  time: 0.3522 (0.0221 -- 2.1290)  data: 0.1946 (0.0001 -- 1.9449)  max mem: 16413
Val: Total time: 0:00:03 (0.3523 s / it)
* Acc@1 51.829 Acc@5 95.732 loss 1.914
Accuracy of the network on the 163 val images: 51.83%
[2023-10-23 16:40:38,758] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:40:38,760] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:40:38,760] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:40:38,760] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:40:40,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:40:40,180] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 51.83%
Epoch: [9]  [ 0/54]  eta: 0:07:15  lr: 0.000047  min_lr: 0.000001  loss: 2.4274 (2.4274)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6546 (2.6546)  time: 8.0560 (8.0560 -- 8.0560)  data: 7.5445 (7.5445 -- 7.5445)  max mem: 16413
Epoch: [9]  [20/54]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000001  loss: 2.2804 (2.3337)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3647 (3.6744)  time: 0.7836 (0.5135 -- 3.7774)  data: 0.1990 (0.0004 -- 3.2520)  max mem: 16413
[2023-10-23 16:41:09,795] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:41:09,796] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-10-23 16:41:09,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:41:09,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [9]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3056 (2.3101)  loss_scale: 2048.0000 (1398.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1250 (3.5291)  time: 0.9234 (0.5219 -- 3.3264)  data: 0.3467 (0.0003 -- 2.6242)  max mem: 16413
Epoch: [9]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.2794 (2.3075)  loss_scale: 2048.0000 (1554.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6868 (3.8086)  time: 0.6929 (0.4923 -- 2.6945)  data: 0.1700 (0.0002 -- 2.1774)  max mem: 16413
Epoch: [9] Total time: 0:00:50 (0.9282 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.2794 (2.3255)  loss_scale: 2048.0000 (1554.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6868 (3.8086)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.8661 (1.8661)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0615 (2.0615 -- 2.0615)  data: 1.8784 (1.8784 -- 1.8784)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.7969 (1.8201)  acc1: 44.4444 (50.0000)  acc5: 100.0000 (92.6829)  time: 0.3432 (0.0222 -- 2.0615)  data: 0.1879 (0.0001 -- 1.8784)  max mem: 16413
Val: Total time: 0:00:03 (0.3433 s / it)
* Acc@1 49.390 Acc@5 95.122 loss 1.833
Accuracy of the network on the 163 val images: 49.39%
Max accuracy: 51.83%
Epoch: [10]  [ 0/54]  eta: 0:06:35  lr: 0.000047  min_lr: 0.000001  loss: 2.3178 (2.3178)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8636 (2.8636)  time: 7.3311 (7.3311 -- 7.3311)  data: 5.4281 (5.4281 -- 5.4281)  max mem: 16413
Epoch: [10]  [20/54]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000001  loss: 2.2158 (2.2457)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8227 (4.5684)  time: 0.8420 (0.5194 -- 2.1268)  data: 0.1909 (0.0003 -- 1.5068)  max mem: 16413
Epoch: [10]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3149 (2.2617)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1548 (4.1708)  time: 0.8905 (0.5068 -- 3.4758)  data: 0.3424 (0.0004 -- 2.9737)  max mem: 16413
Epoch: [10]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3555 (2.2803)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5621 (4.1313)  time: 0.6936 (0.4958 -- 2.4551)  data: 0.1761 (0.0002 -- 1.9437)  max mem: 16413
Epoch: [10] Total time: 0:00:50 (0.9429 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3555 (2.2825)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5621 (4.1313)
Val:  [ 0/10]  eta: 0:00:21  loss: 1.7459 (1.7459)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1508 (2.1508 -- 2.1508)  data: 1.9727 (1.9727 -- 1.9727)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.6489 (1.7032)  acc1: 66.6667 (67.0732)  acc5: 100.0000 (93.9024)  time: 0.3522 (0.0220 -- 2.1508)  data: 0.1973 (0.0001 -- 1.9727)  max mem: 16413
Val: Total time: 0:00:03 (0.3523 s / it)
* Acc@1 62.805 Acc@5 95.732 loss 1.705
Accuracy of the network on the 163 val images: 62.80%
[2023-10-23 16:42:28,187] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:42:28,189] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:42:28,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:42:28,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:42:29,623] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:42:29,623] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.80%
Epoch: [11]  [ 0/54]  eta: 0:06:58  lr: 0.000047  min_lr: 0.000001  loss: 2.3444 (2.3444)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1344 (3.1344)  time: 7.7502 (7.7502 -- 7.7502)  data: 6.4675 (6.4675 -- 6.4675)  max mem: 16413
Epoch: [11]  [20/54]  eta: 0:00:42  lr: 0.000047  min_lr: 0.000001  loss: 2.1907 (2.2401)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5883 (4.0384)  time: 0.9205 (0.5247 -- 4.4374)  data: 0.0782 (0.0003 -- 1.5228)  max mem: 16413
Epoch: [11]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.2113 (2.2352)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8410 (4.1311)  time: 0.8584 (0.5279 -- 2.8812)  data: 0.0014 (0.0003 -- 0.0025)  max mem: 16413
[2023-10-23 16:43:16,078] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:43:16,078] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:43:16,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-10-23 16:43:16,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [11]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1626 (2.2270)  loss_scale: 2048.0000 (2351.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9376 (4.1293)  time: 0.6394 (0.4938 -- 2.8812)  data: 0.0008 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [11] Total time: 0:00:49 (0.9249 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1626 (2.2317)  loss_scale: 2048.0000 (2351.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9376 (4.1293)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.6822 (1.6822)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0208 (2.0208 -- 2.0208)  data: 1.8372 (1.8372 -- 1.8372)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.5698 (1.6272)  acc1: 66.6667 (65.8537)  acc5: 100.0000 (93.9024)  time: 0.3396 (0.0221 -- 2.0208)  data: 0.1838 (0.0001 -- 1.8372)  max mem: 16413
Val: Total time: 0:00:03 (0.3397 s / it)
* Acc@1 58.537 Acc@5 95.122 loss 1.627
Accuracy of the network on the 163 val images: 58.54%
Max accuracy: 62.80%
Epoch: [12]  [ 0/54]  eta: 0:07:54  lr: 0.000047  min_lr: 0.000001  loss: 2.0014 (2.0014)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3592 (3.3592)  time: 8.7822 (8.7822 -- 8.7822)  data: 8.2714 (8.2714 -- 8.2714)  max mem: 16413
Epoch: [12]  [20/54]  eta: 0:00:43  lr: 0.000047  min_lr: 0.000001  loss: 2.2023 (2.1875)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6404 (4.8410)  time: 0.9183 (0.5213 -- 4.9244)  data: 0.2635 (0.0003 -- 4.4179)  max mem: 16413
Epoch: [12]  [40/54]  eta: 0:00:15  lr: 0.000047  min_lr: 0.000001  loss: 2.0764 (2.1646)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2765 (4.8087)  time: 0.8503 (0.5160 -- 3.3439)  data: 0.3101 (0.0003 -- 2.8307)  max mem: 16413
Epoch: [12]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.0919 (2.1624)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2765 (4.7705)  time: 0.6225 (0.4949 -- 2.4443)  data: 0.1052 (0.0001 -- 1.9335)  max mem: 16413
Epoch: [12] Total time: 0:00:50 (0.9430 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.0919 (2.1742)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2765 (4.7705)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.6595 (1.6595)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0334 (2.0334 -- 2.0334)  data: 1.8437 (1.8437 -- 1.8437)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.5008 (1.5722)  acc1: 66.6667 (65.8537)  acc5: 100.0000 (95.1220)  time: 0.3419 (0.0221 -- 2.0334)  data: 0.1845 (0.0001 -- 1.8437)  max mem: 16413
Val: Total time: 0:00:03 (0.3420 s / it)
* Acc@1 59.146 Acc@5 95.732 loss 1.572
Accuracy of the network on the 163 val images: 59.15%
Max accuracy: 62.80%
Epoch: [13]  [ 0/54]  eta: 0:08:11  lr: 0.000047  min_lr: 0.000001  loss: 2.4081 (2.4081)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7488 (7.7488)  time: 9.0943 (9.0943 -- 9.0943)  data: 8.5503 (8.5503 -- 8.5503)  max mem: 16413
Epoch: [13]  [20/54]  eta: 0:00:41  lr: 0.000047  min_lr: 0.000001  loss: 2.0110 (2.1186)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4661 (4.8653)  time: 0.8343 (0.5267 -- 3.5741)  data: 0.1886 (0.0005 -- 2.6906)  max mem: 16413
Epoch: [13]  [40/54]  eta: 0:00:15  lr: 0.000047  min_lr: 0.000001  loss: 2.3469 (2.2044)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0897 (4.7384)  time: 0.9403 (0.5144 -- 4.3041)  data: 0.0012 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [13]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1206 (2.1820)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4992 (4.7913)  time: 0.5679 (0.4951 -- 1.2840)  data: 0.0006 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [13] Total time: 0:00:51 (0.9541 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1206 (2.1781)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4992 (4.7913)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.5269 (1.5269)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 1.9671 (1.9671 -- 1.9671)  data: 1.7553 (1.7553 -- 1.7553)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.4079 (1.5064)  acc1: 66.6667 (67.0732)  acc5: 100.0000 (93.9024)  time: 0.3345 (0.0220 -- 1.9671)  data: 0.1756 (0.0001 -- 1.7553)  max mem: 16413
Val: Total time: 0:00:03 (0.3346 s / it)
* Acc@1 63.415 Acc@5 95.122 loss 1.483
Accuracy of the network on the 163 val images: 63.41%
[2023-10-23 16:45:12,195] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:45:12,197] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:45:12,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:45:12,197] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:45:13,433] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:45:13,434] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.41%
Epoch: [14]  [ 0/54]  eta: 0:06:09  lr: 0.000047  min_lr: 0.000001  loss: 2.2438 (2.2438)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1419 (5.1419)  time: 6.8367 (6.8367 -- 6.8367)  data: 5.8546 (5.8546 -- 5.8546)  max mem: 16413
[2023-10-23 16:45:32,067] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:45:32,068] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-10-23 16:45:32,068] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:45:32,069] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [14]  [20/54]  eta: 0:00:42  lr: 0.000047  min_lr: 0.000001  loss: 2.2193 (2.2028)  loss_scale: 4096.0000 (5851.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2115 (4.5136)  time: 0.9593 (0.5210 -- 4.6392)  data: 0.3585 (0.0005 -- 4.1184)  max mem: 16413
Epoch: [14]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.1003 (2.1650)  loss_scale: 8192.0000 (6993.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3654 (4.5816)  time: 0.8105 (0.5131 -- 2.5522)  data: 0.2639 (0.0002 -- 2.0199)  max mem: 16413
Epoch: [14]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1022 (2.1298)  loss_scale: 8192.0000 (7281.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5277 (4.5886)  time: 0.6312 (0.4948 -- 1.5599)  data: 0.0639 (0.0002 -- 1.0395)  max mem: 16413
Epoch: [14] Total time: 0:00:49 (0.9238 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1022 (2.1341)  loss_scale: 8192.0000 (7281.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5277 (4.5886)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.4800 (1.4800)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0899 (2.0899 -- 2.0899)  data: 1.9196 (1.9196 -- 1.9196)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.3497 (1.4544)  acc1: 66.6667 (67.0732)  acc5: 100.0000 (95.1220)  time: 0.3466 (0.0223 -- 2.0899)  data: 0.1921 (0.0001 -- 1.9196)  max mem: 16413
Val: Total time: 0:00:03 (0.3467 s / it)
* Acc@1 62.195 Acc@5 96.341 loss 1.432
Accuracy of the network on the 163 val images: 62.20%
Max accuracy: 63.41%
Epoch: [15]  [ 0/54]  eta: 0:07:36  lr: 0.000047  min_lr: 0.000001  loss: 1.8271 (1.8271)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4372 (5.4372)  time: 8.4520 (8.4520 -- 8.4520)  data: 5.6330 (5.6330 -- 5.6330)  max mem: 16413
Epoch: [15]  [20/54]  eta: 0:00:41  lr: 0.000047  min_lr: 0.000001  loss: 2.1190 (2.1027)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9026 (5.7000)  time: 0.8529 (0.5242 -- 2.5348)  data: 0.0034 (0.0003 -- 0.0401)  max mem: 16413
Epoch: [15]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.1029 (2.1023)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0625 (5.4755)  time: 0.8181 (0.5196 -- 2.8154)  data: 0.0017 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [15]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1231 (2.1019)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0765 (5.6162)  time: 0.6503 (0.4937 -- 1.9259)  data: 0.0010 (0.0001 -- 0.0028)  max mem: 16413
Epoch: [15] Total time: 0:00:49 (0.9185 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1231 (2.0985)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0765 (5.6162)
Val:  [ 0/10]  eta: 0:00:21  loss: 1.4197 (1.4197)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1163 (2.1163 -- 2.1163)  data: 1.9467 (1.9467 -- 1.9467)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.2923 (1.3896)  acc1: 66.6667 (67.0732)  acc5: 100.0000 (93.9024)  time: 0.3490 (0.0220 -- 2.1163)  data: 0.1947 (0.0001 -- 1.9467)  max mem: 16413
Val: Total time: 0:00:03 (0.3491 s / it)
* Acc@1 62.805 Acc@5 95.732 loss 1.377
Accuracy of the network on the 163 val images: 62.80%
Max accuracy: 63.41%
Epoch: [16]  [ 0/54]  eta: 0:07:10  lr: 0.000047  min_lr: 0.000001  loss: 2.0031 (2.0031)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0432 (5.0432)  time: 7.9743 (7.9743 -- 7.9743)  data: 7.4347 (7.4347 -- 7.4347)  max mem: 16413
Epoch: [16]  [20/54]  eta: 0:00:41  lr: 0.000046  min_lr: 0.000001  loss: 2.1316 (2.1355)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6767 (5.2529)  time: 0.8910 (0.5249 -- 2.4326)  data: 0.1382 (0.0002 -- 1.3169)  max mem: 16413
[2023-10-23 16:47:33,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:47:33,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:47:33,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-10-23 16:47:33,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [16]  [40/54]  eta: 0:00:13  lr: 0.000046  min_lr: 0.000001  loss: 2.0887 (2.1223)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8993 (5.1259)  time: 0.7563 (0.5319 -- 3.1393)  data: 0.0724 (0.0003 -- 1.0895)  max mem: 16413
Epoch: [16]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1229 (2.0911)  loss_scale: 16384.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1158 (5.1323)  time: 0.6517 (0.4961 -- 2.3608)  data: 0.0764 (0.0002 -- 1.1309)  max mem: 16413
Epoch: [16] Total time: 0:00:49 (0.9256 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1229 (2.0795)  loss_scale: 16384.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1158 (5.1323)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.3921 (1.3921)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 1.9613 (1.9613 -- 1.9613)  data: 1.7681 (1.7681 -- 1.7681)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.2649 (1.3665)  acc1: 66.6667 (67.0732)  acc5: 100.0000 (95.1220)  time: 0.3342 (0.0221 -- 1.9613)  data: 0.1769 (0.0001 -- 1.7681)  max mem: 16413
Val: Total time: 0:00:03 (0.3343 s / it)
* Acc@1 62.805 Acc@5 96.341 loss 1.353
Accuracy of the network on the 163 val images: 62.80%
Max accuracy: 63.41%
Epoch: [17]  [ 0/54]  eta: 0:06:57  lr: 0.000046  min_lr: 0.000001  loss: 1.9291 (1.9291)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4390 (3.4390)  time: 7.7249 (7.7249 -- 7.7249)  data: 7.1450 (7.1450 -- 7.1450)  max mem: 16413
Epoch: [17]  [20/54]  eta: 0:00:42  lr: 0.000046  min_lr: 0.000001  loss: 2.1790 (2.1147)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5925 (5.0145)  time: 0.9320 (0.5214 -- 4.5578)  data: 0.3734 (0.0004 -- 4.0227)  max mem: 16413
Epoch: [17]  [40/54]  eta: 0:00:15  lr: 0.000046  min_lr: 0.000001  loss: 2.0683 (2.0905)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0784 (5.1425)  time: 0.9411 (0.5199 -- 2.7937)  data: 0.4002 (0.0002 -- 2.2835)  max mem: 16413
Epoch: [17]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1981 (2.1105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3419 (5.2404)  time: 0.6901 (0.4934 -- 2.7937)  data: 0.1697 (0.0001 -- 2.2835)  max mem: 16413
Epoch: [17] Total time: 0:00:51 (0.9590 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1981 (2.0592)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3419 (5.2404)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.3063 (1.3063)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9833 (1.9833 -- 1.9833)  data: 1.7928 (1.7928 -- 1.7928)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1917 (1.3087)  acc1: 66.6667 (65.8537)  acc5: 100.0000 (93.9024)  time: 0.3356 (0.0223 -- 1.9833)  data: 0.1794 (0.0001 -- 1.7928)  max mem: 16413
Val: Total time: 0:00:03 (0.3357 s / it)
* Acc@1 61.585 Acc@5 95.732 loss 1.297
Accuracy of the network on the 163 val images: 61.59%
Max accuracy: 63.41%
Epoch: [18]  [ 0/54]  eta: 0:06:59  lr: 0.000046  min_lr: 0.000001  loss: 2.1453 (2.1453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5539 (7.5539)  time: 7.7674 (7.7674 -- 7.7674)  data: 7.2341 (7.2341 -- 7.2341)  max mem: 16413
[2023-10-23 16:48:58,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 975
[2023-10-23 16:48:58,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 975
[2023-10-23 16:48:58,353] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384 to 8192.0
[2023-10-23 16:48:58,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384 to 8192.0
[2023-10-23 16:48:58,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384, reducing to 8192.0
Epoch: [18]  [20/54]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.0809 (2.0550)  loss_scale: 8192.0000 (9362.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3071 (5.7185)  time: 0.8249 (0.5185 -- 1.8229)  data: 0.2252 (0.0007 -- 1.2702)  max mem: 16413
[2023-10-23 16:49:20,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=1, lr=[1.1005170051889259e-06, 1.1005170051889259e-06, 1.4673560069185678e-06, 1.4673560069185678e-06, 1.9564746758914238e-06, 1.9564746758914238e-06, 2.608632901188565e-06, 2.608632901188565e-06, 3.478177201584753e-06, 3.478177201584753e-06, 4.637569602113004e-06, 4.637569602113004e-06, 6.183426136150673e-06, 6.183426136150673e-06, 8.24456818153423e-06, 8.24456818153423e-06, 1.0992757575378974e-05, 1.0992757575378974e-05, 1.4657010100505299e-05, 1.4657010100505299e-05, 1.9542680134007064e-05, 1.9542680134007064e-05, 2.605690684534275e-05, 2.605690684534275e-05, 3.4742542460457007e-05, 3.4742542460457007e-05, 4.6323389947276004e-05, 4.6323389947276004e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 16:49:20,251] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=19.516757380664966, CurrSamplesPerSec=23.636295670844014, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.2329 (2.0992)  loss_scale: 8192.0000 (8791.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9460 (6.0747)  time: 0.8926 (0.5198 -- 3.2104)  data: 0.2966 (0.0003 -- 2.6882)  max mem: 16413
Epoch: [18]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2224 (2.0673)  loss_scale: 8192.0000 (8647.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8216 (6.3193)  time: 0.7897 (0.4970 -- 3.2104)  data: 0.2688 (0.0002 -- 2.6882)  max mem: 16413
Epoch: [18] Total time: 0:00:50 (0.9420 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2224 (2.0758)  loss_scale: 8192.0000 (8647.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8216 (6.3193)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.3243 (1.3243)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 1.9861 (1.9861 -- 1.9861)  data: 1.7634 (1.7634 -- 1.7634)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1852 (1.2666)  acc1: 66.6667 (67.0732)  acc5: 100.0000 (96.3415)  time: 0.3362 (0.0221 -- 1.9861)  data: 0.1764 (0.0001 -- 1.7634)  max mem: 16413
Val: Total time: 0:00:03 (0.3363 s / it)
* Acc@1 59.146 Acc@5 96.341 loss 1.279
Accuracy of the network on the 163 val images: 59.15%
Max accuracy: 63.41%
Epoch: [19]  [ 0/54]  eta: 0:06:47  lr: 0.000046  min_lr: 0.000001  loss: 2.2340 (2.2340)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2933 (5.2933)  time: 7.5453 (7.5453 -- 7.5453)  data: 5.5376 (5.5376 -- 5.5376)  max mem: 16413
Epoch: [19]  [20/54]  eta: 0:00:45  lr: 0.000046  min_lr: 0.000001  loss: 2.0610 (2.0446)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5380 (6.5392)  time: 1.0169 (0.5087 -- 7.0750)  data: 0.3345 (0.0003 -- 6.5705)  max mem: 16413
Epoch: [19]  [40/54]  eta: 0:00:15  lr: 0.000046  min_lr: 0.000001  loss: 2.0306 (2.0234)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2682 (6.7034)  time: 0.8981 (0.5113 -- 4.5642)  data: 0.3585 (0.0003 -- 4.0595)  max mem: 16413
Epoch: [19]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0101 (2.0124)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2235 (6.3365)  time: 0.6672 (0.4943 -- 3.4873)  data: 0.1548 (0.0001 -- 2.9789)  max mem: 16413
Epoch: [19] Total time: 0:00:52 (0.9713 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0101 (2.0198)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2235 (6.3365)
[2023-10-23 16:50:35,122] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-10-23 16:50:35,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-10-23 16:50:35,124] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-19/mp_rank_00_model_states.pt
[2023-10-23 16:50:35,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-19/mp_rank_00_model_states.pt...
[2023-10-23 16:50:36,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-19/mp_rank_00_model_states.pt.
[2023-10-23 16:50:36,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/10]  eta: 0:00:19  loss: 1.2311 (1.2311)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9666 (1.9666 -- 1.9666)  data: 1.7748 (1.7748 -- 1.7748)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1243 (1.2284)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (95.1220)  time: 0.3382 (0.0227 -- 1.9666)  data: 0.1816 (0.0001 -- 1.7748)  max mem: 16413
Val: Total time: 0:00:03 (0.3383 s / it)
* Acc@1 64.024 Acc@5 96.341 loss 1.225
Accuracy of the network on the 163 val images: 64.02%
[2023-10-23 16:50:39,549] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:50:39,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:50:39,551] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:50:39,551] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:50:41,177] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:50:41,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.02%
Epoch: [20]  [ 0/54]  eta: 0:06:52  lr: 0.000046  min_lr: 0.000001  loss: 1.4785 (1.4785)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6464 (6.6464)  time: 7.6414 (7.6414 -- 7.6414)  data: 7.1220 (7.1220 -- 7.1220)  max mem: 16413
Epoch: [20]  [20/54]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 1.9564 (1.8879)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9915 (6.1387)  time: 0.7754 (0.5269 -- 3.5514)  data: 0.2256 (0.0003 -- 2.9917)  max mem: 16413
[2023-10-23 16:51:09,529] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:51:09,529] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:51:09,570] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 16:51:09,570] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [20]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.1049 (1.9575)  loss_scale: 16384.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5252 (6.0071)  time: 0.9454 (0.5319 -- 3.5458)  data: 0.3974 (0.0008 -- 2.9826)  max mem: 16413
Epoch: [20]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0237 (1.9647)  loss_scale: 16384.0000 (12743.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8301 (5.9227)  time: 0.6475 (0.4954 -- 2.1517)  data: 0.1266 (0.0002 -- 1.6329)  max mem: 16413
Epoch: [20] Total time: 0:00:48 (0.9024 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0237 (1.9866)  loss_scale: 16384.0000 (12743.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8301 (5.9227)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.1737 (1.1737)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9446 (1.9446 -- 1.9446)  data: 1.7499 (1.7499 -- 1.7499)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0874 (1.1825)  acc1: 77.7778 (70.7317)  acc5: 100.0000 (92.6829)  time: 0.3349 (0.0228 -- 1.9446)  data: 0.1771 (0.0001 -- 1.7499)  max mem: 16413
Val: Total time: 0:00:03 (0.3350 s / it)
* Acc@1 63.415 Acc@5 95.122 loss 1.180
Accuracy of the network on the 163 val images: 63.41%
Max accuracy: 64.02%
Epoch: [21]  [ 0/54]  eta: 0:06:39  lr: 0.000046  min_lr: 0.000001  loss: 2.0154 (2.0154)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3184 (3.3184)  time: 7.4066 (7.4066 -- 7.4066)  data: 5.7042 (5.7042 -- 5.7042)  max mem: 16413
Epoch: [21]  [20/54]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.0214 (1.9956)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5910 (5.7654)  time: 0.8643 (0.5182 -- 3.5655)  data: 0.0931 (0.0006 -- 1.8336)  max mem: 16413
Epoch: [21]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 1.9922 (2.0003)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1300 (6.6828)  time: 0.9478 (0.5118 -- 3.5338)  data: 0.1382 (0.0002 -- 1.2072)  max mem: 16413
Epoch: [21]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0225 (2.0498)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5544 (6.4840)  time: 0.7054 (0.4949 -- 3.5338)  data: 0.0474 (0.0002 -- 0.9326)  max mem: 16413
Epoch: [21] Total time: 0:00:50 (0.9424 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0225 (2.0268)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5544 (6.4840)
Val:  [ 0/10]  eta: 0:00:21  loss: 1.1782 (1.1782)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1320 (2.1320 -- 2.1320)  data: 1.9523 (1.9523 -- 1.9523)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0506 (1.1650)  acc1: 66.6667 (69.5122)  acc5: 100.0000 (93.9024)  time: 0.3516 (0.0228 -- 2.1320)  data: 0.1953 (0.0001 -- 1.9523)  max mem: 16413
Val: Total time: 0:00:03 (0.3517 s / it)
* Acc@1 62.805 Acc@5 95.732 loss 1.177
Accuracy of the network on the 163 val images: 62.80%
Max accuracy: 64.02%
Epoch: [22]  [ 0/54]  eta: 0:07:25  lr: 0.000046  min_lr: 0.000001  loss: 1.7202 (1.7202)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0380 (7.0380)  time: 8.2562 (8.2562 -- 8.2562)  data: 6.1039 (6.1039 -- 6.1039)  max mem: 16413
Epoch: [22]  [20/54]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000001  loss: 1.9402 (1.9657)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4089 (6.4221)  time: 0.8313 (0.5277 -- 4.2543)  data: 0.0850 (0.0006 -- 1.6563)  max mem: 16413
Epoch: [22]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.0121 (1.9618)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3835 (6.7767)  time: 0.9467 (0.5146 -- 4.5020)  data: 0.0500 (0.0003 -- 0.9646)  max mem: 16413
[2023-10-23 16:53:13,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:53:13,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:53:13,585] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 16:53:13,585] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0233 (1.9833)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8454 (6.7175)  time: 0.6550 (0.4935 -- 3.2600)  data: 0.0489 (0.0002 -- 0.9646)  max mem: 16413
Epoch: [22] Total time: 0:00:50 (0.9326 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0233 (1.9860)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8454 (6.7175)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.1652 (1.1652)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9591 (1.9591 -- 1.9591)  data: 1.7611 (1.7611 -- 1.7611)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0189 (1.1423)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3355 (0.0222 -- 1.9591)  data: 0.1762 (0.0001 -- 1.7611)  max mem: 16413
Val: Total time: 0:00:03 (0.3356 s / it)
* Acc@1 65.854 Acc@5 95.732 loss 1.162
Accuracy of the network on the 163 val images: 65.85%
[2023-10-23 16:53:21,446] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:53:21,448] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:53:21,448] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:53:21,448] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:53:22,968] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:53:22,968] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.85%
Epoch: [23]  [ 0/54]  eta: 0:07:36  lr: 0.000046  min_lr: 0.000001  loss: 2.2407 (2.2407)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6163 (11.6163)  time: 8.4463 (8.4463 -- 8.4463)  data: 7.6233 (7.6233 -- 7.6233)  max mem: 16413
[2023-10-23 16:53:32,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1244
[2023-10-23 16:53:32,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1244
[2023-10-23 16:53:32,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 16:53:32,499] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 16:53:32,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [23]  [20/54]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.0212 (2.0127)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2333 (7.2483)  time: 0.8068 (0.5147 -- 3.5084)  data: 0.2607 (0.0004 -- 2.9766)  max mem: 16413
Epoch: [23]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.1353 (2.0157)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9606 (6.7825)  time: 0.9197 (0.5296 -- 3.1227)  data: 0.3728 (0.0002 -- 2.5987)  max mem: 16413
Epoch: [23]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0530 (2.0157)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6653 (6.7287)  time: 0.6980 (0.4978 -- 3.1227)  data: 0.1767 (0.0002 -- 2.5987)  max mem: 16413
Epoch: [23] Total time: 0:00:49 (0.9193 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0530 (2.0203)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6653 (6.7287)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.1378 (1.1378)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0933 (2.0933 -- 2.0933)  data: 1.9234 (1.9234 -- 1.9234)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0292 (1.1349)  acc1: 77.7778 (71.9512)  acc5: 100.0000 (95.1220)  time: 0.3468 (0.0220 -- 2.0933)  data: 0.1924 (0.0001 -- 1.9234)  max mem: 16413
Val: Total time: 0:00:03 (0.3470 s / it)
* Acc@1 64.634 Acc@5 95.732 loss 1.151
Accuracy of the network on the 163 val images: 64.63%
Max accuracy: 65.85%
Epoch: [24]  [ 0/54]  eta: 0:06:28  lr: 0.000046  min_lr: 0.000001  loss: 1.7370 (1.7370)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4372 (6.4372)  time: 7.1904 (7.1904 -- 7.1904)  data: 5.3997 (5.3997 -- 5.3997)  max mem: 16413
Epoch: [24]  [20/54]  eta: 0:00:41  lr: 0.000046  min_lr: 0.000001  loss: 1.8679 (1.8924)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6662 (6.2513)  time: 0.9090 (0.5284 -- 3.0245)  data: 0.2935 (0.0009 -- 2.4838)  max mem: 16413
Epoch: [24]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 1.9300 (1.9520)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3810 (6.3477)  time: 0.8344 (0.5208 -- 3.6871)  data: 0.0114 (0.0004 -- 0.2004)  max mem: 16413
Epoch: [24]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 1.9689 (1.9725)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7069 (7.1056)  time: 0.6127 (0.4941 -- 1.5572)  data: 0.0064 (0.0002 -- 0.1135)  max mem: 16413
Epoch: [24] Total time: 0:00:50 (0.9352 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 1.9689 (1.9809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7069 (7.1056)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.1293 (1.1293)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0791 (2.0791 -- 2.0791)  data: 1.9000 (1.9000 -- 1.9000)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9690 (1.0945)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (95.1220)  time: 0.3453 (0.0220 -- 2.0791)  data: 0.1901 (0.0001 -- 1.9000)  max mem: 16413
Val: Total time: 0:00:03 (0.3454 s / it)
* Acc@1 65.244 Acc@5 95.732 loss 1.112
Accuracy of the network on the 163 val images: 65.24%
Max accuracy: 65.85%
Epoch: [25]  [ 0/54]  eta: 0:05:28  lr: 0.000046  min_lr: 0.000001  loss: 2.2585 (2.2585)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7660 (5.7660)  time: 6.0766 (6.0766 -- 6.0766)  data: 5.2150 (5.2150 -- 5.2150)  max mem: 16413
Epoch: [25]  [20/54]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.1290 (2.0998)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7490 (6.3403)  time: 0.9143 (0.5261 -- 2.9367)  data: 0.0229 (0.0006 -- 0.3565)  max mem: 16413
[2023-10-23 16:55:36,062] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:55:36,062] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:55:36,062] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 16:55:36,062] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.1245 (2.0780)  loss_scale: 32768.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3637 (6.3985)  time: 0.8689 (0.5325 -- 2.3440)  data: 0.1951 (0.0005 -- 1.7946)  max mem: 16413
Epoch: [25]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0260 (2.0525)  loss_scale: 32768.0000 (25789.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5100 (6.3260)  time: 0.6930 (0.4966 -- 2.3440)  data: 0.0935 (0.0002 -- 1.7946)  max mem: 16413
Epoch: [25] Total time: 0:00:48 (0.9032 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0260 (2.0012)  loss_scale: 32768.0000 (25789.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5100 (6.3260)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.2202 (1.2202)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 1.9461 (1.9461 -- 1.9461)  data: 1.7421 (1.7421 -- 1.7421)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9540 (1.0984)  acc1: 66.6667 (69.5122)  acc5: 100.0000 (95.1220)  time: 0.3335 (0.0222 -- 1.9461)  data: 0.1743 (0.0001 -- 1.7421)  max mem: 16413
Val: Total time: 0:00:03 (0.3336 s / it)
* Acc@1 64.634 Acc@5 96.341 loss 1.141
Accuracy of the network on the 163 val images: 64.63%
Max accuracy: 65.85%
Epoch: [26]  [ 0/54]  eta: 0:07:09  lr: 0.000046  min_lr: 0.000001  loss: 2.3882 (2.3882)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1281 (8.1281)  time: 7.9496 (7.9496 -- 7.9496)  data: 7.4194 (7.4194 -- 7.4194)  max mem: 16413
Epoch: [26]  [20/54]  eta: 0:00:41  lr: 0.000046  min_lr: 0.000001  loss: 2.0060 (2.0168)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4812 (6.4015)  time: 0.8744 (0.5153 -- 4.0389)  data: 0.3300 (0.0003 -- 3.5009)  max mem: 16413
Epoch: [26]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 1.9404 (1.9816)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7622 (6.5254)  time: 0.8966 (0.5257 -- 3.2761)  data: 0.3509 (0.0002 -- 2.7631)  max mem: 16413
[2023-10-23 16:56:51,368] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1455
[2023-10-23 16:56:51,368] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1455
[2023-10-23 16:56:51,368] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 16:56:51,368] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 16:56:51,368] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9918 (1.9889)  loss_scale: 32768.0000 (31857.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3543 (6.5772)  time: 0.6773 (0.4819 -- 3.2761)  data: 0.1605 (0.0002 -- 2.7631)  max mem: 16413
Epoch: [26] Total time: 0:00:50 (0.9295 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9918 (1.9845)  loss_scale: 32768.0000 (31857.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3543 (6.5772)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.1248 (1.1248)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0548 (2.0548 -- 2.0548)  data: 1.8861 (1.8861 -- 1.8861)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9307 (1.0749)  acc1: 66.6667 (69.5122)  acc5: 100.0000 (95.1220)  time: 0.3434 (0.0222 -- 2.0548)  data: 0.1887 (0.0001 -- 1.8861)  max mem: 16413
Val: Total time: 0:00:03 (0.3435 s / it)
* Acc@1 64.634 Acc@5 95.732 loss 1.106
Accuracy of the network on the 163 val images: 64.63%
Max accuracy: 65.85%
Epoch: [27]  [ 0/54]  eta: 0:07:07  lr: 0.000045  min_lr: 0.000001  loss: 1.5133 (1.5133)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1162 (9.1162)  time: 7.9114 (7.9114 -- 7.9114)  data: 7.3781 (7.3781 -- 7.3781)  max mem: 16413
Epoch: [27]  [20/54]  eta: 0:00:43  lr: 0.000045  min_lr: 0.000001  loss: 2.0194 (1.9448)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7234 (7.3892)  time: 0.9412 (0.5216 -- 5.2651)  data: 0.4027 (0.0004 -- 4.7550)  max mem: 16413
Epoch: [27]  [40/54]  eta: 0:00:15  lr: 0.000045  min_lr: 0.000001  loss: 1.8361 (1.8975)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5161 (7.0747)  time: 0.9301 (0.5017 -- 5.2148)  data: 0.3962 (0.0002 -- 4.7118)  max mem: 16413
Epoch: [27]  [53/54]  eta: 0:00:01  lr: 0.000045  min_lr: 0.000001  loss: 1.9812 (1.9351)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0288 (7.0848)  time: 0.6599 (0.4927 -- 2.9044)  data: 0.1441 (0.0001 -- 2.3964)  max mem: 16413
Epoch: [27] Total time: 0:00:54 (1.0061 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9812 (1.9334)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0288 (7.0848)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.1013 (1.1013)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0215 (2.0215 -- 2.0215)  data: 1.8344 (1.8344 -- 1.8344)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9138 (1.0435)  acc1: 66.6667 (69.5122)  acc5: 100.0000 (96.3415)  time: 0.3409 (0.0262 -- 2.0215)  data: 0.1835 (0.0001 -- 1.8344)  max mem: 16413
Val: Total time: 0:00:03 (0.3410 s / it)
* Acc@1 65.244 Acc@5 96.951 loss 1.071
Accuracy of the network on the 163 val images: 65.24%
Max accuracy: 65.85%
Epoch: [28]  [ 0/54]  eta: 0:08:38  lr: 0.000045  min_lr: 0.000001  loss: 1.8420 (1.8420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5335 (8.5335)  time: 9.5966 (9.5966 -- 9.5966)  data: 5.7180 (5.7180 -- 5.7180)  max mem: 16413
Epoch: [28]  [20/54]  eta: 0:00:40  lr: 0.000045  min_lr: 0.000001  loss: 2.0937 (2.0732)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9694 (7.5858)  time: 0.7830 (0.5182 -- 3.0732)  data: 0.1348 (0.0003 -- 1.7233)  max mem: 16413
Epoch: [28]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 1.9751 (2.0228)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9675 (7.3114)  time: 0.9252 (0.5211 -- 4.2067)  data: 0.3832 (0.0003 -- 3.6956)  max mem: 16413
Epoch: [28]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9375 (2.0067)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0675 (7.2487)  time: 0.6305 (0.4944 -- 1.7745)  data: 0.1161 (0.0001 -- 1.2739)  max mem: 16413
Epoch: [28] Total time: 0:00:51 (0.9562 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9375 (1.9386)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0675 (7.2487)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.0152 (1.0152)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9706 (1.9706 -- 1.9706)  data: 1.7701 (1.7701 -- 1.7701)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8929 (1.0160)  acc1: 66.6667 (68.2927)  acc5: 100.0000 (96.3415)  time: 0.3353 (0.0221 -- 1.9706)  data: 0.1771 (0.0001 -- 1.7701)  max mem: 16413
Val: Total time: 0:00:03 (0.3354 s / it)
* Acc@1 63.415 Acc@5 96.951 loss 1.036
Accuracy of the network on the 163 val images: 63.41%
Max accuracy: 65.85%
Epoch: [29]  [ 0/54]  eta: 0:07:35  lr: 0.000045  min_lr: 0.000001  loss: 2.0167 (2.0167)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1237 (7.1237)  time: 8.4309 (8.4309 -- 8.4309)  data: 7.0447 (7.0447 -- 7.0447)  max mem: 16413
[2023-10-23 16:59:12,375] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:59:12,375] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 16:59:12,375] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 16:59:12,375] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [20/54]  eta: 0:00:41  lr: 0.000045  min_lr: 0.000001  loss: 1.9092 (1.8309)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6882 (6.1258)  time: 0.8537 (0.5278 -- 4.3807)  data: 0.1794 (0.0005 -- 2.0470)  max mem: 16413
Epoch: [29]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 2.0301 (1.9522)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8761 (6.9467)  time: 0.9036 (0.5303 -- 2.2921)  data: 0.1745 (0.0003 -- 1.7615)  max mem: 16413
[2023-10-23 16:59:36,755] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1613
[2023-10-23 16:59:36,755] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 16:59:36,755] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1613
[2023-10-23 16:59:36,755] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 16:59:36,755] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9151 (1.9296)  loss_scale: 32768.0000 (25182.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5017 (6.8626)  time: 0.7107 (0.4846 -- 2.2296)  data: 0.0860 (0.0001 -- 1.7057)  max mem: 16413
Epoch: [29] Total time: 0:00:51 (0.9522 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9151 (1.9750)  loss_scale: 32768.0000 (25182.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5017 (6.8626)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.0387 (1.0387)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0730 (2.0730 -- 2.0730)  data: 1.8988 (1.8988 -- 1.8988)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8880 (1.0308)  acc1: 66.6667 (71.9512)  acc5: 100.0000 (93.9024)  time: 0.3452 (0.0225 -- 2.0730)  data: 0.1900 (0.0001 -- 1.8988)  max mem: 16413
Val: Total time: 0:00:03 (0.3454 s / it)
* Acc@1 67.683 Acc@5 95.732 loss 1.066
Accuracy of the network on the 163 val images: 67.68%
[2023-10-23 16:59:43,422] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 16:59:43,424] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 16:59:43,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 16:59:43,424] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 16:59:45,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 16:59:45,046] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.68%
Epoch: [30]  [ 0/54]  eta: 0:07:39  lr: 0.000045  min_lr: 0.000001  loss: 1.9844 (1.9844)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4524 (7.4524)  time: 8.5138 (8.5138 -- 8.5138)  data: 4.7422 (4.7422 -- 4.7422)  max mem: 16413
Epoch: [30]  [20/54]  eta: 0:00:43  lr: 0.000045  min_lr: 0.000001  loss: 1.8184 (1.8915)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1418 (7.1456)  time: 0.9291 (0.5320 -- 3.7693)  data: 0.1327 (0.0005 -- 1.5104)  max mem: 16413
Epoch: [30]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 1.7144 (1.8454)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8632 (7.1274)  time: 0.7736 (0.5134 -- 3.0837)  data: 0.0011 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [30]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.8474 (1.8748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1018 (7.6486)  time: 0.6532 (0.4931 -- 3.2621)  data: 0.0006 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [30] Total time: 0:00:51 (0.9592 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.8474 (1.9316)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1018 (7.6486)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.0112 (1.0112)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9647 (1.9647 -- 1.9647)  data: 1.7590 (1.7590 -- 1.7590)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9285 (1.0211)  acc1: 66.6667 (67.0732)  acc5: 100.0000 (96.3415)  time: 0.3340 (0.0221 -- 1.9647)  data: 0.1760 (0.0001 -- 1.7590)  max mem: 16413
Val: Total time: 0:00:03 (0.3341 s / it)
* Acc@1 64.024 Acc@5 96.951 loss 1.047
Accuracy of the network on the 163 val images: 64.02%
Max accuracy: 67.68%
Epoch: [31]  [ 0/54]  eta: 0:06:19  lr: 0.000045  min_lr: 0.000001  loss: 1.5534 (1.5534)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0784 (4.0784)  time: 7.0198 (7.0198 -- 7.0198)  data: 5.2184 (5.2184 -- 5.2184)  max mem: 16413
Epoch: [31]  [20/54]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000001  loss: 1.9383 (1.9088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6585 (6.5047)  time: 0.8377 (0.5232 -- 2.7153)  data: 0.2433 (0.0007 -- 1.5065)  max mem: 16413
Epoch: [31]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 1.9855 (1.9308)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8362 (6.6220)  time: 0.9032 (0.5304 -- 4.1927)  data: 0.3471 (0.0004 -- 3.6596)  max mem: 16413
Epoch: [31]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9869 (1.9408)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4455 (6.8124)  time: 0.6809 (0.4929 -- 2.6942)  data: 0.1601 (0.0001 -- 2.1927)  max mem: 16413
Epoch: [31] Total time: 0:00:51 (0.9564 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9869 (1.9672)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4455 (6.8124)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9840 (0.9840)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9007 (1.9007 -- 1.9007)  data: 1.6759 (1.6759 -- 1.6759)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8783 (0.9838)  acc1: 77.7778 (71.9512)  acc5: 100.0000 (93.9024)  time: 0.3311 (0.0221 -- 1.9007)  data: 0.1702 (0.0001 -- 1.6759)  max mem: 16413
Val: Total time: 0:00:03 (0.3313 s / it)
* Acc@1 66.463 Acc@5 95.732 loss 1.016
Accuracy of the network on the 163 val images: 66.46%
Max accuracy: 67.68%
Epoch: [32]  [ 0/54]  eta: 0:07:04  lr: 0.000045  min_lr: 0.000001  loss: 2.5289 (2.5289)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6533 (5.6533)  time: 7.8523 (7.8523 -- 7.8523)  data: 5.5983 (5.5983 -- 5.5983)  max mem: 16413
[2023-10-23 17:01:53,740] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:01:53,740] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:01:53,740] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:01:53,741] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:01:55,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1744
[2023-10-23 17:01:55,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1744
[2023-10-23 17:01:55,993] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:01:55,993] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:01:55,993] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [20/54]  eta: 0:00:41  lr: 0.000045  min_lr: 0.000001  loss: 2.0152 (1.9317)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1832 (6.6276)  time: 0.8972 (0.5193 -- 3.3707)  data: 0.3010 (0.0005 -- 2.8194)  max mem: 16413
Epoch: [32]  [40/54]  eta: 0:00:15  lr: 0.000045  min_lr: 0.000001  loss: 1.9006 (1.9422)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9279 (6.9289)  time: 0.9179 (0.5186 -- 3.7703)  data: 0.0635 (0.0003 -- 0.6643)  max mem: 16413
Epoch: [32]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.8715 (1.9477)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3041 (6.5630)  time: 0.6465 (0.4949 -- 2.9162)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [32] Total time: 0:00:50 (0.9432 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.8715 (1.8940)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3041 (6.5630)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.0025 (1.0025)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9331 (1.9331 -- 1.9331)  data: 1.7281 (1.7281 -- 1.7281)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8742 (0.9950)  acc1: 66.6667 (69.5122)  acc5: 100.0000 (96.3415)  time: 0.3320 (0.0222 -- 1.9331)  data: 0.1743 (0.0001 -- 1.7281)  max mem: 16413
Val: Total time: 0:00:03 (0.3321 s / it)
* Acc@1 64.024 Acc@5 96.951 loss 1.016
Accuracy of the network on the 163 val images: 64.02%
Max accuracy: 67.68%
Epoch: [33]  [ 0/54]  eta: 0:06:44  lr: 0.000045  min_lr: 0.000001  loss: 1.5443 (1.5443)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4816 (7.4816)  time: 7.4828 (7.4828 -- 7.4828)  data: 6.9547 (6.9547 -- 6.9547)  max mem: 16413
Epoch: [33]  [20/54]  eta: 0:00:42  lr: 0.000044  min_lr: 0.000001  loss: 1.9777 (1.8746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9338 (6.9712)  time: 0.9230 (0.5125 -- 3.2053)  data: 0.1052 (0.0002 -- 2.0740)  max mem: 16413
Epoch: [33]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 1.9485 (1.9370)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3953 (6.8913)  time: 0.7971 (0.5126 -- 4.1199)  data: 0.0016 (0.0003 -- 0.0084)  max mem: 16413
Epoch: [33]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.9859 (1.9455)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4172 (6.9565)  time: 0.6631 (0.4940 -- 2.8931)  data: 0.0012 (0.0002 -- 0.0084)  max mem: 16413
Epoch: [33] Total time: 0:00:51 (0.9457 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.9859 (1.8979)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4172 (6.9565)
Val:  [ 0/10]  eta: 0:00:21  loss: 1.0559 (1.0559)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1183 (2.1183 -- 2.1183)  data: 1.9493 (1.9493 -- 1.9493)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9263 (1.0056)  acc1: 77.7778 (70.7317)  acc5: 100.0000 (95.1220)  time: 0.3491 (0.0220 -- 2.1183)  data: 0.1950 (0.0001 -- 1.9493)  max mem: 16413
Val: Total time: 0:00:03 (0.3492 s / it)
* Acc@1 64.634 Acc@5 96.341 loss 1.035
Accuracy of the network on the 163 val images: 64.63%
Max accuracy: 67.68%
Epoch: [34]  [ 0/54]  eta: 0:07:14  lr: 0.000044  min_lr: 0.000001  loss: 2.1337 (2.1337)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5910 (7.5910)  time: 8.0483 (8.0483 -- 8.0483)  data: 7.5354 (7.5354 -- 7.5354)  max mem: 16413
Epoch: [34]  [20/54]  eta: 0:00:39  lr: 0.000044  min_lr: 0.000001  loss: 1.9188 (1.9178)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0430 (7.1208)  time: 0.8184 (0.5288 -- 3.2823)  data: 0.2755 (0.0004 -- 2.7582)  max mem: 16413
[2023-10-23 17:04:03,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:04:03,234] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:04:03,235] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:04:03,235] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 1.9444 (1.8765)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7121 (7.1447)  time: 0.9222 (0.5262 -- 3.9718)  data: 0.3720 (0.0003 -- 3.4446)  max mem: 16413
[2023-10-23 17:04:13,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1889
[2023-10-23 17:04:13,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1889
[2023-10-23 17:04:13,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:04:13,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:04:13,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.9526 (1.8816)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7121 (7.2909)  time: 0.6391 (0.4817 -- 2.6166)  data: 0.1147 (0.0001 -- 2.0990)  max mem: 16413
Epoch: [34] Total time: 0:00:49 (0.9202 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.9526 (1.8471)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7121 (7.2909)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9352 (0.9352)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9453 (1.9453 -- 1.9453)  data: 1.7144 (1.7144 -- 1.7144)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8564 (0.9582)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (93.9024)  time: 0.3337 (0.0222 -- 1.9453)  data: 0.1715 (0.0001 -- 1.7144)  max mem: 16413
Val: Total time: 0:00:03 (0.3338 s / it)
* Acc@1 65.854 Acc@5 95.732 loss 0.987
Accuracy of the network on the 163 val images: 65.85%
Max accuracy: 67.68%
Epoch: [35]  [ 0/54]  eta: 0:07:26  lr: 0.000044  min_lr: 0.000001  loss: 1.3027 (1.3027)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0623 (6.0623)  time: 8.2628 (8.2628 -- 8.2628)  data: 7.4007 (7.4007 -- 7.4007)  max mem: 16413
Epoch: [35]  [20/54]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000001  loss: 1.9992 (1.9850)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0979 (8.1797)  time: 0.7841 (0.5143 -- 3.5159)  data: 0.2375 (0.0003 -- 2.9888)  max mem: 16413
Epoch: [35]  [40/54]  eta: 0:00:13  lr: 0.000044  min_lr: 0.000001  loss: 1.8088 (1.8811)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5493 (7.2382)  time: 0.8217 (0.5253 -- 2.0873)  data: 0.2780 (0.0002 -- 1.5392)  max mem: 16413
Epoch: [35]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.7235 (1.8629)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7607 (7.2659)  time: 0.7034 (0.4943 -- 2.3386)  data: 0.1817 (0.0001 -- 1.8178)  max mem: 16413
Epoch: [35] Total time: 0:00:48 (0.9059 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.7235 (1.8450)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7607 (7.2659)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9202 (0.9202)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9299 (1.9299 -- 1.9299)  data: 1.7198 (1.7198 -- 1.7198)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8721 (0.9421)  acc1: 77.7778 (71.9512)  acc5: 100.0000 (93.9024)  time: 0.3338 (0.0222 -- 1.9299)  data: 0.1742 (0.0001 -- 1.7198)  max mem: 16413
Val: Total time: 0:00:03 (0.3339 s / it)
* Acc@1 67.683 Acc@5 95.122 loss 0.967
Accuracy of the network on the 163 val images: 67.68%
Max accuracy: 67.68%
Epoch: [36]  [ 0/54]  eta: 0:07:18  lr: 0.000044  min_lr: 0.000001  loss: 2.0835 (2.0835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5384 (8.5384)  time: 8.1249 (8.1249 -- 8.1249)  data: 7.5601 (7.5601 -- 7.5601)  max mem: 16413
Epoch: [36]  [20/54]  eta: 0:00:40  lr: 0.000044  min_lr: 0.000001  loss: 1.8740 (1.8398)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9783 (7.6483)  time: 0.8415 (0.5229 -- 3.0225)  data: 0.1987 (0.0003 -- 1.2504)  max mem: 16413
Epoch: [36]  [40/54]  eta: 0:00:15  lr: 0.000044  min_lr: 0.000001  loss: 1.9238 (1.8476)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2996 (7.5247)  time: 0.9605 (0.5138 -- 4.1078)  data: 0.4159 (0.0003 -- 3.5887)  max mem: 16413
Epoch: [36]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8049 (1.8434)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7883 (7.6008)  time: 0.6275 (0.4939 -- 2.4660)  data: 0.1075 (0.0001 -- 1.9385)  max mem: 16413
Epoch: [36] Total time: 0:00:50 (0.9428 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.8049 (1.8966)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7883 (7.6008)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9867 (0.9867)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0859 (2.0859 -- 2.0859)  data: 1.9170 (1.9170 -- 1.9170)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8574 (0.9654)  acc1: 77.7778 (70.7317)  acc5: 100.0000 (96.3415)  time: 0.3458 (0.0222 -- 2.0859)  data: 0.1918 (0.0001 -- 1.9170)  max mem: 16413
Val: Total time: 0:00:03 (0.3459 s / it)
* Acc@1 66.463 Acc@5 96.341 loss 0.994
Accuracy of the network on the 163 val images: 66.46%
Max accuracy: 67.68%
Epoch: [37]  [ 0/54]  eta: 0:07:19  lr: 0.000044  min_lr: 0.000001  loss: 2.4302 (2.4302)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7460 (7.7460)  time: 8.1357 (8.1357 -- 8.1357)  data: 6.8511 (6.8511 -- 6.8511)  max mem: 16413
[2023-10-23 17:06:12,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=6, lr=[1.041240759062658e-06, 1.041240759062658e-06, 1.388321012083544e-06, 1.388321012083544e-06, 1.8510946827780586e-06, 1.8510946827780586e-06, 2.468126243704078e-06, 2.468126243704078e-06, 3.2908349916054374e-06, 3.2908349916054374e-06, 4.38777998880725e-06, 4.38777998880725e-06, 5.8503733184096665e-06, 5.8503733184096665e-06, 7.800497757879556e-06, 7.800497757879556e-06, 1.0400663677172742e-05, 1.0400663677172742e-05, 1.3867551569563655e-05, 1.3867551569563655e-05, 1.8490068759418207e-05, 1.8490068759418207e-05, 2.4653425012557608e-05, 2.4653425012557608e-05, 3.2871233350076815e-05, 3.2871233350076815e-05, 4.382831113343575e-05, 4.382831113343575e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 17:06:12,480] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=19.304157868591773, CurrSamplesPerSec=23.62261745801174, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-10-23 17:06:29,232] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:06:29,232] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:06:29,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:06:29,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [20/54]  eta: 0:00:41  lr: 0.000044  min_lr: 0.000001  loss: 2.0222 (1.9955)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2208 (7.2169)  time: 0.8642 (0.5093 -- 4.8657)  data: 0.1856 (0.0002 -- 2.1199)  max mem: 16413
Epoch: [37]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 2.0277 (2.0482)  loss_scale: 32768.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3591 (7.0215)  time: 0.8600 (0.5198 -- 3.6577)  data: 0.3144 (0.0002 -- 3.1383)  max mem: 16413
Epoch: [37]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8886 (2.0128)  loss_scale: 32768.0000 (26699.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0562 (7.2306)  time: 0.6832 (0.4943 -- 2.2612)  data: 0.0875 (0.0001 -- 1.7320)  max mem: 16413
Epoch: [37] Total time: 0:00:50 (0.9393 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.8886 (1.9375)  loss_scale: 32768.0000 (26699.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0562 (7.2306)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9612 (0.9612)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9143 (1.9143 -- 1.9143)  data: 1.7173 (1.7173 -- 1.7173)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8638 (0.9491)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (95.1220)  time: 0.3310 (0.0221 -- 1.9143)  data: 0.1718 (0.0001 -- 1.7173)  max mem: 16413
Val: Total time: 0:00:03 (0.3312 s / it)
* Acc@1 66.463 Acc@5 95.732 loss 0.969
Accuracy of the network on the 163 val images: 66.46%
Max accuracy: 67.68%
Epoch: [38]  [ 0/54]  eta: 0:06:23  lr: 0.000044  min_lr: 0.000001  loss: 1.9956 (1.9956)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7771 (5.7771)  time: 7.1093 (7.1093 -- 7.1093)  data: 6.5654 (6.5654 -- 6.5654)  max mem: 16413
[2023-10-23 17:07:18,850] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2068
[2023-10-23 17:07:18,850] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2068
[2023-10-23 17:07:18,850] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:07:18,850] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:07:18,850] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [20/54]  eta: 0:00:39  lr: 0.000044  min_lr: 0.000001  loss: 1.8564 (1.8702)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8152 (7.1420)  time: 0.8507 (0.5189 -- 2.4853)  data: 0.1817 (0.0006 -- 1.9804)  max mem: 16413
Epoch: [38]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 1.9375 (1.9034)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8894 (7.7383)  time: 0.9820 (0.5221 -- 4.6251)  data: 0.4400 (0.0009 -- 4.0814)  max mem: 16413
Epoch: [38]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9589 (1.9141)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7245 (7.5011)  time: 0.6399 (0.4939 -- 2.3518)  data: 0.1197 (0.0002 -- 1.8374)  max mem: 16413
Epoch: [38] Total time: 0:00:50 (0.9439 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9589 (1.9053)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7245 (7.5011)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9109 (0.9109)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9892 (1.9892 -- 1.9892)  data: 1.8033 (1.8033 -- 1.8033)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8397 (0.9361)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (95.1220)  time: 0.3597 (0.0220 -- 1.9892)  data: 0.2029 (0.0001 -- 1.8033)  max mem: 16413
Val: Total time: 0:00:03 (0.3598 s / it)
* Acc@1 68.293 Acc@5 95.732 loss 0.951
Accuracy of the network on the 163 val images: 68.29%
[2023-10-23 17:07:52,447] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 17:07:52,449] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 17:07:52,449] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 17:07:52,449] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 17:07:53,659] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 17:07:53,659] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.29%
Epoch: [39]  [ 0/54]  eta: 0:06:37  lr: 0.000043  min_lr: 0.000001  loss: 2.1516 (2.1516)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8035 (7.8035)  time: 7.3621 (7.3621 -- 7.3621)  data: 4.5915 (4.5915 -- 4.5915)  max mem: 16413
Epoch: [39]  [20/54]  eta: 0:00:42  lr: 0.000043  min_lr: 0.000001  loss: 1.9053 (1.9032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6175 (6.9811)  time: 0.9513 (0.5136 -- 4.5350)  data: 0.3167 (0.0004 -- 4.0153)  max mem: 16413
Epoch: [39]  [40/54]  eta: 0:00:14  lr: 0.000043  min_lr: 0.000001  loss: 1.8779 (1.9024)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0102 (6.9255)  time: 0.8593 (0.5239 -- 4.4857)  data: 0.3139 (0.0003 -- 3.9699)  max mem: 16413
Epoch: [39]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9352 (1.9198)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0534 (6.8498)  time: 0.6343 (0.4951 -- 2.8159)  data: 0.1168 (0.0001 -- 2.2843)  max mem: 16413
Epoch: [39] Total time: 0:00:50 (0.9298 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9352 (1.9171)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0534 (6.8498)
[2023-10-23 17:08:43,873] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-10-23 17:08:43,875] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-39/mp_rank_00_model_states.pt
[2023-10-23 17:08:43,875] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-39/mp_rank_00_model_states.pt...
[2023-10-23 17:08:43,875] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-10-23 17:08:44,763] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-39/mp_rank_00_model_states.pt.
[2023-10-23 17:08:44,763] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9040 (0.9040)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0591 (2.0591 -- 2.0591)  data: 1.8804 (1.8804 -- 1.8804)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8635 (0.9112)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (96.3415)  time: 0.3443 (0.0227 -- 2.0591)  data: 0.1881 (0.0001 -- 1.8804)  max mem: 16413
Val: Total time: 0:00:03 (0.3444 s / it)
* Acc@1 68.902 Acc@5 96.341 loss 0.945
Accuracy of the network on the 163 val images: 68.90%
[2023-10-23 17:08:48,209] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 17:08:48,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 17:08:48,211] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 17:08:48,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 17:08:49,889] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 17:08:49,890] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.90%
Epoch: [40]  [ 0/54]  eta: 0:08:16  lr: 0.000043  min_lr: 0.000001  loss: 2.3857 (2.3857)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1212 (5.1212)  time: 9.2007 (9.2007 -- 9.2007)  data: 5.9125 (5.9125 -- 5.9125)  max mem: 16413
Epoch: [40]  [20/54]  eta: 0:00:43  lr: 0.000043  min_lr: 0.000001  loss: 1.9058 (2.0153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3097 (6.9196)  time: 0.8871 (0.5155 -- 3.1349)  data: 0.1537 (0.0005 -- 2.4136)  max mem: 16413
[2023-10-23 17:09:32,498] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:09:32,498] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:09:32,499] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:09:32,499] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [40/54]  eta: 0:00:15  lr: 0.000043  min_lr: 0.000001  loss: 1.8407 (1.9604)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4504 (6.6371)  time: 0.8654 (0.5150 -- 3.7081)  data: 0.2629 (0.0001 -- 3.2040)  max mem: 16413
Epoch: [40]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9285 (1.9445)  loss_scale: 32768.0000 (21541.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7894 (6.5197)  time: 0.7247 (0.4946 -- 2.6143)  data: 0.2076 (0.0001 -- 2.1021)  max mem: 16413
Epoch: [40] Total time: 0:00:52 (0.9791 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9285 (1.9042)  loss_scale: 32768.0000 (21541.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7894 (6.5197)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9637 (0.9637)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0917 (2.0917 -- 2.0917)  data: 1.9125 (1.9125 -- 1.9125)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8252 (0.9266)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (96.3415)  time: 0.3475 (0.0225 -- 2.0917)  data: 0.1913 (0.0001 -- 1.9125)  max mem: 16413
Val: Total time: 0:00:03 (0.3476 s / it)
* Acc@1 65.854 Acc@5 96.341 loss 0.975
Accuracy of the network on the 163 val images: 65.85%
Max accuracy: 68.90%
Epoch: [41]  [ 0/54]  eta: 0:07:46  lr: 0.000043  min_lr: 0.000001  loss: 2.2926 (2.2926)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0511 (6.0511)  time: 8.6358 (8.6358 -- 8.6358)  data: 8.1105 (8.1105 -- 8.1105)  max mem: 16413
Epoch: [41]  [20/54]  eta: 0:00:40  lr: 0.000043  min_lr: 0.000001  loss: 1.8997 (1.8565)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7818 (6.8140)  time: 0.8179 (0.5114 -- 4.3832)  data: 0.2738 (0.0004 -- 3.8430)  max mem: 16413
[2023-10-23 17:10:24,474] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2251
[2023-10-23 17:10:24,474] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2251
[2023-10-23 17:10:24,474] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:10:24,474] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:10:24,475] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [40/54]  eta: 0:00:15  lr: 0.000043  min_lr: 0.000001  loss: 1.7295 (1.8408)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4626 (7.3367)  time: 0.9548 (0.5221 -- 3.3302)  data: 0.2978 (0.0005 -- 2.8031)  max mem: 16413
Epoch: [41]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.7910 (1.8422)  loss_scale: 16384.0000 (27610.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0893 (7.1732)  time: 0.7493 (0.4940 -- 2.7402)  data: 0.1125 (0.0001 -- 2.0359)  max mem: 16413
Epoch: [41] Total time: 0:00:51 (0.9454 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.7910 (1.8416)  loss_scale: 16384.0000 (27610.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0893 (7.1732)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9890 (0.9890)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0595 (2.0595 -- 2.0595)  data: 1.8850 (1.8850 -- 1.8850)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8208 (0.9238)  acc1: 66.6667 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3444 (0.0220 -- 2.0595)  data: 0.1886 (0.0001 -- 1.8850)  max mem: 16413
Val: Total time: 0:00:03 (0.3445 s / it)
* Acc@1 65.854 Acc@5 96.341 loss 0.982
Accuracy of the network on the 163 val images: 65.85%
Max accuracy: 68.90%
Epoch: [42]  [ 0/54]  eta: 0:06:38  lr: 0.000043  min_lr: 0.000001  loss: 1.9813 (1.9813)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2846 (8.2846)  time: 7.3721 (7.3721 -- 7.3721)  data: 6.0830 (6.0830 -- 6.0830)  max mem: 16413
Epoch: [42]  [20/54]  eta: 0:00:42  lr: 0.000043  min_lr: 0.000001  loss: 1.7613 (1.7615)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5687 (7.9591)  time: 0.9467 (0.5307 -- 3.6630)  data: 0.0526 (0.0005 -- 1.0241)  max mem: 16413
Epoch: [42]  [40/54]  eta: 0:00:13  lr: 0.000043  min_lr: 0.000001  loss: 1.9358 (1.8518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5450 (7.4478)  time: 0.7323 (0.5207 -- 2.2176)  data: 0.0029 (0.0002 -- 0.0163)  max mem: 16413
Epoch: [42]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9799 (1.8684)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7817 (7.1468)  time: 0.7081 (0.4970 -- 3.1180)  data: 0.0016 (0.0001 -- 0.0163)  max mem: 16413
Epoch: [42] Total time: 0:00:51 (0.9469 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9799 (1.8764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7817 (7.1468)
Val:  [ 0/10]  eta: 0:00:18  loss: 0.9090 (0.9090)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.8094 (1.8094 -- 1.8094)  data: 1.6079 (1.6079 -- 1.6079)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8164 (0.8979)  acc1: 66.6667 (73.1707)  acc5: 100.0000 (95.1220)  time: 0.3196 (0.0222 -- 1.8094)  data: 0.1609 (0.0001 -- 1.6079)  max mem: 16413
Val: Total time: 0:00:03 (0.3197 s / it)
* Acc@1 67.683 Acc@5 95.732 loss 0.954
Accuracy of the network on the 163 val images: 67.68%
Max accuracy: 68.90%
Epoch: [43]  [ 0/54]  eta: 0:07:01  lr: 0.000043  min_lr: 0.000001  loss: 1.9222 (1.9222)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6366 (5.6366)  time: 7.8144 (7.8144 -- 7.8144)  data: 6.7091 (6.7091 -- 6.7091)  max mem: 16413
Epoch: [43]  [20/54]  eta: 0:00:41  lr: 0.000043  min_lr: 0.000001  loss: 1.9074 (1.9217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2380 (6.7822)  time: 0.8984 (0.5233 -- 3.9599)  data: 0.2900 (0.0003 -- 3.1812)  max mem: 16413
Epoch: [43]  [40/54]  eta: 0:00:14  lr: 0.000042  min_lr: 0.000001  loss: 1.9069 (1.9162)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0643 (7.1852)  time: 0.8257 (0.5280 -- 3.1234)  data: 0.2180 (0.0004 -- 2.5907)  max mem: 16413
Epoch: [43]  [53/54]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.9449 (1.8975)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5146 (7.2458)  time: 0.7205 (0.4947 -- 2.1178)  data: 0.1521 (0.0002 -- 1.6051)  max mem: 16413
Epoch: [43] Total time: 0:00:51 (0.9488 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.9449 (1.8878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5146 (7.2458)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9196 (0.9196)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0776 (2.0776 -- 2.0776)  data: 1.8968 (1.8968 -- 1.8968)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7988 (0.8836)  acc1: 66.6667 (68.2927)  acc5: 100.0000 (96.3415)  time: 0.3463 (0.0225 -- 2.0776)  data: 0.1898 (0.0001 -- 1.8968)  max mem: 16413
Val: Total time: 0:00:03 (0.3464 s / it)
* Acc@1 67.073 Acc@5 95.732 loss 0.941
Accuracy of the network on the 163 val images: 67.07%
Max accuracy: 68.90%
Epoch: [44]  [ 0/54]  eta: 0:07:40  lr: 0.000042  min_lr: 0.000001  loss: 2.0565 (2.0565)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6833 (8.6833)  time: 8.5254 (8.5254 -- 8.5254)  data: 7.9919 (7.9919 -- 7.9919)  max mem: 16413
[2023-10-23 17:12:41,173] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:12:41,173] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:12:41,176] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:12:41,177] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:12:47,435] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2385
[2023-10-23 17:12:47,435] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2385
[2023-10-23 17:12:47,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:12:47,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:12:47,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [44]  [20/54]  eta: 0:00:38  lr: 0.000042  min_lr: 0.000001  loss: 1.9092 (1.8913)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1402 (6.5968)  time: 0.7688 (0.5200 -- 3.9714)  data: 0.2252 (0.0006 -- 3.4456)  max mem: 16413
Epoch: [44]  [40/54]  eta: 0:00:14  lr: 0.000042  min_lr: 0.000001  loss: 1.7128 (1.8895)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7995 (6.7281)  time: 0.9145 (0.5166 -- 3.5144)  data: 0.3047 (0.0005 -- 2.3101)  max mem: 16413
Epoch: [44]  [53/54]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.7027 (1.8751)  loss_scale: 16384.0000 (17901.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3804 (6.6849)  time: 0.6924 (0.4979 -- 1.9872)  data: 0.1717 (0.0002 -- 1.4529)  max mem: 16413
Epoch: [44] Total time: 0:00:50 (0.9407 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.7027 (1.8854)  loss_scale: 16384.0000 (17901.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3804 (6.6849)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8921 (0.8921)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0726 (2.0726 -- 2.0726)  data: 1.8922 (1.8922 -- 1.8922)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7876 (0.8890)  acc1: 77.7778 (71.9512)  acc5: 100.0000 (93.9024)  time: 0.3446 (0.0221 -- 2.0726)  data: 0.1893 (0.0001 -- 1.8922)  max mem: 16413
Val: Total time: 0:00:03 (0.3447 s / it)
* Acc@1 67.683 Acc@5 95.122 loss 0.948
Accuracy of the network on the 163 val images: 67.68%
Max accuracy: 68.90%
Epoch: [45]  [ 0/54]  eta: 0:05:13  lr: 0.000042  min_lr: 0.000001  loss: 2.2561 (2.2561)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1522 (8.1522)  time: 5.7990 (5.7990 -- 5.7990)  data: 5.2624 (5.2624 -- 5.2624)  max mem: 16413
Epoch: [45]  [20/54]  eta: 0:00:43  lr: 0.000042  min_lr: 0.000001  loss: 1.9085 (1.9635)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2827 (6.6138)  time: 1.0562 (0.5249 -- 3.8870)  data: 0.5184 (0.0005 -- 3.3728)  max mem: 16413
Epoch: [45]  [40/54]  eta: 0:00:14  lr: 0.000042  min_lr: 0.000001  loss: 1.7570 (1.8878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0705 (6.9118)  time: 0.7525 (0.5148 -- 3.4488)  data: 0.2104 (0.0002 -- 2.9154)  max mem: 16413
Epoch: [45]  [53/54]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.8421 (1.8628)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5155 (6.8444)  time: 0.6444 (0.4982 -- 2.5897)  data: 0.1290 (0.0002 -- 2.0726)  max mem: 16413
Epoch: [45] Total time: 0:00:51 (0.9458 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.8421 (1.8516)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5155 (6.8444)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.8692 (0.8692)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1328 (2.1328 -- 2.1328)  data: 1.9554 (1.9554 -- 1.9554)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8257 (0.8643)  acc1: 77.7778 (71.9512)  acc5: 100.0000 (96.3415)  time: 0.3510 (0.0222 -- 2.1328)  data: 0.1956 (0.0001 -- 1.9554)  max mem: 16413
Val: Total time: 0:00:03 (0.3511 s / it)
* Acc@1 68.293 Acc@5 96.341 loss 0.914
Accuracy of the network on the 163 val images: 68.29%
Max accuracy: 68.90%
Epoch: [46]  [ 0/54]  eta: 0:07:44  lr: 0.000042  min_lr: 0.000001  loss: 2.4857 (2.4857)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3474 (7.3474)  time: 8.5993 (8.5993 -- 8.5993)  data: 5.6471 (5.6471 -- 5.6471)  max mem: 16413
Epoch: [46]  [20/54]  eta: 0:00:40  lr: 0.000042  min_lr: 0.000001  loss: 1.8433 (1.8396)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9396 (7.2308)  time: 0.8348 (0.5264 -- 3.5522)  data: 0.1483 (0.0003 -- 2.9037)  max mem: 16413
[2023-10-23 17:14:54,057] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:14:54,057] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:14:54,058] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:14:54,058] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [46]  [40/54]  eta: 0:00:15  lr: 0.000042  min_lr: 0.000001  loss: 1.8306 (1.8308)  loss_scale: 32768.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0121 (7.3912)  time: 0.9451 (0.5196 -- 4.4600)  data: 0.0871 (0.0003 -- 1.6082)  max mem: 16413
[2023-10-23 17:15:09,382] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2533
[2023-10-23 17:15:09,383] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:15:09,383] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2533
[2023-10-23 17:15:09,383] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:15:09,383] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [46]  [53/54]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.8428 (1.8441)  loss_scale: 32768.0000 (22148.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1286 (7.4388)  time: 0.7777 (0.4861 -- 4.4600)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [46] Total time: 0:00:52 (0.9646 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.8428 (1.8221)  loss_scale: 32768.0000 (22148.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1286 (7.4388)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8047 (0.8047)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9099 (1.9099 -- 1.9099)  data: 1.7045 (1.7045 -- 1.7045)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7803 (0.8499)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (95.1220)  time: 0.3329 (0.0222 -- 1.9099)  data: 0.1727 (0.0001 -- 1.7045)  max mem: 16413
Val: Total time: 0:00:03 (0.3331 s / it)
* Acc@1 66.463 Acc@5 95.732 loss 0.913
Accuracy of the network on the 163 val images: 66.46%
Max accuracy: 68.90%
Epoch: [47]  [ 0/54]  eta: 0:07:29  lr: 0.000042  min_lr: 0.000001  loss: 1.3086 (1.3086)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2873 (8.2873)  time: 8.3219 (8.3219 -- 8.3219)  data: 4.9834 (4.9834 -- 4.9834)  max mem: 16413
Epoch: [47]  [20/54]  eta: 0:00:39  lr: 0.000042  min_lr: 0.000001  loss: 1.9512 (1.9168)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8674 (7.7584)  time: 0.8185 (0.5224 -- 4.3710)  data: 0.0531 (0.0002 -- 0.7564)  max mem: 16413
Epoch: [47]  [40/54]  eta: 0:00:14  lr: 0.000042  min_lr: 0.000001  loss: 1.9034 (1.9069)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6090 (8.2202)  time: 0.9250 (0.5173 -- 3.8086)  data: 0.0683 (0.0005 -- 0.9664)  max mem: 16413
Epoch: [47]  [53/54]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.9233 (1.8981)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1199 (8.0256)  time: 0.7364 (0.4950 -- 3.8086)  data: 0.0008 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [47] Total time: 0:00:50 (0.9350 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.9233 (1.8610)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1199 (8.0256)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8691 (0.8691)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9752 (1.9752 -- 1.9752)  data: 1.7730 (1.7730 -- 1.7730)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7725 (0.8669)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (95.1220)  time: 0.3351 (0.0224 -- 1.9752)  data: 0.1774 (0.0001 -- 1.7730)  max mem: 16413
Val: Total time: 0:00:03 (0.3352 s / it)
* Acc@1 67.073 Acc@5 95.732 loss 0.930
Accuracy of the network on the 163 val images: 67.07%
Max accuracy: 68.90%
Epoch: [48]  [ 0/54]  eta: 0:07:03  lr: 0.000041  min_lr: 0.000001  loss: 2.1575 (2.1575)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6363 (5.6363)  time: 7.8346 (7.8346 -- 7.8346)  data: 5.1739 (5.1739 -- 5.1739)  max mem: 16413
Epoch: [48]  [20/54]  eta: 0:00:42  lr: 0.000041  min_lr: 0.000001  loss: 1.9211 (1.8947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6340 (6.9536)  time: 0.9266 (0.5235 -- 4.5146)  data: 0.3707 (0.0003 -- 4.0062)  max mem: 16413
Epoch: [48]  [40/54]  eta: 0:00:15  lr: 0.000041  min_lr: 0.000001  loss: 1.9660 (1.9040)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3494 (6.8159)  time: 0.9266 (0.5174 -- 3.8182)  data: 0.3857 (0.0002 -- 3.2804)  max mem: 16413
Epoch: [48]  [53/54]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.9360 (1.8958)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6751 (7.0182)  time: 0.5766 (0.4959 -- 1.7772)  data: 0.0637 (0.0001 -- 1.2638)  max mem: 16413
Epoch: [48] Total time: 0:00:51 (0.9532 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.9360 (1.9132)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6751 (7.0182)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8448 (0.8448)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9159 (1.9159 -- 1.9159)  data: 1.7172 (1.7172 -- 1.7172)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7500 (0.8514)  acc1: 66.6667 (74.3902)  acc5: 100.0000 (95.1220)  time: 0.3289 (0.0221 -- 1.9159)  data: 0.1718 (0.0001 -- 1.7172)  max mem: 16413
Val: Total time: 0:00:03 (0.3290 s / it)
* Acc@1 68.902 Acc@5 95.732 loss 0.908
Accuracy of the network on the 163 val images: 68.90%
Max accuracy: 68.90%
Epoch: [49]  [ 0/54]  eta: 0:06:24  lr: 0.000041  min_lr: 0.000001  loss: 2.2032 (2.2032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6070 (7.6070)  time: 7.1144 (7.1144 -- 7.1144)  data: 5.4284 (5.4284 -- 5.4284)  max mem: 16413
[2023-10-23 17:17:27,566] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:17:27,567] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:17:27,567] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:17:27,567] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [49]  [20/54]  eta: 0:00:41  lr: 0.000041  min_lr: 0.000001  loss: 1.8730 (1.8042)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8854 (6.5388)  time: 0.9276 (0.5251 -- 5.3387)  data: 0.1115 (0.0004 -- 1.9714)  max mem: 16413
Epoch: [49]  [40/54]  eta: 0:00:14  lr: 0.000041  min_lr: 0.000001  loss: 1.8840 (1.8720)  loss_scale: 32768.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5581 (6.8449)  time: 0.9115 (0.5196 -- 4.9003)  data: 0.0012 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [49]  [53/54]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.8375 (1.8881)  loss_scale: 32768.0000 (27913.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4755 (6.8642)  time: 0.6484 (0.4936 -- 1.7419)  data: 0.0725 (0.0001 -- 0.9667)  max mem: 16413
Epoch: [49] Total time: 0:00:51 (0.9612 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.8375 (1.8696)  loss_scale: 32768.0000 (27913.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4755 (6.8642)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8705 (0.8705)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9585 (1.9585 -- 1.9585)  data: 1.7520 (1.7520 -- 1.7520)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7637 (0.8779)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (95.1220)  time: 0.3361 (0.0221 -- 1.9585)  data: 0.1776 (0.0001 -- 1.7520)  max mem: 16413
Val: Total time: 0:00:03 (0.3362 s / it)
* Acc@1 65.244 Acc@5 95.732 loss 0.941
Accuracy of the network on the 163 val images: 65.24%
Max accuracy: 68.90%
Epoch: [50]  [ 0/54]  eta: 0:07:09  lr: 0.000041  min_lr: 0.000001  loss: 2.2354 (2.2354)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9141 (8.9141)  time: 7.9521 (7.9521 -- 7.9521)  data: 7.1542 (7.1542 -- 7.1542)  max mem: 16413
[2023-10-23 17:18:08,921] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2703
[2023-10-23 17:18:08,922] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2703
[2023-10-23 17:18:08,922] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:18:08,922] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:18:08,922] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [50]  [20/54]  eta: 0:00:41  lr: 0.000041  min_lr: 0.000001  loss: 1.7519 (1.8437)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3675 (7.3452)  time: 0.8955 (0.5126 -- 4.6992)  data: 0.0388 (0.0003 -- 0.7402)  max mem: 16413
Epoch: [50]  [40/54]  eta: 0:00:15  lr: 0.000041  min_lr: 0.000001  loss: 1.6507 (1.7606)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4709 (7.1401)  time: 0.9453 (0.5173 -- 4.2071)  data: 0.0013 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [50]  [53/54]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.6555 (1.7818)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6811 (7.0155)  time: 0.6557 (0.4939 -- 2.6720)  data: 0.0007 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [50] Total time: 0:00:52 (0.9634 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.6555 (1.8457)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6811 (7.0155)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.8144 (0.8144)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1232 (2.1232 -- 2.1232)  data: 1.9447 (1.9447 -- 1.9447)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7613 (0.8763)  acc1: 77.7778 (75.6098)  acc5: 100.0000 (93.9024)  time: 0.3509 (0.0220 -- 2.1232)  data: 0.1945 (0.0001 -- 1.9447)  max mem: 16413
Val: Total time: 0:00:03 (0.3510 s / it)
* Acc@1 70.122 Acc@5 95.122 loss 0.926
Accuracy of the network on the 163 val images: 70.12%
[2023-10-23 17:18:54,864] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 17:18:54,866] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 17:18:54,866] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 17:18:54,866] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 17:18:56,471] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 17:18:56,472] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.12%
Epoch: [51]  [ 0/54]  eta: 0:06:52  lr: 0.000041  min_lr: 0.000001  loss: 1.2763 (1.2763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3898 (12.3898)  time: 7.6363 (7.6363 -- 7.6363)  data: 6.4207 (6.4207 -- 6.4207)  max mem: 16413
Epoch: [51]  [20/54]  eta: 0:00:40  lr: 0.000041  min_lr: 0.000001  loss: 1.9083 (1.8907)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5960 (7.2594)  time: 0.8776 (0.5295 -- 3.9619)  data: 0.0844 (0.0001 -- 0.9544)  max mem: 16413
Epoch: [51]  [40/54]  eta: 0:00:14  lr: 0.000041  min_lr: 0.000001  loss: 1.5422 (1.7690)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8536 (7.5825)  time: 0.9065 (0.5183 -- 3.9911)  data: 0.1225 (0.0003 -- 1.4442)  max mem: 16413
Epoch: [51]  [53/54]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.6395 (1.7780)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1413 (7.5744)  time: 0.6514 (0.4965 -- 1.8985)  data: 0.0248 (0.0001 -- 0.2804)  max mem: 16413
Epoch: [51] Total time: 0:00:51 (0.9556 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.6395 (1.8358)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1413 (7.5744)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8439 (0.8439)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0187 (2.0187 -- 2.0187)  data: 1.8348 (1.8348 -- 1.8348)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8205 (0.8963)  acc1: 66.6667 (75.6098)  acc5: 100.0000 (95.1220)  time: 0.3389 (0.0222 -- 2.0187)  data: 0.1836 (0.0001 -- 1.8348)  max mem: 16413
Val: Total time: 0:00:03 (0.3390 s / it)
* Acc@1 69.512 Acc@5 95.732 loss 0.965
Accuracy of the network on the 163 val images: 69.51%
Max accuracy: 70.12%
Epoch: [52]  [ 0/54]  eta: 0:07:56  lr: 0.000040  min_lr: 0.000001  loss: 2.1587 (2.1587)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6771 (7.6771)  time: 8.8257 (8.8257 -- 8.8257)  data: 8.2798 (8.2798 -- 8.2798)  max mem: 16413
Epoch: [52]  [20/54]  eta: 0:00:40  lr: 0.000040  min_lr: 0.000001  loss: 1.8290 (1.8184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8553 (7.2126)  time: 0.8147 (0.5105 -- 3.7537)  data: 0.2706 (0.0008 -- 3.2342)  max mem: 16413
[2023-10-23 17:20:21,377] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:20:21,377] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:20:21,377] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:20:21,377] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:20:25,253] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2839
[2023-10-23 17:20:25,253] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2839
[2023-10-23 17:20:25,253] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:20:25,253] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:20:25,253] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [52]  [40/54]  eta: 0:00:14  lr: 0.000040  min_lr: 0.000001  loss: 1.6790 (1.7583)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8392 (7.1623)  time: 0.8520 (0.5227 -- 3.1654)  data: 0.2373 (0.0009 -- 2.6473)  max mem: 16413
Epoch: [52]  [53/54]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.6790 (1.7581)  loss_scale: 16384.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8392 (7.1078)  time: 0.6894 (0.4955 -- 2.5695)  data: 0.0376 (0.0001 -- 0.7404)  max mem: 16413
Epoch: [52] Total time: 0:00:51 (0.9525 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.6790 (1.8176)  loss_scale: 16384.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8392 (7.1078)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8028 (0.8028)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0865 (2.0865 -- 2.0865)  data: 1.9136 (1.9136 -- 1.9136)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8028 (0.8483)  acc1: 66.6667 (75.6098)  acc5: 100.0000 (93.9024)  time: 0.3468 (0.0225 -- 2.0865)  data: 0.1915 (0.0001 -- 1.9136)  max mem: 16413
Val: Total time: 0:00:03 (0.3469 s / it)
* Acc@1 68.902 Acc@5 95.122 loss 0.905
Accuracy of the network on the 163 val images: 68.90%
Max accuracy: 70.12%
Epoch: [53]  [ 0/54]  eta: 0:08:03  lr: 0.000040  min_lr: 0.000001  loss: 1.6355 (1.6355)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9886 (7.9886)  time: 8.9514 (8.9514 -- 8.9514)  data: 8.4258 (8.4258 -- 8.4258)  max mem: 16413
Epoch: [53]  [20/54]  eta: 0:00:40  lr: 0.000040  min_lr: 0.000001  loss: 1.8163 (1.7986)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6020 (6.8781)  time: 0.8052 (0.5244 -- 3.1421)  data: 0.2557 (0.0002 -- 2.6166)  max mem: 16413
Epoch: [53]  [40/54]  eta: 0:00:14  lr: 0.000040  min_lr: 0.000001  loss: 1.8418 (1.7992)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1632 (7.9020)  time: 0.9398 (0.5275 -- 4.4524)  data: 0.3967 (0.0004 -- 3.9349)  max mem: 16413
Epoch: [53]  [53/54]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.8495 (1.8191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8558 (7.8530)  time: 0.6277 (0.4932 -- 2.2538)  data: 0.1060 (0.0001 -- 1.7195)  max mem: 16413
Epoch: [53] Total time: 0:00:50 (0.9418 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.8495 (1.8501)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8558 (7.8530)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.8552 (0.8552)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1408 (2.1408 -- 2.1408)  data: 1.9624 (1.9624 -- 1.9624)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7832 (0.8499)  acc1: 66.6667 (73.1707)  acc5: 100.0000 (95.1220)  time: 0.3507 (0.0220 -- 2.1408)  data: 0.1963 (0.0001 -- 1.9624)  max mem: 16413
Val: Total time: 0:00:03 (0.3508 s / it)
* Acc@1 67.683 Acc@5 95.732 loss 0.925
Accuracy of the network on the 163 val images: 67.68%
Max accuracy: 70.12%
Epoch: [54]  [ 0/54]  eta: 0:06:31  lr: 0.000040  min_lr: 0.000001  loss: 2.2748 (2.2748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3493 (12.3493)  time: 7.2503 (7.2503 -- 7.2503)  data: 6.7211 (6.7211 -- 6.7211)  max mem: 16413
Epoch: [54]  [20/54]  eta: 0:00:39  lr: 0.000040  min_lr: 0.000001  loss: 1.8482 (1.8733)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3621 (8.0654)  time: 0.8536 (0.5256 -- 3.3742)  data: 0.3073 (0.0003 -- 2.8457)  max mem: 16413
Epoch: [54]  [40/54]  eta: 0:00:15  lr: 0.000040  min_lr: 0.000001  loss: 1.7244 (1.8077)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8805 (7.6213)  time: 0.9836 (0.5192 -- 3.9878)  data: 0.4419 (0.0006 -- 3.4315)  max mem: 16413
[2023-10-23 17:22:32,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:22:32,585] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:22:32,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:22:32,586] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [54]  [53/54]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.7244 (1.8348)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4403 (7.4351)  time: 0.6698 (0.4956 -- 1.7770)  data: 0.1508 (0.0002 -- 1.2565)  max mem: 16413
Epoch: [54] Total time: 0:00:52 (0.9696 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.7244 (1.8163)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4403 (7.4351)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9042 (0.9042)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 1.9677 (1.9677 -- 1.9677)  data: 1.7831 (1.7831 -- 1.7831)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7769 (0.8508)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (95.1220)  time: 0.3344 (0.0221 -- 1.9677)  data: 0.1784 (0.0001 -- 1.7831)  max mem: 16413
Val: Total time: 0:00:03 (0.3345 s / it)
* Acc@1 66.463 Acc@5 95.732 loss 0.941
Accuracy of the network on the 163 val images: 66.46%
Max accuracy: 70.12%
Epoch: [55]  [ 0/54]  eta: 0:07:16  lr: 0.000040  min_lr: 0.000001  loss: 1.9959 (1.9959)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2335 (6.2335)  time: 8.0781 (8.0781 -- 8.0781)  data: 7.5327 (7.5327 -- 7.5327)  max mem: 16413
[2023-10-23 17:23:00,792] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2988
[2023-10-23 17:23:00,792] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2988
[2023-10-23 17:23:00,793] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:23:00,793] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:23:00,793] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [55]  [20/54]  eta: 0:00:41  lr: 0.000040  min_lr: 0.000001  loss: 1.9606 (1.9092)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3313 (6.2248)  time: 0.8672 (0.5126 -- 2.6390)  data: 0.1850 (0.0007 -- 1.5473)  max mem: 16413
[2023-10-23 17:23:09,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=13, lr=[9.392130821382883e-07, 9.392130821382883e-07, 1.2522841095177177e-06, 1.2522841095177177e-06, 1.6697121460236236e-06, 1.6697121460236236e-06, 2.2262828613648317e-06, 2.2262828613648317e-06, 2.968377148486442e-06, 2.968377148486442e-06, 3.9578361979819225e-06, 3.9578361979819225e-06, 5.277114930642564e-06, 5.277114930642564e-06, 7.036153240856751e-06, 7.036153240856751e-06, 9.38153765447567e-06, 9.38153765447567e-06, 1.2508716872634225e-05, 1.2508716872634225e-05, 1.66782891635123e-05, 1.66782891635123e-05, 2.2237718884683065e-05, 2.2237718884683065e-05, 2.9650291846244087e-05, 2.9650291846244087e-05, 3.9533722461658785e-05, 3.9533722461658785e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 17:23:09,220] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=19.103100883642828, CurrSamplesPerSec=21.574565050777654, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [55]  [40/54]  eta: 0:00:14  lr: 0.000039  min_lr: 0.000001  loss: 1.8208 (1.8568)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4483 (7.0116)  time: 0.9190 (0.5196 -- 3.4394)  data: 0.2158 (0.0003 -- 2.2320)  max mem: 16413
Epoch: [55]  [53/54]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.7714 (1.8471)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5631 (7.0688)  time: 0.6259 (0.4985 -- 2.0300)  data: 0.0354 (0.0001 -- 0.6981)  max mem: 16413
Epoch: [55] Total time: 0:00:52 (0.9745 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.7714 (1.8501)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5631 (7.0688)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8306 (0.8306)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0818 (2.0818 -- 2.0818)  data: 1.9048 (1.9048 -- 1.9048)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7710 (0.8360)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (95.1220)  time: 0.3454 (0.0220 -- 2.0818)  data: 0.1906 (0.0001 -- 1.9048)  max mem: 16413
Val: Total time: 0:00:03 (0.3455 s / it)
* Acc@1 69.512 Acc@5 95.732 loss 0.919
Accuracy of the network on the 163 val images: 69.51%
Max accuracy: 70.12%
Epoch: [56]  [ 0/54]  eta: 0:06:18  lr: 0.000039  min_lr: 0.000001  loss: 1.3722 (1.3722)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9405 (7.9405)  time: 7.0088 (7.0088 -- 7.0088)  data: 6.4898 (6.4898 -- 6.4898)  max mem: 16413
Epoch: [56]  [20/54]  eta: 0:00:43  lr: 0.000039  min_lr: 0.000001  loss: 1.8121 (1.8170)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0643 (7.1579)  time: 0.9791 (0.5185 -- 3.8638)  data: 0.4007 (0.0005 -- 3.3368)  max mem: 16413
Epoch: [56]  [40/54]  eta: 0:00:14  lr: 0.000039  min_lr: 0.000001  loss: 1.8587 (1.8537)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5579 (7.6217)  time: 0.8201 (0.5216 -- 3.1914)  data: 0.2781 (0.0004 -- 2.6771)  max mem: 16413
Epoch: [56]  [53/54]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.7895 (1.8618)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5579 (7.6876)  time: 0.7144 (0.4996 -- 3.1914)  data: 0.1936 (0.0002 -- 2.6771)  max mem: 16413
Epoch: [56] Total time: 0:00:50 (0.9410 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.7895 (1.8095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5579 (7.6876)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8618 (0.8618)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0006 (2.0006 -- 2.0006)  data: 1.8202 (1.8202 -- 1.8202)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7954 (0.8511)  acc1: 66.6667 (69.5122)  acc5: 100.0000 (95.1220)  time: 0.3373 (0.0220 -- 2.0006)  data: 0.1821 (0.0001 -- 1.8202)  max mem: 16413
Val: Total time: 0:00:03 (0.3375 s / it)
* Acc@1 67.683 Acc@5 95.732 loss 0.926
Accuracy of the network on the 163 val images: 67.68%
Max accuracy: 70.12%
Epoch: [57]  [ 0/54]  eta: 0:06:37  lr: 0.000039  min_lr: 0.000001  loss: 2.0269 (2.0269)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4587 (6.4587)  time: 7.3659 (7.3659 -- 7.3659)  data: 6.8042 (6.8042 -- 6.8042)  max mem: 16413
Epoch: [57]  [20/54]  eta: 0:00:43  lr: 0.000039  min_lr: 0.000001  loss: 1.9604 (1.8920)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4599 (8.0886)  time: 0.9789 (0.5172 -- 5.9405)  data: 0.4306 (0.0004 -- 5.4155)  max mem: 16413
[2023-10-23 17:25:11,415] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:25:11,415] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:25:11,415] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:25:11,415] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [40/54]  eta: 0:00:15  lr: 0.000039  min_lr: 0.000001  loss: 1.8488 (1.8612)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4297 (7.5248)  time: 0.9581 (0.4996 -- 4.5118)  data: 0.4332 (0.0002 -- 3.9944)  max mem: 16413
Epoch: [57]  [53/54]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.8170 (1.8313)  loss_scale: 32768.0000 (20935.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2800 (7.3772)  time: 0.5693 (0.4938 -- 1.4369)  data: 0.0605 (0.0001 -- 0.9193)  max mem: 16413
Epoch: [57] Total time: 0:00:52 (0.9799 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.8170 (1.8338)  loss_scale: 32768.0000 (20935.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2800 (7.3772)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8510 (0.8510)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0343 (2.0343 -- 2.0343)  data: 1.8551 (1.8551 -- 1.8551)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7900 (0.8228)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (95.1220)  time: 0.3421 (0.0221 -- 2.0343)  data: 0.1856 (0.0001 -- 1.8551)  max mem: 16413
Val: Total time: 0:00:03 (0.3422 s / it)
* Acc@1 69.512 Acc@5 95.732 loss 0.900
Accuracy of the network on the 163 val images: 69.51%
Max accuracy: 70.12%
Epoch: [58]  [ 0/54]  eta: 0:07:23  lr: 0.000039  min_lr: 0.000001  loss: 1.8777 (1.8777)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9747 (6.9747)  time: 8.2193 (8.2193 -- 8.2193)  data: 5.8661 (5.8661 -- 5.8661)  max mem: 16413
Epoch: [58]  [20/54]  eta: 0:00:39  lr: 0.000039  min_lr: 0.000001  loss: 1.6957 (1.7811)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6737 (7.0949)  time: 0.8118 (0.5292 -- 3.1495)  data: 0.0200 (0.0003 -- 0.3690)  max mem: 16413
[2023-10-23 17:25:56,664] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3162
[2023-10-23 17:25:56,664] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:25:56,664] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3162
[2023-10-23 17:25:56,664] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:25:56,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [40/54]  eta: 0:00:15  lr: 0.000039  min_lr: 0.000001  loss: 1.8884 (1.8193)  loss_scale: 16384.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0429 (7.1099)  time: 0.9796 (0.5232 -- 4.2468)  data: 0.0870 (0.0005 -- 1.1428)  max mem: 16413
Epoch: [58]  [53/54]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.8773 (1.8301)  loss_scale: 16384.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1861 (7.3065)  time: 0.6982 (0.4946 -- 3.7323)  data: 0.0280 (0.0001 -- 0.5443)  max mem: 16413
Epoch: [58] Total time: 0:00:50 (0.9383 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.8773 (1.8521)  loss_scale: 16384.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1861 (7.3065)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7782 (0.7782)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1025 (2.1025 -- 2.1025)  data: 1.9304 (1.9304 -- 1.9304)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7622 (0.8097)  acc1: 66.6667 (75.6098)  acc5: 100.0000 (95.1220)  time: 0.3497 (0.0222 -- 2.1025)  data: 0.1931 (0.0001 -- 1.9304)  max mem: 16413
Val: Total time: 0:00:03 (0.3498 s / it)
* Acc@1 68.293 Acc@5 95.732 loss 0.885
Accuracy of the network on the 163 val images: 68.29%
Max accuracy: 70.12%
Epoch: [59]  [ 0/54]  eta: 0:05:55  lr: 0.000039  min_lr: 0.000001  loss: 1.3841 (1.3841)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5411 (9.5411)  time: 6.5836 (6.5836 -- 6.5836)  data: 6.0582 (6.0582 -- 6.0582)  max mem: 16413
Epoch: [59]  [20/54]  eta: 0:00:38  lr: 0.000038  min_lr: 0.000001  loss: 1.9800 (1.8875)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3235 (7.5720)  time: 0.8452 (0.5104 -- 3.2355)  data: 0.2920 (0.0002 -- 2.7187)  max mem: 16413
Epoch: [59]  [40/54]  eta: 0:00:14  lr: 0.000038  min_lr: 0.000001  loss: 1.8797 (1.8647)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7262 (7.5254)  time: 0.9602 (0.5191 -- 2.6361)  data: 0.4171 (0.0004 -- 2.1084)  max mem: 16413
Epoch: [59]  [53/54]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.8339 (1.8882)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6705 (7.2381)  time: 0.7191 (0.4968 -- 2.6148)  data: 0.1946 (0.0001 -- 2.0949)  max mem: 16413
Epoch: [59] Total time: 0:00:49 (0.9236 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.8339 (1.8899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6705 (7.2381)
[2023-10-23 17:27:07,133] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-10-23 17:27:07,135] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-59/mp_rank_00_model_states.pt
[2023-10-23 17:27:07,135] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-59/mp_rank_00_model_states.pt...
[2023-10-23 17:27:07,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-10-23 17:27:08,018] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-59/mp_rank_00_model_states.pt.
[2023-10-23 17:27:08,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9026 (0.9026)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0208 (2.0208 -- 2.0208)  data: 1.8268 (1.8268 -- 1.8268)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7803 (0.8223)  acc1: 66.6667 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3416 (0.0225 -- 2.0208)  data: 0.1828 (0.0001 -- 1.8268)  max mem: 16413
Val: Total time: 0:00:03 (0.3417 s / it)
* Acc@1 67.073 Acc@5 95.732 loss 0.937
Accuracy of the network on the 163 val images: 67.07%
Max accuracy: 70.12%
Epoch: [60]  [ 0/54]  eta: 0:08:12  lr: 0.000038  min_lr: 0.000001  loss: 2.2185 (2.2185)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3889 (5.3889)  time: 9.1293 (9.1293 -- 9.1293)  data: 8.5993 (8.5993 -- 8.5993)  max mem: 16413
Epoch: [60]  [20/54]  eta: 0:00:44  lr: 0.000038  min_lr: 0.000001  loss: 1.7212 (1.8542)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9931 (8.0226)  time: 0.9085 (0.5306 -- 4.7776)  data: 0.3606 (0.0004 -- 4.2309)  max mem: 16413
Epoch: [60]  [40/54]  eta: 0:00:15  lr: 0.000038  min_lr: 0.000001  loss: 1.7152 (1.7791)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3028 (7.4724)  time: 0.8339 (0.5226 -- 3.4519)  data: 0.2949 (0.0005 -- 2.9081)  max mem: 16413
[2023-10-23 17:28:00,999] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:28:00,999] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:28:01,000] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:28:01,040] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [60]  [53/54]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.7458 (1.7842)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8678 (7.4912)  time: 0.5569 (0.4956 -- 1.3027)  data: 0.0397 (0.0001 -- 0.7796)  max mem: 16413
Epoch: [60] Total time: 0:00:50 (0.9374 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.7458 (1.7981)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8678 (7.4912)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8067 (0.8067)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0477 (2.0477 -- 2.0477)  data: 1.8438 (1.8438 -- 1.8438)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7656 (0.8090)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (93.9024)  time: 0.3420 (0.0221 -- 2.0477)  data: 0.1845 (0.0001 -- 1.8438)  max mem: 16413
Val: Total time: 0:00:03 (0.3421 s / it)
* Acc@1 68.293 Acc@5 95.122 loss 0.891
Accuracy of the network on the 163 val images: 68.29%
Max accuracy: 70.12%
Epoch: [61]  [ 0/54]  eta: 0:06:48  lr: 0.000038  min_lr: 0.000001  loss: 2.1488 (2.1488)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1702 (5.1702)  time: 7.5612 (7.5612 -- 7.5612)  data: 7.0261 (7.0261 -- 7.0261)  max mem: 16413
[2023-10-23 17:28:22,407] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3304
[2023-10-23 17:28:22,407] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3304
[2023-10-23 17:28:22,408] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:28:22,408] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:28:22,408] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [61]  [20/54]  eta: 0:00:43  lr: 0.000038  min_lr: 0.000001  loss: 1.8271 (1.8424)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7539 (6.9154)  time: 0.9796 (0.5081 -- 5.3575)  data: 0.4348 (0.0003 -- 4.8093)  max mem: 16413
Epoch: [61]  [40/54]  eta: 0:00:14  lr: 0.000038  min_lr: 0.000001  loss: 1.8578 (1.8661)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2841 (8.2324)  time: 0.7962 (0.5153 -- 2.5780)  data: 0.2567 (0.0002 -- 2.0578)  max mem: 16413
[2023-10-23 17:28:54,232] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3343
[2023-10-23 17:28:54,233] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 17:28:54,232] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3343
[2023-10-23 17:28:54,233] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 17:28:54,233] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [61]  [53/54]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.7977 (1.8388)  loss_scale: 16384.0000 (18659.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9004 (7.8687)  time: 0.6269 (0.4823 -- 1.6064)  data: 0.1112 (0.0002 -- 1.1038)  max mem: 16413
Epoch: [61] Total time: 0:00:50 (0.9397 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.7977 (1.8091)  loss_scale: 16384.0000 (18659.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9004 (7.8687)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8427 (0.8427)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 1.9969 (1.9969 -- 1.9969)  data: 1.7959 (1.7959 -- 1.7959)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7944 (0.8024)  acc1: 66.6667 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3374 (0.0226 -- 1.9969)  data: 0.1797 (0.0001 -- 1.7959)  max mem: 16413
Val: Total time: 0:00:03 (0.3376 s / it)
* Acc@1 66.463 Acc@5 95.122 loss 0.894
Accuracy of the network on the 163 val images: 66.46%
Max accuracy: 70.12%
Epoch: [62]  [ 0/54]  eta: 0:05:33  lr: 0.000038  min_lr: 0.000001  loss: 1.7454 (1.7454)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8790 (3.8790)  time: 6.1721 (6.1721 -- 6.1721)  data: 5.6047 (5.6047 -- 5.6047)  max mem: 16413
Epoch: [62]  [20/54]  eta: 0:00:41  lr: 0.000038  min_lr: 0.000001  loss: 1.9075 (1.8645)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2511 (7.1842)  time: 0.9582 (0.5304 -- 4.5922)  data: 0.1608 (0.0010 -- 1.9493)  max mem: 16413
Epoch: [62]  [40/54]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000001  loss: 1.9081 (1.8580)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9154 (7.4351)  time: 0.8945 (0.5275 -- 2.8802)  data: 0.0914 (0.0005 -- 1.4119)  max mem: 16413
Epoch: [62]  [53/54]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.7691 (1.8310)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2935 (7.3678)  time: 0.6808 (0.4942 -- 2.8802)  data: 0.0850 (0.0002 -- 1.4119)  max mem: 16413
Epoch: [62] Total time: 0:00:50 (0.9386 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.7691 (1.8445)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2935 (7.3678)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8585 (0.8585)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0151 (2.0151 -- 2.0151)  data: 1.8329 (1.8329 -- 1.8329)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7842 (0.8057)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3392 (0.0220 -- 2.0151)  data: 0.1834 (0.0001 -- 1.8329)  max mem: 16413
Val: Total time: 0:00:03 (0.3393 s / it)
* Acc@1 68.293 Acc@5 96.341 loss 0.919
Accuracy of the network on the 163 val images: 68.29%
Max accuracy: 70.12%
Epoch: [63]  [ 0/54]  eta: 0:06:54  lr: 0.000037  min_lr: 0.000001  loss: 1.8834 (1.8834)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3884 (8.3884)  time: 7.6785 (7.6785 -- 7.6785)  data: 6.7080 (6.7080 -- 6.7080)  max mem: 16413
Epoch: [63]  [20/54]  eta: 0:00:39  lr: 0.000037  min_lr: 0.000001  loss: 1.9544 (1.9307)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1049 (8.3072)  time: 0.8333 (0.5226 -- 3.2526)  data: 0.2719 (0.0005 -- 2.6902)  max mem: 16413
Epoch: [63]  [40/54]  eta: 0:00:15  lr: 0.000037  min_lr: 0.000001  loss: 1.7990 (1.8647)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1616 (7.9063)  time: 1.0201 (0.5189 -- 4.9942)  data: 0.4839 (0.0003 -- 4.4764)  max mem: 16413
Epoch: [63]  [53/54]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.6983 (1.8294)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5803 (7.8939)  time: 0.6428 (0.4934 -- 2.4476)  data: 0.1287 (0.0001 -- 1.9217)  max mem: 16413
Epoch: [63] Total time: 0:00:51 (0.9614 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.6983 (1.8228)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5803 (7.8939)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7690 (0.7690)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9717 (1.9717 -- 1.9717)  data: 1.7602 (1.7602 -- 1.7602)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7206 (0.7817)  acc1: 66.6667 (76.8293)  acc5: 100.0000 (96.3415)  time: 0.3361 (0.0219 -- 1.9717)  data: 0.1761 (0.0001 -- 1.7602)  max mem: 16413
Val: Total time: 0:00:03 (0.3362 s / it)
* Acc@1 69.512 Acc@5 96.341 loss 0.854
Accuracy of the network on the 163 val images: 69.51%
Max accuracy: 70.12%
Epoch: [64]  [ 0/54]  eta: 0:07:31  lr: 0.000037  min_lr: 0.000001  loss: 2.0446 (2.0446)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8867 (10.8867)  time: 8.3527 (8.3527 -- 8.3527)  data: 6.4154 (6.4154 -- 6.4154)  max mem: 16413
[2023-10-23 17:31:11,475] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:31:11,475] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 17:31:11,478] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:31:11,479] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [64]  [20/54]  eta: 0:00:40  lr: 0.000037  min_lr: 0.000001  loss: 1.7743 (1.7160)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7025 (8.3189)  time: 0.8196 (0.5288 -- 4.7743)  data: 0.0785 (0.0003 -- 1.5376)  max mem: 16413
Epoch: [64]  [40/54]  eta: 0:00:15  lr: 0.000037  min_lr: 0.000001  loss: 1.7565 (1.7140)  loss_scale: 16384.0000 (13187.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8010 (8.4096)  time: 1.0056 (0.5325 -- 3.8140)  data: 0.0308 (0.0002 -- 0.5904)  max mem: 16413
Epoch: [64]  [53/54]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.7601 (1.7564)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5509 (8.4633)  time: 0.6477 (0.4938 -- 2.7947)  data: 0.0008 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [64] Total time: 0:00:51 (0.9577 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.7601 (1.7897)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5509 (8.4633)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7296 (0.7296)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1186 (2.1186 -- 2.1186)  data: 1.9479 (1.9479 -- 1.9479)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7296 (0.7779)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (95.1220)  time: 0.3499 (0.0221 -- 2.1186)  data: 0.1949 (0.0001 -- 1.9479)  max mem: 16413
Val: Total time: 0:00:03 (0.3500 s / it)
* Acc@1 70.122 Acc@5 95.732 loss 0.852
Accuracy of the network on the 163 val images: 70.12%
Max accuracy: 70.12%
Epoch: [65]  [ 0/54]  eta: 0:06:15  lr: 0.000037  min_lr: 0.000001  loss: 1.8009 (1.8009)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4013 (6.4013)  time: 6.9563 (6.9563 -- 6.9563)  data: 4.9444 (4.9444 -- 4.9444)  max mem: 16413
Epoch: [65]  [20/54]  eta: 0:00:41  lr: 0.000037  min_lr: 0.000001  loss: 1.7520 (1.7518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9734 (7.7534)  time: 0.9429 (0.5222 -- 3.3156)  data: 0.3670 (0.0008 -- 2.0706)  max mem: 16413
Epoch: [65]  [40/54]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000001  loss: 1.7796 (1.7752)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5635 (7.9553)  time: 0.8684 (0.5207 -- 4.6107)  data: 0.3262 (0.0001 -- 4.0885)  max mem: 16413
Epoch: [65]  [53/54]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.8438 (1.8023)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3025 (8.0001)  time: 0.6877 (0.4965 -- 3.3358)  data: 0.1734 (0.0002 -- 2.8262)  max mem: 16413
Epoch: [65] Total time: 0:00:53 (0.9853 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.8438 (1.8314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3025 (8.0001)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7715 (0.7715)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0147 (2.0147 -- 2.0147)  data: 1.8273 (1.8273 -- 1.8273)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7474 (0.7739)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (97.5610)  time: 0.3386 (0.0220 -- 2.0147)  data: 0.1828 (0.0001 -- 1.8273)  max mem: 16413
Val: Total time: 0:00:03 (0.3387 s / it)
* Acc@1 70.732 Acc@5 96.951 loss 0.857
Accuracy of the network on the 163 val images: 70.73%
[2023-10-23 17:32:40,801] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 17:32:40,802] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 17:32:40,803] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 17:32:40,803] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 17:32:42,260] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 17:32:42,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.73%
Epoch: [66]  [ 0/54]  eta: 0:06:03  lr: 0.000036  min_lr: 0.000001  loss: 2.2255 (2.2255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5263 (7.5263)  time: 6.7228 (6.7228 -- 6.7228)  data: 5.6276 (5.6276 -- 5.6276)  max mem: 16413
Epoch: [66]  [20/54]  eta: 0:00:39  lr: 0.000036  min_lr: 0.000001  loss: 1.7644 (1.7868)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2365 (7.7084)  time: 0.8982 (0.5329 -- 2.4227)  data: 0.2206 (0.0005 -- 1.8919)  max mem: 16413
[2023-10-23 17:33:20,475] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:33:20,475] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:33:20,476] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:33:20,476] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [66]  [40/54]  eta: 0:00:13  lr: 0.000036  min_lr: 0.000001  loss: 1.8083 (1.8348)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1720 (7.5540)  time: 0.8131 (0.5314 -- 2.3890)  data: 0.0646 (0.0004 -- 0.8524)  max mem: 16413
[2023-10-23 17:33:27,164] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3608
[2023-10-23 17:33:27,164] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:33:27,164] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3608
[2023-10-23 17:33:27,164] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:33:27,164] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [53/54]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 2.0216 (1.8509)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7531 (7.4662)  time: 0.6855 (0.4944 -- 2.3471)  data: 0.0211 (0.0001 -- 0.4084)  max mem: 16413
Epoch: [66] Total time: 0:00:49 (0.9258 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 2.0216 (1.8303)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7531 (7.4662)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.8336 (0.8336)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1216 (2.1216 -- 2.1216)  data: 1.9522 (1.9522 -- 1.9522)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7626 (0.7822)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3502 (0.0220 -- 2.1216)  data: 0.1953 (0.0001 -- 1.9522)  max mem: 16413
Val: Total time: 0:00:03 (0.3503 s / it)
* Acc@1 70.732 Acc@5 95.732 loss 0.886
Accuracy of the network on the 163 val images: 70.73%
[2023-10-23 17:33:35,765] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 17:33:35,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 17:33:35,766] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 17:33:35,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 17:33:37,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 17:33:37,048] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.73%
Epoch: [67]  [ 0/54]  eta: 0:07:37  lr: 0.000036  min_lr: 0.000001  loss: 2.2424 (2.2424)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7937 (6.7937)  time: 8.4761 (8.4761 -- 8.4761)  data: 7.9446 (7.9446 -- 7.9446)  max mem: 16413
Epoch: [67]  [20/54]  eta: 0:00:38  lr: 0.000036  min_lr: 0.000001  loss: 1.8826 (1.8501)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8390 (7.9886)  time: 0.7574 (0.5336 -- 2.8092)  data: 0.2078 (0.0005 -- 2.2857)  max mem: 16413
Epoch: [67]  [40/54]  eta: 0:00:14  lr: 0.000036  min_lr: 0.000001  loss: 1.6759 (1.8146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8834 (7.7546)  time: 0.8967 (0.5205 -- 2.5702)  data: 0.3360 (0.0003 -- 2.0362)  max mem: 16413
Epoch: [67]  [53/54]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.9563 (1.8317)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9472 (7.6530)  time: 0.7402 (0.4947 -- 1.9400)  data: 0.1191 (0.0002 -- 1.1204)  max mem: 16413
Epoch: [67] Total time: 0:00:50 (0.9265 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.9563 (1.8447)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9472 (7.6530)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8674 (0.8674)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0994 (2.0994 -- 2.0994)  data: 1.9224 (1.9224 -- 1.9224)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7425 (0.8276)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (96.3415)  time: 0.3473 (0.0221 -- 2.0994)  data: 0.1923 (0.0001 -- 1.9224)  max mem: 16413
Val: Total time: 0:00:03 (0.3474 s / it)
* Acc@1 68.902 Acc@5 95.122 loss 0.900
Accuracy of the network on the 163 val images: 68.90%
Max accuracy: 70.73%
Epoch: [68]  [ 0/54]  eta: 0:08:36  lr: 0.000036  min_lr: 0.000001  loss: 1.6550 (1.6550)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8265 (7.8265)  time: 9.5582 (9.5582 -- 9.5582)  data: 6.2975 (6.2975 -- 6.2975)  max mem: 16413
Epoch: [68]  [20/54]  eta: 0:00:42  lr: 0.000036  min_lr: 0.000001  loss: 1.8529 (1.7649)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8165 (7.5169)  time: 0.8475 (0.5206 -- 4.7322)  data: 0.3014 (0.0003 -- 4.2245)  max mem: 16413
Epoch: [68]  [40/54]  eta: 0:00:14  lr: 0.000036  min_lr: 0.000001  loss: 1.8484 (1.8062)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3334 (8.0887)  time: 0.7612 (0.5207 -- 2.4827)  data: 0.1652 (0.0002 -- 1.9608)  max mem: 16413
Epoch: [68]  [53/54]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8512 (1.8169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6512 (8.0344)  time: 0.6564 (0.4963 -- 1.7058)  data: 0.1090 (0.0001 -- 1.1989)  max mem: 16413
Epoch: [68] Total time: 0:00:50 (0.9436 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8512 (1.7924)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6512 (8.0344)
Val:  [ 0/10]  eta: 0:00:18  loss: 0.7449 (0.7449)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.8934 (1.8934 -- 1.8934)  data: 1.7099 (1.7099 -- 1.7099)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7126 (0.7710)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (95.1220)  time: 0.3277 (0.0225 -- 1.8934)  data: 0.1711 (0.0001 -- 1.7099)  max mem: 16413
Val: Total time: 0:00:03 (0.3278 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.863
Accuracy of the network on the 163 val images: 71.95%
[2023-10-23 17:35:25,245] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 17:35:25,247] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 17:35:25,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 17:35:25,248] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 17:35:26,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 17:35:26,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.95%
Epoch: [69]  [ 0/54]  eta: 0:05:44  lr: 0.000035  min_lr: 0.000001  loss: 2.0030 (2.0030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3511 (8.3511)  time: 6.3814 (6.3814 -- 6.3814)  data: 5.8556 (5.8556 -- 5.8556)  max mem: 16413
[2023-10-23 17:35:42,577] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:35:42,577] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:35:42,578] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:35:42,579] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [69]  [20/54]  eta: 0:00:39  lr: 0.000035  min_lr: 0.000001  loss: 1.8646 (1.8639)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3994 (7.9606)  time: 0.9152 (0.5141 -- 3.1837)  data: 0.2173 (0.0003 -- 2.6626)  max mem: 16413
Epoch: [69]  [40/54]  eta: 0:00:14  lr: 0.000035  min_lr: 0.000001  loss: 1.8178 (1.8415)  loss_scale: 32768.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1455 (7.5423)  time: 0.8582 (0.5156 -- 3.1623)  data: 0.0123 (0.0003 -- 0.2212)  max mem: 16413
Epoch: [69]  [53/54]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8178 (1.8235)  loss_scale: 32768.0000 (29430.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3666 (7.6118)  time: 0.6008 (0.4947 -- 1.9673)  data: 0.0118 (0.0002 -- 0.2212)  max mem: 16413
Epoch: [69] Total time: 0:00:49 (0.9237 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8178 (1.8347)  loss_scale: 32768.0000 (29430.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3666 (7.6118)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8405 (0.8405)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0819 (2.0819 -- 2.0819)  data: 1.9079 (1.9079 -- 1.9079)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7689 (0.8045)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (97.5610)  time: 0.3457 (0.0220 -- 2.0819)  data: 0.1909 (0.0001 -- 1.9079)  max mem: 16413
Val: Total time: 0:00:03 (0.3458 s / it)
* Acc@1 68.902 Acc@5 96.341 loss 0.899
Accuracy of the network on the 163 val images: 68.90%
Max accuracy: 71.95%
Epoch: [70]  [ 0/54]  eta: 0:07:00  lr: 0.000035  min_lr: 0.000001  loss: 1.3779 (1.3779)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8605 (8.8605)  time: 7.7924 (7.7924 -- 7.7924)  data: 7.2782 (7.2782 -- 7.2782)  max mem: 16413
[2023-10-23 17:36:29,941] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3783
[2023-10-23 17:36:29,941] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3783
[2023-10-23 17:36:29,942] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:36:29,942] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:36:29,942] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [70]  [20/54]  eta: 0:00:40  lr: 0.000035  min_lr: 0.000001  loss: 1.9305 (1.8802)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2046 (7.4676)  time: 0.8640 (0.5325 -- 3.2079)  data: 0.2525 (0.0002 -- 2.6810)  max mem: 16413
Epoch: [70]  [40/54]  eta: 0:00:14  lr: 0.000035  min_lr: 0.000001  loss: 1.6640 (1.8014)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3562 (7.5318)  time: 0.9316 (0.5162 -- 3.2098)  data: 0.2618 (0.0004 -- 2.6633)  max mem: 16413
Epoch: [70]  [53/54]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8383 (1.8069)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7320 (7.4616)  time: 0.6222 (0.4950 -- 2.3665)  data: 0.0007 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [70] Total time: 0:00:50 (0.9361 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8383 (1.8282)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7320 (7.4616)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8232 (0.8232)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9726 (1.9726 -- 1.9726)  data: 1.7419 (1.7419 -- 1.7419)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7521 (0.7822)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (97.5610)  time: 0.3360 (0.0224 -- 1.9726)  data: 0.1743 (0.0001 -- 1.7419)  max mem: 16413
Val: Total time: 0:00:03 (0.3361 s / it)
* Acc@1 71.341 Acc@5 96.951 loss 0.871
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 71.95%
Epoch: [71]  [ 0/54]  eta: 0:06:17  lr: 0.000035  min_lr: 0.000001  loss: 1.8206 (1.8206)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1432 (5.1432)  time: 6.9816 (6.9816 -- 6.9816)  data: 5.4415 (5.4415 -- 5.4415)  max mem: 16413
Epoch: [71]  [20/54]  eta: 0:00:40  lr: 0.000035  min_lr: 0.000001  loss: 1.9371 (1.8145)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5278 (7.7046)  time: 0.9010 (0.5230 -- 2.9531)  data: 0.3262 (0.0008 -- 2.4305)  max mem: 16413
[2023-10-23 17:37:52,078] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3871
[2023-10-23 17:37:52,078] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3871
[2023-10-23 17:37:52,078] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 17:37:52,078] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 17:37:52,078] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [71]  [40/54]  eta: 0:00:13  lr: 0.000035  min_lr: 0.000001  loss: 1.7023 (1.7852)  loss_scale: 16384.0000 (15584.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9080 (7.5661)  time: 0.7844 (0.5267 -- 2.9061)  data: 0.2352 (0.0002 -- 2.3714)  max mem: 16413
Epoch: [71]  [53/54]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.7446 (1.7714)  loss_scale: 8192.0000 (13805.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3306 (8.1470)  time: 0.7338 (0.4949 -- 1.6723)  data: 0.2069 (0.0002 -- 1.1787)  max mem: 16413
Epoch: [71] Total time: 0:00:50 (0.9286 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.7446 (1.7581)  loss_scale: 8192.0000 (13805.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3306 (8.1470)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7518 (0.7518)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0391 (2.0391 -- 2.0391)  data: 1.8643 (1.8643 -- 1.8643)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7330 (0.7798)  acc1: 77.7778 (71.9512)  acc5: 100.0000 (96.3415)  time: 0.3423 (0.0223 -- 2.0391)  data: 0.1865 (0.0001 -- 1.8643)  max mem: 16413
Val: Total time: 0:00:03 (0.3425 s / it)
* Acc@1 68.293 Acc@5 96.341 loss 0.866
Accuracy of the network on the 163 val images: 68.29%
Max accuracy: 71.95%
Epoch: [72]  [ 0/54]  eta: 0:07:22  lr: 0.000035  min_lr: 0.000001  loss: 1.8311 (1.8311)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8949 (9.8949)  time: 8.1985 (8.1985 -- 8.1985)  data: 7.3595 (7.3595 -- 7.3595)  max mem: 16413
Epoch: [72]  [20/54]  eta: 0:00:41  lr: 0.000034  min_lr: 0.000001  loss: 1.6894 (1.7301)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7589 (7.5607)  time: 0.8563 (0.5200 -- 3.4834)  data: 0.1702 (0.0002 -- 1.8844)  max mem: 16413
Epoch: [72]  [40/54]  eta: 0:00:15  lr: 0.000034  min_lr: 0.000001  loss: 1.7830 (1.7634)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1701 (7.9299)  time: 0.9309 (0.5288 -- 3.0675)  data: 0.2820 (0.0002 -- 2.5456)  max mem: 16413
Epoch: [72]  [53/54]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.7109 (1.7727)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2048 (7.7989)  time: 0.7376 (0.4952 -- 3.0675)  data: 0.1783 (0.0002 -- 2.5456)  max mem: 16413
Epoch: [72] Total time: 0:00:51 (0.9534 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.7109 (1.7813)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2048 (7.7989)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7671 (0.7671)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0962 (2.0962 -- 2.0962)  data: 1.9197 (1.9197 -- 1.9197)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7380 (0.7540)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3481 (0.0225 -- 2.0962)  data: 0.1921 (0.0001 -- 1.9197)  max mem: 16413
Val: Total time: 0:00:03 (0.3483 s / it)
* Acc@1 71.341 Acc@5 96.341 loss 0.851
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 71.95%
Epoch: [73]  [ 0/54]  eta: 0:08:47  lr: 0.000034  min_lr: 0.000001  loss: 2.1876 (2.1876)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5619 (5.5619)  time: 9.7746 (9.7746 -- 9.7746)  data: 6.6524 (6.6524 -- 6.6524)  max mem: 16413
Epoch: [73]  [20/54]  eta: 0:00:42  lr: 0.000034  min_lr: 0.000001  loss: 1.6664 (1.7589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6433 (7.8504)  time: 0.8361 (0.5233 -- 3.8907)  data: 0.1696 (0.0003 -- 3.3632)  max mem: 16413
Epoch: [73]  [40/54]  eta: 0:00:14  lr: 0.000034  min_lr: 0.000001  loss: 1.7526 (1.7924)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7959 (7.5332)  time: 0.8488 (0.5356 -- 3.0821)  data: 0.2949 (0.0004 -- 2.3771)  max mem: 16413
Epoch: [73]  [53/54]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.7778 (1.7963)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1984 (7.8205)  time: 0.6780 (0.4947 -- 2.7360)  data: 0.1117 (0.0002 -- 2.2212)  max mem: 16413
Epoch: [73] Total time: 0:00:51 (0.9449 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.7778 (1.7712)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1984 (7.8205)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7330 (0.7330)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9551 (1.9551 -- 1.9551)  data: 1.7647 (1.7647 -- 1.7647)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6823 (0.7654)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3339 (0.0222 -- 1.9551)  data: 0.1766 (0.0001 -- 1.7647)  max mem: 16413
Val: Total time: 0:00:03 (0.3341 s / it)
* Acc@1 70.732 Acc@5 96.341 loss 0.832
Accuracy of the network on the 163 val images: 70.73%
Max accuracy: 71.95%
Epoch: [74]  [ 0/54]  eta: 0:05:41  lr: 0.000034  min_lr: 0.000001  loss: 2.1789 (2.1789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3898 (7.3898)  time: 6.3249 (6.3249 -- 6.3249)  data: 5.5999 (5.5999 -- 5.5999)  max mem: 16413
[2023-10-23 17:40:06,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=19, lr=[8.034483821500753e-07, 8.034483821500753e-07, 1.0712645095334337e-06, 1.0712645095334337e-06, 1.4283526793779117e-06, 1.4283526793779117e-06, 1.9044702391705489e-06, 1.9044702391705489e-06, 2.5392936522273985e-06, 2.5392936522273985e-06, 3.3857248696365312e-06, 3.3857248696365312e-06, 4.514299826182042e-06, 4.514299826182042e-06, 6.019066434909389e-06, 6.019066434909389e-06, 8.02542191321252e-06, 8.02542191321252e-06, 1.0700562550950024e-05, 1.0700562550950024e-05, 1.4267416734600034e-05, 1.4267416734600034e-05, 1.9023222312800043e-05, 1.9023222312800043e-05, 2.5364296417066724e-05, 2.5364296417066724e-05, 3.38190618894223e-05, 3.38190618894223e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 17:40:06,943] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=19.217969901138684, CurrSamplesPerSec=21.88593249804542, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-10-23 17:40:07,451] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:40:07,451] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 17:40:07,452] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:40:07,452] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [74]  [20/54]  eta: 0:00:39  lr: 0.000034  min_lr: 0.000001  loss: 1.8154 (1.7406)  loss_scale: 16384.0000 (14823.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6654 (7.3398)  time: 0.9061 (0.5205 -- 3.4459)  data: 0.2291 (0.0004 -- 1.7648)  max mem: 16413
Epoch: [74]  [40/54]  eta: 0:00:13  lr: 0.000034  min_lr: 0.000001  loss: 1.9705 (1.8173)  loss_scale: 16384.0000 (15584.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2780 (7.3903)  time: 0.8263 (0.5285 -- 2.9602)  data: 0.0345 (0.0004 -- 0.6595)  max mem: 16413
Epoch: [74]  [53/54]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.9768 (1.8479)  loss_scale: 16384.0000 (15777.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7325 (7.3155)  time: 0.7226 (0.4960 -- 2.9602)  data: 0.0124 (0.0002 -- 0.2315)  max mem: 16413
Epoch: [74] Total time: 0:00:49 (0.9090 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.9768 (1.8145)  loss_scale: 16384.0000 (15777.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7325 (7.3155)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7134 (0.7134)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0183 (2.0183 -- 2.0183)  data: 1.8218 (1.8218 -- 1.8218)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6133 (0.7388)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3416 (0.0267 -- 2.0183)  data: 0.1823 (0.0001 -- 1.8218)  max mem: 16413
Val: Total time: 0:00:03 (0.3418 s / it)
* Acc@1 71.951 Acc@5 95.732 loss 0.816
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 71.95%
Epoch: [75]  [ 0/54]  eta: 0:06:12  lr: 0.000033  min_lr: 0.000001  loss: 1.9756 (1.9756)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1879 (8.1879)  time: 6.8929 (6.8929 -- 6.8929)  data: 6.3664 (6.3664 -- 6.3664)  max mem: 16413
Epoch: [75]  [20/54]  eta: 0:00:41  lr: 0.000033  min_lr: 0.000001  loss: 1.6959 (1.6790)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3701 (7.4435)  time: 0.9427 (0.5145 -- 5.1562)  data: 0.3642 (0.0002 -- 4.6132)  max mem: 16413
Epoch: [75]  [40/54]  eta: 0:00:15  lr: 0.000033  min_lr: 0.000001  loss: 1.6977 (1.6696)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6947 (8.2302)  time: 0.9255 (0.5131 -- 4.7053)  data: 0.3872 (0.0002 -- 4.2022)  max mem: 16413
Epoch: [75]  [53/54]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.6545 (1.6937)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4732 (8.1015)  time: 0.6709 (0.4939 -- 2.6975)  data: 0.1540 (0.0001 -- 2.1881)  max mem: 16413
Epoch: [75] Total time: 0:00:51 (0.9577 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.6545 (1.7460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4732 (8.1015)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7332 (0.7332)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1400 (2.1400 -- 2.1400)  data: 1.9631 (1.9631 -- 1.9631)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6583 (0.7393)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (96.3415)  time: 0.3529 (0.0263 -- 2.1400)  data: 0.1964 (0.0001 -- 1.9631)  max mem: 16413
Val: Total time: 0:00:03 (0.3531 s / it)
* Acc@1 71.341 Acc@5 95.732 loss 0.837
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 71.95%
Epoch: [76]  [ 0/54]  eta: 0:06:41  lr: 0.000033  min_lr: 0.000001  loss: 0.7996 (0.7996)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8943 (8.8943)  time: 7.4272 (7.4272 -- 7.4272)  data: 6.8887 (6.8887 -- 6.8887)  max mem: 16413
Epoch: [76]  [20/54]  eta: 0:00:38  lr: 0.000033  min_lr: 0.000001  loss: 1.7075 (1.7221)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7913 (6.6899)  time: 0.8273 (0.5167 -- 2.5542)  data: 0.2032 (0.0006 -- 2.0239)  max mem: 16413
[2023-10-23 17:42:11,676] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:42:11,676] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:42:11,677] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:42:11,677] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [76]  [40/54]  eta: 0:00:14  lr: 0.000033  min_lr: 0.000001  loss: 1.7663 (1.7356)  loss_scale: 32768.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3730 (7.1151)  time: 0.9703 (0.5298 -- 3.1145)  data: 0.2540 (0.0004 -- 2.5929)  max mem: 16413
[2023-10-23 17:42:29,142] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4146
[2023-10-23 17:42:29,142] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4146
[2023-10-23 17:42:29,142] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:42:29,142] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:42:29,142] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [76]  [53/54]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.6670 (1.7062)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1352 (7.3489)  time: 0.7734 (0.4960 -- 3.1145)  data: 0.2260 (0.0001 -- 2.5929)  max mem: 16413
Epoch: [76] Total time: 0:00:50 (0.9307 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.6670 (1.7676)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1352 (7.3489)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7373 (0.7373)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0406 (2.0406 -- 2.0406)  data: 1.8679 (1.8679 -- 1.8679)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7103 (0.7701)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3430 (0.0220 -- 2.0406)  data: 0.1869 (0.0001 -- 1.8679)  max mem: 16413
Val: Total time: 0:00:03 (0.3431 s / it)
* Acc@1 70.732 Acc@5 96.341 loss 0.850
Accuracy of the network on the 163 val images: 70.73%
Max accuracy: 71.95%
Epoch: [77]  [ 0/54]  eta: 0:07:35  lr: 0.000033  min_lr: 0.000001  loss: 1.7950 (1.7950)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7236 (5.7236)  time: 8.4378 (8.4378 -- 8.4378)  data: 5.6024 (5.6024 -- 5.6024)  max mem: 16413
Epoch: [77]  [20/54]  eta: 0:00:39  lr: 0.000033  min_lr: 0.000001  loss: 1.9751 (1.9044)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7906 (7.6427)  time: 0.8022 (0.5279 -- 2.5396)  data: 0.0691 (0.0006 -- 0.7096)  max mem: 16413
Epoch: [77]  [40/54]  eta: 0:00:14  lr: 0.000033  min_lr: 0.000001  loss: 1.8562 (1.8713)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1975 (7.9940)  time: 0.8758 (0.5061 -- 3.6963)  data: 0.1716 (0.0003 -- 1.8223)  max mem: 16413
Epoch: [77]  [53/54]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.9097 (1.8652)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8208 (8.0767)  time: 0.6335 (0.4924 -- 2.2522)  data: 0.1175 (0.0002 -- 1.7191)  max mem: 16413
Epoch: [77] Total time: 0:00:50 (0.9429 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.9097 (1.8460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8208 (8.0767)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7420 (0.7420)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9525 (1.9525 -- 1.9525)  data: 1.7653 (1.7653 -- 1.7653)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6721 (0.7688)  acc1: 77.7778 (71.9512)  acc5: 100.0000 (96.3415)  time: 0.3333 (0.0223 -- 1.9525)  data: 0.1766 (0.0001 -- 1.7653)  max mem: 16413
Val: Total time: 0:00:03 (0.3334 s / it)
* Acc@1 69.512 Acc@5 95.732 loss 0.850
Accuracy of the network on the 163 val images: 69.51%
Max accuracy: 71.95%
Epoch: [78]  [ 0/54]  eta: 0:07:11  lr: 0.000032  min_lr: 0.000001  loss: 1.3555 (1.3555)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1357 (5.1357)  time: 7.9998 (7.9998 -- 7.9998)  data: 5.9734 (5.9734 -- 5.9734)  max mem: 16413
Epoch: [78]  [20/54]  eta: 0:00:40  lr: 0.000032  min_lr: 0.000001  loss: 1.7473 (1.7077)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9874 (7.3065)  time: 0.8448 (0.5315 -- 2.7960)  data: 0.0130 (0.0002 -- 0.2245)  max mem: 16413
Epoch: [78]  [40/54]  eta: 0:00:14  lr: 0.000032  min_lr: 0.000001  loss: 1.8188 (1.7760)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2630 (7.4868)  time: 0.8364 (0.5151 -- 3.9755)  data: 0.0176 (0.0003 -- 0.3210)  max mem: 16413
[2023-10-23 17:44:23,715] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4265
[2023-10-23 17:44:23,715] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 17:44:23,715] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-10-23 17:44:23,715] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4265
[2023-10-23 17:44:23,715] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [78]  [53/54]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.8150 (1.7689)  loss_scale: 16384.0000 (16232.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3819 (7.6101)  time: 0.7041 (0.4810 -- 3.3398)  data: 0.0166 (0.0001 -- 0.3210)  max mem: 16413
Epoch: [78] Total time: 0:00:50 (0.9443 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.8150 (1.7582)  loss_scale: 16384.0000 (16232.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3819 (7.6101)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7131 (0.7131)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9530 (1.9530 -- 1.9530)  data: 1.7303 (1.7303 -- 1.7303)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6398 (0.7610)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3395 (0.0229 -- 1.9530)  data: 0.1763 (0.0001 -- 1.7303)  max mem: 16413
Val: Total time: 0:00:03 (0.3396 s / it)
* Acc@1 70.122 Acc@5 95.732 loss 0.843
Accuracy of the network on the 163 val images: 70.12%
Max accuracy: 71.95%
Epoch: [79]  [ 0/54]  eta: 0:06:36  lr: 0.000032  min_lr: 0.000001  loss: 1.4873 (1.4873)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2376 (8.2376)  time: 7.3402 (7.3402 -- 7.3402)  data: 6.8221 (6.8221 -- 6.8221)  max mem: 16413
Epoch: [79]  [20/54]  eta: 0:00:38  lr: 0.000032  min_lr: 0.000001  loss: 1.6476 (1.7429)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2732 (7.4360)  time: 0.8317 (0.5218 -- 2.9045)  data: 0.2712 (0.0002 -- 2.3794)  max mem: 16413
Epoch: [79]  [40/54]  eta: 0:00:14  lr: 0.000032  min_lr: 0.000001  loss: 1.8121 (1.7789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0828 (7.4164)  time: 0.9052 (0.5236 -- 2.6409)  data: 0.2690 (0.0003 -- 2.0995)  max mem: 16413
Epoch: [79]  [53/54]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.8948 (1.8056)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6407 (7.2188)  time: 0.7158 (0.4932 -- 2.6409)  data: 0.1063 (0.0001 -- 2.0995)  max mem: 16413
Epoch: [79] Total time: 0:00:50 (0.9354 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.8948 (1.7692)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6407 (7.2188)
[2023-10-23 17:45:17,672] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-10-23 17:45:17,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-10-23 17:45:17,677] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-79/mp_rank_00_model_states.pt
[2023-10-23 17:45:17,677] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-79/mp_rank_00_model_states.pt...
[2023-10-23 17:45:18,789] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-79/mp_rank_00_model_states.pt.
[2023-10-23 17:45:18,789] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7513 (0.7513)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9554 (1.9554 -- 1.9554)  data: 1.7756 (1.7756 -- 1.7756)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6948 (0.7563)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (96.3415)  time: 0.3373 (0.0228 -- 1.9554)  data: 0.1806 (0.0001 -- 1.7756)  max mem: 16413
Val: Total time: 0:00:03 (0.3374 s / it)
* Acc@1 71.341 Acc@5 95.732 loss 0.853
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 71.95%
Epoch: [80]  [ 0/54]  eta: 0:06:46  lr: 0.000032  min_lr: 0.000001  loss: 1.6294 (1.6294)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3180 (8.3180)  time: 7.5197 (7.5197 -- 7.5197)  data: 6.5166 (6.5166 -- 6.5166)  max mem: 16413
Epoch: [80]  [20/54]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000001  loss: 1.6241 (1.6068)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2200 (6.6399)  time: 0.7975 (0.5383 -- 2.7822)  data: 0.2407 (0.0003 -- 2.2671)  max mem: 16413
Epoch: [80]  [40/54]  eta: 0:00:14  lr: 0.000032  min_lr: 0.000001  loss: 1.8160 (1.6953)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3670 (7.1908)  time: 0.9085 (0.5285 -- 3.7572)  data: 0.3356 (0.0004 -- 2.6355)  max mem: 16413
Epoch: [80]  [53/54]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.8066 (1.7146)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3785 (7.4599)  time: 0.6949 (0.4956 -- 2.7185)  data: 0.1684 (0.0002 -- 2.2053)  max mem: 16413
Epoch: [80] Total time: 0:00:49 (0.9168 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.8066 (1.7678)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3785 (7.4599)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8330 (0.8330)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0125 (2.0125 -- 2.0125)  data: 1.8285 (1.8285 -- 1.8285)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7637 (0.7772)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (96.3415)  time: 0.3399 (0.0221 -- 2.0125)  data: 0.1829 (0.0001 -- 1.8285)  max mem: 16413
Val: Total time: 0:00:03 (0.3400 s / it)
* Acc@1 69.512 Acc@5 96.341 loss 0.904
Accuracy of the network on the 163 val images: 69.51%
Max accuracy: 71.95%
Epoch: [81]  [ 0/54]  eta: 0:07:18  lr: 0.000031  min_lr: 0.000001  loss: 1.5998 (1.5998)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9228 (8.9228)  time: 8.1245 (8.1245 -- 8.1245)  data: 6.4701 (6.4701 -- 6.4701)  max mem: 16413
[2023-10-23 17:46:39,119] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:46:39,119] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 17:46:39,119] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:46:39,120] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [81]  [20/54]  eta: 0:00:38  lr: 0.000031  min_lr: 0.000001  loss: 1.9660 (1.8387)  loss_scale: 8192.0000 (8582.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5670 (8.1835)  time: 0.7951 (0.5303 -- 3.6397)  data: 0.0696 (0.0003 -- 0.8034)  max mem: 16413
Epoch: [81]  [40/54]  eta: 0:00:14  lr: 0.000031  min_lr: 0.000001  loss: 1.8120 (1.8204)  loss_scale: 16384.0000 (12387.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1502 (7.4451)  time: 0.8713 (0.5260 -- 3.8048)  data: 0.0430 (0.0004 -- 0.8280)  max mem: 16413
Epoch: [81]  [53/54]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.6882 (1.7738)  loss_scale: 16384.0000 (13349.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3149 (7.4878)  time: 0.7190 (0.4956 -- 1.6270)  data: 0.0008 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [81] Total time: 0:00:50 (0.9361 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.6882 (1.7868)  loss_scale: 16384.0000 (13349.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3149 (7.4878)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7683 (0.7683)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1030 (2.1030 -- 2.1030)  data: 1.9249 (1.9249 -- 1.9249)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6810 (0.7390)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (97.5610)  time: 0.3473 (0.0221 -- 2.1030)  data: 0.1926 (0.0001 -- 1.9249)  max mem: 16413
Val: Total time: 0:00:03 (0.3474 s / it)
* Acc@1 70.732 Acc@5 96.341 loss 0.868
Accuracy of the network on the 163 val images: 70.73%
Max accuracy: 71.95%
Epoch: [82]  [ 0/54]  eta: 0:06:08  lr: 0.000031  min_lr: 0.000001  loss: 1.8242 (1.8242)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4224 (6.4224)  time: 6.8244 (6.8244 -- 6.8244)  data: 5.9681 (5.9681 -- 5.9681)  max mem: 16413
Epoch: [82]  [20/54]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.8530 (1.8825)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1368 (7.3585)  time: 0.7927 (0.5253 -- 3.1051)  data: 0.2411 (0.0004 -- 2.5706)  max mem: 16413
Epoch: [82]  [40/54]  eta: 0:00:14  lr: 0.000031  min_lr: 0.000001  loss: 1.8194 (1.8152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1715 (7.3545)  time: 0.9805 (0.5252 -- 2.8784)  data: 0.2448 (0.0003 -- 2.3742)  max mem: 16413
[2023-10-23 17:47:56,682] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4476
[2023-10-23 17:47:56,682] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 17:47:56,682] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4476
[2023-10-23 17:47:56,682] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-10-23 17:47:56,723] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [82]  [53/54]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.8012 (1.7936)  loss_scale: 16384.0000 (15473.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3864 (7.4637)  time: 0.7288 (0.4938 -- 2.1971)  data: 0.0151 (0.0002 -- 0.2809)  max mem: 16413
Epoch: [82] Total time: 0:00:50 (0.9276 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.8012 (1.8027)  loss_scale: 16384.0000 (15473.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3864 (7.4637)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7569 (0.7569)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0650 (2.0650 -- 2.0650)  data: 1.8869 (1.8869 -- 1.8869)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7267 (0.7585)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (96.3415)  time: 0.3443 (0.0224 -- 2.0650)  data: 0.1888 (0.0001 -- 1.8869)  max mem: 16413
Val: Total time: 0:00:03 (0.3444 s / it)
* Acc@1 70.732 Acc@5 96.341 loss 0.878
Accuracy of the network on the 163 val images: 70.73%
Max accuracy: 71.95%
Epoch: [83]  [ 0/54]  eta: 0:07:36  lr: 0.000031  min_lr: 0.000001  loss: 2.4452 (2.4452)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9947 (7.9947)  time: 8.4555 (8.4555 -- 8.4555)  data: 7.2504 (7.2504 -- 7.2504)  max mem: 16413
Epoch: [83]  [20/54]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.6838 (1.7151)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5867 (6.8525)  time: 0.7187 (0.5319 -- 2.1722)  data: 0.0586 (0.0002 -- 0.9680)  max mem: 16413
Epoch: [83]  [40/54]  eta: 0:00:14  lr: 0.000030  min_lr: 0.000001  loss: 1.9207 (1.7764)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7607 (7.6628)  time: 0.9647 (0.5248 -- 2.8117)  data: 0.2981 (0.0004 -- 2.2862)  max mem: 16413
Epoch: [83]  [53/54]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.8767 (1.8082)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8310 (7.4860)  time: 0.7279 (0.4959 -- 2.8117)  data: 0.2083 (0.0001 -- 2.2862)  max mem: 16413
Epoch: [83] Total time: 0:00:50 (0.9262 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.8767 (1.8051)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8310 (7.4860)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8199 (0.8199)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0617 (2.0617 -- 2.0617)  data: 1.8821 (1.8821 -- 1.8821)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6706 (0.7608)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (97.5610)  time: 0.3436 (0.0220 -- 2.0617)  data: 0.1883 (0.0001 -- 1.8821)  max mem: 16413
Val: Total time: 0:00:03 (0.3437 s / it)
* Acc@1 67.683 Acc@5 96.341 loss 0.896
Accuracy of the network on the 163 val images: 67.68%
Max accuracy: 71.95%
Epoch: [84]  [ 0/54]  eta: 0:06:31  lr: 0.000030  min_lr: 0.000001  loss: 2.0965 (2.0965)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8031 (11.8031)  time: 7.2511 (7.2511 -- 7.2511)  data: 6.7051 (6.7051 -- 6.7051)  max mem: 16413
Epoch: [84]  [20/54]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000001  loss: 1.8169 (1.8428)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6921 (7.6029)  time: 0.8068 (0.5154 -- 3.3601)  data: 0.2433 (0.0004 -- 2.8149)  max mem: 16413
Epoch: [84]  [40/54]  eta: 0:00:15  lr: 0.000030  min_lr: 0.000001  loss: 1.8210 (1.8331)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7415 (7.7296)  time: 1.0678 (0.5277 -- 4.6176)  data: 0.5215 (0.0005 -- 4.1013)  max mem: 16413
Epoch: [84]  [53/54]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.8156 (1.8252)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5929 (7.6616)  time: 0.7423 (0.4944 -- 4.6176)  data: 0.2227 (0.0002 -- 4.1013)  max mem: 16413
Epoch: [84] Total time: 0:00:51 (0.9562 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.8156 (1.8201)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5929 (7.6616)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7213 (0.7213)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0170 (2.0170 -- 2.0170)  data: 1.8268 (1.8268 -- 1.8268)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6636 (0.7170)  acc1: 77.7778 (75.6098)  acc5: 100.0000 (96.3415)  time: 0.3385 (0.0219 -- 2.0170)  data: 0.1828 (0.0001 -- 1.8268)  max mem: 16413
Val: Total time: 0:00:03 (0.3386 s / it)
* Acc@1 71.341 Acc@5 95.732 loss 0.835
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 71.95%
Epoch: [85]  [ 0/54]  eta: 0:07:01  lr: 0.000030  min_lr: 0.000001  loss: 1.5735 (1.5735)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6027 (7.6027)  time: 7.7967 (7.7967 -- 7.7967)  data: 5.5916 (5.5916 -- 5.5916)  max mem: 16413
[2023-10-23 17:50:09,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:50:09,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 17:50:09,438] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:50:09,438] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [85]  [20/54]  eta: 0:00:39  lr: 0.000030  min_lr: 0.000001  loss: 1.7044 (1.7299)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0918 (6.8661)  time: 0.8291 (0.5283 -- 2.7528)  data: 0.1106 (0.0007 -- 1.2616)  max mem: 16413
Epoch: [85]  [40/54]  eta: 0:00:14  lr: 0.000030  min_lr: 0.000001  loss: 1.7926 (1.8049)  loss_scale: 16384.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5461 (8.1193)  time: 0.9104 (0.5203 -- 3.3842)  data: 0.0939 (0.0003 -- 0.8079)  max mem: 16413
Epoch: [85]  [53/54]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.7983 (1.8048)  loss_scale: 16384.0000 (14108.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0349 (8.2746)  time: 0.7168 (0.4914 -- 2.4660)  data: 0.1498 (0.0002 -- 1.9660)  max mem: 16413
Epoch: [85] Total time: 0:00:51 (0.9516 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.7983 (1.7823)  loss_scale: 16384.0000 (14108.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0349 (8.2746)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7454 (0.7454)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0709 (2.0709 -- 2.0709)  data: 1.8919 (1.8919 -- 1.8919)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6372 (0.7377)  acc1: 77.7778 (75.6098)  acc5: 100.0000 (96.3415)  time: 0.3446 (0.0222 -- 2.0709)  data: 0.1893 (0.0001 -- 1.8919)  max mem: 16413
Val: Total time: 0:00:03 (0.3447 s / it)
* Acc@1 71.341 Acc@5 95.732 loss 0.862
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 71.95%
Epoch: [86]  [ 0/54]  eta: 0:07:39  lr: 0.000030  min_lr: 0.000001  loss: 1.4960 (1.4960)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1049 (7.1049)  time: 8.5003 (8.5003 -- 8.5003)  data: 6.6049 (6.6049 -- 6.6049)  max mem: 16413
Epoch: [86]  [20/54]  eta: 0:00:40  lr: 0.000029  min_lr: 0.000001  loss: 1.7830 (1.7411)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2168 (8.2876)  time: 0.8195 (0.5317 -- 3.5829)  data: 0.0019 (0.0009 -- 0.0045)  max mem: 16413
Epoch: [86]  [40/54]  eta: 0:00:15  lr: 0.000029  min_lr: 0.000001  loss: 1.8064 (1.7668)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7623 (8.1750)  time: 0.9689 (0.5146 -- 4.1833)  data: 0.0013 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [86]  [53/54]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.8342 (1.7958)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7623 (8.2129)  time: 0.6352 (0.4949 -- 2.9553)  data: 0.0008 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [86] Total time: 0:00:50 (0.9417 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.8342 (1.7932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7623 (8.2129)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7506 (0.7506)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0842 (2.0842 -- 2.0842)  data: 1.9118 (1.9118 -- 1.9118)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6870 (0.7493)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (96.3415)  time: 0.3456 (0.0221 -- 2.0842)  data: 0.1913 (0.0001 -- 1.9118)  max mem: 16413
Val: Total time: 0:00:03 (0.3457 s / it)
* Acc@1 70.732 Acc@5 95.732 loss 0.862
Accuracy of the network on the 163 val images: 70.73%
Max accuracy: 71.95%
Epoch: [87]  [ 0/54]  eta: 0:06:14  lr: 0.000029  min_lr: 0.000001  loss: 1.9032 (1.9032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0153 (8.0153)  time: 6.9295 (6.9295 -- 6.9295)  data: 4.9884 (4.9884 -- 4.9884)  max mem: 16413
Epoch: [87]  [20/54]  eta: 0:00:39  lr: 0.000029  min_lr: 0.000001  loss: 1.7164 (1.7283)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9103 (7.2053)  time: 0.8756 (0.5181 -- 3.2603)  data: 0.0424 (0.0004 -- 0.6837)  max mem: 16413
[2023-10-23 17:52:19,037] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:52:19,037] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:52:19,037] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:52:19,037] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [87]  [40/54]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000001  loss: 1.8217 (1.7684)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7830 (6.8974)  time: 0.8755 (0.5323 -- 3.0491)  data: 0.0399 (0.0004 -- 0.4955)  max mem: 16413
[2023-10-23 17:52:26,748] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4744
[2023-10-23 17:52:26,748] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4744
[2023-10-23 17:52:26,748] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:52:26,748] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:52:26,748] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [87]  [53/54]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.8625 (1.7658)  loss_scale: 32768.0000 (19721.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5260 (7.0478)  time: 0.7542 (0.4824 -- 2.9541)  data: 0.0289 (0.0002 -- 0.4955)  max mem: 16413
Epoch: [87] Total time: 0:00:50 (0.9321 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.8625 (1.8022)  loss_scale: 32768.0000 (19721.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5260 (7.0478)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7756 (0.7756)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0971 (2.0971 -- 2.0971)  data: 1.9253 (1.9253 -- 1.9253)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6742 (0.7415)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (96.3415)  time: 0.3466 (0.0221 -- 2.0971)  data: 0.1926 (0.0001 -- 1.9253)  max mem: 16413
Val: Total time: 0:00:03 (0.3467 s / it)
* Acc@1 71.341 Acc@5 95.732 loss 0.869
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 71.95%
Epoch: [88]  [ 0/54]  eta: 0:08:20  lr: 0.000029  min_lr: 0.000001  loss: 1.8878 (1.8878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4593 (9.4593)  time: 9.2619 (9.2619 -- 9.2619)  data: 8.7623 (8.7623 -- 8.7623)  max mem: 16413
Epoch: [88]  [20/54]  eta: 0:00:39  lr: 0.000029  min_lr: 0.000001  loss: 1.7336 (1.7667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1808 (7.1294)  time: 0.7548 (0.5125 -- 2.8986)  data: 0.2173 (0.0001 -- 2.3651)  max mem: 16413
Epoch: [88]  [40/54]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000001  loss: 1.8476 (1.8100)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1085 (7.8029)  time: 0.9469 (0.5168 -- 3.2179)  data: 0.4059 (0.0005 -- 2.6824)  max mem: 16413
Epoch: [88]  [53/54]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.9168 (1.7871)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2506 (7.5982)  time: 0.6937 (0.4939 -- 2.0225)  data: 0.1761 (0.0002 -- 1.4936)  max mem: 16413
Epoch: [88] Total time: 0:00:50 (0.9389 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.9168 (1.7661)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2506 (7.5982)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7235 (0.7235)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 1.9574 (1.9574 -- 1.9574)  data: 1.7807 (1.7807 -- 1.7807)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6194 (0.7235)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (96.3415)  time: 0.4164 (0.0222 -- 1.9574)  data: 0.2600 (0.0001 -- 1.7807)  max mem: 16413
Val: Total time: 0:00:04 (0.4166 s / it)
* Acc@1 73.171 Acc@5 95.732 loss 0.847
Accuracy of the network on the 163 val images: 73.17%
[2023-10-23 17:53:28,996] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 17:53:28,998] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 17:53:28,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 17:53:28,998] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 17:53:30,508] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 17:53:30,509] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.17%
Epoch: [89]  [ 0/54]  eta: 0:06:51  lr: 0.000029  min_lr: 0.000001  loss: 1.6236 (1.6236)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1723 (4.1723)  time: 7.6163 (7.6163 -- 7.6163)  data: 5.5437 (5.5437 -- 5.5437)  max mem: 16413
Epoch: [89]  [20/54]  eta: 0:00:41  lr: 0.000028  min_lr: 0.000001  loss: 1.6247 (1.6799)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1127 (6.6701)  time: 0.8964 (0.5213 -- 3.9907)  data: 0.3496 (0.0009 -- 3.4481)  max mem: 16413
Epoch: [89]  [40/54]  eta: 0:00:15  lr: 0.000028  min_lr: 0.000001  loss: 1.7747 (1.7173)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5215 (7.5568)  time: 0.9403 (0.5201 -- 4.7492)  data: 0.3923 (0.0002 -- 4.2395)  max mem: 16413
Epoch: [89]  [53/54]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.5739 (1.6853)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2880 (7.4502)  time: 0.7350 (0.4937 -- 4.7492)  data: 0.2126 (0.0001 -- 4.2395)  max mem: 16413
Epoch: [89] Total time: 0:00:50 (0.9412 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.5739 (1.7551)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2880 (7.4502)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7685 (0.7685)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1502 (2.1502 -- 2.1502)  data: 1.9765 (1.9765 -- 1.9765)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6665 (0.7169)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (97.5610)  time: 0.3518 (0.0220 -- 2.1502)  data: 0.1977 (0.0001 -- 1.9765)  max mem: 16413
Val: Total time: 0:00:03 (0.3519 s / it)
* Acc@1 72.561 Acc@5 96.341 loss 0.854
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.17%
Epoch: [90]  [ 0/54]  eta: 0:07:32  lr: 0.000028  min_lr: 0.000001  loss: 1.5963 (1.5963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7804 (8.7804)  time: 8.3716 (8.3716 -- 8.3716)  data: 7.8640 (7.8640 -- 7.8640)  max mem: 16413
[2023-10-23 17:54:44,039] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:54:44,039] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:54:44,040] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:54:44,040] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [90]  [20/54]  eta: 0:00:42  lr: 0.000028  min_lr: 0.000001  loss: 1.5613 (1.5857)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5646 (6.9270)  time: 0.8886 (0.5199 -- 2.3787)  data: 0.1390 (0.0003 -- 1.8510)  max mem: 16413
[2023-10-23 17:55:06,188] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4898
[2023-10-23 17:55:06,188] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4898
[2023-10-23 17:55:06,188] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:55:06,188] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:55:06,188] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [90]  [40/54]  eta: 0:00:14  lr: 0.000028  min_lr: 0.000001  loss: 1.6972 (1.6759)  loss_scale: 32768.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0392 (7.0494)  time: 0.8126 (0.5130 -- 3.2383)  data: 0.1552 (0.0003 -- 2.7273)  max mem: 16413
Epoch: [90]  [53/54]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.6436 (1.7132)  loss_scale: 16384.0000 (23969.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0392 (7.1758)  time: 0.6552 (0.4944 -- 2.4413)  data: 0.0181 (0.0002 -- 0.3486)  max mem: 16413
Epoch: [90] Total time: 0:00:51 (0.9517 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.6436 (1.7225)  loss_scale: 16384.0000 (23969.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0392 (7.1758)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7499 (0.7499)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1024 (2.1024 -- 2.1024)  data: 1.9276 (1.9276 -- 1.9276)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6683 (0.7115)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (96.3415)  time: 0.3482 (0.0221 -- 2.1024)  data: 0.1928 (0.0001 -- 1.9276)  max mem: 16413
Val: Total time: 0:00:03 (0.3483 s / it)
* Acc@1 69.512 Acc@5 95.732 loss 0.857
Accuracy of the network on the 163 val images: 69.51%
Max accuracy: 73.17%
Epoch: [91]  [ 0/54]  eta: 0:06:22  lr: 0.000028  min_lr: 0.000001  loss: 1.7337 (1.7337)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9609 (4.9609)  time: 7.0868 (7.0868 -- 7.0868)  data: 6.0836 (6.0836 -- 6.0836)  max mem: 16413
Epoch: [91]  [20/54]  eta: 0:00:42  lr: 0.000028  min_lr: 0.000001  loss: 1.7094 (1.7954)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1773 (7.3339)  time: 0.9698 (0.5262 -- 5.3450)  data: 0.2468 (0.0003 -- 2.9540)  max mem: 16413
Epoch: [91]  [40/54]  eta: 0:00:14  lr: 0.000027  min_lr: 0.000001  loss: 1.6127 (1.7166)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2447 (7.8364)  time: 0.8225 (0.5150 -- 3.5718)  data: 0.0691 (0.0003 -- 1.1959)  max mem: 16413
Epoch: [91]  [53/54]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.4850 (1.7186)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7672 (7.7964)  time: 0.7174 (0.4937 -- 2.1947)  data: 0.0604 (0.0002 -- 1.1959)  max mem: 16413
Epoch: [91] Total time: 0:00:51 (0.9496 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.4850 (1.7399)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7672 (7.7964)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7282 (0.7282)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0015 (2.0015 -- 2.0015)  data: 1.8089 (1.8089 -- 1.8089)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6433 (0.7141)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (97.5610)  time: 0.3391 (0.0223 -- 2.0015)  data: 0.1810 (0.0001 -- 1.8089)  max mem: 16413
Val: Total time: 0:00:03 (0.3392 s / it)
* Acc@1 71.341 Acc@5 96.341 loss 0.839
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.17%
Epoch: [92]  [ 0/54]  eta: 0:06:58  lr: 0.000027  min_lr: 0.000001  loss: 2.7495 (2.7495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1402 (8.1402)  time: 7.7526 (7.7526 -- 7.7526)  data: 7.2293 (7.2293 -- 7.2293)  max mem: 16413
Epoch: [92]  [20/54]  eta: 0:00:42  lr: 0.000027  min_lr: 0.000001  loss: 1.9748 (1.9946)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3780 (8.4067)  time: 0.9372 (0.5194 -- 4.0444)  data: 0.3181 (0.0004 -- 3.5264)  max mem: 16413
[2023-10-23 17:56:48,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[6.459418196123603e-07, 6.459418196123603e-07, 8.61255759483147e-07, 8.61255759483147e-07, 1.148341012644196e-06, 1.148341012644196e-06, 1.5311213501922614e-06, 1.5311213501922614e-06, 2.0414951335896818e-06, 2.0414951335896818e-06, 2.721993511452909e-06, 2.721993511452909e-06, 3.629324681937212e-06, 3.629324681937212e-06, 4.839099575916283e-06, 4.839099575916283e-06, 6.452132767888377e-06, 6.452132767888377e-06, 8.602843690517836e-06, 8.602843690517836e-06, 1.1470458254023781e-05, 1.1470458254023781e-05, 1.5293944338698375e-05, 1.5293944338698375e-05, 2.0391925784931167e-05, 2.0391925784931167e-05, 2.7189234379908223e-05, 2.7189234379908223e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 17:56:48,063] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=19.04536255363853, CurrSamplesPerSec=22.398600144453557, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [92]  [40/54]  eta: 0:00:14  lr: 0.000027  min_lr: 0.000001  loss: 1.8530 (1.8968)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4131 (7.9127)  time: 0.8384 (0.5276 -- 2.8744)  data: 0.2975 (0.0003 -- 2.3740)  max mem: 16413
Epoch: [92]  [53/54]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.8292 (1.9000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7651 (7.9676)  time: 0.7440 (0.4949 -- 2.8744)  data: 0.2270 (0.0002 -- 2.3740)  max mem: 16413
Epoch: [92] Total time: 0:00:51 (0.9627 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.8292 (1.8244)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7651 (7.9676)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7771 (0.7771)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1053 (2.1053 -- 2.1053)  data: 1.9335 (1.9335 -- 1.9335)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6524 (0.7108)  acc1: 77.7778 (75.6098)  acc5: 100.0000 (97.5610)  time: 0.3492 (0.0223 -- 2.1053)  data: 0.1934 (0.0001 -- 1.9335)  max mem: 16413
Val: Total time: 0:00:03 (0.3493 s / it)
* Acc@1 70.122 Acc@5 96.341 loss 0.859
Accuracy of the network on the 163 val images: 70.12%
Max accuracy: 73.17%
Epoch: [93]  [ 0/54]  eta: 0:06:51  lr: 0.000027  min_lr: 0.000001  loss: 1.6323 (1.6323)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3057 (5.3057)  time: 7.6191 (7.6191 -- 7.6191)  data: 7.0802 (7.0802 -- 7.0802)  max mem: 16413
[2023-10-23 17:57:20,323] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:57:20,323] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 17:57:20,324] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 17:57:20,324] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [93]  [20/54]  eta: 0:00:39  lr: 0.000027  min_lr: 0.000001  loss: 1.6301 (1.7256)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1683 (7.1660)  time: 0.8366 (0.5269 -- 4.8519)  data: 0.2853 (0.0007 -- 4.3169)  max mem: 16413
Epoch: [93]  [40/54]  eta: 0:00:14  lr: 0.000027  min_lr: 0.000001  loss: 1.7489 (1.7830)  loss_scale: 32768.0000 (30769.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5390 (7.5200)  time: 0.9142 (0.5374 -- 2.9592)  data: 0.3242 (0.0005 -- 2.4350)  max mem: 16413
Epoch: [93]  [53/54]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.7422 (1.7324)  loss_scale: 32768.0000 (31250.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2608 (7.4557)  time: 0.7077 (0.4939 -- 1.6317)  data: 0.0791 (0.0002 -- 0.6507)  max mem: 16413
Epoch: [93] Total time: 0:00:51 (0.9458 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.7422 (1.7835)  loss_scale: 32768.0000 (31250.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2608 (7.4557)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7334 (0.7334)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0865 (2.0865 -- 2.0865)  data: 1.9077 (1.9077 -- 1.9077)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6679 (0.6985)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (97.5610)  time: 0.3463 (0.0221 -- 2.0865)  data: 0.1908 (0.0001 -- 1.9077)  max mem: 16413
Val: Total time: 0:00:03 (0.3464 s / it)
* Acc@1 72.561 Acc@5 96.341 loss 0.834
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.17%
Epoch: [94]  [ 0/54]  eta: 0:07:14  lr: 0.000027  min_lr: 0.000001  loss: 1.3466 (1.3466)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1141 (6.1141)  time: 8.0488 (8.0488 -- 8.0488)  data: 7.5170 (7.5170 -- 7.5170)  max mem: 16413
Epoch: [94]  [20/54]  eta: 0:00:38  lr: 0.000027  min_lr: 0.000001  loss: 1.6523 (1.6324)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0723 (7.5389)  time: 0.7714 (0.5255 -- 2.8004)  data: 0.2257 (0.0006 -- 2.2711)  max mem: 16413
Epoch: [94]  [40/54]  eta: 0:00:14  lr: 0.000026  min_lr: 0.000001  loss: 1.6756 (1.6643)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4555 (7.7249)  time: 0.9609 (0.5245 -- 3.8063)  data: 0.3670 (0.0005 -- 3.2734)  max mem: 16413
[2023-10-23 17:58:50,543] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5122
[2023-10-23 17:58:50,544] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:58:50,543] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5122
[2023-10-23 17:58:50,544] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 17:58:50,544] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [94]  [53/54]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.7211 (1.6839)  loss_scale: 32768.0000 (30340.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8430 (7.8045)  time: 0.6899 (0.4930 -- 1.9183)  data: 0.1085 (0.0001 -- 1.4141)  max mem: 16413
Epoch: [94] Total time: 0:00:50 (0.9318 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.7211 (1.7520)  loss_scale: 32768.0000 (30340.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8430 (7.8045)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7411 (0.7411)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0578 (2.0578 -- 2.0578)  data: 1.8855 (1.8855 -- 1.8855)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6272 (0.6909)  acc1: 77.7778 (75.6098)  acc5: 100.0000 (97.5610)  time: 0.3432 (0.0220 -- 2.0578)  data: 0.1886 (0.0001 -- 1.8855)  max mem: 16413
Val: Total time: 0:00:03 (0.3433 s / it)
* Acc@1 70.122 Acc@5 95.732 loss 0.827
Accuracy of the network on the 163 val images: 70.12%
Max accuracy: 73.17%
Epoch: [95]  [ 0/54]  eta: 0:07:17  lr: 0.000026  min_lr: 0.000001  loss: 2.0793 (2.0793)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3544 (8.3544)  time: 8.0988 (8.0988 -- 8.0988)  data: 7.5624 (7.5624 -- 7.5624)  max mem: 16413
Epoch: [95]  [20/54]  eta: 0:00:41  lr: 0.000026  min_lr: 0.000001  loss: 1.7202 (1.7820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4545 (7.6927)  time: 0.8718 (0.5268 -- 4.4697)  data: 0.3242 (0.0004 -- 3.9555)  max mem: 16413
[2023-10-23 17:59:31,077] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5159
[2023-10-23 17:59:31,077] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5159
[2023-10-23 17:59:31,077] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 17:59:31,077] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 17:59:31,078] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [95]  [40/54]  eta: 0:00:14  lr: 0.000026  min_lr: 0.000001  loss: 1.8846 (1.8126)  loss_scale: 8192.0000 (13986.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2293 (7.6775)  time: 0.8200 (0.5177 -- 2.2561)  data: 0.2455 (0.0003 -- 1.6963)  max mem: 16413
Epoch: [95]  [53/54]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.8898 (1.8070)  loss_scale: 8192.0000 (12591.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9145 (7.5754)  time: 0.7254 (0.4934 -- 2.0746)  data: 0.1684 (0.0003 -- 1.1149)  max mem: 16413
Epoch: [95] Total time: 0:00:50 (0.9441 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.8898 (1.7550)  loss_scale: 8192.0000 (12591.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9145 (7.5754)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7406 (0.7406)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 1.9792 (1.9792 -- 1.9792)  data: 1.7803 (1.7803 -- 1.7803)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6492 (0.7066)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (96.3415)  time: 0.3376 (0.0275 -- 1.9792)  data: 0.1781 (0.0001 -- 1.7803)  max mem: 16413
Val: Total time: 0:00:03 (0.3377 s / it)
* Acc@1 72.561 Acc@5 95.732 loss 0.864
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.17%
Epoch: [96]  [ 0/54]  eta: 0:07:19  lr: 0.000026  min_lr: 0.000001  loss: 1.8405 (1.8405)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1671 (9.1671)  time: 8.1343 (8.1343 -- 8.1343)  data: 7.2246 (7.2246 -- 7.2246)  max mem: 16413
Epoch: [96]  [20/54]  eta: 0:00:43  lr: 0.000026  min_lr: 0.000001  loss: 1.7402 (1.7596)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5205 (7.6281)  time: 0.9276 (0.5173 -- 4.7690)  data: 0.1207 (0.0004 -- 2.0026)  max mem: 16413
Epoch: [96]  [40/54]  eta: 0:00:15  lr: 0.000026  min_lr: 0.000001  loss: 1.8157 (1.7998)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9475 (7.5479)  time: 0.8929 (0.5241 -- 3.2180)  data: 0.0015 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [96]  [53/54]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.7376 (1.7821)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4403 (7.6135)  time: 0.6492 (0.4942 -- 3.2149)  data: 0.0008 (0.0001 -- 0.0044)  max mem: 16413
Epoch: [96] Total time: 0:00:51 (0.9463 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.7376 (1.7848)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4403 (7.6135)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7156 (0.7156)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0833 (2.0833 -- 2.0833)  data: 1.9080 (1.9080 -- 1.9080)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6713 (0.7148)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (96.3415)  time: 0.3461 (0.0222 -- 2.0833)  data: 0.1909 (0.0001 -- 1.9080)  max mem: 16413
Val: Total time: 0:00:03 (0.3462 s / it)
* Acc@1 73.780 Acc@5 96.341 loss 0.846
Accuracy of the network on the 163 val images: 73.78%
[2023-10-23 18:00:47,140] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 18:00:47,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 18:00:47,141] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 18:00:47,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 18:00:48,441] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 18:00:48,442] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.78%
Epoch: [97]  [ 0/54]  eta: 0:06:55  lr: 0.000026  min_lr: 0.000001  loss: 2.1346 (2.1346)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2593 (10.2593)  time: 7.7026 (7.7026 -- 7.7026)  data: 7.1654 (7.1654 -- 7.1654)  max mem: 16413
Epoch: [97]  [20/54]  eta: 0:00:42  lr: 0.000025  min_lr: 0.000001  loss: 1.8843 (1.7780)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9711 (7.7221)  time: 0.9322 (0.5234 -- 3.7616)  data: 0.2639 (0.0003 -- 1.5963)  max mem: 16413
Epoch: [97]  [40/54]  eta: 0:00:15  lr: 0.000025  min_lr: 0.000001  loss: 1.8491 (1.7770)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4382 (8.1667)  time: 0.8892 (0.5139 -- 3.5284)  data: 0.1035 (0.0004 -- 2.0422)  max mem: 16413
[2023-10-23 18:01:37,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:01:37,753] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 18:01:37,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:01:37,794] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [97]  [53/54]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.7334 (1.7658)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2178 (8.0371)  time: 0.7037 (0.4961 -- 3.5284)  data: 0.1354 (0.0002 -- 2.0422)  max mem: 16413
Epoch: [97] Total time: 0:00:51 (0.9523 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.7334 (1.7468)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2178 (8.0371)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7703 (0.7703)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1102 (2.1102 -- 2.1102)  data: 1.9395 (1.9395 -- 1.9395)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6919 (0.7117)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3481 (0.0221 -- 2.1102)  data: 0.1940 (0.0001 -- 1.9395)  max mem: 16413
Val: Total time: 0:00:03 (0.3482 s / it)
* Acc@1 71.951 Acc@5 96.341 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [98]  [ 0/54]  eta: 0:06:14  lr: 0.000025  min_lr: 0.000001  loss: 1.5639 (1.5639)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3190 (6.3190)  time: 6.9305 (6.9305 -- 6.9305)  data: 5.7996 (5.7996 -- 5.7996)  max mem: 16413
Epoch: [98]  [20/54]  eta: 0:00:42  lr: 0.000025  min_lr: 0.000001  loss: 1.7874 (1.7735)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1511 (7.4623)  time: 0.9751 (0.5248 -- 3.4718)  data: 0.1210 (0.0002 -- 1.3860)  max mem: 16413
Epoch: [98]  [40/54]  eta: 0:00:15  lr: 0.000025  min_lr: 0.000001  loss: 1.6752 (1.7130)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9767 (7.5374)  time: 0.8976 (0.5134 -- 5.2537)  data: 0.0138 (0.0003 -- 0.2532)  max mem: 16413
Epoch: [98]  [53/54]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.8002 (1.7369)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2906 (7.6005)  time: 0.6667 (0.4944 -- 2.7939)  data: 0.0006 (0.0001 -- 0.0014)  max mem: 16413
Epoch: [98] Total time: 0:00:51 (0.9589 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.8002 (1.7423)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2906 (7.6005)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7787 (0.7787)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9137 (1.9137 -- 1.9137)  data: 1.7098 (1.7098 -- 1.7098)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7007 (0.7308)  acc1: 77.7778 (75.6098)  acc5: 100.0000 (97.5610)  time: 0.3290 (0.0223 -- 1.9137)  data: 0.1711 (0.0001 -- 1.7098)  max mem: 16413
Val: Total time: 0:00:03 (0.3292 s / it)
* Acc@1 70.732 Acc@5 96.341 loss 0.891
Accuracy of the network on the 163 val images: 70.73%
Max accuracy: 73.78%
Epoch: [99]  [ 0/54]  eta: 0:06:15  lr: 0.000025  min_lr: 0.000001  loss: 1.7969 (1.7969)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4832 (7.4832)  time: 6.9467 (6.9467 -- 6.9467)  data: 5.2465 (5.2465 -- 5.2465)  max mem: 16413
Epoch: [99]  [20/54]  eta: 0:00:39  lr: 0.000025  min_lr: 0.000001  loss: 1.5727 (1.6509)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4210 (7.7426)  time: 0.8809 (0.5150 -- 3.6964)  data: 0.3402 (0.0003 -- 3.1681)  max mem: 16413
Epoch: [99]  [40/54]  eta: 0:00:14  lr: 0.000025  min_lr: 0.000001  loss: 1.6736 (1.6721)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0681 (7.8430)  time: 0.8350 (0.5235 -- 2.9099)  data: 0.2914 (0.0004 -- 2.3804)  max mem: 16413
Epoch: [99]  [53/54]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.8958 (1.7549)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3232 (8.3217)  time: 0.6087 (0.4939 -- 2.3430)  data: 0.0924 (0.0002 -- 1.8303)  max mem: 16413
Epoch: [99] Total time: 0:00:49 (0.9199 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.8958 (1.7353)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3232 (8.3217)
[2023-10-23 18:03:28,241] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-10-23 18:03:28,244] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-99/mp_rank_00_model_states.pt
[2023-10-23 18:03:28,244] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-10-23 18:03:28,244] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-99/mp_rank_00_model_states.pt...
[2023-10-23 18:03:29,508] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-99/mp_rank_00_model_states.pt.
[2023-10-23 18:03:29,509] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/10]  eta: 0:00:21  loss: 0.8260 (0.8260)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1042 (2.1042 -- 2.1042)  data: 1.9267 (1.9267 -- 1.9267)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6987 (0.6898)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (97.5610)  time: 0.3494 (0.0231 -- 2.1042)  data: 0.1928 (0.0001 -- 1.9267)  max mem: 16413
Val: Total time: 0:00:03 (0.3495 s / it)
* Acc@1 71.341 Acc@5 96.341 loss 0.869
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.78%
Epoch: [100]  [ 0/54]  eta: 0:07:42  lr: 0.000024  min_lr: 0.000001  loss: 1.6365 (1.6365)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4147 (9.4147)  time: 8.5584 (8.5584 -- 8.5584)  data: 7.9962 (7.9962 -- 7.9962)  max mem: 16413
[2023-10-23 18:03:56,177] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:03:56,177] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:03:56,180] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:03:56,180] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:03:57,765] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5419
[2023-10-23 18:03:57,765] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5419
[2023-10-23 18:03:57,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:03:57,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:03:57,765] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [20/54]  eta: 0:00:40  lr: 0.000024  min_lr: 0.000001  loss: 1.8118 (1.7912)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7443 (8.2796)  time: 0.8371 (0.5174 -- 3.6907)  data: 0.2964 (0.0003 -- 3.1454)  max mem: 16413
Epoch: [100]  [40/54]  eta: 0:00:15  lr: 0.000024  min_lr: 0.000001  loss: 1.6679 (1.7445)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7548 (7.4748)  time: 0.9741 (0.5235 -- 4.1738)  data: 0.1664 (0.0003 -- 1.2916)  max mem: 16413
Epoch: [100]  [53/54]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.8615 (1.7442)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4414 (7.5176)  time: 0.7498 (0.4961 -- 4.1738)  data: 0.0505 (0.0001 -- 0.9966)  max mem: 16413
Epoch: [100] Total time: 0:00:52 (0.9696 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.8615 (1.7712)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4414 (7.5176)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7788 (0.7788)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0888 (2.0888 -- 2.0888)  data: 1.9196 (1.9196 -- 1.9196)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7041 (0.7066)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3459 (0.0220 -- 2.0888)  data: 0.1920 (0.0001 -- 1.9196)  max mem: 16413
Val: Total time: 0:00:03 (0.3460 s / it)
* Acc@1 71.951 Acc@5 96.341 loss 0.878
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [101]  [ 0/54]  eta: 0:06:54  lr: 0.000024  min_lr: 0.000001  loss: 1.3508 (1.3508)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2243 (6.2243)  time: 7.6770 (7.6770 -- 7.6770)  data: 7.1357 (7.1357 -- 7.1357)  max mem: 16413
Epoch: [101]  [20/54]  eta: 0:00:38  lr: 0.000024  min_lr: 0.000001  loss: 1.9677 (1.8280)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9309 (7.1264)  time: 0.7998 (0.5321 -- 3.3110)  data: 0.2483 (0.0002 -- 2.7487)  max mem: 16413
Epoch: [101]  [40/54]  eta: 0:00:14  lr: 0.000024  min_lr: 0.000001  loss: 1.6621 (1.7779)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8999 (7.2472)  time: 0.9160 (0.5249 -- 3.4248)  data: 0.3718 (0.0003 -- 2.8960)  max mem: 16413
Epoch: [101]  [53/54]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.6413 (1.7653)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3993 (7.4860)  time: 0.6781 (0.4930 -- 2.2314)  data: 0.1572 (0.0002 -- 1.7019)  max mem: 16413
Epoch: [101] Total time: 0:00:50 (0.9271 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.6413 (1.7633)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3993 (7.4860)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8022 (0.8022)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0424 (2.0424 -- 2.0424)  data: 1.8626 (1.8626 -- 1.8626)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7168 (0.7033)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (96.3415)  time: 0.3426 (0.0258 -- 2.0424)  data: 0.1863 (0.0001 -- 1.8626)  max mem: 16413
Val: Total time: 0:00:03 (0.3427 s / it)
* Acc@1 71.951 Acc@5 95.732 loss 0.876
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [102]  [ 0/54]  eta: 0:06:41  lr: 0.000024  min_lr: 0.000001  loss: 1.9270 (1.9270)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0266 (8.0266)  time: 7.4293 (7.4293 -- 7.4293)  data: 6.8844 (6.8844 -- 6.8844)  max mem: 16413
Epoch: [102]  [20/54]  eta: 0:00:46  lr: 0.000024  min_lr: 0.000001  loss: 1.6683 (1.7798)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4741 (8.2506)  time: 1.0765 (0.4979 -- 6.0439)  data: 0.0011 (0.0003 -- 0.0032)  max mem: 16413
[2023-10-23 18:05:52,368] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5530
[2023-10-23 18:05:52,368] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5530
[2023-10-23 18:05:52,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:05:52,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:05:52,369] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [102]  [40/54]  eta: 0:00:15  lr: 0.000023  min_lr: 0.000001  loss: 1.8423 (1.7512)  loss_scale: 8192.0000 (12587.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8743 (7.9798)  time: 0.8852 (0.4972 -- 3.6456)  data: 0.0015 (0.0001 -- 0.0059)  max mem: 16413
Epoch: [102]  [53/54]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7153 (1.7241)  loss_scale: 8192.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8357 (8.1698)  time: 0.6710 (0.4946 -- 3.6456)  data: 0.0006 (0.0001 -- 0.0019)  max mem: 16413
Epoch: [102] Total time: 0:00:53 (0.9854 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7153 (1.7663)  loss_scale: 8192.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8357 (8.1698)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7711 (0.7711)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0881 (2.0881 -- 2.0881)  data: 1.9083 (1.9083 -- 1.9083)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6984 (0.6989)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3469 (0.0221 -- 2.0881)  data: 0.1909 (0.0001 -- 1.9083)  max mem: 16413
Val: Total time: 0:00:03 (0.3470 s / it)
* Acc@1 72.561 Acc@5 96.341 loss 0.871
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [103]  [ 0/54]  eta: 0:06:52  lr: 0.000023  min_lr: 0.000001  loss: 1.7101 (1.7101)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4051 (7.4051)  time: 7.6383 (7.6383 -- 7.6383)  data: 5.6364 (5.6364 -- 5.6364)  max mem: 16413
Epoch: [103]  [20/54]  eta: 0:00:40  lr: 0.000023  min_lr: 0.000001  loss: 1.7706 (1.7455)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3089 (7.8854)  time: 0.8804 (0.5283 -- 2.6282)  data: 0.0906 (0.0006 -- 0.7218)  max mem: 16413
Epoch: [103]  [40/54]  eta: 0:00:15  lr: 0.000023  min_lr: 0.000001  loss: 1.6792 (1.7128)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8265 (7.9243)  time: 0.9462 (0.5250 -- 3.2405)  data: 0.2619 (0.0004 -- 2.7245)  max mem: 16413
Epoch: [103]  [53/54]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7272 (1.7106)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9716 (7.7797)  time: 0.6942 (0.4936 -- 3.2405)  data: 0.1779 (0.0001 -- 2.7245)  max mem: 16413
Epoch: [103] Total time: 0:00:51 (0.9543 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7272 (1.7451)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9716 (7.7797)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7819 (0.7819)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0831 (2.0831 -- 2.0831)  data: 1.9044 (1.9044 -- 1.9044)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6682 (0.6933)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (97.5610)  time: 0.3455 (0.0220 -- 2.0831)  data: 0.1905 (0.0001 -- 1.9044)  max mem: 16413
Val: Total time: 0:00:03 (0.3456 s / it)
* Acc@1 70.732 Acc@5 96.341 loss 0.861
Accuracy of the network on the 163 val images: 70.73%
Max accuracy: 73.78%
Epoch: [104]  [ 0/54]  eta: 0:06:58  lr: 0.000023  min_lr: 0.000001  loss: 1.5979 (1.5979)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6505 (10.6505)  time: 7.7515 (7.7515 -- 7.7515)  data: 4.9728 (4.9728 -- 4.9728)  max mem: 16413
Epoch: [104]  [20/54]  eta: 0:00:40  lr: 0.000023  min_lr: 0.000001  loss: 1.8978 (1.8700)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2697 (7.9057)  time: 0.8624 (0.5191 -- 2.4244)  data: 0.0808 (0.0003 -- 1.1346)  max mem: 16413
Epoch: [104]  [40/54]  eta: 0:00:14  lr: 0.000023  min_lr: 0.000001  loss: 1.7675 (1.7912)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5823 (7.7703)  time: 0.8935 (0.5234 -- 4.5397)  data: 0.3534 (0.0003 -- 3.9859)  max mem: 16413
[2023-10-23 18:08:01,316] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:08:01,316] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 18:08:01,316] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:08:01,317] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [104]  [53/54]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.6177 (1.7509)  loss_scale: 16384.0000 (9860.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5965 (7.8471)  time: 0.6622 (0.4945 -- 3.3566)  data: 0.1491 (0.0002 -- 2.8519)  max mem: 16413
Epoch: [104] Total time: 0:00:52 (0.9699 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.6177 (1.7504)  loss_scale: 16384.0000 (9860.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5965 (7.8471)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7813 (0.7813)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0231 (2.0231 -- 2.0231)  data: 1.8305 (1.8305 -- 1.8305)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6602 (0.6890)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3396 (0.0220 -- 2.0231)  data: 0.1831 (0.0001 -- 1.8305)  max mem: 16413
Val: Total time: 0:00:03 (0.3397 s / it)
* Acc@1 71.951 Acc@5 96.341 loss 0.865
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [105]  [ 0/54]  eta: 0:07:04  lr: 0.000023  min_lr: 0.000001  loss: 2.0574 (2.0574)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1713 (6.1713)  time: 7.8637 (7.8637 -- 7.8637)  data: 6.5635 (6.5635 -- 6.5635)  max mem: 16413
Epoch: [105]  [20/54]  eta: 0:00:41  lr: 0.000022  min_lr: 0.000001  loss: 1.7978 (1.7810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7074 (7.4077)  time: 0.8860 (0.5186 -- 5.0446)  data: 0.2165 (0.0002 -- 2.2135)  max mem: 16413
Epoch: [105]  [40/54]  eta: 0:00:15  lr: 0.000022  min_lr: 0.000001  loss: 1.9325 (1.8337)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4557 (7.6033)  time: 0.9634 (0.5229 -- 4.1812)  data: 0.1818 (0.0002 -- 3.1819)  max mem: 16413
Epoch: [105]  [53/54]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.6154 (1.7796)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2956 (7.8062)  time: 0.6568 (0.4947 -- 3.0772)  data: 0.0073 (0.0001 -- 0.1295)  max mem: 16413
Epoch: [105] Total time: 0:00:51 (0.9551 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.6154 (1.7577)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2956 (7.8062)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7773 (0.7773)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0151 (2.0151 -- 2.0151)  data: 1.8371 (1.8371 -- 1.8371)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6582 (0.6794)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3390 (0.0258 -- 2.0151)  data: 0.1838 (0.0001 -- 1.8371)  max mem: 16413
Val: Total time: 0:00:03 (0.3391 s / it)
* Acc@1 71.341 Acc@5 96.341 loss 0.855
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.78%
Epoch: [106]  [ 0/54]  eta: 0:06:09  lr: 0.000022  min_lr: 0.000001  loss: 2.1513 (2.1513)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6966 (7.6966)  time: 6.8363 (6.8363 -- 6.8363)  data: 6.1594 (6.1594 -- 6.1594)  max mem: 16413
Epoch: [106]  [20/54]  eta: 0:00:40  lr: 0.000022  min_lr: 0.000001  loss: 1.6998 (1.7628)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7542 (7.2825)  time: 0.9211 (0.5102 -- 4.4483)  data: 0.3797 (0.0003 -- 3.9312)  max mem: 16413
Epoch: [106]  [40/54]  eta: 0:00:14  lr: 0.000022  min_lr: 0.000001  loss: 1.7384 (1.7925)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7324 (7.2124)  time: 0.8462 (0.5248 -- 3.0277)  data: 0.2965 (0.0003 -- 2.4914)  max mem: 16413
Epoch: [106]  [53/54]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.6854 (1.7696)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4824 (7.2532)  time: 0.7101 (0.4958 -- 2.8750)  data: 0.0727 (0.0001 -- 1.2072)  max mem: 16413
Epoch: [106] Total time: 0:00:51 (0.9512 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.6854 (1.7501)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4824 (7.2532)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7856 (0.7856)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0816 (2.0816 -- 2.0816)  data: 1.9002 (1.9002 -- 1.9002)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6509 (0.6865)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (97.5610)  time: 0.3454 (0.0221 -- 2.0816)  data: 0.1901 (0.0001 -- 1.9002)  max mem: 16413
Val: Total time: 0:00:03 (0.3455 s / it)
* Acc@1 72.561 Acc@5 96.341 loss 0.867
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [107]  [ 0/54]  eta: 0:06:52  lr: 0.000022  min_lr: 0.000001  loss: 1.3196 (1.3196)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0117 (8.0117)  time: 7.6333 (7.6333 -- 7.6333)  data: 5.6631 (5.6631 -- 5.6631)  max mem: 16413
[2023-10-23 18:10:15,386] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:10:15,386] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:10:15,389] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:10:15,389] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [107]  [20/54]  eta: 0:00:41  lr: 0.000022  min_lr: 0.000001  loss: 1.7954 (1.8017)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5655 (7.9338)  time: 0.8909 (0.5218 -- 3.4200)  data: 0.2116 (0.0004 -- 1.3966)  max mem: 16413
Epoch: [107]  [40/54]  eta: 0:00:15  lr: 0.000021  min_lr: 0.000001  loss: 1.5874 (1.7187)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1077 (8.0738)  time: 0.9632 (0.5269 -- 4.5606)  data: 0.1216 (0.0003 -- 1.9268)  max mem: 16413
[2023-10-23 18:10:48,438] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5826
[2023-10-23 18:10:48,439] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5826
[2023-10-23 18:10:48,439] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:10:48,439] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:10:48,439] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [107]  [53/54]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.6797 (1.7386)  loss_scale: 32768.0000 (28216.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5220 (8.0127)  time: 0.7564 (0.4901 -- 4.5606)  data: 0.1105 (0.0001 -- 1.9268)  max mem: 16413
Epoch: [107] Total time: 0:00:51 (0.9547 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.6797 (1.7681)  loss_scale: 32768.0000 (28216.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5220 (8.0127)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7882 (0.7882)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9944 (1.9944 -- 1.9944)  data: 1.7765 (1.7765 -- 1.7765)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6748 (0.6869)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3384 (0.0222 -- 1.9944)  data: 0.1778 (0.0001 -- 1.7765)  max mem: 16413
Val: Total time: 0:00:03 (0.3385 s / it)
* Acc@1 73.780 Acc@5 95.732 loss 0.860
Accuracy of the network on the 163 val images: 73.78%
Max accuracy: 73.78%
Epoch: [108]  [ 0/54]  eta: 0:07:08  lr: 0.000021  min_lr: 0.000001  loss: 2.0981 (2.0981)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4244 (5.4244)  time: 7.9331 (7.9331 -- 7.9331)  data: 7.2794 (7.2794 -- 7.2794)  max mem: 16413
Epoch: [108]  [20/54]  eta: 0:00:39  lr: 0.000021  min_lr: 0.000001  loss: 1.6903 (1.7181)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1002 (6.8805)  time: 0.8212 (0.5276 -- 2.8288)  data: 0.1764 (0.0005 -- 1.7842)  max mem: 16413
Epoch: [108]  [40/54]  eta: 0:00:14  lr: 0.000021  min_lr: 0.000001  loss: 1.8777 (1.8241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1653 (7.7251)  time: 0.9624 (0.5324 -- 3.5110)  data: 0.3798 (0.0002 -- 2.8551)  max mem: 16413
Epoch: [108]  [53/54]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.6810 (1.8083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1653 (7.7776)  time: 0.6612 (0.4956 -- 2.9957)  data: 0.1188 (0.0002 -- 2.0070)  max mem: 16413
Epoch: [108] Total time: 0:00:50 (0.9369 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.6810 (1.7620)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1653 (7.7776)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8090 (0.8090)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9833 (1.9833 -- 1.9833)  data: 1.7668 (1.7668 -- 1.7668)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6675 (0.6811)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (97.5610)  time: 0.3364 (0.0220 -- 1.9833)  data: 0.1768 (0.0001 -- 1.7668)  max mem: 16413
Val: Total time: 0:00:03 (0.3365 s / it)
* Acc@1 72.561 Acc@5 95.732 loss 0.865
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [109]  [ 0/54]  eta: 0:06:49  lr: 0.000021  min_lr: 0.000000  loss: 2.1401 (2.1401)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0121 (5.0121)  time: 7.5753 (7.5753 -- 7.5753)  data: 6.7512 (6.7512 -- 6.7512)  max mem: 16413
Epoch: [109]  [20/54]  eta: 0:00:42  lr: 0.000021  min_lr: 0.000000  loss: 1.5053 (1.5954)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7379 (7.3920)  time: 0.9368 (0.5151 -- 3.1907)  data: 0.3852 (0.0003 -- 2.5875)  max mem: 16413
Epoch: [109]  [40/54]  eta: 0:00:15  lr: 0.000021  min_lr: 0.000000  loss: 1.8502 (1.7003)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8239 (7.7869)  time: 0.8889 (0.5224 -- 2.5335)  data: 0.3515 (0.0002 -- 2.0082)  max mem: 16413
Epoch: [109]  [53/54]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.8396 (1.7529)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4109 (7.7882)  time: 0.6374 (0.4990 -- 2.3382)  data: 0.1184 (0.0001 -- 1.8121)  max mem: 16413
Epoch: [109] Total time: 0:00:51 (0.9484 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.8396 (1.7484)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4109 (7.7882)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7713 (0.7713)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9182 (1.9182 -- 1.9182)  data: 1.6923 (1.6923 -- 1.6923)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6782 (0.6924)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (95.1220)  time: 0.3345 (0.0221 -- 1.9182)  data: 0.1735 (0.0001 -- 1.6923)  max mem: 16413
Val: Total time: 0:00:03 (0.3346 s / it)
* Acc@1 73.171 Acc@5 94.512 loss 0.870
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [110]  [ 0/54]  eta: 0:07:22  lr: 0.000021  min_lr: 0.000000  loss: 1.6451 (1.6451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2874 (8.2874)  time: 8.2003 (8.2003 -- 8.2003)  data: 6.4852 (6.4852 -- 6.4852)  max mem: 16413
[2023-10-23 18:13:03,445] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:13:03,445] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:13:03,446] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:13:03,446] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [110]  [20/54]  eta: 0:00:42  lr: 0.000021  min_lr: 0.000000  loss: 1.8183 (1.7103)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7731 (6.8796)  time: 0.9031 (0.5237 -- 4.3579)  data: 0.0861 (0.0002 -- 1.0268)  max mem: 16413
[2023-10-23 18:13:14,819] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5965
[2023-10-23 18:13:14,819] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5965
[2023-10-23 18:13:14,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:13:14,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:13:14,820] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [110]  [40/54]  eta: 0:00:15  lr: 0.000020  min_lr: 0.000000  loss: 1.7212 (1.7256)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0377 (7.2692)  time: 0.9542 (0.5221 -- 3.4917)  data: 0.0368 (0.0004 -- 0.6997)  max mem: 16413
Epoch: [110]  [53/54]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.6848 (1.7263)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0807 (7.4500)  time: 0.6591 (0.4938 -- 3.4917)  data: 0.0008 (0.0001 -- 0.0036)  max mem: 16413
Epoch: [110] Total time: 0:00:51 (0.9596 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.6848 (1.7600)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0807 (7.4500)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7403 (0.7403)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0286 (2.0286 -- 2.0286)  data: 1.8504 (1.8504 -- 1.8504)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6816 (0.6960)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (96.3415)  time: 0.3402 (0.0220 -- 2.0286)  data: 0.1851 (0.0001 -- 1.8504)  max mem: 16413
Val: Total time: 0:00:03 (0.3403 s / it)
* Acc@1 73.171 Acc@5 95.732 loss 0.853
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [111]  [ 0/54]  eta: 0:07:37  lr: 0.000020  min_lr: 0.000000  loss: 1.5216 (1.5216)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7199 (6.7199)  time: 8.4770 (8.4770 -- 8.4770)  data: 7.9592 (7.9592 -- 7.9592)  max mem: 16413
[2023-10-23 18:13:49,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=30, lr=[4.806095045291858e-07, 4.806095045291858e-07, 6.408126727055812e-07, 6.408126727055812e-07, 8.544168969407748e-07, 8.544168969407748e-07, 1.1392225292543665e-06, 1.1392225292543665e-06, 1.5189633723391553e-06, 1.5189633723391553e-06, 2.025284496452207e-06, 2.025284496452207e-06, 2.7003793286029426e-06, 2.7003793286029426e-06, 3.6005057714705903e-06, 3.6005057714705903e-06, 4.800674361960787e-06, 4.800674361960787e-06, 6.400899149281049e-06, 6.400899149281049e-06, 8.5345321990414e-06, 8.5345321990414e-06, 1.1379376265388532e-05, 1.1379376265388532e-05, 1.517250168718471e-05, 1.517250168718471e-05, 2.0230002249579613e-05, 2.0230002249579613e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 18:13:49,645] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=18.997238610208935, CurrSamplesPerSec=23.202744223590972, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [111]  [20/54]  eta: 0:00:42  lr: 0.000020  min_lr: 0.000000  loss: 1.7745 (1.8043)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1333 (7.4790)  time: 0.8935 (0.5245 -- 3.9572)  data: 0.1823 (0.0005 -- 1.9011)  max mem: 16413
Epoch: [111]  [40/54]  eta: 0:00:15  lr: 0.000020  min_lr: 0.000000  loss: 1.6870 (1.7214)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2559 (7.3751)  time: 0.9302 (0.5227 -- 4.5631)  data: 0.3665 (0.0002 -- 4.0469)  max mem: 16413
Epoch: [111]  [53/54]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.5848 (1.7086)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4056 (7.7243)  time: 0.6576 (0.4969 -- 3.0726)  data: 0.1385 (0.0002 -- 2.5698)  max mem: 16413
Epoch: [111] Total time: 0:00:51 (0.9592 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.5848 (1.7590)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4056 (7.7243)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7634 (0.7634)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9860 (1.9860 -- 1.9860)  data: 1.7984 (1.7984 -- 1.7984)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6708 (0.6860)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (96.3415)  time: 0.3360 (0.0220 -- 1.9860)  data: 0.1799 (0.0001 -- 1.7984)  max mem: 16413
Val: Total time: 0:00:03 (0.3361 s / it)
* Acc@1 71.341 Acc@5 95.732 loss 0.853
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.78%
Epoch: [112]  [ 0/54]  eta: 0:06:20  lr: 0.000020  min_lr: 0.000000  loss: 1.5416 (1.5416)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3356 (8.3356)  time: 7.0486 (7.0486 -- 7.0486)  data: 6.4884 (6.4884 -- 6.4884)  max mem: 16413
Epoch: [112]  [20/54]  eta: 0:00:40  lr: 0.000020  min_lr: 0.000000  loss: 1.7471 (1.7944)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1521 (7.8059)  time: 0.8989 (0.5202 -- 3.9624)  data: 0.3575 (0.0009 -- 3.4334)  max mem: 16413
Epoch: [112]  [40/54]  eta: 0:00:15  lr: 0.000020  min_lr: 0.000000  loss: 1.8792 (1.8109)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9263 (7.6594)  time: 0.9960 (0.5295 -- 4.5355)  data: 0.3306 (0.0005 -- 2.8113)  max mem: 16413
[2023-10-23 18:15:21,517] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:15:21,518] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:15:21,518] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:15:21,518] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [112]  [53/54]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7540 (1.7790)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8259 (7.4240)  time: 0.7154 (0.4935 -- 4.5355)  data: 0.0884 (0.0001 -- 1.7550)  max mem: 16413
Epoch: [112] Total time: 0:00:51 (0.9523 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7540 (1.7612)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8259 (7.4240)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7762 (0.7762)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9122 (1.9122 -- 1.9122)  data: 1.7094 (1.7094 -- 1.7094)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6958 (0.7047)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (97.5610)  time: 0.3287 (0.0222 -- 1.9122)  data: 0.1710 (0.0001 -- 1.7094)  max mem: 16413
Val: Total time: 0:00:03 (0.3288 s / it)
* Acc@1 71.951 Acc@5 96.341 loss 0.875
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [113]  [ 0/54]  eta: 0:06:32  lr: 0.000020  min_lr: 0.000000  loss: 2.2595 (2.2595)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0381 (12.0381)  time: 7.2648 (7.2648 -- 7.2648)  data: 6.7467 (6.7467 -- 6.7467)  max mem: 16413
Epoch: [113]  [20/54]  eta: 0:00:42  lr: 0.000019  min_lr: 0.000000  loss: 1.6833 (1.6843)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1833 (7.5654)  time: 0.9340 (0.5276 -- 4.0712)  data: 0.3816 (0.0006 -- 3.5312)  max mem: 16413
Epoch: [113]  [40/54]  eta: 0:00:14  lr: 0.000019  min_lr: 0.000000  loss: 1.6904 (1.6641)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7389 (7.4072)  time: 0.7868 (0.5198 -- 3.1768)  data: 0.2441 (0.0002 -- 2.6551)  max mem: 16413
[2023-10-23 18:16:12,601] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6143
[2023-10-23 18:16:12,601] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6143
[2023-10-23 18:16:12,601] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:16:12,601] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:16:12,601] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [113]  [53/54]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.6736 (1.7079)  loss_scale: 16384.0000 (28823.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6670 (7.6472)  time: 0.6411 (0.4975 -- 2.5255)  data: 0.0447 (0.0002 -- 0.5834)  max mem: 16413
Epoch: [113] Total time: 0:00:50 (0.9372 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.6736 (1.7119)  loss_scale: 16384.0000 (28823.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6670 (7.6472)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7906 (0.7906)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9205 (1.9205 -- 1.9205)  data: 1.7265 (1.7265 -- 1.7265)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6856 (0.6981)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (97.5610)  time: 0.3296 (0.0222 -- 1.9205)  data: 0.1727 (0.0001 -- 1.7265)  max mem: 16413
Val: Total time: 0:00:03 (0.3298 s / it)
* Acc@1 71.341 Acc@5 95.732 loss 0.880
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.78%
Epoch: [114]  [ 0/54]  eta: 0:07:48  lr: 0.000019  min_lr: 0.000000  loss: 1.2085 (1.2085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1909 (6.1909)  time: 8.6772 (8.6772 -- 8.6772)  data: 8.1554 (8.1554 -- 8.1554)  max mem: 16413
Epoch: [114]  [20/54]  eta: 0:00:39  lr: 0.000019  min_lr: 0.000000  loss: 1.7464 (1.7229)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2428 (7.1439)  time: 0.7831 (0.5256 -- 3.2821)  data: 0.2234 (0.0004 -- 2.7298)  max mem: 16413
Epoch: [114]  [40/54]  eta: 0:00:14  lr: 0.000019  min_lr: 0.000000  loss: 1.8166 (1.7235)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0731 (7.2998)  time: 0.9177 (0.5262 -- 2.3922)  data: 0.1670 (0.0009 -- 1.8496)  max mem: 16413
Epoch: [114]  [53/54]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.5650 (1.7001)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8641 (7.6615)  time: 0.7164 (0.4954 -- 2.1633)  data: 0.0008 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [114] Total time: 0:00:51 (0.9613 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.5650 (1.7221)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8641 (7.6615)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7976 (0.7976)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0524 (2.0524 -- 2.0524)  data: 1.8749 (1.8749 -- 1.8749)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6934 (0.7069)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (97.5610)  time: 0.3430 (0.0222 -- 2.0524)  data: 0.1876 (0.0001 -- 1.8749)  max mem: 16413
Val: Total time: 0:00:03 (0.3431 s / it)
* Acc@1 71.951 Acc@5 95.732 loss 0.891
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [115]  [ 0/54]  eta: 0:07:20  lr: 0.000019  min_lr: 0.000000  loss: 1.6409 (1.6409)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4495 (6.4495)  time: 8.1506 (8.1506 -- 8.1506)  data: 5.7435 (5.7435 -- 5.7435)  max mem: 16413
Epoch: [115]  [20/54]  eta: 0:00:39  lr: 0.000019  min_lr: 0.000000  loss: 1.7346 (1.7433)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5032 (8.6237)  time: 0.8122 (0.5244 -- 3.1647)  data: 0.0052 (0.0003 -- 0.0759)  max mem: 16413
Epoch: [115]  [40/54]  eta: 0:00:14  lr: 0.000019  min_lr: 0.000000  loss: 1.7955 (1.7906)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2806 (8.0145)  time: 0.9707 (0.5225 -- 4.9330)  data: 0.1063 (0.0003 -- 1.8975)  max mem: 16413
Epoch: [115]  [53/54]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7843 (1.8136)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2806 (7.8984)  time: 0.6219 (0.4972 -- 1.6144)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [115] Total time: 0:00:51 (0.9514 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7843 (1.7766)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2806 (7.8984)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.8310 (0.8310)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1143 (2.1143 -- 2.1143)  data: 1.9335 (1.9335 -- 1.9335)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6943 (0.6972)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (97.5610)  time: 0.3488 (0.0220 -- 2.1143)  data: 0.1934 (0.0001 -- 1.9335)  max mem: 16413
Val: Total time: 0:00:03 (0.3490 s / it)
* Acc@1 71.951 Acc@5 95.732 loss 0.891
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [116]  [ 0/54]  eta: 0:07:15  lr: 0.000018  min_lr: 0.000000  loss: 1.9305 (1.9305)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8063 (8.8063)  time: 8.0624 (8.0624 -- 8.0624)  data: 7.5064 (7.5064 -- 7.5064)  max mem: 16413
[2023-10-23 18:18:28,893] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:18:28,893] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:18:28,894] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:18:28,894] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:18:30,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6275
[2023-10-23 18:18:30,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6275
[2023-10-23 18:18:30,500] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:18:30,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 18:18:30,500] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [116]  [20/54]  eta: 0:00:41  lr: 0.000018  min_lr: 0.000000  loss: 1.8328 (1.8060)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4033 (7.4307)  time: 0.8730 (0.5039 -- 4.3911)  data: 0.3330 (0.0002 -- 3.8612)  max mem: 16413
Epoch: [116]  [40/54]  eta: 0:00:14  lr: 0.000018  min_lr: 0.000000  loss: 2.0292 (1.8607)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7742 (7.4371)  time: 0.8499 (0.5287 -- 2.5415)  data: 0.3050 (0.0003 -- 2.0112)  max mem: 16413
Epoch: [116]  [53/54]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7738 (1.8660)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8307 (7.4646)  time: 0.6682 (0.4932 -- 2.1712)  data: 0.1463 (0.0002 -- 1.6375)  max mem: 16413
Epoch: [116] Total time: 0:00:50 (0.9339 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7738 (1.8072)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8307 (7.4646)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8378 (0.8378)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0951 (2.0951 -- 2.0951)  data: 1.9203 (1.9203 -- 1.9203)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7080 (0.7146)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (96.3415)  time: 0.3473 (0.0259 -- 2.0951)  data: 0.1921 (0.0001 -- 1.9203)  max mem: 16413
Val: Total time: 0:00:03 (0.3474 s / it)
* Acc@1 71.341 Acc@5 94.512 loss 0.912
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.78%
Epoch: [117]  [ 0/54]  eta: 0:08:03  lr: 0.000018  min_lr: 0.000000  loss: 2.0327 (2.0327)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8412 (4.8412)  time: 8.9599 (8.9599 -- 8.9599)  data: 8.4131 (8.4131 -- 8.4131)  max mem: 16413
Epoch: [117]  [20/54]  eta: 0:00:43  lr: 0.000018  min_lr: 0.000000  loss: 1.6245 (1.6786)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1265 (7.9219)  time: 0.8826 (0.5224 -- 3.5235)  data: 0.3470 (0.0004 -- 3.0053)  max mem: 16413
Epoch: [117]  [40/54]  eta: 0:00:15  lr: 0.000018  min_lr: 0.000000  loss: 1.7377 (1.6763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0356 (7.6445)  time: 0.8816 (0.5207 -- 3.2344)  data: 0.3450 (0.0001 -- 2.7225)  max mem: 16413
Epoch: [117]  [53/54]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9259 (1.7301)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6935 (7.6711)  time: 0.6808 (0.4953 -- 3.2344)  data: 0.1659 (0.0001 -- 2.7225)  max mem: 16413
Epoch: [117] Total time: 0:00:50 (0.9436 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9259 (1.7667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6935 (7.6711)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8204 (0.8204)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0785 (2.0785 -- 2.0785)  data: 1.9049 (1.9049 -- 1.9049)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7053 (0.7072)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3466 (0.0220 -- 2.0785)  data: 0.1906 (0.0001 -- 1.9049)  max mem: 16413
Val: Total time: 0:00:03 (0.3467 s / it)
* Acc@1 70.732 Acc@5 95.732 loss 0.906
Accuracy of the network on the 163 val images: 70.73%
Max accuracy: 73.78%
Epoch: [118]  [ 0/54]  eta: 0:08:16  lr: 0.000018  min_lr: 0.000000  loss: 1.6634 (1.6634)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1998 (12.1998)  time: 9.2002 (9.2002 -- 9.2002)  data: 7.7778 (7.7778 -- 7.7778)  max mem: 16413
Epoch: [118]  [20/54]  eta: 0:00:40  lr: 0.000018  min_lr: 0.000000  loss: 1.7678 (1.7683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4200 (7.6454)  time: 0.8054 (0.5146 -- 3.4194)  data: 0.1683 (0.0002 -- 1.9725)  max mem: 16413
[2023-10-23 18:20:39,280] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:20:39,280] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:20:39,282] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:20:39,282] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [118]  [40/54]  eta: 0:00:15  lr: 0.000017  min_lr: 0.000000  loss: 1.8906 (1.7809)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3817 (7.8187)  time: 0.9921 (0.5179 -- 4.9087)  data: 0.0253 (0.0002 -- 0.4778)  max mem: 16413
Epoch: [118]  [53/54]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.7569 (1.8099)  loss_scale: 32768.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7509 (7.5773)  time: 0.6879 (0.4948 -- 3.0258)  data: 0.0006 (0.0001 -- 0.0019)  max mem: 16413
Epoch: [118] Total time: 0:00:52 (0.9751 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.7569 (1.7863)  loss_scale: 32768.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7509 (7.5773)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7963 (0.7963)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1407 (2.1407 -- 2.1407)  data: 1.9695 (1.9695 -- 1.9695)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6949 (0.6932)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3522 (0.0221 -- 2.1407)  data: 0.1970 (0.0001 -- 1.9695)  max mem: 16413
Val: Total time: 0:00:03 (0.3523 s / it)
* Acc@1 70.122 Acc@5 95.732 loss 0.886
Accuracy of the network on the 163 val images: 70.12%
Max accuracy: 73.78%
Epoch: [119]  [ 0/54]  eta: 0:06:09  lr: 0.000017  min_lr: 0.000000  loss: 1.8770 (1.8770)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3723 (8.3723)  time: 6.8466 (6.8466 -- 6.8466)  data: 6.2943 (6.2943 -- 6.2943)  max mem: 16413
[2023-10-23 18:21:05,988] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6428
[2023-10-23 18:21:05,989] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:21:05,989] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6428
[2023-10-23 18:21:05,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 18:21:05,989] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [119]  [20/54]  eta: 0:00:38  lr: 0.000017  min_lr: 0.000000  loss: 1.7878 (1.7496)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4416 (8.0614)  time: 0.8403 (0.5276 -- 3.8220)  data: 0.1726 (0.0007 -- 2.0194)  max mem: 16413
Epoch: [119]  [40/54]  eta: 0:00:13  lr: 0.000017  min_lr: 0.000000  loss: 1.6872 (1.7225)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3263 (7.8830)  time: 0.8657 (0.5122 -- 2.9978)  data: 0.2940 (0.0004 -- 2.4472)  max mem: 16413
Epoch: [119]  [53/54]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.6615 (1.7167)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8351 (8.1572)  time: 0.6239 (0.4953 -- 1.8078)  data: 0.1027 (0.0002 -- 1.3043)  max mem: 16413
Epoch: [119] Total time: 0:00:49 (0.9201 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.6615 (1.7638)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8351 (8.1572)
[2023-10-23 18:21:46,809] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-10-23 18:21:46,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-10-23 18:21:46,811] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-119/mp_rank_00_model_states.pt
[2023-10-23 18:21:46,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-119/mp_rank_00_model_states.pt...
[2023-10-23 18:21:47,825] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-119/mp_rank_00_model_states.pt.
[2023-10-23 18:21:47,825] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7791 (0.7791)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9111 (1.9111 -- 1.9111)  data: 1.6995 (1.6995 -- 1.6995)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6986 (0.6991)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3315 (0.0226 -- 1.9111)  data: 0.1701 (0.0001 -- 1.6995)  max mem: 16413
Val: Total time: 0:00:03 (0.3317 s / it)
* Acc@1 73.780 Acc@5 95.732 loss 0.882
Accuracy of the network on the 163 val images: 73.78%
Max accuracy: 73.78%
Epoch: [120]  [ 0/54]  eta: 0:05:56  lr: 0.000017  min_lr: 0.000000  loss: 1.3781 (1.3781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1549 (9.1549)  time: 6.6080 (6.6080 -- 6.6080)  data: 4.2620 (4.2620 -- 4.2620)  max mem: 16413
Epoch: [120]  [20/54]  eta: 0:00:40  lr: 0.000017  min_lr: 0.000000  loss: 1.6226 (1.6374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1891 (8.5962)  time: 0.9252 (0.5386 -- 3.4550)  data: 0.2973 (0.0006 -- 2.9429)  max mem: 16413
Epoch: [120]  [40/54]  eta: 0:00:14  lr: 0.000017  min_lr: 0.000000  loss: 1.8407 (1.7251)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4074 (8.0330)  time: 0.9359 (0.5272 -- 4.0694)  data: 0.0694 (0.0003 -- 1.3655)  max mem: 16413
Epoch: [120]  [53/54]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.6561 (1.7216)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1738 (7.8322)  time: 0.6936 (0.4938 -- 4.0694)  data: 0.0008 (0.0001 -- 0.0039)  max mem: 16413
Epoch: [120] Total time: 0:00:50 (0.9331 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.6561 (1.7446)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1738 (7.8322)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7392 (0.7392)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0469 (2.0469 -- 2.0469)  data: 1.8663 (1.8663 -- 1.8663)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6629 (0.6885)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3435 (0.0225 -- 2.0469)  data: 0.1867 (0.0001 -- 1.8663)  max mem: 16413
Val: Total time: 0:00:03 (0.3437 s / it)
* Acc@1 71.341 Acc@5 95.732 loss 0.865
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.78%
Epoch: [121]  [ 0/54]  eta: 0:07:15  lr: 0.000017  min_lr: 0.000000  loss: 1.8888 (1.8888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7937 (6.7937)  time: 8.0652 (8.0652 -- 8.0652)  data: 7.5371 (7.5371 -- 7.5371)  max mem: 16413
Epoch: [121]  [20/54]  eta: 0:00:39  lr: 0.000016  min_lr: 0.000000  loss: 1.6811 (1.7071)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6623 (8.4363)  time: 0.8280 (0.5263 -- 2.5598)  data: 0.2112 (0.0003 -- 2.0338)  max mem: 16413
[2023-10-23 18:23:11,454] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:23:11,454] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:23:11,454] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:23:11,454] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [121]  [40/54]  eta: 0:00:14  lr: 0.000016  min_lr: 0.000000  loss: 1.7447 (1.7385)  loss_scale: 32768.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2312 (8.2124)  time: 0.9322 (0.5215 -- 3.0031)  data: 0.1078 (0.0003 -- 1.6814)  max mem: 16413
Epoch: [121]  [53/54]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 2.0185 (1.7696)  loss_scale: 32768.0000 (25789.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0564 (8.1467)  time: 0.6888 (0.4968 -- 2.2407)  data: 0.0937 (0.0001 -- 1.6814)  max mem: 16413
Epoch: [121] Total time: 0:00:51 (0.9543 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 2.0185 (1.7947)  loss_scale: 32768.0000 (25789.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0564 (8.1467)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7823 (0.7823)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9528 (1.9528 -- 1.9528)  data: 1.7516 (1.7516 -- 1.7516)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6915 (0.6915)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (96.3415)  time: 0.3344 (0.0220 -- 1.9528)  data: 0.1770 (0.0001 -- 1.7516)  max mem: 16413
Val: Total time: 0:00:03 (0.3345 s / it)
* Acc@1 71.341 Acc@5 95.122 loss 0.879
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.78%
Epoch: [122]  [ 0/54]  eta: 0:07:03  lr: 0.000016  min_lr: 0.000000  loss: 2.0204 (2.0204)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6920 (5.6920)  time: 7.8479 (7.8479 -- 7.8479)  data: 7.2580 (7.2580 -- 7.2580)  max mem: 16413
[2023-10-23 18:23:49,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6592
[2023-10-23 18:23:49,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6592
[2023-10-23 18:23:49,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:23:49,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:23:49,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [122]  [20/54]  eta: 0:00:41  lr: 0.000016  min_lr: 0.000000  loss: 1.5950 (1.6321)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5418 (7.7948)  time: 0.8823 (0.5179 -- 3.3039)  data: 0.1703 (0.0004 -- 2.7786)  max mem: 16413
Epoch: [122]  [40/54]  eta: 0:00:14  lr: 0.000016  min_lr: 0.000000  loss: 1.7182 (1.6860)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6700 (7.4430)  time: 0.8706 (0.5171 -- 4.9425)  data: 0.3339 (0.0002 -- 4.4255)  max mem: 16413
Epoch: [122]  [53/54]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7005 (1.7288)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7495 (7.3156)  time: 0.6626 (0.4949 -- 1.8788)  data: 0.1097 (0.0001 -- 1.3831)  max mem: 16413
Epoch: [122] Total time: 0:00:51 (0.9550 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7005 (1.7324)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7495 (7.3156)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7411 (0.7411)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1673 (2.1673 -- 2.1673)  data: 1.9834 (1.9834 -- 1.9834)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7022 (0.6866)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3570 (0.0285 -- 2.1673)  data: 0.1984 (0.0001 -- 1.9834)  max mem: 16413
Val: Total time: 0:00:03 (0.3571 s / it)
* Acc@1 73.171 Acc@5 96.341 loss 0.863
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [123]  [ 0/54]  eta: 0:06:22  lr: 0.000016  min_lr: 0.000000  loss: 1.7934 (1.7934)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7187 (10.7187)  time: 7.0787 (7.0787 -- 7.0787)  data: 5.3788 (5.3788 -- 5.3788)  max mem: 16413
Epoch: [123]  [20/54]  eta: 0:00:38  lr: 0.000016  min_lr: 0.000000  loss: 1.7912 (1.7488)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9298 (7.3076)  time: 0.8385 (0.5253 -- 2.3179)  data: 0.1026 (0.0003 -- 1.3779)  max mem: 16413
[2023-10-23 18:25:18,184] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6682
[2023-10-23 18:25:18,184] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6682
[2023-10-23 18:25:18,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:25:18,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:25:18,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [123]  [40/54]  eta: 0:00:14  lr: 0.000016  min_lr: 0.000000  loss: 1.6791 (1.6995)  loss_scale: 16384.0000 (16184.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6398 (7.3918)  time: 0.9555 (0.5236 -- 3.2232)  data: 0.1774 (0.0003 -- 2.3068)  max mem: 16413
Epoch: [123]  [53/54]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7682 (1.7052)  loss_scale: 8192.0000 (14260.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8538 (7.5817)  time: 0.6801 (0.4935 -- 2.8337)  data: 0.1564 (0.0002 -- 2.3068)  max mem: 16413
Epoch: [123] Total time: 0:00:50 (0.9335 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7682 (1.7297)  loss_scale: 8192.0000 (14260.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8538 (7.5817)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7147 (0.7147)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0781 (2.0781 -- 2.0781)  data: 1.9049 (1.9049 -- 1.9049)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7065 (0.6845)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3463 (0.0264 -- 2.0781)  data: 0.1906 (0.0001 -- 1.9049)  max mem: 16413
Val: Total time: 0:00:03 (0.3464 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.857
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [124]  [ 0/54]  eta: 0:08:11  lr: 0.000016  min_lr: 0.000000  loss: 2.1561 (2.1561)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6287 (7.6287)  time: 9.1109 (9.1109 -- 9.1109)  data: 8.5595 (8.5595 -- 8.5595)  max mem: 16413
Epoch: [124]  [20/54]  eta: 0:00:41  lr: 0.000015  min_lr: 0.000000  loss: 1.8020 (1.8154)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2141 (7.6275)  time: 0.8181 (0.5323 -- 3.6673)  data: 0.2715 (0.0002 -- 3.1512)  max mem: 16413
Epoch: [124]  [40/54]  eta: 0:00:15  lr: 0.000015  min_lr: 0.000000  loss: 1.7443 (1.8147)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2971 (7.7015)  time: 0.9346 (0.5184 -- 3.5731)  data: 0.3989 (0.0002 -- 3.0348)  max mem: 16413
Epoch: [124]  [53/54]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.5590 (1.7427)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2401 (7.6400)  time: 0.6250 (0.4935 -- 2.7747)  data: 0.1138 (0.0001 -- 2.2656)  max mem: 16413
Epoch: [124] Total time: 0:00:50 (0.9390 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.5590 (1.7100)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2401 (7.6400)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7433 (0.7433)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9653 (1.9653 -- 1.9653)  data: 1.7588 (1.7588 -- 1.7588)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7120 (0.7060)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3346 (0.0223 -- 1.9653)  data: 0.1760 (0.0001 -- 1.7588)  max mem: 16413
Val: Total time: 0:00:03 (0.3348 s / it)
* Acc@1 72.561 Acc@5 95.732 loss 0.878
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [125]  [ 0/54]  eta: 0:09:17  lr: 0.000015  min_lr: 0.000000  loss: 1.8634 (1.8634)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1100 (7.1100)  time: 10.3184 (10.3184 -- 10.3184)  data: 7.0218 (7.0218 -- 7.0218)  max mem: 16413
Epoch: [125]  [20/54]  eta: 0:00:42  lr: 0.000015  min_lr: 0.000000  loss: 1.6344 (1.6539)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7467 (7.4153)  time: 0.7979 (0.5247 -- 2.7852)  data: 0.0216 (0.0002 -- 0.4043)  max mem: 16413
Epoch: [125]  [40/54]  eta: 0:00:14  lr: 0.000015  min_lr: 0.000000  loss: 1.7268 (1.7172)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4675 (7.6424)  time: 0.8246 (0.5276 -- 2.1570)  data: 0.0792 (0.0007 -- 0.9899)  max mem: 16413
Epoch: [125]  [53/54]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.7708 (1.7411)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6889 (7.6522)  time: 0.7231 (0.4937 -- 2.5568)  data: 0.1535 (0.0002 -- 2.0608)  max mem: 16413
Epoch: [125] Total time: 0:00:52 (0.9697 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.7708 (1.7646)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6889 (7.6522)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7191 (0.7191)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9622 (1.9622 -- 1.9622)  data: 1.7661 (1.7661 -- 1.7661)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7104 (0.6922)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3336 (0.0222 -- 1.9622)  data: 0.1767 (0.0001 -- 1.7661)  max mem: 16413
Val: Total time: 0:00:03 (0.3337 s / it)
* Acc@1 71.951 Acc@5 95.732 loss 0.865
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [126]  [ 0/54]  eta: 0:07:46  lr: 0.000015  min_lr: 0.000000  loss: 1.6575 (1.6575)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9030 (7.9030)  time: 8.6407 (8.6407 -- 8.6407)  data: 8.0953 (8.0953 -- 8.0953)  max mem: 16413
[2023-10-23 18:27:31,536] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:27:31,537] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 18:27:31,537] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:27:31,537] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [126]  [20/54]  eta: 0:00:40  lr: 0.000015  min_lr: 0.000000  loss: 1.5910 (1.6238)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1625 (7.3079)  time: 0.8286 (0.5231 -- 3.4088)  data: 0.2848 (0.0003 -- 2.8950)  max mem: 16413
Epoch: [126]  [40/54]  eta: 0:00:15  lr: 0.000015  min_lr: 0.000000  loss: 1.8313 (1.6814)  loss_scale: 16384.0000 (14985.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3524 (7.5223)  time: 1.0497 (0.5077 -- 5.0014)  data: 0.5106 (0.0004 -- 4.4935)  max mem: 16413
Epoch: [126]  [53/54]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.4951 (1.6471)  loss_scale: 16384.0000 (15322.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5061 (7.2932)  time: 0.6622 (0.4950 -- 3.5820)  data: 0.1547 (0.0001 -- 3.0844)  max mem: 16413
Epoch: [126] Total time: 0:00:52 (0.9756 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.4951 (1.7109)  loss_scale: 16384.0000 (15322.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5061 (7.2932)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7120 (0.7120)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0481 (2.0481 -- 2.0481)  data: 1.8689 (1.8689 -- 1.8689)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7120 (0.6940)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (96.3415)  time: 0.3433 (0.0221 -- 2.0481)  data: 0.1870 (0.0001 -- 1.8689)  max mem: 16413
Val: Total time: 0:00:03 (0.3434 s / it)
* Acc@1 72.561 Acc@5 95.732 loss 0.865
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [127]  [ 0/54]  eta: 0:08:17  lr: 0.000014  min_lr: 0.000000  loss: 1.5471 (1.5471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9856 (6.9856)  time: 9.2076 (9.2076 -- 9.2076)  data: 8.6759 (8.6759 -- 8.6759)  max mem: 16413
Epoch: [127]  [20/54]  eta: 0:00:43  lr: 0.000014  min_lr: 0.000000  loss: 1.7331 (1.7024)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0009 (7.5094)  time: 0.8964 (0.5124 -- 4.6572)  data: 0.3612 (0.0003 -- 4.1273)  max mem: 16413
Epoch: [127]  [40/54]  eta: 0:00:15  lr: 0.000014  min_lr: 0.000000  loss: 1.6963 (1.7339)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7921 (7.7374)  time: 0.8855 (0.5237 -- 3.1663)  data: 0.3369 (0.0003 -- 2.6350)  max mem: 16413
Epoch: [127]  [53/54]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7827 (1.7028)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5915 (7.8212)  time: 0.6826 (0.4933 -- 2.1632)  data: 0.1613 (0.0001 -- 1.6499)  max mem: 16413
Epoch: [127] Total time: 0:00:53 (0.9815 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7827 (1.6990)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5915 (7.8212)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6956 (0.6956)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0125 (2.0125 -- 2.0125)  data: 1.8212 (1.8212 -- 1.8212)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6956 (0.6873)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (97.5610)  time: 0.3391 (0.0222 -- 2.0125)  data: 0.1822 (0.0001 -- 1.8212)  max mem: 16413
Val: Total time: 0:00:03 (0.3392 s / it)
* Acc@1 73.780 Acc@5 95.732 loss 0.854
Accuracy of the network on the 163 val images: 73.78%
Max accuracy: 73.78%
Epoch: [128]  [ 0/54]  eta: 0:06:59  lr: 0.000014  min_lr: 0.000000  loss: 2.2556 (2.2556)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7272 (9.7272)  time: 7.7664 (7.7664 -- 7.7664)  data: 7.2190 (7.2190 -- 7.2190)  max mem: 16413
Epoch: [128]  [20/54]  eta: 0:00:41  lr: 0.000014  min_lr: 0.000000  loss: 1.7422 (1.7237)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5611 (8.2544)  time: 0.8809 (0.5219 -- 3.6744)  data: 0.2210 (0.0005 -- 2.2717)  max mem: 16413
[2023-10-23 18:29:43,398] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:29:43,399] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:29:43,400] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:29:43,400] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [128]  [40/54]  eta: 0:00:14  lr: 0.000014  min_lr: 0.000000  loss: 1.5637 (1.7156)  loss_scale: 32768.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2456 (8.3120)  time: 0.9122 (0.5263 -- 4.6599)  data: 0.0440 (0.0004 -- 0.7415)  max mem: 16413
Epoch: [128]  [53/54]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7743 (1.7240)  loss_scale: 32768.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2904 (8.2337)  time: 0.6019 (0.4949 -- 1.3565)  data: 0.0007 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [128] Total time: 0:00:51 (0.9472 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7743 (1.7603)  loss_scale: 32768.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2904 (8.2337)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7549 (0.7549)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0502 (2.0502 -- 2.0502)  data: 1.8697 (1.8697 -- 1.8697)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7253 (0.6945)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3423 (0.0221 -- 2.0502)  data: 0.1870 (0.0001 -- 1.8697)  max mem: 16413
Val: Total time: 0:00:03 (0.3424 s / it)
* Acc@1 73.171 Acc@5 95.732 loss 0.883
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [129]  [ 0/54]  eta: 0:06:59  lr: 0.000014  min_lr: 0.000000  loss: 1.4062 (1.4062)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3626 (9.3626)  time: 7.7610 (7.7610 -- 7.7610)  data: 7.2531 (7.2531 -- 7.2531)  max mem: 16413
Epoch: [129]  [20/54]  eta: 0:00:38  lr: 0.000014  min_lr: 0.000000  loss: 1.7082 (1.7056)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3822 (7.6617)  time: 0.7862 (0.5260 -- 2.6741)  data: 0.2423 (0.0003 -- 2.1550)  max mem: 16413
[2023-10-23 18:30:42,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=35, lr=[3.2205897227464015e-07, 3.2205897227464015e-07, 4.2941196303285355e-07, 4.2941196303285355e-07, 5.725492840438047e-07, 5.725492840438047e-07, 7.633990453917397e-07, 7.633990453917397e-07, 1.017865393855653e-06, 1.017865393855653e-06, 1.3571538584742038e-06, 1.3571538584742038e-06, 1.8095384779656052e-06, 1.8095384779656052e-06, 2.412717970620807e-06, 2.412717970620807e-06, 3.2169572941610758e-06, 3.2169572941610758e-06, 4.289276392214767e-06, 4.289276392214767e-06, 5.7190351896196904e-06, 5.7190351896196904e-06, 7.625380252826253e-06, 7.625380252826253e-06, 1.0167173670435005e-05, 1.0167173670435005e-05, 1.3556231560580007e-05, 1.3556231560580007e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 18:30:42,297] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=19.028675918003156, CurrSamplesPerSec=6.419650809470671, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [129]  [40/54]  eta: 0:00:14  lr: 0.000014  min_lr: 0.000000  loss: 1.5555 (1.6256)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7373 (7.1518)  time: 0.9089 (0.5281 -- 2.3647)  data: 0.0683 (0.0005 -- 1.3318)  max mem: 16413
Epoch: [129]  [53/54]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.5961 (1.6533)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3481 (6.9888)  time: 0.7193 (0.4968 -- 2.3647)  data: 0.0270 (0.0002 -- 0.5253)  max mem: 16413
Epoch: [129] Total time: 0:00:50 (0.9340 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.5961 (1.6666)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3481 (6.9888)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7351 (0.7351)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0627 (2.0627 -- 2.0627)  data: 1.8828 (1.8828 -- 1.8828)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7026 (0.6811)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3440 (0.0222 -- 2.0627)  data: 0.1884 (0.0001 -- 1.8828)  max mem: 16413
Val: Total time: 0:00:03 (0.3441 s / it)
* Acc@1 73.171 Acc@5 95.732 loss 0.865
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [130]  [ 0/54]  eta: 0:07:51  lr: 0.000013  min_lr: 0.000000  loss: 1.6267 (1.6267)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5150 (12.5150)  time: 8.7385 (8.7385 -- 8.7385)  data: 8.2036 (8.2036 -- 8.2036)  max mem: 16413
[2023-10-23 18:31:20,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7035
[2023-10-23 18:31:20,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7035
[2023-10-23 18:31:20,478] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:31:20,478] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:31:20,478] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [130]  [20/54]  eta: 0:00:41  lr: 0.000013  min_lr: 0.000000  loss: 1.7543 (1.7139)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1446 (7.7041)  time: 0.8592 (0.5236 -- 2.8106)  data: 0.3121 (0.0004 -- 2.2852)  max mem: 16413
Epoch: [130]  [40/54]  eta: 0:00:15  lr: 0.000013  min_lr: 0.000000  loss: 1.8492 (1.7726)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5135 (8.0476)  time: 0.9025 (0.5183 -- 4.4278)  data: 0.3632 (0.0003 -- 3.9250)  max mem: 16413
Epoch: [130]  [53/54]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8250 (1.7813)  loss_scale: 16384.0000 (20935.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7350 (7.9789)  time: 0.6517 (0.4946 -- 3.2416)  data: 0.1369 (0.0001 -- 2.7176)  max mem: 16413
Epoch: [130] Total time: 0:00:50 (0.9361 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8250 (1.7691)  loss_scale: 16384.0000 (20935.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7350 (7.9789)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7556 (0.7556)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9441 (1.9441 -- 1.9441)  data: 1.7552 (1.7552 -- 1.7552)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6745 (0.6886)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3334 (0.0223 -- 1.9441)  data: 0.1756 (0.0001 -- 1.7552)  max mem: 16413
Val: Total time: 0:00:03 (0.3335 s / it)
* Acc@1 73.171 Acc@5 95.122 loss 0.877
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [131]  [ 0/54]  eta: 0:07:06  lr: 0.000013  min_lr: 0.000000  loss: 2.0061 (2.0061)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4697 (6.4697)  time: 7.8967 (7.8967 -- 7.8967)  data: 7.3790 (7.3790 -- 7.3790)  max mem: 16413
Epoch: [131]  [20/54]  eta: 0:00:40  lr: 0.000013  min_lr: 0.000000  loss: 1.7259 (1.7618)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6745 (7.8876)  time: 0.8538 (0.5254 -- 3.4428)  data: 0.1939 (0.0003 -- 2.9058)  max mem: 16413
Epoch: [131]  [40/54]  eta: 0:00:14  lr: 0.000013  min_lr: 0.000000  loss: 1.7202 (1.7671)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0779 (7.7962)  time: 0.8603 (0.5145 -- 3.2743)  data: 0.1095 (0.0002 -- 1.1526)  max mem: 16413
Epoch: [131]  [53/54]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.6772 (1.7483)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9465 (7.7951)  time: 0.6944 (0.4929 -- 1.9484)  data: 0.1565 (0.0002 -- 1.4124)  max mem: 16413
Epoch: [131] Total time: 0:00:50 (0.9426 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.6772 (1.7890)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9465 (7.7951)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7623 (0.7623)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0186 (2.0186 -- 2.0186)  data: 1.8439 (1.8439 -- 1.8439)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6925 (0.7002)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3393 (0.0221 -- 2.0186)  data: 0.1845 (0.0001 -- 1.8439)  max mem: 16413
Val: Total time: 0:00:03 (0.3394 s / it)
* Acc@1 73.171 Acc@5 95.732 loss 0.893
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [132]  [ 0/54]  eta: 0:06:14  lr: 0.000013  min_lr: 0.000000  loss: 1.4770 (1.4770)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6176 (4.6176)  time: 6.9302 (6.9302 -- 6.9302)  data: 6.3653 (6.3653 -- 6.3653)  max mem: 16413
Epoch: [132]  [20/54]  eta: 0:00:41  lr: 0.000013  min_lr: 0.000000  loss: 1.8818 (1.8368)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6661 (7.8246)  time: 0.9232 (0.5301 -- 3.9285)  data: 0.0704 (0.0005 -- 1.2223)  max mem: 16413
[2023-10-23 18:33:28,340] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:33:28,340] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:33:28,341] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:33:28,341] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [132]  [40/54]  eta: 0:00:14  lr: 0.000012  min_lr: 0.000000  loss: 1.7667 (1.8060)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6732 (7.9577)  time: 0.8860 (0.5190 -- 4.7068)  data: 0.0330 (0.0004 -- 0.6299)  max mem: 16413
Epoch: [132]  [53/54]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7667 (1.7840)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5545 (7.9498)  time: 0.6640 (0.4934 -- 2.4270)  data: 0.0010 (0.0001 -- 0.0043)  max mem: 16413
Epoch: [132] Total time: 0:00:51 (0.9561 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7667 (1.7428)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5545 (7.9498)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7403 (0.7403)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1495 (2.1495 -- 2.1495)  data: 1.9622 (1.9622 -- 1.9622)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6922 (0.6947)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3541 (0.0221 -- 2.1495)  data: 0.1963 (0.0001 -- 1.9622)  max mem: 16413
Val: Total time: 0:00:03 (0.3542 s / it)
* Acc@1 73.171 Acc@5 95.732 loss 0.879
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [133]  [ 0/54]  eta: 0:07:24  lr: 0.000012  min_lr: 0.000000  loss: 1.7399 (1.7399)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9971 (6.9971)  time: 8.2309 (8.2309 -- 8.2309)  data: 7.1406 (7.1406 -- 7.1406)  max mem: 16413
[2023-10-23 18:34:02,521] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7196
[2023-10-23 18:34:02,521] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7196
[2023-10-23 18:34:02,521] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:34:02,521] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:34:02,521] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [133]  [20/54]  eta: 0:00:41  lr: 0.000012  min_lr: 0.000000  loss: 1.6758 (1.7015)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7824 (7.0357)  time: 0.8592 (0.5119 -- 3.7659)  data: 0.3180 (0.0003 -- 3.2551)  max mem: 16413
Epoch: [133]  [40/54]  eta: 0:00:15  lr: 0.000012  min_lr: 0.000000  loss: 1.6932 (1.7083)  loss_scale: 16384.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0411 (7.8954)  time: 0.9733 (0.5140 -- 6.5271)  data: 0.4335 (0.0002 -- 6.0215)  max mem: 16413
Epoch: [133]  [53/54]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.6666 (1.7018)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5743 (7.8472)  time: 0.5944 (0.4939 -- 1.3816)  data: 0.0789 (0.0001 -- 0.8717)  max mem: 16413
Epoch: [133] Total time: 0:00:52 (0.9653 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.6666 (1.6837)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5743 (7.8472)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7263 (0.7263)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0159 (2.0159 -- 2.0159)  data: 1.8415 (1.8415 -- 1.8415)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6519 (0.6793)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3385 (0.0221 -- 2.0159)  data: 0.1842 (0.0001 -- 1.8415)  max mem: 16413
Val: Total time: 0:00:03 (0.3386 s / it)
* Acc@1 71.951 Acc@5 95.732 loss 0.859
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [134]  [ 0/54]  eta: 0:07:13  lr: 0.000012  min_lr: 0.000000  loss: 2.0455 (2.0455)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5938 (8.5938)  time: 8.0256 (8.0256 -- 8.0256)  data: 5.6972 (5.6972 -- 5.6972)  max mem: 16413
Epoch: [134]  [20/54]  eta: 0:00:40  lr: 0.000012  min_lr: 0.000000  loss: 1.7269 (1.7548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6759 (8.7102)  time: 0.8440 (0.5279 -- 3.0387)  data: 0.0019 (0.0003 -- 0.0071)  max mem: 16413
Epoch: [134]  [40/54]  eta: 0:00:14  lr: 0.000012  min_lr: 0.000000  loss: 1.7406 (1.7730)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7207 (8.2536)  time: 0.9370 (0.5148 -- 5.2623)  data: 0.0011 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [134]  [53/54]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7406 (1.7379)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1346 (8.1901)  time: 0.6196 (0.4946 -- 2.4119)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [134] Total time: 0:00:50 (0.9315 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7406 (1.7123)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1346 (8.1901)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7094 (0.7094)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0093 (2.0093 -- 2.0093)  data: 1.8155 (1.8155 -- 1.8155)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6609 (0.6755)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3390 (0.0221 -- 2.0093)  data: 0.1816 (0.0001 -- 1.8155)  max mem: 16413
Val: Total time: 0:00:03 (0.3391 s / it)
* Acc@1 73.780 Acc@5 96.341 loss 0.853
Accuracy of the network on the 163 val images: 73.78%
Max accuracy: 73.78%
Epoch: [135]  [ 0/54]  eta: 0:06:50  lr: 0.000012  min_lr: 0.000000  loss: 2.1676 (2.1676)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7590 (5.7590)  time: 7.6025 (7.6025 -- 7.6025)  data: 7.0847 (7.0847 -- 7.0847)  max mem: 16413
Epoch: [135]  [20/54]  eta: 0:00:40  lr: 0.000012  min_lr: 0.000000  loss: 1.5629 (1.6297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7301 (6.8711)  time: 0.8748 (0.5210 -- 5.1835)  data: 0.3269 (0.0002 -- 4.6523)  max mem: 16413
[2023-10-23 18:36:10,891] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:36:10,891] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:36:10,892] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:36:10,892] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:36:11,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7326
[2023-10-23 18:36:11,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7326
[2023-10-23 18:36:11,477] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:36:11,518] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:36:11,518] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [135]  [40/54]  eta: 0:00:15  lr: 0.000012  min_lr: 0.000000  loss: 1.4774 (1.6407)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8648 (7.5189)  time: 0.9674 (0.5259 -- 4.1729)  data: 0.4270 (0.0003 -- 3.6740)  max mem: 16413
Epoch: [135]  [53/54]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7567 (1.6855)  loss_scale: 16384.0000 (16687.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0134 (7.7901)  time: 0.7206 (0.4934 -- 4.1729)  data: 0.2045 (0.0001 -- 3.6740)  max mem: 16413
Epoch: [135] Total time: 0:00:51 (0.9515 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7567 (1.6932)  loss_scale: 16384.0000 (16687.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0134 (7.7901)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7141 (0.7141)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9481 (1.9481 -- 1.9481)  data: 1.7501 (1.7501 -- 1.7501)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6730 (0.6793)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (97.5610)  time: 0.3322 (0.0222 -- 1.9481)  data: 0.1751 (0.0001 -- 1.7501)  max mem: 16413
Val: Total time: 0:00:03 (0.3324 s / it)
* Acc@1 73.171 Acc@5 96.341 loss 0.859
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [136]  [ 0/54]  eta: 0:07:40  lr: 0.000011  min_lr: 0.000000  loss: 2.0514 (2.0514)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8622 (6.8622)  time: 8.5212 (8.5212 -- 8.5212)  data: 7.9722 (7.9722 -- 7.9722)  max mem: 16413
Epoch: [136]  [20/54]  eta: 0:00:42  lr: 0.000011  min_lr: 0.000000  loss: 1.6674 (1.7018)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4756 (7.5897)  time: 0.8781 (0.5316 -- 3.9377)  data: 0.3340 (0.0004 -- 3.4253)  max mem: 16413
[2023-10-23 18:37:07,042] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7379
[2023-10-23 18:37:07,042] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7379
[2023-10-23 18:37:07,042] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:37:07,042] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:37:07,042] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [136]  [40/54]  eta: 0:00:14  lr: 0.000011  min_lr: 0.000000  loss: 1.7484 (1.7107)  loss_scale: 16384.0000 (15185.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0823 (8.4468)  time: 0.8779 (0.5251 -- 4.0551)  data: 0.3369 (0.0003 -- 3.5164)  max mem: 16413
Epoch: [136]  [53/54]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.6013 (1.7051)  loss_scale: 8192.0000 (13501.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2587 (8.2975)  time: 0.6128 (0.4966 -- 2.0362)  data: 0.0945 (0.0001 -- 1.5268)  max mem: 16413
Epoch: [136] Total time: 0:00:50 (0.9376 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.6013 (1.6972)  loss_scale: 8192.0000 (13501.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2587 (8.2975)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6944 (0.6944)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1161 (2.1161 -- 2.1161)  data: 1.9472 (1.9472 -- 1.9472)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6600 (0.6748)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3492 (0.0220 -- 2.1161)  data: 0.1948 (0.0001 -- 1.9472)  max mem: 16413
Val: Total time: 0:00:03 (0.3493 s / it)
* Acc@1 73.171 Acc@5 96.341 loss 0.847
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [137]  [ 0/54]  eta: 0:07:01  lr: 0.000011  min_lr: 0.000000  loss: 1.1912 (1.1912)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7101 (13.7101)  time: 7.7981 (7.7981 -- 7.7981)  data: 7.2599 (7.2599 -- 7.2599)  max mem: 16413
Epoch: [137]  [20/54]  eta: 0:00:39  lr: 0.000011  min_lr: 0.000000  loss: 1.6869 (1.6856)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6962 (9.2212)  time: 0.8187 (0.5183 -- 2.5124)  data: 0.2520 (0.0003 -- 1.9793)  max mem: 16413
Epoch: [137]  [40/54]  eta: 0:00:14  lr: 0.000011  min_lr: 0.000000  loss: 1.8140 (1.7336)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6701 (8.6368)  time: 0.8557 (0.5081 -- 3.1427)  data: 0.1303 (0.0003 -- 1.7350)  max mem: 16413
Epoch: [137]  [53/54]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.4920 (1.7272)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0326 (8.5389)  time: 0.6386 (0.4965 -- 2.0120)  data: 0.0008 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [137] Total time: 0:00:50 (0.9262 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.4920 (1.7441)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0326 (8.5389)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7053 (0.7053)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1180 (2.1180 -- 2.1180)  data: 1.9386 (1.9386 -- 1.9386)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6681 (0.6818)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3491 (0.0221 -- 2.1180)  data: 0.1939 (0.0000 -- 1.9386)  max mem: 16413
Val: Total time: 0:00:03 (0.3492 s / it)
* Acc@1 72.561 Acc@5 96.341 loss 0.857
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [138]  [ 0/54]  eta: 0:06:16  lr: 0.000011  min_lr: 0.000000  loss: 2.0455 (2.0455)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5574 (9.5574)  time: 6.9742 (6.9742 -- 6.9742)  data: 5.6546 (5.6546 -- 5.6546)  max mem: 16413
Epoch: [138]  [20/54]  eta: 0:00:39  lr: 0.000011  min_lr: 0.000000  loss: 1.7905 (1.8285)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3709 (8.7555)  time: 0.8782 (0.5206 -- 4.2604)  data: 0.3340 (0.0005 -- 3.7353)  max mem: 16413
Epoch: [138]  [40/54]  eta: 0:00:15  lr: 0.000011  min_lr: 0.000000  loss: 1.7022 (1.7541)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9539 (8.2571)  time: 0.9757 (0.5207 -- 4.6352)  data: 0.4306 (0.0003 -- 4.0877)  max mem: 16413
Epoch: [138]  [53/54]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.6931 (1.7423)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1069 (8.1387)  time: 0.6679 (0.4951 -- 3.4881)  data: 0.1486 (0.0002 -- 2.9575)  max mem: 16413
Epoch: [138] Total time: 0:00:50 (0.9383 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.6931 (1.7536)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1069 (8.1387)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.7144 (0.7144)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1437 (2.1437 -- 2.1437)  data: 1.9649 (1.9649 -- 1.9649)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6894 (0.6923)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3525 (0.0220 -- 2.1437)  data: 0.1966 (0.0001 -- 1.9649)  max mem: 16413
Val: Total time: 0:00:03 (0.3526 s / it)
* Acc@1 73.171 Acc@5 95.732 loss 0.869
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [139]  [ 0/54]  eta: 0:08:21  lr: 0.000010  min_lr: 0.000000  loss: 1.4957 (1.4957)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9388 (6.9388)  time: 9.2835 (9.2835 -- 9.2835)  data: 7.6666 (7.6666 -- 7.6666)  max mem: 16413
[2023-10-23 18:39:19,873] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:39:19,873] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 18:39:19,873] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:39:19,873] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [139]  [20/54]  eta: 0:00:40  lr: 0.000010  min_lr: 0.000000  loss: 1.7898 (1.7936)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9040 (7.2389)  time: 0.7973 (0.5242 -- 3.6142)  data: 0.2460 (0.0002 -- 3.0859)  max mem: 16413
Epoch: [139]  [40/54]  eta: 0:00:14  lr: 0.000010  min_lr: 0.000000  loss: 1.8422 (1.7817)  loss_scale: 16384.0000 (15984.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0374 (7.5152)  time: 0.8378 (0.5205 -- 3.1844)  data: 0.1830 (0.0003 -- 1.5399)  max mem: 16413
Epoch: [139]  [53/54]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7110 (1.7437)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7781 (7.6400)  time: 0.7627 (0.4961 -- 3.1844)  data: 0.1344 (0.0002 -- 1.5399)  max mem: 16413
Epoch: [139] Total time: 0:00:50 (0.9417 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7110 (1.7677)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7781 (7.6400)
[2023-10-23 18:40:00,304] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-10-23 18:40:00,306] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-139/mp_rank_00_model_states.pt
[2023-10-23 18:40:00,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-139/mp_rank_00_model_states.pt...
[2023-10-23 18:40:00,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-10-23 18:40:01,332] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-139/mp_rank_00_model_states.pt.
[2023-10-23 18:40:01,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7218 (0.7218)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9712 (1.9712 -- 1.9712)  data: 1.7916 (1.7916 -- 1.7916)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6890 (0.6928)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3357 (0.0228 -- 1.9712)  data: 0.1793 (0.0001 -- 1.7916)  max mem: 16413
Val: Total time: 0:00:03 (0.3358 s / it)
* Acc@1 73.171 Acc@5 95.122 loss 0.874
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [140]  [ 0/54]  eta: 0:06:21  lr: 0.000010  min_lr: 0.000000  loss: 1.6485 (1.6485)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4930 (5.4930)  time: 7.0649 (7.0649 -- 7.0649)  data: 6.5350 (6.5350 -- 6.5350)  max mem: 16413
Epoch: [140]  [20/54]  eta: 0:00:41  lr: 0.000010  min_lr: 0.000000  loss: 1.6940 (1.7018)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2446 (7.0488)  time: 0.9373 (0.5085 -- 2.8446)  data: 0.2973 (0.0003 -- 2.2972)  max mem: 16413
Epoch: [140]  [40/54]  eta: 0:00:15  lr: 0.000010  min_lr: 0.000000  loss: 1.6694 (1.7359)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8618 (7.6879)  time: 0.9094 (0.5158 -- 3.1925)  data: 0.3363 (0.0005 -- 2.6742)  max mem: 16413
Epoch: [140]  [53/54]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.6658 (1.7241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0693 (7.8485)  time: 0.6783 (0.4946 -- 3.0174)  data: 0.1359 (0.0002 -- 2.5000)  max mem: 16413
Epoch: [140] Total time: 0:00:51 (0.9489 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.6658 (1.7461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0693 (7.8485)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7173 (0.7173)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9371 (1.9371 -- 1.9371)  data: 1.7440 (1.7440 -- 1.7440)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7025 (0.6908)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3329 (0.0222 -- 1.9371)  data: 0.1758 (0.0001 -- 1.7440)  max mem: 16413
Val: Total time: 0:00:03 (0.3330 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.876
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [141]  [ 0/54]  eta: 0:07:29  lr: 0.000010  min_lr: 0.000000  loss: 1.2351 (1.2351)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9296 (6.9296)  time: 8.3246 (8.3246 -- 8.3246)  data: 6.1545 (6.1545 -- 6.1545)  max mem: 16413
Epoch: [141]  [20/54]  eta: 0:00:44  lr: 0.000010  min_lr: 0.000000  loss: 1.8931 (1.8047)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1174 (8.5294)  time: 0.9731 (0.5127 -- 5.9938)  data: 0.0011 (0.0003 -- 0.0020)  max mem: 16413
[2023-10-23 18:41:28,217] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:41:28,218] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:41:28,219] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:41:28,219] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [141]  [40/54]  eta: 0:00:15  lr: 0.000010  min_lr: 0.000000  loss: 1.5726 (1.7474)  loss_scale: 32768.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8382 (7.8994)  time: 0.8944 (0.5197 -- 3.0642)  data: 0.0012 (0.0002 -- 0.0048)  max mem: 16413
[2023-10-23 18:41:47,064] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7658
[2023-10-23 18:41:47,064] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7658
[2023-10-23 18:41:47,064] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:41:47,064] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:41:47,064] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [141]  [53/54]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.6633 (1.7669)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1983 (8.1308)  time: 0.6200 (0.4807 -- 2.6607)  data: 0.0005 (0.0001 -- 0.0017)  max mem: 16413
Epoch: [141] Total time: 0:00:52 (0.9673 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.6633 (1.7683)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1983 (8.1308)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7256 (0.7256)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9994 (1.9994 -- 1.9994)  data: 1.8041 (1.8041 -- 1.8041)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6839 (0.6826)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (95.1220)  time: 0.3380 (0.0220 -- 1.9994)  data: 0.1805 (0.0001 -- 1.8041)  max mem: 16413
Val: Total time: 0:00:03 (0.3382 s / it)
* Acc@1 72.561 Acc@5 94.512 loss 0.872
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [142]  [ 0/54]  eta: 0:08:15  lr: 0.000010  min_lr: 0.000000  loss: 1.5988 (1.5988)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.4914 (13.4914)  time: 9.1690 (9.1690 -- 9.1690)  data: 8.6323 (8.6323 -- 8.6323)  max mem: 16413
Epoch: [142]  [20/54]  eta: 0:00:42  lr: 0.000009  min_lr: 0.000000  loss: 1.6172 (1.6414)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5026 (7.6972)  time: 0.8602 (0.4991 -- 6.8125)  data: 0.3171 (0.0003 -- 6.3019)  max mem: 16413
Epoch: [142]  [40/54]  eta: 0:00:15  lr: 0.000009  min_lr: 0.000000  loss: 1.7817 (1.7478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1669 (7.6734)  time: 1.0199 (0.5206 -- 4.7432)  data: 0.4849 (0.0004 -- 4.2378)  max mem: 16413
Epoch: [142]  [53/54]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.5130 (1.6825)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0624 (7.8013)  time: 0.6211 (0.4939 -- 2.3935)  data: 0.1130 (0.0001 -- 1.8939)  max mem: 16413
Epoch: [142] Total time: 0:00:53 (0.9929 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.5130 (1.7228)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0624 (7.8013)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7190 (0.7190)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9561 (1.9561 -- 1.9561)  data: 1.7513 (1.7513 -- 1.7513)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6731 (0.6814)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (95.1220)  time: 0.3337 (0.0221 -- 1.9561)  data: 0.1752 (0.0001 -- 1.7513)  max mem: 16413
Val: Total time: 0:00:03 (0.3338 s / it)
* Acc@1 71.341 Acc@5 95.122 loss 0.871
Accuracy of the network on the 163 val images: 71.34%
Max accuracy: 73.78%
Epoch: [143]  [ 0/54]  eta: 0:06:55  lr: 0.000009  min_lr: 0.000000  loss: 1.6712 (1.6712)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3606 (9.3606)  time: 7.7031 (7.7031 -- 7.7031)  data: 6.0073 (6.0073 -- 6.0073)  max mem: 16413
Epoch: [143]  [20/54]  eta: 0:00:42  lr: 0.000009  min_lr: 0.000000  loss: 1.7694 (1.7033)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1735 (7.2203)  time: 0.9231 (0.5219 -- 4.8103)  data: 0.2180 (0.0006 -- 2.6146)  max mem: 16413
Epoch: [143]  [40/54]  eta: 0:00:15  lr: 0.000009  min_lr: 0.000000  loss: 1.7020 (1.6958)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4743 (8.0343)  time: 0.9177 (0.5312 -- 4.0748)  data: 0.0387 (0.0003 -- 0.7402)  max mem: 16413
Epoch: [143]  [53/54]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.6718 (1.6898)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2655 (7.9026)  time: 0.6418 (0.4943 -- 2.5186)  data: 0.0009 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [143] Total time: 0:00:51 (0.9550 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.6718 (1.7216)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2655 (7.9026)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.7081 (0.7081)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9579 (1.9579 -- 1.9579)  data: 1.7596 (1.7596 -- 1.7596)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7041 (0.6856)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (96.3415)  time: 0.3338 (0.0264 -- 1.9579)  data: 0.1760 (0.0001 -- 1.7596)  max mem: 16413
Val: Total time: 0:00:03 (0.3339 s / it)
* Acc@1 72.561 Acc@5 95.732 loss 0.870
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [144]  [ 0/54]  eta: 0:05:59  lr: 0.000009  min_lr: 0.000000  loss: 1.9814 (1.9814)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1945 (11.1945)  time: 6.6563 (6.6563 -- 6.6563)  data: 6.1017 (6.1017 -- 6.1017)  max mem: 16413
[2023-10-23 18:44:02,973] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:44:02,974] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:44:02,974] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:44:02,974] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:44:04,637] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7790
[2023-10-23 18:44:04,637] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7790
[2023-10-23 18:44:04,637] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:44:04,637] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:44:04,637] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [144]  [20/54]  eta: 0:00:40  lr: 0.000009  min_lr: 0.000000  loss: 1.6876 (1.7410)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7357 (7.3995)  time: 0.9269 (0.5176 -- 3.8944)  data: 0.1650 (0.0005 -- 1.1867)  max mem: 16413
Epoch: [144]  [40/54]  eta: 0:00:14  lr: 0.000009  min_lr: 0.000000  loss: 1.8424 (1.7577)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2091 (7.7411)  time: 0.9088 (0.5117 -- 3.0217)  data: 0.0487 (0.0005 -- 0.5724)  max mem: 16413
Epoch: [144]  [53/54]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7933 (1.7271)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1243 (7.9870)  time: 0.6659 (0.4955 -- 2.1883)  data: 0.0425 (0.0002 -- 0.5724)  max mem: 16413
Epoch: [144] Total time: 0:00:50 (0.9327 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7933 (1.6875)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1243 (7.9870)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.7033 (0.7033)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0198 (2.0198 -- 2.0198)  data: 1.8281 (1.8281 -- 1.8281)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6868 (0.6788)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3394 (0.0221 -- 2.0198)  data: 0.1829 (0.0001 -- 1.8281)  max mem: 16413
Val: Total time: 0:00:03 (0.3395 s / it)
* Acc@1 73.171 Acc@5 94.512 loss 0.863
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [145]  [ 0/54]  eta: 0:07:30  lr: 0.000009  min_lr: 0.000000  loss: 1.7351 (1.7351)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9759 (9.9759)  time: 8.3376 (8.3376 -- 8.3376)  data: 7.7989 (7.7989 -- 7.7989)  max mem: 16413
Epoch: [145]  [20/54]  eta: 0:00:39  lr: 0.000009  min_lr: 0.000000  loss: 1.7223 (1.7122)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9922 (7.8848)  time: 0.7971 (0.5218 -- 4.0442)  data: 0.2459 (0.0003 -- 3.5315)  max mem: 16413
Epoch: [145]  [40/54]  eta: 0:00:15  lr: 0.000008  min_lr: 0.000000  loss: 1.6178 (1.6718)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5358 (7.8460)  time: 1.0468 (0.5296 -- 4.1754)  data: 0.0600 (0.0003 -- 0.8230)  max mem: 16413
Epoch: [145]  [53/54]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6969 (1.6732)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3981 (7.9062)  time: 0.6559 (0.4954 -- 3.2544)  data: 0.0006 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [145] Total time: 0:00:51 (0.9583 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6969 (1.7626)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3981 (7.9062)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6886 (0.6886)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1343 (2.1343 -- 2.1343)  data: 1.9499 (1.9499 -- 1.9499)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6664 (0.6700)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3512 (0.0221 -- 2.1343)  data: 0.1951 (0.0001 -- 1.9499)  max mem: 16413
Val: Total time: 0:00:03 (0.3513 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.854
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [146]  [ 0/54]  eta: 0:07:10  lr: 0.000008  min_lr: 0.000000  loss: 1.3991 (1.3991)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3256 (8.3256)  time: 7.9801 (7.9801 -- 7.9801)  data: 7.4636 (7.4636 -- 7.4636)  max mem: 16413
Epoch: [146]  [20/54]  eta: 0:00:44  lr: 0.000008  min_lr: 0.000000  loss: 1.9659 (1.8567)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6702 (7.9799)  time: 0.9622 (0.5263 -- 4.8224)  data: 0.4108 (0.0004 -- 4.3022)  max mem: 16413
[2023-10-23 18:46:15,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:46:15,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:46:15,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:46:15,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [146]  [40/54]  eta: 0:00:15  lr: 0.000008  min_lr: 0.000000  loss: 1.6170 (1.8098)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9452 (8.1517)  time: 0.8682 (0.5330 -- 2.8773)  data: 0.3150 (0.0004 -- 2.3708)  max mem: 16413
Epoch: [146]  [53/54]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7218 (1.7790)  loss_scale: 32768.0000 (22148.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4440 (7.9608)  time: 0.6684 (0.4936 -- 2.8773)  data: 0.1451 (0.0002 -- 2.3708)  max mem: 16413
Epoch: [146] Total time: 0:00:51 (0.9568 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7218 (1.7581)  loss_scale: 32768.0000 (22148.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4440 (7.9608)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6938 (0.6938)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0499 (2.0499 -- 2.0499)  data: 1.8796 (1.8796 -- 1.8796)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6830 (0.6733)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (95.1220)  time: 0.3427 (0.0225 -- 2.0499)  data: 0.1881 (0.0001 -- 1.8796)  max mem: 16413
Val: Total time: 0:00:03 (0.3429 s / it)
* Acc@1 73.171 Acc@5 95.122 loss 0.860
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [147]  [ 0/54]  eta: 0:07:37  lr: 0.000008  min_lr: 0.000000  loss: 1.6321 (1.6321)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0373 (8.0373)  time: 8.4696 (8.4696 -- 8.4696)  data: 7.4547 (7.4547 -- 7.4547)  max mem: 16413
[2023-10-23 18:46:43,290] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7945
[2023-10-23 18:46:43,290] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7945
[2023-10-23 18:46:43,291] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:46:43,291] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:46:43,291] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [147]  [20/54]  eta: 0:00:41  lr: 0.000008  min_lr: 0.000000  loss: 1.6394 (1.6967)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5925 (8.1736)  time: 0.8544 (0.5117 -- 3.7878)  data: 0.2453 (0.0007 -- 2.9727)  max mem: 16413
Epoch: [147]  [40/54]  eta: 0:00:15  lr: 0.000008  min_lr: 0.000000  loss: 1.5393 (1.6072)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6264 (7.4946)  time: 0.9263 (0.5283 -- 4.4232)  data: 0.2038 (0.0003 -- 2.4315)  max mem: 16413
[2023-10-23 18:47:17,119] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7982
[2023-10-23 18:47:17,119] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7982
[2023-10-23 18:47:17,119] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:47:17,119] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:47:17,120] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [147]  [53/54]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6666 (1.6349)  loss_scale: 8192.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8396 (7.5144)  time: 0.6487 (0.4849 -- 2.8881)  data: 0.0212 (0.0002 -- 0.4033)  max mem: 16413
Epoch: [147] Total time: 0:00:50 (0.9414 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6666 (1.6916)  loss_scale: 8192.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8396 (7.5144)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6970 (0.6970)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9619 (1.9619 -- 1.9619)  data: 1.7289 (1.7289 -- 1.7289)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6854 (0.6742)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3340 (0.0222 -- 1.9619)  data: 0.1730 (0.0001 -- 1.7289)  max mem: 16413
Val: Total time: 0:00:03 (0.3341 s / it)
* Acc@1 73.171 Acc@5 94.512 loss 0.862
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [148]  [ 0/54]  eta: 0:07:05  lr: 0.000008  min_lr: 0.000000  loss: 0.9073 (0.9073)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1155 (6.1155)  time: 7.8725 (7.8725 -- 7.8725)  data: 7.3373 (7.3373 -- 7.3373)  max mem: 16413
[2023-10-23 18:47:36,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[1.8429857026298065e-07, 1.8429857026298065e-07, 2.4573142701730754e-07, 2.4573142701730754e-07, 3.276419026897434e-07, 3.276419026897434e-07, 4.368558702529912e-07, 4.368558702529912e-07, 5.824744936706549e-07, 5.824744936706549e-07, 7.766326582275399e-07, 7.766326582275399e-07, 1.0355102109700531e-06, 1.0355102109700531e-06, 1.3806802812934043e-06, 1.3806802812934043e-06, 1.840907041724539e-06, 1.840907041724539e-06, 2.454542722299385e-06, 2.454542722299385e-06, 3.2727236297325138e-06, 3.2727236297325138e-06, 4.363631506310018e-06, 4.363631506310018e-06, 5.818175341746691e-06, 5.818175341746691e-06, 7.757567122328922e-06, 7.757567122328922e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 18:47:36,932] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=19.028518255680773, CurrSamplesPerSec=21.715736382267284, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [148]  [20/54]  eta: 0:00:41  lr: 0.000008  min_lr: 0.000000  loss: 1.7044 (1.6467)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8075 (8.4600)  time: 0.8770 (0.5191 -- 5.0045)  data: 0.3307 (0.0004 -- 4.4848)  max mem: 16413
Epoch: [148]  [40/54]  eta: 0:00:14  lr: 0.000008  min_lr: 0.000000  loss: 1.6285 (1.6609)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1015 (7.9502)  time: 0.9084 (0.5191 -- 4.2123)  data: 0.3651 (0.0004 -- 3.6829)  max mem: 16413
Epoch: [148]  [53/54]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6747 (1.6883)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8028 (8.1843)  time: 0.6192 (0.4938 -- 2.1110)  data: 0.1013 (0.0001 -- 1.6128)  max mem: 16413
Epoch: [148] Total time: 0:00:51 (0.9591 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6747 (1.7595)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8028 (8.1843)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6972 (0.6972)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9916 (1.9916 -- 1.9916)  data: 1.8053 (1.8053 -- 1.8053)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6824 (0.6686)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3373 (0.0262 -- 1.9916)  data: 0.1806 (0.0001 -- 1.8053)  max mem: 16413
Val: Total time: 0:00:03 (0.3374 s / it)
* Acc@1 72.561 Acc@5 94.512 loss 0.862
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [149]  [ 0/54]  eta: 0:06:29  lr: 0.000008  min_lr: 0.000000  loss: 2.0681 (2.0681)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3888 (12.3888)  time: 7.2057 (7.2057 -- 7.2057)  data: 6.6331 (6.6331 -- 6.6331)  max mem: 16413
Epoch: [149]  [20/54]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000000  loss: 1.5500 (1.7200)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7593 (7.5203)  time: 0.8547 (0.5213 -- 4.1468)  data: 0.3036 (0.0003 -- 3.6045)  max mem: 16413
Epoch: [149]  [40/54]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 1.8212 (1.7459)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3084 (7.5976)  time: 0.9720 (0.5146 -- 4.0796)  data: 0.4260 (0.0005 -- 3.5677)  max mem: 16413
Epoch: [149]  [53/54]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.6828 (1.7163)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2043 (8.0159)  time: 0.6501 (0.4954 -- 3.1275)  data: 0.1340 (0.0001 -- 2.6158)  max mem: 16413
Epoch: [149] Total time: 0:00:50 (0.9315 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.6828 (1.7201)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2043 (8.0159)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6949 (0.6949)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0078 (2.0078 -- 2.0078)  data: 1.8208 (1.8208 -- 1.8208)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6949 (0.6666)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3384 (0.0221 -- 2.0078)  data: 0.1822 (0.0001 -- 1.8208)  max mem: 16413
Val: Total time: 0:00:03 (0.3385 s / it)
* Acc@1 72.561 Acc@5 95.732 loss 0.862
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [150]  [ 0/54]  eta: 0:06:50  lr: 0.000007  min_lr: 0.000000  loss: 1.7460 (1.7460)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5423 (6.5423)  time: 7.5959 (7.5959 -- 7.5959)  data: 6.3290 (6.3290 -- 6.3290)  max mem: 16413
[2023-10-23 18:49:31,729] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:49:31,730] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 18:49:31,734] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:49:31,734] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [150]  [20/54]  eta: 0:00:42  lr: 0.000007  min_lr: 0.000000  loss: 1.6689 (1.7290)  loss_scale: 8192.0000 (12092.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4301 (7.9621)  time: 0.9396 (0.5221 -- 2.7284)  data: 0.1708 (0.0005 -- 1.7258)  max mem: 16413
Epoch: [150]  [40/54]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 1.8321 (1.7569)  loss_scale: 16384.0000 (14186.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5470 (8.3374)  time: 0.8125 (0.5242 -- 2.0816)  data: 0.0216 (0.0002 -- 0.4050)  max mem: 16413
Epoch: [150]  [53/54]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8214 (1.7407)  loss_scale: 16384.0000 (14715.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5576 (8.0911)  time: 0.6788 (0.4959 -- 2.0816)  data: 0.0387 (0.0001 -- 0.4050)  max mem: 16413
Epoch: [150] Total time: 0:00:50 (0.9358 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8214 (1.7285)  loss_scale: 16384.0000 (14715.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5576 (8.0911)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6976 (0.6976)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1228 (2.1228 -- 2.1228)  data: 1.9418 (1.9418 -- 1.9418)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6818 (0.6684)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3519 (0.0220 -- 2.1228)  data: 0.1943 (0.0001 -- 1.9418)  max mem: 16413
Val: Total time: 0:00:03 (0.3520 s / it)
* Acc@1 73.171 Acc@5 95.122 loss 0.862
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [151]  [ 0/54]  eta: 0:06:10  lr: 0.000007  min_lr: 0.000000  loss: 2.3353 (2.3353)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8744 (6.8744)  time: 6.8530 (6.8530 -- 6.8530)  data: 6.3231 (6.3231 -- 6.3231)  max mem: 16413
Epoch: [151]  [20/54]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000000  loss: 1.7148 (1.7380)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9230 (8.0073)  time: 0.8767 (0.5194 -- 4.6184)  data: 0.3199 (0.0003 -- 4.0494)  max mem: 16413
Epoch: [151]  [40/54]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 1.7903 (1.7282)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8370 (8.0746)  time: 0.9199 (0.5313 -- 3.2959)  data: 0.3287 (0.0007 -- 2.7481)  max mem: 16413
Epoch: [151]  [53/54]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8668 (1.7794)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7847 (7.9851)  time: 0.6551 (0.4946 -- 2.6560)  data: 0.1351 (0.0002 -- 2.1325)  max mem: 16413
Epoch: [151] Total time: 0:00:49 (0.9246 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8668 (1.7393)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7847 (7.9851)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6805 (0.6805)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9459 (1.9459 -- 1.9459)  data: 1.7474 (1.7474 -- 1.7474)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6743 (0.6702)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3338 (0.0222 -- 1.9459)  data: 0.1748 (0.0001 -- 1.7474)  max mem: 16413
Val: Total time: 0:00:03 (0.3339 s / it)
* Acc@1 72.561 Acc@5 94.512 loss 0.858
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [152]  [ 0/54]  eta: 0:05:49  lr: 0.000007  min_lr: 0.000000  loss: 2.3557 (2.3557)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2274 (11.2274)  time: 6.4674 (6.4674 -- 6.4674)  data: 5.6949 (5.6949 -- 5.6949)  max mem: 16413
Epoch: [152]  [20/54]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000000  loss: 1.6011 (1.6740)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1231 (8.0299)  time: 0.9080 (0.5297 -- 3.3888)  data: 0.1532 (0.0004 -- 1.8110)  max mem: 16413
[2023-10-23 18:51:34,571] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:51:34,571] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 18:51:34,571] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:51:34,572] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [152]  [40/54]  eta: 0:00:15  lr: 0.000007  min_lr: 0.000000  loss: 1.8830 (1.7253)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9963 (7.6472)  time: 0.9870 (0.5206 -- 4.3198)  data: 0.0933 (0.0005 -- 0.6746)  max mem: 16413
Epoch: [152]  [53/54]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8398 (1.7408)  loss_scale: 32768.0000 (23362.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1303 (7.5843)  time: 0.7756 (0.4956 -- 4.3198)  data: 0.0259 (0.0001 -- 0.4990)  max mem: 16413
Epoch: [152] Total time: 0:00:52 (0.9677 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8398 (1.7612)  loss_scale: 32768.0000 (23362.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1303 (7.5843)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6922 (0.6922)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0539 (2.0539 -- 2.0539)  data: 1.8748 (1.8748 -- 1.8748)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6922 (0.6786)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3434 (0.0221 -- 2.0539)  data: 0.1876 (0.0001 -- 1.8748)  max mem: 16413
Val: Total time: 0:00:03 (0.3436 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.868
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [153]  [ 0/54]  eta: 0:07:57  lr: 0.000006  min_lr: 0.000000  loss: 2.0449 (2.0449)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9299 (3.9299)  time: 8.8342 (8.8342 -- 8.8342)  data: 4.9355 (4.9355 -- 4.9355)  max mem: 16413
[2023-10-23 18:52:10,415] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8267
[2023-10-23 18:52:10,415] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8267
[2023-10-23 18:52:10,415] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:52:10,415] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 18:52:10,415] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [153]  [20/54]  eta: 0:00:42  lr: 0.000006  min_lr: 0.000000  loss: 1.7426 (1.7589)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0341 (8.5838)  time: 0.8701 (0.5147 -- 2.2542)  data: 0.0016 (0.0002 -- 0.0030)  max mem: 16413
[2023-10-23 18:52:34,839] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8297
[2023-10-23 18:52:34,839] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:52:34,839] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8297
[2023-10-23 18:52:34,839] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:52:34,840] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [153]  [40/54]  eta: 0:00:14  lr: 0.000006  min_lr: 0.000000  loss: 1.6785 (1.7090)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4189 (8.9595)  time: 0.8090 (0.5297 -- 2.6173)  data: 0.1070 (0.0005 -- 2.0720)  max mem: 16413
Epoch: [153]  [53/54]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6109 (1.6986)  loss_scale: 8192.0000 (15018.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6011 (8.6678)  time: 0.7212 (0.4935 -- 2.6173)  data: 0.1892 (0.0001 -- 2.0720)  max mem: 16413
Epoch: [153] Total time: 0:00:50 (0.9432 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6109 (1.7227)  loss_scale: 8192.0000 (15018.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6011 (8.6678)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6947 (0.6947)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0348 (2.0348 -- 2.0348)  data: 1.8550 (1.8550 -- 1.8550)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6929 (0.6803)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3412 (0.0221 -- 2.0348)  data: 0.1856 (0.0001 -- 1.8550)  max mem: 16413
Val: Total time: 0:00:03 (0.3413 s / it)
* Acc@1 73.171 Acc@5 95.122 loss 0.870
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [154]  [ 0/54]  eta: 0:07:30  lr: 0.000006  min_lr: 0.000000  loss: 1.8172 (1.8172)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0563 (7.0563)  time: 8.3378 (8.3378 -- 8.3378)  data: 7.8028 (7.8028 -- 7.8028)  max mem: 16413
Epoch: [154]  [20/54]  eta: 0:00:42  lr: 0.000006  min_lr: 0.000000  loss: 1.6963 (1.6590)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5358 (7.9040)  time: 0.9093 (0.5193 -- 4.2298)  data: 0.3667 (0.0005 -- 3.6722)  max mem: 16413
Epoch: [154]  [40/54]  eta: 0:00:15  lr: 0.000006  min_lr: 0.000000  loss: 1.7067 (1.6800)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1881 (8.1210)  time: 0.8993 (0.5165 -- 3.4198)  data: 0.3622 (0.0003 -- 2.8960)  max mem: 16413
Epoch: [154]  [53/54]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6906 (1.6704)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9122 (7.8570)  time: 0.6311 (0.4948 -- 2.1090)  data: 0.1158 (0.0002 -- 1.5776)  max mem: 16413
Epoch: [154] Total time: 0:00:51 (0.9595 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6906 (1.6976)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9122 (7.8570)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6873 (0.6873)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9796 (1.9796 -- 1.9796)  data: 1.7895 (1.7895 -- 1.7895)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6829 (0.6788)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3362 (0.0220 -- 1.9796)  data: 0.1799 (0.0001 -- 1.7895)  max mem: 16413
Val: Total time: 0:00:03 (0.3363 s / it)
* Acc@1 71.951 Acc@5 95.122 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [155]  [ 0/54]  eta: 0:09:50  lr: 0.000006  min_lr: 0.000000  loss: 1.1819 (1.1819)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5861 (7.5861)  time: 10.9312 (10.9312 -- 10.9312)  data: 10.4032 (10.4032 -- 10.4032)  max mem: 16413
Epoch: [155]  [20/54]  eta: 0:00:43  lr: 0.000006  min_lr: 0.000000  loss: 1.7908 (1.7396)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9590 (8.0870)  time: 0.7982 (0.5091 -- 3.9332)  data: 0.2572 (0.0004 -- 3.4216)  max mem: 16413
Epoch: [155]  [40/54]  eta: 0:00:15  lr: 0.000006  min_lr: 0.000000  loss: 1.6133 (1.7287)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9408 (8.6241)  time: 0.8883 (0.5147 -- 4.5219)  data: 0.3432 (0.0003 -- 4.0077)  max mem: 16413
Epoch: [155]  [53/54]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8487 (1.7237)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6817 (8.4320)  time: 0.5180 (0.4950 -- 0.5896)  data: 0.0005 (0.0001 -- 0.0010)  max mem: 16413
Epoch: [155] Total time: 0:00:51 (0.9500 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8487 (1.7062)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6817 (8.4320)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6928 (0.6928)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0337 (2.0337 -- 2.0337)  data: 1.8395 (1.8395 -- 1.8395)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6814 (0.6796)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3408 (0.0221 -- 2.0337)  data: 0.1840 (0.0001 -- 1.8395)  max mem: 16413
Val: Total time: 0:00:03 (0.3410 s / it)
* Acc@1 73.171 Acc@5 95.122 loss 0.869
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [156]  [ 0/54]  eta: 0:08:24  lr: 0.000006  min_lr: 0.000000  loss: 2.2878 (2.2878)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7889 (7.7889)  time: 9.3512 (9.3512 -- 9.3512)  data: 5.9088 (5.9088 -- 5.9088)  max mem: 16413
[2023-10-23 18:54:51,917] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:54:51,917] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:54:51,917] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 18:54:51,917] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [156]  [20/54]  eta: 0:00:41  lr: 0.000006  min_lr: 0.000000  loss: 1.7640 (1.7520)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0445 (6.9665)  time: 0.8238 (0.5194 -- 3.5084)  data: 0.1109 (0.0005 -- 1.7983)  max mem: 16413
Epoch: [156]  [40/54]  eta: 0:00:15  lr: 0.000006  min_lr: 0.000000  loss: 1.8921 (1.7807)  loss_scale: 16384.0000 (15984.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9065 (7.4837)  time: 0.9545 (0.5117 -- 5.1044)  data: 0.0836 (0.0002 -- 1.6063)  max mem: 16413
Epoch: [156]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7241 (1.7698)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0349 (7.3485)  time: 0.5794 (0.4961 -- 1.6718)  data: 0.0005 (0.0001 -- 0.0012)  max mem: 16413
Epoch: [156] Total time: 0:00:51 (0.9569 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7241 (1.7779)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0349 (7.3485)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6911 (0.6911)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0015 (2.0015 -- 2.0015)  data: 1.8074 (1.8074 -- 1.8074)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6805 (0.6769)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (96.3415)  time: 0.3386 (0.0223 -- 2.0015)  data: 0.1808 (0.0001 -- 1.8074)  max mem: 16413
Val: Total time: 0:00:03 (0.3387 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.865
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [157]  [ 0/54]  eta: 0:08:40  lr: 0.000005  min_lr: 0.000000  loss: 1.3937 (1.3937)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8501 (4.8501)  time: 9.6408 (9.6408 -- 9.6408)  data: 9.1134 (9.1134 -- 9.1134)  max mem: 16413
Epoch: [157]  [20/54]  eta: 0:00:42  lr: 0.000005  min_lr: 0.000000  loss: 1.6141 (1.6219)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5755 (7.8086)  time: 0.8242 (0.5286 -- 3.5005)  data: 0.2791 (0.0002 -- 2.9683)  max mem: 16413
[2023-10-23 18:56:03,261] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8499
[2023-10-23 18:56:03,261] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:56:03,262] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8499
[2023-10-23 18:56:03,262] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 18:56:03,263] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [157]  [40/54]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 1.6178 (1.6142)  loss_scale: 8192.0000 (12387.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4088 (7.4969)  time: 0.8604 (0.5302 -- 3.1907)  data: 0.1168 (0.0006 -- 0.9010)  max mem: 16413
Epoch: [157]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7572 (1.6692)  loss_scale: 8192.0000 (11377.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2588 (7.5966)  time: 0.7100 (0.4933 -- 2.7758)  data: 0.0758 (0.0002 -- 1.3629)  max mem: 16413
Epoch: [157] Total time: 0:00:51 (0.9531 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7572 (1.7288)  loss_scale: 8192.0000 (11377.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2588 (7.5966)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6802 (0.6802)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0307 (2.0307 -- 2.0307)  data: 1.8497 (1.8497 -- 1.8497)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6802 (0.6685)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3409 (0.0221 -- 2.0307)  data: 0.1851 (0.0001 -- 1.8497)  max mem: 16413
Val: Total time: 0:00:03 (0.3410 s / it)
* Acc@1 73.171 Acc@5 95.732 loss 0.857
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [158]  [ 0/54]  eta: 0:07:38  lr: 0.000005  min_lr: 0.000000  loss: 1.4174 (1.4174)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1083 (9.1083)  time: 8.4898 (8.4898 -- 8.4898)  data: 5.9493 (5.9493 -- 5.9493)  max mem: 16413
Epoch: [158]  [20/54]  eta: 0:00:43  lr: 0.000005  min_lr: 0.000000  loss: 1.7692 (1.6889)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9427 (8.1289)  time: 0.9307 (0.5229 -- 4.5219)  data: 0.0423 (0.0004 -- 0.8155)  max mem: 16413
Epoch: [158]  [40/54]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 1.6773 (1.7068)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8646 (7.4763)  time: 0.8166 (0.5140 -- 4.2586)  data: 0.0011 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [158]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.5227 (1.6674)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1628 (7.4577)  time: 0.5960 (0.4953 -- 2.0682)  data: 0.0786 (0.0002 -- 1.5603)  max mem: 16413
Epoch: [158] Total time: 0:00:51 (0.9548 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.5227 (1.6764)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1628 (7.4577)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6828 (0.6828)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1720 (2.1720 -- 2.1720)  data: 1.9947 (1.9947 -- 1.9947)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6671 (0.6657)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3543 (0.0219 -- 2.1720)  data: 0.1995 (0.0001 -- 1.9947)  max mem: 16413
Val: Total time: 0:00:03 (0.3544 s / it)
* Acc@1 73.780 Acc@5 95.122 loss 0.857
Accuracy of the network on the 163 val images: 73.78%
Max accuracy: 73.78%
Epoch: [159]  [ 0/54]  eta: 0:06:14  lr: 0.000005  min_lr: 0.000000  loss: 2.1785 (2.1785)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8969 (5.8969)  time: 6.9304 (6.9304 -- 6.9304)  data: 5.7394 (5.7394 -- 5.7394)  max mem: 16413
Epoch: [159]  [20/54]  eta: 0:00:41  lr: 0.000005  min_lr: 0.000000  loss: 1.7978 (1.7978)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3566 (8.2612)  time: 0.9315 (0.5124 -- 4.4494)  data: 0.3913 (0.0003 -- 3.9145)  max mem: 16413
Epoch: [159]  [40/54]  eta: 0:00:13  lr: 0.000005  min_lr: 0.000000  loss: 1.7885 (1.7787)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6855 (8.0364)  time: 0.7613 (0.5158 -- 3.0574)  data: 0.2153 (0.0004 -- 2.5418)  max mem: 16413
[2023-10-23 18:58:09,800] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:58:09,800] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 18:58:09,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 18:58:09,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [159]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.6387 (1.7449)  loss_scale: 16384.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6855 (7.8056)  time: 0.6550 (0.4947 -- 1.8731)  data: 0.1368 (0.0001 -- 1.3470)  max mem: 16413
Epoch: [159] Total time: 0:00:49 (0.9213 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.6387 (1.7364)  loss_scale: 16384.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6855 (7.8056)
[2023-10-23 18:58:16,315] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-10-23 18:58:16,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-10-23 18:58:16,320] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-159/mp_rank_00_model_states.pt
[2023-10-23 18:58:16,320] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-159/mp_rank_00_model_states.pt...
[2023-10-23 18:58:17,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-159/mp_rank_00_model_states.pt.
[2023-10-23 18:58:17,523] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/10]  eta: 0:00:22  loss: 0.6794 (0.6794)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2005 (2.2005 -- 2.2005)  data: 2.0217 (2.0217 -- 2.0217)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6637 (0.6647)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3585 (0.0229 -- 2.2005)  data: 0.2023 (0.0001 -- 2.0217)  max mem: 16413
Val: Total time: 0:00:03 (0.3586 s / it)
* Acc@1 73.171 Acc@5 95.122 loss 0.855
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 73.78%
Epoch: [160]  [ 0/54]  eta: 0:06:34  lr: 0.000005  min_lr: 0.000000  loss: 1.7574 (1.7574)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8730 (9.8730)  time: 7.3102 (7.3102 -- 7.3102)  data: 6.7921 (6.7921 -- 6.7921)  max mem: 16413
Epoch: [160]  [20/54]  eta: 0:00:43  lr: 0.000005  min_lr: 0.000000  loss: 1.5882 (1.6050)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0020 (7.3700)  time: 0.9817 (0.5220 -- 3.0004)  data: 0.1500 (0.0003 -- 2.3502)  max mem: 16413
Epoch: [160]  [40/54]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 1.6937 (1.6395)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3877 (7.7542)  time: 0.8409 (0.5135 -- 3.0172)  data: 0.1565 (0.0001 -- 2.4873)  max mem: 16413
Epoch: [160]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7636 (1.6687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6710 (7.8583)  time: 0.6908 (0.4937 -- 3.0172)  data: 0.1757 (0.0001 -- 2.4873)  max mem: 16413
Epoch: [160] Total time: 0:00:51 (0.9516 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7636 (1.6841)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6710 (7.8583)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6754 (0.6754)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0599 (2.0599 -- 2.0599)  data: 1.8803 (1.8803 -- 1.8803)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6728 (0.6665)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3439 (0.0221 -- 2.0599)  data: 0.1881 (0.0001 -- 1.8803)  max mem: 16413
Val: Total time: 0:00:03 (0.3440 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.854
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [161]  [ 0/54]  eta: 0:06:25  lr: 0.000005  min_lr: 0.000000  loss: 1.3777 (1.3777)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4124 (9.4124)  time: 7.1403 (7.1403 -- 7.1403)  data: 6.5952 (6.5952 -- 6.5952)  max mem: 16413
Epoch: [161]  [20/54]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.8602 (1.7924)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1916 (8.1738)  time: 0.8100 (0.5240 -- 2.9476)  data: 0.1215 (0.0002 -- 2.4030)  max mem: 16413
Epoch: [161]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.7823 (1.7970)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8094 (7.8719)  time: 0.9305 (0.5253 -- 3.5583)  data: 0.0299 (0.0004 -- 0.3680)  max mem: 16413
Epoch: [161]  [53/54]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6741 (1.8006)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0732 (7.6505)  time: 0.6975 (0.4959 -- 2.0996)  data: 0.0118 (0.0002 -- 0.2077)  max mem: 16413
Epoch: [161] Total time: 0:00:50 (0.9278 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6741 (1.7312)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0732 (7.6505)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6814 (0.6814)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9814 (1.9814 -- 1.9814)  data: 1.7869 (1.7869 -- 1.7869)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6814 (0.6714)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3362 (0.0224 -- 1.9814)  data: 0.1788 (0.0001 -- 1.7869)  max mem: 16413
Val: Total time: 0:00:03 (0.3363 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.861
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [162]  [ 0/54]  eta: 0:06:49  lr: 0.000004  min_lr: 0.000000  loss: 1.3107 (1.3107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1013 (8.1013)  time: 7.5873 (7.5873 -- 7.5873)  data: 7.0406 (7.0406 -- 7.0406)  max mem: 16413
[2023-10-23 19:00:24,138] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:00:24,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 19:00:24,138] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:00:24,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 19:00:32,160] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8766
[2023-10-23 19:00:32,160] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8766
[2023-10-23 19:00:32,161] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:00:32,161] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:00:32,161] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [162]  [20/54]  eta: 0:00:38  lr: 0.000004  min_lr: 0.000000  loss: 1.9173 (1.7903)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2062 (8.2513)  time: 0.8139 (0.5227 -- 3.2517)  data: 0.2385 (0.0002 -- 2.4215)  max mem: 16413
Epoch: [162]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.6869 (1.7419)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9718 (7.9002)  time: 0.9883 (0.5113 -- 4.8377)  data: 0.4410 (0.0003 -- 4.2941)  max mem: 16413
Epoch: [162]  [53/54]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6246 (1.7218)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9084 (7.7219)  time: 0.7194 (0.4964 -- 3.4569)  data: 0.1927 (0.0001 -- 2.9473)  max mem: 16413
Epoch: [162] Total time: 0:00:51 (0.9463 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6246 (1.7306)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9084 (7.7219)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6884 (0.6884)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0792 (2.0792 -- 2.0792)  data: 1.9003 (1.9003 -- 1.9003)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6786 (0.6706)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (97.5610)  time: 0.3462 (0.0223 -- 2.0792)  data: 0.1901 (0.0001 -- 1.9003)  max mem: 16413
Val: Total time: 0:00:03 (0.3463 s / it)
* Acc@1 72.561 Acc@5 95.732 loss 0.863
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [163]  [ 0/54]  eta: 0:06:05  lr: 0.000004  min_lr: 0.000000  loss: 1.9763 (1.9763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.3787 (13.3787)  time: 6.7757 (6.7757 -- 6.7757)  data: 4.9650 (4.9650 -- 4.9650)  max mem: 16413
[2023-10-23 19:01:20,548] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8811
[2023-10-23 19:01:20,548] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8811
[2023-10-23 19:01:20,548] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:01:20,549] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:01:20,549] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [163]  [20/54]  eta: 0:00:40  lr: 0.000004  min_lr: 0.000000  loss: 1.8314 (1.8711)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3660 (8.0315)  time: 0.9270 (0.5238 -- 3.5014)  data: 0.1134 (0.0005 -- 1.5439)  max mem: 16413
Epoch: [163]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.7874 (1.8327)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1524 (7.9662)  time: 0.9147 (0.5150 -- 2.7325)  data: 0.2492 (0.0002 -- 1.8223)  max mem: 16413
Epoch: [163]  [53/54]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7239 (1.8403)  loss_scale: 8192.0000 (9557.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7983 (7.9099)  time: 0.7019 (0.4948 -- 2.0404)  data: 0.1488 (0.0001 -- 1.5249)  max mem: 16413
Epoch: [163] Total time: 0:00:51 (0.9489 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7239 (1.7807)  loss_scale: 8192.0000 (9557.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7983 (7.9099)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6947 (0.6947)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1528 (2.1528 -- 2.1528)  data: 1.9739 (1.9739 -- 1.9739)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6703 (0.6706)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3538 (0.0226 -- 2.1528)  data: 0.1975 (0.0001 -- 1.9739)  max mem: 16413
Val: Total time: 0:00:03 (0.3539 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.867
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [164]  [ 0/54]  eta: 0:08:25  lr: 0.000004  min_lr: 0.000000  loss: 2.0257 (2.0257)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5162 (6.5162)  time: 9.3522 (9.3522 -- 9.3522)  data: 6.9947 (6.9947 -- 6.9947)  max mem: 16413
Epoch: [164]  [20/54]  eta: 0:00:45  lr: 0.000004  min_lr: 0.000000  loss: 1.5974 (1.6941)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7657 (7.6743)  time: 0.9327 (0.5024 -- 4.5297)  data: 0.0293 (0.0002 -- 0.5648)  max mem: 16413
Epoch: [164]  [40/54]  eta: 0:00:15  lr: 0.000004  min_lr: 0.000000  loss: 1.7915 (1.7323)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6089 (7.7550)  time: 0.8907 (0.5224 -- 3.4723)  data: 0.0013 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [164]  [53/54]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6356 (1.7063)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2946 (7.8691)  time: 0.6643 (0.4934 -- 3.4723)  data: 0.0006 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [164] Total time: 0:00:52 (0.9688 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6356 (1.7104)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2946 (7.8691)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6954 (0.6954)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0617 (2.0617 -- 2.0617)  data: 1.8755 (1.8755 -- 1.8755)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6733 (0.6685)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3448 (0.0255 -- 2.0617)  data: 0.1876 (0.0001 -- 1.8755)  max mem: 16413
Val: Total time: 0:00:03 (0.3449 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.867
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [165]  [ 0/54]  eta: 0:07:03  lr: 0.000004  min_lr: 0.000000  loss: 2.1648 (2.1648)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5560 (6.5560)  time: 7.8346 (7.8346 -- 7.8346)  data: 6.4086 (6.4086 -- 6.4086)  max mem: 16413
Epoch: [165]  [20/54]  eta: 0:00:39  lr: 0.000004  min_lr: 0.000000  loss: 1.8605 (1.8381)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3492 (7.9273)  time: 0.8256 (0.5205 -- 3.2427)  data: 0.2005 (0.0009 -- 2.1318)  max mem: 16413
[2023-10-23 19:03:27,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:03:27,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:03:27,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 19:03:27,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [165]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.4866 (1.7089)  loss_scale: 16384.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1263 (7.6780)  time: 0.8856 (0.5253 -- 2.4572)  data: 0.0575 (0.0003 -- 1.1173)  max mem: 16413
Epoch: [165]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6198 (1.7034)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1263 (7.6836)  time: 0.7105 (0.4948 -- 2.1691)  data: 0.0013 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [165] Total time: 0:00:50 (0.9264 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6198 (1.7054)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1263 (7.6836)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6926 (0.6926)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1255 (2.1255 -- 2.1255)  data: 1.9566 (1.9566 -- 1.9566)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6721 (0.6676)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3506 (0.0223 -- 2.1255)  data: 0.1957 (0.0001 -- 1.9566)  max mem: 16413
Val: Total time: 0:00:03 (0.3507 s / it)
* Acc@1 72.561 Acc@5 95.122 loss 0.865
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [166]  [ 0/54]  eta: 0:05:50  lr: 0.000003  min_lr: 0.000000  loss: 1.8049 (1.8049)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.4427 (12.4427)  time: 6.4836 (6.4836 -- 6.4836)  data: 5.9494 (5.9494 -- 5.9494)  max mem: 16413
Epoch: [166]  [20/54]  eta: 0:00:40  lr: 0.000003  min_lr: 0.000000  loss: 1.8510 (1.6674)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8631 (8.2506)  time: 0.9359 (0.5276 -- 3.8371)  data: 0.3936 (0.0003 -- 3.3323)  max mem: 16413
[2023-10-23 19:04:20,045] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8992
[2023-10-23 19:04:20,046] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:04:20,046] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8992
[2023-10-23 19:04:20,046] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:04:20,047] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-10-23 19:04:27,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[7.949978441649432e-08, 7.949978441649432e-08, 1.0599971255532576e-07, 1.0599971255532576e-07, 1.4133295007376767e-07, 1.4133295007376767e-07, 1.8844393343169025e-07, 1.8844393343169025e-07, 2.512585779089203e-07, 2.512585779089203e-07, 3.3501143721189377e-07, 3.3501143721189377e-07, 4.46681916282525e-07, 4.46681916282525e-07, 5.955758883767e-07, 5.955758883767e-07, 7.941011845022667e-07, 7.941011845022667e-07, 1.0588015793363556e-06, 1.0588015793363556e-06, 1.4117354391151407e-06, 1.4117354391151407e-06, 1.8823139188201878e-06, 1.8823139188201878e-06, 2.5097518917602504e-06, 2.5097518917602504e-06, 3.3463358556803337e-06, 3.3463358556803337e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 19:04:27,061] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=18.959185581639833, CurrSamplesPerSec=6.606662337626539, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [166]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.8289 (1.6811)  loss_scale: 8192.0000 (13786.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6788 (8.2871)  time: 0.8962 (0.5323 -- 2.3912)  data: 0.1800 (0.0004 -- 1.8452)  max mem: 16413
Epoch: [166]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7273 (1.7063)  loss_scale: 8192.0000 (12439.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6788 (8.3081)  time: 0.7421 (0.4966 -- 2.0110)  data: 0.0405 (0.0001 -- 0.7923)  max mem: 16413
Epoch: [166] Total time: 0:00:51 (0.9523 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7273 (1.7032)  loss_scale: 8192.0000 (12439.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6788 (8.3081)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6907 (0.6907)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9528 (1.9528 -- 1.9528)  data: 1.7484 (1.7484 -- 1.7484)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6735 (0.6661)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3334 (0.0222 -- 1.9528)  data: 0.1749 (0.0001 -- 1.7484)  max mem: 16413
Val: Total time: 0:00:03 (0.3335 s / it)
* Acc@1 71.951 Acc@5 95.122 loss 0.863
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [167]  [ 0/54]  eta: 0:06:44  lr: 0.000003  min_lr: 0.000000  loss: 1.8098 (1.8098)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5561 (8.5561)  time: 7.4839 (7.4839 -- 7.4839)  data: 6.9511 (6.9511 -- 6.9511)  max mem: 16413
Epoch: [167]  [20/54]  eta: 0:00:41  lr: 0.000003  min_lr: 0.000000  loss: 1.7149 (1.7311)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1470 (7.8942)  time: 0.9225 (0.5116 -- 4.5614)  data: 0.1150 (0.0008 -- 0.9590)  max mem: 16413
Epoch: [167]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.5826 (1.6575)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5829 (8.2659)  time: 0.8086 (0.5151 -- 2.2200)  data: 0.0901 (0.0004 -- 1.1648)  max mem: 16413
Epoch: [167]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6459 (1.6770)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6137 (8.4394)  time: 0.6340 (0.4949 -- 1.5754)  data: 0.0972 (0.0003 -- 1.0670)  max mem: 16413
Epoch: [167] Total time: 0:00:50 (0.9369 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6459 (1.7035)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6137 (8.4394)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6880 (0.6880)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9713 (1.9713 -- 1.9713)  data: 1.7528 (1.7528 -- 1.7528)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6742 (0.6689)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3355 (0.0221 -- 1.9713)  data: 0.1754 (0.0001 -- 1.7528)  max mem: 16413
Val: Total time: 0:00:03 (0.3356 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.862
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [168]  [ 0/54]  eta: 0:06:49  lr: 0.000003  min_lr: 0.000000  loss: 1.3795 (1.3795)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0518 (9.0518)  time: 7.5782 (7.5782 -- 7.5782)  data: 5.8437 (5.8437 -- 5.8437)  max mem: 16413
Epoch: [168]  [20/54]  eta: 0:00:40  lr: 0.000003  min_lr: 0.000000  loss: 1.9465 (1.7894)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4536 (7.7226)  time: 0.8569 (0.5287 -- 3.5019)  data: 0.0992 (0.0004 -- 1.4063)  max mem: 16413
Epoch: [168]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.8039 (1.7974)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1927 (8.5377)  time: 0.9590 (0.5351 -- 3.8972)  data: 0.0017 (0.0003 -- 0.0074)  max mem: 16413
[2023-10-23 19:06:25,622] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:06:25,622] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 19:06:25,622] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:06:25,663] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [168]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7621 (1.7452)  loss_scale: 8192.0000 (8950.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4770 (8.3173)  time: 0.6997 (0.4944 -- 3.8972)  data: 0.0008 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [168] Total time: 0:00:50 (0.9393 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7621 (1.7195)  loss_scale: 8192.0000 (8950.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4770 (8.3173)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6844 (0.6844)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9621 (1.9621 -- 1.9621)  data: 1.7687 (1.7687 -- 1.7687)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6769 (0.6697)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3395 (0.0221 -- 1.9621)  data: 0.1815 (0.0001 -- 1.7687)  max mem: 16413
Val: Total time: 0:00:03 (0.3396 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.862
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [169]  [ 0/54]  eta: 0:06:47  lr: 0.000003  min_lr: 0.000000  loss: 2.0081 (2.0081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7806 (8.7806)  time: 7.5544 (7.5544 -- 7.5544)  data: 6.7517 (6.7517 -- 6.7517)  max mem: 16413
Epoch: [169]  [20/54]  eta: 0:00:40  lr: 0.000003  min_lr: 0.000000  loss: 1.9059 (1.8516)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6517 (8.2865)  time: 0.8616 (0.5325 -- 2.7766)  data: 0.3093 (0.0010 -- 2.2487)  max mem: 16413
Epoch: [169]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.6820 (1.7723)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6054 (7.9205)  time: 0.8839 (0.5310 -- 3.4263)  data: 0.1882 (0.0004 -- 2.9120)  max mem: 16413
Epoch: [169]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6871 (1.7306)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2844 (7.6623)  time: 0.7503 (0.4951 -- 1.8397)  data: 0.0358 (0.0002 -- 0.7015)  max mem: 16413
Epoch: [169] Total time: 0:00:50 (0.9352 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6871 (1.7325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2844 (7.6623)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6812 (0.6812)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0845 (2.0845 -- 2.0845)  data: 1.9072 (1.9072 -- 1.9072)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6725 (0.6693)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3454 (0.0220 -- 2.0845)  data: 0.1908 (0.0001 -- 1.9072)  max mem: 16413
Val: Total time: 0:00:03 (0.3455 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.860
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [170]  [ 0/54]  eta: 0:06:09  lr: 0.000003  min_lr: 0.000000  loss: 1.2532 (1.2532)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5891 (7.5891)  time: 6.8405 (6.8405 -- 6.8405)  data: 6.3101 (6.3101 -- 6.3101)  max mem: 16413
[2023-10-23 19:07:47,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9199
[2023-10-23 19:07:47,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9199
[2023-10-23 19:07:47,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:07:47,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:07:47,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [170]  [20/54]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 1.7790 (1.7104)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0030 (7.9294)  time: 0.8285 (0.5135 -- 3.5780)  data: 0.2834 (0.0004 -- 3.0163)  max mem: 16413
Epoch: [170]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.8073 (1.7373)  loss_scale: 8192.0000 (11988.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4161 (8.1477)  time: 0.8903 (0.5210 -- 2.8397)  data: 0.3226 (0.0004 -- 2.3271)  max mem: 16413
Epoch: [170]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7269 (1.7119)  loss_scale: 8192.0000 (11074.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0347 (8.0526)  time: 0.8041 (0.4929 -- 2.8397)  data: 0.1725 (0.0001 -- 2.3271)  max mem: 16413
Epoch: [170] Total time: 0:00:51 (0.9487 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7269 (1.7256)  loss_scale: 8192.0000 (11074.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0347 (8.0526)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6864 (0.6864)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1075 (2.1075 -- 2.1075)  data: 1.9362 (1.9362 -- 1.9362)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6697 (0.6703)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (96.3415)  time: 0.3477 (0.0219 -- 2.1075)  data: 0.1937 (0.0001 -- 1.9362)  max mem: 16413
Val: Total time: 0:00:03 (0.3478 s / it)
* Acc@1 71.951 Acc@5 95.122 loss 0.862
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [171]  [ 0/54]  eta: 0:07:10  lr: 0.000003  min_lr: 0.000000  loss: 1.6393 (1.6393)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7203 (4.7203)  time: 7.9805 (7.9805 -- 7.9805)  data: 7.4460 (7.4460 -- 7.4460)  max mem: 16413
Epoch: [171]  [20/54]  eta: 0:00:42  lr: 0.000002  min_lr: 0.000000  loss: 1.7733 (1.6971)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2033 (7.9530)  time: 0.9090 (0.5221 -- 6.3667)  data: 0.3675 (0.0003 -- 5.8455)  max mem: 16413
Epoch: [171]  [40/54]  eta: 0:00:15  lr: 0.000002  min_lr: 0.000000  loss: 1.7962 (1.7434)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5057 (8.0821)  time: 0.9837 (0.5033 -- 5.8154)  data: 0.4528 (0.0003 -- 5.3173)  max mem: 16413
Epoch: [171]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7876 (1.7088)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2822 (7.9083)  time: 0.5620 (0.4930 -- 1.5382)  data: 0.0528 (0.0001 -- 1.0467)  max mem: 16413
Epoch: [171] Total time: 0:00:53 (0.9903 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7876 (1.7508)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2822 (7.9083)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6867 (0.6867)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0686 (2.0686 -- 2.0686)  data: 1.8958 (1.8958 -- 1.8958)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6677 (0.6691)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3434 (0.0222 -- 2.0686)  data: 0.1897 (0.0001 -- 1.8958)  max mem: 16413
Val: Total time: 0:00:03 (0.3435 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.862
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [172]  [ 0/54]  eta: 0:06:46  lr: 0.000002  min_lr: 0.000000  loss: 1.6842 (1.6842)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6277 (7.6277)  time: 7.5251 (7.5251 -- 7.5251)  data: 6.6931 (6.6931 -- 6.6931)  max mem: 16413
Epoch: [172]  [20/54]  eta: 0:00:40  lr: 0.000002  min_lr: 0.000000  loss: 1.7296 (1.7390)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3959 (7.5932)  time: 0.8738 (0.5210 -- 4.6213)  data: 0.1506 (0.0003 -- 1.8601)  max mem: 16413
[2023-10-23 19:10:00,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:10:00,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:10:00,856] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 19:10:00,856] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [172]  [40/54]  eta: 0:00:15  lr: 0.000002  min_lr: 0.000000  loss: 1.8047 (1.7189)  loss_scale: 8192.0000 (8391.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7347 (8.0538)  time: 0.9578 (0.5195 -- 3.9358)  data: 0.0015 (0.0005 -- 0.0035)  max mem: 16413
Epoch: [172]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5762 (1.7062)  loss_scale: 16384.0000 (10315.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0861 (7.6621)  time: 0.6880 (0.4938 -- 2.3847)  data: 0.0009 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [172] Total time: 0:00:52 (0.9748 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5762 (1.7158)  loss_scale: 16384.0000 (10315.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0861 (7.6621)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6886 (0.6886)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1084 (2.1084 -- 2.1084)  data: 1.9337 (1.9337 -- 1.9337)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6677 (0.6675)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3481 (0.0220 -- 2.1084)  data: 0.1934 (0.0001 -- 1.9337)  max mem: 16413
Val: Total time: 0:00:03 (0.3482 s / it)
* Acc@1 72.561 Acc@5 94.512 loss 0.862
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 73.78%
Epoch: [173]  [ 0/54]  eta: 0:05:52  lr: 0.000002  min_lr: 0.000000  loss: 1.3785 (1.3785)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3671 (9.3671)  time: 6.5192 (6.5192 -- 6.5192)  data: 6.0075 (6.0075 -- 6.0075)  max mem: 16413
Epoch: [173]  [20/54]  eta: 0:00:43  lr: 0.000002  min_lr: 0.000000  loss: 1.8247 (1.7246)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3851 (8.8719)  time: 1.0145 (0.5101 -- 4.5016)  data: 0.4749 (0.0003 -- 3.9855)  max mem: 16413
Epoch: [173]  [40/54]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 1.8776 (1.7807)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0796 (8.3115)  time: 0.8057 (0.5238 -- 3.2459)  data: 0.2521 (0.0005 -- 2.6695)  max mem: 16413
Epoch: [173]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.9761 (1.8171)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1445 (8.3597)  time: 0.6641 (0.4950 -- 3.3169)  data: 0.1415 (0.0001 -- 2.8087)  max mem: 16413
Epoch: [173] Total time: 0:00:52 (0.9684 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.9761 (1.7836)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1445 (8.3597)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6881 (0.6881)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9604 (1.9604 -- 1.9604)  data: 1.7679 (1.7679 -- 1.7679)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6658 (0.6659)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3361 (0.0224 -- 1.9604)  data: 0.1788 (0.0001 -- 1.7679)  max mem: 16413
Val: Total time: 0:00:03 (0.3362 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.861
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [174]  [ 0/54]  eta: 0:07:34  lr: 0.000002  min_lr: 0.000000  loss: 1.1764 (1.1764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3353 (6.3353)  time: 8.4168 (8.4168 -- 8.4168)  data: 7.8696 (7.8696 -- 7.8696)  max mem: 16413
Epoch: [174]  [20/54]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000000  loss: 1.8209 (1.7223)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9019 (7.5217)  time: 0.7575 (0.5144 -- 3.1574)  data: 0.2126 (0.0004 -- 2.6198)  max mem: 16413
Epoch: [174]  [40/54]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 1.7210 (1.7318)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8448 (7.5015)  time: 0.9255 (0.5283 -- 4.1458)  data: 0.1888 (0.0003 -- 1.9754)  max mem: 16413
Epoch: [174]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6871 (1.7237)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8448 (7.5798)  time: 0.7065 (0.4952 -- 2.4871)  data: 0.1883 (0.0001 -- 1.9754)  max mem: 16413
Epoch: [174] Total time: 0:00:50 (0.9348 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6871 (1.7255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8448 (7.5798)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6862 (0.6862)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9602 (1.9602 -- 1.9602)  data: 1.7663 (1.7663 -- 1.7663)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6691 (0.6650)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3354 (0.0228 -- 1.9602)  data: 0.1768 (0.0001 -- 1.7663)  max mem: 16413
Val: Total time: 0:00:03 (0.3356 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.859
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [175]  [ 0/54]  eta: 0:06:37  lr: 0.000002  min_lr: 0.000000  loss: 1.1280 (1.1280)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6884 (6.6884)  time: 7.3631 (7.3631 -- 7.3631)  data: 6.8087 (6.8087 -- 6.8087)  max mem: 16413
[2023-10-23 19:12:13,105] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:12:13,106] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 19:12:13,106] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:12:13,107] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [175]  [20/54]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8483 (1.7755)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4236 (7.5789)  time: 0.7923 (0.5284 -- 4.4675)  data: 0.0410 (0.0003 -- 0.7914)  max mem: 16413
[2023-10-23 19:12:26,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9471
[2023-10-23 19:12:26,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9471
[2023-10-23 19:12:26,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:12:26,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:12:26,243] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [175]  [40/54]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 1.7460 (1.7746)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4888 (7.5318)  time: 0.9468 (0.5086 -- 4.1856)  data: 0.3011 (0.0003 -- 3.6560)  max mem: 16413
Epoch: [175]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6650 (1.7500)  loss_scale: 16384.0000 (20935.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4305 (8.0315)  time: 0.6409 (0.4942 -- 1.7604)  data: 0.1187 (0.0002 -- 1.2413)  max mem: 16413
Epoch: [175] Total time: 0:00:51 (0.9478 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6650 (1.7198)  loss_scale: 16384.0000 (20935.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4305 (8.0315)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6881 (0.6881)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0907 (2.0907 -- 2.0907)  data: 1.9232 (1.9232 -- 1.9232)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6724 (0.6664)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3465 (0.0222 -- 2.0907)  data: 0.1924 (0.0001 -- 1.9232)  max mem: 16413
Val: Total time: 0:00:03 (0.3466 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.861
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [176]  [ 0/54]  eta: 0:06:11  lr: 0.000002  min_lr: 0.000000  loss: 1.5416 (1.5416)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4066 (6.4066)  time: 6.8846 (6.8846 -- 6.8846)  data: 6.3669 (6.3669 -- 6.3669)  max mem: 16413
Epoch: [176]  [20/54]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000000  loss: 1.5660 (1.6141)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7634 (8.1842)  time: 0.8559 (0.5330 -- 2.1056)  data: 0.1729 (0.0005 -- 1.5567)  max mem: 16413
Epoch: [176]  [40/54]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 1.6600 (1.6546)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1099 (8.0722)  time: 0.9769 (0.5342 -- 4.1405)  data: 0.4277 (0.0002 -- 3.6091)  max mem: 16413
Epoch: [176]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7105 (1.6794)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6227 (8.2940)  time: 0.6724 (0.4939 -- 2.1737)  data: 0.1464 (0.0001 -- 1.6452)  max mem: 16413
Epoch: [176] Total time: 0:00:51 (0.9526 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7105 (1.6763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6227 (8.2940)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6906 (0.6906)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0856 (2.0856 -- 2.0856)  data: 1.9076 (1.9076 -- 1.9076)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6766 (0.6675)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3457 (0.0222 -- 2.0856)  data: 0.1908 (0.0001 -- 1.9076)  max mem: 16413
Val: Total time: 0:00:03 (0.3458 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.863
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [177]  [ 0/54]  eta: 0:06:57  lr: 0.000002  min_lr: 0.000000  loss: 2.2152 (2.2152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1562 (6.1562)  time: 7.7290 (7.7290 -- 7.7290)  data: 7.2038 (7.2038 -- 7.2038)  max mem: 16413
Epoch: [177]  [20/54]  eta: 0:00:40  lr: 0.000002  min_lr: 0.000000  loss: 1.6026 (1.6336)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0789 (8.1293)  time: 0.8557 (0.5175 -- 4.0202)  data: 0.3157 (0.0005 -- 3.5067)  max mem: 16413
Epoch: [177]  [40/54]  eta: 0:00:15  lr: 0.000002  min_lr: 0.000000  loss: 1.5777 (1.6250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4742 (8.1284)  time: 1.0098 (0.5078 -- 5.5946)  data: 0.4812 (0.0003 -- 5.0709)  max mem: 16413
[2023-10-23 19:14:38,088] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:14:38,088] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:14:38,088] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 19:14:38,088] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [177]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6580 (1.6252)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4742 (7.9842)  time: 0.5909 (0.4964 -- 2.0577)  data: 0.0811 (0.0001 -- 1.5358)  max mem: 16413
Epoch: [177] Total time: 0:00:51 (0.9566 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6580 (1.6508)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4742 (7.9842)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6897 (0.6897)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9309 (1.9309 -- 1.9309)  data: 1.7306 (1.7306 -- 1.7306)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6763 (0.6672)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3344 (0.0222 -- 1.9309)  data: 0.1763 (0.0001 -- 1.7306)  max mem: 16413
Val: Total time: 0:00:03 (0.3345 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.864
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [178]  [ 0/54]  eta: 0:06:39  lr: 0.000002  min_lr: 0.000000  loss: 1.3632 (1.3632)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2659 (6.2659)  time: 7.4048 (7.4048 -- 7.4048)  data: 6.8710 (6.8710 -- 6.8710)  max mem: 16413
Epoch: [178]  [20/54]  eta: 0:00:42  lr: 0.000001  min_lr: 0.000000  loss: 1.7278 (1.7058)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4581 (8.3487)  time: 0.9283 (0.5208 -- 5.7133)  data: 0.3779 (0.0003 -- 5.1870)  max mem: 16413
Epoch: [178]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.6822 (1.6744)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5122 (7.8812)  time: 0.8515 (0.5202 -- 3.8099)  data: 0.3098 (0.0003 -- 3.2990)  max mem: 16413
[2023-10-23 19:15:33,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9658
[2023-10-23 19:15:33,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9658
[2023-10-23 19:15:33,191] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:15:33,191] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:15:33,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 19:15:35,028] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9661
[2023-10-23 19:15:35,028] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9661
[2023-10-23 19:15:35,028] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:15:35,028] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:15:35,028] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [178]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7572 (1.6839)  loss_scale: 32768.0000 (29582.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7842 (7.8507)  time: 0.6483 (0.4862 -- 1.5119)  data: 0.1228 (0.0003 -- 0.9871)  max mem: 16413
Epoch: [178] Total time: 0:00:51 (0.9472 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7572 (1.7488)  loss_scale: 32768.0000 (29582.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7842 (7.8507)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6915 (0.6915)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0498 (2.0498 -- 2.0498)  data: 1.8779 (1.8779 -- 1.8779)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6780 (0.6683)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3425 (0.0220 -- 2.0498)  data: 0.1879 (0.0001 -- 1.8779)  max mem: 16413
Val: Total time: 0:00:03 (0.3426 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.866
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [179]  [ 0/54]  eta: 0:06:49  lr: 0.000001  min_lr: 0.000000  loss: 2.0197 (2.0197)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5575 (9.5575)  time: 7.5845 (7.5845 -- 7.5845)  data: 6.6895 (6.6895 -- 6.6895)  max mem: 16413
Epoch: [179]  [20/54]  eta: 0:00:40  lr: 0.000001  min_lr: 0.000000  loss: 1.7920 (1.8562)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2345 (7.8841)  time: 0.8766 (0.5364 -- 3.0875)  data: 0.2057 (0.0004 -- 2.1691)  max mem: 16413
Epoch: [179]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7531 (1.8377)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9451 (7.6610)  time: 0.8904 (0.5339 -- 3.0673)  data: 0.0480 (0.0006 -- 0.7476)  max mem: 16413
Epoch: [179]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7531 (1.8196)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2597 (7.7739)  time: 0.6686 (0.4968 -- 2.1919)  data: 0.0646 (0.0001 -- 0.9690)  max mem: 16413
Epoch: [179] Total time: 0:00:50 (0.9389 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7531 (1.7722)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2597 (7.7739)
[2023-10-23 19:16:32,328] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-10-23 19:16:32,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-10-23 19:16:32,333] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-179/mp_rank_00_model_states.pt
[2023-10-23 19:16:32,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-179/mp_rank_00_model_states.pt...
[2023-10-23 19:16:33,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-179/mp_rank_00_model_states.pt.
[2023-10-23 19:16:33,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6943 (0.6943)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0346 (2.0346 -- 2.0346)  data: 1.8552 (1.8552 -- 1.8552)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6784 (0.6685)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3421 (0.0230 -- 2.0346)  data: 0.1856 (0.0001 -- 1.8552)  max mem: 16413
Val: Total time: 0:00:03 (0.3422 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [180]  [ 0/54]  eta: 0:06:59  lr: 0.000001  min_lr: 0.000000  loss: 1.3475 (1.3475)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2719 (8.2719)  time: 7.7658 (7.7658 -- 7.7658)  data: 7.2524 (7.2524 -- 7.2524)  max mem: 16413
Epoch: [180]  [20/54]  eta: 0:00:42  lr: 0.000001  min_lr: 0.000000  loss: 1.7377 (1.7262)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9990 (7.4472)  time: 0.9230 (0.5207 -- 3.0090)  data: 0.3531 (0.0001 -- 2.4832)  max mem: 16413
Epoch: [180]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.6558 (1.7346)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8989 (7.4698)  time: 0.7888 (0.5194 -- 3.0960)  data: 0.1313 (0.0003 -- 1.8351)  max mem: 16413
Epoch: [180]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6115 (1.7026)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4145 (7.4869)  time: 0.7178 (0.4944 -- 2.4721)  data: 0.2034 (0.0001 -- 1.9656)  max mem: 16413
Epoch: [180] Total time: 0:00:50 (0.9403 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6115 (1.7513)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4145 (7.4869)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6954 (0.6954)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0391 (2.0391 -- 2.0391)  data: 1.8396 (1.8396 -- 1.8396)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6796 (0.6689)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3418 (0.0223 -- 2.0391)  data: 0.1840 (0.0001 -- 1.8396)  max mem: 16413
Val: Total time: 0:00:03 (0.3419 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [181]  [ 0/54]  eta: 0:05:20  lr: 0.000001  min_lr: 0.000000  loss: 1.8589 (1.8589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8517 (6.8517)  time: 5.9409 (5.9409 -- 5.9409)  data: 5.4129 (5.4129 -- 5.4129)  max mem: 16413
[2023-10-23 19:17:51,242] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:17:51,243] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 19:17:51,246] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:17:51,247] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [181]  [20/54]  eta: 0:00:41  lr: 0.000001  min_lr: 0.000000  loss: 1.6131 (1.6672)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0005 (7.3821)  time: 0.9773 (0.5197 -- 3.6268)  data: 0.2708 (0.0008 -- 2.3248)  max mem: 16413
Epoch: [181]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7607 (1.6933)  loss_scale: 16384.0000 (13187.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1387 (7.7642)  time: 0.8407 (0.5205 -- 3.5085)  data: 0.0091 (0.0003 -- 0.1536)  max mem: 16413
Epoch: [181]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7931 (1.7062)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8495 (7.7099)  time: 0.6302 (0.4947 -- 1.7711)  data: 0.0494 (0.0001 -- 0.5103)  max mem: 16413
Epoch: [181] Total time: 0:00:51 (0.9468 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7931 (1.7141)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8495 (7.7099)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6943 (0.6943)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0230 (2.0230 -- 2.0230)  data: 1.8427 (1.8427 -- 1.8427)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6814 (0.6687)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3400 (0.0224 -- 2.0230)  data: 0.1843 (0.0001 -- 1.8427)  max mem: 16413
Val: Total time: 0:00:03 (0.3401 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [182]  [ 0/54]  eta: 0:07:11  lr: 0.000001  min_lr: 0.000000  loss: 1.4305 (1.4305)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6694 (5.6694)  time: 7.9879 (7.9879 -- 7.9879)  data: 7.4251 (7.4251 -- 7.4251)  max mem: 16413
[2023-10-23 19:18:37,431] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9835
[2023-10-23 19:18:37,431] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:18:37,431] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9835
[2023-10-23 19:18:37,432] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:18:37,432] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [182]  [20/54]  eta: 0:00:40  lr: 0.000001  min_lr: 0.000000  loss: 1.5944 (1.6071)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0286 (7.6929)  time: 0.8454 (0.5147 -- 2.4860)  data: 0.1942 (0.0004 -- 1.9528)  max mem: 16413
Epoch: [182]  [40/54]  eta: 0:00:13  lr: 0.000001  min_lr: 0.000000  loss: 1.6795 (1.6322)  loss_scale: 8192.0000 (9590.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5926 (7.7560)  time: 0.7918 (0.5162 -- 4.5674)  data: 0.1211 (0.0002 -- 2.3940)  max mem: 16413
Epoch: [182]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6855 (1.6589)  loss_scale: 8192.0000 (9253.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2725 (7.8108)  time: 0.6623 (0.4943 -- 3.1981)  data: 0.0431 (0.0003 -- 0.8448)  max mem: 16413
Epoch: [182] Total time: 0:00:50 (0.9264 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6855 (1.7045)  loss_scale: 8192.0000 (9253.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2725 (7.8108)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6954 (0.6954)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0261 (2.0261 -- 2.0261)  data: 1.8532 (1.8532 -- 1.8532)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6809 (0.6692)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3402 (0.0222 -- 2.0261)  data: 0.1854 (0.0001 -- 1.8532)  max mem: 16413
Val: Total time: 0:00:03 (0.3403 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.868
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [183]  [ 0/54]  eta: 0:06:18  lr: 0.000001  min_lr: 0.000000  loss: 2.3438 (2.3438)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3007 (8.3007)  time: 7.0139 (7.0139 -- 7.0139)  data: 6.4860 (6.4860 -- 6.4860)  max mem: 16413
Epoch: [183]  [20/54]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.6686 (1.7629)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0536 (8.5142)  time: 0.8052 (0.5234 -- 2.8623)  data: 0.2570 (0.0005 -- 2.3361)  max mem: 16413
Epoch: [183]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7967 (1.7639)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9264 (7.9485)  time: 0.9089 (0.5087 -- 3.4023)  data: 0.1270 (0.0001 -- 1.9257)  max mem: 16413
Epoch: [183]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7542 (1.7711)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5429 (7.9393)  time: 0.7183 (0.4957 -- 2.2238)  data: 0.0732 (0.0001 -- 0.9665)  max mem: 16413
Epoch: [183] Total time: 0:00:50 (0.9290 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7542 (1.7431)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5429 (7.9393)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6976 (0.6976)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0241 (2.0241 -- 2.0241)  data: 1.8447 (1.8447 -- 1.8447)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6827 (0.6695)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3405 (0.0228 -- 2.0241)  data: 0.1846 (0.0001 -- 1.8447)  max mem: 16413
Val: Total time: 0:00:03 (0.3406 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.869
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [184]  [ 0/54]  eta: 0:08:26  lr: 0.000001  min_lr: 0.000000  loss: 1.5914 (1.5914)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7512 (7.7512)  time: 9.3858 (9.3858 -- 9.3858)  data: 8.8567 (8.8567 -- 8.8567)  max mem: 16413
Epoch: [184]  [20/54]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.8122 (1.8019)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3577 (7.8274)  time: 0.7159 (0.5190 -- 2.5481)  data: 0.1678 (0.0001 -- 1.9944)  max mem: 16413
[2023-10-23 19:20:43,604] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:20:43,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 19:20:43,606] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:20:43,606] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [184]  [40/54]  eta: 0:00:15  lr: 0.000001  min_lr: 0.000000  loss: 1.7061 (1.7559)  loss_scale: 16384.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2008 (7.4880)  time: 1.0711 (0.5243 -- 4.8674)  data: 0.4640 (0.0002 -- 4.3527)  max mem: 16413
Epoch: [184]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7275 (1.7560)  loss_scale: 16384.0000 (12136.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5683 (7.5710)  time: 0.7546 (0.4947 -- 4.8674)  data: 0.2432 (0.0002 -- 4.3527)  max mem: 16413
Epoch: [184] Total time: 0:00:52 (0.9658 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7275 (1.6477)  loss_scale: 16384.0000 (12136.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5683 (7.5710)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6965 (0.6965)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0981 (2.0981 -- 2.0981)  data: 1.9203 (1.9203 -- 1.9203)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6812 (0.6692)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3471 (0.0220 -- 2.0981)  data: 0.1921 (0.0001 -- 1.9203)  max mem: 16413
Val: Total time: 0:00:03 (0.3472 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.869
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [185]  [ 0/54]  eta: 0:06:47  lr: 0.000001  min_lr: 0.000000  loss: 2.1146 (2.1146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4051 (6.4051)  time: 7.5416 (7.5416 -- 7.5416)  data: 5.7681 (5.7681 -- 5.7681)  max mem: 16413
[2023-10-23 19:21:24,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=54, lr=[1.6921857071706688e-08, 1.6921857071706688e-08, 2.2562476095608917e-08, 2.2562476095608917e-08, 3.008330146081189e-08, 3.008330146081189e-08, 4.011106861441585e-08, 4.011106861441585e-08, 5.3481424819221134e-08, 5.3481424819221134e-08, 7.130856642562817e-08, 7.130856642562817e-08, 9.507808856750424e-08, 9.507808856750424e-08, 1.2677078475667232e-07, 1.2677078475667232e-07, 1.6902771300889643e-07, 1.6902771300889643e-07, 2.2537028401186189e-07, 2.2537028401186189e-07, 3.0049371201581583e-07, 3.0049371201581583e-07, 4.006582826877545e-07, 4.006582826877545e-07, 5.342110435836727e-07, 5.342110435836727e-07, 7.122813914448968e-07, 7.122813914448968e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 19:21:24,598] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=18.996368197538477, CurrSamplesPerSec=22.708564317996117, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [185]  [20/54]  eta: 0:00:42  lr: 0.000001  min_lr: 0.000000  loss: 1.7141 (1.7991)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4127 (8.2549)  time: 0.9374 (0.5245 -- 4.4486)  data: 0.1415 (0.0003 -- 1.3574)  max mem: 16413
Epoch: [185]  [40/54]  eta: 0:00:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6875 (1.7516)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0217 (8.2116)  time: 0.9266 (0.5199 -- 3.4326)  data: 0.2912 (0.0002 -- 2.9294)  max mem: 16413
Epoch: [185]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6040 (1.7557)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5309 (8.0376)  time: 0.7038 (0.4956 -- 3.3850)  data: 0.1853 (0.0002 -- 2.8715)  max mem: 16413
Epoch: [185] Total time: 0:00:52 (0.9674 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6040 (1.7321)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5309 (8.0376)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.6942 (0.6942)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1499 (2.1499 -- 2.1499)  data: 1.9718 (1.9718 -- 1.9718)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6794 (0.6688)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3522 (0.0220 -- 2.1499)  data: 0.1973 (0.0001 -- 1.9718)  max mem: 16413
Val: Total time: 0:00:03 (0.3523 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [186]  [ 0/54]  eta: 0:05:46  lr: 0.000001  min_lr: 0.000000  loss: 1.6271 (1.6271)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5260 (9.5260)  time: 6.4229 (6.4229 -- 6.4229)  data: 4.6565 (4.6565 -- 4.6565)  max mem: 16413
Epoch: [186]  [20/54]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7133 (1.7602)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0268 (7.6110)  time: 0.8465 (0.5232 -- 2.0273)  data: 0.0910 (0.0002 -- 1.4824)  max mem: 16413
Epoch: [186]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7035 (1.7233)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1263 (7.6795)  time: 0.9530 (0.5170 -- 2.5792)  data: 0.1666 (0.0001 -- 2.0192)  max mem: 16413
[2023-10-23 19:22:51,929] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:22:51,929] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 19:22:51,929] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:22:51,929] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [186]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7035 (1.7125)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9852 (7.5683)  time: 0.7546 (0.4956 -- 2.5792)  data: 0.1012 (0.0001 -- 1.2855)  max mem: 16413
Epoch: [186] Total time: 0:00:50 (0.9340 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7035 (1.7257)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9852 (7.5683)
Val:  [ 0/10]  eta: 0:00:18  loss: 0.6941 (0.6941)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.8876 (1.8876 -- 1.8876)  data: 1.6844 (1.6844 -- 1.6844)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6789 (0.6686)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3264 (0.0228 -- 1.8876)  data: 0.1685 (0.0001 -- 1.6844)  max mem: 16413
Val: Total time: 0:00:03 (0.3266 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [187]  [ 0/54]  eta: 0:06:40  lr: 0.000001  min_lr: 0.000000  loss: 2.4279 (2.4279)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0808 (9.0808)  time: 7.4188 (7.4188 -- 7.4188)  data: 5.3189 (5.3189 -- 5.3189)  max mem: 16413
[2023-10-23 19:23:16,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10112
[2023-10-23 19:23:16,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10112
[2023-10-23 19:23:16,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:23:16,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:23:16,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [187]  [20/54]  eta: 0:00:42  lr: 0.000001  min_lr: 0.000000  loss: 1.7755 (1.8214)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3672 (8.0589)  time: 0.9524 (0.5192 -- 4.8220)  data: 0.0337 (0.0004 -- 0.3322)  max mem: 16413
[2023-10-23 19:23:25,776] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10121
[2023-10-23 19:23:25,776] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10121
[2023-10-23 19:23:25,776] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:23:25,776] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:23:25,776] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [187]  [40/54]  eta: 0:00:15  lr: 0.000001  min_lr: 0.000000  loss: 1.8162 (1.8185)  loss_scale: 8192.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2273 (7.9558)  time: 0.9080 (0.5217 -- 3.0410)  data: 0.0096 (0.0004 -- 0.1613)  max mem: 16413
Epoch: [187]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7057 (1.7603)  loss_scale: 8192.0000 (15928.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7016 (7.9983)  time: 0.6421 (0.4947 -- 2.9856)  data: 0.0090 (0.0002 -- 0.1613)  max mem: 16413
Epoch: [187] Total time: 0:00:51 (0.9483 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7057 (1.7713)  loss_scale: 8192.0000 (15928.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7016 (7.9983)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6934 (0.6934)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0248 (2.0248 -- 2.0248)  data: 1.8448 (1.8448 -- 1.8448)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6809 (0.6687)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3420 (0.0226 -- 2.0248)  data: 0.1846 (0.0001 -- 1.8448)  max mem: 16413
Val: Total time: 0:00:03 (0.3421 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [188]  [ 0/54]  eta: 0:07:49  lr: 0.000000  min_lr: 0.000000  loss: 2.0020 (2.0020)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0277 (12.0277)  time: 8.6976 (8.6976 -- 8.6976)  data: 8.1479 (8.1479 -- 8.1479)  max mem: 16413
Epoch: [188]  [20/54]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.6440 (1.6621)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8388 (7.9013)  time: 0.7799 (0.5316 -- 2.5444)  data: 0.1905 (0.0003 -- 2.0095)  max mem: 16413
Epoch: [188]  [40/54]  eta: 0:00:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8567 (1.7234)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7584 (7.9757)  time: 0.9931 (0.5212 -- 5.3660)  data: 0.4362 (0.0009 -- 4.8404)  max mem: 16413
Epoch: [188]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8582 (1.7571)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0852 (7.8768)  time: 0.6356 (0.4933 -- 2.0835)  data: 0.1177 (0.0001 -- 1.5815)  max mem: 16413
Epoch: [188] Total time: 0:00:51 (0.9541 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8582 (1.7404)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0852 (7.8768)
Val:  [ 0/10]  eta: 0:00:18  loss: 0.6939 (0.6939)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.8894 (1.8894 -- 1.8894)  data: 1.6987 (1.6987 -- 1.6987)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6803 (0.6681)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3269 (0.0223 -- 1.8894)  data: 0.1700 (0.0001 -- 1.6987)  max mem: 16413
Val: Total time: 0:00:03 (0.3270 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [189]  [ 0/54]  eta: 0:06:35  lr: 0.000000  min_lr: 0.000000  loss: 2.2047 (2.2047)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5074 (8.5074)  time: 7.3258 (7.3258 -- 7.3258)  data: 6.7815 (6.7815 -- 6.7815)  max mem: 16413
Epoch: [189]  [20/54]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.5219 (1.5989)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4787 (8.0864)  time: 0.8147 (0.5270 -- 3.5416)  data: 0.2621 (0.0002 -- 3.0110)  max mem: 16413
Epoch: [189]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.8686 (1.6876)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5593 (7.7342)  time: 0.9162 (0.5110 -- 5.0708)  data: 0.3327 (0.0004 -- 4.5443)  max mem: 16413
[2023-10-23 19:25:32,859] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:25:32,859] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 19:25:32,861] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:25:32,861] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [189]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.9183 (1.7395)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0658 (7.6907)  time: 0.6575 (0.4940 -- 2.2118)  data: 0.1407 (0.0002 -- 1.6885)  max mem: 16413
Epoch: [189] Total time: 0:00:50 (0.9376 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.9183 (1.7181)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0658 (7.6907)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6933 (0.6933)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0872 (2.0872 -- 2.0872)  data: 1.9107 (1.9107 -- 1.9107)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6805 (0.6683)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3458 (0.0222 -- 2.0872)  data: 0.1911 (0.0001 -- 1.9107)  max mem: 16413
Val: Total time: 0:00:03 (0.3459 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [190]  [ 0/54]  eta: 0:10:30  lr: 0.000000  min_lr: 0.000000  loss: 1.8555 (1.8555)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7157 (10.7157)  time: 11.6711 (11.6711 -- 11.6711)  data: 11.1590 (11.1590 -- 11.1590)  max mem: 16413
Epoch: [190]  [20/54]  eta: 0:00:41  lr: 0.000000  min_lr: 0.000000  loss: 1.5980 (1.6732)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2804 (7.7946)  time: 0.7050 (0.5225 -- 2.4769)  data: 0.1562 (0.0002 -- 1.9584)  max mem: 16413
Epoch: [190]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7688 (1.7269)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6179 (7.8474)  time: 0.9063 (0.5343 -- 4.1867)  data: 0.3578 (0.0004 -- 3.6722)  max mem: 16413
Epoch: [190]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5860 (1.6744)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5299 (8.0013)  time: 0.7152 (0.4939 -- 2.6946)  data: 0.1916 (0.0002 -- 2.1766)  max mem: 16413
Epoch: [190] Total time: 0:00:52 (0.9660 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5860 (1.7101)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5299 (8.0013)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6938 (0.6938)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9597 (1.9597 -- 1.9597)  data: 1.7591 (1.7591 -- 1.7591)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6809 (0.6685)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3354 (0.0221 -- 1.9597)  data: 0.1779 (0.0001 -- 1.7591)  max mem: 16413
Val: Total time: 0:00:03 (0.3355 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [191]  [ 0/54]  eta: 0:06:58  lr: 0.000000  min_lr: 0.000000  loss: 1.4598 (1.4598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5185 (8.5185)  time: 7.7557 (7.7557 -- 7.7557)  data: 7.1857 (7.1857 -- 7.1857)  max mem: 16413
Epoch: [191]  [20/54]  eta: 0:00:44  lr: 0.000000  min_lr: 0.000000  loss: 1.8436 (1.7363)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6571 (8.8775)  time: 0.9807 (0.5310 -- 3.1993)  data: 0.4334 (0.0002 -- 2.6854)  max mem: 16413
Epoch: [191]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7485 (1.7449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9502 (8.3064)  time: 0.7917 (0.5201 -- 3.9556)  data: 0.2465 (0.0004 -- 3.4042)  max mem: 16413
Epoch: [191]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7793 (1.7796)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1260 (8.0393)  time: 0.8939 (0.4931 -- 4.5681)  data: 0.3744 (0.0001 -- 4.0670)  max mem: 16413
Epoch: [191] Total time: 0:00:53 (0.9965 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7793 (1.7618)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1260 (8.0393)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6933 (0.6933)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0936 (2.0936 -- 2.0936)  data: 1.9089 (1.9089 -- 1.9089)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6805 (0.6682)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3483 (0.0223 -- 2.0936)  data: 0.1910 (0.0001 -- 1.9089)  max mem: 16413
Val: Total time: 0:00:03 (0.3484 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [192]  [ 0/54]  eta: 0:08:25  lr: 0.000000  min_lr: 0.000000  loss: 2.0030 (2.0030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6789 (10.6789)  time: 9.3604 (9.3604 -- 9.3604)  data: 8.8121 (8.8121 -- 8.8121)  max mem: 16413
[2023-10-23 19:27:51,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:27:51,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 19:27:51,553] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:27:51,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [192]  [20/54]  eta: 0:00:41  lr: 0.000000  min_lr: 0.000000  loss: 1.5888 (1.6125)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5744 (7.6153)  time: 0.8286 (0.5177 -- 3.5615)  data: 0.2874 (0.0005 -- 3.0355)  max mem: 16413
[2023-10-23 19:28:04,685] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10394
[2023-10-23 19:28:04,685] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10394
[2023-10-23 19:28:04,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:28:04,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:28:04,686] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [192]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7550 (1.6960)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3593 (7.8678)  time: 0.8873 (0.5208 -- 4.4896)  data: 0.1664 (0.0004 -- 1.7361)  max mem: 16413
Epoch: [192]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6604 (1.6871)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6018 (8.1091)  time: 0.6878 (0.4931 -- 2.3661)  data: 0.1031 (0.0001 -- 1.8702)  max mem: 16413
Epoch: [192] Total time: 0:00:52 (0.9673 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6604 (1.7219)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6018 (8.1091)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6933 (0.6933)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0082 (2.0082 -- 2.0082)  data: 1.8224 (1.8224 -- 1.8224)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6810 (0.6684)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3385 (0.0220 -- 2.0082)  data: 0.1823 (0.0001 -- 1.8224)  max mem: 16413
Val: Total time: 0:00:03 (0.3386 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [193]  [ 0/54]  eta: 0:05:37  lr: 0.000000  min_lr: 0.000000  loss: 1.4375 (1.4375)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0996 (6.0996)  time: 6.2508 (6.2508 -- 6.2508)  data: 5.7156 (5.7156 -- 5.7156)  max mem: 16413
Epoch: [193]  [20/54]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6180 (1.5947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7322 (7.1960)  time: 0.8838 (0.5361 -- 3.2139)  data: 0.1875 (0.0007 -- 2.1780)  max mem: 16413
Epoch: [193]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7077 (1.6419)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5815 (7.1132)  time: 0.9849 (0.5297 -- 2.9278)  data: 0.3377 (0.0004 -- 2.4085)  max mem: 16413
Epoch: [193]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7721 (1.6953)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1273 (7.2020)  time: 0.7643 (0.4934 -- 2.4580)  data: 0.2459 (0.0001 -- 1.9293)  max mem: 16413
Epoch: [193] Total time: 0:00:51 (0.9550 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7721 (1.6922)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1273 (7.2020)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6934 (0.6934)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9762 (1.9762 -- 1.9762)  data: 1.7574 (1.7574 -- 1.7574)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6801 (0.6682)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3375 (0.0228 -- 1.9762)  data: 0.1759 (0.0001 -- 1.7574)  max mem: 16413
Val: Total time: 0:00:03 (0.3376 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [194]  [ 0/54]  eta: 0:06:59  lr: 0.000000  min_lr: 0.000000  loss: 1.4638 (1.4638)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0413 (9.0413)  time: 7.7642 (7.7642 -- 7.7642)  data: 5.4131 (5.4131 -- 5.4131)  max mem: 16413
Epoch: [194]  [20/54]  eta: 0:00:40  lr: 0.000000  min_lr: 0.000000  loss: 1.6870 (1.7085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7812 (7.5547)  time: 0.8584 (0.5251 -- 4.3819)  data: 0.1058 (0.0005 -- 1.1723)  max mem: 16413
Epoch: [194]  [40/54]  eta: 0:00:16  lr: 0.000000  min_lr: 0.000000  loss: 1.8679 (1.7601)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7697 (7.8475)  time: 1.1380 (0.4959 -- 7.2265)  data: 0.0167 (0.0003 -- 0.3203)  max mem: 16413
[2023-10-23 19:30:15,875] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:30:15,875] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 19:30:15,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:30:15,876] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [194]  [53/54]  eta: 0:00:01  lr: 0.000000  min_lr: 0.000000  loss: 1.7783 (1.7524)  loss_scale: 16384.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4838 (7.7764)  time: 0.5912 (0.4929 -- 2.2313)  data: 0.0007 (0.0001 -- 0.0042)  max mem: 16413
Epoch: [194] Total time: 0:00:54 (1.0036 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7783 (1.7551)  loss_scale: 16384.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4838 (7.7764)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6933 (0.6933)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0559 (2.0559 -- 2.0559)  data: 1.8766 (1.8766 -- 1.8766)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6803 (0.6682)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3433 (0.0222 -- 2.0559)  data: 0.1877 (0.0001 -- 1.8766)  max mem: 16413
Val: Total time: 0:00:03 (0.3434 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [195]  [ 0/54]  eta: 0:06:13  lr: 0.000000  min_lr: 0.000000  loss: 1.6118 (1.6118)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3682 (6.3682)  time: 6.9226 (6.9226 -- 6.9226)  data: 6.3788 (6.3788 -- 6.3788)  max mem: 16413
Epoch: [195]  [20/54]  eta: 0:00:42  lr: 0.000000  min_lr: 0.000000  loss: 1.6642 (1.6760)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0535 (7.2204)  time: 0.9705 (0.5388 -- 2.7677)  data: 0.3308 (0.0004 -- 2.2555)  max mem: 16413
Epoch: [195]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6006 (1.6669)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1334 (7.3024)  time: 0.8219 (0.5224 -- 3.2930)  data: 0.2827 (0.0003 -- 2.7597)  max mem: 16413
[2023-10-23 19:31:15,108] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10583
[2023-10-23 19:31:15,108] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10583
[2023-10-23 19:31:15,108] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:31:15,108] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:31:15,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [195]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6943 (1.6712)  loss_scale: 32768.0000 (32464.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2661 (7.2255)  time: 0.8298 (0.4790 -- 3.1394)  data: 0.3101 (0.0002 -- 2.6417)  max mem: 16413
Epoch: [195] Total time: 0:00:52 (0.9771 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6943 (1.6956)  loss_scale: 32768.0000 (32464.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2661 (7.2255)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6933 (0.6933)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9855 (1.9855 -- 1.9855)  data: 1.7866 (1.7866 -- 1.7866)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6808 (0.6682)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3411 (0.0225 -- 1.9855)  data: 0.1817 (0.0001 -- 1.7866)  max mem: 16413
Val: Total time: 0:00:03 (0.3412 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [196]  [ 0/54]  eta: 0:07:30  lr: 0.000000  min_lr: 0.000000  loss: 1.6020 (1.6020)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4734 (6.4734)  time: 8.3440 (8.3440 -- 8.3440)  data: 7.6464 (7.6464 -- 7.6464)  max mem: 16413
Epoch: [196]  [20/54]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.4790 (1.5745)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6919 (8.0921)  time: 0.8074 (0.5279 -- 3.1500)  data: 0.1778 (0.0002 -- 2.5385)  max mem: 16413
Epoch: [196]  [40/54]  eta: 0:00:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8096 (1.6358)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2204 (7.5636)  time: 1.0539 (0.5079 -- 5.8684)  data: 0.5178 (0.0003 -- 5.3447)  max mem: 16413
Epoch: [196]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7854 (1.6731)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3667 (7.8351)  time: 0.6498 (0.4955 -- 2.8917)  data: 0.1394 (0.0001 -- 2.3779)  max mem: 16413
Epoch: [196] Total time: 0:00:52 (0.9718 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7854 (1.6885)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3667 (7.8351)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6933 (0.6933)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9023 (1.9023 -- 1.9023)  data: 1.6992 (1.6992 -- 1.6992)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6810 (0.6682)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3278 (0.0224 -- 1.9023)  data: 0.1700 (0.0001 -- 1.6992)  max mem: 16413
Val: Total time: 0:00:03 (0.3279 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [197]  [ 0/54]  eta: 0:07:11  lr: 0.000000  min_lr: 0.000000  loss: 1.5114 (1.5114)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1720 (7.1720)  time: 7.9870 (7.9870 -- 7.9870)  data: 7.4568 (7.4568 -- 7.4568)  max mem: 16413
Epoch: [197]  [20/54]  eta: 0:00:40  lr: 0.000000  min_lr: 0.000000  loss: 1.8600 (1.7276)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4510 (7.2470)  time: 0.8466 (0.5251 -- 3.2629)  data: 0.2797 (0.0007 -- 2.2896)  max mem: 16413
Epoch: [197]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.8763 (1.7523)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1069 (7.3493)  time: 0.9318 (0.5170 -- 5.1100)  data: 0.2315 (0.0004 -- 2.4218)  max mem: 16413
Epoch: [197]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6565 (1.7272)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9392 (7.5622)  time: 0.6717 (0.4951 -- 2.9322)  data: 0.1524 (0.0001 -- 2.4218)  max mem: 16413
Epoch: [197] Total time: 0:00:50 (0.9403 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6565 (1.7267)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9392 (7.5622)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.6940 (0.6940)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 1.9360 (1.9360 -- 1.9360)  data: 1.7460 (1.7460 -- 1.7460)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6807 (0.6683)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3310 (0.0228 -- 1.9360)  data: 0.1747 (0.0001 -- 1.7460)  max mem: 16413
Val: Total time: 0:00:03 (0.3311 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [198]  [ 0/54]  eta: 0:07:47  lr: 0.000000  min_lr: 0.000000  loss: 2.2341 (2.2341)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2119 (10.2119)  time: 8.6514 (8.6514 -- 8.6514)  data: 8.1298 (8.1298 -- 8.1298)  max mem: 16413
[2023-10-23 19:33:34,032] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:33:34,033] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 19:33:34,034] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 19:33:34,034] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [198]  [20/54]  eta: 0:00:41  lr: 0.000000  min_lr: 0.000000  loss: 1.7173 (1.7844)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5655 (8.2986)  time: 0.8409 (0.5198 -- 3.5723)  data: 0.3015 (0.0008 -- 3.0513)  max mem: 16413
Epoch: [198]  [40/54]  eta: 0:00:15  lr: 0.000000  min_lr: 0.000000  loss: 1.6024 (1.7083)  loss_scale: 32768.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8443 (8.0164)  time: 0.9361 (0.5190 -- 4.0381)  data: 0.4000 (0.0004 -- 3.5082)  max mem: 16413
[2023-10-23 19:33:54,883] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10736
[2023-10-23 19:33:54,883] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:33:54,883] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10736
[2023-10-23 19:33:54,883] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 19:33:54,883] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 19:33:56,367] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10739
[2023-10-23 19:33:56,367] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 19:33:56,367] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-10-23 19:33:56,367] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10739
[2023-10-23 19:33:56,367] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [198]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6168 (1.6929)  loss_scale: 16384.0000 (22603.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9199 (8.1554)  time: 0.5847 (0.4800 -- 1.6129)  data: 0.0702 (0.0002 -- 1.0935)  max mem: 16413
Epoch: [198] Total time: 0:00:51 (0.9460 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6168 (1.7291)  loss_scale: 16384.0000 (22603.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9199 (8.1554)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6932 (0.6932)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0264 (2.0264 -- 2.0264)  data: 1.8148 (1.8148 -- 1.8148)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6808 (0.6684)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3399 (0.0223 -- 2.0264)  data: 0.1816 (0.0001 -- 1.8148)  max mem: 16413
Val: Total time: 0:00:03 (0.3400 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Epoch: [199]  [ 0/54]  eta: 0:07:52  lr: 0.000000  min_lr: 0.000000  loss: 1.4526 (1.4526)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3237 (9.3237)  time: 8.7452 (8.7452 -- 8.7452)  data: 5.4933 (5.4933 -- 5.4933)  max mem: 16413
Epoch: [199]  [20/54]  eta: 0:00:41  lr: 0.000000  min_lr: 0.000000  loss: 1.6598 (1.7610)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6076 (8.8559)  time: 0.8435 (0.5373 -- 3.2645)  data: 0.0589 (0.0005 -- 0.6844)  max mem: 16413
Epoch: [199]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6247 (1.7135)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0487 (8.7283)  time: 0.8465 (0.5228 -- 2.6956)  data: 0.1830 (0.0005 -- 1.8062)  max mem: 16413
Epoch: [199]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6075 (1.6978)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2299 (8.7026)  time: 0.7105 (0.4947 -- 2.1489)  data: 0.1405 (0.0001 -- 1.6263)  max mem: 16413
Epoch: [199] Total time: 0:00:51 (0.9512 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6075 (1.7110)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2299 (8.7026)
[2023-10-23 19:34:54,494] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-10-23 19:34:54,496] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-199/mp_rank_00_model_states.pt
[2023-10-23 19:34:54,496] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-199/mp_rank_00_model_states.pt...
[2023-10-23 19:34:54,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-10-23 19:34:55,516] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_dash/checkpoint-199/mp_rank_00_model_states.pt.
[2023-10-23 19:34:55,516] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 0.6932 (0.6932)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0440 (2.0440 -- 2.0440)  data: 1.8635 (1.8635 -- 1.8635)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6805 (0.6683)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3433 (0.0227 -- 2.0440)  data: 0.1865 (0.0001 -- 1.8635)  max mem: 16413
Val: Total time: 0:00:03 (0.3434 s / it)
* Acc@1 71.951 Acc@5 94.512 loss 0.867
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 73.78%
Test:  [  0/408]  eta: 0:35:15  loss: 0.4355 (0.4355)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 5.1858 (5.1858 -- 5.1858)  data: 4.8924 (4.8924 -- 4.8924)  max mem: 16413
Test:  [ 10/408]  eta: 0:09:16  loss: 0.7928 (0.8525)  acc1: 66.6667 (74.2424)  acc5: 100.0000 (92.4242)  time: 1.3985 (0.1344 -- 5.1858)  data: 1.1778 (0.0011 -- 4.8924)  max mem: 16413
Test:  [ 20/408]  eta: 0:12:23  loss: 0.9306 (0.8995)  acc1: 66.6667 (69.8413)  acc5: 100.0000 (94.4444)  time: 1.7525 (0.1315 -- 19.3189)  data: 1.2201 (0.0011 -- 16.1690)  max mem: 16413
Test:  [ 30/408]  eta: 0:18:22  loss: 0.8731 (0.8562)  acc1: 66.6667 (69.3548)  acc5: 100.0000 (95.6989)  time: 3.7502 (0.1315 -- 26.8468)  data: 2.9101 (0.0012 -- 26.5264)  max mem: 16413
Test:  [ 40/408]  eta: 0:20:16  loss: 0.7740 (0.8756)  acc1: 66.6667 (70.3252)  acc5: 100.0000 (94.3089)  time: 4.7636 (0.1387 -- 26.8468)  data: 3.9576 (0.0007 -- 26.5264)  max mem: 16413
Test:  [ 50/408]  eta: 0:19:13  loss: 0.6883 (0.8407)  acc1: 66.6667 (71.5686)  acc5: 100.0000 (94.7712)  time: 3.6964 (0.1350 -- 22.4510)  data: 3.1531 (0.0004 -- 22.2805)  max mem: 16413
Test:  [ 60/408]  eta: 0:18:00  loss: 0.6588 (0.8221)  acc1: 66.6667 (71.5847)  acc5: 100.0000 (95.3552)  time: 2.6920 (0.1350 -- 22.4510)  data: 2.0797 (0.0004 -- 22.2805)  max mem: 16413
Test:  [ 70/408]  eta: 0:19:24  loss: 0.6085 (0.8107)  acc1: 66.6667 (72.3005)  acc5: 100.0000 (95.3052)  time: 4.0166 (0.1566 -- 35.5748)  data: 3.1843 (0.0007 -- 34.2678)  max mem: 16413
Test:  [ 80/408]  eta: 0:20:59  loss: 0.5455 (0.8066)  acc1: 83.3333 (72.6337)  acc5: 100.0000 (95.2675)  time: 6.0852 (0.1922 -- 35.5748)  data: 5.2344 (0.0007 -- 34.2678)  max mem: 16413
Test:  [ 90/408]  eta: 0:20:11  loss: 0.5455 (0.7828)  acc1: 83.3333 (73.2601)  acc5: 100.0000 (95.4212)  time: 5.0998 (0.1551 -- 33.8903)  data: 4.3205 (0.0003 -- 33.1715)  max mem: 16413
Test:  [100/408]  eta: 0:19:26  loss: 0.6092 (0.7873)  acc1: 66.6667 (73.1023)  acc5: 100.0000 (95.7096)  time: 3.5659 (0.1551 -- 30.4827)  data: 2.9707 (0.0003 -- 29.5482)  max mem: 16413
Test:  [110/408]  eta: 0:19:11  loss: 0.5960 (0.7799)  acc1: 66.6667 (72.8228)  acc5: 100.0000 (95.9459)  time: 4.1150 (0.1278 -- 35.4574)  data: 3.5318 (0.0004 -- 35.2952)  max mem: 16413
Test:  [120/408]  eta: 0:19:07  loss: 0.5878 (0.7715)  acc1: 83.3333 (73.5537)  acc5: 100.0000 (96.0055)  time: 4.9874 (0.1278 -- 35.4574)  data: 4.3225 (0.0004 -- 35.2952)  max mem: 16413
Test:  [130/408]  eta: 0:19:06  loss: 0.6150 (0.7623)  acc1: 83.3333 (73.7914)  acc5: 100.0000 (96.1832)  time: 5.5613 (0.1490 -- 21.3226)  data: 4.8059 (0.0006 -- 20.4139)  max mem: 16413
Test:  [140/408]  eta: 0:17:31  loss: 0.4880 (0.7542)  acc1: 83.3333 (74.1135)  acc5: 100.0000 (96.3357)  time: 3.5531 (0.1307 -- 21.3226)  data: 2.9238 (0.0006 -- 20.4139)  max mem: 16413
Test:  [150/408]  eta: 0:16:57  loss: 0.5322 (0.7569)  acc1: 66.6667 (73.9514)  acc5: 100.0000 (96.5784)  time: 2.7757 (0.1307 -- 21.2822)  data: 2.2452 (0.0007 -- 21.1314)  max mem: 16413
Test:  [160/408]  eta: 0:16:09  loss: 0.5975 (0.7527)  acc1: 66.6667 (74.1201)  acc5: 100.0000 (96.5839)  time: 3.8233 (0.1343 -- 21.2822)  data: 3.2427 (0.0007 -- 21.1314)  max mem: 16413
Test:  [170/408]  eta: 0:15:20  loss: 0.5975 (0.7550)  acc1: 83.3333 (74.2690)  acc5: 100.0000 (96.4912)  time: 3.2798 (0.1953 -- 20.8402)  data: 2.5761 (0.0009 -- 20.5460)  max mem: 16413
Test:  [180/408]  eta: 0:14:40  loss: 0.6115 (0.7524)  acc1: 66.6667 (73.9411)  acc5: 100.0000 (96.5930)  time: 3.4841 (0.1771 -- 16.0746)  data: 2.8334 (0.0006 -- 15.7172)  max mem: 16413
Test:  [190/408]  eta: 0:14:08  loss: 0.7038 (0.7666)  acc1: 66.6667 (73.1239)  acc5: 100.0000 (96.5969)  time: 4.1264 (0.1329 -- 25.5852)  data: 3.7896 (0.0006 -- 25.3401)  max mem: 16413
Test:  [200/408]  eta: 0:13:26  loss: 0.5402 (0.7580)  acc1: 66.6667 (73.5489)  acc5: 100.0000 (96.5174)  time: 4.0024 (0.1231 -- 25.5852)  data: 3.8344 (0.0004 -- 25.3401)  max mem: 16413
Test:  [210/408]  eta: 0:13:07  loss: 0.5225 (0.7602)  acc1: 66.6667 (73.0648)  acc5: 100.0000 (96.5245)  time: 4.7861 (0.1231 -- 23.3676)  data: 4.4883 (0.0004 -- 23.1715)  max mem: 16413
Test:  [220/408]  eta: 0:12:13  loss: 0.6615 (0.7703)  acc1: 66.6667 (73.0769)  acc5: 100.0000 (96.3801)  time: 4.1429 (0.1255 -- 23.3676)  data: 3.8399 (0.0002 -- 23.1715)  max mem: 16413
Test:  [230/408]  eta: 0:11:46  loss: 0.6615 (0.7695)  acc1: 83.3333 (73.3045)  acc5: 100.0000 (96.3203)  time: 3.8391 (0.1255 -- 26.7640)  data: 3.4573 (0.0002 -- 26.1008)  max mem: 16413
Test:  [240/408]  eta: 0:11:10  loss: 0.6240 (0.7692)  acc1: 66.6667 (73.0982)  acc5: 100.0000 (96.2656)  time: 4.9659 (0.1334 -- 27.6915)  data: 4.2650 (0.0008 -- 26.1008)  max mem: 16413
Test:  [250/408]  eta: 0:10:19  loss: 0.6301 (0.7756)  acc1: 66.6667 (72.9748)  acc5: 100.0000 (96.1487)  time: 3.3897 (0.1417 -- 27.6915)  data: 2.6385 (0.0007 -- 26.0527)  max mem: 16413
Test:  [260/408]  eta: 0:09:35  loss: 0.6012 (0.7730)  acc1: 66.6667 (72.8608)  acc5: 100.0000 (96.1686)  time: 2.6870 (0.1417 -- 13.2846)  data: 1.9076 (0.0007 -- 13.1284)  max mem: 16413
Test:  [270/408]  eta: 0:09:01  loss: 0.5890 (0.7841)  acc1: 66.6667 (72.5707)  acc5: 100.0000 (95.9410)  time: 3.9348 (0.1264 -- 39.8078)  data: 3.3654 (0.0004 -- 39.6593)  max mem: 16413
Test:  [280/408]  eta: 0:08:23  loss: 0.5860 (0.7838)  acc1: 66.6667 (72.6572)  acc5: 100.0000 (95.8482)  time: 4.4805 (0.1264 -- 39.8078)  data: 4.0889 (0.0004 -- 39.6593)  max mem: 16413
Test:  [290/408]  eta: 0:07:44  loss: 0.6206 (0.7816)  acc1: 66.6667 (72.5086)  acc5: 100.0000 (95.9908)  time: 4.0957 (0.1361 -- 33.4577)  data: 3.5077 (0.0006 -- 33.0912)  max mem: 16413
Test:  [300/408]  eta: 0:07:04  loss: 0.8145 (0.7919)  acc1: 66.6667 (72.2038)  acc5: 100.0000 (95.7918)  time: 3.8761 (0.1542 -- 33.4577)  data: 3.3204 (0.0004 -- 33.2385)  max mem: 16413
Test:  [310/408]  eta: 0:06:26  loss: 0.8182 (0.7921)  acc1: 66.6667 (72.1865)  acc5: 100.0000 (95.8735)  time: 4.0341 (0.1542 -- 35.5472)  data: 3.4355 (0.0004 -- 35.3814)  max mem: 16413
Test:  [320/408]  eta: 0:05:49  loss: 0.8888 (0.7997)  acc1: 66.6667 (71.8588)  acc5: 100.0000 (95.7944)  time: 4.6097 (0.1375 -- 35.5472)  data: 3.9594 (0.0007 -- 35.3814)  max mem: 16413
Test:  [330/408]  eta: 0:05:11  loss: 0.9101 (0.8001)  acc1: 66.6667 (71.8026)  acc5: 100.0000 (95.8207)  time: 4.8122 (0.1347 -- 33.0072)  data: 4.3689 (0.0007 -- 32.8453)  max mem: 16413
Test:  [340/408]  eta: 0:04:33  loss: 0.8829 (0.8022)  acc1: 66.6667 (71.7498)  acc5: 100.0000 (95.7967)  time: 4.9573 (0.1309 -- 36.6063)  data: 4.5761 (0.0006 -- 36.3863)  max mem: 16413
Test:  [350/408]  eta: 0:03:54  loss: 0.8829 (0.8083)  acc1: 66.6667 (71.5575)  acc5: 100.0000 (95.6315)  time: 4.8738 (0.1309 -- 36.6063)  data: 4.4174 (0.0006 -- 36.3863)  max mem: 16413
Test:  [360/408]  eta: 0:03:18  loss: 0.9341 (0.8145)  acc1: 66.6667 (71.3758)  acc5: 100.0000 (95.5679)  time: 5.8415 (0.1550 -- 28.5972)  data: 4.9198 (0.0012 -- 28.4086)  max mem: 16413
Test:  [370/408]  eta: 0:02:35  loss: 0.6423 (0.8091)  acc1: 66.6667 (71.4735)  acc5: 100.0000 (95.6873)  time: 4.7974 (0.2607 -- 28.5972)  data: 3.5447 (0.0018 -- 28.4086)  max mem: 16413
Test:  [380/408]  eta: 0:01:54  loss: 0.7910 (0.8224)  acc1: 66.6667 (71.2161)  acc5: 100.0000 (95.4943)  time: 3.5796 (0.2607 -- 15.8643)  data: 2.2144 (0.0021 -- 15.4799)  max mem: 16413
Test:  [390/408]  eta: 0:01:14  loss: 1.0096 (0.8255)  acc1: 66.6667 (71.0997)  acc5: 100.0000 (95.3112)  time: 5.2655 (0.1489 -- 19.6244)  data: 4.0945 (0.0005 -- 18.9777)  max mem: 16413
Test:  [400/408]  eta: 0:00:32  loss: 0.7456 (0.8294)  acc1: 66.6667 (70.9476)  acc5: 100.0000 (95.2203)  time: 4.5319 (0.1456 -- 29.3737)  data: 3.9883 (0.0002 -- 29.2041)  max mem: 16413
Test:  [407/408]  eta: 0:00:04  loss: 0.6789 (0.8356)  acc1: 66.6667 (70.8384)  acc5: 100.0000 (95.2147)  time: 4.9282 (0.1310 -- 41.4568)  data: 4.5322 (0.0002 -- 41.3044)  max mem: 16413
Test: Total time: 0:28:21 (4.1692 s / it)
* Acc@1 70.613 Acc@5 95.337 loss 0.838
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 4890 test videos: Top-1: 79.75%, Top-5: 98.16%
Training time 3:32:08
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-10-23 20:04:39,492] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-23 20:04:39,511] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast/annotations/Rear_view', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, img_diff_json_path=None, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sample_mode='normal', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=10, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fae9a785f10>
Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 648
Number of training training per epoch = 54
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-10-23 20:04:45,375] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-10-23 20:04:45,375] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-10-23 20:04:45,457] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-10-23 20:04:45,458] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-10-23 20:04:45,564] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.45723485946655273 seconds
[2023-10-23 20:04:46,707] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-10-23 20:04:46,714] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-10-23 20:04:46,714] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-10-23 20:04:46,729] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-10-23 20:04:46,729] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-23 20:04:46,730] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-10-23 20:04:46,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 20:04:46,730] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   amp_params ................... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fad9b294b20>
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   dump_state ................... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-23 20:04:46,731] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   pld_params ................... False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   world_size ................... 2
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-23 20:04:46,732] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-10-23 20:04:46,732] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 270
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [ 0/54]  eta: 0:11:44  lr: 0.000000  min_lr: 0.000000  loss: 2.7733 (2.7733)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 13.0507 (13.0507 -- 13.0507)  data: 6.3679 (6.3679 -- 6.3679)  max mem: 16413
Epoch: [0]  [20/54]  eta: 0:00:42  lr: 0.000003  min_lr: 0.000000  loss: 2.7728 (2.7729)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4143 (1.4394)  time: 0.6453 (0.4941 -- 2.7717)  data: 0.0020 (0.0004 -- 0.0131)  max mem: 16413
Epoch: [0]  [40/54]  eta: 0:00:15  lr: 0.000007  min_lr: 0.000000  loss: 2.7725 (2.7727)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3898 (1.4293)  time: 0.9007 (0.5183 -- 3.6662)  data: 0.0017 (0.0004 -- 0.0054)  max mem: 16413
Epoch: [0]  [53/54]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.7720 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4258 (1.4531)  time: 0.5875 (0.4808 -- 1.9024)  data: 0.0007 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [0] Total time: 0:00:50 (0.9384 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.7720 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4258 (1.4531)
Val:  [ 0/10]  eta: 0:00:25  loss: 2.7700 (2.7700)  acc1: 44.4444 (44.4444)  acc5: 77.7778 (77.7778)  time: 2.5065 (2.5065 -- 2.5065)  data: 2.2196 (2.2196 -- 2.2196)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.7697 (2.7698)  acc1: 44.4444 (42.6829)  acc5: 77.7778 (70.7317)  time: 0.3927 (0.0472 -- 2.5065)  data: 0.2220 (0.0001 -- 2.2196)  max mem: 16413
Val: Total time: 0:00:03 (0.3928 s / it)
* Acc@1 36.585 Acc@5 70.122 loss 2.770
Accuracy of the network on the 163 val images: 36.59%
[2023-10-23 20:05:41,370] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-10-23 20:05:41,374] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:05:41,374] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:05:41,374] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:05:42,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:05:42,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 36.59%
Epoch: [1]  [ 0/54]  eta: 0:06:32  lr: 0.000009  min_lr: 0.000000  loss: 2.7712 (2.7712)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4894 (1.4894)  time: 7.2633 (7.2633 -- 7.2633)  data: 6.0212 (6.0212 -- 6.0212)  max mem: 16413
Epoch: [1]  [20/54]  eta: 0:00:43  lr: 0.000013  min_lr: 0.000000  loss: 2.7710 (2.7708)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4390 (1.4581)  time: 0.9847 (0.5146 -- 5.5810)  data: 0.1371 (0.0002 -- 2.4086)  max mem: 16413
Epoch: [1]  [40/54]  eta: 0:00:14  lr: 0.000016  min_lr: 0.000000  loss: 2.7699 (2.7700)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4678 (1.4719)  time: 0.7970 (0.5244 -- 2.9186)  data: 0.0020 (0.0003 -- 0.0060)  max mem: 16413
Epoch: [1]  [53/54]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.7687 (2.7696)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4798 (1.4823)  time: 0.5590 (0.4874 -- 1.0984)  data: 0.0009 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [1] Total time: 0:00:49 (0.9214 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.7687 (2.7697)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4798 (1.4823)
Val:  [ 0/10]  eta: 0:00:19  loss: 2.7587 (2.7587)  acc1: 44.4444 (44.4444)  acc5: 66.6667 (66.6667)  time: 1.9062 (1.9062 -- 1.9062)  data: 1.6931 (1.6931 -- 1.6931)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.7580 (2.7560)  acc1: 33.3333 (37.8049)  acc5: 88.8889 (84.1463)  time: 0.3301 (0.0221 -- 1.9062)  data: 0.1706 (0.0001 -- 1.6931)  max mem: 16413
Val: Total time: 0:00:03 (0.3302 s / it)
* Acc@1 40.854 Acc@5 84.756 loss 2.757
Accuracy of the network on the 163 val images: 40.85%
[2023-10-23 20:06:35,493] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:06:35,495] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:06:35,495] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:06:35,496] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:06:36,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:06:36,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.85%
Epoch: [2]  [ 0/54]  eta: 0:07:19  lr: 0.000019  min_lr: 0.000000  loss: 2.7686 (2.7686)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9470 (1.9470)  time: 8.1481 (8.1481 -- 8.1481)  data: 5.7161 (5.7161 -- 5.7161)  max mem: 16413
[2023-10-23 20:07:00,121] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:07:00,121] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-10-23 20:07:00,122] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:07:00,122] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [2]  [20/54]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 2.7626 (2.7630)  loss_scale: 128.0000 (134.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5195 (1.6086)  time: 0.7483 (0.5119 -- 2.0830)  data: 0.1357 (0.0005 -- 1.3978)  max mem: 16413
Epoch: [2]  [40/54]  eta: 0:00:13  lr: 0.000026  min_lr: 0.000001  loss: 2.7563 (2.7590)  loss_scale: 256.0000 (193.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5268 (1.5640)  time: 0.8551 (0.5265 -- 2.0131)  data: 0.0756 (0.0003 -- 0.8134)  max mem: 16413
Epoch: [2]  [53/54]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 2.7501 (2.7566)  loss_scale: 256.0000 (208.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5106 (1.5629)  time: 0.6860 (0.4894 -- 2.3052)  data: 0.1068 (0.0001 -- 1.2539)  max mem: 16413
Epoch: [2] Total time: 0:00:49 (0.9168 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 2.7501 (2.7555)  loss_scale: 256.0000 (208.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5106 (1.5629)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.7161 (2.7161)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.0073 (2.0073 -- 2.0073)  data: 1.7728 (1.7728 -- 1.7728)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.7116 (2.7111)  acc1: 44.4444 (46.3415)  acc5: 88.8889 (84.1463)  time: 0.3380 (0.0223 -- 2.0073)  data: 0.1774 (0.0001 -- 1.7728)  max mem: 16413
Val: Total time: 0:00:03 (0.3381 s / it)
* Acc@1 51.220 Acc@5 87.805 loss 2.715
Accuracy of the network on the 163 val images: 51.22%
[2023-10-23 20:07:29,904] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:07:29,905] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:07:29,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:07:29,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:07:31,281] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:07:31,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 51.22%
Epoch: [3]  [ 0/54]  eta: 0:06:46  lr: 0.000028  min_lr: 0.000001  loss: 2.7254 (2.7254)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6404 (1.6404)  time: 7.5328 (7.5328 -- 7.5328)  data: 6.9869 (6.9869 -- 6.9869)  max mem: 16413
Epoch: [3]  [20/54]  eta: 0:00:42  lr: 0.000032  min_lr: 0.000001  loss: 2.7400 (2.7348)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5701 (1.5725)  time: 0.9238 (0.5232 -- 3.5991)  data: 0.2684 (0.0005 -- 3.0846)  max mem: 16413
Epoch: [3]  [40/54]  eta: 0:00:14  lr: 0.000035  min_lr: 0.000001  loss: 2.7086 (2.7223)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5739 (1.5813)  time: 0.8061 (0.5231 -- 2.2718)  data: 0.0840 (0.0001 -- 1.3513)  max mem: 16413
Epoch: [3]  [53/54]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 2.6974 (2.7144)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5403 (1.5940)  time: 0.6907 (0.4916 -- 1.9983)  data: 0.0161 (0.0001 -- 0.3067)  max mem: 16413
Epoch: [3] Total time: 0:00:50 (0.9392 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 2.6974 (2.7156)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5403 (1.5940)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.6141 (2.6141)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 2.0358 (2.0358 -- 2.0358)  data: 1.8234 (1.8234 -- 1.8234)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.6013 (2.6091)  acc1: 44.4444 (47.5610)  acc5: 88.8889 (84.1463)  time: 0.3419 (0.0223 -- 2.0358)  data: 0.1824 (0.0001 -- 1.8234)  max mem: 16413
Val: Total time: 0:00:03 (0.3420 s / it)
* Acc@1 51.220 Acc@5 87.195 loss 2.623
Accuracy of the network on the 163 val images: 51.22%
[2023-10-23 20:08:25,437] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:08:25,439] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:08:25,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:08:25,441] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:08:26,825] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:08:26,825] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 51.22%
Epoch: [4]  [ 0/54]  eta: 0:06:00  lr: 0.000038  min_lr: 0.000001  loss: 2.6905 (2.6905)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5032 (1.5032)  time: 6.6715 (6.6715 -- 6.6715)  data: 6.1213 (6.1213 -- 6.1213)  max mem: 16413
Epoch: [4]  [20/54]  eta: 0:00:39  lr: 0.000041  min_lr: 0.000001  loss: 2.6805 (2.6830)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6627 (1.6669)  time: 0.8813 (0.5251 -- 2.9852)  data: 0.1423 (0.0003 -- 2.3230)  max mem: 16413
[2023-10-23 20:09:10,132] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:09:10,132] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-10-23 20:09:10,132] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:09:10,133] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [4]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 2.6327 (2.6602)  loss_scale: 256.0000 (262.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7684 (1.7130)  time: 0.9506 (0.5256 -- 3.2085)  data: 0.3768 (0.0003 -- 2.6954)  max mem: 16413
Epoch: [4]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.6199 (2.6479)  loss_scale: 512.0000 (322.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7810 (1.7239)  time: 0.7258 (0.4922 -- 3.2085)  data: 0.1757 (0.0002 -- 2.6954)  max mem: 16413
Epoch: [4] Total time: 0:00:50 (0.9341 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.6199 (2.6469)  loss_scale: 512.0000 (322.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7810 (1.7239)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.4792 (2.4792)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 2.0007 (2.0007 -- 2.0007)  data: 1.8070 (1.8070 -- 1.8070)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.4390 (2.4660)  acc1: 44.4444 (43.9024)  acc5: 88.8889 (85.3659)  time: 0.3373 (0.0223 -- 2.0007)  data: 0.1808 (0.0001 -- 1.8070)  max mem: 16413
Val: Total time: 0:00:03 (0.3374 s / it)
* Acc@1 50.610 Acc@5 89.024 loss 2.493
Accuracy of the network on the 163 val images: 50.61%
Max accuracy: 51.22%
Epoch: [5]  [ 0/54]  eta: 0:06:33  lr: 0.000047  min_lr: 0.000001  loss: 2.6359 (2.6359)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9969 (1.9969)  time: 7.2857 (7.2857 -- 7.2857)  data: 6.3617 (6.3617 -- 6.3617)  max mem: 16413
Epoch: [5]  [20/54]  eta: 0:00:42  lr: 0.000047  min_lr: 0.000001  loss: 2.6148 (2.6092)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7983 (1.8719)  time: 0.9335 (0.5206 -- 4.6097)  data: 0.3926 (0.0005 -- 4.1052)  max mem: 16413
Epoch: [5]  [40/54]  eta: 0:00:15  lr: 0.000047  min_lr: 0.000001  loss: 2.5518 (2.5793)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9060 (1.8891)  time: 0.9074 (0.5081 -- 3.3911)  data: 0.3641 (0.0005 -- 2.8611)  max mem: 16413
Epoch: [5]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.5824 (2.5858)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9325 (1.9266)  time: 0.6747 (0.4933 -- 2.5640)  data: 0.1583 (0.0001 -- 2.0569)  max mem: 16413
Epoch: [5] Total time: 0:00:51 (0.9575 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.5824 (2.5747)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9325 (1.9266)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.3458 (2.3458)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 2.0480 (2.0480 -- 2.0480)  data: 1.8502 (1.8502 -- 1.8502)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.3046 (2.3247)  acc1: 44.4444 (51.2195)  acc5: 88.8889 (89.0244)  time: 0.3422 (0.0223 -- 2.0480)  data: 0.1851 (0.0001 -- 1.8502)  max mem: 16413
Val: Total time: 0:00:03 (0.3423 s / it)
* Acc@1 53.049 Acc@5 90.854 loss 2.360
Accuracy of the network on the 163 val images: 53.05%
[2023-10-23 20:10:15,964] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:10:15,966] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:10:15,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:10:15,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:10:17,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:10:17,300] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 53.05%
Epoch: [6]  [ 0/54]  eta: 0:07:06  lr: 0.000047  min_lr: 0.000001  loss: 2.5828 (2.5828)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9607 (1.9607)  time: 7.8941 (7.8941 -- 7.8941)  data: 5.4562 (5.4562 -- 5.4562)  max mem: 16413
Epoch: [6]  [20/54]  eta: 0:00:41  lr: 0.000047  min_lr: 0.000001  loss: 2.5494 (2.5420)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0644 (2.2092)  time: 0.8870 (0.5100 -- 3.7828)  data: 0.2601 (0.0003 -- 2.4460)  max mem: 16413
Epoch: [6]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.5273 (2.5288)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2953 (2.3388)  time: 0.7743 (0.5239 -- 3.0306)  data: 0.2350 (0.0003 -- 2.5048)  max mem: 16413
Epoch: [6]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4961 (2.5213)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3604 (2.3434)  time: 0.6940 (0.4924 -- 3.5295)  data: 0.1799 (0.0002 -- 3.0294)  max mem: 16413
Epoch: [6] Total time: 0:00:51 (0.9479 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4961 (2.5148)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3604 (2.3434)
Val:  [ 0/10]  eta: 0:00:19  loss: 2.2409 (2.2409)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 1.9902 (1.9902 -- 1.9902)  data: 1.7639 (1.7639 -- 1.7639)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.1933 (2.2178)  acc1: 55.5556 (53.6585)  acc5: 88.8889 (84.1463)  time: 0.3365 (0.0223 -- 1.9902)  data: 0.1765 (0.0001 -- 1.7639)  max mem: 16413
Val: Total time: 0:00:03 (0.3366 s / it)
* Acc@1 51.829 Acc@5 89.024 loss 2.259
Accuracy of the network on the 163 val images: 51.83%
Max accuracy: 53.05%
Epoch: [7]  [ 0/54]  eta: 0:06:34  lr: 0.000047  min_lr: 0.000001  loss: 2.4078 (2.4078)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7550 (2.7550)  time: 7.3098 (7.3098 -- 7.3098)  data: 5.4564 (5.4564 -- 5.4564)  max mem: 16413
[2023-10-23 20:11:22,666] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:11:22,666] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-10-23 20:11:22,667] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:11:22,667] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [7]  [20/54]  eta: 0:00:43  lr: 0.000047  min_lr: 0.000001  loss: 2.4802 (2.4598)  loss_scale: 1024.0000 (877.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4807 (2.6062)  time: 0.9831 (0.5164 -- 3.8295)  data: 0.0386 (0.0003 -- 0.4750)  max mem: 16413
Epoch: [7]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3961 (2.4370)  loss_scale: 1024.0000 (949.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4891 (2.5949)  time: 0.8023 (0.5287 -- 3.5801)  data: 0.0020 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [7]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4781 (2.4550)  loss_scale: 1024.0000 (967.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4859 (2.5741)  time: 0.7109 (0.4925 -- 3.5801)  data: 0.0011 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [7] Total time: 0:00:50 (0.9325 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4781 (2.4499)  loss_scale: 1024.0000 (967.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4859 (2.5741)
Val:  [ 0/10]  eta: 0:00:20  loss: 2.1324 (2.1324)  acc1: 33.3333 (33.3333)  acc5: 88.8889 (88.8889)  time: 2.0265 (2.0265 -- 2.0265)  data: 1.8499 (1.8499 -- 1.8499)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.0813 (2.1041)  acc1: 44.4444 (48.7805)  acc5: 88.8889 (87.8049)  time: 0.3399 (0.0223 -- 2.0265)  data: 0.1851 (0.0001 -- 1.8499)  max mem: 16413
Val: Total time: 0:00:03 (0.3400 s / it)
* Acc@1 50.000 Acc@5 90.244 loss 2.139
Accuracy of the network on the 163 val images: 50.00%
Max accuracy: 53.05%
Epoch: [8]  [ 0/54]  eta: 0:05:59  lr: 0.000047  min_lr: 0.000001  loss: 2.5420 (2.5420)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6154 (2.6154)  time: 6.6523 (6.6523 -- 6.6523)  data: 5.8187 (5.8187 -- 5.8187)  max mem: 16413
Epoch: [8]  [20/54]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000001  loss: 2.4058 (2.4278)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4778 (2.6130)  time: 0.8627 (0.5228 -- 2.4503)  data: 0.1556 (0.0005 -- 1.3683)  max mem: 16413
Epoch: [8]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3685 (2.4126)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8384 (2.8167)  time: 0.9105 (0.5389 -- 2.7933)  data: 0.3656 (0.0005 -- 2.2809)  max mem: 16413
Epoch: [8]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3685 (2.4020)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3355 (2.9273)  time: 0.7417 (0.4926 -- 2.2963)  data: 0.2264 (0.0001 -- 1.7910)  max mem: 16413
Epoch: [8] Total time: 0:00:50 (0.9345 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3685 (2.3947)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3355 (2.9273)
Val:  [ 0/10]  eta: 0:00:18  loss: 2.0591 (2.0591)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 1.8910 (1.8910 -- 1.8910)  data: 1.6788 (1.6788 -- 1.6788)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 2.0106 (2.0072)  acc1: 55.5556 (53.6585)  acc5: 88.8889 (87.8049)  time: 0.3308 (0.0223 -- 1.8910)  data: 0.1709 (0.0001 -- 1.6788)  max mem: 16413
Val: Total time: 0:00:03 (0.3310 s / it)
* Acc@1 54.268 Acc@5 90.854 loss 2.054
Accuracy of the network on the 163 val images: 54.27%
[2023-10-23 20:12:59,824] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:12:59,826] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:12:59,826] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:12:59,826] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:13:01,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:13:01,350] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 54.27%
Epoch: [9]  [ 0/54]  eta: 0:07:26  lr: 0.000047  min_lr: 0.000001  loss: 2.5694 (2.5694)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2311 (2.2311)  time: 8.2672 (8.2672 -- 8.2672)  data: 7.7556 (7.7556 -- 7.7556)  max mem: 16413
Epoch: [9]  [20/54]  eta: 0:00:41  lr: 0.000047  min_lr: 0.000001  loss: 2.3835 (2.3786)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7763 (2.8527)  time: 0.8601 (0.5150 -- 3.8875)  data: 0.2127 (0.0004 -- 3.3468)  max mem: 16413
[2023-10-23 20:13:30,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:13:30,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:13:30,482] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-10-23 20:13:30,482] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [9]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3454 (2.3591)  loss_scale: 2048.0000 (1398.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0942 (3.0862)  time: 0.8531 (0.5267 -- 4.4225)  data: 0.3037 (0.0002 -- 3.8995)  max mem: 16413
Epoch: [9]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3776 (2.3706)  loss_scale: 2048.0000 (1554.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4756 (3.1928)  time: 0.6501 (0.4933 -- 2.3382)  data: 0.1260 (0.0001 -- 1.8199)  max mem: 16413
Epoch: [9] Total time: 0:00:49 (0.9236 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3776 (2.3656)  loss_scale: 2048.0000 (1554.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4756 (3.1928)
Val:  [ 0/10]  eta: 0:00:19  loss: 2.0115 (2.0115)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 1.9726 (1.9726 -- 1.9726)  data: 1.7559 (1.7559 -- 1.7559)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.9313 (1.9424)  acc1: 44.4444 (47.5610)  acc5: 88.8889 (87.8049)  time: 0.3353 (0.0223 -- 1.9726)  data: 0.1757 (0.0001 -- 1.7559)  max mem: 16413
Val: Total time: 0:00:03 (0.3354 s / it)
* Acc@1 49.390 Acc@5 91.463 loss 1.994
Accuracy of the network on the 163 val images: 49.39%
Max accuracy: 54.27%
Epoch: [10]  [ 0/54]  eta: 0:06:59  lr: 0.000047  min_lr: 0.000001  loss: 2.2511 (2.2511)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8325 (2.8325)  time: 7.7674 (7.7674 -- 7.7674)  data: 7.2442 (7.2442 -- 7.2442)  max mem: 16413
Epoch: [10]  [20/54]  eta: 0:00:40  lr: 0.000047  min_lr: 0.000001  loss: 2.3361 (2.2926)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1515 (3.3649)  time: 0.8626 (0.5134 -- 2.2470)  data: 0.2308 (0.0002 -- 1.7005)  max mem: 16413
Epoch: [10]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3743 (2.3088)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2477 (3.4346)  time: 0.8858 (0.5222 -- 4.5131)  data: 0.3422 (0.0003 -- 3.9957)  max mem: 16413
Epoch: [10]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3341 (2.3189)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7391 (3.5690)  time: 0.7596 (0.4941 -- 3.3533)  data: 0.2475 (0.0001 -- 2.8296)  max mem: 16413
Epoch: [10] Total time: 0:00:51 (0.9509 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3341 (2.3041)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7391 (3.5690)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.9351 (1.9351)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 2.0197 (2.0197 -- 2.0197)  data: 1.8366 (1.8366 -- 1.8366)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.8365 (1.8466)  acc1: 44.4444 (53.6585)  acc5: 88.8889 (91.4634)  time: 0.3398 (0.0222 -- 2.0197)  data: 0.1838 (0.0001 -- 1.8366)  max mem: 16413
Val: Total time: 0:00:03 (0.3399 s / it)
* Acc@1 52.439 Acc@5 93.293 loss 1.907
Accuracy of the network on the 163 val images: 52.44%
Max accuracy: 54.27%
Epoch: [11]  [ 0/54]  eta: 0:05:39  lr: 0.000047  min_lr: 0.000001  loss: 2.4293 (2.4293)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9391 (3.9391)  time: 6.2808 (6.2808 -- 6.2808)  data: 5.7576 (5.7576 -- 5.7576)  max mem: 16413
Epoch: [11]  [20/54]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000001  loss: 2.2994 (2.3027)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2612 (4.6616)  time: 0.8698 (0.5343 -- 2.5326)  data: 0.1038 (0.0007 -- 1.0320)  max mem: 16413
Epoch: [11]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.2623 (2.3117)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6068 (4.3185)  time: 0.9085 (0.5241 -- 3.4144)  data: 0.0017 (0.0003 -- 0.0058)  max mem: 16413
[2023-10-23 20:15:35,495] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:15:35,495] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:15:35,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-10-23 20:15:35,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [11]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.2165 (2.3022)  loss_scale: 2048.0000 (2351.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4039 (4.1626)  time: 0.6931 (0.4941 -- 2.5551)  data: 0.0009 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [11] Total time: 0:00:49 (0.9246 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.2165 (2.3151)  loss_scale: 2048.0000 (2351.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4039 (4.1626)
Val:  [ 0/10]  eta: 0:00:18  loss: 1.8939 (1.8939)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.8929 (1.8929 -- 1.8929)  data: 1.6780 (1.6780 -- 1.6780)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.7848 (1.7916)  acc1: 55.5556 (60.9756)  acc5: 88.8889 (90.2439)  time: 0.3291 (0.0221 -- 1.8929)  data: 0.1679 (0.0001 -- 1.6780)  max mem: 16413
Val: Total time: 0:00:03 (0.3293 s / it)
* Acc@1 56.707 Acc@5 92.073 loss 1.852
Accuracy of the network on the 163 val images: 56.71%
[2023-10-23 20:15:42,933] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:15:42,935] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:15:42,935] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:15:42,935] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:15:44,323] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:15:44,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 56.71%
Epoch: [12]  [ 0/54]  eta: 0:06:18  lr: 0.000047  min_lr: 0.000001  loss: 2.0338 (2.0338)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7150 (5.7150)  time: 7.0029 (7.0029 -- 7.0029)  data: 4.6327 (4.6327 -- 4.6327)  max mem: 16413
Epoch: [12]  [20/54]  eta: 0:00:42  lr: 0.000047  min_lr: 0.000001  loss: 2.3250 (2.2722)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8834 (4.0615)  time: 0.9542 (0.5198 -- 2.6804)  data: 0.0996 (0.0003 -- 0.9328)  max mem: 16413
Epoch: [12]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.0905 (2.2368)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9082 (4.0783)  time: 0.8532 (0.5002 -- 4.0778)  data: 0.0275 (0.0004 -- 0.5242)  max mem: 16413
Epoch: [12]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.0728 (2.2219)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1451 (4.1381)  time: 0.7252 (0.4947 -- 2.6878)  data: 0.0007 (0.0001 -- 0.0030)  max mem: 16413
Epoch: [12] Total time: 0:00:51 (0.9574 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.0728 (2.2422)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1451 (4.1381)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.8762 (1.8762)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.9972 (1.9972 -- 1.9972)  data: 1.8017 (1.8017 -- 1.8017)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.7263 (1.7355)  acc1: 55.5556 (56.0976)  acc5: 88.8889 (89.0244)  time: 0.3383 (0.0223 -- 1.9972)  data: 0.1803 (0.0001 -- 1.8017)  max mem: 16413
Val: Total time: 0:00:03 (0.3384 s / it)
* Acc@1 54.268 Acc@5 91.463 loss 1.823
Accuracy of the network on the 163 val images: 54.27%
Max accuracy: 56.71%
Epoch: [13]  [ 0/54]  eta: 0:06:45  lr: 0.000047  min_lr: 0.000001  loss: 2.3439 (2.3439)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3319 (3.3319)  time: 7.5179 (7.5179 -- 7.5179)  data: 6.9911 (6.9911 -- 6.9911)  max mem: 16413
Epoch: [13]  [20/54]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000001  loss: 2.2376 (2.1558)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5653 (4.3796)  time: 0.8441 (0.5262 -- 4.1971)  data: 0.2990 (0.0004 -- 3.6510)  max mem: 16413
Epoch: [13]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3298 (2.2276)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3371 (4.3715)  time: 0.8471 (0.5150 -- 3.4937)  data: 0.2996 (0.0004 -- 2.9675)  max mem: 16413
Epoch: [13]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.2424 (2.2084)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9087 (4.4813)  time: 0.6055 (0.4928 -- 1.8381)  data: 0.0870 (0.0002 -- 1.3254)  max mem: 16413
Epoch: [13] Total time: 0:00:49 (0.9198 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.2424 (2.2203)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9087 (4.4813)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.8085 (1.8085)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.9411 (1.9411 -- 1.9411)  data: 1.7093 (1.7093 -- 1.7093)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.6726 (1.6665)  acc1: 55.5556 (54.8781)  acc5: 88.8889 (87.8049)  time: 0.3319 (0.0226 -- 1.9411)  data: 0.1710 (0.0001 -- 1.7093)  max mem: 16413
Val: Total time: 0:00:03 (0.3320 s / it)
* Acc@1 56.098 Acc@5 91.463 loss 1.741
Accuracy of the network on the 163 val images: 56.10%
Max accuracy: 56.71%
Epoch: [14]  [ 0/54]  eta: 0:05:05  lr: 0.000047  min_lr: 0.000001  loss: 2.3084 (2.3084)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2749 (3.2749)  time: 5.6524 (5.6524 -- 5.6524)  data: 5.0972 (5.0972 -- 5.0972)  max mem: 16413
[2023-10-23 20:17:48,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:17:48,568] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-10-23 20:17:48,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:17:48,569] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [14]  [20/54]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000001  loss: 2.2613 (2.2785)  loss_scale: 4096.0000 (5851.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2345 (5.0497)  time: 0.9410 (0.5247 -- 3.3509)  data: 0.3501 (0.0004 -- 2.8075)  max mem: 16413
Epoch: [14]  [40/54]  eta: 0:00:14  lr: 0.000047  min_lr: 0.000001  loss: 2.2413 (2.2466)  loss_scale: 8192.0000 (6993.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1510 (5.0055)  time: 0.8441 (0.5341 -- 3.1539)  data: 0.1427 (0.0006 -- 0.9200)  max mem: 16413
Epoch: [14]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1320 (2.2050)  loss_scale: 8192.0000 (7281.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8869 (4.8245)  time: 0.7683 (0.4946 -- 3.1539)  data: 0.0417 (0.0002 -- 0.8135)  max mem: 16413
Epoch: [14] Total time: 0:00:49 (0.9168 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1320 (2.1806)  loss_scale: 8192.0000 (7281.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8869 (4.8245)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.6994 (1.6994)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.9663 (1.9663 -- 1.9663)  data: 1.7657 (1.7657 -- 1.7657)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.5526 (1.5869)  acc1: 66.6667 (62.1951)  acc5: 88.8889 (89.0244)  time: 0.3376 (0.0222 -- 1.9663)  data: 0.1793 (0.0001 -- 1.7657)  max mem: 16413
Val: Total time: 0:00:03 (0.3377 s / it)
* Acc@1 59.756 Acc@5 92.073 loss 1.647
Accuracy of the network on the 163 val images: 59.76%
[2023-10-23 20:18:25,623] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:18:25,625] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:18:25,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:18:25,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:18:26,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:18:26,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.76%
Epoch: [15]  [ 0/54]  eta: 0:06:47  lr: 0.000047  min_lr: 0.000001  loss: 1.7911 (1.7911)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0316 (5.0316)  time: 7.5458 (7.5458 -- 7.5458)  data: 5.5005 (5.5005 -- 5.5005)  max mem: 16413
Epoch: [15]  [20/54]  eta: 0:00:41  lr: 0.000047  min_lr: 0.000001  loss: 2.2859 (2.2231)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5220 (4.9859)  time: 0.9179 (0.5228 -- 5.5483)  data: 0.1078 (0.0004 -- 2.1294)  max mem: 16413
Epoch: [15]  [40/54]  eta: 0:00:15  lr: 0.000047  min_lr: 0.000001  loss: 2.2862 (2.2276)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5689 (5.5908)  time: 0.9086 (0.5271 -- 3.0531)  data: 0.0014 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [15]  [53/54]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1985 (2.1870)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8175 (5.5130)  time: 0.6938 (0.4929 -- 3.0494)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [15] Total time: 0:00:51 (0.9565 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1985 (2.1686)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8175 (5.5130)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.6651 (1.6651)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.9274 (1.9274 -- 1.9274)  data: 1.7101 (1.7101 -- 1.7101)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.5103 (1.5411)  acc1: 66.6667 (62.1951)  acc5: 88.8889 (89.0244)  time: 0.3378 (0.0224 -- 1.9274)  data: 0.1769 (0.0001 -- 1.7101)  max mem: 16413
Val: Total time: 0:00:03 (0.3379 s / it)
* Acc@1 59.146 Acc@5 92.073 loss 1.610
Accuracy of the network on the 163 val images: 59.15%
Max accuracy: 59.76%
Epoch: [16]  [ 0/54]  eta: 0:05:00  lr: 0.000047  min_lr: 0.000001  loss: 2.0607 (2.0607)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5671 (5.5671)  time: 5.5586 (5.5586 -- 5.5586)  data: 5.0367 (5.0367 -- 5.0367)  max mem: 16413
Epoch: [16]  [20/54]  eta: 0:00:43  lr: 0.000046  min_lr: 0.000001  loss: 2.2550 (2.2072)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8125 (5.7576)  time: 1.0535 (0.5261 -- 3.0532)  data: 0.4731 (0.0004 -- 2.4925)  max mem: 16413
[2023-10-23 20:19:56,960] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:19:56,960] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:19:56,960] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-10-23 20:19:56,960] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [16]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.1213 (2.1652)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9752 (5.5889)  time: 0.8075 (0.5321 -- 2.4183)  data: 0.2360 (0.0003 -- 1.8921)  max mem: 16413
Epoch: [16]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0730 (2.1502)  loss_scale: 16384.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1025 (5.7159)  time: 0.7261 (0.4945 -- 2.4183)  data: 0.2132 (0.0002 -- 1.8921)  max mem: 16413
Epoch: [16] Total time: 0:00:50 (0.9388 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0730 (2.1538)  loss_scale: 16384.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1025 (5.7159)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.5571 (1.5571)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 1.9684 (1.9684 -- 1.9684)  data: 1.7627 (1.7627 -- 1.7627)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.4108 (1.4722)  acc1: 55.5556 (59.7561)  acc5: 100.0000 (92.6829)  time: 0.3342 (0.0222 -- 1.9684)  data: 0.1764 (0.0001 -- 1.7627)  max mem: 16413
Val: Total time: 0:00:03 (0.3343 s / it)
* Acc@1 59.756 Acc@5 94.512 loss 1.521
Accuracy of the network on the 163 val images: 59.76%
[2023-10-23 20:20:16,013] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:20:16,015] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:20:16,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:20:16,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:20:17,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:20:17,408] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.76%
Epoch: [17]  [ 0/54]  eta: 0:07:03  lr: 0.000046  min_lr: 0.000001  loss: 1.8068 (1.8068)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9154 (7.9154)  time: 7.8406 (7.8406 -- 7.8406)  data: 7.3112 (7.3112 -- 7.3112)  max mem: 16413
Epoch: [17]  [20/54]  eta: 0:00:41  lr: 0.000046  min_lr: 0.000001  loss: 2.2184 (2.2041)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0382 (5.6503)  time: 0.8771 (0.5122 -- 4.0605)  data: 0.3324 (0.0002 -- 3.5425)  max mem: 16413
Epoch: [17]  [40/54]  eta: 0:00:15  lr: 0.000046  min_lr: 0.000001  loss: 1.9890 (2.1257)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8374 (5.3188)  time: 0.9334 (0.5121 -- 4.4423)  data: 0.4003 (0.0002 -- 3.9090)  max mem: 16413
Epoch: [17]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2660 (2.1768)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8374 (5.5609)  time: 0.5419 (0.4948 -- 1.0589)  data: 0.0265 (0.0001 -- 0.5201)  max mem: 16413
Epoch: [17] Total time: 0:00:50 (0.9381 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2660 (2.0931)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8374 (5.5609)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.5750 (1.5750)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.9710 (1.9710 -- 1.9710)  data: 1.7256 (1.7256 -- 1.7256)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.4122 (1.4396)  acc1: 55.5556 (58.5366)  acc5: 88.8889 (90.2439)  time: 0.3342 (0.0224 -- 1.9710)  data: 0.1726 (0.0001 -- 1.7256)  max mem: 16413
Val: Total time: 0:00:03 (0.3343 s / it)
* Acc@1 59.146 Acc@5 92.683 loss 1.519
Accuracy of the network on the 163 val images: 59.15%
Max accuracy: 59.76%
Epoch: [18]  [ 0/54]  eta: 0:07:15  lr: 0.000046  min_lr: 0.000001  loss: 1.9132 (1.9132)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3482 (7.3482)  time: 8.0716 (8.0716 -- 8.0716)  data: 6.2509 (6.2509 -- 6.2509)  max mem: 16413
Epoch: [18]  [20/54]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.2003 (2.1345)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4503 (5.9844)  time: 0.8076 (0.5252 -- 2.4666)  data: 0.0373 (0.0004 -- 0.7176)  max mem: 16413
[2023-10-23 20:21:42,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.1005170051889259e-06, 1.1005170051889259e-06, 1.4673560069185678e-06, 1.4673560069185678e-06, 1.9564746758914238e-06, 1.9564746758914238e-06, 2.608632901188565e-06, 2.608632901188565e-06, 3.478177201584753e-06, 3.478177201584753e-06, 4.637569602113004e-06, 4.637569602113004e-06, 6.183426136150673e-06, 6.183426136150673e-06, 8.24456818153423e-06, 8.24456818153423e-06, 1.0992757575378974e-05, 1.0992757575378974e-05, 1.4657010100505299e-05, 1.4657010100505299e-05, 1.9542680134007064e-05, 1.9542680134007064e-05, 2.605690684534275e-05, 2.605690684534275e-05, 3.4742542460457007e-05, 3.4742542460457007e-05, 4.6323389947276004e-05, 4.6323389947276004e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 20:21:42,545] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=18.499502226750785, CurrSamplesPerSec=23.9453988564817, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.1619 (2.1580)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8616 (5.5541)  time: 0.9312 (0.5223 -- 4.2795)  data: 0.2798 (0.0003 -- 3.7497)  max mem: 16413
[2023-10-23 20:22:01,271] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:22:01,271] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-10-23 20:22:01,271] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:22:01,271] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [18]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1059 (2.1271)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8536 (5.3836)  time: 0.7940 (0.4971 -- 4.2795)  data: 0.1925 (0.0002 -- 3.7497)  max mem: 16413
Epoch: [18] Total time: 0:00:50 (0.9311 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1059 (2.1243)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8536 (5.3836)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.6041 (1.6041)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.0642 (2.0642 -- 2.0642)  data: 1.8758 (1.8758 -- 1.8758)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.3103 (1.3977)  acc1: 66.6667 (65.8537)  acc5: 88.8889 (90.2439)  time: 0.3448 (0.0223 -- 2.0642)  data: 0.1877 (0.0001 -- 1.8758)  max mem: 16413
Val: Total time: 0:00:03 (0.3449 s / it)
* Acc@1 62.195 Acc@5 92.073 loss 1.488
Accuracy of the network on the 163 val images: 62.20%
[2023-10-23 20:22:05,238] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:22:05,240] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:22:05,240] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:22:05,240] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:22:06,425] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:22:06,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.20%
Epoch: [19]  [ 0/54]  eta: 0:07:19  lr: 0.000046  min_lr: 0.000001  loss: 2.1582 (2.1582)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4201 (5.4201)  time: 8.1356 (8.1356 -- 8.1356)  data: 7.5883 (7.5883 -- 7.5883)  max mem: 16413
Epoch: [19]  [20/54]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.1212 (2.1143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6808 (6.1229)  time: 0.8251 (0.5217 -- 3.1319)  data: 0.2533 (0.0004 -- 2.5833)  max mem: 16413
Epoch: [19]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.0245 (2.0942)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0142 (6.0853)  time: 0.8835 (0.5235 -- 2.5573)  data: 0.3028 (0.0002 -- 2.0123)  max mem: 16413
Epoch: [19]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0793 (2.0897)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9766 (5.8664)  time: 0.7492 (0.4959 -- 2.6467)  data: 0.2148 (0.0001 -- 2.1490)  max mem: 16413
Epoch: [19] Total time: 0:00:51 (0.9552 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0793 (2.1016)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9766 (5.8664)
[2023-10-23 20:22:58,012] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-10-23 20:22:58,014] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-19/mp_rank_00_model_states.pt
[2023-10-23 20:22:58,014] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-19/mp_rank_00_model_states.pt...
[2023-10-23 20:22:58,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-10-23 20:22:59,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-19/mp_rank_00_model_states.pt.
[2023-10-23 20:22:59,026] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/10]  eta: 0:00:19  loss: 1.4836 (1.4836)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9488 (1.9488 -- 1.9488)  data: 1.7287 (1.7287 -- 1.7287)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.3124 (1.3720)  acc1: 66.6667 (65.8537)  acc5: 88.8889 (90.2439)  time: 0.3322 (0.0225 -- 1.9488)  data: 0.1730 (0.0001 -- 1.7287)  max mem: 16413
Val: Total time: 0:00:03 (0.3323 s / it)
* Acc@1 62.805 Acc@5 93.293 loss 1.450
Accuracy of the network on the 163 val images: 62.80%
[2023-10-23 20:23:02,588] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:23:02,590] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:23:02,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:23:02,590] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:23:04,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:23:04,006] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.80%
Epoch: [20]  [ 0/54]  eta: 0:06:45  lr: 0.000046  min_lr: 0.000001  loss: 1.8666 (1.8666)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7413 (3.7413)  time: 7.5028 (7.5028 -- 7.5028)  data: 6.9425 (6.9425 -- 6.9425)  max mem: 16413
Epoch: [20]  [20/54]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.1166 (2.0530)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8958 (5.9613)  time: 0.8376 (0.5246 -- 3.9606)  data: 0.2889 (0.0004 -- 3.3736)  max mem: 16413
[2023-10-23 20:23:40,004] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1112
[2023-10-23 20:23:40,004] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1112
[2023-10-23 20:23:40,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2023-10-23 20:23:40,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2023-10-23 20:23:40,004] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768, reducing to 16384.0
Epoch: [20]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.1244 (2.0827)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1750 (6.1939)  time: 0.9274 (0.5231 -- 3.6312)  data: 0.3847 (0.0003 -- 3.0923)  max mem: 16413
Epoch: [20]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 1.9943 (2.0566)  loss_scale: 16384.0000 (26093.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7388 (6.0826)  time: 0.6723 (0.4963 -- 2.9992)  data: 0.1274 (0.0001 -- 2.4754)  max mem: 16413
Epoch: [20] Total time: 0:00:49 (0.9254 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 1.9943 (2.0859)  loss_scale: 16384.0000 (26093.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7388 (6.0826)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.4933 (1.4933)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.0424 (2.0424 -- 2.0424)  data: 1.8576 (1.8576 -- 1.8576)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.2364 (1.3327)  acc1: 66.6667 (67.0732)  acc5: 88.8889 (90.2439)  time: 0.3422 (0.0222 -- 2.0424)  data: 0.1859 (0.0001 -- 1.8576)  max mem: 16413
Val: Total time: 0:00:03 (0.3423 s / it)
* Acc@1 62.805 Acc@5 92.073 loss 1.426
Accuracy of the network on the 163 val images: 62.80%
Max accuracy: 62.80%
Epoch: [21]  [ 0/54]  eta: 0:06:43  lr: 0.000046  min_lr: 0.000001  loss: 1.8224 (1.8224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3204 (6.3204)  time: 7.4672 (7.4672 -- 7.4672)  data: 6.5757 (6.5757 -- 6.5757)  max mem: 16413
Epoch: [21]  [20/54]  eta: 0:00:41  lr: 0.000046  min_lr: 0.000001  loss: 2.1633 (2.0879)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9068 (6.4469)  time: 0.9236 (0.5190 -- 4.1788)  data: 0.3351 (0.0002 -- 3.6573)  max mem: 16413
Epoch: [21]  [40/54]  eta: 0:00:15  lr: 0.000046  min_lr: 0.000001  loss: 2.1002 (2.0937)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6266 (6.2679)  time: 0.9531 (0.5155 -- 4.4745)  data: 0.3997 (0.0002 -- 3.9636)  max mem: 16413
[2023-10-23 20:24:46,444] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1182
[2023-10-23 20:24:46,444] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 20:24:46,444] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1182
[2023-10-23 20:24:46,444] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 20:24:46,444] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [21]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1184 (2.1235)  loss_scale: 16384.0000 (15473.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8295 (6.2882)  time: 0.7272 (0.4816 -- 4.4745)  data: 0.2178 (0.0001 -- 3.9636)  max mem: 16413
Epoch: [21] Total time: 0:00:51 (0.9602 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1184 (2.0840)  loss_scale: 16384.0000 (15473.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8295 (6.2882)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.4696 (1.4696)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9549 (1.9549 -- 1.9549)  data: 1.7472 (1.7472 -- 1.7472)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.2117 (1.3103)  acc1: 66.6667 (65.8537)  acc5: 88.8889 (90.2439)  time: 0.3327 (0.0227 -- 1.9549)  data: 0.1748 (0.0001 -- 1.7472)  max mem: 16413
Val: Total time: 0:00:03 (0.3328 s / it)
* Acc@1 63.415 Acc@5 92.683 loss 1.397
Accuracy of the network on the 163 val images: 63.41%
[2023-10-23 20:24:52,778] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:24:52,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:24:52,780] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:24:52,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:24:54,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:24:54,321] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.41%
Epoch: [22]  [ 0/54]  eta: 0:06:21  lr: 0.000046  min_lr: 0.000001  loss: 1.9835 (1.9835)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3216 (6.3216)  time: 7.0593 (7.0593 -- 7.0593)  data: 6.4919 (6.4919 -- 6.4919)  max mem: 16413
Epoch: [22]  [20/54]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000001  loss: 2.0570 (2.0354)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9853 (6.2750)  time: 0.8837 (0.5119 -- 4.6797)  data: 0.3457 (0.0004 -- 4.1622)  max mem: 16413
Epoch: [22]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.0872 (2.0599)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8512 (5.8000)  time: 0.9515 (0.5122 -- 4.3464)  data: 0.4099 (0.0003 -- 3.8135)  max mem: 16413
Epoch: [22]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0774 (2.0796)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7459 (5.6928)  time: 0.6653 (0.4952 -- 2.0476)  data: 0.1184 (0.0001 -- 1.3746)  max mem: 16413
Epoch: [22] Total time: 0:00:51 (0.9607 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0774 (2.0524)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7459 (5.6928)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.4701 (1.4701)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.0543 (2.0543 -- 2.0543)  data: 1.8645 (1.8645 -- 1.8645)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.2558 (1.3011)  acc1: 66.6667 (64.6341)  acc5: 88.8889 (90.2439)  time: 0.3430 (0.0220 -- 2.0543)  data: 0.1865 (0.0001 -- 1.8645)  max mem: 16413
Val: Total time: 0:00:03 (0.3431 s / it)
* Acc@1 62.805 Acc@5 92.683 loss 1.416
Accuracy of the network on the 163 val images: 62.80%
Max accuracy: 63.41%
Epoch: [23]  [ 0/54]  eta: 0:06:13  lr: 0.000046  min_lr: 0.000001  loss: 2.2678 (2.2678)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7061 (4.7061)  time: 6.9218 (6.9218 -- 6.9218)  data: 6.3867 (6.3867 -- 6.3867)  max mem: 16413
Epoch: [23]  [20/54]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.1055 (2.0556)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4558 (5.6264)  time: 0.8774 (0.5197 -- 2.4472)  data: 0.1552 (0.0002 -- 1.9197)  max mem: 16413
Epoch: [23]  [40/54]  eta: 0:00:15  lr: 0.000046  min_lr: 0.000001  loss: 2.0256 (2.0895)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6821 (5.6357)  time: 0.9762 (0.5193 -- 4.7075)  data: 0.4366 (0.0002 -- 4.1708)  max mem: 16413
Epoch: [23]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1438 (2.1039)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4468 (5.6059)  time: 0.6461 (0.4946 -- 2.8373)  data: 0.1171 (0.0001 -- 2.3310)  max mem: 16413
Epoch: [23] Total time: 0:00:50 (0.9416 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1438 (2.0859)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4468 (5.6059)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.4783 (1.4783)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.9530 (1.9530 -- 1.9530)  data: 1.7510 (1.7510 -- 1.7510)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.2619 (1.2806)  acc1: 66.6667 (64.6341)  acc5: 88.8889 (89.0244)  time: 0.3334 (0.0221 -- 1.9530)  data: 0.1752 (0.0001 -- 1.7510)  max mem: 16413
Val: Total time: 0:00:03 (0.3335 s / it)
* Acc@1 62.805 Acc@5 92.073 loss 1.394
Accuracy of the network on the 163 val images: 62.80%
Max accuracy: 63.41%
Epoch: [24]  [ 0/54]  eta: 0:06:53  lr: 0.000046  min_lr: 0.000001  loss: 1.9088 (1.9088)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9171 (7.9171)  time: 7.6500 (7.6500 -- 7.6500)  data: 6.9190 (6.9190 -- 6.9190)  max mem: 16413
[2023-10-23 20:27:03,186] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:27:03,186] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 20:27:03,186] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:27:03,187] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [24]  [20/54]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000001  loss: 1.9919 (1.9543)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6827 (6.6241)  time: 0.8532 (0.5235 -- 3.4469)  data: 0.0801 (0.0008 -- 0.7355)  max mem: 16413
Epoch: [24]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.0414 (1.9824)  loss_scale: 16384.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1824 (6.2426)  time: 0.8149 (0.5269 -- 1.6943)  data: 0.1384 (0.0003 -- 0.9393)  max mem: 16413
Epoch: [24]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0323 (1.9990)  loss_scale: 16384.0000 (14108.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3548 (6.1553)  time: 0.7002 (0.4943 -- 2.5850)  data: 0.0562 (0.0002 -- 0.5941)  max mem: 16413
Epoch: [24] Total time: 0:00:50 (0.9319 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0323 (2.0053)  loss_scale: 16384.0000 (14108.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3548 (6.1553)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.4010 (1.4010)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0912 (2.0912 -- 2.0912)  data: 1.9226 (1.9226 -- 1.9226)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1873 (1.2367)  acc1: 66.6667 (68.2927)  acc5: 88.8889 (90.2439)  time: 0.3471 (0.0222 -- 2.0912)  data: 0.1924 (0.0001 -- 1.9226)  max mem: 16413
Val: Total time: 0:00:03 (0.3472 s / it)
* Acc@1 67.683 Acc@5 92.073 loss 1.344
Accuracy of the network on the 163 val images: 67.68%
[2023-10-23 20:27:37,859] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:27:37,861] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:27:37,861] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:27:37,861] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:27:39,393] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:27:39,393] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.68%
Epoch: [25]  [ 0/54]  eta: 0:04:59  lr: 0.000046  min_lr: 0.000001  loss: 1.9504 (1.9504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9364 (4.9364)  time: 5.5492 (5.5492 -- 5.5492)  data: 4.9923 (4.9923 -- 4.9923)  max mem: 16413
Epoch: [25]  [20/54]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000001  loss: 2.1584 (2.0724)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9566 (5.9886)  time: 0.9886 (0.5362 -- 3.3763)  data: 0.3981 (0.0003 -- 2.8650)  max mem: 16413
Epoch: [25]  [40/54]  eta: 0:00:14  lr: 0.000046  min_lr: 0.000001  loss: 2.0216 (2.0902)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5689 (6.4619)  time: 0.8175 (0.5147 -- 2.2040)  data: 0.0651 (0.0003 -- 0.9486)  max mem: 16413
Epoch: [25]  [53/54]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0216 (2.0583)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5182 (6.2337)  time: 0.7401 (0.4967 -- 2.9464)  data: 0.0171 (0.0002 -- 0.3277)  max mem: 16413
Epoch: [25] Total time: 0:00:50 (0.9393 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0216 (2.0230)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5182 (6.2337)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.3786 (1.3786)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.0100 (2.0100 -- 2.0100)  data: 1.8154 (1.8154 -- 1.8154)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1839 (1.2068)  acc1: 66.6667 (67.0732)  acc5: 88.8889 (90.2439)  time: 0.3386 (0.0223 -- 2.0100)  data: 0.1816 (0.0001 -- 1.8154)  max mem: 16413
Val: Total time: 0:00:03 (0.3387 s / it)
* Acc@1 67.683 Acc@5 92.683 loss 1.312
Accuracy of the network on the 163 val images: 67.68%
Max accuracy: 67.68%
Epoch: [26]  [ 0/54]  eta: 0:08:09  lr: 0.000046  min_lr: 0.000001  loss: 2.3131 (2.3131)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8263 (3.8263)  time: 9.0720 (9.0720 -- 9.0720)  data: 8.5550 (8.5550 -- 8.5550)  max mem: 16413
Epoch: [26]  [20/54]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000001  loss: 1.9125 (2.0248)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4889 (6.4024)  time: 0.8006 (0.5193 -- 2.4937)  data: 0.1759 (0.0004 -- 1.1606)  max mem: 16413
[2023-10-23 20:29:13,462] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:29:13,463] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:29:13,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:29:13,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 2.1014 (2.0334)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3028 (6.6499)  time: 0.8769 (0.5186 -- 4.0392)  data: 0.0021 (0.0004 -- 0.0127)  max mem: 16413
Epoch: [26]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.1874 (2.0673)  loss_scale: 32768.0000 (22148.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3147 (6.7420)  time: 0.7745 (0.4950 -- 3.5828)  data: 0.0007 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [26] Total time: 0:00:51 (0.9496 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.1874 (2.0526)  loss_scale: 32768.0000 (22148.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3147 (6.7420)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.4024 (1.4024)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.0235 (2.0235 -- 2.0235)  data: 1.8337 (1.8337 -- 1.8337)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1300 (1.1851)  acc1: 66.6667 (68.2927)  acc5: 88.8889 (90.2439)  time: 0.3409 (0.0222 -- 2.0235)  data: 0.1835 (0.0001 -- 1.8337)  max mem: 16413
Val: Total time: 0:00:03 (0.3410 s / it)
* Acc@1 67.073 Acc@5 92.683 loss 1.313
Accuracy of the network on the 163 val images: 67.07%
Max accuracy: 67.68%
Epoch: [27]  [ 0/54]  eta: 0:07:25  lr: 0.000045  min_lr: 0.000001  loss: 1.3274 (1.3274)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5439 (5.5439)  time: 8.2453 (8.2453 -- 8.2453)  data: 5.6420 (5.6420 -- 5.6420)  max mem: 16413
[2023-10-23 20:29:38,728] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1462
[2023-10-23 20:29:38,728] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:29:38,728] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 20:29:38,729] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1462
[2023-10-23 20:29:38,729] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [27]  [20/54]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000001  loss: 1.9540 (1.9124)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8481 (6.1115)  time: 0.7623 (0.5236 -- 2.3798)  data: 0.0584 (0.0004 -- 1.1367)  max mem: 16413
Epoch: [27]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 1.6802 (1.8629)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0444 (6.0729)  time: 0.9678 (0.5241 -- 2.7239)  data: 0.4079 (0.0003 -- 2.2055)  max mem: 16413
Epoch: [27]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9737 (1.9117)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1616 (6.3146)  time: 0.6797 (0.4965 -- 1.9438)  data: 0.1424 (0.0002 -- 1.4001)  max mem: 16413
Epoch: [27] Total time: 0:00:49 (0.9223 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9737 (1.9585)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1616 (6.3146)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.3324 (1.3324)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0480 (2.0480 -- 2.0480)  data: 1.8598 (1.8598 -- 1.8598)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1893 (1.1459)  acc1: 77.7778 (70.7317)  acc5: 100.0000 (92.6829)  time: 0.3424 (0.0223 -- 2.0480)  data: 0.1861 (0.0001 -- 1.8598)  max mem: 16413
Val: Total time: 0:00:03 (0.3425 s / it)
* Acc@1 69.512 Acc@5 93.902 loss 1.287
Accuracy of the network on the 163 val images: 69.51%
[2023-10-23 20:30:21,496] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:30:21,498] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:30:21,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:30:21,498] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:30:22,907] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:30:22,908] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.51%
Epoch: [28]  [ 0/54]  eta: 0:09:11  lr: 0.000045  min_lr: 0.000001  loss: 2.0900 (2.0900)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4554 (7.4554)  time: 10.2057 (10.2057 -- 10.2057)  data: 9.6523 (9.6523 -- 9.6523)  max mem: 16413
Epoch: [28]  [20/54]  eta: 0:00:39  lr: 0.000045  min_lr: 0.000001  loss: 2.0005 (2.0009)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7557 (6.4587)  time: 0.7125 (0.5261 -- 2.7583)  data: 0.1578 (0.0005 -- 2.2289)  max mem: 16413
Epoch: [28]  [40/54]  eta: 0:00:14  lr: 0.000045  min_lr: 0.000001  loss: 2.0818 (1.9927)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9589 (6.2401)  time: 0.9262 (0.5263 -- 3.4118)  data: 0.3591 (0.0005 -- 2.8722)  max mem: 16413
Epoch: [28]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9111 (1.9805)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4036 (6.3413)  time: 0.6780 (0.4942 -- 2.1973)  data: 0.0847 (0.0001 -- 1.6804)  max mem: 16413
Epoch: [28] Total time: 0:00:50 (0.9374 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9111 (1.9650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4036 (6.3413)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.3235 (1.3235)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9825 (1.9825 -- 1.9825)  data: 1.7824 (1.7824 -- 1.7824)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1567 (1.1307)  acc1: 77.7778 (69.5122)  acc5: 100.0000 (92.6829)  time: 0.3358 (0.0222 -- 1.9825)  data: 0.1783 (0.0001 -- 1.7824)  max mem: 16413
Val: Total time: 0:00:03 (0.3359 s / it)
* Acc@1 68.293 Acc@5 93.902 loss 1.277
Accuracy of the network on the 163 val images: 68.29%
Max accuracy: 69.51%
Epoch: [29]  [ 0/54]  eta: 0:08:08  lr: 0.000045  min_lr: 0.000001  loss: 1.8980 (1.8980)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7846 (4.7846)  time: 9.0496 (9.0496 -- 9.0496)  data: 6.2556 (6.2556 -- 6.2556)  max mem: 16413
Epoch: [29]  [20/54]  eta: 0:00:40  lr: 0.000045  min_lr: 0.000001  loss: 1.7064 (1.8250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6507 (5.6228)  time: 0.7961 (0.5236 -- 2.4107)  data: 0.0015 (0.0004 -- 0.0026)  max mem: 16413
[2023-10-23 20:31:47,787] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:31:47,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:31:47,789] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:31:47,789] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [40/54]  eta: 0:00:15  lr: 0.000045  min_lr: 0.000001  loss: 2.0604 (1.9548)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6466 (5.7680)  time: 1.0581 (0.5154 -- 4.3651)  data: 0.0011 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [29]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9983 (1.9597)  loss_scale: 32768.0000 (25182.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5105 (6.0432)  time: 0.7119 (0.4947 -- 4.3651)  data: 0.0006 (0.0001 -- 0.0018)  max mem: 16413
Epoch: [29] Total time: 0:00:52 (0.9779 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9983 (2.0004)  loss_scale: 32768.0000 (25182.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5105 (6.0432)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.3063 (1.3063)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0358 (2.0358 -- 2.0358)  data: 1.8375 (1.8375 -- 1.8375)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1904 (1.1208)  acc1: 66.6667 (67.0732)  acc5: 100.0000 (92.6829)  time: 0.3416 (0.0223 -- 2.0358)  data: 0.1838 (0.0001 -- 1.8375)  max mem: 16413
Val: Total time: 0:00:03 (0.3418 s / it)
* Acc@1 67.683 Acc@5 93.902 loss 1.254
Accuracy of the network on the 163 val images: 67.68%
Max accuracy: 69.51%
Epoch: [30]  [ 0/54]  eta: 0:05:57  lr: 0.000045  min_lr: 0.000001  loss: 1.8615 (1.8615)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3357 (4.3357)  time: 6.6253 (6.6253 -- 6.6253)  data: 4.7263 (4.7263 -- 4.7263)  max mem: 16413
Epoch: [30]  [20/54]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000001  loss: 1.9667 (1.9504)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4725 (6.3396)  time: 0.8602 (0.5122 -- 2.8108)  data: 0.2858 (0.0009 -- 2.2980)  max mem: 16413
Epoch: [30]  [40/54]  eta: 0:00:13  lr: 0.000045  min_lr: 0.000001  loss: 1.6755 (1.8506)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9361 (6.1646)  time: 0.8293 (0.5246 -- 4.1514)  data: 0.2731 (0.0004 -- 3.6256)  max mem: 16413
Epoch: [30]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.8985 (1.8823)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1030 (6.3270)  time: 0.6499 (0.4947 -- 3.1342)  data: 0.1318 (0.0002 -- 2.6001)  max mem: 16413
Epoch: [30] Total time: 0:00:49 (0.9191 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.8985 (1.9272)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1030 (6.3270)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.3174 (1.3174)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 1.9398 (1.9398 -- 1.9398)  data: 1.7292 (1.7292 -- 1.7292)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1962 (1.0977)  acc1: 66.6667 (68.2927)  acc5: 100.0000 (91.4634)  time: 0.3317 (0.0222 -- 1.9398)  data: 0.1730 (0.0001 -- 1.7292)  max mem: 16413
Val: Total time: 0:00:03 (0.3318 s / it)
* Acc@1 68.902 Acc@5 93.293 loss 1.239
Accuracy of the network on the 163 val images: 68.90%
Max accuracy: 69.51%
Epoch: [31]  [ 0/54]  eta: 0:05:21  lr: 0.000045  min_lr: 0.000001  loss: 1.5835 (1.5835)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6819 (5.6819)  time: 5.9608 (5.9608 -- 5.9608)  data: 5.4238 (5.4238 -- 5.4238)  max mem: 16413
Epoch: [31]  [20/54]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000001  loss: 2.0166 (1.9592)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3918 (5.8492)  time: 0.8834 (0.5215 -- 3.1705)  data: 0.2742 (0.0003 -- 2.0218)  max mem: 16413
[2023-10-23 20:33:30,578] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1695
[2023-10-23 20:33:30,578] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1695
[2023-10-23 20:33:30,579] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:33:30,579] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:33:30,579] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [40/54]  eta: 0:00:13  lr: 0.000045  min_lr: 0.000001  loss: 1.9202 (1.9790)  loss_scale: 16384.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5984 (6.2242)  time: 0.8477 (0.5329 -- 3.8704)  data: 0.2986 (0.0004 -- 3.3478)  max mem: 16413
Epoch: [31]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0013 (1.9958)  loss_scale: 16384.0000 (22755.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5926 (6.4429)  time: 0.7276 (0.4924 -- 3.3712)  data: 0.2096 (0.0002 -- 2.8391)  max mem: 16413
Epoch: [31] Total time: 0:00:51 (0.9509 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0013 (1.9962)  loss_scale: 16384.0000 (22755.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5926 (6.4429)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.2877 (1.2877)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0183 (2.0183 -- 2.0183)  data: 1.7954 (1.7954 -- 1.7954)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0746 (1.0557)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3408 (0.0221 -- 2.0183)  data: 0.1800 (0.0001 -- 1.7954)  max mem: 16413
Val: Total time: 0:00:03 (0.3410 s / it)
* Acc@1 72.561 Acc@5 93.902 loss 1.191
Accuracy of the network on the 163 val images: 72.56%
[2023-10-23 20:34:01,131] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:34:01,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:34:01,133] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:34:01,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:34:02,658] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:34:02,659] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 72.56%
Epoch: [32]  [ 0/54]  eta: 0:05:43  lr: 0.000045  min_lr: 0.000001  loss: 2.4691 (2.4691)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4173 (4.4173)  time: 6.3645 (6.3645 -- 6.3645)  data: 5.1282 (5.1282 -- 5.1282)  max mem: 16413
Epoch: [32]  [20/54]  eta: 0:00:39  lr: 0.000045  min_lr: 0.000001  loss: 1.9523 (1.9940)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2028 (6.4979)  time: 0.9112 (0.5093 -- 4.9091)  data: 0.3661 (0.0005 -- 4.3708)  max mem: 16413
Epoch: [32]  [40/54]  eta: 0:00:13  lr: 0.000045  min_lr: 0.000001  loss: 1.9608 (2.0034)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4386 (6.5046)  time: 0.7955 (0.5367 -- 2.6753)  data: 0.1954 (0.0002 -- 2.1444)  max mem: 16413
Epoch: [32]  [53/54]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0613 (2.0060)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5468 (6.5845)  time: 0.6569 (0.4946 -- 2.4103)  data: 0.0951 (0.0002 -- 1.8880)  max mem: 16413
Epoch: [32] Total time: 0:00:49 (0.9077 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0613 (1.9463)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5468 (6.5845)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.2880 (1.2880)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9485 (1.9485 -- 1.9485)  data: 1.7325 (1.7325 -- 1.7325)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0853 (1.0586)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3325 (0.0224 -- 1.9485)  data: 0.1733 (0.0001 -- 1.7325)  max mem: 16413
Val: Total time: 0:00:03 (0.3326 s / it)
* Acc@1 72.561 Acc@5 94.512 loss 1.202
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 72.56%
Epoch: [33]  [ 0/54]  eta: 0:06:51  lr: 0.000045  min_lr: 0.000001  loss: 1.7823 (1.7823)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9877 (4.9877)  time: 7.6190 (7.6190 -- 7.6190)  data: 7.0840 (7.0840 -- 7.0840)  max mem: 16413
Epoch: [33]  [20/54]  eta: 0:00:41  lr: 0.000044  min_lr: 0.000001  loss: 1.9764 (1.9710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4311 (7.0805)  time: 0.8857 (0.5217 -- 2.9463)  data: 0.2158 (0.0005 -- 1.9337)  max mem: 16413
Epoch: [33]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 2.0371 (1.9852)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4875 (7.0316)  time: 0.8316 (0.5364 -- 2.9638)  data: 0.1600 (0.0003 -- 2.4332)  max mem: 16413
[2023-10-23 20:35:40,372] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:35:40,372] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:35:40,373] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:35:40,373] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.0908 (2.0013)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8755 (6.8704)  time: 0.7364 (0.4933 -- 2.5978)  data: 0.1532 (0.0002 -- 2.0918)  max mem: 16413
Epoch: [33] Total time: 0:00:51 (0.9586 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.0908 (1.9752)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8755 (6.8704)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.2903 (1.2903)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0331 (2.0331 -- 2.0331)  data: 1.8457 (1.8457 -- 1.8457)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0716 (1.0780)  acc1: 77.7778 (71.9512)  acc5: 88.8889 (92.6829)  time: 0.3414 (0.0222 -- 2.0331)  data: 0.1847 (0.0001 -- 1.8457)  max mem: 16413
Val: Total time: 0:00:03 (0.3415 s / it)
* Acc@1 71.951 Acc@5 93.902 loss 1.232
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 72.56%
Epoch: [34]  [ 0/54]  eta: 0:04:32  lr: 0.000044  min_lr: 0.000001  loss: 2.2251 (2.2251)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5064 (5.5064)  time: 5.0545 (5.0545 -- 5.0545)  data: 4.5346 (4.5346 -- 4.5346)  max mem: 16413
Epoch: [34]  [20/54]  eta: 0:00:42  lr: 0.000044  min_lr: 0.000001  loss: 1.9041 (1.8833)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1383 (6.9100)  time: 1.0726 (0.5284 -- 3.9126)  data: 0.4306 (0.0005 -- 3.3752)  max mem: 16413
Epoch: [34]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 1.9711 (1.8952)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8274 (6.7789)  time: 0.7663 (0.5343 -- 3.3454)  data: 0.2096 (0.0003 -- 2.8230)  max mem: 16413
[2023-10-23 20:36:38,552] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1884
[2023-10-23 20:36:38,552] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1884
[2023-10-23 20:36:38,552] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:36:38,552] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:36:38,552] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.0418 (1.9027)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2976 (6.7597)  time: 0.7071 (0.4849 -- 2.8908)  data: 0.1866 (0.0001 -- 2.3780)  max mem: 16413
Epoch: [34] Total time: 0:00:50 (0.9401 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.0418 (1.8679)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2976 (6.7597)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.2074 (1.2074)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0523 (2.0523 -- 2.0523)  data: 1.8671 (1.8671 -- 1.8671)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0964 (1.0354)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3433 (0.0221 -- 2.0523)  data: 0.1868 (0.0001 -- 1.8671)  max mem: 16413
Val: Total time: 0:00:03 (0.3434 s / it)
* Acc@1 74.390 Acc@5 94.512 loss 1.170
Accuracy of the network on the 163 val images: 74.39%
[2023-10-23 20:36:44,485] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:36:44,487] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:36:44,487] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:36:44,487] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:36:46,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:36:46,013] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.39%
Epoch: [35]  [ 0/54]  eta: 0:05:29  lr: 0.000044  min_lr: 0.000001  loss: 1.1467 (1.1467)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0160 (8.0160)  time: 6.0984 (6.0984 -- 6.0984)  data: 4.1568 (4.1568 -- 4.1568)  max mem: 16413
Epoch: [35]  [20/54]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000001  loss: 1.9269 (1.8963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3746 (6.7073)  time: 0.8967 (0.5315 -- 2.6358)  data: 0.1540 (0.0007 -- 1.3190)  max mem: 16413
Epoch: [35]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 1.8046 (1.8511)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8645 (6.3823)  time: 0.9807 (0.5119 -- 4.7608)  data: 0.0199 (0.0003 -- 0.3776)  max mem: 16413
Epoch: [35]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.7546 (1.8312)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7998 (6.5098)  time: 0.6255 (0.4946 -- 2.7836)  data: 0.0005 (0.0001 -- 0.0013)  max mem: 16413
Epoch: [35] Total time: 0:00:52 (0.9711 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.7546 (1.8890)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7998 (6.5098)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.1658 (1.1658)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9780 (1.9780 -- 1.9780)  data: 1.7617 (1.7617 -- 1.7617)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0588 (1.0215)  acc1: 66.6667 (69.5122)  acc5: 100.0000 (92.6829)  time: 0.3353 (0.0222 -- 1.9780)  data: 0.1763 (0.0001 -- 1.7617)  max mem: 16413
Val: Total time: 0:00:03 (0.3354 s / it)
* Acc@1 71.951 Acc@5 93.902 loss 1.148
Accuracy of the network on the 163 val images: 71.95%
Max accuracy: 74.39%
Epoch: [36]  [ 0/54]  eta: 0:06:11  lr: 0.000044  min_lr: 0.000001  loss: 2.4136 (2.4136)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5283 (7.5283)  time: 6.8762 (6.8762 -- 6.8762)  data: 6.3609 (6.3609 -- 6.3609)  max mem: 16413
Epoch: [36]  [20/54]  eta: 0:00:41  lr: 0.000044  min_lr: 0.000001  loss: 1.9204 (1.8939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1670 (6.9735)  time: 0.9278 (0.5257 -- 4.1238)  data: 0.3273 (0.0005 -- 3.6037)  max mem: 16413
Epoch: [36]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 1.9609 (1.9048)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0755 (7.3477)  time: 0.8440 (0.5312 -- 2.3601)  data: 0.2372 (0.0004 -- 1.8096)  max mem: 16413
Epoch: [36]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8796 (1.8880)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3806 (7.1001)  time: 0.6749 (0.4952 -- 2.1100)  data: 0.0798 (0.0001 -- 1.5813)  max mem: 16413
Epoch: [36] Total time: 0:00:50 (0.9336 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.8796 (1.9106)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3806 (7.1001)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.1865 (1.1865)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9818 (1.9818 -- 1.9818)  data: 1.7909 (1.7909 -- 1.7909)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.1168 (1.0170)  acc1: 66.6667 (70.7317)  acc5: 100.0000 (93.9024)  time: 0.3356 (0.0224 -- 1.9818)  data: 0.1792 (0.0001 -- 1.7909)  max mem: 16413
Val: Total time: 0:00:03 (0.3357 s / it)
* Acc@1 73.171 Acc@5 94.512 loss 1.149
Accuracy of the network on the 163 val images: 73.17%
Max accuracy: 74.39%
Epoch: [37]  [ 0/54]  eta: 0:07:33  lr: 0.000044  min_lr: 0.000001  loss: 1.9043 (1.9043)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3597 (8.3597)  time: 8.3907 (8.3907 -- 8.3907)  data: 6.1111 (6.1111 -- 6.1111)  max mem: 16413
[2023-10-23 20:38:44,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[1.041240759062658e-06, 1.041240759062658e-06, 1.388321012083544e-06, 1.388321012083544e-06, 1.8510946827780586e-06, 1.8510946827780586e-06, 2.468126243704078e-06, 2.468126243704078e-06, 3.2908349916054374e-06, 3.2908349916054374e-06, 4.38777998880725e-06, 4.38777998880725e-06, 5.8503733184096665e-06, 5.8503733184096665e-06, 7.800497757879556e-06, 7.800497757879556e-06, 1.0400663677172742e-05, 1.0400663677172742e-05, 1.3867551569563655e-05, 1.3867551569563655e-05, 1.8490068759418207e-05, 1.8490068759418207e-05, 2.4653425012557608e-05, 2.4653425012557608e-05, 3.2871233350076815e-05, 3.2871233350076815e-05, 4.382831113343575e-05, 4.382831113343575e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 20:38:44,779] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=18.94054437438851, CurrSamplesPerSec=20.595826382009175, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-10-23 20:38:55,562] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:38:55,562] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:38:55,562] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:38:55,562] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [20/54]  eta: 0:00:43  lr: 0.000044  min_lr: 0.000001  loss: 2.0242 (1.9535)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5470 (6.4563)  time: 0.9332 (0.5113 -- 5.1553)  data: 0.0780 (0.0003 -- 0.9852)  max mem: 16413
[2023-10-23 20:39:16,933] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2036
[2023-10-23 20:39:16,933] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2036
[2023-10-23 20:39:16,934] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:39:16,934] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:39:16,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [37]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 1.9347 (1.9528)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6923 (6.8129)  time: 0.8246 (0.5131 -- 3.3135)  data: 0.0012 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [37]  [53/54]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8471 (1.9357)  loss_scale: 16384.0000 (23362.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6950 (6.8864)  time: 0.6758 (0.4945 -- 1.8539)  data: 0.0007 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [37] Total time: 0:00:51 (0.9503 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.8471 (1.9006)  loss_scale: 16384.0000 (23362.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6950 (6.8864)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.1518 (1.1518)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0415 (2.0415 -- 2.0415)  data: 1.8481 (1.8481 -- 1.8481)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0569 (1.0121)  acc1: 77.7778 (69.5122)  acc5: 100.0000 (93.9024)  time: 0.3418 (0.0221 -- 2.0415)  data: 0.1849 (0.0001 -- 1.8481)  max mem: 16413
Val: Total time: 0:00:03 (0.3419 s / it)
* Acc@1 72.561 Acc@5 93.902 loss 1.137
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 74.39%
Epoch: [38]  [ 0/54]  eta: 0:08:04  lr: 0.000044  min_lr: 0.000001  loss: 2.4471 (2.4471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4204 (5.4204)  time: 8.9664 (8.9664 -- 8.9664)  data: 5.2998 (5.2998 -- 5.2998)  max mem: 16413
Epoch: [38]  [20/54]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000001  loss: 1.9031 (1.8792)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8226 (6.7416)  time: 0.7344 (0.5195 -- 2.7181)  data: 0.1185 (0.0001 -- 1.7464)  max mem: 16413
Epoch: [38]  [40/54]  eta: 0:00:14  lr: 0.000044  min_lr: 0.000001  loss: 1.9281 (1.8979)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1935 (6.8719)  time: 0.8924 (0.5263 -- 3.5057)  data: 0.2629 (0.0004 -- 2.9663)  max mem: 16413
Epoch: [38]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.0859 (1.9226)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9227 (6.9415)  time: 0.6278 (0.4970 -- 1.6017)  data: 0.0062 (0.0001 -- 0.1024)  max mem: 16413
Epoch: [38] Total time: 0:00:49 (0.9101 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.0859 (1.9146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9227 (6.9415)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.1475 (1.1475)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0563 (2.0563 -- 2.0563)  data: 1.8582 (1.8582 -- 1.8582)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0382 (0.9794)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3440 (0.0222 -- 2.0563)  data: 0.1859 (0.0001 -- 1.8582)  max mem: 16413
Val: Total time: 0:00:03 (0.3442 s / it)
* Acc@1 74.390 Acc@5 94.512 loss 1.113
Accuracy of the network on the 163 val images: 74.39%
[2023-10-23 20:40:23,185] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:40:23,187] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:40:23,187] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:40:23,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:40:24,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:40:24,423] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.39%
Epoch: [39]  [ 0/54]  eta: 0:06:43  lr: 0.000043  min_lr: 0.000001  loss: 2.4441 (2.4441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7507 (8.7507)  time: 7.4672 (7.4672 -- 7.4672)  data: 6.4887 (6.4887 -- 6.4887)  max mem: 16413
Epoch: [39]  [20/54]  eta: 0:00:39  lr: 0.000043  min_lr: 0.000001  loss: 1.7770 (1.9498)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1202 (6.8466)  time: 0.8578 (0.5184 -- 3.5104)  data: 0.1550 (0.0006 -- 1.7026)  max mem: 16413
Epoch: [39]  [40/54]  eta: 0:00:14  lr: 0.000043  min_lr: 0.000001  loss: 1.9706 (1.9568)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7406 (6.8691)  time: 0.9600 (0.5173 -- 5.0603)  data: 0.0009 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [39]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.0101 (1.9401)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5221 (6.8122)  time: 0.5478 (0.4943 -- 1.1456)  data: 0.0327 (0.0002 -- 0.6437)  max mem: 16413
Epoch: [39] Total time: 0:00:51 (0.9459 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.0101 (1.9510)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5221 (6.8122)
[2023-10-23 20:41:15,506] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-10-23 20:41:15,508] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-39/mp_rank_00_model_states.pt
[2023-10-23 20:41:15,508] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-39/mp_rank_00_model_states.pt...
[2023-10-23 20:41:15,508] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-10-23 20:41:16,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-39/mp_rank_00_model_states.pt.
[2023-10-23 20:41:16,416] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 1.0961 (1.0961)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0495 (2.0495 -- 2.0495)  data: 1.8469 (1.8469 -- 1.8469)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9715 (0.9682)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3422 (0.0223 -- 2.0495)  data: 0.1848 (0.0001 -- 1.8469)  max mem: 16413
Val: Total time: 0:00:03 (0.3423 s / it)
* Acc@1 73.780 Acc@5 93.902 loss 1.085
Accuracy of the network on the 163 val images: 73.78%
Max accuracy: 74.39%
Epoch: [40]  [ 0/54]  eta: 0:05:41  lr: 0.000043  min_lr: 0.000001  loss: 2.2759 (2.2759)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2717 (5.2717)  time: 6.3318 (6.3318 -- 6.3318)  data: 5.1178 (5.1178 -- 5.1178)  max mem: 16413
[2023-10-23 20:41:30,609] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:41:30,610] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:41:30,611] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:41:30,611] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [20/54]  eta: 0:00:38  lr: 0.000043  min_lr: 0.000001  loss: 1.9230 (1.9546)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4637 (6.6696)  time: 0.8756 (0.5225 -- 3.8211)  data: 0.0022 (0.0005 -- 0.0146)  max mem: 16413
Epoch: [40]  [40/54]  eta: 0:00:14  lr: 0.000043  min_lr: 0.000001  loss: 1.9504 (1.9469)  loss_scale: 32768.0000 (30769.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9712 (7.2461)  time: 0.9242 (0.5180 -- 2.8255)  data: 0.1893 (0.0004 -- 2.3023)  max mem: 16413
[2023-10-23 20:42:09,766] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2212
[2023-10-23 20:42:09,766] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2212
[2023-10-23 20:42:09,766] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:42:09,766] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:42:09,766] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [40]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9012 (1.9463)  loss_scale: 32768.0000 (30644.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9712 (7.2757)  time: 0.6254 (0.4799 -- 1.5195)  data: 0.0849 (0.0001 -- 1.0196)  max mem: 16413
Epoch: [40] Total time: 0:00:50 (0.9338 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9012 (1.9007)  loss_scale: 32768.0000 (30644.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9712 (7.2757)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.1000 (1.1000)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0161 (2.0161 -- 2.0161)  data: 1.8125 (1.8125 -- 1.8125)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9032 (0.9527)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (93.9024)  time: 0.3392 (0.0220 -- 2.0161)  data: 0.1813 (0.0001 -- 1.8125)  max mem: 16413
Val: Total time: 0:00:03 (0.3393 s / it)
* Acc@1 76.220 Acc@5 93.902 loss 1.087
Accuracy of the network on the 163 val images: 76.22%
[2023-10-23 20:42:13,660] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:42:13,662] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:42:13,662] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:42:13,662] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:42:14,811] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:42:14,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.22%
Epoch: [41]  [ 0/54]  eta: 0:08:38  lr: 0.000043  min_lr: 0.000001  loss: 2.4247 (2.4247)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9039 (5.9039)  time: 9.5986 (9.5986 -- 9.5986)  data: 9.0654 (9.0654 -- 9.0654)  max mem: 16413
Epoch: [41]  [20/54]  eta: 0:00:38  lr: 0.000043  min_lr: 0.000001  loss: 1.7884 (1.8498)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0045 (6.9940)  time: 0.6976 (0.5257 -- 1.5737)  data: 0.0912 (0.0004 -- 0.9413)  max mem: 16413
Epoch: [41]  [40/54]  eta: 0:00:13  lr: 0.000043  min_lr: 0.000001  loss: 1.8252 (1.8508)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9010 (6.8602)  time: 0.8222 (0.5208 -- 2.7304)  data: 0.1457 (0.0002 -- 2.1923)  max mem: 16413
Epoch: [41]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.8799 (1.8609)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8963 (7.2520)  time: 0.7070 (0.4966 -- 3.0116)  data: 0.0290 (0.0002 -- 0.5614)  max mem: 16413
Epoch: [41] Total time: 0:00:49 (0.9109 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.8799 (1.8658)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8963 (7.2520)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.0636 (1.0636)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9863 (1.9863 -- 1.9863)  data: 1.7938 (1.7938 -- 1.7938)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8382 (0.9394)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3367 (0.0222 -- 1.9863)  data: 0.1795 (0.0001 -- 1.7938)  max mem: 16413
Val: Total time: 0:00:03 (0.3368 s / it)
* Acc@1 73.780 Acc@5 93.902 loss 1.060
Accuracy of the network on the 163 val images: 73.78%
Max accuracy: 76.22%
Epoch: [42]  [ 0/54]  eta: 0:06:35  lr: 0.000043  min_lr: 0.000001  loss: 1.2204 (1.2204)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0691 (7.0691)  time: 7.3248 (7.3248 -- 7.3248)  data: 5.6300 (5.6300 -- 5.6300)  max mem: 16413
Epoch: [42]  [20/54]  eta: 0:00:38  lr: 0.000043  min_lr: 0.000001  loss: 1.8649 (1.8473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4836 (6.7862)  time: 0.8295 (0.5220 -- 2.6641)  data: 0.0827 (0.0009 -- 1.1929)  max mem: 16413
Epoch: [42]  [40/54]  eta: 0:00:13  lr: 0.000043  min_lr: 0.000001  loss: 1.9712 (1.8844)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4111 (6.9637)  time: 0.8396 (0.5194 -- 3.8915)  data: 0.1942 (0.0002 -- 2.0891)  max mem: 16413
Epoch: [42]  [53/54]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9937 (1.8978)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4111 (7.0667)  time: 0.6605 (0.4949 -- 2.5180)  data: 0.1040 (0.0002 -- 1.2272)  max mem: 16413
Epoch: [42] Total time: 0:00:50 (0.9276 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9937 (1.9030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4111 (7.0667)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.0924 (1.0924)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0034 (2.0034 -- 2.0034)  data: 1.8095 (1.8095 -- 1.8095)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0512 (0.9527)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3379 (0.0223 -- 2.0034)  data: 0.1810 (0.0001 -- 1.8095)  max mem: 16413
Val: Total time: 0:00:03 (0.3380 s / it)
* Acc@1 74.390 Acc@5 93.902 loss 1.082
Accuracy of the network on the 163 val images: 74.39%
Max accuracy: 76.22%
Epoch: [43]  [ 0/54]  eta: 0:07:59  lr: 0.000043  min_lr: 0.000001  loss: 2.1832 (2.1832)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0886 (8.0886)  time: 8.8792 (8.8792 -- 8.8792)  data: 8.3165 (8.3165 -- 8.3165)  max mem: 16413
[2023-10-23 20:44:26,034] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:44:26,035] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:44:26,035] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:44:26,036] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [43]  [20/54]  eta: 0:00:41  lr: 0.000043  min_lr: 0.000001  loss: 1.9586 (1.9194)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4892 (7.2926)  time: 0.8321 (0.5218 -- 3.8415)  data: 0.2775 (0.0003 -- 3.3132)  max mem: 16413
[2023-10-23 20:44:40,798] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2360
[2023-10-23 20:44:40,798] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2360
[2023-10-23 20:44:40,798] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:44:40,798] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:44:40,798] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [43]  [40/54]  eta: 0:00:14  lr: 0.000042  min_lr: 0.000001  loss: 1.7578 (1.8775)  loss_scale: 32768.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8691 (7.2928)  time: 0.7945 (0.5368 -- 3.1190)  data: 0.2384 (0.0005 -- 2.5863)  max mem: 16413
Epoch: [43]  [53/54]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.7354 (1.8646)  loss_scale: 16384.0000 (22148.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8691 (7.2721)  time: 0.6151 (0.4949 -- 1.6776)  data: 0.0296 (0.0001 -- 0.5777)  max mem: 16413
Epoch: [43] Total time: 0:00:49 (0.9121 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.7354 (1.8790)  loss_scale: 16384.0000 (22148.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8691 (7.2721)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.0911 (1.0911)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0100 (2.0100 -- 2.0100)  data: 1.8018 (1.8018 -- 1.8018)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0152 (0.9604)  acc1: 66.6667 (70.7317)  acc5: 88.8889 (92.6829)  time: 0.3390 (0.0222 -- 2.0100)  data: 0.1803 (0.0001 -- 1.8018)  max mem: 16413
Val: Total time: 0:00:03 (0.3391 s / it)
* Acc@1 72.561 Acc@5 93.902 loss 1.094
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 76.22%
Epoch: [44]  [ 0/54]  eta: 0:04:32  lr: 0.000042  min_lr: 0.000001  loss: 2.3120 (2.3120)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1796 (7.1796)  time: 5.0537 (5.0537 -- 5.0537)  data: 4.5285 (4.5285 -- 4.5285)  max mem: 16413
Epoch: [44]  [20/54]  eta: 0:00:38  lr: 0.000042  min_lr: 0.000001  loss: 1.9369 (1.9450)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8333 (7.0948)  time: 0.9468 (0.5306 -- 3.5190)  data: 0.2600 (0.0008 -- 2.7961)  max mem: 16413
Epoch: [44]  [40/54]  eta: 0:00:13  lr: 0.000042  min_lr: 0.000001  loss: 1.8106 (1.9234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4998 (7.0772)  time: 0.7871 (0.5187 -- 2.7592)  data: 0.0927 (0.0004 -- 1.1052)  max mem: 16413
Epoch: [44]  [53/54]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.7770 (1.9142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1503 (7.5966)  time: 0.7586 (0.4932 -- 3.8180)  data: 0.0775 (0.0001 -- 1.5377)  max mem: 16413
Epoch: [44] Total time: 0:00:50 (0.9366 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.7770 (1.9016)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1503 (7.5966)
Val:  [ 0/10]  eta: 0:00:20  loss: 1.0524 (1.0524)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0602 (2.0602 -- 2.0602)  data: 1.8844 (1.8844 -- 1.8844)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 1.0298 (0.9503)  acc1: 66.6667 (69.5122)  acc5: 88.8889 (92.6829)  time: 0.3439 (0.0221 -- 2.0602)  data: 0.1885 (0.0001 -- 1.8844)  max mem: 16413
Val: Total time: 0:00:03 (0.3440 s / it)
* Acc@1 72.561 Acc@5 93.293 loss 1.091
Accuracy of the network on the 163 val images: 72.56%
Max accuracy: 76.22%
Epoch: [45]  [ 0/54]  eta: 0:06:51  lr: 0.000042  min_lr: 0.000001  loss: 2.0314 (2.0314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0343 (9.0343)  time: 7.6170 (7.6170 -- 7.6170)  data: 7.0711 (7.0711 -- 7.0711)  max mem: 16413
Epoch: [45]  [20/54]  eta: 0:00:43  lr: 0.000042  min_lr: 0.000001  loss: 1.8415 (1.9133)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0403 (7.3322)  time: 0.9487 (0.5113 -- 3.7264)  data: 0.3020 (0.0004 -- 3.1906)  max mem: 16413
Epoch: [45]  [40/54]  eta: 0:00:14  lr: 0.000042  min_lr: 0.000001  loss: 2.0069 (1.8982)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1378 (7.1290)  time: 0.8200 (0.5296 -- 2.7642)  data: 0.1493 (0.0003 -- 2.2227)  max mem: 16413
Epoch: [45]  [53/54]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.8771 (1.8803)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2426 (7.1072)  time: 0.8394 (0.4940 -- 2.7642)  data: 0.1707 (0.0002 -- 2.2227)  max mem: 16413
Epoch: [45] Total time: 0:00:51 (0.9560 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.8771 (1.8722)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2426 (7.1072)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.0279 (1.0279)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9945 (1.9945 -- 1.9945)  data: 1.7732 (1.7732 -- 1.7732)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9787 (0.9435)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (93.9024)  time: 0.3377 (0.0221 -- 1.9945)  data: 0.1774 (0.0001 -- 1.7732)  max mem: 16413
Val: Total time: 0:00:03 (0.3379 s / it)
* Acc@1 75.000 Acc@5 93.902 loss 1.077
Accuracy of the network on the 163 val images: 75.00%
Max accuracy: 76.22%
Epoch: [46]  [ 0/54]  eta: 0:06:20  lr: 0.000042  min_lr: 0.000001  loss: 2.4423 (2.4423)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4279 (8.4279)  time: 7.0468 (7.0468 -- 7.0468)  data: 6.5075 (6.5075 -- 6.5075)  max mem: 16413
[2023-10-23 20:46:53,508] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:46:53,509] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:46:53,509] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:46:53,509] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [46]  [20/54]  eta: 0:00:41  lr: 0.000042  min_lr: 0.000001  loss: 1.8183 (1.8906)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3237 (7.1077)  time: 0.9207 (0.5160 -- 4.8846)  data: 0.3801 (0.0002 -- 4.3519)  max mem: 16413
Epoch: [46]  [40/54]  eta: 0:00:14  lr: 0.000042  min_lr: 0.000001  loss: 1.8623 (1.8754)  loss_scale: 32768.0000 (30769.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0362 (6.7137)  time: 0.9093 (0.5182 -- 3.3690)  data: 0.3673 (0.0003 -- 2.8332)  max mem: 16413
Epoch: [46]  [53/54]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.8413 (1.8665)  loss_scale: 32768.0000 (31250.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7420 (6.8334)  time: 0.6173 (0.4968 -- 2.4769)  data: 0.0989 (0.0001 -- 1.9626)  max mem: 16413
Epoch: [46] Total time: 0:00:50 (0.9309 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.8413 (1.8313)  loss_scale: 32768.0000 (31250.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7420 (6.8334)
Val:  [ 0/10]  eta: 0:00:19  loss: 1.0050 (1.0050)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9991 (1.9991 -- 1.9991)  data: 1.7800 (1.7800 -- 1.7800)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9446 (0.9054)  acc1: 77.7778 (73.1707)  acc5: 100.0000 (93.9024)  time: 0.3378 (0.0223 -- 1.9991)  data: 0.1781 (0.0001 -- 1.7800)  max mem: 16413
Val: Total time: 0:00:03 (0.3379 s / it)
* Acc@1 73.780 Acc@5 94.512 loss 1.038
Accuracy of the network on the 163 val images: 73.78%
Max accuracy: 76.22%
Epoch: [47]  [ 0/54]  eta: 0:08:30  lr: 0.000042  min_lr: 0.000001  loss: 1.5986 (1.5986)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3791 (8.3791)  time: 9.4567 (9.4567 -- 9.4567)  data: 4.6781 (4.6781 -- 4.6781)  max mem: 16413
Epoch: [47]  [20/54]  eta: 0:00:40  lr: 0.000042  min_lr: 0.000001  loss: 1.9454 (1.8757)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5346 (6.9697)  time: 0.7851 (0.5276 -- 3.2968)  data: 0.0028 (0.0004 -- 0.0168)  max mem: 16413
[2023-10-23 20:48:06,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2562
[2023-10-23 20:48:06,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2562
[2023-10-23 20:48:06,343] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:48:06,343] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:48:06,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [40/54]  eta: 0:00:15  lr: 0.000042  min_lr: 0.000001  loss: 2.0094 (1.9029)  loss_scale: 16384.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9326 (7.1727)  time: 0.9692 (0.5173 -- 4.8927)  data: 0.0020 (0.0003 -- 0.0087)  max mem: 16413
Epoch: [47]  [53/54]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.9012 (1.8813)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4443 (7.0016)  time: 0.6489 (0.4946 -- 2.3782)  data: 0.0010 (0.0002 -- 0.0086)  max mem: 16413
Epoch: [47] Total time: 0:00:51 (0.9609 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.9012 (1.8569)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4443 (7.0016)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9466 (0.9466)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9951 (1.9951 -- 1.9951)  data: 1.8049 (1.8049 -- 1.8049)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.9166 (0.8788)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (92.6829)  time: 0.3376 (0.0223 -- 1.9951)  data: 0.1806 (0.0001 -- 1.8049)  max mem: 16413
Val: Total time: 0:00:03 (0.3377 s / it)
* Acc@1 75.000 Acc@5 93.293 loss 1.006
Accuracy of the network on the 163 val images: 75.00%
Max accuracy: 76.22%
Epoch: [48]  [ 0/54]  eta: 0:06:16  lr: 0.000041  min_lr: 0.000001  loss: 1.9794 (1.9794)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1030 (8.1030)  time: 6.9770 (6.9770 -- 6.9770)  data: 6.4433 (6.4433 -- 6.4433)  max mem: 16413
Epoch: [48]  [20/54]  eta: 0:00:41  lr: 0.000041  min_lr: 0.000001  loss: 1.8987 (1.8430)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9494 (7.5572)  time: 0.9421 (0.5288 -- 2.4585)  data: 0.2998 (0.0004 -- 1.9151)  max mem: 16413
Epoch: [48]  [40/54]  eta: 0:00:14  lr: 0.000041  min_lr: 0.000001  loss: 1.9733 (1.9153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2124 (7.2697)  time: 0.8382 (0.5190 -- 1.9716)  data: 0.1737 (0.0002 -- 1.3716)  max mem: 16413
Epoch: [48]  [53/54]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.8936 (1.8929)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0453 (7.4215)  time: 0.6700 (0.4965 -- 1.9559)  data: 0.0646 (0.0001 -- 1.2733)  max mem: 16413
Epoch: [48] Total time: 0:00:50 (0.9429 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.8936 (1.8939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0453 (7.4215)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9727 (0.9727)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9842 (1.9842 -- 1.9842)  data: 1.7899 (1.7899 -- 1.7899)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8013 (0.8682)  acc1: 77.7778 (74.3902)  acc5: 100.0000 (93.9024)  time: 0.3360 (0.0224 -- 1.9842)  data: 0.1791 (0.0001 -- 1.7899)  max mem: 16413
Val: Total time: 0:00:03 (0.3361 s / it)
* Acc@1 75.000 Acc@5 94.512 loss 1.010
Accuracy of the network on the 163 val images: 75.00%
Max accuracy: 76.22%
Epoch: [49]  [ 0/54]  eta: 0:06:09  lr: 0.000041  min_lr: 0.000001  loss: 2.0493 (2.0493)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0040 (10.0040)  time: 6.8423 (6.8423 -- 6.8423)  data: 5.7059 (5.7059 -- 5.7059)  max mem: 16413
Epoch: [49]  [20/54]  eta: 0:00:40  lr: 0.000041  min_lr: 0.000001  loss: 1.8903 (1.9149)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2453 (7.8533)  time: 0.9137 (0.5251 -- 2.2277)  data: 0.2102 (0.0007 -- 1.6986)  max mem: 16413
Epoch: [49]  [40/54]  eta: 0:00:14  lr: 0.000041  min_lr: 0.000001  loss: 2.0171 (1.9173)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0474 (7.6664)  time: 0.9086 (0.5174 -- 2.7852)  data: 0.2237 (0.0002 -- 2.2755)  max mem: 16413
[2023-10-23 20:50:13,154] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:50:13,155] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:50:13,155] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:50:13,155] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [49]  [53/54]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 2.0327 (1.9374)  loss_scale: 16384.0000 (19114.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5311 (7.4907)  time: 0.6898 (0.4956 -- 2.6807)  data: 0.1482 (0.0001 -- 2.1735)  max mem: 16413
Epoch: [49] Total time: 0:00:51 (0.9460 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 2.0327 (1.9157)  loss_scale: 16384.0000 (19114.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5311 (7.4907)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9481 (0.9481)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0593 (2.0593 -- 2.0593)  data: 1.8745 (1.8745 -- 1.8745)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6681 (0.8506)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (92.6829)  time: 0.3466 (0.0222 -- 2.0593)  data: 0.1894 (0.0001 -- 1.8745)  max mem: 16413
Val: Total time: 0:00:03 (0.3468 s / it)
* Acc@1 76.829 Acc@5 93.293 loss 0.980
Accuracy of the network on the 163 val images: 76.83%
[2023-10-23 20:50:21,032] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:50:21,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:50:21,034] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:50:21,034] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:50:22,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:50:22,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.83%
Epoch: [50]  [ 0/54]  eta: 0:06:36  lr: 0.000041  min_lr: 0.000001  loss: 1.8405 (1.8405)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2187 (8.2187)  time: 7.3345 (7.3345 -- 7.3345)  data: 6.5664 (6.5664 -- 6.5664)  max mem: 16413
[2023-10-23 20:50:32,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2704
[2023-10-23 20:50:32,082] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:50:32,082] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 20:50:32,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2704
[2023-10-23 20:50:32,082] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [50]  [20/54]  eta: 0:00:42  lr: 0.000041  min_lr: 0.000001  loss: 1.8457 (1.8568)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6890 (9.1791)  time: 0.9422 (0.5107 -- 3.0827)  data: 0.1409 (0.0006 -- 2.5589)  max mem: 16413
Epoch: [50]  [40/54]  eta: 0:00:14  lr: 0.000041  min_lr: 0.000001  loss: 1.8080 (1.8227)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0708 (8.4821)  time: 0.7577 (0.5254 -- 2.2207)  data: 0.0034 (0.0004 -- 0.0411)  max mem: 16413
Epoch: [50]  [53/54]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.8070 (1.8410)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8933 (8.0673)  time: 0.7045 (0.4934 -- 3.0655)  data: 0.0029 (0.0001 -- 0.0411)  max mem: 16413
Epoch: [50] Total time: 0:00:51 (0.9554 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.8070 (1.8727)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8933 (8.0673)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9600 (0.9600)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0074 (2.0074 -- 2.0074)  data: 1.8176 (1.8176 -- 1.8176)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8734 (0.8688)  acc1: 77.7778 (75.6098)  acc5: 100.0000 (92.6829)  time: 0.3388 (0.0221 -- 2.0074)  data: 0.1818 (0.0001 -- 1.8176)  max mem: 16413
Val: Total time: 0:00:03 (0.3389 s / it)
* Acc@1 77.439 Acc@5 93.902 loss 0.993
Accuracy of the network on the 163 val images: 77.44%
[2023-10-23 20:51:17,570] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:51:17,572] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:51:17,572] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:51:17,572] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:51:18,877] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:51:18,877] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.44%
Epoch: [51]  [ 0/54]  eta: 0:07:47  lr: 0.000041  min_lr: 0.000001  loss: 1.2441 (1.2441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7412 (7.7412)  time: 8.6516 (8.6516 -- 8.6516)  data: 8.1112 (8.1112 -- 8.1112)  max mem: 16413
Epoch: [51]  [20/54]  eta: 0:00:40  lr: 0.000041  min_lr: 0.000001  loss: 1.9202 (1.8484)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3956 (7.4773)  time: 0.8121 (0.5262 -- 3.7848)  data: 0.2680 (0.0004 -- 3.2626)  max mem: 16413
Epoch: [51]  [40/54]  eta: 0:00:14  lr: 0.000041  min_lr: 0.000001  loss: 1.6418 (1.7766)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5358 (7.6887)  time: 0.9051 (0.5160 -- 3.7882)  data: 0.3617 (0.0003 -- 3.2465)  max mem: 16413
Epoch: [51]  [53/54]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.9715 (1.8235)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6691 (7.4883)  time: 0.6594 (0.4954 -- 2.6835)  data: 0.1400 (0.0002 -- 2.1459)  max mem: 16413
Epoch: [51] Total time: 0:00:50 (0.9304 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.9715 (1.8391)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6691 (7.4883)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9617 (0.9617)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0261 (2.0261 -- 2.0261)  data: 1.8328 (1.8328 -- 1.8328)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8903 (0.8817)  acc1: 77.7778 (74.3902)  acc5: 88.8889 (91.4634)  time: 0.3412 (0.0223 -- 2.0261)  data: 0.1834 (0.0001 -- 1.8328)  max mem: 16413
Val: Total time: 0:00:03 (0.3413 s / it)
* Acc@1 76.220 Acc@5 92.683 loss 1.014
Accuracy of the network on the 163 val images: 76.22%
Max accuracy: 77.44%
Epoch: [52]  [ 0/54]  eta: 0:07:08  lr: 0.000040  min_lr: 0.000001  loss: 1.9551 (1.9551)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7373 (4.7373)  time: 7.9387 (7.9387 -- 7.9387)  data: 5.7101 (5.7101 -- 5.7101)  max mem: 16413
Epoch: [52]  [20/54]  eta: 0:00:39  lr: 0.000040  min_lr: 0.000001  loss: 1.7560 (1.8528)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9046 (8.1462)  time: 0.8107 (0.5181 -- 3.6076)  data: 0.0916 (0.0003 -- 0.9884)  max mem: 16413
[2023-10-23 20:52:40,781] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:52:40,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 20:52:40,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:52:40,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [52]  [40/54]  eta: 0:00:14  lr: 0.000040  min_lr: 0.000001  loss: 1.7892 (1.8102)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6478 (7.8857)  time: 0.8479 (0.5362 -- 1.9417)  data: 0.0016 (0.0005 -- 0.0046)  max mem: 16413
Epoch: [52]  [53/54]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.6745 (1.8113)  loss_scale: 32768.0000 (25182.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7635 (7.8748)  time: 0.7618 (0.4970 -- 1.9417)  data: 0.0010 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [52] Total time: 0:00:49 (0.9207 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.6745 (1.8383)  loss_scale: 32768.0000 (25182.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7635 (7.8748)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9061 (0.9061)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9875 (1.9875 -- 1.9875)  data: 1.7892 (1.7892 -- 1.7892)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7850 (0.8340)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (93.9024)  time: 0.3374 (0.0223 -- 1.9875)  data: 0.1798 (0.0001 -- 1.7892)  max mem: 16413
Val: Total time: 0:00:03 (0.3375 s / it)
* Acc@1 77.439 Acc@5 93.902 loss 0.951
Accuracy of the network on the 163 val images: 77.44%
[2023-10-23 20:53:05,879] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:53:05,880] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:53:05,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:53:05,881] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:53:07,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:53:07,293] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.44%
Epoch: [53]  [ 0/54]  eta: 0:05:43  lr: 0.000040  min_lr: 0.000001  loss: 1.7208 (1.7208)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8371 (7.8371)  time: 6.3670 (6.3670 -- 6.3670)  data: 5.4265 (5.4265 -- 5.4265)  max mem: 16413
Epoch: [53]  [20/54]  eta: 0:00:39  lr: 0.000040  min_lr: 0.000001  loss: 1.7183 (1.7361)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6589 (7.6503)  time: 0.8994 (0.5280 -- 3.4673)  data: 0.0014 (0.0003 -- 0.0025)  max mem: 16413
[2023-10-23 20:53:41,129] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2893
[2023-10-23 20:53:41,129] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2893
[2023-10-23 20:53:41,129] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:53:41,129] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 20:53:41,130] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [53]  [40/54]  eta: 0:00:14  lr: 0.000040  min_lr: 0.000001  loss: 1.7245 (1.7755)  loss_scale: 16384.0000 (28771.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6204 (7.2480)  time: 0.9027 (0.5173 -- 3.6933)  data: 0.0014 (0.0004 -- 0.0036)  max mem: 16413
[2023-10-23 20:53:52,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2906
[2023-10-23 20:53:52,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2906
[2023-10-23 20:53:52,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 20:53:52,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 20:53:52,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [53]  [53/54]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.9172 (1.7990)  loss_scale: 8192.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4423 (7.1788)  time: 0.7425 (0.4829 -- 3.6933)  data: 0.0008 (0.0001 -- 0.0036)  max mem: 16413
Epoch: [53] Total time: 0:00:50 (0.9324 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.9172 (1.8741)  loss_scale: 8192.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4423 (7.1788)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9260 (0.9260)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0913 (2.0913 -- 2.0913)  data: 1.9126 (1.9126 -- 1.9126)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8576 (0.8353)  acc1: 77.7778 (75.6098)  acc5: 100.0000 (92.6829)  time: 0.3472 (0.0222 -- 2.0913)  data: 0.1913 (0.0001 -- 1.9126)  max mem: 16413
Val: Total time: 0:00:03 (0.3474 s / it)
* Acc@1 76.829 Acc@5 93.902 loss 0.969
Accuracy of the network on the 163 val images: 76.83%
Max accuracy: 77.44%
Epoch: [54]  [ 0/54]  eta: 0:06:36  lr: 0.000040  min_lr: 0.000001  loss: 2.2384 (2.2384)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1123 (8.1123)  time: 7.3459 (7.3459 -- 7.3459)  data: 6.8087 (6.8087 -- 6.8087)  max mem: 16413
Epoch: [54]  [20/54]  eta: 0:00:39  lr: 0.000040  min_lr: 0.000001  loss: 1.8127 (1.8891)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7001 (7.8053)  time: 0.8677 (0.5213 -- 2.4863)  data: 0.1940 (0.0003 -- 1.9479)  max mem: 16413
Epoch: [54]  [40/54]  eta: 0:00:14  lr: 0.000040  min_lr: 0.000001  loss: 1.8282 (1.8416)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2959 (7.9740)  time: 0.8797 (0.5162 -- 2.4385)  data: 0.0013 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [54]  [53/54]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.7318 (1.8158)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7838 (7.7256)  time: 0.7683 (0.4935 -- 2.4385)  data: 0.0007 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [54] Total time: 0:00:50 (0.9407 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.7318 (1.8439)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7838 (7.7256)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9085 (0.9085)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9859 (1.9859 -- 1.9859)  data: 1.7897 (1.7897 -- 1.7897)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8236 (0.8402)  acc1: 77.7778 (76.8293)  acc5: 88.8889 (91.4634)  time: 0.3365 (0.0221 -- 1.9859)  data: 0.1791 (0.0001 -- 1.7897)  max mem: 16413
Val: Total time: 0:00:03 (0.3366 s / it)
* Acc@1 80.488 Acc@5 92.683 loss 0.968
Accuracy of the network on the 163 val images: 80.49%
[2023-10-23 20:54:55,465] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:54:55,467] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:54:55,467] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:54:55,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:54:56,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:54:56,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.49%
Epoch: [55]  [ 0/54]  eta: 0:04:44  lr: 0.000040  min_lr: 0.000001  loss: 1.5297 (1.5297)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9788 (4.9788)  time: 5.2593 (5.2593 -- 5.2593)  data: 4.7291 (4.7291 -- 4.7291)  max mem: 16413
Epoch: [55]  [20/54]  eta: 0:00:40  lr: 0.000040  min_lr: 0.000001  loss: 1.7738 (1.8087)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3454 (6.6290)  time: 0.9924 (0.5322 -- 4.4250)  data: 0.2151 (0.0009 -- 1.9478)  max mem: 16413
[2023-10-23 20:55:29,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[9.392130821382883e-07, 9.392130821382883e-07, 1.2522841095177177e-06, 1.2522841095177177e-06, 1.6697121460236236e-06, 1.6697121460236236e-06, 2.2262828613648317e-06, 2.2262828613648317e-06, 2.968377148486442e-06, 2.968377148486442e-06, 3.9578361979819225e-06, 3.9578361979819225e-06, 5.277114930642564e-06, 5.277114930642564e-06, 7.036153240856751e-06, 7.036153240856751e-06, 9.38153765447567e-06, 9.38153765447567e-06, 1.2508716872634225e-05, 1.2508716872634225e-05, 1.66782891635123e-05, 1.66782891635123e-05, 2.2237718884683065e-05, 2.2237718884683065e-05, 2.9650291846244087e-05, 2.9650291846244087e-05, 3.9533722461658785e-05, 3.9533722461658785e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 20:55:29,449] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=18.411484630762622, CurrSamplesPerSec=22.186528241604396, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [55]  [40/54]  eta: 0:00:14  lr: 0.000039  min_lr: 0.000001  loss: 1.8955 (1.8312)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4558 (7.1543)  time: 0.8504 (0.5162 -- 4.1140)  data: 0.0013 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [55]  [53/54]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.8468 (1.8259)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7292 (7.3325)  time: 0.5784 (0.4951 -- 1.7465)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [55] Total time: 0:00:49 (0.9246 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.8468 (1.8180)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7292 (7.3325)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8895 (0.8895)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0386 (2.0386 -- 2.0386)  data: 1.8470 (1.8470 -- 1.8470)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8777 (0.8497)  acc1: 77.7778 (75.6098)  acc5: 88.8889 (91.4634)  time: 0.3413 (0.0225 -- 2.0386)  data: 0.1848 (0.0001 -- 1.8470)  max mem: 16413
Val: Total time: 0:00:03 (0.3415 s / it)
* Acc@1 79.268 Acc@5 92.683 loss 0.957
Accuracy of the network on the 163 val images: 79.27%
Max accuracy: 80.49%
Epoch: [56]  [ 0/54]  eta: 0:05:51  lr: 0.000039  min_lr: 0.000001  loss: 1.3295 (1.3295)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.6713 (13.6713)  time: 6.5048 (6.5048 -- 6.5048)  data: 5.5288 (5.5288 -- 5.5288)  max mem: 16413
[2023-10-23 20:56:07,723] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:56:07,723] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 20:56:07,723] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:56:07,724] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [56]  [20/54]  eta: 0:00:41  lr: 0.000039  min_lr: 0.000001  loss: 1.8205 (1.7922)  loss_scale: 8192.0000 (12092.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0107 (7.1134)  time: 0.9660 (0.5291 -- 5.1964)  data: 0.0657 (0.0006 -- 1.1466)  max mem: 16413
Epoch: [56]  [40/54]  eta: 0:00:14  lr: 0.000039  min_lr: 0.000001  loss: 1.7810 (1.8030)  loss_scale: 16384.0000 (14186.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9593 (7.1422)  time: 0.8140 (0.5170 -- 3.2948)  data: 0.0024 (0.0006 -- 0.0163)  max mem: 16413
Epoch: [56]  [53/54]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.7905 (1.8261)  loss_scale: 16384.0000 (14715.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3501 (7.6675)  time: 0.6870 (0.4955 -- 1.8429)  data: 0.0803 (0.0003 -- 1.3354)  max mem: 16413
Epoch: [56] Total time: 0:00:50 (0.9322 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.7905 (1.8141)  loss_scale: 16384.0000 (14715.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3501 (7.6675)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8986 (0.8986)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0641 (2.0641 -- 2.0641)  data: 1.8877 (1.8877 -- 1.8877)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8986 (0.8558)  acc1: 77.7778 (76.8293)  acc5: 88.8889 (91.4634)  time: 0.3440 (0.0221 -- 2.0641)  data: 0.1889 (0.0001 -- 1.8877)  max mem: 16413
Val: Total time: 0:00:03 (0.3441 s / it)
* Acc@1 80.488 Acc@5 93.293 loss 0.964
Accuracy of the network on the 163 val images: 80.49%
Max accuracy: 80.49%
Epoch: [57]  [ 0/54]  eta: 0:07:18  lr: 0.000039  min_lr: 0.000001  loss: 1.8251 (1.8251)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8068 (5.8068)  time: 8.1209 (8.1209 -- 8.1209)  data: 7.5918 (7.5918 -- 7.5918)  max mem: 16413
Epoch: [57]  [20/54]  eta: 0:00:39  lr: 0.000039  min_lr: 0.000001  loss: 1.8406 (1.8699)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9745 (6.8023)  time: 0.8148 (0.5264 -- 3.6051)  data: 0.2674 (0.0003 -- 3.0558)  max mem: 16413
[2023-10-23 20:57:15,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3104
[2023-10-23 20:57:15,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3104
[2023-10-23 20:57:15,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 20:57:15,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 20:57:15,146] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [57]  [40/54]  eta: 0:00:14  lr: 0.000039  min_lr: 0.000001  loss: 1.9762 (1.8764)  loss_scale: 8192.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7779 (7.2287)  time: 0.8807 (0.5193 -- 3.8115)  data: 0.3333 (0.0004 -- 3.2861)  max mem: 16413
Epoch: [57]  [53/54]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.9648 (1.8622)  loss_scale: 8192.0000 (12136.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8798 (7.2828)  time: 0.6932 (0.4941 -- 1.9592)  data: 0.1766 (0.0002 -- 1.4477)  max mem: 16413
Epoch: [57] Total time: 0:00:51 (0.9465 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.9648 (1.9021)  loss_scale: 8192.0000 (12136.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8798 (7.2828)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8902 (0.8902)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9906 (1.9906 -- 1.9906)  data: 1.7979 (1.7979 -- 1.7979)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8469 (0.8439)  acc1: 66.6667 (74.3902)  acc5: 100.0000 (93.9024)  time: 0.3366 (0.0223 -- 1.9906)  data: 0.1799 (0.0001 -- 1.7979)  max mem: 16413
Val: Total time: 0:00:03 (0.3367 s / it)
* Acc@1 77.439 Acc@5 93.902 loss 0.954
Accuracy of the network on the 163 val images: 77.44%
Max accuracy: 80.49%
Epoch: [58]  [ 0/54]  eta: 0:06:43  lr: 0.000039  min_lr: 0.000001  loss: 1.9109 (1.9109)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8774 (7.8774)  time: 7.4661 (7.4661 -- 7.4661)  data: 6.9341 (6.9341 -- 6.9341)  max mem: 16413
Epoch: [58]  [20/54]  eta: 0:00:40  lr: 0.000039  min_lr: 0.000001  loss: 1.6548 (1.7365)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0226 (7.4473)  time: 0.8738 (0.5330 -- 3.6811)  data: 0.2248 (0.0006 -- 3.0892)  max mem: 16413
Epoch: [58]  [40/54]  eta: 0:00:14  lr: 0.000039  min_lr: 0.000001  loss: 1.7329 (1.7807)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1405 (7.6217)  time: 0.8140 (0.5127 -- 3.5760)  data: 0.2702 (0.0003 -- 3.0247)  max mem: 16413
Epoch: [58]  [53/54]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.7745 (1.7994)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0257 (7.6686)  time: 0.6595 (0.4957 -- 2.2283)  data: 0.1419 (0.0002 -- 1.7085)  max mem: 16413
Epoch: [58] Total time: 0:00:50 (0.9378 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.7745 (1.8028)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0257 (7.6686)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8697 (0.8697)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0505 (2.0505 -- 2.0505)  data: 1.8750 (1.8750 -- 1.8750)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8697 (0.8324)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (93.9024)  time: 0.3430 (0.0222 -- 2.0505)  data: 0.1876 (0.0001 -- 1.8750)  max mem: 16413
Val: Total time: 0:00:03 (0.3431 s / it)
* Acc@1 81.098 Acc@5 94.512 loss 0.947
Accuracy of the network on the 163 val images: 81.10%
[2023-10-23 20:58:32,772] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 20:58:32,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 20:58:32,773] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 20:58:32,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 20:58:34,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 20:58:34,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.10%
Epoch: [59]  [ 0/54]  eta: 0:06:31  lr: 0.000039  min_lr: 0.000001  loss: 1.5253 (1.5253)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2323 (5.2323)  time: 7.2568 (7.2568 -- 7.2568)  data: 6.7082 (6.7082 -- 6.7082)  max mem: 16413
Epoch: [59]  [20/54]  eta: 0:00:40  lr: 0.000038  min_lr: 0.000001  loss: 1.9359 (1.9192)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3083 (7.3763)  time: 0.8972 (0.5275 -- 2.8642)  data: 0.3517 (0.0005 -- 2.3617)  max mem: 16413
Epoch: [59]  [40/54]  eta: 0:00:14  lr: 0.000038  min_lr: 0.000001  loss: 1.6912 (1.8102)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5420 (7.0259)  time: 0.8272 (0.5127 -- 2.2324)  data: 0.2862 (0.0003 -- 1.6781)  max mem: 16413
[2023-10-23 20:59:21,616] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:59:21,617] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 20:59:21,617] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 20:59:21,617] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [59]  [53/54]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.8780 (1.8620)  loss_scale: 8192.0000 (9253.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1064 (6.8997)  time: 0.7197 (0.4947 -- 2.4891)  data: 0.2039 (0.0002 -- 1.9851)  max mem: 16413
Epoch: [59] Total time: 0:00:51 (0.9503 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.8780 (1.8389)  loss_scale: 8192.0000 (9253.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1064 (6.8997)
[2023-10-23 20:59:25,639] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-10-23 20:59:25,641] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-59/mp_rank_00_model_states.pt
[2023-10-23 20:59:25,641] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-59/mp_rank_00_model_states.pt...
[2023-10-23 20:59:25,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-10-23 20:59:26,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-59/mp_rank_00_model_states.pt.
[2023-10-23 20:59:26,666] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/10]  eta: 0:00:18  loss: 0.8722 (0.8722)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.8613 (1.8613 -- 1.8613)  data: 1.6490 (1.6490 -- 1.6490)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8441 (0.8117)  acc1: 77.7778 (76.8293)  acc5: 100.0000 (95.1220)  time: 0.3306 (0.0224 -- 1.8613)  data: 0.1715 (0.0001 -- 1.6490)  max mem: 16413
Val: Total time: 0:00:03 (0.3309 s / it)
* Acc@1 79.878 Acc@5 95.122 loss 0.920
Accuracy of the network on the 163 val images: 79.88%
Max accuracy: 81.10%
Epoch: [60]  [ 0/54]  eta: 0:05:58  lr: 0.000038  min_lr: 0.000001  loss: 1.5775 (1.5775)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0194 (9.0194)  time: 6.6432 (6.6432 -- 6.6432)  data: 6.0359 (6.0359 -- 6.0359)  max mem: 16413
Epoch: [60]  [20/54]  eta: 0:00:42  lr: 0.000038  min_lr: 0.000001  loss: 1.7755 (1.7587)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0544 (7.7060)  time: 0.9673 (0.5204 -- 3.7535)  data: 0.0570 (0.0002 -- 0.9573)  max mem: 16413
Epoch: [60]  [40/54]  eta: 0:00:14  lr: 0.000038  min_lr: 0.000001  loss: 1.7120 (1.7471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4053 (7.3845)  time: 0.8300 (0.5206 -- 2.6535)  data: 0.1956 (0.0002 -- 2.1081)  max mem: 16413
Epoch: [60]  [53/54]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.7639 (1.7937)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3587 (7.3346)  time: 0.7168 (0.4948 -- 1.9034)  data: 0.0693 (0.0002 -- 1.3731)  max mem: 16413
Epoch: [60] Total time: 0:00:51 (0.9585 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.7639 (1.8229)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3587 (7.3346)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9024 (0.9024)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9981 (1.9981 -- 1.9981)  data: 1.7795 (1.7795 -- 1.7795)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8727 (0.8309)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (93.9024)  time: 0.3402 (0.0223 -- 1.9981)  data: 0.1805 (0.0001 -- 1.7795)  max mem: 16413
Val: Total time: 0:00:03 (0.3403 s / it)
* Acc@1 81.098 Acc@5 93.902 loss 0.947
Accuracy of the network on the 163 val images: 81.10%
Max accuracy: 81.10%
Epoch: [61]  [ 0/54]  eta: 0:06:25  lr: 0.000038  min_lr: 0.000001  loss: 2.1590 (2.1590)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8263 (5.8263)  time: 7.1469 (7.1469 -- 7.1469)  data: 5.6428 (5.6428 -- 5.6428)  max mem: 16413
Epoch: [61]  [20/54]  eta: 0:00:38  lr: 0.000038  min_lr: 0.000001  loss: 1.8336 (1.8069)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9138 (7.9653)  time: 0.8452 (0.5201 -- 2.1812)  data: 0.0915 (0.0003 -- 0.9997)  max mem: 16413
Epoch: [61]  [40/54]  eta: 0:00:14  lr: 0.000038  min_lr: 0.000001  loss: 1.7417 (1.7965)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0877 (7.6438)  time: 0.8983 (0.5229 -- 3.7521)  data: 0.3500 (0.0006 -- 3.1808)  max mem: 16413
Epoch: [61]  [53/54]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.6543 (1.7691)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0877 (7.6289)  time: 0.6727 (0.4942 -- 2.0989)  data: 0.1376 (0.0002 -- 1.5627)  max mem: 16413
Epoch: [61] Total time: 0:00:50 (0.9282 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.6543 (1.7810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0877 (7.6289)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9323 (0.9323)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0406 (2.0406 -- 2.0406)  data: 1.8519 (1.8519 -- 1.8519)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7845 (0.8156)  acc1: 77.7778 (76.8293)  acc5: 88.8889 (91.4634)  time: 0.3415 (0.0220 -- 2.0406)  data: 0.1853 (0.0001 -- 1.8519)  max mem: 16413
Val: Total time: 0:00:03 (0.3416 s / it)
* Acc@1 79.878 Acc@5 92.683 loss 0.954
Accuracy of the network on the 163 val images: 79.88%
Max accuracy: 81.10%
Epoch: [62]  [ 0/54]  eta: 0:07:12  lr: 0.000038  min_lr: 0.000001  loss: 1.8123 (1.8123)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4950 (10.4950)  time: 8.0041 (8.0041 -- 8.0041)  data: 6.5763 (6.5763 -- 6.5763)  max mem: 16413
[2023-10-23 21:01:37,783] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:01:37,783] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:01:37,786] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:01:37,786] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [62]  [20/54]  eta: 0:00:39  lr: 0.000038  min_lr: 0.000001  loss: 1.7937 (1.8164)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2570 (7.5670)  time: 0.8174 (0.5336 -- 3.3857)  data: 0.0719 (0.0004 -- 0.5713)  max mem: 16413
[2023-10-23 21:01:44,951] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3371
[2023-10-23 21:01:44,952] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:01:44,952] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3371
[2023-10-23 21:01:44,952] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:01:44,953] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [62]  [40/54]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000001  loss: 1.9162 (1.8806)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3695 (7.6628)  time: 0.8809 (0.5376 -- 2.3211)  data: 0.2517 (0.0007 -- 1.7843)  max mem: 16413
Epoch: [62]  [53/54]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.8608 (1.8547)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1486 (7.5776)  time: 0.7399 (0.4958 -- 2.2564)  data: 0.1543 (0.0001 -- 1.7241)  max mem: 16413
Epoch: [62] Total time: 0:00:50 (0.9275 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.8608 (1.8518)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1486 (7.5776)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8919 (0.8919)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0727 (2.0727 -- 2.0727)  data: 1.8930 (1.8930 -- 1.8930)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.8733 (0.8229)  acc1: 77.7778 (78.0488)  acc5: 100.0000 (92.6829)  time: 0.3462 (0.0224 -- 2.0727)  data: 0.1894 (0.0001 -- 1.8930)  max mem: 16413
Val: Total time: 0:00:03 (0.3463 s / it)
* Acc@1 80.488 Acc@5 93.902 loss 0.945
Accuracy of the network on the 163 val images: 80.49%
Max accuracy: 81.10%
Epoch: [63]  [ 0/54]  eta: 0:06:51  lr: 0.000037  min_lr: 0.000001  loss: 1.8533 (1.8533)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0899 (6.0899)  time: 7.6209 (7.6209 -- 7.6209)  data: 7.0929 (7.0929 -- 7.0929)  max mem: 16413
Epoch: [63]  [20/54]  eta: 0:00:41  lr: 0.000037  min_lr: 0.000001  loss: 1.9076 (1.8629)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1699 (7.8393)  time: 0.8909 (0.5304 -- 5.2412)  data: 0.3403 (0.0004 -- 4.7031)  max mem: 16413
Epoch: [63]  [40/54]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000001  loss: 1.7363 (1.8203)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2392 (8.0035)  time: 0.7792 (0.5288 -- 3.1972)  data: 0.2093 (0.0002 -- 2.6837)  max mem: 16413
Epoch: [63]  [53/54]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.9201 (1.8451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5694 (7.9909)  time: 0.6888 (0.4954 -- 1.4449)  data: 0.0616 (0.0002 -- 0.8140)  max mem: 16413
Epoch: [63] Total time: 0:00:49 (0.9195 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.9201 (1.8161)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5694 (7.9909)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8725 (0.8725)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0315 (2.0315 -- 2.0315)  data: 1.8437 (1.8437 -- 1.8437)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7064 (0.7801)  acc1: 77.7778 (78.0488)  acc5: 88.8889 (90.2439)  time: 0.3410 (0.0223 -- 2.0315)  data: 0.1845 (0.0001 -- 1.8437)  max mem: 16413
Val: Total time: 0:00:03 (0.3411 s / it)
* Acc@1 81.707 Acc@5 92.683 loss 0.917
Accuracy of the network on the 163 val images: 81.71%
[2023-10-23 21:03:05,545] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 21:03:05,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 21:03:05,547] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 21:03:05,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 21:03:07,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 21:03:07,057] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.71%
Epoch: [64]  [ 0/54]  eta: 0:06:14  lr: 0.000037  min_lr: 0.000001  loss: 2.2957 (2.2957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7043 (8.7043)  time: 6.9285 (6.9285 -- 6.9285)  data: 5.8172 (5.8172 -- 5.8172)  max mem: 16413
Epoch: [64]  [20/54]  eta: 0:00:40  lr: 0.000037  min_lr: 0.000001  loss: 1.7327 (1.7259)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9603 (7.6039)  time: 0.9166 (0.5298 -- 2.8785)  data: 0.0894 (0.0003 -- 1.6468)  max mem: 16413
Epoch: [64]  [40/54]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000001  loss: 1.7322 (1.7425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5714 (7.6687)  time: 0.8634 (0.5160 -- 3.4869)  data: 0.0658 (0.0003 -- 0.6548)  max mem: 16413
[2023-10-23 21:03:53,173] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:03:53,173] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:03:53,173] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:03:53,173] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [64]  [53/54]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.6969 (1.7613)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1027 (7.4924)  time: 0.7215 (0.4953 -- 1.8863)  data: 0.0140 (0.0001 -- 0.2669)  max mem: 16413
Epoch: [64] Total time: 0:00:51 (0.9557 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.6969 (1.7839)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1027 (7.4924)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8754 (0.8754)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9404 (1.9404 -- 1.9404)  data: 1.6975 (1.6975 -- 1.6975)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7440 (0.7984)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (92.6829)  time: 0.3318 (0.0224 -- 1.9404)  data: 0.1698 (0.0001 -- 1.6975)  max mem: 16413
Val: Total time: 0:00:03 (0.3319 s / it)
* Acc@1 81.707 Acc@5 93.293 loss 0.937
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 81.71%
Epoch: [65]  [ 0/54]  eta: 0:06:25  lr: 0.000037  min_lr: 0.000001  loss: 1.5754 (1.5754)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2578 (8.2578)  time: 7.1396 (7.1396 -- 7.1396)  data: 5.6190 (5.6190 -- 5.6190)  max mem: 16413
[2023-10-23 21:04:10,401] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3512
[2023-10-23 21:04:10,401] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3512
[2023-10-23 21:04:10,401] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:04:10,401] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:04:10,401] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [65]  [20/54]  eta: 0:00:44  lr: 0.000037  min_lr: 0.000001  loss: 1.8092 (1.7922)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8290 (7.9526)  time: 1.0089 (0.5294 -- 3.8527)  data: 0.0429 (0.0005 -- 0.4757)  max mem: 16413
Epoch: [65]  [40/54]  eta: 0:00:14  lr: 0.000037  min_lr: 0.000001  loss: 1.6770 (1.7468)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9491 (7.7462)  time: 0.7509 (0.5210 -- 2.8448)  data: 0.0018 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [65]  [53/54]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.9957 (1.8095)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5077 (7.6080)  time: 0.7422 (0.4955 -- 3.0728)  data: 0.0089 (0.0001 -- 0.1621)  max mem: 16413
Epoch: [65] Total time: 0:00:51 (0.9559 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.9957 (1.8468)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5077 (7.6080)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8716 (0.8716)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0431 (2.0431 -- 2.0431)  data: 1.8541 (1.8541 -- 1.8541)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7453 (0.7686)  acc1: 77.7778 (80.4878)  acc5: 88.8889 (90.2439)  time: 0.3425 (0.0223 -- 2.0431)  data: 0.1855 (0.0001 -- 1.8541)  max mem: 16413
Val: Total time: 0:00:03 (0.3426 s / it)
* Acc@1 82.317 Acc@5 92.683 loss 0.910
Accuracy of the network on the 163 val images: 82.32%
[2023-10-23 21:04:57,174] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 21:04:57,176] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 21:04:57,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 21:04:57,176] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 21:04:58,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 21:04:58,593] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.32%
Epoch: [66]  [ 0/54]  eta: 0:05:53  lr: 0.000036  min_lr: 0.000001  loss: 2.0900 (2.0900)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5743 (7.5743)  time: 6.5469 (6.5469 -- 6.5469)  data: 5.5126 (5.5126 -- 5.5126)  max mem: 16413
Epoch: [66]  [20/54]  eta: 0:00:39  lr: 0.000036  min_lr: 0.000001  loss: 1.7179 (1.8489)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3180 (6.7967)  time: 0.8876 (0.5193 -- 2.4050)  data: 0.1293 (0.0005 -- 1.7283)  max mem: 16413
Epoch: [66]  [40/54]  eta: 0:00:14  lr: 0.000036  min_lr: 0.000001  loss: 1.9375 (1.8797)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4727 (6.8710)  time: 0.9123 (0.5161 -- 3.9874)  data: 0.0610 (0.0005 -- 0.7896)  max mem: 16413
Epoch: [66]  [53/54]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.9407 (1.8697)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0442 (7.0555)  time: 0.7649 (0.4963 -- 3.9874)  data: 0.0009 (0.0001 -- 0.0038)  max mem: 16413
Epoch: [66] Total time: 0:00:50 (0.9364 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.9407 (1.8236)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0442 (7.0555)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8677 (0.8677)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0486 (2.0486 -- 2.0486)  data: 1.8703 (1.8703 -- 1.8703)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6150 (0.7647)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (91.4634)  time: 0.3455 (0.0221 -- 2.0486)  data: 0.1896 (0.0001 -- 1.8703)  max mem: 16413
Val: Total time: 0:00:03 (0.3457 s / it)
* Acc@1 82.317 Acc@5 93.293 loss 0.905
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 82.32%
Epoch: [67]  [ 0/54]  eta: 0:07:12  lr: 0.000036  min_lr: 0.000001  loss: 2.4513 (2.4513)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6635 (6.6635)  time: 8.0150 (8.0150 -- 8.0150)  data: 7.4856 (7.4856 -- 7.4856)  max mem: 16413
Epoch: [67]  [20/54]  eta: 0:00:42  lr: 0.000036  min_lr: 0.000001  loss: 1.6921 (1.7985)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5303 (9.3025)  time: 0.9245 (0.5196 -- 4.4105)  data: 0.0823 (0.0005 -- 1.6010)  max mem: 16413
[2023-10-23 21:06:20,766] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:06:20,766] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:06:20,766] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:06:20,766] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [67]  [40/54]  eta: 0:00:14  lr: 0.000036  min_lr: 0.000001  loss: 1.9672 (1.8559)  loss_scale: 32768.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7431 (8.9278)  time: 0.8182 (0.5199 -- 4.0231)  data: 0.0513 (0.0001 -- 0.9999)  max mem: 16413
[2023-10-23 21:06:38,629] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3664
[2023-10-23 21:06:38,629] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3664
[2023-10-23 21:06:38,629] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:06:38,629] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:06:38,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [67]  [53/54]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.8263 (1.8725)  loss_scale: 32768.0000 (23362.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7819 (8.7948)  time: 0.6104 (0.4834 -- 1.7716)  data: 0.0010 (0.0001 -- 0.0076)  max mem: 16413
Epoch: [67] Total time: 0:00:50 (0.9402 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.8263 (1.8725)  loss_scale: 32768.0000 (23362.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7819 (8.7948)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8716 (0.8716)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0719 (2.0719 -- 2.0719)  data: 1.8842 (1.8842 -- 1.8842)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6876 (0.7836)  acc1: 77.7778 (78.0488)  acc5: 88.8889 (92.6829)  time: 0.3453 (0.0221 -- 2.0719)  data: 0.1885 (0.0001 -- 1.8842)  max mem: 16413
Val: Total time: 0:00:03 (0.3454 s / it)
* Acc@1 81.098 Acc@5 93.902 loss 0.916
Accuracy of the network on the 163 val images: 81.10%
Max accuracy: 82.32%
Epoch: [68]  [ 0/54]  eta: 0:06:12  lr: 0.000036  min_lr: 0.000001  loss: 1.7009 (1.7009)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6885 (4.6885)  time: 6.9038 (6.9038 -- 6.9038)  data: 5.2964 (5.2964 -- 5.2964)  max mem: 16413
Epoch: [68]  [20/54]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000001  loss: 1.8547 (1.8160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8055 (7.1260)  time: 0.8283 (0.5149 -- 2.8618)  data: 0.2549 (0.0004 -- 2.3301)  max mem: 16413
Epoch: [68]  [40/54]  eta: 0:00:13  lr: 0.000036  min_lr: 0.000001  loss: 1.9226 (1.8390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0812 (7.3685)  time: 0.8695 (0.5220 -- 2.7875)  data: 0.2645 (0.0003 -- 1.7034)  max mem: 16413
Epoch: [68]  [53/54]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8550 (1.8715)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7057 (7.4692)  time: 0.6757 (0.4948 -- 1.5168)  data: 0.1300 (0.0002 -- 0.9914)  max mem: 16413
Epoch: [68] Total time: 0:00:49 (0.9192 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8550 (1.8404)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7057 (7.4692)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9027 (0.9027)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0002 (2.0002 -- 2.0002)  data: 1.7988 (1.7988 -- 1.7988)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6916 (0.7662)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3406 (0.0224 -- 2.0002)  data: 0.1819 (0.0001 -- 1.7988)  max mem: 16413
Val: Total time: 0:00:03 (0.3408 s / it)
* Acc@1 81.098 Acc@5 95.122 loss 0.896
Accuracy of the network on the 163 val images: 81.10%
Max accuracy: 82.32%
Epoch: [69]  [ 0/54]  eta: 0:07:38  lr: 0.000035  min_lr: 0.000001  loss: 2.1762 (2.1762)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5189 (4.5189)  time: 8.4949 (8.4949 -- 8.4949)  data: 5.6196 (5.6196 -- 5.6196)  max mem: 16413
Epoch: [69]  [20/54]  eta: 0:00:39  lr: 0.000035  min_lr: 0.000001  loss: 2.0084 (1.9546)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1619 (7.7556)  time: 0.7925 (0.5262 -- 3.1875)  data: 0.1119 (0.0005 -- 1.1170)  max mem: 16413
Epoch: [69]  [40/54]  eta: 0:00:13  lr: 0.000035  min_lr: 0.000001  loss: 1.6808 (1.8273)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3257 (7.6288)  time: 0.8238 (0.5177 -- 3.7521)  data: 0.2629 (0.0005 -- 3.2299)  max mem: 16413
Epoch: [69]  [53/54]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8303 (1.8285)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9860 (7.4680)  time: 0.6563 (0.4959 -- 1.9667)  data: 0.0504 (0.0001 -- 0.6733)  max mem: 16413
Epoch: [69] Total time: 0:00:49 (0.9098 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8303 (1.8293)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9860 (7.4680)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8954 (0.8954)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9883 (1.9883 -- 1.9883)  data: 1.7804 (1.7804 -- 1.7804)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7260 (0.7783)  acc1: 77.7778 (80.4878)  acc5: 88.8889 (91.4634)  time: 0.3364 (0.0223 -- 1.9883)  data: 0.1781 (0.0001 -- 1.7804)  max mem: 16413
Val: Total time: 0:00:03 (0.3365 s / it)
* Acc@1 81.707 Acc@5 93.293 loss 0.914
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 82.32%
Epoch: [70]  [ 0/54]  eta: 0:06:01  lr: 0.000035  min_lr: 0.000001  loss: 1.6883 (1.6883)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1319 (9.1319)  time: 6.6894 (6.6894 -- 6.6894)  data: 5.3979 (5.3979 -- 5.3979)  max mem: 16413
[2023-10-23 21:08:50,895] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:08:50,895] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:08:50,903] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:08:50,904] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:08:56,869] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3799
[2023-10-23 21:08:56,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:08:56,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3799
[2023-10-23 21:08:56,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:08:56,871] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [70]  [20/54]  eta: 0:00:40  lr: 0.000035  min_lr: 0.000001  loss: 1.7846 (1.8748)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8771 (7.1157)  time: 0.9087 (0.5316 -- 4.0363)  data: 0.1991 (0.0002 -- 3.5037)  max mem: 16413
Epoch: [70]  [40/54]  eta: 0:00:15  lr: 0.000035  min_lr: 0.000001  loss: 1.6505 (1.8185)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6872 (7.1531)  time: 1.0045 (0.5228 -- 3.9806)  data: 0.3867 (0.0002 -- 3.4693)  max mem: 16413
Epoch: [70]  [53/54]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8435 (1.8267)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8535 (7.1441)  time: 0.6909 (0.4945 -- 3.9806)  data: 0.1743 (0.0001 -- 3.4693)  max mem: 16413
Epoch: [70] Total time: 0:00:51 (0.9536 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8435 (1.8291)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8535 (7.1441)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8992 (0.8992)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9660 (1.9660 -- 1.9660)  data: 1.7406 (1.7406 -- 1.7406)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7055 (0.7785)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (93.9024)  time: 0.3399 (0.0222 -- 1.9660)  data: 0.1786 (0.0001 -- 1.7406)  max mem: 16413
Val: Total time: 0:00:03 (0.3400 s / it)
* Acc@1 81.098 Acc@5 93.902 loss 0.912
Accuracy of the network on the 163 val images: 81.10%
Max accuracy: 82.32%
Epoch: [71]  [ 0/54]  eta: 0:05:31  lr: 0.000035  min_lr: 0.000001  loss: 1.9235 (1.9235)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1295 (7.1295)  time: 6.1479 (6.1479 -- 6.1479)  data: 5.5886 (5.5886 -- 5.5886)  max mem: 16413
Epoch: [71]  [20/54]  eta: 0:00:39  lr: 0.000035  min_lr: 0.000001  loss: 1.8587 (1.8867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6175 (8.1259)  time: 0.9175 (0.5227 -- 4.2478)  data: 0.0481 (0.0005 -- 0.5213)  max mem: 16413
Epoch: [71]  [40/54]  eta: 0:00:14  lr: 0.000035  min_lr: 0.000001  loss: 1.7941 (1.8261)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9678 (8.6344)  time: 0.9241 (0.5154 -- 3.9890)  data: 0.0842 (0.0003 -- 1.6608)  max mem: 16413
Epoch: [71]  [53/54]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8419 (1.7955)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5606 (8.4093)  time: 0.6493 (0.4938 -- 2.1838)  data: 0.1346 (0.0002 -- 1.6608)  max mem: 16413
Epoch: [71] Total time: 0:00:50 (0.9361 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8419 (1.7658)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5606 (8.4093)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8714 (0.8714)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0561 (2.0561 -- 2.0561)  data: 1.8714 (1.8714 -- 1.8714)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7139 (0.7520)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (93.9024)  time: 0.3433 (0.0223 -- 2.0561)  data: 0.1872 (0.0001 -- 1.8714)  max mem: 16413
Val: Total time: 0:00:03 (0.3435 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.879
Accuracy of the network on the 163 val images: 82.93%
[2023-10-23 21:10:21,651] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 21:10:21,652] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 21:10:21,652] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 21:10:21,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 21:10:23,033] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 21:10:23,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.93%
Epoch: [72]  [ 0/54]  eta: 0:05:36  lr: 0.000035  min_lr: 0.000001  loss: 1.6546 (1.6546)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7283 (12.7283)  time: 6.2277 (6.2277 -- 6.2277)  data: 5.6914 (5.6914 -- 5.6914)  max mem: 16413
Epoch: [72]  [20/54]  eta: 0:00:40  lr: 0.000034  min_lr: 0.000001  loss: 1.7326 (1.7172)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3881 (8.0639)  time: 0.9516 (0.5298 -- 2.7770)  data: 0.2275 (0.0005 -- 1.3901)  max mem: 16413
[2023-10-23 21:11:05,513] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:11:05,513] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:11:05,516] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:11:05,516] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [72]  [40/54]  eta: 0:00:14  lr: 0.000034  min_lr: 0.000001  loss: 1.9538 (1.7855)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7583 (8.1614)  time: 0.8614 (0.5155 -- 3.7820)  data: 0.0967 (0.0004 -- 1.3464)  max mem: 16413
Epoch: [72]  [53/54]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.7483 (1.7621)  loss_scale: 32768.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8336 (7.9783)  time: 0.5962 (0.4944 -- 2.1467)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [72] Total time: 0:00:50 (0.9380 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.7483 (1.7797)  loss_scale: 32768.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8336 (7.9783)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8897 (0.8897)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9405 (1.9405 -- 1.9405)  data: 1.7367 (1.7367 -- 1.7367)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7245 (0.7574)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (93.9024)  time: 0.3318 (0.0221 -- 1.9405)  data: 0.1738 (0.0001 -- 1.7367)  max mem: 16413
Val: Total time: 0:00:03 (0.3319 s / it)
* Acc@1 83.537 Acc@5 94.512 loss 0.878
Accuracy of the network on the 163 val images: 83.54%
[2023-10-23 21:11:17,180] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 21:11:17,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 21:11:17,182] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 21:11:17,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 21:11:18,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 21:11:18,598] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.54%
Epoch: [73]  [ 0/54]  eta: 0:08:26  lr: 0.000034  min_lr: 0.000001  loss: 1.8143 (1.8143)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6324 (6.6324)  time: 9.3836 (9.3836 -- 9.3836)  data: 8.8583 (8.8583 -- 8.8583)  max mem: 16413
[2023-10-23 21:11:29,043] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3944
[2023-10-23 21:11:29,043] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3944
[2023-10-23 21:11:29,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:11:29,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:11:29,044] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [73]  [20/54]  eta: 0:00:42  lr: 0.000034  min_lr: 0.000001  loss: 1.7112 (1.7118)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9304 (6.9139)  time: 0.8420 (0.5140 -- 4.0861)  data: 0.2722 (0.0003 -- 3.5706)  max mem: 16413
Epoch: [73]  [40/54]  eta: 0:00:14  lr: 0.000034  min_lr: 0.000001  loss: 1.9274 (1.8172)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9129 (7.0157)  time: 0.8434 (0.5224 -- 3.3205)  data: 0.3004 (0.0002 -- 2.8032)  max mem: 16413
Epoch: [73]  [53/54]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.8495 (1.8180)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0573 (7.2304)  time: 0.6141 (0.4951 -- 1.9842)  data: 0.0972 (0.0002 -- 1.4602)  max mem: 16413
Epoch: [73] Total time: 0:00:50 (0.9286 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.8495 (1.7572)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0573 (7.2304)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9055 (0.9055)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0616 (2.0616 -- 2.0616)  data: 1.8854 (1.8854 -- 1.8854)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6530 (0.7479)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3438 (0.0221 -- 2.0616)  data: 0.1886 (0.0001 -- 1.8854)  max mem: 16413
Val: Total time: 0:00:03 (0.3440 s / it)
* Acc@1 81.707 Acc@5 93.902 loss 0.856
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 83.54%
Epoch: [74]  [ 0/54]  eta: 0:06:36  lr: 0.000034  min_lr: 0.000001  loss: 2.1477 (2.1477)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9309 (10.9309)  time: 7.3503 (7.3503 -- 7.3503)  data: 6.8136 (6.8136 -- 6.8136)  max mem: 16413
[2023-10-23 21:12:21,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[8.034483821500753e-07, 8.034483821500753e-07, 1.0712645095334337e-06, 1.0712645095334337e-06, 1.4283526793779117e-06, 1.4283526793779117e-06, 1.9044702391705489e-06, 1.9044702391705489e-06, 2.5392936522273985e-06, 2.5392936522273985e-06, 3.3857248696365312e-06, 3.3857248696365312e-06, 4.514299826182042e-06, 4.514299826182042e-06, 6.019066434909389e-06, 6.019066434909389e-06, 8.02542191321252e-06, 8.02542191321252e-06, 1.0700562550950024e-05, 1.0700562550950024e-05, 1.4267416734600034e-05, 1.4267416734600034e-05, 1.9023222312800043e-05, 1.9023222312800043e-05, 2.5364296417066724e-05, 2.5364296417066724e-05, 3.38190618894223e-05, 3.38190618894223e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 21:12:21,236] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=18.345444492465067, CurrSamplesPerSec=21.696546253987414, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [20/54]  eta: 0:00:42  lr: 0.000034  min_lr: 0.000001  loss: 1.8191 (1.8136)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3984 (7.7950)  time: 0.9550 (0.5251 -- 2.9840)  data: 0.1967 (0.0005 -- 2.0312)  max mem: 16413
Epoch: [74]  [40/54]  eta: 0:00:14  lr: 0.000034  min_lr: 0.000001  loss: 1.8776 (1.8465)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9793 (8.5101)  time: 0.7594 (0.5325 -- 3.5391)  data: 0.0016 (0.0003 -- 0.0060)  max mem: 16413
Epoch: [74]  [53/54]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 2.0191 (1.8911)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9499 (8.5805)  time: 0.6492 (0.4955 -- 1.8854)  data: 0.0011 (0.0003 -- 0.0060)  max mem: 16413
Epoch: [74] Total time: 0:00:49 (0.9191 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 2.0191 (1.8285)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9499 (8.5805)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9001 (0.9001)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0452 (2.0452 -- 2.0452)  data: 1.8655 (1.8655 -- 1.8655)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6996 (0.7642)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (92.6829)  time: 0.3431 (0.0222 -- 2.0452)  data: 0.1866 (0.0001 -- 1.8655)  max mem: 16413
Val: Total time: 0:00:03 (0.3432 s / it)
* Acc@1 82.317 Acc@5 93.902 loss 0.881
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 83.54%
Epoch: [75]  [ 0/54]  eta: 0:07:23  lr: 0.000033  min_lr: 0.000001  loss: 2.0095 (2.0095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2737 (5.2737)  time: 8.2168 (8.2168 -- 8.2168)  data: 5.0817 (5.0817 -- 5.0817)  max mem: 16413
Epoch: [75]  [20/54]  eta: 0:00:42  lr: 0.000033  min_lr: 0.000001  loss: 1.8464 (1.7755)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6739 (7.5396)  time: 0.9137 (0.5232 -- 4.0912)  data: 0.0014 (0.0006 -- 0.0026)  max mem: 16413
[2023-10-23 21:13:33,404] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:13:33,404] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:13:33,409] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:13:33,409] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:13:36,395] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4077
[2023-10-23 21:13:36,395] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4077
[2023-10-23 21:13:36,396] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:13:36,396] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 21:13:36,396] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [75]  [40/54]  eta: 0:00:14  lr: 0.000033  min_lr: 0.000001  loss: 1.6692 (1.7487)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4650 (7.6303)  time: 0.7725 (0.5387 -- 4.1492)  data: 0.0025 (0.0002 -- 0.0117)  max mem: 16413
Epoch: [75]  [53/54]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.6334 (1.7216)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0622 (7.5807)  time: 0.6785 (0.4940 -- 2.7306)  data: 0.0013 (0.0002 -- 0.0117)  max mem: 16413
Epoch: [75] Total time: 0:00:51 (0.9567 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.6334 (1.7644)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0622 (7.5807)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9074 (0.9074)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9549 (1.9549 -- 1.9549)  data: 1.7520 (1.7520 -- 1.7520)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.7232 (0.7648)  acc1: 77.7778 (79.2683)  acc5: 88.8889 (91.4634)  time: 0.3343 (0.0223 -- 1.9549)  data: 0.1753 (0.0001 -- 1.7520)  max mem: 16413
Val: Total time: 0:00:03 (0.3345 s / it)
* Acc@1 81.098 Acc@5 93.902 loss 0.899
Accuracy of the network on the 163 val images: 81.10%
Max accuracy: 83.54%
Epoch: [76]  [ 0/54]  eta: 0:06:29  lr: 0.000033  min_lr: 0.000001  loss: 0.7988 (0.7988)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4605 (5.4605)  time: 7.2096 (7.2096 -- 7.2096)  data: 5.2435 (5.2435 -- 5.2435)  max mem: 16413
[2023-10-23 21:14:10,973] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4110
[2023-10-23 21:14:10,973] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4110
[2023-10-23 21:14:10,973] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:14:10,973] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:14:10,973] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [76]  [20/54]  eta: 0:00:39  lr: 0.000033  min_lr: 0.000001  loss: 1.7470 (1.7001)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5482 (7.3293)  time: 0.8580 (0.5094 -- 2.5985)  data: 0.1860 (0.0003 -- 1.6809)  max mem: 16413
Epoch: [76]  [40/54]  eta: 0:00:14  lr: 0.000033  min_lr: 0.000001  loss: 1.8169 (1.7269)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4893 (7.6146)  time: 0.8961 (0.5225 -- 2.5770)  data: 0.1063 (0.0002 -- 2.0175)  max mem: 16413
Epoch: [76]  [53/54]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.6071 (1.7045)  loss_scale: 8192.0000 (9102.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2114 (7.6693)  time: 0.7429 (0.4953 -- 2.5449)  data: 0.0050 (0.0001 -- 0.0862)  max mem: 16413
Epoch: [76] Total time: 0:00:49 (0.9254 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.6071 (1.7914)  loss_scale: 8192.0000 (9102.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2114 (7.6693)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8935 (0.8935)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0667 (2.0667 -- 2.0667)  data: 1.8819 (1.8819 -- 1.8819)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6979 (0.7395)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3440 (0.0222 -- 2.0667)  data: 0.1883 (0.0001 -- 1.8819)  max mem: 16413
Val: Total time: 0:00:03 (0.3441 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.865
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 83.54%
Epoch: [77]  [ 0/54]  eta: 0:05:04  lr: 0.000033  min_lr: 0.000001  loss: 1.9562 (1.9562)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5940 (6.5940)  time: 5.6450 (5.6450 -- 5.6450)  data: 5.1125 (5.1125 -- 5.1125)  max mem: 16413
Epoch: [77]  [20/54]  eta: 0:00:39  lr: 0.000033  min_lr: 0.000001  loss: 2.0300 (1.9678)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6426 (8.1875)  time: 0.9526 (0.5338 -- 3.1830)  data: 0.0232 (0.0006 -- 0.4345)  max mem: 16413
Epoch: [77]  [40/54]  eta: 0:00:14  lr: 0.000033  min_lr: 0.000001  loss: 1.7717 (1.8432)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7486 (7.6849)  time: 0.8474 (0.5229 -- 2.3069)  data: 0.0686 (0.0005 -- 0.7378)  max mem: 16413
Epoch: [77]  [53/54]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.7905 (1.8308)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7177 (7.3959)  time: 0.7309 (0.4934 -- 2.2431)  data: 0.0417 (0.0001 -- 0.5687)  max mem: 16413
Epoch: [77] Total time: 0:00:49 (0.9231 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.7905 (1.7991)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7177 (7.3959)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8902 (0.8902)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0116 (2.0116 -- 2.0116)  data: 1.8119 (1.8119 -- 1.8119)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6528 (0.7408)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3392 (0.0222 -- 2.0116)  data: 0.1814 (0.0001 -- 1.8119)  max mem: 16413
Val: Total time: 0:00:03 (0.3394 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.864
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 83.54%
Epoch: [78]  [ 0/54]  eta: 0:07:14  lr: 0.000032  min_lr: 0.000001  loss: 1.6485 (1.6485)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2732 (7.2732)  time: 8.0383 (8.0383 -- 8.0383)  data: 6.4110 (6.4110 -- 6.4110)  max mem: 16413
Epoch: [78]  [20/54]  eta: 0:00:40  lr: 0.000032  min_lr: 0.000001  loss: 1.5833 (1.6547)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2539 (7.5768)  time: 0.8630 (0.5200 -- 3.8727)  data: 0.0015 (0.0003 -- 0.0062)  max mem: 16413
[2023-10-23 21:16:19,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:16:19,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:16:19,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 21:16:19,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [78]  [40/54]  eta: 0:00:14  lr: 0.000032  min_lr: 0.000001  loss: 1.7668 (1.7160)  loss_scale: 16384.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1802 (7.4848)  time: 0.8273 (0.5299 -- 1.7059)  data: 0.0013 (0.0005 -- 0.0026)  max mem: 16413
Epoch: [78]  [53/54]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.7806 (1.7259)  loss_scale: 16384.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3523 (7.5740)  time: 0.7179 (0.4951 -- 2.1342)  data: 0.0009 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [78] Total time: 0:00:50 (0.9277 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.7806 (1.7573)  loss_scale: 16384.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3523 (7.5740)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8503 (0.8503)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0323 (2.0323 -- 2.0323)  data: 1.8598 (1.8598 -- 1.8598)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6916 (0.7241)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (92.6829)  time: 0.3414 (0.0223 -- 2.0323)  data: 0.1861 (0.0001 -- 1.8598)  max mem: 16413
Val: Total time: 0:00:03 (0.3415 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.839
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 83.54%
Epoch: [79]  [ 0/54]  eta: 0:06:20  lr: 0.000032  min_lr: 0.000001  loss: 1.3311 (1.3311)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6755 (7.6755)  time: 7.0545 (7.0545 -- 7.0545)  data: 6.2900 (6.2900 -- 6.2900)  max mem: 16413
Epoch: [79]  [20/54]  eta: 0:00:41  lr: 0.000032  min_lr: 0.000001  loss: 1.5749 (1.6392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4586 (7.6777)  time: 0.9250 (0.5343 -- 4.2445)  data: 0.1529 (0.0004 -- 1.6750)  max mem: 16413
Epoch: [79]  [40/54]  eta: 0:00:14  lr: 0.000032  min_lr: 0.000001  loss: 1.7977 (1.6604)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9231 (7.4069)  time: 0.8614 (0.5220 -- 3.8889)  data: 0.0018 (0.0002 -- 0.0061)  max mem: 16413
Epoch: [79]  [53/54]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.8554 (1.7107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2324 (7.9335)  time: 0.6593 (0.4955 -- 2.3605)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [79] Total time: 0:00:52 (0.9667 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.8554 (1.7226)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2324 (7.9335)
[2023-10-23 21:17:33,050] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-10-23 21:17:33,052] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-79/mp_rank_00_model_states.pt
[2023-10-23 21:17:33,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-79/mp_rank_00_model_states.pt...
[2023-10-23 21:17:33,052] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-10-23 21:17:34,070] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-79/mp_rank_00_model_states.pt.
[2023-10-23 21:17:34,070] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8996 (0.8996)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0340 (2.0340 -- 2.0340)  data: 1.8352 (1.8352 -- 1.8352)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6443 (0.7215)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (92.6829)  time: 0.3423 (0.0223 -- 2.0340)  data: 0.1836 (0.0001 -- 1.8352)  max mem: 16413
Val: Total time: 0:00:03 (0.3424 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.858
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 83.54%
Epoch: [80]  [ 0/54]  eta: 0:06:28  lr: 0.000032  min_lr: 0.000001  loss: 1.8307 (1.8307)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3059 (6.3059)  time: 7.1919 (7.1919 -- 7.1919)  data: 5.1060 (5.1060 -- 5.1060)  max mem: 16413
Epoch: [80]  [20/54]  eta: 0:00:38  lr: 0.000032  min_lr: 0.000001  loss: 1.6982 (1.6363)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6475 (7.2303)  time: 0.8309 (0.5216 -- 2.0998)  data: 0.2787 (0.0004 -- 1.5803)  max mem: 16413
Epoch: [80]  [40/54]  eta: 0:00:13  lr: 0.000032  min_lr: 0.000001  loss: 1.8333 (1.6885)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1443 (7.3121)  time: 0.8488 (0.5099 -- 3.3429)  data: 0.2773 (0.0005 -- 2.8229)  max mem: 16413
[2023-10-23 21:18:23,333] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:18:23,333] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:18:23,333] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:18:23,333] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [80]  [53/54]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.8333 (1.7536)  loss_scale: 16384.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6397 (7.3111)  time: 0.7135 (0.4971 -- 2.0950)  data: 0.1307 (0.0002 -- 1.5923)  max mem: 16413
Epoch: [80] Total time: 0:00:49 (0.9202 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.8333 (1.7631)  loss_scale: 16384.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6397 (7.3111)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8931 (0.8931)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9647 (1.9647 -- 1.9647)  data: 1.7590 (1.7590 -- 1.7590)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6505 (0.7154)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3340 (0.0222 -- 1.9647)  data: 0.1760 (0.0001 -- 1.7590)  max mem: 16413
Val: Total time: 0:00:03 (0.3342 s / it)
* Acc@1 83.537 Acc@5 93.902 loss 0.845
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 83.54%
Epoch: [81]  [ 0/54]  eta: 0:06:41  lr: 0.000031  min_lr: 0.000001  loss: 1.7451 (1.7451)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3406 (8.3406)  time: 7.4396 (7.4396 -- 7.4396)  data: 6.9201 (6.9201 -- 6.9201)  max mem: 16413
[2023-10-23 21:18:41,456] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4380
[2023-10-23 21:18:41,456] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4380
[2023-10-23 21:18:41,456] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:18:41,456] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:18:41,456] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [81]  [20/54]  eta: 0:00:37  lr: 0.000031  min_lr: 0.000001  loss: 1.6567 (1.6883)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4519 (7.5682)  time: 0.7926 (0.5202 -- 3.0186)  data: 0.2362 (0.0002 -- 2.5008)  max mem: 16413
Epoch: [81]  [40/54]  eta: 0:00:14  lr: 0.000031  min_lr: 0.000001  loss: 1.8932 (1.7625)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6476 (7.9285)  time: 0.9575 (0.5205 -- 3.8508)  data: 0.3652 (0.0004 -- 3.3177)  max mem: 16413
Epoch: [81]  [53/54]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.7146 (1.7611)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3698 (8.0879)  time: 0.6504 (0.4963 -- 2.4313)  data: 0.0551 (0.0001 -- 1.0877)  max mem: 16413
Epoch: [81] Total time: 0:00:49 (0.9218 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.7146 (1.7733)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3698 (8.0879)
Val:  [ 0/10]  eta: 0:00:21  loss: 0.9002 (0.9002)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.1020 (2.1020 -- 2.1020)  data: 1.9308 (1.9308 -- 1.9308)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5989 (0.7136)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (92.6829)  time: 0.3483 (0.0222 -- 2.1020)  data: 0.1932 (0.0001 -- 1.9308)  max mem: 16413
Val: Total time: 0:00:03 (0.3500 s / it)
* Acc@1 82.317 Acc@5 93.293 loss 0.845
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 83.54%
Epoch: [82]  [ 0/54]  eta: 0:07:19  lr: 0.000031  min_lr: 0.000001  loss: 1.8811 (1.8811)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6594 (7.6594)  time: 8.1370 (8.1370 -- 8.1370)  data: 7.6025 (7.6025 -- 7.6025)  max mem: 16413
[2023-10-23 21:19:47,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4448
[2023-10-23 21:19:47,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4448
[2023-10-23 21:19:47,546] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:19:47,546] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:19:47,546] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [82]  [20/54]  eta: 0:00:38  lr: 0.000031  min_lr: 0.000001  loss: 1.7907 (1.8038)  loss_scale: 16384.0000 (15993.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2953 (7.6585)  time: 0.7722 (0.5233 -- 3.3548)  data: 0.2227 (0.0003 -- 2.8241)  max mem: 16413
Epoch: [82]  [40/54]  eta: 0:00:14  lr: 0.000031  min_lr: 0.000001  loss: 1.7674 (1.7521)  loss_scale: 8192.0000 (12188.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2542 (8.2627)  time: 0.9846 (0.5255 -- 2.9529)  data: 0.1186 (0.0003 -- 0.9714)  max mem: 16413
Epoch: [82]  [53/54]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.7836 (1.7580)  loss_scale: 8192.0000 (11226.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2542 (8.2451)  time: 0.6425 (0.4967 -- 2.8990)  data: 0.0006 (0.0001 -- 0.0031)  max mem: 16413
Epoch: [82] Total time: 0:00:49 (0.9254 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.7836 (1.7926)  loss_scale: 8192.0000 (11226.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2542 (8.2451)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8899 (0.8899)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0018 (2.0018 -- 2.0018)  data: 1.7809 (1.7809 -- 1.7809)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6246 (0.7225)  acc1: 77.7778 (82.9268)  acc5: 100.0000 (91.4634)  time: 0.3377 (0.0221 -- 2.0018)  data: 0.1782 (0.0001 -- 1.7809)  max mem: 16413
Val: Total time: 0:00:03 (0.3378 s / it)
* Acc@1 83.537 Acc@5 93.293 loss 0.858
Accuracy of the network on the 163 val images: 83.54%
[2023-10-23 21:20:17,425] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 21:20:17,426] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 21:20:17,426] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 21:20:17,426] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 21:20:18,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 21:20:18,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.54%
Epoch: [83]  [ 0/54]  eta: 0:06:58  lr: 0.000031  min_lr: 0.000001  loss: 2.1136 (2.1136)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3466 (10.3466)  time: 7.7489 (7.7489 -- 7.7489)  data: 5.4563 (5.4563 -- 5.4563)  max mem: 16413
Epoch: [83]  [20/54]  eta: 0:00:38  lr: 0.000031  min_lr: 0.000001  loss: 1.8166 (1.7755)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1505 (8.5673)  time: 0.7935 (0.5167 -- 1.7911)  data: 0.2488 (0.0007 -- 1.2585)  max mem: 16413
Epoch: [83]  [40/54]  eta: 0:00:14  lr: 0.000030  min_lr: 0.000001  loss: 1.7198 (1.7542)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2091 (8.1665)  time: 0.8920 (0.5227 -- 3.2099)  data: 0.2364 (0.0004 -- 2.0532)  max mem: 16413
Epoch: [83]  [53/54]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.8325 (1.7805)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0941 (8.0364)  time: 0.8116 (0.4954 -- 3.2099)  data: 0.1622 (0.0002 -- 1.7942)  max mem: 16413
Epoch: [83] Total time: 0:00:51 (0.9488 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.8325 (1.7812)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0941 (8.0364)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9393 (0.9393)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9630 (1.9630 -- 1.9630)  data: 1.7590 (1.7590 -- 1.7590)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6047 (0.7292)  acc1: 77.7778 (80.4878)  acc5: 88.8889 (91.4634)  time: 0.3345 (0.0222 -- 1.9630)  data: 0.1760 (0.0001 -- 1.7590)  max mem: 16413
Val: Total time: 0:00:03 (0.3347 s / it)
* Acc@1 81.707 Acc@5 93.293 loss 0.874
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 83.54%
Epoch: [84]  [ 0/54]  eta: 0:07:06  lr: 0.000030  min_lr: 0.000001  loss: 1.6103 (1.6103)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7134 (7.7134)  time: 7.9015 (7.9015 -- 7.9015)  data: 7.0062 (7.0062 -- 7.0062)  max mem: 16413
Epoch: [84]  [20/54]  eta: 0:00:42  lr: 0.000030  min_lr: 0.000001  loss: 1.9284 (1.8904)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4330 (8.2781)  time: 0.9081 (0.5199 -- 5.3294)  data: 0.0084 (0.0004 -- 0.1377)  max mem: 16413
Epoch: [84]  [40/54]  eta: 0:00:14  lr: 0.000030  min_lr: 0.000001  loss: 1.8889 (1.8585)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5491 (8.1805)  time: 0.8542 (0.5228 -- 3.0946)  data: 0.0012 (0.0002 -- 0.0025)  max mem: 16413
[2023-10-23 21:21:57,300] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:21:57,300] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 21:21:57,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:21:57,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [84]  [53/54]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9047 (1.8552)  loss_scale: 16384.0000 (10164.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4917 (8.1109)  time: 0.7044 (0.4939 -- 3.0946)  data: 0.0008 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [84] Total time: 0:00:50 (0.9418 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9047 (1.8150)  loss_scale: 16384.0000 (10164.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4917 (8.1109)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8925 (0.8925)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0442 (2.0442 -- 2.0442)  data: 1.8540 (1.8540 -- 1.8540)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6455 (0.7082)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (92.6829)  time: 0.3421 (0.0222 -- 2.0442)  data: 0.1855 (0.0001 -- 1.8540)  max mem: 16413
Val: Total time: 0:00:03 (0.3422 s / it)
* Acc@1 83.537 Acc@5 93.902 loss 0.837
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 83.54%
Epoch: [85]  [ 0/54]  eta: 0:07:45  lr: 0.000030  min_lr: 0.000001  loss: 1.9228 (1.9228)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6549 (7.6549)  time: 8.6145 (8.6145 -- 8.6145)  data: 8.0939 (8.0939 -- 8.0939)  max mem: 16413
Epoch: [85]  [20/54]  eta: 0:00:38  lr: 0.000030  min_lr: 0.000001  loss: 1.7503 (1.7588)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6679 (8.3285)  time: 0.7636 (0.5233 -- 3.3249)  data: 0.2204 (0.0005 -- 2.8164)  max mem: 16413
Epoch: [85]  [40/54]  eta: 0:00:14  lr: 0.000030  min_lr: 0.000001  loss: 1.7363 (1.7644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5386 (8.1143)  time: 0.9830 (0.5324 -- 4.1010)  data: 0.2847 (0.0005 -- 3.5371)  max mem: 16413
Epoch: [85]  [53/54]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.7776 (1.8027)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0707 (8.0706)  time: 0.6894 (0.4952 -- 3.8927)  data: 0.0545 (0.0001 -- 1.0734)  max mem: 16413
Epoch: [85] Total time: 0:00:50 (0.9284 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.7776 (1.7271)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0707 (8.0706)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8751 (0.8751)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0678 (2.0678 -- 2.0678)  data: 1.8659 (1.8659 -- 1.8659)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6558 (0.7112)  acc1: 77.7778 (82.9268)  acc5: 100.0000 (92.6829)  time: 0.3448 (0.0222 -- 2.0678)  data: 0.1867 (0.0001 -- 1.8659)  max mem: 16413
Val: Total time: 0:00:03 (0.3450 s / it)
* Acc@1 84.146 Acc@5 93.902 loss 0.828
Accuracy of the network on the 163 val images: 84.15%
[2023-10-23 21:23:01,494] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 21:23:01,496] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 21:23:01,496] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 21:23:01,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 21:23:02,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 21:23:02,978] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.15%
Epoch: [86]  [ 0/54]  eta: 0:06:26  lr: 0.000030  min_lr: 0.000001  loss: 1.3770 (1.3770)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2324 (10.2324)  time: 7.1536 (7.1536 -- 7.1536)  data: 5.2511 (5.2511 -- 5.2511)  max mem: 16413
Epoch: [86]  [20/54]  eta: 0:00:39  lr: 0.000029  min_lr: 0.000001  loss: 1.8034 (1.8212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9892 (7.7237)  time: 0.8553 (0.5322 -- 2.5915)  data: 0.0468 (0.0005 -- 0.3508)  max mem: 16413
Epoch: [86]  [40/54]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000001  loss: 1.9241 (1.8342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6822 (8.4629)  time: 0.8725 (0.5197 -- 2.5776)  data: 0.0431 (0.0003 -- 0.4388)  max mem: 16413
Epoch: [86]  [53/54]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.9421 (1.8339)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3425 (8.5620)  time: 0.8031 (0.4970 -- 2.5959)  data: 0.0283 (0.0002 -- 0.4388)  max mem: 16413
Epoch: [86] Total time: 0:00:51 (0.9601 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.9421 (1.8073)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3425 (8.5620)
Val:  [ 0/10]  eta: 0:00:18  loss: 0.9219 (0.9219)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.8800 (1.8800 -- 1.8800)  data: 1.6638 (1.6638 -- 1.6638)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6355 (0.7104)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3321 (0.0222 -- 1.8800)  data: 0.1722 (0.0001 -- 1.6638)  max mem: 16413
Val: Total time: 0:00:03 (0.3323 s / it)
* Acc@1 82.927 Acc@5 93.902 loss 0.845
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [87]  [ 0/54]  eta: 0:06:54  lr: 0.000029  min_lr: 0.000001  loss: 1.7648 (1.7648)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0074 (8.0074)  time: 7.6728 (7.6728 -- 7.6728)  data: 7.1402 (7.1402 -- 7.1402)  max mem: 16413
[2023-10-23 21:24:10,175] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:24:10,176] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:24:10,182] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:24:10,182] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:24:21,209] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4718
[2023-10-23 21:24:21,209] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:24:21,209] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4718
[2023-10-23 21:24:21,209] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:24:21,209] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [87]  [20/54]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 1.8239 (1.8681)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7990 (7.7215)  time: 0.7627 (0.5145 -- 2.8798)  data: 0.2221 (0.0005 -- 2.3672)  max mem: 16413
Epoch: [87]  [40/54]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000001  loss: 1.9289 (1.9027)  loss_scale: 16384.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5536 (7.5981)  time: 0.9831 (0.5303 -- 2.3986)  data: 0.4148 (0.0004 -- 1.8205)  max mem: 16413
Epoch: [87]  [53/54]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.8426 (1.8614)  loss_scale: 16384.0000 (20328.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5973 (7.7160)  time: 0.7151 (0.4962 -- 2.0151)  data: 0.1590 (0.0001 -- 1.4277)  max mem: 16413
Epoch: [87] Total time: 0:00:50 (0.9290 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.8426 (1.7889)  loss_scale: 16384.0000 (20328.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5973 (7.7160)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9445 (0.9445)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0693 (2.0693 -- 2.0693)  data: 1.8798 (1.8798 -- 1.8798)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6460 (0.7265)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3451 (0.0222 -- 2.0693)  data: 0.1881 (0.0001 -- 1.8798)  max mem: 16413
Val: Total time: 0:00:03 (0.3452 s / it)
* Acc@1 81.707 Acc@5 93.902 loss 0.857
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [88]  [ 0/54]  eta: 0:07:06  lr: 0.000029  min_lr: 0.000001  loss: 1.8326 (1.8326)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3424 (7.3424)  time: 7.9073 (7.9073 -- 7.9073)  data: 6.4293 (6.4293 -- 6.4293)  max mem: 16413
Epoch: [88]  [20/54]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 1.6910 (1.7738)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3946 (7.6888)  time: 0.7673 (0.5262 -- 2.5729)  data: 0.1352 (0.0002 -- 2.0182)  max mem: 16413
Epoch: [88]  [40/54]  eta: 0:00:14  lr: 0.000029  min_lr: 0.000001  loss: 1.7791 (1.7653)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6051 (8.2709)  time: 0.9486 (0.5092 -- 3.2198)  data: 0.4014 (0.0005 -- 2.6955)  max mem: 16413
Epoch: [88]  [53/54]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.7063 (1.7532)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6745 (7.9393)  time: 0.6760 (0.4951 -- 2.7962)  data: 0.1546 (0.0002 -- 2.2703)  max mem: 16413
Epoch: [88] Total time: 0:00:49 (0.9206 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.7063 (1.7470)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6745 (7.9393)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9505 (0.9505)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9758 (1.9758 -- 1.9758)  data: 1.7806 (1.7806 -- 1.7806)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5584 (0.7162)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (92.6829)  time: 0.3366 (0.0223 -- 1.9758)  data: 0.1793 (0.0001 -- 1.7806)  max mem: 16413
Val: Total time: 0:00:03 (0.3367 s / it)
* Acc@1 81.707 Acc@5 93.902 loss 0.866
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [89]  [ 0/54]  eta: 0:06:56  lr: 0.000029  min_lr: 0.000001  loss: 1.6374 (1.6374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4987 (8.4987)  time: 7.7180 (7.7180 -- 7.7180)  data: 5.5713 (5.5713 -- 5.5713)  max mem: 16413
Epoch: [89]  [20/54]  eta: 0:00:39  lr: 0.000028  min_lr: 0.000001  loss: 1.6720 (1.6817)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4533 (7.1538)  time: 0.8325 (0.5309 -- 3.3373)  data: 0.2437 (0.0004 -- 2.8046)  max mem: 16413
Epoch: [89]  [40/54]  eta: 0:00:14  lr: 0.000028  min_lr: 0.000001  loss: 1.7618 (1.7054)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8786 (7.3638)  time: 0.8857 (0.5195 -- 2.5057)  data: 0.2055 (0.0004 -- 1.9627)  max mem: 16413
[2023-10-23 21:26:27,674] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:26:27,674] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:26:27,675] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:26:27,675] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:26:28,734] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4849
[2023-10-23 21:26:28,734] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:26:28,734] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4849
[2023-10-23 21:26:28,734] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:26:28,734] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [89]  [53/54]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.6077 (1.6826)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8786 (7.5395)  time: 0.7845 (0.4957 -- 2.4974)  data: 0.0683 (0.0002 -- 1.3529)  max mem: 16413
Epoch: [89] Total time: 0:00:50 (0.9379 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.6077 (1.7459)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8786 (7.5395)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9067 (0.9067)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0088 (2.0088 -- 2.0088)  data: 1.8233 (1.8233 -- 1.8233)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6193 (0.7325)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3402 (0.0225 -- 2.0088)  data: 0.1836 (0.0001 -- 1.8233)  max mem: 16413
Val: Total time: 0:00:03 (0.3403 s / it)
* Acc@1 81.707 Acc@5 93.902 loss 0.874
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [90]  [ 0/54]  eta: 0:06:27  lr: 0.000028  min_lr: 0.000001  loss: 1.1260 (1.1260)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1707 (6.1707)  time: 7.1688 (7.1688 -- 7.1688)  data: 5.4636 (5.4636 -- 5.4636)  max mem: 16413
Epoch: [90]  [20/54]  eta: 0:00:38  lr: 0.000028  min_lr: 0.000001  loss: 1.5542 (1.6157)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3376 (7.4790)  time: 0.8292 (0.5204 -- 3.1690)  data: 0.1382 (0.0004 -- 2.6329)  max mem: 16413
Epoch: [90]  [40/54]  eta: 0:00:14  lr: 0.000028  min_lr: 0.000001  loss: 1.7861 (1.6829)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4130 (7.4828)  time: 0.9010 (0.5209 -- 4.1602)  data: 0.0841 (0.0003 -- 1.4434)  max mem: 16413
Epoch: [90]  [53/54]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.6517 (1.6811)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3379 (7.5432)  time: 0.6926 (0.4944 -- 3.2396)  data: 0.0006 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [90] Total time: 0:00:51 (0.9609 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.6517 (1.6784)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3379 (7.5432)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9051 (0.9051)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0162 (2.0162 -- 2.0162)  data: 1.8274 (1.8274 -- 1.8274)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6341 (0.7193)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3390 (0.0223 -- 2.0162)  data: 0.1828 (0.0001 -- 1.8274)  max mem: 16413
Val: Total time: 0:00:03 (0.3391 s / it)
* Acc@1 81.707 Acc@5 93.902 loss 0.855
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [91]  [ 0/54]  eta: 0:07:09  lr: 0.000028  min_lr: 0.000001  loss: 1.8000 (1.8000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4783 (6.4783)  time: 7.9624 (7.9624 -- 7.9624)  data: 5.8954 (5.8954 -- 5.8954)  max mem: 16413
Epoch: [91]  [20/54]  eta: 0:00:39  lr: 0.000028  min_lr: 0.000001  loss: 1.9143 (1.8649)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0360 (7.6881)  time: 0.8353 (0.5328 -- 3.7216)  data: 0.1410 (0.0008 -- 1.5262)  max mem: 16413
Epoch: [91]  [40/54]  eta: 0:00:14  lr: 0.000027  min_lr: 0.000001  loss: 1.7515 (1.8178)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8172 (7.7321)  time: 0.9301 (0.5356 -- 3.1667)  data: 0.2901 (0.0004 -- 2.4584)  max mem: 16413
Epoch: [91]  [53/54]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.5649 (1.7936)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9286 (7.9334)  time: 0.6602 (0.4951 -- 2.9699)  data: 0.1363 (0.0001 -- 2.4584)  max mem: 16413
Epoch: [91] Total time: 0:00:50 (0.9300 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.5649 (1.7449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9286 (7.9334)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8833 (0.8833)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9942 (1.9942 -- 1.9942)  data: 1.7975 (1.7975 -- 1.7975)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6282 (0.7004)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3382 (0.0224 -- 1.9942)  data: 0.1798 (0.0001 -- 1.7975)  max mem: 16413
Val: Total time: 0:00:03 (0.3383 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.836
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [92]  [ 0/54]  eta: 0:05:57  lr: 0.000027  min_lr: 0.000001  loss: 2.4018 (2.4018)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.5686 (14.5686)  time: 6.6257 (6.6257 -- 6.6257)  data: 5.7956 (5.7956 -- 5.7956)  max mem: 16413
[2023-10-23 21:28:44,517] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:28:44,517] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:28:44,517] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:28:44,517] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [92]  [20/54]  eta: 0:00:38  lr: 0.000027  min_lr: 0.000001  loss: 1.7783 (1.8391)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1076 (7.7129)  time: 0.8620 (0.5246 -- 2.2543)  data: 0.2312 (0.0006 -- 1.7109)  max mem: 16413
[2023-10-23 21:28:58,610] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4993
[2023-10-23 21:28:58,610] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4993
[2023-10-23 21:28:58,610] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:28:58,610] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:28:58,611] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 21:29:01,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=25, lr=[6.459418196123603e-07, 6.459418196123603e-07, 8.61255759483147e-07, 8.61255759483147e-07, 1.148341012644196e-06, 1.148341012644196e-06, 1.5311213501922614e-06, 1.5311213501922614e-06, 2.0414951335896818e-06, 2.0414951335896818e-06, 2.721993511452909e-06, 2.721993511452909e-06, 3.629324681937212e-06, 3.629324681937212e-06, 4.839099575916283e-06, 4.839099575916283e-06, 6.452132767888377e-06, 6.452132767888377e-06, 8.602843690517836e-06, 8.602843690517836e-06, 1.1470458254023781e-05, 1.1470458254023781e-05, 1.5293944338698375e-05, 1.5293944338698375e-05, 2.0391925784931167e-05, 2.0391925784931167e-05, 2.7189234379908223e-05, 2.7189234379908223e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 21:29:01,855] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=18.166900505584454, CurrSamplesPerSec=22.50356253755687, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [92]  [40/54]  eta: 0:00:14  lr: 0.000027  min_lr: 0.000001  loss: 1.7359 (1.7777)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9752 (7.9956)  time: 0.9531 (0.5277 -- 4.9988)  data: 0.4066 (0.0003 -- 4.4501)  max mem: 16413
Epoch: [92]  [53/54]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.7243 (1.7622)  loss_scale: 16384.0000 (20935.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6843 (7.7107)  time: 0.5970 (0.4946 -- 1.8145)  data: 0.0793 (0.0002 -- 1.2837)  max mem: 16413
Epoch: [92] Total time: 0:00:51 (0.9460 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.7243 (1.7501)  loss_scale: 16384.0000 (20935.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6843 (7.7107)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8699 (0.8699)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0492 (2.0492 -- 2.0492)  data: 1.8688 (1.8688 -- 1.8688)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6419 (0.6967)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3429 (0.0222 -- 2.0492)  data: 0.1870 (0.0001 -- 1.8688)  max mem: 16413
Val: Total time: 0:00:03 (0.3430 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.834
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [93]  [ 0/54]  eta: 0:06:35  lr: 0.000027  min_lr: 0.000001  loss: 1.3189 (1.3189)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2473 (5.2473)  time: 7.3305 (7.3305 -- 7.3305)  data: 6.8227 (6.8227 -- 6.8227)  max mem: 16413
Epoch: [93]  [20/54]  eta: 0:00:39  lr: 0.000027  min_lr: 0.000001  loss: 1.7450 (1.7512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3521 (7.6095)  time: 0.8651 (0.5240 -- 2.6149)  data: 0.3243 (0.0004 -- 2.0866)  max mem: 16413
Epoch: [93]  [40/54]  eta: 0:00:14  lr: 0.000027  min_lr: 0.000001  loss: 1.6003 (1.7067)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7246 (7.2263)  time: 0.8251 (0.5198 -- 2.3658)  data: 0.2821 (0.0004 -- 1.8280)  max mem: 16413
Epoch: [93]  [53/54]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.7744 (1.7107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7052 (7.2879)  time: 0.6962 (0.4951 -- 1.8869)  data: 0.1788 (0.0002 -- 1.3573)  max mem: 16413
Epoch: [93] Total time: 0:00:49 (0.9233 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.7744 (1.7412)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7052 (7.2879)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8552 (0.8552)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0074 (2.0074 -- 2.0074)  data: 1.8041 (1.8041 -- 1.8041)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5950 (0.6905)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3384 (0.0222 -- 2.0074)  data: 0.1805 (0.0001 -- 1.8041)  max mem: 16413
Val: Total time: 0:00:03 (0.3385 s / it)
* Acc@1 82.927 Acc@5 95.122 loss 0.819
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [94]  [ 0/54]  eta: 0:06:58  lr: 0.000027  min_lr: 0.000001  loss: 0.9834 (0.9834)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3534 (7.3534)  time: 7.7542 (7.7542 -- 7.7542)  data: 4.9707 (4.9707 -- 4.9707)  max mem: 16413
[2023-10-23 21:30:31,416] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5085
[2023-10-23 21:30:31,416] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5085
[2023-10-23 21:30:31,416] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:30:31,416] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:30:31,416] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [94]  [20/54]  eta: 0:00:43  lr: 0.000027  min_lr: 0.000001  loss: 1.7136 (1.6619)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5262 (6.8985)  time: 0.9497 (0.5366 -- 2.7319)  data: 0.3819 (0.0003 -- 2.2042)  max mem: 16413
Epoch: [94]  [40/54]  eta: 0:00:14  lr: 0.000026  min_lr: 0.000001  loss: 1.7171 (1.7075)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5654 (7.2583)  time: 0.7142 (0.5258 -- 1.7718)  data: 0.1643 (0.0004 -- 1.2422)  max mem: 16413
Epoch: [94]  [53/54]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.6782 (1.7114)  loss_scale: 8192.0000 (9557.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4152 (7.1837)  time: 0.6851 (0.4957 -- 3.1078)  data: 0.1637 (0.0003 -- 2.6079)  max mem: 16413
Epoch: [94] Total time: 0:00:50 (0.9422 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.6782 (1.7545)  loss_scale: 8192.0000 (9557.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4152 (7.1837)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8618 (0.8618)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9651 (1.9651 -- 1.9651)  data: 1.7596 (1.7596 -- 1.7596)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6143 (0.6926)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3351 (0.0222 -- 1.9651)  data: 0.1761 (0.0001 -- 1.7596)  max mem: 16413
Val: Total time: 0:00:03 (0.3352 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.815
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [95]  [ 0/54]  eta: 0:06:35  lr: 0.000026  min_lr: 0.000001  loss: 2.0012 (2.0012)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9341 (7.9341)  time: 7.3323 (7.3323 -- 7.3323)  data: 6.7654 (6.7654 -- 6.7654)  max mem: 16413
Epoch: [95]  [20/54]  eta: 0:00:39  lr: 0.000026  min_lr: 0.000001  loss: 1.7449 (1.7548)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2434 (7.8238)  time: 0.8464 (0.5234 -- 2.1782)  data: 0.1290 (0.0002 -- 1.6459)  max mem: 16413
Epoch: [95]  [40/54]  eta: 0:00:15  lr: 0.000026  min_lr: 0.000001  loss: 1.8138 (1.7701)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9988 (7.7095)  time: 0.9892 (0.5233 -- 4.4090)  data: 0.4510 (0.0004 -- 3.8916)  max mem: 16413
Epoch: [95]  [53/54]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.7250 (1.7841)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2614 (7.5820)  time: 0.6961 (0.4945 -- 2.8476)  data: 0.1761 (0.0002 -- 2.3141)  max mem: 16413
Epoch: [95] Total time: 0:00:51 (0.9603 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.7250 (1.7830)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2614 (7.5820)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8573 (0.8573)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9441 (1.9441 -- 1.9441)  data: 1.7395 (1.7395 -- 1.7395)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6152 (0.6849)  acc1: 77.7778 (82.9268)  acc5: 100.0000 (93.9024)  time: 0.3324 (0.0224 -- 1.9441)  data: 0.1741 (0.0001 -- 1.7395)  max mem: 16413
Val: Total time: 0:00:03 (0.3325 s / it)
* Acc@1 83.537 Acc@5 94.512 loss 0.814
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [96]  [ 0/54]  eta: 0:08:14  lr: 0.000026  min_lr: 0.000001  loss: 2.5889 (2.5889)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8449 (6.8449)  time: 9.1556 (9.1556 -- 9.1556)  data: 8.5751 (8.5751 -- 8.5751)  max mem: 16413
Epoch: [96]  [20/54]  eta: 0:00:40  lr: 0.000026  min_lr: 0.000001  loss: 1.7120 (1.7995)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1104 (7.4435)  time: 0.7810 (0.5217 -- 2.8942)  data: 0.2323 (0.0004 -- 2.3593)  max mem: 16413
[2023-10-23 21:32:36,945] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:32:36,945] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:32:36,945] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 21:32:36,945] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [96]  [40/54]  eta: 0:00:13  lr: 0.000026  min_lr: 0.000001  loss: 1.8263 (1.8228)  loss_scale: 16384.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5800 (7.6769)  time: 0.7917 (0.5315 -- 2.5808)  data: 0.1959 (0.0004 -- 2.0572)  max mem: 16413
Epoch: [96]  [53/54]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.7489 (1.8027)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3312 (7.7425)  time: 0.7156 (0.4987 -- 1.4904)  data: 0.1050 (0.0002 -- 0.9483)  max mem: 16413
Epoch: [96] Total time: 0:00:48 (0.9072 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.7489 (1.7857)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3312 (7.7425)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8612 (0.8612)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0596 (2.0596 -- 2.0596)  data: 1.8861 (1.8861 -- 1.8861)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.4958 (0.6667)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (92.6829)  time: 0.3443 (0.0222 -- 2.0596)  data: 0.1887 (0.0001 -- 1.8861)  max mem: 16413
Val: Total time: 0:00:03 (0.3444 s / it)
* Acc@1 82.927 Acc@5 93.293 loss 0.787
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [97]  [ 0/54]  eta: 0:06:27  lr: 0.000026  min_lr: 0.000001  loss: 1.8500 (1.8500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8554 (7.8554)  time: 7.1815 (7.1815 -- 7.1815)  data: 6.2541 (6.2541 -- 6.2541)  max mem: 16413
Epoch: [97]  [20/54]  eta: 0:00:39  lr: 0.000025  min_lr: 0.000001  loss: 1.8358 (1.8605)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5169 (7.6625)  time: 0.8666 (0.5169 -- 3.6252)  data: 0.3126 (0.0004 -- 3.0634)  max mem: 16413
Epoch: [97]  [40/54]  eta: 0:00:13  lr: 0.000025  min_lr: 0.000001  loss: 1.6653 (1.7917)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7699 (7.8026)  time: 0.8104 (0.5325 -- 2.3987)  data: 0.1699 (0.0004 -- 1.3628)  max mem: 16413
Epoch: [97]  [53/54]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.7486 (1.7686)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5934 (8.2178)  time: 0.8233 (0.4965 -- 3.2265)  data: 0.0444 (0.0001 -- 0.8518)  max mem: 16413
Epoch: [97] Total time: 0:00:50 (0.9386 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.7486 (1.6971)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5934 (8.2178)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8832 (0.8832)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9511 (1.9511 -- 1.9511)  data: 1.7237 (1.7237 -- 1.7237)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5283 (0.6758)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (93.9024)  time: 0.3368 (0.0225 -- 1.9511)  data: 0.1763 (0.0001 -- 1.7237)  max mem: 16413
Val: Total time: 0:00:03 (0.3369 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.807
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [98]  [ 0/54]  eta: 0:07:27  lr: 0.000025  min_lr: 0.000001  loss: 1.4837 (1.4837)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7257 (8.7257)  time: 8.2863 (8.2863 -- 8.2863)  data: 7.7490 (7.7490 -- 7.7490)  max mem: 16413
Epoch: [98]  [20/54]  eta: 0:00:39  lr: 0.000025  min_lr: 0.000001  loss: 1.7986 (1.7563)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1762 (7.5037)  time: 0.8118 (0.5179 -- 4.0324)  data: 0.2364 (0.0004 -- 3.5002)  max mem: 16413
Epoch: [98]  [40/54]  eta: 0:00:14  lr: 0.000025  min_lr: 0.000001  loss: 1.7256 (1.7418)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6882 (7.8777)  time: 0.8408 (0.5248 -- 4.2308)  data: 0.2952 (0.0003 -- 3.6707)  max mem: 16413
[2023-10-23 21:34:40,510] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:34:40,510] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:34:40,510] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:34:40,510] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [98]  [53/54]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.7867 (1.7526)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8872 (7.7768)  time: 0.6415 (0.4926 -- 1.8108)  data: 0.1123 (0.0002 -- 1.3192)  max mem: 16413
Epoch: [98] Total time: 0:00:49 (0.9146 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.7867 (1.7402)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8872 (7.7768)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8598 (0.8598)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0349 (2.0349 -- 2.0349)  data: 1.8390 (1.8390 -- 1.8390)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5218 (0.6750)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3415 (0.0221 -- 2.0349)  data: 0.1840 (0.0001 -- 1.8390)  max mem: 16413
Val: Total time: 0:00:03 (0.3416 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.798
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [99]  [ 0/54]  eta: 0:08:38  lr: 0.000025  min_lr: 0.000001  loss: 1.8386 (1.8386)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4264 (11.4264)  time: 9.5964 (9.5964 -- 9.5964)  data: 5.6614 (5.6614 -- 5.6614)  max mem: 16413
Epoch: [99]  [20/54]  eta: 0:00:42  lr: 0.000025  min_lr: 0.000001  loss: 1.6714 (1.7256)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9542 (7.3988)  time: 0.8230 (0.5236 -- 3.7086)  data: 0.0016 (0.0009 -- 0.0028)  max mem: 16413
[2023-10-23 21:35:17,387] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5373
[2023-10-23 21:35:17,387] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5373
[2023-10-23 21:35:17,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:35:17,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:35:17,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [99]  [40/54]  eta: 0:00:14  lr: 0.000025  min_lr: 0.000001  loss: 1.7590 (1.7013)  loss_scale: 16384.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3501 (7.4995)  time: 0.8079 (0.5239 -- 2.5098)  data: 0.0125 (0.0002 -- 0.1325)  max mem: 16413
Epoch: [99]  [53/54]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.8218 (1.7383)  loss_scale: 16384.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6477 (7.5124)  time: 0.6956 (0.4960 -- 1.7864)  data: 0.1101 (0.0002 -- 1.2697)  max mem: 16413
Epoch: [99] Total time: 0:00:50 (0.9441 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.8218 (1.7270)  loss_scale: 16384.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6477 (7.5124)
[2023-10-23 21:35:36,443] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-10-23 21:35:36,446] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-10-23 21:35:36,448] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-99/mp_rank_00_model_states.pt
[2023-10-23 21:35:36,448] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-99/mp_rank_00_model_states.pt...
[2023-10-23 21:35:37,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-99/mp_rank_00_model_states.pt.
[2023-10-23 21:35:37,618] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8861 (0.8861)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9779 (1.9779 -- 1.9779)  data: 1.7690 (1.7690 -- 1.7690)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5558 (0.6775)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3355 (0.0222 -- 1.9779)  data: 0.1770 (0.0001 -- 1.7690)  max mem: 16413
Val: Total time: 0:00:03 (0.3357 s / it)
* Acc@1 83.537 Acc@5 94.512 loss 0.807
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [100]  [ 0/54]  eta: 0:06:25  lr: 0.000024  min_lr: 0.000001  loss: 2.1141 (2.1141)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3798 (7.3798)  time: 7.1328 (7.1328 -- 7.1328)  data: 6.4941 (6.4941 -- 6.4941)  max mem: 16413
Epoch: [100]  [20/54]  eta: 0:00:40  lr: 0.000024  min_lr: 0.000001  loss: 1.7631 (1.7933)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3386 (8.3571)  time: 0.8911 (0.5328 -- 4.7200)  data: 0.3432 (0.0006 -- 4.1892)  max mem: 16413
Epoch: [100]  [40/54]  eta: 0:00:14  lr: 0.000024  min_lr: 0.000001  loss: 1.6710 (1.6981)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5601 (8.4585)  time: 0.8736 (0.5242 -- 2.9134)  data: 0.3289 (0.0004 -- 2.3872)  max mem: 16413
Epoch: [100]  [53/54]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.8160 (1.7185)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8180 (8.3820)  time: 0.6590 (0.4964 -- 2.6043)  data: 0.1151 (0.0002 -- 2.0707)  max mem: 16413
Epoch: [100] Total time: 0:00:49 (0.9212 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.8160 (1.7328)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8180 (8.3820)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8863 (0.8863)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0384 (2.0384 -- 2.0384)  data: 1.8571 (1.8571 -- 1.8571)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6429 (0.6962)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3420 (0.0223 -- 2.0384)  data: 0.1858 (0.0001 -- 1.8571)  max mem: 16413
Val: Total time: 0:00:03 (0.3421 s / it)
* Acc@1 82.927 Acc@5 95.122 loss 0.825
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [101]  [ 0/54]  eta: 0:06:28  lr: 0.000024  min_lr: 0.000001  loss: 1.6174 (1.6174)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9877 (10.9877)  time: 7.1958 (7.1958 -- 7.1958)  data: 5.5682 (5.5682 -- 5.5682)  max mem: 16413
Epoch: [101]  [20/54]  eta: 0:00:41  lr: 0.000024  min_lr: 0.000001  loss: 1.8735 (1.7964)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1005 (8.0161)  time: 0.9334 (0.5124 -- 3.4919)  data: 0.2975 (0.0005 -- 2.9693)  max mem: 16413
Epoch: [101]  [40/54]  eta: 0:00:14  lr: 0.000024  min_lr: 0.000001  loss: 1.6710 (1.7694)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4876 (8.3782)  time: 0.8307 (0.5145 -- 3.3356)  data: 0.2643 (0.0003 -- 2.8209)  max mem: 16413
[2023-10-23 21:37:22,303] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:37:22,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:37:22,304] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:37:22,304] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [101]  [53/54]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.7820 (1.7524)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6928 (8.6451)  time: 0.6545 (0.4946 -- 1.9500)  data: 0.1383 (0.0002 -- 1.4490)  max mem: 16413
Epoch: [101] Total time: 0:00:51 (0.9592 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.7820 (1.7168)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6928 (8.6451)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8886 (0.8886)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0550 (2.0550 -- 2.0550)  data: 1.8640 (1.8640 -- 1.8640)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5991 (0.6828)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (95.1220)  time: 0.3438 (0.0222 -- 2.0550)  data: 0.1865 (0.0001 -- 1.8640)  max mem: 16413
Val: Total time: 0:00:03 (0.3439 s / it)
* Acc@1 82.317 Acc@5 95.122 loss 0.817
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [102]  [ 0/54]  eta: 0:05:24  lr: 0.000024  min_lr: 0.000001  loss: 1.9682 (1.9682)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9734 (5.9734)  time: 6.0110 (6.0110 -- 6.0110)  data: 5.4716 (5.4716 -- 5.4716)  max mem: 16413
[2023-10-23 21:37:54,203] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5525
[2023-10-23 21:37:54,203] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:37:54,203] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5525
[2023-10-23 21:37:54,203] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:37:54,203] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [102]  [20/54]  eta: 0:00:42  lr: 0.000024  min_lr: 0.000001  loss: 1.7861 (1.6835)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0403 (7.4234)  time: 1.0147 (0.5316 -- 4.3141)  data: 0.2072 (0.0004 -- 1.6029)  max mem: 16413
[2023-10-23 21:38:10,348] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5546
[2023-10-23 21:38:10,348] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5546
[2023-10-23 21:38:10,348] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:38:10,348] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:38:10,348] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [102]  [40/54]  eta: 0:00:14  lr: 0.000023  min_lr: 0.000001  loss: 1.7181 (1.6904)  loss_scale: 16384.0000 (22577.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3105 (7.6141)  time: 0.8548 (0.5260 -- 3.4324)  data: 0.0014 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [102]  [53/54]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.4743 (1.6581)  loss_scale: 8192.0000 (19114.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3649 (7.6867)  time: 0.6374 (0.4963 -- 2.0703)  data: 0.0007 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [102] Total time: 0:00:50 (0.9422 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.4743 (1.7359)  loss_scale: 8192.0000 (19114.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3649 (7.6867)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8920 (0.8920)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9614 (1.9614 -- 1.9614)  data: 1.7527 (1.7527 -- 1.7527)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5089 (0.6735)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (93.9024)  time: 0.3337 (0.0222 -- 1.9614)  data: 0.1754 (0.0001 -- 1.7527)  max mem: 16413
Val: Total time: 0:00:03 (0.3339 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.805
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [103]  [ 0/54]  eta: 0:05:38  lr: 0.000023  min_lr: 0.000001  loss: 1.6342 (1.6342)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1923 (9.1923)  time: 6.2610 (6.2610 -- 6.2610)  data: 5.2328 (5.2328 -- 5.2328)  max mem: 16413
Epoch: [103]  [20/54]  eta: 0:00:43  lr: 0.000023  min_lr: 0.000001  loss: 1.6435 (1.6849)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4850 (8.6305)  time: 1.0338 (0.5192 -- 5.2646)  data: 0.4825 (0.0004 -- 4.7245)  max mem: 16413
Epoch: [103]  [40/54]  eta: 0:00:14  lr: 0.000023  min_lr: 0.000001  loss: 1.8433 (1.7210)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4744 (8.1429)  time: 0.8352 (0.5309 -- 3.1141)  data: 0.0711 (0.0006 -- 1.3891)  max mem: 16413
Epoch: [103]  [53/54]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7165 (1.7121)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0741 (8.2566)  time: 0.6511 (0.4951 -- 3.1141)  data: 0.0007 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [103] Total time: 0:00:50 (0.9301 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7165 (1.7363)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0741 (8.2566)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8509 (0.8509)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9880 (1.9880 -- 1.9880)  data: 1.7661 (1.7661 -- 1.7661)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5216 (0.6586)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3372 (0.0223 -- 1.9880)  data: 0.1767 (0.0001 -- 1.7661)  max mem: 16413
Val: Total time: 0:00:03 (0.3373 s / it)
* Acc@1 83.537 Acc@5 95.122 loss 0.778
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [104]  [ 0/54]  eta: 0:05:30  lr: 0.000023  min_lr: 0.000001  loss: 1.9274 (1.9274)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5382 (9.5382)  time: 6.1256 (6.1256 -- 6.1256)  data: 5.6080 (5.6080 -- 5.6080)  max mem: 16413
Epoch: [104]  [20/54]  eta: 0:00:41  lr: 0.000023  min_lr: 0.000001  loss: 1.7938 (1.7658)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1111 (8.7519)  time: 0.9685 (0.5327 -- 2.7922)  data: 0.1643 (0.0008 -- 1.3487)  max mem: 16413
Epoch: [104]  [40/54]  eta: 0:00:14  lr: 0.000023  min_lr: 0.000001  loss: 1.7180 (1.7481)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5458 (8.1209)  time: 0.8442 (0.5197 -- 3.7825)  data: 0.0506 (0.0003 -- 0.7633)  max mem: 16413
Epoch: [104]  [53/54]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7418 (1.7306)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8571 (8.1759)  time: 0.7794 (0.4941 -- 3.7825)  data: 0.0119 (0.0001 -- 0.2262)  max mem: 16413
Epoch: [104] Total time: 0:00:50 (0.9437 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7418 (1.7473)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8571 (8.1759)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8492 (0.8492)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0331 (2.0331 -- 2.0331)  data: 1.8554 (1.8554 -- 1.8554)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5937 (0.6747)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3416 (0.0223 -- 2.0331)  data: 0.1856 (0.0001 -- 1.8554)  max mem: 16413
Val: Total time: 0:00:03 (0.3418 s / it)
* Acc@1 82.927 Acc@5 93.902 loss 0.787
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [105]  [ 0/54]  eta: 0:06:05  lr: 0.000023  min_lr: 0.000001  loss: 1.9186 (1.9186)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3343 (7.3343)  time: 6.7717 (6.7717 -- 6.7717)  data: 4.6749 (4.6749 -- 4.6749)  max mem: 16413
[2023-10-23 21:40:21,590] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:40:21,590] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:40:21,591] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 21:40:21,591] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [105]  [20/54]  eta: 0:00:38  lr: 0.000022  min_lr: 0.000001  loss: 1.6620 (1.6908)  loss_scale: 16384.0000 (14433.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5945 (7.6214)  time: 0.8622 (0.5333 -- 2.1743)  data: 0.1841 (0.0003 -- 1.1931)  max mem: 16413
Epoch: [105]  [40/54]  eta: 0:00:14  lr: 0.000022  min_lr: 0.000001  loss: 1.7616 (1.7473)  loss_scale: 16384.0000 (15384.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3040 (7.8185)  time: 0.8602 (0.5221 -- 2.2924)  data: 0.1447 (0.0004 -- 1.1791)  max mem: 16413
Epoch: [105]  [53/54]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.5251 (1.7255)  loss_scale: 16384.0000 (15625.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1700 (8.0413)  time: 0.7800 (0.4951 -- 2.8248)  data: 0.1758 (0.0001 -- 2.3244)  max mem: 16413
Epoch: [105] Total time: 0:00:50 (0.9292 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.5251 (1.7404)  loss_scale: 16384.0000 (15625.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1700 (8.0413)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8991 (0.8991)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0377 (2.0377 -- 2.0377)  data: 1.8595 (1.8595 -- 1.8595)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5547 (0.6743)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3422 (0.0222 -- 2.0377)  data: 0.1860 (0.0001 -- 1.8595)  max mem: 16413
Val: Total time: 0:00:03 (0.3423 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.805
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [106]  [ 0/54]  eta: 0:07:26  lr: 0.000022  min_lr: 0.000001  loss: 2.0518 (2.0518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1257 (6.1257)  time: 8.2729 (8.2729 -- 8.2729)  data: 7.7443 (7.7443 -- 7.7443)  max mem: 16413
Epoch: [106]  [20/54]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 1.6545 (1.7416)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9144 (8.0750)  time: 0.7557 (0.5222 -- 2.9138)  data: 0.2082 (0.0004 -- 2.3923)  max mem: 16413
Epoch: [106]  [40/54]  eta: 0:00:14  lr: 0.000022  min_lr: 0.000001  loss: 1.8737 (1.7667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3523 (8.3259)  time: 1.0121 (0.5259 -- 4.2076)  data: 0.4707 (0.0002 -- 3.6920)  max mem: 16413
Epoch: [106]  [53/54]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.6193 (1.7137)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4638 (8.2810)  time: 0.7099 (0.4923 -- 2.7350)  data: 0.1874 (0.0002 -- 2.2140)  max mem: 16413
Epoch: [106] Total time: 0:00:51 (0.9590 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.6193 (1.7315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4638 (8.2810)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8965 (0.8965)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9897 (1.9897 -- 1.9897)  data: 1.7814 (1.7814 -- 1.7814)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5468 (0.6707)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3364 (0.0222 -- 1.9897)  data: 0.1782 (0.0001 -- 1.7814)  max mem: 16413
Val: Total time: 0:00:03 (0.3365 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.803
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [107]  [ 0/54]  eta: 0:07:07  lr: 0.000022  min_lr: 0.000001  loss: 1.2938 (1.2938)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8465 (6.8465)  time: 7.9167 (7.9167 -- 7.9167)  data: 7.3840 (7.3840 -- 7.3840)  max mem: 16413
Epoch: [107]  [20/54]  eta: 0:00:38  lr: 0.000022  min_lr: 0.000001  loss: 1.6279 (1.6935)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6262 (7.0110)  time: 0.7847 (0.5266 -- 3.1616)  data: 0.1447 (0.0007 -- 1.4661)  max mem: 16413
[2023-10-23 21:42:29,882] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:42:29,882] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:42:29,884] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:42:29,884] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:42:30,421] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5804
[2023-10-23 21:42:30,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:42:30,421] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5804
[2023-10-23 21:42:30,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:42:30,421] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [107]  [40/54]  eta: 0:00:14  lr: 0.000021  min_lr: 0.000001  loss: 1.6276 (1.6947)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4751 (7.2684)  time: 0.8728 (0.5134 -- 3.1393)  data: 0.2746 (0.0002 -- 2.5916)  max mem: 16413
Epoch: [107]  [53/54]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.7538 (1.7252)  loss_scale: 16384.0000 (16687.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5727 (7.4606)  time: 0.6624 (0.4960 -- 1.8017)  data: 0.0473 (0.0002 -- 0.5625)  max mem: 16413
Epoch: [107] Total time: 0:00:49 (0.9248 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.7538 (1.7439)  loss_scale: 16384.0000 (16687.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5727 (7.4606)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8809 (0.8809)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9862 (1.9862 -- 1.9862)  data: 1.7810 (1.7810 -- 1.7810)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5696 (0.6706)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3372 (0.0223 -- 1.9862)  data: 0.1782 (0.0001 -- 1.7810)  max mem: 16413
Val: Total time: 0:00:03 (0.3373 s / it)
* Acc@1 81.707 Acc@5 95.122 loss 0.796
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [108]  [ 0/54]  eta: 0:06:28  lr: 0.000021  min_lr: 0.000001  loss: 1.8576 (1.8576)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7336 (4.7336)  time: 7.1894 (7.1894 -- 7.1894)  data: 6.4777 (6.4777 -- 6.4777)  max mem: 16413
Epoch: [108]  [20/54]  eta: 0:00:40  lr: 0.000021  min_lr: 0.000001  loss: 1.7542 (1.7387)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1173 (7.1064)  time: 0.8849 (0.5159 -- 4.3353)  data: 0.3433 (0.0005 -- 3.7810)  max mem: 16413
Epoch: [108]  [40/54]  eta: 0:00:15  lr: 0.000021  min_lr: 0.000001  loss: 1.8617 (1.8049)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8139 (7.4300)  time: 1.0020 (0.5179 -- 5.2774)  data: 0.4617 (0.0002 -- 4.7532)  max mem: 16413
Epoch: [108]  [53/54]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.7044 (1.7926)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4907 (7.3260)  time: 0.6714 (0.4938 -- 2.9839)  data: 0.1580 (0.0002 -- 2.4660)  max mem: 16413
Epoch: [108] Total time: 0:00:52 (0.9652 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.7044 (1.7390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4907 (7.3260)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8734 (0.8734)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0321 (2.0321 -- 2.0321)  data: 1.8267 (1.8267 -- 1.8267)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5283 (0.6654)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3406 (0.0223 -- 2.0321)  data: 0.1828 (0.0001 -- 1.8267)  max mem: 16413
Val: Total time: 0:00:03 (0.3407 s / it)
* Acc@1 82.927 Acc@5 95.122 loss 0.799
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [109]  [ 0/54]  eta: 0:07:59  lr: 0.000021  min_lr: 0.000000  loss: 1.5703 (1.5703)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0534 (5.0534)  time: 8.8784 (8.8784 -- 8.8784)  data: 6.9449 (6.9449 -- 6.9449)  max mem: 16413
Epoch: [109]  [20/54]  eta: 0:00:38  lr: 0.000021  min_lr: 0.000000  loss: 1.5302 (1.5397)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9668 (8.0588)  time: 0.7460 (0.5197 -- 3.8184)  data: 0.0674 (0.0003 -- 1.3219)  max mem: 16413
Epoch: [109]  [40/54]  eta: 0:00:14  lr: 0.000021  min_lr: 0.000000  loss: 1.8465 (1.6614)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7112 (8.3729)  time: 0.9145 (0.5234 -- 3.6369)  data: 0.3409 (0.0004 -- 3.0692)  max mem: 16413
[2023-10-23 21:44:35,440] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:44:35,440] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:44:35,440] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:44:35,440] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [53/54]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.8480 (1.7017)  loss_scale: 16384.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4586 (8.0644)  time: 0.6379 (0.4944 -- 1.9561)  data: 0.1122 (0.0001 -- 1.4334)  max mem: 16413
Epoch: [109] Total time: 0:00:49 (0.9177 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.8480 (1.7428)  loss_scale: 16384.0000 (18507.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4586 (8.0644)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8755 (0.8755)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9527 (1.9527 -- 1.9527)  data: 1.7302 (1.7302 -- 1.7302)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5424 (0.6647)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3328 (0.0222 -- 1.9527)  data: 0.1731 (0.0001 -- 1.7302)  max mem: 16413
Val: Total time: 0:00:03 (0.3329 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.798
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [110]  [ 0/54]  eta: 0:06:45  lr: 0.000021  min_lr: 0.000000  loss: 1.7398 (1.7398)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1717 (5.1717)  time: 7.5040 (7.5040 -- 7.5040)  data: 6.1581 (6.1581 -- 6.1581)  max mem: 16413
Epoch: [110]  [20/54]  eta: 0:00:40  lr: 0.000021  min_lr: 0.000000  loss: 1.7941 (1.7439)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9305 (7.2159)  time: 0.8831 (0.5216 -- 2.5343)  data: 0.3358 (0.0006 -- 2.0141)  max mem: 16413
Epoch: [110]  [40/54]  eta: 0:00:14  lr: 0.000020  min_lr: 0.000000  loss: 1.7661 (1.7668)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5910 (7.5748)  time: 0.8695 (0.5263 -- 4.5312)  data: 0.3265 (0.0004 -- 4.0296)  max mem: 16413
[2023-10-23 21:45:25,771] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5981
[2023-10-23 21:45:25,771] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5981
[2023-10-23 21:45:25,771] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:45:25,771] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:45:25,771] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [110]  [53/54]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7870 (1.7758)  loss_scale: 16384.0000 (28823.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9115 (7.5483)  time: 0.7194 (0.4956 -- 3.0068)  data: 0.2039 (0.0001 -- 2.4818)  max mem: 16413
Epoch: [110] Total time: 0:00:50 (0.9385 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7870 (1.7792)  loss_scale: 16384.0000 (28823.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9115 (7.5483)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8702 (0.8702)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0397 (2.0397 -- 2.0397)  data: 1.8530 (1.8530 -- 1.8530)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5519 (0.6596)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (93.9024)  time: 0.3415 (0.0222 -- 2.0397)  data: 0.1854 (0.0001 -- 1.8530)  max mem: 16413
Val: Total time: 0:00:03 (0.3416 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.793
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [111]  [ 0/54]  eta: 0:06:35  lr: 0.000020  min_lr: 0.000000  loss: 1.6480 (1.6480)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4351 (6.4351)  time: 7.3188 (7.3188 -- 7.3188)  data: 6.7606 (6.7606 -- 6.7606)  max mem: 16413
[2023-10-23 21:45:47,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=31, lr=[4.806095045291858e-07, 4.806095045291858e-07, 6.408126727055812e-07, 6.408126727055812e-07, 8.544168969407748e-07, 8.544168969407748e-07, 1.1392225292543665e-06, 1.1392225292543665e-06, 1.5189633723391553e-06, 1.5189633723391553e-06, 2.025284496452207e-06, 2.025284496452207e-06, 2.7003793286029426e-06, 2.7003793286029426e-06, 3.6005057714705903e-06, 3.6005057714705903e-06, 4.800674361960787e-06, 4.800674361960787e-06, 6.400899149281049e-06, 6.400899149281049e-06, 8.5345321990414e-06, 8.5345321990414e-06, 1.1379376265388532e-05, 1.1379376265388532e-05, 1.517250168718471e-05, 1.517250168718471e-05, 2.0230002249579613e-05, 2.0230002249579613e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 21:45:47,625] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=18.431622512649408, CurrSamplesPerSec=22.676826825992478, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [111]  [20/54]  eta: 0:00:39  lr: 0.000020  min_lr: 0.000000  loss: 1.8035 (1.8055)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9511 (7.5274)  time: 0.8434 (0.5339 -- 3.1157)  data: 0.2293 (0.0005 -- 2.5783)  max mem: 16413
Epoch: [111]  [40/54]  eta: 0:00:14  lr: 0.000020  min_lr: 0.000000  loss: 1.7035 (1.7413)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3641 (7.8857)  time: 0.9098 (0.5170 -- 2.7408)  data: 0.2454 (0.0004 -- 1.8020)  max mem: 16413
[2023-10-23 21:46:25,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6043
[2023-10-23 21:46:25,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:46:25,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6043
[2023-10-23 21:46:25,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:46:25,369] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [111]  [53/54]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7947 (1.7594)  loss_scale: 16384.0000 (15625.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3185 (7.9564)  time: 0.6978 (0.4822 -- 1.7438)  data: 0.0901 (0.0002 -- 1.2089)  max mem: 16413
Epoch: [111] Total time: 0:00:50 (0.9354 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7947 (1.7785)  loss_scale: 16384.0000 (15625.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3185 (7.9564)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8860 (0.8860)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0196 (2.0196 -- 2.0196)  data: 1.8259 (1.8259 -- 1.8259)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5243 (0.6608)  acc1: 88.8889 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3400 (0.0225 -- 2.0196)  data: 0.1827 (0.0001 -- 1.8259)  max mem: 16413
Val: Total time: 0:00:03 (0.3401 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.792
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [112]  [ 0/54]  eta: 0:08:02  lr: 0.000020  min_lr: 0.000000  loss: 1.6731 (1.6731)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8172 (6.8172)  time: 8.9327 (8.9327 -- 8.9327)  data: 6.2227 (6.2227 -- 6.2227)  max mem: 16413
Epoch: [112]  [20/54]  eta: 0:00:42  lr: 0.000020  min_lr: 0.000000  loss: 1.7228 (1.7905)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8666 (8.0071)  time: 0.8631 (0.5281 -- 4.3257)  data: 0.0238 (0.0006 -- 0.2849)  max mem: 16413
Epoch: [112]  [40/54]  eta: 0:00:14  lr: 0.000020  min_lr: 0.000000  loss: 1.8116 (1.8042)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8715 (8.0079)  time: 0.7744 (0.5274 -- 2.5231)  data: 0.1627 (0.0007 -- 1.9934)  max mem: 16413
Epoch: [112]  [53/54]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7873 (1.7695)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8045 (8.1703)  time: 0.6800 (0.4960 -- 2.0675)  data: 0.0691 (0.0002 -- 1.2213)  max mem: 16413
Epoch: [112] Total time: 0:00:49 (0.9175 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7873 (1.7732)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8045 (8.1703)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8858 (0.8858)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0083 (2.0083 -- 2.0083)  data: 1.8071 (1.8071 -- 1.8071)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5154 (0.6574)  acc1: 88.8889 (82.9268)  acc5: 100.0000 (95.1220)  time: 0.3387 (0.0223 -- 2.0083)  data: 0.1808 (0.0001 -- 1.8071)  max mem: 16413
Val: Total time: 0:00:03 (0.3388 s / it)
* Acc@1 84.146 Acc@5 94.512 loss 0.790
Accuracy of the network on the 163 val images: 84.15%
Max accuracy: 84.15%
Epoch: [113]  [ 0/54]  eta: 0:06:33  lr: 0.000020  min_lr: 0.000000  loss: 2.3458 (2.3458)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7765 (5.7765)  time: 7.2893 (7.2893 -- 7.2893)  data: 5.2883 (5.2883 -- 5.2883)  max mem: 16413
Epoch: [113]  [20/54]  eta: 0:00:42  lr: 0.000019  min_lr: 0.000000  loss: 1.7315 (1.7793)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7789 (9.2230)  time: 0.9384 (0.5100 -- 4.5776)  data: 0.0071 (0.0004 -- 0.1139)  max mem: 16413
Epoch: [113]  [40/54]  eta: 0:00:13  lr: 0.000019  min_lr: 0.000000  loss: 1.7141 (1.7388)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1295 (9.4405)  time: 0.7468 (0.5267 -- 2.4050)  data: 0.0308 (0.0002 -- 0.5834)  max mem: 16413
Epoch: [113]  [53/54]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.6324 (1.7274)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7237 (8.9229)  time: 0.7494 (0.4933 -- 2.3191)  data: 0.0301 (0.0002 -- 0.5834)  max mem: 16413
Epoch: [113] Total time: 0:00:51 (0.9445 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.6324 (1.7340)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7237 (8.9229)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9001 (0.9001)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9936 (1.9936 -- 1.9936)  data: 1.7886 (1.7886 -- 1.7886)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5026 (0.6655)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3370 (0.0222 -- 1.9936)  data: 0.1789 (0.0001 -- 1.7886)  max mem: 16413
Val: Total time: 0:00:03 (0.3371 s / it)
* Acc@1 82.927 Acc@5 93.902 loss 0.801
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [114]  [ 0/54]  eta: 0:06:16  lr: 0.000019  min_lr: 0.000000  loss: 1.6034 (1.6034)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9781 (7.9781)  time: 6.9773 (6.9773 -- 6.9773)  data: 6.4418 (6.4418 -- 6.4418)  max mem: 16413
[2023-10-23 21:48:39,994] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:48:39,994] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 21:48:39,995] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:48:39,995] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [114]  [20/54]  eta: 0:00:39  lr: 0.000019  min_lr: 0.000000  loss: 1.6850 (1.7816)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5945 (8.3896)  time: 0.8824 (0.5142 -- 2.7272)  data: 0.1654 (0.0005 -- 2.2004)  max mem: 16413
Epoch: [114]  [40/54]  eta: 0:00:14  lr: 0.000019  min_lr: 0.000000  loss: 1.6924 (1.7413)  loss_scale: 16384.0000 (13187.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2565 (8.0975)  time: 0.8919 (0.5233 -- 3.7437)  data: 0.1128 (0.0004 -- 1.4694)  max mem: 16413
Epoch: [114]  [53/54]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.6623 (1.6928)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1589 (8.0128)  time: 0.6147 (0.4947 -- 2.4031)  data: 0.0008 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [114] Total time: 0:00:50 (0.9421 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.6623 (1.7459)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1589 (8.0128)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9121 (0.9121)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9517 (1.9517 -- 1.9517)  data: 1.7465 (1.7465 -- 1.7465)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5173 (0.6677)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3326 (0.0223 -- 1.9517)  data: 0.1747 (0.0001 -- 1.7465)  max mem: 16413
Val: Total time: 0:00:03 (0.3327 s / it)
* Acc@1 82.317 Acc@5 93.902 loss 0.801
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [115]  [ 0/54]  eta: 0:06:56  lr: 0.000019  min_lr: 0.000000  loss: 2.0611 (2.0611)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1539 (13.1539)  time: 7.7195 (7.7195 -- 7.7195)  data: 7.2027 (7.2027 -- 7.2027)  max mem: 16413
Epoch: [115]  [20/54]  eta: 0:00:40  lr: 0.000019  min_lr: 0.000000  loss: 1.6148 (1.6052)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0551 (8.8451)  time: 0.8740 (0.5202 -- 3.9187)  data: 0.0241 (0.0004 -- 0.4421)  max mem: 16413
Epoch: [115]  [40/54]  eta: 0:00:14  lr: 0.000019  min_lr: 0.000000  loss: 1.8113 (1.7184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1564 (8.4816)  time: 0.9197 (0.5113 -- 5.8251)  data: 0.0066 (0.0003 -- 0.1096)  max mem: 16413
Epoch: [115]  [53/54]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8134 (1.7483)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6400 (8.2911)  time: 0.5621 (0.4946 -- 1.5306)  data: 0.0005 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [115] Total time: 0:00:50 (0.9292 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8134 (1.7314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6400 (8.2911)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8959 (0.8959)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9955 (1.9955 -- 1.9955)  data: 1.8156 (1.8156 -- 1.8156)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6126 (0.6688)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (93.9024)  time: 0.3502 (0.0225 -- 1.9955)  data: 0.1944 (0.0001 -- 1.8156)  max mem: 16413
Val: Total time: 0:00:03 (0.3503 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.806
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [116]  [ 0/54]  eta: 0:08:29  lr: 0.000018  min_lr: 0.000000  loss: 2.0496 (2.0496)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8902 (6.8902)  time: 9.4409 (9.4409 -- 9.4409)  data: 8.8921 (8.8921 -- 8.8921)  max mem: 16413
Epoch: [116]  [20/54]  eta: 0:00:42  lr: 0.000018  min_lr: 0.000000  loss: 1.7657 (1.7930)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2495 (7.5815)  time: 0.8394 (0.5227 -- 3.5549)  data: 0.2959 (0.0005 -- 3.0318)  max mem: 16413
[2023-10-23 21:50:48,317] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:50:48,318] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 21:50:48,318] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:50:48,319] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [116]  [40/54]  eta: 0:00:15  lr: 0.000018  min_lr: 0.000000  loss: 1.9330 (1.8781)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1491 (7.6726)  time: 0.9559 (0.5090 -- 4.5048)  data: 0.4234 (0.0003 -- 3.9753)  max mem: 16413
[2023-10-23 21:50:55,257] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6311
[2023-10-23 21:50:55,257] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6311
[2023-10-23 21:50:55,257] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:50:55,257] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 21:50:55,257] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [53/54]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8700 (1.8501)  loss_scale: 32768.0000 (19721.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2753 (7.8312)  time: 0.5754 (0.4826 -- 1.7474)  data: 0.0608 (0.0001 -- 1.2059)  max mem: 16413
Epoch: [116] Total time: 0:00:51 (0.9608 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8700 (1.7970)  loss_scale: 32768.0000 (19721.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2753 (7.8312)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9029 (0.9029)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0159 (2.0159 -- 2.0159)  data: 1.7709 (1.7709 -- 1.7709)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6077 (0.6673)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (93.9024)  time: 0.3397 (0.0223 -- 2.0159)  data: 0.1772 (0.0001 -- 1.7709)  max mem: 16413
Val: Total time: 0:00:03 (0.3398 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.808
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [117]  [ 0/54]  eta: 0:06:28  lr: 0.000018  min_lr: 0.000000  loss: 2.1277 (2.1277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6450 (4.6450)  time: 7.1881 (7.1881 -- 7.1881)  data: 6.3190 (6.3190 -- 6.3190)  max mem: 16413
Epoch: [117]  [20/54]  eta: 0:00:38  lr: 0.000018  min_lr: 0.000000  loss: 1.7696 (1.7695)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9095 (7.9071)  time: 0.8218 (0.5231 -- 3.5902)  data: 0.2737 (0.0002 -- 3.0322)  max mem: 16413
Epoch: [117]  [40/54]  eta: 0:00:14  lr: 0.000018  min_lr: 0.000000  loss: 1.7284 (1.6979)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5178 (7.8334)  time: 0.9053 (0.5308 -- 4.1587)  data: 0.3192 (0.0003 -- 3.6325)  max mem: 16413
Epoch: [117]  [53/54]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7291 (1.7389)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5151 (7.8552)  time: 0.6953 (0.4937 -- 2.3821)  data: 0.0404 (0.0001 -- 0.7789)  max mem: 16413
Epoch: [117] Total time: 0:00:50 (0.9292 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7291 (1.7250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5151 (7.8552)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8794 (0.8794)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0026 (2.0026 -- 2.0026)  data: 1.8171 (1.8171 -- 1.8171)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5807 (0.6565)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3482 (0.0220 -- 2.0026)  data: 0.1921 (0.0001 -- 1.8171)  max mem: 16413
Val: Total time: 0:00:03 (0.3483 s / it)
* Acc@1 82.927 Acc@5 95.122 loss 0.793
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [118]  [ 0/54]  eta: 0:07:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7463 (1.7463)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9080 (6.9080)  time: 7.7846 (7.7846 -- 7.7846)  data: 7.2083 (7.2083 -- 7.2083)  max mem: 16413
[2023-10-23 21:52:05,297] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6376
[2023-10-23 21:52:05,297] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:52:05,297] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6376
[2023-10-23 21:52:05,298] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:52:05,298] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [118]  [20/54]  eta: 0:00:39  lr: 0.000018  min_lr: 0.000000  loss: 1.7445 (1.7655)  loss_scale: 8192.0000 (9752.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0642 (7.3710)  time: 0.8155 (0.5168 -- 3.0521)  data: 0.1908 (0.0002 -- 2.1327)  max mem: 16413
Epoch: [118]  [40/54]  eta: 0:00:15  lr: 0.000017  min_lr: 0.000000  loss: 1.7293 (1.7730)  loss_scale: 8192.0000 (8991.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7531 (8.1677)  time: 0.9989 (0.5132 -- 4.0615)  data: 0.4608 (0.0002 -- 3.5455)  max mem: 16413
Epoch: [118]  [53/54]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8233 (1.7733)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9875 (7.8695)  time: 0.6471 (0.4934 -- 3.1172)  data: 0.1314 (0.0001 -- 2.6150)  max mem: 16413
Epoch: [118] Total time: 0:00:50 (0.9381 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8233 (1.7287)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9875 (7.8695)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8807 (0.8807)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0266 (2.0266 -- 2.0266)  data: 1.8365 (1.8365 -- 1.8365)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5560 (0.6528)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3405 (0.0224 -- 2.0266)  data: 0.1837 (0.0001 -- 1.8365)  max mem: 16413
Val: Total time: 0:00:03 (0.3406 s / it)
* Acc@1 83.537 Acc@5 95.122 loss 0.785
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [119]  [ 0/54]  eta: 0:06:31  lr: 0.000017  min_lr: 0.000000  loss: 2.2269 (2.2269)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5243 (6.5243)  time: 7.2478 (7.2478 -- 7.2478)  data: 6.6897 (6.6897 -- 6.6897)  max mem: 16413
Epoch: [119]  [20/54]  eta: 0:00:39  lr: 0.000017  min_lr: 0.000000  loss: 1.6142 (1.6861)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9962 (8.3412)  time: 0.8707 (0.5294 -- 3.3148)  data: 0.3262 (0.0004 -- 2.7871)  max mem: 16413
Epoch: [119]  [40/54]  eta: 0:00:14  lr: 0.000017  min_lr: 0.000000  loss: 1.6673 (1.6988)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4682 (8.2268)  time: 0.8682 (0.5218 -- 2.6332)  data: 0.3281 (0.0003 -- 2.1163)  max mem: 16413
Epoch: [119]  [53/54]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.6667 (1.6712)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4682 (8.2495)  time: 0.7549 (0.4963 -- 2.6332)  data: 0.2372 (0.0002 -- 2.1163)  max mem: 16413
Epoch: [119] Total time: 0:00:51 (0.9488 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.6667 (1.6928)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4682 (8.2495)
[2023-10-23 21:53:40,816] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-10-23 21:53:40,817] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-119/mp_rank_00_model_states.pt
[2023-10-23 21:53:40,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-119/mp_rank_00_model_states.pt...
[2023-10-23 21:53:40,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-10-23 21:53:41,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-119/mp_rank_00_model_states.pt.
[2023-10-23 21:53:41,859] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/10]  eta: 0:00:21  loss: 0.8767 (0.8767)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.1189 (2.1189 -- 2.1189)  data: 1.9406 (1.9406 -- 1.9406)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5935 (0.6592)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3512 (0.0230 -- 2.1189)  data: 0.1942 (0.0001 -- 1.9406)  max mem: 16413
Val: Total time: 0:00:03 (0.3513 s / it)
* Acc@1 82.927 Acc@5 95.122 loss 0.788
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [120]  [ 0/54]  eta: 0:06:48  lr: 0.000017  min_lr: 0.000000  loss: 1.9246 (1.9246)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0725 (6.0725)  time: 7.5732 (7.5732 -- 7.5732)  data: 4.8389 (4.8389 -- 4.8389)  max mem: 16413
Epoch: [120]  [20/54]  eta: 0:00:42  lr: 0.000017  min_lr: 0.000000  loss: 1.5674 (1.7179)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5539 (7.4085)  time: 0.9410 (0.5285 -- 3.0137)  data: 0.0012 (0.0004 -- 0.0039)  max mem: 16413
[2023-10-23 21:54:15,773] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:54:15,773] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:54:15,773] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 21:54:15,773] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [120]  [40/54]  eta: 0:00:14  lr: 0.000017  min_lr: 0.000000  loss: 1.8557 (1.7418)  loss_scale: 16384.0000 (11388.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1945 (7.5591)  time: 0.8466 (0.5263 -- 3.2271)  data: 0.0016 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [120]  [53/54]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.7404 (1.7521)  loss_scale: 16384.0000 (12591.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4321 (7.5774)  time: 0.6590 (0.4942 -- 3.2271)  data: 0.0008 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [120] Total time: 0:00:49 (0.9258 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.7404 (1.7358)  loss_scale: 16384.0000 (12591.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4321 (7.5774)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8818 (0.8818)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9782 (1.9782 -- 1.9782)  data: 1.7597 (1.7597 -- 1.7597)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5884 (0.6700)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3356 (0.0221 -- 1.9782)  data: 0.1761 (0.0001 -- 1.7597)  max mem: 16413
Val: Total time: 0:00:03 (0.3357 s / it)
* Acc@1 83.537 Acc@5 95.122 loss 0.794
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [121]  [ 0/54]  eta: 0:05:48  lr: 0.000017  min_lr: 0.000000  loss: 1.7143 (1.7143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4843 (5.4843)  time: 6.4461 (6.4461 -- 6.4461)  data: 5.9084 (5.9084 -- 5.9084)  max mem: 16413
[2023-10-23 21:54:54,391] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6543
[2023-10-23 21:54:54,391] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6543
[2023-10-23 21:54:54,391] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:54:54,391] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 21:54:54,392] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [121]  [20/54]  eta: 0:00:40  lr: 0.000016  min_lr: 0.000000  loss: 1.4991 (1.6354)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0910 (8.4662)  time: 0.9175 (0.5144 -- 4.3285)  data: 0.1134 (0.0007 -- 1.5031)  max mem: 16413
[2023-10-23 21:55:11,066] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6560
[2023-10-23 21:55:11,066] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6560
[2023-10-23 21:55:11,066] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-10-23 21:55:11,066] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-10-23 21:55:11,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [121]  [40/54]  eta: 0:00:14  lr: 0.000016  min_lr: 0.000000  loss: 1.8218 (1.6958)  loss_scale: 4096.0000 (8491.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0176 (8.4218)  time: 0.9106 (0.5116 -- 4.7454)  data: 0.0010 (0.0004 -- 0.0021)  max mem: 16413
Epoch: [121]  [53/54]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9018 (1.7157)  loss_scale: 4096.0000 (7433.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5948 (8.2531)  time: 0.7097 (0.4936 -- 2.9710)  data: 0.0004 (0.0001 -- 0.0009)  max mem: 16413
Epoch: [121] Total time: 0:00:52 (0.9668 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9018 (1.7527)  loss_scale: 4096.0000 (7433.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5948 (8.2531)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8855 (0.8855)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0751 (2.0751 -- 2.0751)  data: 1.8853 (1.8853 -- 1.8853)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5946 (0.6657)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3459 (0.0226 -- 2.0751)  data: 0.1886 (0.0001 -- 1.8853)  max mem: 16413
Val: Total time: 0:00:03 (0.3460 s / it)
* Acc@1 82.317 Acc@5 95.122 loss 0.788
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [122]  [ 0/54]  eta: 0:07:58  lr: 0.000016  min_lr: 0.000000  loss: 1.8326 (1.8326)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1538 (7.1538)  time: 8.8556 (8.8556 -- 8.8556)  data: 6.1068 (6.1068 -- 6.1068)  max mem: 16413
Epoch: [122]  [20/54]  eta: 0:00:41  lr: 0.000016  min_lr: 0.000000  loss: 1.5840 (1.6551)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3939 (7.2551)  time: 0.8489 (0.5229 -- 2.7233)  data: 0.3023 (0.0007 -- 2.1919)  max mem: 16413
Epoch: [122]  [40/54]  eta: 0:00:14  lr: 0.000016  min_lr: 0.000000  loss: 1.7128 (1.7155)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3305 (7.6816)  time: 0.7583 (0.5293 -- 2.9401)  data: 0.2093 (0.0004 -- 2.4063)  max mem: 16413
Epoch: [122]  [53/54]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.6426 (1.7289)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3305 (7.6492)  time: 0.6530 (0.4953 -- 2.3910)  data: 0.0969 (0.0001 -- 1.8807)  max mem: 16413
Epoch: [122] Total time: 0:00:50 (0.9301 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.6426 (1.7267)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3305 (7.6492)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8788 (0.8788)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9698 (1.9698 -- 1.9698)  data: 1.7350 (1.7350 -- 1.7350)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5646 (0.6601)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3414 (0.0223 -- 1.9698)  data: 0.1791 (0.0001 -- 1.7350)  max mem: 16413
Val: Total time: 0:00:03 (0.3415 s / it)
* Acc@1 82.927 Acc@5 95.122 loss 0.786
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [123]  [ 0/54]  eta: 0:06:56  lr: 0.000016  min_lr: 0.000000  loss: 1.7839 (1.7839)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6948 (4.6948)  time: 7.7161 (7.7161 -- 7.7161)  data: 5.6029 (5.6029 -- 5.6029)  max mem: 16413
Epoch: [123]  [20/54]  eta: 0:00:39  lr: 0.000016  min_lr: 0.000000  loss: 1.8327 (1.7680)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0303 (7.1095)  time: 0.8473 (0.5288 -- 2.5074)  data: 0.0753 (0.0009 -- 0.5820)  max mem: 16413
Epoch: [123]  [40/54]  eta: 0:00:14  lr: 0.000016  min_lr: 0.000000  loss: 1.7910 (1.7154)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7809 (7.5277)  time: 0.8959 (0.5150 -- 4.6977)  data: 0.1108 (0.0003 -- 1.1332)  max mem: 16413
[2023-10-23 21:57:15,509] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:57:15,509] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-10-23 21:57:15,509] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:57:15,509] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [123]  [53/54]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7793 (1.7131)  loss_scale: 4096.0000 (4626.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7809 (7.6109)  time: 0.5691 (0.4934 -- 1.5553)  data: 0.0006 (0.0001 -- 0.0014)  max mem: 16413
Epoch: [123] Total time: 0:00:50 (0.9299 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7793 (1.7340)  loss_scale: 4096.0000 (4626.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7809 (7.6109)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9002 (0.9002)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9832 (1.9832 -- 1.9832)  data: 1.7765 (1.7765 -- 1.7765)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5417 (0.6654)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3360 (0.0223 -- 1.9832)  data: 0.1780 (0.0001 -- 1.7765)  max mem: 16413
Val: Total time: 0:00:03 (0.3361 s / it)
* Acc@1 82.927 Acc@5 95.122 loss 0.795
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [124]  [ 0/54]  eta: 0:06:41  lr: 0.000016  min_lr: 0.000000  loss: 1.8918 (1.8918)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1361 (7.1361)  time: 7.4298 (7.4298 -- 7.4298)  data: 6.6959 (6.6959 -- 6.6959)  max mem: 16413
Epoch: [124]  [20/54]  eta: 0:00:39  lr: 0.000015  min_lr: 0.000000  loss: 1.8321 (1.8183)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9540 (7.8556)  time: 0.8476 (0.5227 -- 2.3649)  data: 0.2190 (0.0009 -- 1.5563)  max mem: 16413
Epoch: [124]  [40/54]  eta: 0:00:14  lr: 0.000015  min_lr: 0.000000  loss: 1.8644 (1.8256)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0586 (7.4734)  time: 0.8878 (0.5204 -- 3.0859)  data: 0.1087 (0.0002 -- 1.5328)  max mem: 16413
Epoch: [124]  [53/54]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.5256 (1.7309)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5883 (7.8684)  time: 0.7459 (0.4944 -- 1.7590)  data: 0.0257 (0.0001 -- 0.4966)  max mem: 16413
Epoch: [124] Total time: 0:00:51 (0.9515 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.5256 (1.7100)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5883 (7.8684)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8818 (0.8818)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9978 (1.9978 -- 1.9978)  data: 1.7973 (1.7973 -- 1.7973)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5914 (0.6670)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3372 (0.0223 -- 1.9978)  data: 0.1798 (0.0001 -- 1.7973)  max mem: 16413
Val: Total time: 0:00:03 (0.3373 s / it)
* Acc@1 83.537 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [125]  [ 0/54]  eta: 0:08:11  lr: 0.000015  min_lr: 0.000000  loss: 1.6866 (1.6866)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4028 (7.4028)  time: 9.1066 (9.1066 -- 9.1066)  data: 4.9806 (4.9806 -- 4.9806)  max mem: 16413
Epoch: [125]  [20/54]  eta: 0:00:38  lr: 0.000015  min_lr: 0.000000  loss: 1.6224 (1.6482)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4841 (8.6968)  time: 0.7422 (0.5295 -- 3.0362)  data: 0.0095 (0.0007 -- 0.1610)  max mem: 16413
Epoch: [125]  [40/54]  eta: 0:00:14  lr: 0.000015  min_lr: 0.000000  loss: 1.7520 (1.6932)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9742 (8.2991)  time: 0.9681 (0.5260 -- 3.4198)  data: 0.0899 (0.0008 -- 0.9188)  max mem: 16413
Epoch: [125]  [53/54]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.6878 (1.6926)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6272 (8.0070)  time: 0.7115 (0.4952 -- 3.4198)  data: 0.0467 (0.0001 -- 0.9188)  max mem: 16413
Epoch: [125] Total time: 0:00:49 (0.9251 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.6878 (1.7219)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6272 (8.0070)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8872 (0.8872)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0309 (2.0309 -- 2.0309)  data: 1.8547 (1.8547 -- 1.8547)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5950 (0.6737)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3412 (0.0224 -- 2.0309)  data: 0.1856 (0.0001 -- 1.8547)  max mem: 16413
Val: Total time: 0:00:03 (0.3413 s / it)
* Acc@1 83.537 Acc@5 93.902 loss 0.791
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [126]  [ 0/54]  eta: 0:07:32  lr: 0.000015  min_lr: 0.000000  loss: 1.2839 (1.2839)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8744 (9.8744)  time: 8.3747 (8.3747 -- 8.3747)  data: 6.9517 (6.9517 -- 6.9517)  max mem: 16413
[2023-10-23 21:59:28,183] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:59:28,184] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 21:59:28,189] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 21:59:28,190] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [126]  [20/54]  eta: 0:00:40  lr: 0.000015  min_lr: 0.000000  loss: 1.6442 (1.5981)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3309 (7.9578)  time: 0.8167 (0.5132 -- 3.4887)  data: 0.2436 (0.0004 -- 2.4412)  max mem: 16413
Epoch: [126]  [40/54]  eta: 0:00:14  lr: 0.000015  min_lr: 0.000000  loss: 1.6957 (1.6301)  loss_scale: 16384.0000 (13786.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5262 (7.9544)  time: 0.9250 (0.5216 -- 3.2585)  data: 0.1020 (0.0004 -- 0.7946)  max mem: 16413
Epoch: [126]  [53/54]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.4745 (1.5898)  loss_scale: 16384.0000 (14411.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6604 (7.8310)  time: 0.7055 (0.4929 -- 3.2585)  data: 0.0343 (0.0002 -- 0.6683)  max mem: 16413
Epoch: [126] Total time: 0:00:50 (0.9400 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.4745 (1.6463)  loss_scale: 16384.0000 (14411.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6604 (7.8310)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8752 (0.8752)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0165 (2.0165 -- 2.0165)  data: 1.8117 (1.8117 -- 1.8117)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5928 (0.6740)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (92.6829)  time: 0.3393 (0.0224 -- 2.0165)  data: 0.1813 (0.0001 -- 1.8117)  max mem: 16413
Val: Total time: 0:00:03 (0.3394 s / it)
* Acc@1 83.537 Acc@5 93.293 loss 0.798
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [127]  [ 0/54]  eta: 0:06:11  lr: 0.000014  min_lr: 0.000000  loss: 1.8540 (1.8540)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7698 (6.7698)  time: 6.8741 (6.8741 -- 6.8741)  data: 4.5458 (4.5458 -- 4.5458)  max mem: 16413
Epoch: [127]  [20/54]  eta: 0:00:41  lr: 0.000014  min_lr: 0.000000  loss: 1.6577 (1.7145)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5413 (7.3312)  time: 0.9362 (0.5204 -- 3.1697)  data: 0.2782 (0.0005 -- 2.6317)  max mem: 16413
Epoch: [127]  [40/54]  eta: 0:00:14  lr: 0.000014  min_lr: 0.000000  loss: 1.7909 (1.7556)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1414 (7.9581)  time: 0.7928 (0.5221 -- 3.9918)  data: 0.2505 (0.0003 -- 3.4609)  max mem: 16413
Epoch: [127]  [53/54]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.5832 (1.6834)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7560 (8.0008)  time: 0.6900 (0.4951 -- 3.9192)  data: 0.1711 (0.0002 -- 3.4038)  max mem: 16413
Epoch: [127] Total time: 0:00:51 (0.9527 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.5832 (1.7230)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7560 (8.0008)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8950 (0.8950)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0376 (2.0376 -- 2.0376)  data: 1.8444 (1.8444 -- 1.8444)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.6032 (0.6804)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3418 (0.0224 -- 2.0376)  data: 0.1845 (0.0001 -- 1.8444)  max mem: 16413
Val: Total time: 0:00:03 (0.3420 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.803
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [128]  [ 0/54]  eta: 0:06:55  lr: 0.000014  min_lr: 0.000000  loss: 1.8746 (1.8746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9830 (3.9830)  time: 7.6881 (7.6881 -- 7.6881)  data: 7.1413 (7.1413 -- 7.1413)  max mem: 16413
Epoch: [128]  [20/54]  eta: 0:00:42  lr: 0.000014  min_lr: 0.000000  loss: 1.5317 (1.6371)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7605 (8.0934)  time: 0.9306 (0.5197 -- 2.8909)  data: 0.3875 (0.0004 -- 2.3062)  max mem: 16413
[2023-10-23 22:01:37,310] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:01:37,311] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:01:37,312] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:01:37,312] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [128]  [40/54]  eta: 0:00:14  lr: 0.000014  min_lr: 0.000000  loss: 1.6721 (1.6440)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7943 (8.0038)  time: 0.7673 (0.5368 -- 2.2508)  data: 0.2159 (0.0002 -- 1.6829)  max mem: 16413
[2023-10-23 22:01:46,211] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6957
[2023-10-23 22:01:46,211] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:01:46,211] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6957
[2023-10-23 22:01:46,211] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:01:46,211] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [128]  [53/54]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.6580 (1.6655)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9857 (7.8723)  time: 0.6514 (0.4941 -- 3.0005)  data: 0.1318 (0.0002 -- 2.4554)  max mem: 16413
Epoch: [128] Total time: 0:00:50 (0.9424 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.6580 (1.7400)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9857 (7.8723)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8982 (0.8982)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0488 (2.0488 -- 2.0488)  data: 1.8626 (1.8626 -- 1.8626)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5984 (0.6689)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3430 (0.0223 -- 2.0488)  data: 0.1863 (0.0001 -- 1.8626)  max mem: 16413
Val: Total time: 0:00:03 (0.3431 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.793
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [129]  [ 0/54]  eta: 0:06:49  lr: 0.000014  min_lr: 0.000000  loss: 0.9645 (0.9645)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9504 (9.9504)  time: 7.5909 (7.5909 -- 7.5909)  data: 7.0590 (7.0590 -- 7.0590)  max mem: 16413
Epoch: [129]  [20/54]  eta: 0:00:39  lr: 0.000014  min_lr: 0.000000  loss: 1.6983 (1.6411)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0852 (8.8288)  time: 0.8425 (0.5394 -- 2.8760)  data: 0.2442 (0.0009 -- 2.3290)  max mem: 16413
[2023-10-23 22:02:29,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=37, lr=[3.2205897227464015e-07, 3.2205897227464015e-07, 4.2941196303285355e-07, 4.2941196303285355e-07, 5.725492840438047e-07, 5.725492840438047e-07, 7.633990453917397e-07, 7.633990453917397e-07, 1.017865393855653e-06, 1.017865393855653e-06, 1.3571538584742038e-06, 1.3571538584742038e-06, 1.8095384779656052e-06, 1.8095384779656052e-06, 2.412717970620807e-06, 2.412717970620807e-06, 3.2169572941610758e-06, 3.2169572941610758e-06, 4.289276392214767e-06, 4.289276392214767e-06, 5.7190351896196904e-06, 5.7190351896196904e-06, 7.625380252826253e-06, 7.625380252826253e-06, 1.0167173670435005e-05, 1.0167173670435005e-05, 1.3556231560580007e-05, 1.3556231560580007e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 22:02:29,501] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=18.380561330097745, CurrSamplesPerSec=23.004378595887594, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [129]  [40/54]  eta: 0:00:13  lr: 0.000014  min_lr: 0.000000  loss: 1.5072 (1.6147)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6813 (8.1823)  time: 0.7829 (0.5243 -- 2.1277)  data: 0.2441 (0.0005 -- 1.5917)  max mem: 16413
Epoch: [129]  [53/54]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7716 (1.6733)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6786 (8.0428)  time: 0.7172 (0.4958 -- 2.7863)  data: 0.1733 (0.0003 -- 2.2528)  max mem: 16413
Epoch: [129] Total time: 0:00:49 (0.9251 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.7716 (1.6428)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6786 (8.0428)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9033 (0.9033)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9553 (1.9553 -- 1.9553)  data: 1.7528 (1.7528 -- 1.7528)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5885 (0.6733)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (93.9024)  time: 0.3421 (0.0223 -- 1.9553)  data: 0.1839 (0.0001 -- 1.7528)  max mem: 16413
Val: Total time: 0:00:03 (0.3422 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.804
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [130]  [ 0/54]  eta: 0:06:50  lr: 0.000013  min_lr: 0.000000  loss: 1.6920 (1.6920)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0340 (9.0340)  time: 7.6056 (7.6056 -- 7.6056)  data: 5.5439 (5.5439 -- 5.5439)  max mem: 16413
Epoch: [130]  [20/54]  eta: 0:00:39  lr: 0.000013  min_lr: 0.000000  loss: 1.6806 (1.7135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4978 (8.7356)  time: 0.8549 (0.5277 -- 2.9776)  data: 0.0503 (0.0007 -- 0.7765)  max mem: 16413
Epoch: [130]  [40/54]  eta: 0:00:14  lr: 0.000013  min_lr: 0.000000  loss: 1.8619 (1.7067)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6294 (7.8085)  time: 0.9260 (0.5260 -- 3.8375)  data: 0.0179 (0.0006 -- 0.3256)  max mem: 16413
Epoch: [130]  [53/54]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8870 (1.7217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9981 (7.7733)  time: 0.6937 (0.4933 -- 3.5206)  data: 0.0251 (0.0002 -- 0.3256)  max mem: 16413
Epoch: [130] Total time: 0:00:50 (0.9268 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8870 (1.7444)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9981 (7.7733)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9102 (0.9102)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0326 (2.0326 -- 2.0326)  data: 1.8571 (1.8571 -- 1.8571)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5322 (0.6644)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3410 (0.0224 -- 2.0326)  data: 0.1858 (0.0001 -- 1.8571)  max mem: 16413
Val: Total time: 0:00:03 (0.3411 s / it)
* Acc@1 82.317 Acc@5 93.902 loss 0.798
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [131]  [ 0/54]  eta: 0:06:44  lr: 0.000013  min_lr: 0.000000  loss: 2.0415 (2.0415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8089 (8.8089)  time: 7.4911 (7.4911 -- 7.4911)  data: 6.9619 (6.9619 -- 6.9619)  max mem: 16413
[2023-10-23 22:03:59,004] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:03:59,004] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:03:59,005] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:03:59,005] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [131]  [20/54]  eta: 0:00:39  lr: 0.000013  min_lr: 0.000000  loss: 1.6605 (1.7608)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1809 (7.9927)  time: 0.8592 (0.5220 -- 4.5171)  data: 0.3073 (0.0004 -- 3.9675)  max mem: 16413
[2023-10-23 22:04:13,114] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7102
[2023-10-23 22:04:13,115] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:04:13,114] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7102
[2023-10-23 22:04:13,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 22:04:13,115] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [131]  [40/54]  eta: 0:00:14  lr: 0.000013  min_lr: 0.000000  loss: 1.9433 (1.8182)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0546 (7.7477)  time: 0.8715 (0.5133 -- 2.8994)  data: 0.2865 (0.0003 -- 2.3788)  max mem: 16413
Epoch: [131]  [53/54]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.6816 (1.7860)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9782 (8.0110)  time: 0.6890 (0.4973 -- 2.2127)  data: 0.1676 (0.0001 -- 1.7026)  max mem: 16413
Epoch: [131] Total time: 0:00:50 (0.9342 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.6816 (1.7838)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9782 (8.0110)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9099 (0.9099)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9716 (1.9716 -- 1.9716)  data: 1.7730 (1.7730 -- 1.7730)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5908 (0.6699)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (93.9024)  time: 0.3349 (0.0223 -- 1.9716)  data: 0.1774 (0.0001 -- 1.7730)  max mem: 16413
Val: Total time: 0:00:03 (0.3350 s / it)
* Acc@1 81.707 Acc@5 93.902 loss 0.801
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [132]  [ 0/54]  eta: 0:07:32  lr: 0.000013  min_lr: 0.000000  loss: 1.3078 (1.3078)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1791 (7.1791)  time: 8.3759 (8.3759 -- 8.3759)  data: 6.0355 (6.0355 -- 6.0355)  max mem: 16413
Epoch: [132]  [20/54]  eta: 0:00:39  lr: 0.000013  min_lr: 0.000000  loss: 1.7565 (1.7624)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6158 (7.3760)  time: 0.7918 (0.5293 -- 2.5957)  data: 0.1927 (0.0002 -- 2.0816)  max mem: 16413
Epoch: [132]  [40/54]  eta: 0:00:14  lr: 0.000012  min_lr: 0.000000  loss: 1.8140 (1.7738)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5359 (7.8317)  time: 0.8657 (0.5169 -- 3.0436)  data: 0.3247 (0.0004 -- 2.5092)  max mem: 16413
Epoch: [132]  [53/54]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.5655 (1.7510)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8797 (8.2897)  time: 0.6239 (0.4936 -- 2.6529)  data: 0.1077 (0.0002 -- 2.1386)  max mem: 16413
Epoch: [132] Total time: 0:00:50 (0.9304 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.5655 (1.7192)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8797 (8.2897)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9074 (0.9074)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9722 (1.9722 -- 1.9722)  data: 1.7679 (1.7679 -- 1.7679)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5874 (0.6707)  acc1: 77.7778 (79.2683)  acc5: 100.0000 (93.9024)  time: 0.3349 (0.0223 -- 1.9722)  data: 0.1769 (0.0001 -- 1.7679)  max mem: 16413
Val: Total time: 0:00:03 (0.3350 s / it)
* Acc@1 82.317 Acc@5 93.902 loss 0.799
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [133]  [ 0/54]  eta: 0:04:50  lr: 0.000012  min_lr: 0.000000  loss: 1.7434 (1.7434)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3390 (5.3390)  time: 5.3802 (5.3802 -- 5.3802)  data: 4.6927 (4.6927 -- 4.6927)  max mem: 16413
Epoch: [133]  [20/54]  eta: 0:00:39  lr: 0.000012  min_lr: 0.000000  loss: 1.8288 (1.7599)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8812 (8.1205)  time: 0.9629 (0.5120 -- 3.6684)  data: 0.2183 (0.0004 -- 2.4398)  max mem: 16413
Epoch: [133]  [40/54]  eta: 0:00:14  lr: 0.000012  min_lr: 0.000000  loss: 1.6621 (1.7491)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3576 (8.3060)  time: 0.8323 (0.5185 -- 2.7022)  data: 0.1570 (0.0002 -- 1.1927)  max mem: 16413
[2023-10-23 22:06:17,041] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:06:17,041] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:06:17,041] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:06:17,042] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [133]  [53/54]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.6574 (1.7219)  loss_scale: 16384.0000 (17901.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3049 (8.2137)  time: 0.6959 (0.4955 -- 2.4172)  data: 0.0404 (0.0002 -- 0.7941)  max mem: 16413
Epoch: [133] Total time: 0:00:50 (0.9374 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.6574 (1.6800)  loss_scale: 16384.0000 (17901.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3049 (8.2137)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9231 (0.9231)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0228 (2.0228 -- 2.0228)  data: 1.8301 (1.8301 -- 1.8301)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5791 (0.6753)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3400 (0.0222 -- 2.0228)  data: 0.1831 (0.0001 -- 1.8301)  max mem: 16413
Val: Total time: 0:00:03 (0.3402 s / it)
* Acc@1 83.537 Acc@5 93.902 loss 0.809
Accuracy of the network on the 163 val images: 83.54%
Max accuracy: 84.15%
Epoch: [134]  [ 0/54]  eta: 0:06:42  lr: 0.000012  min_lr: 0.000000  loss: 1.7479 (1.7479)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7791 (9.7791)  time: 7.4554 (7.4554 -- 7.4554)  data: 6.7366 (6.7366 -- 6.7366)  max mem: 16413
[2023-10-23 22:06:40,344] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7249
[2023-10-23 22:06:40,344] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7249
[2023-10-23 22:06:40,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:06:40,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:06:40,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [20/54]  eta: 0:00:42  lr: 0.000012  min_lr: 0.000000  loss: 1.6244 (1.6706)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0526 (7.6106)  time: 0.9398 (0.5291 -- 3.8355)  data: 0.2969 (0.0004 -- 3.0890)  max mem: 16413
Epoch: [134]  [40/54]  eta: 0:00:14  lr: 0.000012  min_lr: 0.000000  loss: 1.8484 (1.7484)  loss_scale: 16384.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8757 (7.9992)  time: 0.8575 (0.5084 -- 4.3426)  data: 0.3234 (0.0002 -- 3.8173)  max mem: 16413
Epoch: [134]  [53/54]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.6150 (1.7139)  loss_scale: 16384.0000 (20328.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7664 (7.9232)  time: 0.7798 (0.4958 -- 4.3426)  data: 0.2702 (0.0001 -- 3.8173)  max mem: 16413
Epoch: [134] Total time: 0:00:51 (0.9544 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.6150 (1.7070)  loss_scale: 16384.0000 (20328.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7664 (7.9232)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9157 (0.9157)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0250 (2.0250 -- 2.0250)  data: 1.8218 (1.8218 -- 1.8218)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5817 (0.6742)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (93.9024)  time: 0.3399 (0.0222 -- 2.0250)  data: 0.1823 (0.0001 -- 1.8218)  max mem: 16413
Val: Total time: 0:00:03 (0.3400 s / it)
* Acc@1 82.317 Acc@5 93.902 loss 0.807
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [135]  [ 0/54]  eta: 0:06:27  lr: 0.000012  min_lr: 0.000000  loss: 2.1405 (2.1405)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6727 (6.6727)  time: 7.1790 (7.1790 -- 7.1790)  data: 6.4313 (6.4313 -- 6.4313)  max mem: 16413
Epoch: [135]  [20/54]  eta: 0:00:40  lr: 0.000012  min_lr: 0.000000  loss: 1.6161 (1.6676)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1196 (7.4485)  time: 0.8792 (0.5162 -- 3.4991)  data: 0.2568 (0.0003 -- 2.9718)  max mem: 16413
Epoch: [135]  [40/54]  eta: 0:00:15  lr: 0.000012  min_lr: 0.000000  loss: 1.6914 (1.6596)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4947 (8.2810)  time: 0.9743 (0.5119 -- 4.2782)  data: 0.0556 (0.0003 -- 1.0795)  max mem: 16413
Epoch: [135]  [53/54]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7012 (1.6710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9248 (8.1018)  time: 0.6040 (0.4940 -- 2.3028)  data: 0.0007 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [135] Total time: 0:00:50 (0.9411 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7012 (1.7120)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9248 (8.1018)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9170 (0.9170)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0261 (2.0261 -- 2.0261)  data: 1.8393 (1.8393 -- 1.8393)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5723 (0.6750)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3406 (0.0220 -- 2.0261)  data: 0.1840 (0.0001 -- 1.8393)  max mem: 16413
Val: Total time: 0:00:03 (0.3407 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.805
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [136]  [ 0/54]  eta: 0:05:46  lr: 0.000011  min_lr: 0.000000  loss: 1.6754 (1.6754)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8620 (8.8620)  time: 6.4137 (6.4137 -- 6.4137)  data: 5.8767 (5.8767 -- 5.8767)  max mem: 16413
Epoch: [136]  [20/54]  eta: 0:00:39  lr: 0.000011  min_lr: 0.000000  loss: 1.8262 (1.7395)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3882 (7.6668)  time: 0.8876 (0.5276 -- 2.9849)  data: 0.3479 (0.0007 -- 2.4687)  max mem: 16413
[2023-10-23 22:08:48,382] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:08:48,382] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:08:48,382] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:08:48,382] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [136]  [40/54]  eta: 0:00:14  lr: 0.000011  min_lr: 0.000000  loss: 1.8168 (1.7518)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5758 (7.9404)  time: 0.9628 (0.5285 -- 4.2319)  data: 0.4217 (0.0003 -- 3.7008)  max mem: 16413
Epoch: [136]  [53/54]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7442 (1.7449)  loss_scale: 32768.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7602 (8.0769)  time: 0.7117 (0.4931 -- 4.2319)  data: 0.1940 (0.0002 -- 3.7008)  max mem: 16413
Epoch: [136] Total time: 0:00:50 (0.9297 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7442 (1.6840)  loss_scale: 32768.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7602 (8.0769)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9031 (0.9031)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9826 (1.9826 -- 1.9826)  data: 1.7796 (1.7796 -- 1.7796)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5678 (0.6676)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3365 (0.0224 -- 1.9826)  data: 0.1780 (0.0001 -- 1.7796)  max mem: 16413
Val: Total time: 0:00:03 (0.3366 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.792
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [137]  [ 0/54]  eta: 0:06:59  lr: 0.000011  min_lr: 0.000000  loss: 1.2760 (1.2760)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5829 (8.5829)  time: 7.7657 (7.7657 -- 7.7657)  data: 7.2460 (7.2460 -- 7.2460)  max mem: 16413
Epoch: [137]  [20/54]  eta: 0:00:40  lr: 0.000011  min_lr: 0.000000  loss: 1.8411 (1.7488)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9337 (7.5505)  time: 0.8567 (0.5206 -- 4.1697)  data: 0.1564 (0.0005 -- 2.4340)  max mem: 16413
Epoch: [137]  [40/54]  eta: 0:00:14  lr: 0.000011  min_lr: 0.000000  loss: 1.6425 (1.7087)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8622 (7.8813)  time: 0.8342 (0.5175 -- 3.0026)  data: 0.0015 (0.0004 -- 0.0039)  max mem: 16413
[2023-10-23 22:09:52,808] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7448
[2023-10-23 22:09:52,808] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:09:52,809] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7448
[2023-10-23 22:09:52,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:09:52,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [137]  [53/54]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.5244 (1.6963)  loss_scale: 32768.0000 (31554.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2522 (8.0207)  time: 0.6093 (0.4828 -- 1.7118)  data: 0.0160 (0.0002 -- 0.3020)  max mem: 16413
Epoch: [137] Total time: 0:00:48 (0.9036 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.5244 (1.6942)  loss_scale: 32768.0000 (31554.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2522 (8.0207)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9064 (0.9064)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0556 (2.0556 -- 2.0556)  data: 1.8770 (1.8770 -- 1.8770)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5710 (0.6722)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3431 (0.0223 -- 2.0556)  data: 0.1878 (0.0001 -- 1.8770)  max mem: 16413
Val: Total time: 0:00:03 (0.3432 s / it)
* Acc@1 82.317 Acc@5 95.122 loss 0.800
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [138]  [ 0/54]  eta: 0:05:38  lr: 0.000011  min_lr: 0.000000  loss: 1.6944 (1.6944)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4651 (6.4651)  time: 6.2751 (6.2751 -- 6.2751)  data: 5.7167 (5.7167 -- 5.7167)  max mem: 16413
Epoch: [138]  [20/54]  eta: 0:00:42  lr: 0.000011  min_lr: 0.000000  loss: 1.7484 (1.7395)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3818 (7.8067)  time: 0.9884 (0.5223 -- 3.7933)  data: 0.3630 (0.0003 -- 3.2678)  max mem: 16413
Epoch: [138]  [40/54]  eta: 0:00:14  lr: 0.000011  min_lr: 0.000000  loss: 1.5831 (1.6808)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0833 (7.8494)  time: 0.8625 (0.5116 -- 2.4806)  data: 0.2403 (0.0004 -- 1.9493)  max mem: 16413
Epoch: [138]  [53/54]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.6815 (1.7059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6311 (7.8771)  time: 0.6552 (0.4968 -- 2.0431)  data: 0.0774 (0.0001 -- 1.5363)  max mem: 16413
Epoch: [138] Total time: 0:00:50 (0.9389 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.6815 (1.7473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6311 (7.8771)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9066 (0.9066)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0568 (2.0568 -- 2.0568)  data: 1.8503 (1.8503 -- 1.8503)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5599 (0.6707)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (93.9024)  time: 0.3442 (0.0231 -- 2.0568)  data: 0.1851 (0.0001 -- 1.8503)  max mem: 16413
Val: Total time: 0:00:03 (0.3443 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.797
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [139]  [ 0/54]  eta: 0:06:14  lr: 0.000010  min_lr: 0.000000  loss: 1.6335 (1.6335)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1598 (9.1598)  time: 6.9409 (6.9409 -- 6.9409)  data: 6.0979 (6.0979 -- 6.0979)  max mem: 16413
Epoch: [139]  [20/54]  eta: 0:00:39  lr: 0.000010  min_lr: 0.000000  loss: 1.6462 (1.6728)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6694 (8.5262)  time: 0.8834 (0.5155 -- 2.3983)  data: 0.2580 (0.0003 -- 1.8797)  max mem: 16413
Epoch: [139]  [40/54]  eta: 0:00:14  lr: 0.000010  min_lr: 0.000000  loss: 1.8230 (1.7282)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5484 (8.0905)  time: 0.9615 (0.5267 -- 4.7295)  data: 0.3257 (0.0003 -- 4.2122)  max mem: 16413
[2023-10-23 22:11:36,901] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7548
[2023-10-23 22:11:36,901] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7548
[2023-10-23 22:11:36,901] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:11:36,901] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:11:36,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [139]  [53/54]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7079 (1.6909)  loss_scale: 8192.0000 (14563.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6208 (7.8945)  time: 0.7431 (0.4950 -- 4.7295)  data: 0.2248 (0.0001 -- 4.2122)  max mem: 16413
Epoch: [139] Total time: 0:00:50 (0.9392 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7079 (1.7152)  loss_scale: 8192.0000 (14563.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6208 (7.8945)
[2023-10-23 22:11:42,721] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-10-23 22:11:42,724] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-10-23 22:11:42,724] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-139/mp_rank_00_model_states.pt
[2023-10-23 22:11:42,724] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-139/mp_rank_00_model_states.pt...
[2023-10-23 22:11:43,768] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-139/mp_rank_00_model_states.pt.
[2023-10-23 22:11:43,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9060 (0.9060)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9660 (1.9660 -- 1.9660)  data: 1.7757 (1.7757 -- 1.7757)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5667 (0.6734)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3351 (0.0222 -- 1.9660)  data: 0.1777 (0.0001 -- 1.7757)  max mem: 16413
Val: Total time: 0:00:03 (0.3353 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.797
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [140]  [ 0/54]  eta: 0:06:48  lr: 0.000010  min_lr: 0.000000  loss: 1.4826 (1.4826)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7318 (7.7318)  time: 7.5641 (7.5641 -- 7.5641)  data: 6.9938 (6.9938 -- 6.9938)  max mem: 16413
Epoch: [140]  [20/54]  eta: 0:00:40  lr: 0.000010  min_lr: 0.000000  loss: 1.8221 (1.7352)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7067 (7.8625)  time: 0.8752 (0.5243 -- 4.1690)  data: 0.3264 (0.0005 -- 3.6424)  max mem: 16413
[2023-10-23 22:12:17,898] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7584
[2023-10-23 22:12:17,898] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7584
[2023-10-23 22:12:17,898] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-10-23 22:12:17,898] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-10-23 22:12:17,899] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [140]  [40/54]  eta: 0:00:15  lr: 0.000010  min_lr: 0.000000  loss: 1.6219 (1.6789)  loss_scale: 4096.0000 (6493.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3565 (8.0556)  time: 1.0523 (0.5162 -- 4.3604)  data: 0.5122 (0.0001 -- 3.8605)  max mem: 16413
Epoch: [140]  [53/54]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.6664 (1.6887)  loss_scale: 4096.0000 (5916.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9230 (8.4118)  time: 0.7085 (0.4937 -- 4.3604)  data: 0.1936 (0.0001 -- 3.8605)  max mem: 16413
Epoch: [140] Total time: 0:00:52 (0.9756 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.6664 (1.7221)  loss_scale: 4096.0000 (5916.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9230 (8.4118)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9080 (0.9080)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9757 (1.9757 -- 1.9757)  data: 1.7665 (1.7665 -- 1.7665)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5729 (0.6794)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3353 (0.0224 -- 1.9757)  data: 0.1768 (0.0001 -- 1.7665)  max mem: 16413
Val: Total time: 0:00:03 (0.3354 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.807
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [141]  [ 0/54]  eta: 0:06:42  lr: 0.000010  min_lr: 0.000000  loss: 1.6607 (1.6607)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4090 (8.4090)  time: 7.4580 (7.4580 -- 7.4580)  data: 6.5914 (6.5914 -- 6.5914)  max mem: 16413
Epoch: [141]  [20/54]  eta: 0:00:46  lr: 0.000010  min_lr: 0.000000  loss: 1.8197 (1.7523)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0003 (8.8974)  time: 1.0714 (0.5158 -- 6.4086)  data: 0.0632 (0.0003 -- 1.2424)  max mem: 16413
Epoch: [141]  [40/54]  eta: 0:00:15  lr: 0.000010  min_lr: 0.000000  loss: 1.5457 (1.6851)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7210 (7.9351)  time: 0.8440 (0.5312 -- 4.2232)  data: 0.0014 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [141]  [53/54]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7987 (1.7338)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8629 (8.1320)  time: 0.7320 (0.4936 -- 4.2232)  data: 0.0008 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [141] Total time: 0:00:52 (0.9792 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7987 (1.7275)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8629 (8.1320)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9179 (0.9179)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0176 (2.0176 -- 2.0176)  data: 1.8369 (1.8369 -- 1.8369)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5764 (0.6806)  acc1: 77.7778 (79.2683)  acc5: 88.8889 (92.6829)  time: 0.3404 (0.0227 -- 2.0176)  data: 0.1838 (0.0001 -- 1.8369)  max mem: 16413
Val: Total time: 0:00:03 (0.3406 s / it)
* Acc@1 82.317 Acc@5 93.293 loss 0.812
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [142]  [ 0/54]  eta: 0:08:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8393 (1.8393)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9062 (10.9062)  time: 8.8935 (8.8935 -- 8.8935)  data: 8.3445 (8.3445 -- 8.3445)  max mem: 16413
Epoch: [142]  [20/54]  eta: 0:00:43  lr: 0.000009  min_lr: 0.000000  loss: 1.6641 (1.7016)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0642 (8.0609)  time: 0.8844 (0.5210 -- 5.0166)  data: 0.3446 (0.0003 -- 4.4920)  max mem: 16413
Epoch: [142]  [40/54]  eta: 0:00:15  lr: 0.000009  min_lr: 0.000000  loss: 1.7606 (1.7681)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3062 (7.6593)  time: 0.9634 (0.5289 -- 3.4206)  data: 0.4171 (0.0002 -- 2.8920)  max mem: 16413
[2023-10-23 22:14:28,045] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:14:28,045] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-10-23 22:14:28,045] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:14:28,045] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [142]  [53/54]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.6336 (1.6912)  loss_scale: 4096.0000 (4778.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7042 (7.5100)  time: 0.6637 (0.4935 -- 3.0578)  data: 0.1505 (0.0001 -- 2.5527)  max mem: 16413
Epoch: [142] Total time: 0:00:52 (0.9774 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.6336 (1.7332)  loss_scale: 4096.0000 (4778.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7042 (7.5100)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9114 (0.9114)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9877 (1.9877 -- 1.9877)  data: 1.7696 (1.7696 -- 1.7696)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5714 (0.6690)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3358 (0.0221 -- 1.9877)  data: 0.1770 (0.0001 -- 1.7696)  max mem: 16413
Val: Total time: 0:00:03 (0.3359 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.800
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [143]  [ 0/54]  eta: 0:07:01  lr: 0.000009  min_lr: 0.000000  loss: 1.2130 (1.2130)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2597 (10.2597)  time: 7.7978 (7.7978 -- 7.7978)  data: 7.2801 (7.2801 -- 7.2801)  max mem: 16413
Epoch: [143]  [20/54]  eta: 0:00:38  lr: 0.000009  min_lr: 0.000000  loss: 1.8729 (1.7446)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4815 (7.8283)  time: 0.7947 (0.5251 -- 3.0457)  data: 0.1739 (0.0006 -- 1.7685)  max mem: 16413
Epoch: [143]  [40/54]  eta: 0:00:14  lr: 0.000009  min_lr: 0.000000  loss: 1.6803 (1.7130)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5653 (8.0552)  time: 0.9660 (0.5279 -- 3.2766)  data: 0.3053 (0.0005 -- 2.6341)  max mem: 16413
Epoch: [143]  [53/54]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.5571 (1.6927)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8990 (7.8129)  time: 0.7648 (0.4969 -- 3.2766)  data: 0.1774 (0.0001 -- 2.6341)  max mem: 16413
Epoch: [143] Total time: 0:00:51 (0.9470 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.5571 (1.6977)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8990 (7.8129)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9161 (0.9161)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0056 (2.0056 -- 2.0056)  data: 1.7981 (1.7981 -- 1.7981)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5664 (0.6703)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3390 (0.0223 -- 2.0056)  data: 0.1799 (0.0001 -- 1.7981)  max mem: 16413
Val: Total time: 0:00:03 (0.3391 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.801
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [144]  [ 0/54]  eta: 0:06:26  lr: 0.000009  min_lr: 0.000000  loss: 1.7099 (1.7099)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2252 (6.2252)  time: 7.1587 (7.1587 -- 7.1587)  data: 6.3491 (6.3491 -- 6.3491)  max mem: 16413
Epoch: [144]  [20/54]  eta: 0:00:38  lr: 0.000009  min_lr: 0.000000  loss: 1.5810 (1.6375)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0354 (8.5399)  time: 0.8380 (0.5226 -- 3.1852)  data: 0.2270 (0.0004 -- 2.6619)  max mem: 16413
Epoch: [144]  [40/54]  eta: 0:00:14  lr: 0.000009  min_lr: 0.000000  loss: 1.8125 (1.7323)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9155 (8.3090)  time: 0.9458 (0.5252 -- 3.7785)  data: 0.3551 (0.0005 -- 3.2374)  max mem: 16413
Epoch: [144]  [53/54]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8458 (1.7267)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6845 (8.0351)  time: 0.6574 (0.4947 -- 3.2239)  data: 0.0887 (0.0001 -- 1.7583)  max mem: 16413
Epoch: [144] Total time: 0:00:49 (0.9156 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.8458 (1.6865)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6845 (8.0351)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9023 (0.9023)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0295 (2.0295 -- 2.0295)  data: 1.8229 (1.8229 -- 1.8229)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5662 (0.6664)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3428 (0.0231 -- 2.0295)  data: 0.1824 (0.0001 -- 1.8229)  max mem: 16413
Val: Total time: 0:00:03 (0.3430 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.794
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [145]  [ 0/54]  eta: 0:08:37  lr: 0.000009  min_lr: 0.000000  loss: 1.5164 (1.5164)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4244 (6.4244)  time: 9.5888 (9.5888 -- 9.5888)  data: 9.0366 (9.0366 -- 9.0366)  max mem: 16413
[2023-10-23 22:16:41,704] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:16:41,704] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 22:16:41,705] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:16:41,705] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [145]  [20/54]  eta: 0:00:41  lr: 0.000009  min_lr: 0.000000  loss: 1.8306 (1.7899)  loss_scale: 8192.0000 (12092.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9899 (7.7217)  time: 0.7947 (0.5159 -- 3.2442)  data: 0.2452 (0.0005 -- 2.7128)  max mem: 16413
Epoch: [145]  [40/54]  eta: 0:00:15  lr: 0.000008  min_lr: 0.000000  loss: 1.4972 (1.7032)  loss_scale: 16384.0000 (14186.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6457 (8.0131)  time: 0.9366 (0.5235 -- 3.5066)  data: 0.3892 (0.0004 -- 2.9843)  max mem: 16413
Epoch: [145]  [53/54]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6771 (1.7042)  loss_scale: 16384.0000 (14715.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7093 (7.9579)  time: 0.7367 (0.4933 -- 3.4684)  data: 0.2183 (0.0001 -- 2.9649)  max mem: 16413
Epoch: [145] Total time: 0:00:52 (0.9658 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6771 (1.7217)  loss_scale: 16384.0000 (14715.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7093 (7.9579)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9038 (0.9038)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9683 (1.9683 -- 1.9683)  data: 1.7748 (1.7748 -- 1.7748)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5727 (0.6654)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3365 (0.0259 -- 1.9683)  data: 0.1791 (0.0001 -- 1.7748)  max mem: 16413
Val: Total time: 0:00:03 (0.3366 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.793
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [146]  [ 0/54]  eta: 0:07:52  lr: 0.000008  min_lr: 0.000000  loss: 0.9012 (0.9012)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0961 (7.0961)  time: 8.7463 (8.7463 -- 8.7463)  data: 8.2199 (8.2199 -- 8.2199)  max mem: 16413
Epoch: [146]  [20/54]  eta: 0:00:41  lr: 0.000008  min_lr: 0.000000  loss: 1.9519 (1.8211)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9214 (8.4201)  time: 0.8419 (0.5201 -- 3.4518)  data: 0.2913 (0.0004 -- 2.9370)  max mem: 16413
Epoch: [146]  [40/54]  eta: 0:00:14  lr: 0.000008  min_lr: 0.000000  loss: 1.7011 (1.7354)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2525 (8.6716)  time: 0.9101 (0.5248 -- 2.7677)  data: 0.2309 (0.0003 -- 2.2274)  max mem: 16413
Epoch: [146]  [53/54]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6802 (1.6835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0796 (8.3963)  time: 0.6543 (0.4960 -- 2.1681)  data: 0.0096 (0.0001 -- 0.1801)  max mem: 16413
Epoch: [146] Total time: 0:00:51 (0.9502 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6802 (1.7318)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0796 (8.3963)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9014 (0.9014)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0174 (2.0174 -- 2.0174)  data: 1.8216 (1.8216 -- 1.8216)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5811 (0.6649)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3401 (0.0221 -- 2.0174)  data: 0.1830 (0.0001 -- 1.8216)  max mem: 16413
Val: Total time: 0:00:03 (0.3402 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.795
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [147]  [ 0/54]  eta: 0:07:37  lr: 0.000008  min_lr: 0.000000  loss: 2.3190 (2.3190)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3793 (8.3793)  time: 8.4656 (8.4656 -- 8.4656)  data: 7.3833 (7.3833 -- 7.3833)  max mem: 16413
[2023-10-23 22:18:29,335] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7948
[2023-10-23 22:18:29,335] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7948
[2023-10-23 22:18:29,335] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:18:29,335] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:18:29,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [147]  [20/54]  eta: 0:00:40  lr: 0.000008  min_lr: 0.000000  loss: 1.6723 (1.7235)  loss_scale: 8192.0000 (12092.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8173 (7.7398)  time: 0.8148 (0.5219 -- 3.2043)  data: 0.1982 (0.0004 -- 2.6668)  max mem: 16413
Epoch: [147]  [40/54]  eta: 0:00:14  lr: 0.000008  min_lr: 0.000000  loss: 1.7581 (1.7049)  loss_scale: 8192.0000 (10190.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8686 (7.7313)  time: 0.8131 (0.5286 -- 2.2768)  data: 0.1401 (0.0005 -- 1.5871)  max mem: 16413
Epoch: [147]  [53/54]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.9439 (1.7531)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2658 (7.9037)  time: 0.7165 (0.4953 -- 2.1238)  data: 0.1234 (0.0002 -- 1.5871)  max mem: 16413
Epoch: [147] Total time: 0:00:49 (0.9115 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.9439 (1.7565)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2658 (7.9037)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8972 (0.8972)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0195 (2.0195 -- 2.0195)  data: 1.8152 (1.8152 -- 1.8152)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5717 (0.6606)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3400 (0.0223 -- 2.0195)  data: 0.1816 (0.0001 -- 1.8152)  max mem: 16413
Val: Total time: 0:00:03 (0.3401 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.792
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [148]  [ 0/54]  eta: 0:05:58  lr: 0.000008  min_lr: 0.000000  loss: 1.0723 (1.0723)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7382 (6.7382)  time: 6.6410 (6.6410 -- 6.6410)  data: 6.0969 (6.0969 -- 6.0969)  max mem: 16413
[2023-10-23 22:19:18,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[1.8429857026298065e-07, 1.8429857026298065e-07, 2.4573142701730754e-07, 2.4573142701730754e-07, 3.276419026897434e-07, 3.276419026897434e-07, 4.368558702529912e-07, 4.368558702529912e-07, 5.824744936706549e-07, 5.824744936706549e-07, 7.766326582275399e-07, 7.766326582275399e-07, 1.0355102109700531e-06, 1.0355102109700531e-06, 1.3806802812934043e-06, 1.3806802812934043e-06, 1.840907041724539e-06, 1.840907041724539e-06, 2.454542722299385e-06, 2.454542722299385e-06, 3.2727236297325138e-06, 3.2727236297325138e-06, 4.363631506310018e-06, 4.363631506310018e-06, 5.818175341746691e-06, 5.818175341746691e-06, 7.757567122328922e-06, 7.757567122328922e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 22:19:18,886] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=18.524381020579348, CurrSamplesPerSec=22.805634660529783, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [148]  [20/54]  eta: 0:00:40  lr: 0.000008  min_lr: 0.000000  loss: 1.4699 (1.5465)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1553 (7.9901)  time: 0.9280 (0.5159 -- 3.6095)  data: 0.0410 (0.0002 -- 0.7916)  max mem: 16413
Epoch: [148]  [40/54]  eta: 0:00:14  lr: 0.000008  min_lr: 0.000000  loss: 1.7293 (1.6290)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9980 (8.1582)  time: 0.8377 (0.5274 -- 3.0049)  data: 0.0015 (0.0005 -- 0.0040)  max mem: 16413
Epoch: [148]  [53/54]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6658 (1.6631)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9417 (8.1435)  time: 0.7378 (0.4959 -- 2.4386)  data: 0.0009 (0.0001 -- 0.0040)  max mem: 16413
Epoch: [148] Total time: 0:00:51 (0.9617 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6658 (1.7496)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9417 (8.1435)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8976 (0.8976)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0805 (2.0805 -- 2.0805)  data: 1.8989 (1.8989 -- 1.8989)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5715 (0.6569)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3456 (0.0221 -- 2.0805)  data: 0.1900 (0.0001 -- 1.8989)  max mem: 16413
Val: Total time: 0:00:03 (0.3457 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.789
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [149]  [ 0/54]  eta: 0:05:54  lr: 0.000008  min_lr: 0.000000  loss: 2.0675 (2.0675)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5814 (6.5814)  time: 6.5718 (6.5718 -- 6.5718)  data: 6.0438 (6.0438 -- 6.0438)  max mem: 16413
Epoch: [149]  [20/54]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000000  loss: 1.6272 (1.6293)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6298 (7.8697)  time: 0.8866 (0.5379 -- 2.9019)  data: 0.2188 (0.0001 -- 2.3652)  max mem: 16413
[2023-10-23 22:20:36,049] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:20:36,049] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 22:20:36,050] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:20:36,050] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [149]  [40/54]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 1.8100 (1.6824)  loss_scale: 8192.0000 (10190.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1125 (7.8836)  time: 0.9067 (0.5149 -- 3.6597)  data: 0.3668 (0.0005 -- 3.1382)  max mem: 16413
Epoch: [149]  [53/54]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.6886 (1.6893)  loss_scale: 16384.0000 (11681.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6449 (8.1383)  time: 0.7269 (0.4971 -- 3.6597)  data: 0.2107 (0.0002 -- 3.1382)  max mem: 16413
Epoch: [149] Total time: 0:00:50 (0.9271 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.6886 (1.6635)  loss_scale: 16384.0000 (11681.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6449 (8.1383)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8966 (0.8966)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9894 (1.9894 -- 1.9894)  data: 1.7623 (1.7623 -- 1.7623)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5591 (0.6489)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3368 (0.0225 -- 1.9894)  data: 0.1763 (0.0001 -- 1.7623)  max mem: 16413
Val: Total time: 0:00:03 (0.3370 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [150]  [ 0/54]  eta: 0:05:55  lr: 0.000007  min_lr: 0.000000  loss: 1.8857 (1.8857)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3791 (11.3791)  time: 6.5800 (6.5800 -- 6.5800)  data: 6.0686 (6.0686 -- 6.0686)  max mem: 16413
Epoch: [150]  [20/54]  eta: 0:00:41  lr: 0.000007  min_lr: 0.000000  loss: 1.7317 (1.6997)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1592 (8.5961)  time: 0.9599 (0.5238 -- 3.8894)  data: 0.4215 (0.0004 -- 3.3673)  max mem: 16413
Epoch: [150]  [40/54]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 1.6976 (1.7055)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4155 (8.2674)  time: 0.7885 (0.5299 -- 2.6534)  data: 0.2447 (0.0003 -- 2.1297)  max mem: 16413
Epoch: [150]  [53/54]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7100 (1.7204)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4155 (8.3488)  time: 0.6884 (0.4965 -- 2.6471)  data: 0.1638 (0.0002 -- 2.1365)  max mem: 16413
Epoch: [150] Total time: 0:00:50 (0.9324 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7100 (1.6761)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4155 (8.3488)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9037 (0.9037)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0114 (2.0114 -- 2.0114)  data: 1.8057 (1.8057 -- 1.8057)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5722 (0.6510)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3388 (0.0222 -- 2.0114)  data: 0.1807 (0.0001 -- 1.8057)  max mem: 16413
Val: Total time: 0:00:03 (0.3389 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.791
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [151]  [ 0/54]  eta: 0:07:10  lr: 0.000007  min_lr: 0.000000  loss: 2.0993 (2.0993)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0611 (10.0611)  time: 7.9743 (7.9743 -- 7.9743)  data: 7.4586 (7.4586 -- 7.4586)  max mem: 16413
Epoch: [151]  [20/54]  eta: 0:00:40  lr: 0.000007  min_lr: 0.000000  loss: 1.7003 (1.7116)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9685 (8.1322)  time: 0.8498 (0.5182 -- 3.4710)  data: 0.2460 (0.0003 -- 2.9052)  max mem: 16413
Epoch: [151]  [40/54]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 1.7034 (1.7079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5858 (8.1409)  time: 0.8927 (0.5301 -- 3.0854)  data: 0.3229 (0.0007 -- 2.5396)  max mem: 16413
[2023-10-23 22:22:39,152] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:22:39,152] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:22:39,153] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:22:39,153] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [151]  [53/54]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8567 (1.7150)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6027 (7.9786)  time: 0.7056 (0.4935 -- 2.7980)  data: 0.1825 (0.0001 -- 2.2557)  max mem: 16413
Epoch: [151] Total time: 0:00:50 (0.9414 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8567 (1.6964)  loss_scale: 16384.0000 (17294.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6027 (7.9786)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9109 (0.9109)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0265 (2.0265 -- 2.0265)  data: 1.8266 (1.8266 -- 1.8266)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5676 (0.6509)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3402 (0.0223 -- 2.0265)  data: 0.1828 (0.0001 -- 1.8266)  max mem: 16413
Val: Total time: 0:00:03 (0.3403 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.793
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [152]  [ 0/54]  eta: 0:05:12  lr: 0.000007  min_lr: 0.000000  loss: 1.3180 (1.3180)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8844 (7.8844)  time: 5.7924 (5.7924 -- 5.7924)  data: 4.7569 (4.7569 -- 4.7569)  max mem: 16413
[2023-10-23 22:22:51,730] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8209
[2023-10-23 22:22:51,730] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8209
[2023-10-23 22:22:51,731] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:22:51,731] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:22:51,731] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [152]  [20/54]  eta: 0:00:41  lr: 0.000007  min_lr: 0.000000  loss: 1.6663 (1.6378)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6261 (7.8553)  time: 1.0000 (0.5204 -- 3.4474)  data: 0.3122 (0.0006 -- 2.4020)  max mem: 16413
Epoch: [152]  [40/54]  eta: 0:00:14  lr: 0.000007  min_lr: 0.000000  loss: 1.7405 (1.6577)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0703 (7.7305)  time: 0.8513 (0.5214 -- 3.3750)  data: 0.0389 (0.0002 -- 0.5880)  max mem: 16413
Epoch: [152]  [53/54]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7446 (1.6976)  loss_scale: 16384.0000 (16687.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8909 (8.0107)  time: 0.8018 (0.4943 -- 3.3750)  data: 0.0085 (0.0001 -- 0.1577)  max mem: 16413
Epoch: [152] Total time: 0:00:52 (0.9653 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7446 (1.7155)  loss_scale: 16384.0000 (16687.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8909 (8.0107)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9087 (0.9087)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9868 (1.9868 -- 1.9868)  data: 1.7456 (1.7456 -- 1.7456)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5727 (0.6494)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3369 (0.0223 -- 1.9868)  data: 0.1746 (0.0001 -- 1.7456)  max mem: 16413
Val: Total time: 0:00:03 (0.3370 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.790
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [153]  [ 0/54]  eta: 0:05:21  lr: 0.000006  min_lr: 0.000000  loss: 2.0437 (2.0437)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7297 (7.7297)  time: 5.9513 (5.9513 -- 5.9513)  data: 5.3258 (5.3258 -- 5.3258)  max mem: 16413
[2023-10-23 22:23:47,534] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8264
[2023-10-23 22:23:47,534] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8264
[2023-10-23 22:23:47,535] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:23:47,535] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:23:47,535] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [153]  [20/54]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.7762 (1.7903)  loss_scale: 8192.0000 (8972.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9547 (8.7269)  time: 0.8754 (0.5196 -- 2.9080)  data: 0.2039 (0.0004 -- 1.6175)  max mem: 16413
Epoch: [153]  [40/54]  eta: 0:00:14  lr: 0.000006  min_lr: 0.000000  loss: 1.5527 (1.7170)  loss_scale: 8192.0000 (8591.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2147 (8.1050)  time: 0.9063 (0.5230 -- 2.3165)  data: 0.2980 (0.0004 -- 1.7932)  max mem: 16413
Epoch: [153]  [53/54]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.5104 (1.7248)  loss_scale: 8192.0000 (8495.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7023 (7.9335)  time: 0.7229 (0.4939 -- 2.3401)  data: 0.2067 (0.0002 -- 1.8182)  max mem: 16413
Epoch: [153] Total time: 0:00:51 (0.9499 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.5104 (1.7399)  loss_scale: 8192.0000 (8495.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7023 (7.9335)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9088 (0.9088)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9828 (1.9828 -- 1.9828)  data: 1.7740 (1.7740 -- 1.7740)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5713 (0.6477)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3357 (0.0224 -- 1.9828)  data: 0.1775 (0.0001 -- 1.7740)  max mem: 16413
Val: Total time: 0:00:03 (0.3358 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.790
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [154]  [ 0/54]  eta: 0:06:13  lr: 0.000006  min_lr: 0.000000  loss: 1.4595 (1.4595)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7150 (6.7150)  time: 6.9155 (6.9155 -- 6.9155)  data: 4.6646 (4.6646 -- 4.6646)  max mem: 16413
Epoch: [154]  [20/54]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000000  loss: 1.6300 (1.5945)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4618 (7.3915)  time: 0.8852 (0.5269 -- 2.4002)  data: 0.1233 (0.0007 -- 1.2822)  max mem: 16413
Epoch: [154]  [40/54]  eta: 0:00:14  lr: 0.000006  min_lr: 0.000000  loss: 1.5511 (1.6079)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5943 (7.9234)  time: 0.9236 (0.5148 -- 3.0579)  data: 0.3442 (0.0003 -- 2.5637)  max mem: 16413
Epoch: [154]  [53/54]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7904 (1.6288)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2603 (7.7922)  time: 0.7350 (0.4927 -- 3.0579)  data: 0.2268 (0.0001 -- 2.5637)  max mem: 16413
Epoch: [154] Total time: 0:00:51 (0.9553 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7904 (1.6877)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2603 (7.7922)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9064 (0.9064)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0054 (2.0054 -- 2.0054)  data: 1.8059 (1.8059 -- 1.8059)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5686 (0.6462)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3385 (0.0224 -- 2.0054)  data: 0.1807 (0.0001 -- 1.8059)  max mem: 16413
Val: Total time: 0:00:03 (0.3386 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.788
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [155]  [ 0/54]  eta: 0:06:09  lr: 0.000006  min_lr: 0.000000  loss: 1.4366 (1.4366)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6199 (7.6199)  time: 6.8436 (6.8436 -- 6.8436)  data: 6.2784 (6.2784 -- 6.2784)  max mem: 16413
Epoch: [155]  [20/54]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.7442 (1.7322)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3687 (7.5897)  time: 0.8290 (0.5310 -- 3.2412)  data: 0.2068 (0.0003 -- 2.6618)  max mem: 16413
[2023-10-23 22:25:54,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:25:54,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 22:25:54,133] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:25:54,133] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [155]  [40/54]  eta: 0:00:13  lr: 0.000006  min_lr: 0.000000  loss: 1.6339 (1.7421)  loss_scale: 16384.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9905 (7.4793)  time: 0.8350 (0.5314 -- 1.9967)  data: 0.2068 (0.0004 -- 1.4533)  max mem: 16413
Epoch: [155]  [53/54]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.5936 (1.7010)  loss_scale: 16384.0000 (12894.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9047 (7.9052)  time: 0.7897 (0.4946 -- 2.3545)  data: 0.1626 (0.0001 -- 1.8453)  max mem: 16413
Epoch: [155] Total time: 0:00:50 (0.9279 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.5936 (1.6722)  loss_scale: 16384.0000 (12894.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9047 (7.9052)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9149 (0.9149)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0374 (2.0374 -- 2.0374)  data: 1.8612 (1.8612 -- 1.8612)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5649 (0.6482)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3412 (0.0222 -- 2.0374)  data: 0.1862 (0.0001 -- 1.8612)  max mem: 16413
Val: Total time: 0:00:03 (0.3413 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.792
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [156]  [ 0/54]  eta: 0:06:23  lr: 0.000006  min_lr: 0.000000  loss: 1.8145 (1.8145)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3961 (9.3961)  time: 7.1089 (7.1089 -- 7.1089)  data: 6.5969 (6.5969 -- 6.5969)  max mem: 16413
Epoch: [156]  [20/54]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000000  loss: 1.9182 (1.7904)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2185 (8.5364)  time: 0.8770 (0.5370 -- 3.2077)  data: 0.2902 (0.0005 -- 2.6701)  max mem: 16413
Epoch: [156]  [40/54]  eta: 0:00:14  lr: 0.000006  min_lr: 0.000000  loss: 1.6834 (1.7029)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9771 (8.6940)  time: 0.8550 (0.5255 -- 2.7521)  data: 0.0643 (0.0004 -- 0.3701)  max mem: 16413
Epoch: [156]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7134 (1.7138)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1750 (8.4277)  time: 0.6742 (0.4946 -- 1.5783)  data: 0.0920 (0.0001 -- 0.7755)  max mem: 16413
Epoch: [156] Total time: 0:00:49 (0.9245 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7134 (1.7630)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1750 (8.4277)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9104 (0.9104)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9780 (1.9780 -- 1.9780)  data: 1.7853 (1.7853 -- 1.7853)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5714 (0.6491)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3358 (0.0225 -- 1.9780)  data: 0.1786 (0.0001 -- 1.7853)  max mem: 16413
Val: Total time: 0:00:03 (0.3360 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.792
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [157]  [ 0/54]  eta: 0:07:30  lr: 0.000005  min_lr: 0.000000  loss: 1.5010 (1.5010)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9523 (8.9523)  time: 8.3417 (8.3417 -- 8.3417)  data: 6.2556 (6.2556 -- 6.2556)  max mem: 16413
Epoch: [157]  [20/54]  eta: 0:00:42  lr: 0.000005  min_lr: 0.000000  loss: 1.6990 (1.7202)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6357 (8.1399)  time: 0.9070 (0.5193 -- 5.8155)  data: 0.0846 (0.0003 -- 1.6697)  max mem: 16413
Epoch: [157]  [40/54]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 1.6767 (1.7290)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9575 (8.2926)  time: 0.8226 (0.5175 -- 3.6708)  data: 0.0281 (0.0003 -- 0.5414)  max mem: 16413
[2023-10-23 22:28:00,572] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:28:00,572] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:28:00,573] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:28:00,573] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [157]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.5373 (1.7420)  loss_scale: 32768.0000 (19721.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8576 (8.1046)  time: 0.6795 (0.4964 -- 2.0876)  data: 0.0077 (0.0002 -- 0.1405)  max mem: 16413
Epoch: [157] Total time: 0:00:51 (0.9473 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.5373 (1.7412)  loss_scale: 32768.0000 (19721.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8576 (8.1046)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9056 (0.9056)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9904 (1.9904 -- 1.9904)  data: 1.7853 (1.7853 -- 1.7853)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5576 (0.6480)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3369 (0.0221 -- 1.9904)  data: 0.1786 (0.0001 -- 1.7853)  max mem: 16413
Val: Total time: 0:00:03 (0.3370 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.789
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [158]  [ 0/54]  eta: 0:06:21  lr: 0.000005  min_lr: 0.000000  loss: 1.4917 (1.4917)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2611 (9.2611)  time: 7.0638 (7.0638 -- 7.0638)  data: 6.3102 (6.3102 -- 6.3102)  max mem: 16413
[2023-10-23 22:28:27,171] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8542
[2023-10-23 22:28:27,171] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8542
[2023-10-23 22:28:27,171] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:28:27,171] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:28:27,171] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [158]  [20/54]  eta: 0:00:39  lr: 0.000005  min_lr: 0.000000  loss: 1.7359 (1.6951)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1928 (7.2657)  time: 0.8708 (0.5205 -- 2.3079)  data: 0.2692 (0.0003 -- 1.8099)  max mem: 16413
Epoch: [158]  [40/54]  eta: 0:00:13  lr: 0.000005  min_lr: 0.000000  loss: 1.7155 (1.6981)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6859 (7.4401)  time: 0.8213 (0.5182 -- 2.3107)  data: 0.2759 (0.0004 -- 1.7626)  max mem: 16413
Epoch: [158]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.5628 (1.6604)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4879 (7.8147)  time: 0.7534 (0.4975 -- 2.2414)  data: 0.1003 (0.0002 -- 1.1554)  max mem: 16413
Epoch: [158] Total time: 0:00:50 (0.9302 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.5628 (1.7012)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4879 (7.8147)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9001 (0.9001)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9811 (1.9811 -- 1.9811)  data: 1.7719 (1.7719 -- 1.7719)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5612 (0.6469)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3361 (0.0222 -- 1.9811)  data: 0.1773 (0.0001 -- 1.7719)  max mem: 16413
Val: Total time: 0:00:03 (0.3362 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.787
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [159]  [ 0/54]  eta: 0:05:33  lr: 0.000005  min_lr: 0.000000  loss: 1.9290 (1.9290)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5224 (4.5224)  time: 6.1839 (6.1839 -- 6.1839)  data: 5.6510 (5.6510 -- 5.6510)  max mem: 16413
Epoch: [159]  [20/54]  eta: 0:00:41  lr: 0.000005  min_lr: 0.000000  loss: 1.7949 (1.7695)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5375 (7.5917)  time: 0.9583 (0.5210 -- 3.3734)  data: 0.2797 (0.0003 -- 2.8398)  max mem: 16413
Epoch: [159]  [40/54]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 1.6818 (1.7313)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0520 (7.6880)  time: 0.9001 (0.5107 -- 4.2672)  data: 0.3576 (0.0004 -- 3.7454)  max mem: 16413
Epoch: [159]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7204 (1.7291)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0622 (8.0541)  time: 0.6397 (0.4943 -- 1.7582)  data: 0.1170 (0.0002 -- 1.2212)  max mem: 16413
Epoch: [159] Total time: 0:00:52 (0.9696 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7204 (1.7153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0622 (8.0541)
[2023-10-23 22:29:56,670] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-10-23 22:29:56,673] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-159/mp_rank_00_model_states.pt
[2023-10-23 22:29:56,673] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-159/mp_rank_00_model_states.pt...
[2023-10-23 22:29:56,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-10-23 22:29:57,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-159/mp_rank_00_model_states.pt.
[2023-10-23 22:29:57,710] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8960 (0.8960)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0818 (2.0818 -- 2.0818)  data: 1.9015 (1.9015 -- 1.9015)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5502 (0.6464)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3477 (0.0231 -- 2.0818)  data: 0.1903 (0.0001 -- 1.9015)  max mem: 16413
Val: Total time: 0:00:03 (0.3478 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.784
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [160]  [ 0/54]  eta: 0:07:17  lr: 0.000005  min_lr: 0.000000  loss: 2.1153 (2.1153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5671 (6.5671)  time: 8.0995 (8.0995 -- 8.0995)  data: 5.4346 (5.4346 -- 5.4346)  max mem: 16413
Epoch: [160]  [20/54]  eta: 0:00:43  lr: 0.000005  min_lr: 0.000000  loss: 1.5524 (1.6763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5683 (6.7949)  time: 0.9444 (0.5278 -- 4.3398)  data: 0.3328 (0.0004 -- 3.8048)  max mem: 16413
[2023-10-23 22:30:36,790] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:30:36,790] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:30:36,790] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:30:36,791] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [160]  [40/54]  eta: 0:00:14  lr: 0.000005  min_lr: 0.000000  loss: 1.5778 (1.6691)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0653 (7.1380)  time: 0.7220 (0.5272 -- 3.0943)  data: 0.1756 (0.0003 -- 2.5698)  max mem: 16413
Epoch: [160]  [53/54]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7050 (1.6808)  loss_scale: 32768.0000 (23362.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1678 (7.4736)  time: 0.7135 (0.4949 -- 2.3575)  data: 0.1922 (0.0001 -- 1.8527)  max mem: 16413
Epoch: [160] Total time: 0:00:50 (0.9429 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7050 (1.6757)  loss_scale: 32768.0000 (23362.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1678 (7.4736)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8919 (0.8919)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9950 (1.9950 -- 1.9950)  data: 1.7840 (1.7840 -- 1.7840)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5475 (0.6440)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3375 (0.0222 -- 1.9950)  data: 0.1785 (0.0001 -- 1.7840)  max mem: 16413
Val: Total time: 0:00:03 (0.3376 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.782
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [161]  [ 0/54]  eta: 0:07:08  lr: 0.000005  min_lr: 0.000000  loss: 1.5733 (1.5733)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8857 (12.8857)  time: 7.9266 (7.9266 -- 7.9266)  data: 7.3835 (7.3835 -- 7.3835)  max mem: 16413
Epoch: [161]  [20/54]  eta: 0:00:40  lr: 0.000004  min_lr: 0.000000  loss: 1.7807 (1.7551)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3859 (8.0139)  time: 0.8541 (0.5270 -- 3.5792)  data: 0.3039 (0.0005 -- 3.0412)  max mem: 16413
[2023-10-23 22:31:21,161] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8715
[2023-10-23 22:31:21,161] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8715
[2023-10-23 22:31:21,161] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:31:21,161] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:31:21,161] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [161]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.7839 (1.7956)  loss_scale: 16384.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3043 (8.4371)  time: 0.8102 (0.5163 -- 4.9688)  data: 0.2703 (0.0004 -- 4.4481)  max mem: 16413
Epoch: [161]  [53/54]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8228 (1.7875)  loss_scale: 16384.0000 (22755.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8963 (8.3284)  time: 0.6535 (0.4959 -- 2.5577)  data: 0.1316 (0.0002 -- 2.0187)  max mem: 16413
Epoch: [161] Total time: 0:00:50 (0.9352 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8228 (1.7109)  loss_scale: 16384.0000 (22755.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8963 (8.3284)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8865 (0.8865)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0365 (2.0365 -- 2.0365)  data: 1.8423 (1.8423 -- 1.8423)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5622 (0.6444)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3411 (0.0222 -- 2.0365)  data: 0.1843 (0.0001 -- 1.8423)  max mem: 16413
Val: Total time: 0:00:03 (0.3412 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.781
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [162]  [ 0/54]  eta: 0:08:10  lr: 0.000004  min_lr: 0.000000  loss: 1.4658 (1.4658)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8653 (9.8653)  time: 9.0763 (9.0763 -- 9.0763)  data: 8.5601 (8.5601 -- 8.5601)  max mem: 16413
Epoch: [162]  [20/54]  eta: 0:00:42  lr: 0.000004  min_lr: 0.000000  loss: 1.8785 (1.8259)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4572 (8.7048)  time: 0.8550 (0.5176 -- 4.6868)  data: 0.3084 (0.0002 -- 4.1648)  max mem: 16413
Epoch: [162]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.8045 (1.8125)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5660 (8.2885)  time: 0.8740 (0.5204 -- 3.2829)  data: 0.3187 (0.0007 -- 2.7442)  max mem: 16413
Epoch: [162]  [53/54]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7547 (1.8092)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2395 (8.1346)  time: 0.6346 (0.4961 -- 2.4116)  data: 0.1170 (0.0002 -- 1.8653)  max mem: 16413
Epoch: [162] Total time: 0:00:50 (0.9394 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7547 (1.7825)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2395 (8.1346)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8941 (0.8941)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0221 (2.0221 -- 2.0221)  data: 1.8421 (1.8421 -- 1.8421)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5691 (0.6481)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3399 (0.0222 -- 2.0221)  data: 0.1843 (0.0001 -- 1.8421)  max mem: 16413
Val: Total time: 0:00:03 (0.3400 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.784
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [163]  [ 0/54]  eta: 0:06:46  lr: 0.000004  min_lr: 0.000000  loss: 1.5986 (1.5986)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2586 (9.2586)  time: 7.5190 (7.5190 -- 7.5190)  data: 4.1982 (4.1982 -- 4.1982)  max mem: 16413
Epoch: [163]  [20/54]  eta: 0:00:42  lr: 0.000004  min_lr: 0.000000  loss: 1.7437 (1.8107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5481 (8.1104)  time: 0.9356 (0.5366 -- 3.1378)  data: 0.1674 (0.0005 -- 2.6010)  max mem: 16413
Epoch: [163]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.6050 (1.7548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4196 (8.0653)  time: 0.8714 (0.5132 -- 4.2344)  data: 0.3284 (0.0002 -- 3.7144)  max mem: 16413
[2023-10-23 22:33:28,664] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:33:28,664] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:33:28,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:33:28,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [163]  [53/54]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7701 (1.7663)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6409 (7.8504)  time: 0.7384 (0.4944 -- 3.3484)  data: 0.2224 (0.0002 -- 2.8378)  max mem: 16413
Epoch: [163] Total time: 0:00:51 (0.9602 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7701 (1.7402)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6409 (7.8504)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8944 (0.8944)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0421 (2.0421 -- 2.0421)  data: 1.8382 (1.8382 -- 1.8382)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5671 (0.6483)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3420 (0.0221 -- 2.0421)  data: 0.1839 (0.0001 -- 1.8382)  max mem: 16413
Val: Total time: 0:00:03 (0.3421 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [164]  [ 0/54]  eta: 0:06:12  lr: 0.000004  min_lr: 0.000000  loss: 1.7678 (1.7678)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8284 (8.8284)  time: 6.9022 (6.9022 -- 6.9022)  data: 6.3749 (6.3749 -- 6.3749)  max mem: 16413
[2023-10-23 22:33:46,685] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8857
[2023-10-23 22:33:46,685] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8857
[2023-10-23 22:33:46,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:33:46,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:33:46,685] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [164]  [20/54]  eta: 0:00:39  lr: 0.000004  min_lr: 0.000000  loss: 1.7667 (1.7241)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3099 (8.7297)  time: 0.8719 (0.5188 -- 4.5784)  data: 0.3173 (0.0007 -- 4.0483)  max mem: 16413
[2023-10-23 22:34:20,392] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8895
[2023-10-23 22:34:20,392] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8895
[2023-10-23 22:34:20,392] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:34:20,392] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:34:20,393] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [164]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.8204 (1.7510)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5751 (8.8603)  time: 0.9080 (0.5221 -- 3.2417)  data: 0.2281 (0.0005 -- 2.6965)  max mem: 16413
Epoch: [164]  [53/54]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6859 (1.7073)  loss_scale: 8192.0000 (14411.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2898 (8.4900)  time: 0.7864 (0.4939 -- 3.3997)  data: 0.0419 (0.0002 -- 0.8221)  max mem: 16413
Epoch: [164] Total time: 0:00:51 (0.9613 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6859 (1.7002)  loss_scale: 8192.0000 (14411.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2898 (8.4900)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8989 (0.8989)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0168 (2.0168 -- 2.0168)  data: 1.8335 (1.8335 -- 1.8335)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5685 (0.6518)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3403 (0.0221 -- 2.0168)  data: 0.1834 (0.0001 -- 1.8335)  max mem: 16413
Val: Total time: 0:00:03 (0.3404 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.787
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [165]  [ 0/54]  eta: 0:06:12  lr: 0.000004  min_lr: 0.000000  loss: 2.1813 (2.1813)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4938 (8.4938)  time: 6.8914 (6.8914 -- 6.8914)  data: 6.3796 (6.3796 -- 6.3796)  max mem: 16413
Epoch: [165]  [20/54]  eta: 0:00:41  lr: 0.000004  min_lr: 0.000000  loss: 1.8049 (1.8475)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5195 (7.6341)  time: 0.9373 (0.5215 -- 3.2267)  data: 0.3935 (0.0008 -- 2.6655)  max mem: 16413
Epoch: [165]  [40/54]  eta: 0:00:14  lr: 0.000004  min_lr: 0.000000  loss: 1.5040 (1.7210)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4529 (7.8394)  time: 0.7774 (0.5089 -- 3.1931)  data: 0.2321 (0.0002 -- 2.6552)  max mem: 16413
[2023-10-23 22:35:19,920] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8953
[2023-10-23 22:35:19,920] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-10-23 22:35:19,920] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8953
[2023-10-23 22:35:19,921] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-10-23 22:35:19,921] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [165]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5682 (1.7135)  loss_scale: 4096.0000 (7357.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2568 (7.7456)  time: 0.6436 (0.4960 -- 2.9631)  data: 0.1227 (0.0001 -- 2.4332)  max mem: 16413
Epoch: [165] Total time: 0:00:50 (0.9300 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5682 (1.7244)  loss_scale: 4096.0000 (7357.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2568 (7.7456)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9003 (0.9003)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9780 (1.9780 -- 1.9780)  data: 1.7756 (1.7756 -- 1.7756)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5721 (0.6530)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3367 (0.0223 -- 1.9780)  data: 0.1782 (0.0001 -- 1.7756)  max mem: 16413
Val: Total time: 0:00:03 (0.3368 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.790
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [166]  [ 0/54]  eta: 0:06:31  lr: 0.000003  min_lr: 0.000000  loss: 2.1655 (2.1655)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4011 (8.4011)  time: 7.2522 (7.2522 -- 7.2522)  data: 6.6994 (6.6994 -- 6.6994)  max mem: 16413
Epoch: [166]  [20/54]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000000  loss: 1.6219 (1.6788)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5816 (8.1962)  time: 0.8501 (0.5221 -- 3.7068)  data: 0.2720 (0.0007 -- 3.1616)  max mem: 16413
[2023-10-23 22:36:06,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=50, lr=[7.949978441649432e-08, 7.949978441649432e-08, 1.0599971255532576e-07, 1.0599971255532576e-07, 1.4133295007376767e-07, 1.4133295007376767e-07, 1.8844393343169025e-07, 1.8844393343169025e-07, 2.512585779089203e-07, 2.512585779089203e-07, 3.3501143721189377e-07, 3.3501143721189377e-07, 4.46681916282525e-07, 4.46681916282525e-07, 5.955758883767e-07, 5.955758883767e-07, 7.941011845022667e-07, 7.941011845022667e-07, 1.0588015793363556e-06, 1.0588015793363556e-06, 1.4117354391151407e-06, 1.4117354391151407e-06, 1.8823139188201878e-06, 1.8823139188201878e-06, 2.5097518917602504e-06, 2.5097518917602504e-06, 3.3463358556803337e-06, 3.3463358556803337e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 22:36:06,552] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=18.63209087543709, CurrSamplesPerSec=9.907949052801163, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [166]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.7345 (1.6842)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7854 (8.4473)  time: 0.8512 (0.5176 -- 2.3626)  data: 0.1891 (0.0004 -- 1.5849)  max mem: 16413
Epoch: [166]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5268 (1.6850)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7417 (8.0644)  time: 0.7367 (0.4951 -- 2.5767)  data: 0.0287 (0.0002 -- 0.4776)  max mem: 16413
Epoch: [166] Total time: 0:00:51 (0.9458 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5268 (1.6983)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7417 (8.0644)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9048 (0.9048)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0126 (2.0126 -- 2.0126)  data: 1.8060 (1.8060 -- 1.8060)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5744 (0.6526)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3398 (0.0221 -- 2.0126)  data: 0.1807 (0.0001 -- 1.8060)  max mem: 16413
Val: Total time: 0:00:03 (0.3399 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.791
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [167]  [ 0/54]  eta: 0:05:53  lr: 0.000003  min_lr: 0.000000  loss: 1.9568 (1.9568)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3505 (12.3505)  time: 6.5518 (6.5518 -- 6.5518)  data: 6.0183 (6.0183 -- 6.0183)  max mem: 16413
Epoch: [167]  [20/54]  eta: 0:00:40  lr: 0.000003  min_lr: 0.000000  loss: 1.7216 (1.7398)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2400 (9.0369)  time: 0.9127 (0.5271 -- 3.1834)  data: 0.3026 (0.0006 -- 2.6149)  max mem: 16413
Epoch: [167]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.5014 (1.6439)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2604 (8.7051)  time: 0.8936 (0.5289 -- 5.0081)  data: 0.3434 (0.0003 -- 4.4859)  max mem: 16413
Epoch: [167]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5917 (1.6565)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5615 (8.8077)  time: 0.8294 (0.4939 -- 5.0081)  data: 0.3152 (0.0001 -- 4.4859)  max mem: 16413
Epoch: [167] Total time: 0:00:50 (0.9444 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5917 (1.6685)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5615 (8.8077)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.9011 (0.9011)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9325 (1.9325 -- 1.9325)  data: 1.7178 (1.7178 -- 1.7178)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5743 (0.6525)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3313 (0.0223 -- 1.9325)  data: 0.1719 (0.0001 -- 1.7178)  max mem: 16413
Val: Total time: 0:00:03 (0.3315 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.790
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [168]  [ 0/54]  eta: 0:06:33  lr: 0.000003  min_lr: 0.000000  loss: 1.8874 (1.8874)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1285 (8.1285)  time: 7.2793 (7.2793 -- 7.2793)  data: 6.7483 (6.7483 -- 6.7483)  max mem: 16413
[2023-10-23 22:37:32,126] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:37:32,127] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-10-23 22:37:32,127] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:37:32,127] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [168]  [20/54]  eta: 0:00:38  lr: 0.000003  min_lr: 0.000000  loss: 1.8139 (1.7445)  loss_scale: 8192.0000 (6241.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0369 (8.1345)  time: 0.8367 (0.5118 -- 3.0570)  data: 0.1186 (0.0008 -- 1.1711)  max mem: 16413
Epoch: [168]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.8004 (1.7280)  loss_scale: 8192.0000 (7192.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2503 (8.0494)  time: 0.8513 (0.5168 -- 4.4959)  data: 0.0023 (0.0002 -- 0.0163)  max mem: 16413
Epoch: [168]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8102 (1.7062)  loss_scale: 8192.0000 (7433.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8200 (8.0464)  time: 0.6621 (0.4937 -- 2.5657)  data: 0.0207 (0.0002 -- 0.3991)  max mem: 16413
Epoch: [168] Total time: 0:00:50 (0.9339 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8102 (1.6667)  loss_scale: 8192.0000 (7433.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8200 (8.0464)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9028 (0.9028)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0018 (2.0018 -- 2.0018)  data: 1.8004 (1.8004 -- 1.8004)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5725 (0.6545)  acc1: 77.7778 (80.4878)  acc5: 100.0000 (95.1220)  time: 0.3400 (0.0228 -- 2.0018)  data: 0.1806 (0.0001 -- 1.8004)  max mem: 16413
Val: Total time: 0:00:03 (0.3402 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.791
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [169]  [ 0/54]  eta: 0:06:19  lr: 0.000003  min_lr: 0.000000  loss: 1.8024 (1.8024)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3478 (7.3478)  time: 7.0337 (7.0337 -- 7.0337)  data: 6.4744 (6.4744 -- 6.4744)  max mem: 16413
Epoch: [169]  [20/54]  eta: 0:00:41  lr: 0.000003  min_lr: 0.000000  loss: 1.7273 (1.6933)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3630 (7.5263)  time: 0.9366 (0.5321 -- 2.9909)  data: 0.2720 (0.0003 -- 2.4662)  max mem: 16413
Epoch: [169]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.5011 (1.6336)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5734 (7.7983)  time: 0.8705 (0.5064 -- 4.2821)  data: 0.2593 (0.0005 -- 3.7551)  max mem: 16413
Epoch: [169]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7767 (1.6436)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5734 (7.7236)  time: 0.7783 (0.4938 -- 4.2821)  data: 0.2682 (0.0002 -- 3.7551)  max mem: 16413
Epoch: [169] Total time: 0:00:51 (0.9508 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7767 (1.6953)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5734 (7.7236)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9018 (0.9018)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0070 (2.0070 -- 2.0070)  data: 1.8061 (1.8061 -- 1.8061)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5728 (0.6550)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3383 (0.0223 -- 2.0070)  data: 0.1807 (0.0001 -- 1.8061)  max mem: 16413
Val: Total time: 0:00:03 (0.3384 s / it)
* Acc@1 82.317 Acc@5 94.512 loss 0.792
Accuracy of the network on the 163 val images: 82.32%
Max accuracy: 84.15%
Epoch: [170]  [ 0/54]  eta: 0:06:50  lr: 0.000003  min_lr: 0.000000  loss: 1.0460 (1.0460)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9099 (11.9099)  time: 7.6105 (7.6105 -- 7.6105)  data: 4.8985 (4.8985 -- 4.8985)  max mem: 16413
Epoch: [170]  [20/54]  eta: 0:00:41  lr: 0.000003  min_lr: 0.000000  loss: 1.7040 (1.6849)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7665 (8.3268)  time: 0.8903 (0.5220 -- 3.0002)  data: 0.1235 (0.0004 -- 1.0650)  max mem: 16413
[2023-10-23 22:39:40,312] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:39:40,312] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 22:39:40,313] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:39:40,313] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [170]  [40/54]  eta: 0:00:14  lr: 0.000003  min_lr: 0.000000  loss: 1.6670 (1.6943)  loss_scale: 16384.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1010 (7.9173)  time: 0.8294 (0.5154 -- 3.6482)  data: 0.0310 (0.0005 -- 0.5899)  max mem: 16413
Epoch: [170]  [53/54]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8741 (1.7220)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1156 (8.1995)  time: 0.6380 (0.4970 -- 2.2480)  data: 0.0305 (0.0001 -- 0.5899)  max mem: 16413
Epoch: [170] Total time: 0:00:50 (0.9326 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8741 (1.7144)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1156 (8.1995)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.9011 (0.9011)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0413 (2.0413 -- 2.0413)  data: 1.8486 (1.8486 -- 1.8486)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5711 (0.6526)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3416 (0.0223 -- 2.0413)  data: 0.1850 (0.0001 -- 1.8486)  max mem: 16413
Val: Total time: 0:00:03 (0.3417 s / it)
* Acc@1 81.707 Acc@5 94.512 loss 0.789
Accuracy of the network on the 163 val images: 81.71%
Max accuracy: 84.15%
Epoch: [171]  [ 0/54]  eta: 0:07:07  lr: 0.000003  min_lr: 0.000000  loss: 1.4169 (1.4169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1556 (9.1556)  time: 7.9158 (7.9158 -- 7.9158)  data: 7.2677 (7.2677 -- 7.2677)  max mem: 16413
Epoch: [171]  [20/54]  eta: 0:00:42  lr: 0.000002  min_lr: 0.000000  loss: 1.6940 (1.7137)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8891 (7.0269)  time: 0.9110 (0.5278 -- 4.9109)  data: 0.3684 (0.0004 -- 4.3601)  max mem: 16413
Epoch: [171]  [40/54]  eta: 0:00:15  lr: 0.000002  min_lr: 0.000000  loss: 1.9644 (1.7856)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5893 (7.4822)  time: 0.9445 (0.5173 -- 3.8005)  data: 0.4028 (0.0003 -- 3.2735)  max mem: 16413
Epoch: [171]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7403 (1.7626)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7970 (7.6591)  time: 0.7029 (0.4948 -- 3.5142)  data: 0.1864 (0.0001 -- 3.0092)  max mem: 16413
Epoch: [171] Total time: 0:00:52 (0.9680 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7403 (1.7813)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7970 (7.6591)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8975 (0.8975)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0278 (2.0278 -- 2.0278)  data: 1.8219 (1.8219 -- 1.8219)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5705 (0.6524)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3477 (0.0220 -- 2.0278)  data: 0.1884 (0.0001 -- 1.8219)  max mem: 16413
Val: Total time: 0:00:03 (0.3478 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.788
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [172]  [ 0/54]  eta: 0:08:29  lr: 0.000002  min_lr: 0.000000  loss: 1.5820 (1.5820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4883 (7.4883)  time: 9.4384 (9.4384 -- 9.4384)  data: 8.8890 (8.8890 -- 8.8890)  max mem: 16413
Epoch: [172]  [20/54]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 1.5133 (1.6136)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4360 (7.7458)  time: 0.7548 (0.5236 -- 2.7317)  data: 0.1884 (0.0008 -- 2.2023)  max mem: 16413
Epoch: [172]  [40/54]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 1.7287 (1.6373)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7519 (8.4406)  time: 0.8557 (0.5281 -- 2.5727)  data: 0.2396 (0.0007 -- 2.0387)  max mem: 16413
[2023-10-23 22:41:44,610] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:41:44,610] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:41:44,611] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:41:44,611] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [172]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6459 (1.6574)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8961 (8.2696)  time: 0.7276 (0.4949 -- 1.8939)  data: 0.1686 (0.0002 -- 1.3346)  max mem: 16413
Epoch: [172] Total time: 0:00:50 (0.9301 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6459 (1.6660)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8961 (8.2696)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8936 (0.8936)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0711 (2.0711 -- 2.0711)  data: 1.8738 (1.8738 -- 1.8738)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5697 (0.6515)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3456 (0.0223 -- 2.0711)  data: 0.1875 (0.0001 -- 1.8738)  max mem: 16413
Val: Total time: 0:00:03 (0.3457 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.786
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [173]  [ 0/54]  eta: 0:07:57  lr: 0.000002  min_lr: 0.000000  loss: 1.7859 (1.7859)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8290 (7.8290)  time: 8.8425 (8.8425 -- 8.8425)  data: 8.3052 (8.3052 -- 8.3052)  max mem: 16413
Epoch: [173]  [20/54]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 1.8594 (1.7804)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4900 (8.6546)  time: 0.7677 (0.5260 -- 2.3432)  data: 0.1007 (0.0005 -- 0.9630)  max mem: 16413
[2023-10-23 22:42:27,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9379
[2023-10-23 22:42:27,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9379
[2023-10-23 22:42:27,965] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:42:27,965] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:42:27,965] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [173]  [40/54]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 1.7924 (1.7925)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2368 (8.3668)  time: 0.8856 (0.5162 -- 2.1390)  data: 0.3066 (0.0003 -- 1.3209)  max mem: 16413
Epoch: [173]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7924 (1.7770)  loss_scale: 16384.0000 (27610.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8487 (8.4426)  time: 0.7516 (0.4930 -- 1.8682)  data: 0.2295 (0.0002 -- 1.3270)  max mem: 16413
Epoch: [173] Total time: 0:00:50 (0.9375 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7924 (1.7336)  loss_scale: 16384.0000 (27610.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8487 (8.4426)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8914 (0.8914)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0595 (2.0595 -- 2.0595)  data: 1.8833 (1.8833 -- 1.8833)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5687 (0.6497)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3444 (0.0222 -- 2.0595)  data: 0.1884 (0.0001 -- 1.8833)  max mem: 16413
Val: Total time: 0:00:03 (0.3445 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [174]  [ 0/54]  eta: 0:08:11  lr: 0.000002  min_lr: 0.000000  loss: 1.1934 (1.1934)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4456 (8.4456)  time: 9.1083 (9.1083 -- 9.1083)  data: 8.5848 (8.5848 -- 8.5848)  max mem: 16413
Epoch: [174]  [20/54]  eta: 0:00:44  lr: 0.000002  min_lr: 0.000000  loss: 1.7965 (1.7775)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9030 (8.3410)  time: 0.9161 (0.5287 -- 3.6013)  data: 0.3365 (0.0005 -- 3.0846)  max mem: 16413
Epoch: [174]  [40/54]  eta: 0:00:15  lr: 0.000002  min_lr: 0.000000  loss: 1.7160 (1.7512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3190 (8.0196)  time: 0.8531 (0.5134 -- 3.0189)  data: 0.0024 (0.0004 -- 0.0181)  max mem: 16413
Epoch: [174]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7172 (1.7553)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9020 (7.9487)  time: 0.6705 (0.4941 -- 2.8129)  data: 0.0008 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [174] Total time: 0:00:51 (0.9590 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7172 (1.7414)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9020 (7.9487)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8917 (0.8917)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0448 (2.0448 -- 2.0448)  data: 1.8544 (1.8544 -- 1.8544)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5710 (0.6506)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3429 (0.0222 -- 2.0448)  data: 0.1855 (0.0001 -- 1.8544)  max mem: 16413
Val: Total time: 0:00:03 (0.3430 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.785
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [175]  [ 0/54]  eta: 0:06:20  lr: 0.000002  min_lr: 0.000000  loss: 1.8802 (1.8802)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8094 (6.8094)  time: 7.0525 (7.0525 -- 7.0525)  data: 6.5092 (6.5092 -- 6.5092)  max mem: 16413
Epoch: [175]  [20/54]  eta: 0:00:40  lr: 0.000002  min_lr: 0.000000  loss: 1.6086 (1.6969)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2770 (8.2089)  time: 0.8897 (0.5263 -- 3.5057)  data: 0.0930 (0.0005 -- 1.8341)  max mem: 16413
Epoch: [175]  [40/54]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 1.7624 (1.7017)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1126 (7.8479)  time: 0.9183 (0.5250 -- 3.9336)  data: 0.0016 (0.0004 -- 0.0056)  max mem: 16413
Epoch: [175]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7624 (1.6971)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6070 (7.9447)  time: 0.7960 (0.4944 -- 3.9336)  data: 0.0007 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [175] Total time: 0:00:51 (0.9551 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7624 (1.7039)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6070 (7.9447)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8925 (0.8925)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0216 (2.0216 -- 2.0216)  data: 1.7990 (1.7990 -- 1.7990)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5723 (0.6514)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3423 (0.0231 -- 2.0216)  data: 0.1800 (0.0001 -- 1.7990)  max mem: 16413
Val: Total time: 0:00:03 (0.3424 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.786
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [176]  [ 0/54]  eta: 0:07:35  lr: 0.000002  min_lr: 0.000000  loss: 1.1980 (1.1980)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8175 (7.8175)  time: 8.4331 (8.4331 -- 8.4331)  data: 5.3253 (5.3253 -- 5.3253)  max mem: 16413
[2023-10-23 22:44:44,698] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:44:44,698] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:44:44,699] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:44:44,699] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:44:49,787] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9512
[2023-10-23 22:44:49,787] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9512
[2023-10-23 22:44:49,787] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:44:49,787] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:44:49,787] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [176]  [20/54]  eta: 0:00:42  lr: 0.000002  min_lr: 0.000000  loss: 1.6675 (1.6580)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6332 (8.6033)  time: 0.8786 (0.5184 -- 4.3128)  data: 0.0017 (0.0003 -- 0.0082)  max mem: 16413
Epoch: [176]  [40/54]  eta: 0:00:15  lr: 0.000002  min_lr: 0.000000  loss: 1.5661 (1.6015)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2332 (8.2409)  time: 0.9368 (0.5033 -- 3.9569)  data: 0.0012 (0.0003 -- 0.0048)  max mem: 16413
Epoch: [176]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5143 (1.6064)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2480 (8.1165)  time: 0.5899 (0.4942 -- 1.6508)  data: 0.0007 (0.0001 -- 0.0023)  max mem: 16413
Epoch: [176] Total time: 0:00:51 (0.9552 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5143 (1.6086)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2480 (8.1165)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8919 (0.8919)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0491 (2.0491 -- 2.0491)  data: 1.8541 (1.8541 -- 1.8541)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5714 (0.6502)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3427 (0.0222 -- 2.0491)  data: 0.1855 (0.0001 -- 1.8541)  max mem: 16413
Val: Total time: 0:00:03 (0.3428 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.785
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [177]  [ 0/54]  eta: 0:07:31  lr: 0.000002  min_lr: 0.000000  loss: 2.0613 (2.0613)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8258 (8.8258)  time: 8.3581 (8.3581 -- 8.3581)  data: 6.3447 (6.3447 -- 6.3447)  max mem: 16413
Epoch: [177]  [20/54]  eta: 0:00:40  lr: 0.000002  min_lr: 0.000000  loss: 1.6257 (1.6642)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3853 (8.4589)  time: 0.8299 (0.5269 -- 3.3300)  data: 0.2824 (0.0005 -- 2.7941)  max mem: 16413
Epoch: [177]  [40/54]  eta: 0:00:14  lr: 0.000002  min_lr: 0.000000  loss: 1.6179 (1.6741)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0933 (7.9303)  time: 0.9437 (0.5397 -- 3.9859)  data: 0.2648 (0.0004 -- 3.4651)  max mem: 16413
Epoch: [177]  [53/54]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5996 (1.6761)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0287 (8.0363)  time: 0.7055 (0.4952 -- 3.7490)  data: 0.0559 (0.0001 -- 1.1002)  max mem: 16413
Epoch: [177] Total time: 0:00:50 (0.9338 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5996 (1.6721)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0287 (8.0363)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8928 (0.8928)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0487 (2.0487 -- 2.0487)  data: 1.8623 (1.8623 -- 1.8623)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5699 (0.6494)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3436 (0.0223 -- 2.0487)  data: 0.1863 (0.0001 -- 1.8623)  max mem: 16413
Val: Total time: 0:00:03 (0.3437 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.785
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [178]  [ 0/54]  eta: 0:07:20  lr: 0.000002  min_lr: 0.000000  loss: 1.7753 (1.7753)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7892 (6.7892)  time: 8.1492 (8.1492 -- 8.1492)  data: 7.6325 (7.6325 -- 7.6325)  max mem: 16413
Epoch: [178]  [20/54]  eta: 0:00:41  lr: 0.000001  min_lr: 0.000000  loss: 1.7352 (1.7560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0265 (8.2016)  time: 0.8697 (0.5314 -- 3.7035)  data: 0.3074 (0.0002 -- 3.1886)  max mem: 16413
[2023-10-23 22:46:56,249] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:46:56,249] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:46:56,249] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:46:56,249] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:46:57,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9643
[2023-10-23 22:46:57,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9643
[2023-10-23 22:46:57,334] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:46:57,334] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 22:46:57,334] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [178]  [40/54]  eta: 0:00:15  lr: 0.000001  min_lr: 0.000000  loss: 1.7496 (1.7139)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8760 (7.9812)  time: 0.9579 (0.5201 -- 4.0182)  data: 0.1683 (0.0004 -- 1.3076)  max mem: 16413
Epoch: [178]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7496 (1.7154)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8770 (7.9916)  time: 0.6910 (0.4944 -- 4.0182)  data: 0.0660 (0.0001 -- 1.3076)  max mem: 16413
Epoch: [178] Total time: 0:00:51 (0.9490 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7496 (1.7462)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8770 (7.9916)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8906 (0.8906)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0247 (2.0247 -- 2.0247)  data: 1.8206 (1.8206 -- 1.8206)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5693 (0.6495)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3459 (0.0223 -- 2.0247)  data: 0.1862 (0.0001 -- 1.8206)  max mem: 16413
Val: Total time: 0:00:03 (0.3460 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.784
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [179]  [ 0/54]  eta: 0:06:35  lr: 0.000001  min_lr: 0.000000  loss: 2.0634 (2.0634)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8728 (9.8728)  time: 7.3297 (7.3297 -- 7.3297)  data: 5.4794 (5.4794 -- 5.4794)  max mem: 16413
Epoch: [179]  [20/54]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 1.7994 (1.8422)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7079 (8.9988)  time: 0.8615 (0.5213 -- 3.1449)  data: 0.2786 (0.0007 -- 2.5981)  max mem: 16413
Epoch: [179]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7681 (1.7872)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2396 (8.7195)  time: 0.9164 (0.5206 -- 4.2014)  data: 0.3674 (0.0007 -- 3.6613)  max mem: 16413
Epoch: [179]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6231 (1.7733)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3780 (8.7721)  time: 0.6899 (0.4946 -- 2.6517)  data: 0.1695 (0.0002 -- 2.1402)  max mem: 16413
Epoch: [179] Total time: 0:00:50 (0.9399 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6231 (1.7739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3780 (8.7721)
[2023-10-23 22:48:08,630] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-10-23 22:48:08,632] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-10-23 22:48:08,632] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-179/mp_rank_00_model_states.pt
[2023-10-23 22:48:08,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-179/mp_rank_00_model_states.pt...
[2023-10-23 22:48:09,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-179/mp_rank_00_model_states.pt.
[2023-10-23 22:48:09,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8914 (0.8914)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0287 (2.0287 -- 2.0287)  data: 1.8469 (1.8469 -- 1.8469)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5705 (0.6496)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3412 (0.0222 -- 2.0287)  data: 0.1848 (0.0001 -- 1.8469)  max mem: 16413
Val: Total time: 0:00:03 (0.3414 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.784
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [180]  [ 0/54]  eta: 0:06:08  lr: 0.000001  min_lr: 0.000000  loss: 1.5160 (1.5160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1924 (6.1924)  time: 6.8165 (6.8165 -- 6.8165)  data: 6.2632 (6.2632 -- 6.2632)  max mem: 16413
Epoch: [180]  [20/54]  eta: 0:00:40  lr: 0.000001  min_lr: 0.000000  loss: 1.7838 (1.7214)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3626 (8.2340)  time: 0.9246 (0.5344 -- 2.9704)  data: 0.2739 (0.0007 -- 2.4347)  max mem: 16413
Epoch: [180]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.8557 (1.7325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2994 (8.1041)  time: 0.8073 (0.5163 -- 2.6623)  data: 0.1327 (0.0002 -- 1.5524)  max mem: 16413
[2023-10-23 22:49:04,023] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:49:04,023] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:49:04,026] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:49:04,026] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [180]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5352 (1.6921)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5547 (7.9476)  time: 0.7565 (0.4958 -- 2.6659)  data: 0.2307 (0.0002 -- 2.1538)  max mem: 16413
Epoch: [180] Total time: 0:00:51 (0.9501 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5352 (1.7434)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5547 (7.9476)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8893 (0.8893)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0662 (2.0662 -- 2.0662)  data: 1.8779 (1.8779 -- 1.8779)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5717 (0.6487)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3448 (0.0221 -- 2.0662)  data: 0.1879 (0.0001 -- 1.8779)  max mem: 16413
Val: Total time: 0:00:03 (0.3449 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [181]  [ 0/54]  eta: 0:06:35  lr: 0.000001  min_lr: 0.000000  loss: 1.9031 (1.9031)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0449 (8.0449)  time: 7.3222 (7.3222 -- 7.3222)  data: 5.8523 (5.8523 -- 5.8523)  max mem: 16413
Epoch: [181]  [20/54]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6933 (1.7101)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8341 (8.6599)  time: 0.7715 (0.5245 -- 2.4438)  data: 0.0740 (0.0005 -- 0.6907)  max mem: 16413
[2023-10-23 22:49:34,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9798
[2023-10-23 22:49:34,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9798
[2023-10-23 22:49:34,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:49:34,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:49:34,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [181]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.6715 (1.6768)  loss_scale: 16384.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0682 (8.5380)  time: 0.9430 (0.5320 -- 2.5136)  data: 0.1380 (0.0004 -- 0.8652)  max mem: 16413
Epoch: [181]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6532 (1.6812)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2074 (8.2671)  time: 0.7061 (0.4989 -- 2.4403)  data: 0.0014 (0.0001 -- 0.0133)  max mem: 16413
Epoch: [181] Total time: 0:00:50 (0.9268 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6532 (1.6981)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2074 (8.2671)
Val:  [ 0/10]  eta: 0:00:18  loss: 0.8898 (0.8898)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.8939 (1.8939 -- 1.8939)  data: 1.6802 (1.6802 -- 1.6802)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5728 (0.6487)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3320 (0.0224 -- 1.8939)  data: 0.1723 (0.0001 -- 1.6802)  max mem: 16413
Val: Total time: 0:00:03 (0.3322 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.784
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [182]  [ 0/54]  eta: 0:06:57  lr: 0.000001  min_lr: 0.000000  loss: 1.3956 (1.3956)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4864 (9.4864)  time: 7.7306 (7.7306 -- 7.7306)  data: 5.5477 (5.5477 -- 5.5477)  max mem: 16413
Epoch: [182]  [20/54]  eta: 0:00:40  lr: 0.000001  min_lr: 0.000000  loss: 1.7211 (1.7303)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4018 (7.9817)  time: 0.8635 (0.5246 -- 3.2610)  data: 0.2591 (0.0004 -- 2.7039)  max mem: 16413
Epoch: [182]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7947 (1.7495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8330 (8.0880)  time: 0.9007 (0.5249 -- 4.5557)  data: 0.3566 (0.0001 -- 4.0329)  max mem: 16413
[2023-10-23 22:50:47,550] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9873
[2023-10-23 22:50:47,550] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9873
[2023-10-23 22:50:47,550] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:50:47,550] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 22:50:47,550] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [182]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7762 (1.7609)  loss_scale: 16384.0000 (15018.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8836 (8.2499)  time: 0.5948 (0.4929 -- 1.6403)  data: 0.0568 (0.0001 -- 1.1164)  max mem: 16413
Epoch: [182] Total time: 0:00:49 (0.9254 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7762 (1.7025)  loss_scale: 16384.0000 (15018.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8836 (8.2499)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8877 (0.8877)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9186 (1.9186 -- 1.9186)  data: 1.6928 (1.6928 -- 1.6928)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5728 (0.6484)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3307 (0.0223 -- 1.9186)  data: 0.1696 (0.0001 -- 1.6928)  max mem: 16413
Val: Total time: 0:00:03 (0.3308 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.782
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [183]  [ 0/54]  eta: 0:07:31  lr: 0.000001  min_lr: 0.000000  loss: 2.2790 (2.2790)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4047 (7.4047)  time: 8.3575 (8.3575 -- 8.3575)  data: 7.7987 (7.7987 -- 7.7987)  max mem: 16413
Epoch: [183]  [20/54]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 1.6447 (1.6967)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5229 (7.5915)  time: 0.8123 (0.5349 -- 2.7123)  data: 0.1200 (0.0006 -- 1.3192)  max mem: 16413
Epoch: [183]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.6185 (1.6890)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1427 (8.0774)  time: 0.8410 (0.5346 -- 2.3165)  data: 0.0181 (0.0004 -- 0.2142)  max mem: 16413
Epoch: [183]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4308 (1.6756)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8344 (8.1111)  time: 0.7239 (0.4975 -- 2.3165)  data: 0.0123 (0.0001 -- 0.2142)  max mem: 16413
Epoch: [183] Total time: 0:00:49 (0.9153 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4308 (1.7036)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8344 (8.1111)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8871 (0.8871)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0222 (2.0222 -- 2.0222)  data: 1.8004 (1.8004 -- 1.8004)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5718 (0.6481)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3399 (0.0223 -- 2.0222)  data: 0.1801 (0.0001 -- 1.8004)  max mem: 16413
Val: Total time: 0:00:03 (0.3400 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.782
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [184]  [ 0/54]  eta: 0:06:34  lr: 0.000001  min_lr: 0.000000  loss: 1.7409 (1.7409)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7069 (7.7069)  time: 7.3070 (7.3070 -- 7.3070)  data: 5.8475 (5.8475 -- 5.8475)  max mem: 16413
Epoch: [184]  [20/54]  eta: 0:00:40  lr: 0.000001  min_lr: 0.000000  loss: 1.8371 (1.7855)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4748 (7.5045)  time: 0.9000 (0.5180 -- 3.1540)  data: 0.1367 (0.0004 -- 1.0273)  max mem: 16413
Epoch: [184]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.5964 (1.7014)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6501 (7.9692)  time: 0.8997 (0.5097 -- 3.3091)  data: 0.2385 (0.0002 -- 2.7994)  max mem: 16413
Epoch: [184]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7525 (1.7062)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3221 (7.9696)  time: 0.7537 (0.4937 -- 3.3091)  data: 0.2332 (0.0001 -- 2.7994)  max mem: 16413
Epoch: [184] Total time: 0:00:51 (0.9576 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7525 (1.6438)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3221 (7.9696)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8874 (0.8874)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9436 (1.9436 -- 1.9436)  data: 1.7075 (1.7075 -- 1.7075)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5721 (0.6481)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3361 (0.0224 -- 1.9436)  data: 0.1747 (0.0001 -- 1.7075)  max mem: 16413
Val: Total time: 0:00:03 (0.3362 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.782
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [185]  [ 0/54]  eta: 0:06:51  lr: 0.000001  min_lr: 0.000000  loss: 1.9329 (1.9329)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0380 (7.0380)  time: 7.6263 (7.6263 -- 7.6263)  data: 6.8347 (6.8347 -- 6.8347)  max mem: 16413
[2023-10-23 22:52:59,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=55, lr=[1.6921857071706688e-08, 1.6921857071706688e-08, 2.2562476095608917e-08, 2.2562476095608917e-08, 3.008330146081189e-08, 3.008330146081189e-08, 4.011106861441585e-08, 4.011106861441585e-08, 5.3481424819221134e-08, 5.3481424819221134e-08, 7.130856642562817e-08, 7.130856642562817e-08, 9.507808856750424e-08, 9.507808856750424e-08, 1.2677078475667232e-07, 1.2677078475667232e-07, 1.6902771300889643e-07, 1.6902771300889643e-07, 2.2537028401186189e-07, 2.2537028401186189e-07, 3.0049371201581583e-07, 3.0049371201581583e-07, 4.006582826877545e-07, 4.006582826877545e-07, 5.342110435836727e-07, 5.342110435836727e-07, 7.122813914448968e-07, 7.122813914448968e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 22:52:59,271] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=18.616606955277884, CurrSamplesPerSec=10.494509797543703, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-10-23 22:53:00,901] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:53:00,902] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 22:53:00,905] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:53:00,905] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [185]  [20/54]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 1.8754 (1.7974)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5044 (7.3532)  time: 0.8448 (0.5242 -- 2.7508)  data: 0.2461 (0.0004 -- 2.2137)  max mem: 16413
Epoch: [185]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7642 (1.7813)  loss_scale: 16384.0000 (13986.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5788 (7.7972)  time: 0.9318 (0.5174 -- 3.0735)  data: 0.2127 (0.0002 -- 1.2147)  max mem: 16413
Epoch: [185]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7711 (1.7896)  loss_scale: 16384.0000 (14563.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7033 (8.1891)  time: 0.6988 (0.4965 -- 3.0735)  data: 0.0722 (0.0001 -- 0.9560)  max mem: 16413
Epoch: [185] Total time: 0:00:50 (0.9412 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7711 (1.7660)  loss_scale: 16384.0000 (14563.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7033 (8.1891)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8881 (0.8881)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0316 (2.0316 -- 2.0316)  data: 1.8448 (1.8448 -- 1.8448)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5710 (0.6479)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3410 (0.0224 -- 2.0316)  data: 0.1846 (0.0001 -- 1.8448)  max mem: 16413
Val: Total time: 0:00:03 (0.3411 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.782
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [186]  [ 0/54]  eta: 0:08:36  lr: 0.000001  min_lr: 0.000000  loss: 1.4876 (1.4876)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7351 (10.7351)  time: 9.5615 (9.5615 -- 9.5615)  data: 4.5463 (4.5463 -- 4.5463)  max mem: 16413
Epoch: [186]  [20/54]  eta: 0:00:40  lr: 0.000001  min_lr: 0.000000  loss: 1.8602 (1.7673)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1119 (7.3074)  time: 0.7863 (0.5272 -- 3.7067)  data: 0.0018 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [186]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.6428 (1.7085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2680 (7.7464)  time: 0.7944 (0.5142 -- 2.1709)  data: 0.0017 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [186]  [53/54]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5121 (1.7128)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2262 (7.8946)  time: 0.6666 (0.4959 -- 2.2345)  data: 0.0008 (0.0001 -- 0.0033)  max mem: 16413
Epoch: [186] Total time: 0:00:50 (0.9281 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5121 (1.7451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2262 (7.8946)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8883 (0.8883)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0414 (2.0414 -- 2.0414)  data: 1.8341 (1.8341 -- 1.8341)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5700 (0.6474)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3421 (0.0222 -- 2.0414)  data: 0.1835 (0.0001 -- 1.8341)  max mem: 16413
Val: Total time: 0:00:03 (0.3423 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.781
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [187]  [ 0/54]  eta: 0:06:16  lr: 0.000001  min_lr: 0.000000  loss: 1.9635 (1.9635)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2932 (11.2932)  time: 6.9788 (6.9788 -- 6.9788)  data: 6.0493 (6.0493 -- 6.0493)  max mem: 16413
Epoch: [187]  [20/54]  eta: 0:00:43  lr: 0.000001  min_lr: 0.000000  loss: 1.6972 (1.7598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5292 (9.1650)  time: 0.9926 (0.5100 -- 3.7546)  data: 0.1116 (0.0003 -- 1.6803)  max mem: 16413
[2023-10-23 22:55:07,007] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:55:07,007] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:55:07,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:55:07,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [187]  [40/54]  eta: 0:00:14  lr: 0.000001  min_lr: 0.000000  loss: 1.8043 (1.7307)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5796 (8.4048)  time: 0.7815 (0.5230 -- 3.0622)  data: 0.0013 (0.0004 -- 0.0038)  max mem: 16413
[2023-10-23 22:55:18,879] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10144
[2023-10-23 22:55:18,879] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:55:18,879] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10144
[2023-10-23 22:55:18,920] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:55:18,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [187]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7202 (1.7082)  loss_scale: 32768.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9652 (8.2278)  time: 0.8056 (0.4950 -- 2.7930)  data: 0.0009 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [187] Total time: 0:00:52 (0.9734 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7202 (1.7353)  loss_scale: 32768.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9652 (8.2278)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8887 (0.8887)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9623 (1.9623 -- 1.9623)  data: 1.7444 (1.7444 -- 1.7444)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5696 (0.6476)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3344 (0.0223 -- 1.9623)  data: 0.1745 (0.0001 -- 1.7444)  max mem: 16413
Val: Total time: 0:00:03 (0.3345 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.782
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [188]  [ 0/54]  eta: 0:07:11  lr: 0.000000  min_lr: 0.000000  loss: 1.8198 (1.8198)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9973 (8.9973)  time: 7.9907 (7.9907 -- 7.9907)  data: 5.6319 (5.6319 -- 5.6319)  max mem: 16413
Epoch: [188]  [20/54]  eta: 0:00:42  lr: 0.000000  min_lr: 0.000000  loss: 1.6364 (1.6836)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8991 (7.8094)  time: 0.9095 (0.5209 -- 3.6282)  data: 0.1288 (0.0003 -- 1.4918)  max mem: 16413
Epoch: [188]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7847 (1.7331)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6761 (7.9115)  time: 0.7596 (0.5308 -- 1.7511)  data: 0.0949 (0.0003 -- 1.2201)  max mem: 16413
Epoch: [188]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.9372 (1.7756)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8768 (8.1667)  time: 0.6918 (0.4968 -- 1.7380)  data: 0.1129 (0.0002 -- 1.2201)  max mem: 16413
Epoch: [188] Total time: 0:00:50 (0.9294 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.9372 (1.7290)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8768 (8.1667)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8895 (0.8895)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9879 (1.9879 -- 1.9879)  data: 1.7780 (1.7780 -- 1.7780)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5702 (0.6477)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3371 (0.0225 -- 1.9879)  data: 0.1779 (0.0001 -- 1.7780)  max mem: 16413
Val: Total time: 0:00:03 (0.3372 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.782
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [189]  [ 0/54]  eta: 0:07:34  lr: 0.000000  min_lr: 0.000000  loss: 2.0957 (2.0957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7827 (7.7827)  time: 8.4133 (8.4133 -- 8.4133)  data: 7.8684 (7.8684 -- 7.8684)  max mem: 16413
Epoch: [189]  [20/54]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.5050 (1.6191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2242 (8.0250)  time: 0.7984 (0.5224 -- 2.4189)  data: 0.1981 (0.0004 -- 1.9000)  max mem: 16413
Epoch: [189]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7439 (1.7145)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1306 (7.7293)  time: 0.8470 (0.5267 -- 2.8505)  data: 0.0095 (0.0004 -- 0.1596)  max mem: 16413
Epoch: [189]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7706 (1.7208)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0449 (7.9526)  time: 0.7319 (0.4953 -- 2.8505)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [189] Total time: 0:00:49 (0.9213 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7706 (1.7099)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0449 (7.9526)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8903 (0.8903)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0769 (2.0769 -- 2.0769)  data: 1.9012 (1.9012 -- 1.9012)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5700 (0.6480)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3464 (0.0224 -- 2.0769)  data: 0.1902 (0.0001 -- 1.9012)  max mem: 16413
Val: Total time: 0:00:03 (0.3465 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [190]  [ 0/54]  eta: 0:06:33  lr: 0.000000  min_lr: 0.000000  loss: 2.2864 (2.2864)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5799 (6.5799)  time: 7.2888 (7.2888 -- 7.2888)  data: 6.2036 (6.2036 -- 6.2036)  max mem: 16413
[2023-10-23 22:57:33,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:57:33,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:57:33,079] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:57:33,079] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [190]  [20/54]  eta: 0:00:41  lr: 0.000000  min_lr: 0.000000  loss: 1.6456 (1.7179)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1575 (7.9711)  time: 0.9026 (0.5297 -- 2.6910)  data: 0.1693 (0.0004 -- 1.8484)  max mem: 16413
[2023-10-23 22:57:47,224] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10291
[2023-10-23 22:57:47,224] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10291
[2023-10-23 22:57:47,225] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:57:47,225] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 22:57:47,225] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [190]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7130 (1.7230)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8326 (8.1716)  time: 0.8567 (0.5237 -- 2.6764)  data: 0.1702 (0.0004 -- 2.1593)  max mem: 16413
Epoch: [190]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4756 (1.6799)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6851 (8.2080)  time: 0.6868 (0.4952 -- 2.0202)  data: 0.0892 (0.0001 -- 0.8889)  max mem: 16413
Epoch: [190] Total time: 0:00:50 (0.9266 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4756 (1.6861)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6851 (8.2080)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8900 (0.8900)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0169 (2.0169 -- 2.0169)  data: 1.8167 (1.8167 -- 1.8167)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5704 (0.6478)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3393 (0.0222 -- 2.0169)  data: 0.1818 (0.0001 -- 1.8167)  max mem: 16413
Val: Total time: 0:00:03 (0.3394 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [191]  [ 0/54]  eta: 0:06:48  lr: 0.000000  min_lr: 0.000000  loss: 1.7182 (1.7182)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9556 (6.9556)  time: 7.5660 (7.5660 -- 7.5660)  data: 7.0097 (7.0097 -- 7.0097)  max mem: 16413
Epoch: [191]  [20/54]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.7491 (1.7109)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2095 (8.2286)  time: 0.8326 (0.5231 -- 4.2640)  data: 0.2577 (0.0006 -- 3.7311)  max mem: 16413
Epoch: [191]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7374 (1.7030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5711 (7.7954)  time: 0.9035 (0.5244 -- 3.4189)  data: 0.3252 (0.0006 -- 2.8723)  max mem: 16413
Epoch: [191]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7374 (1.7331)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5874 (7.8342)  time: 0.7005 (0.4955 -- 1.6426)  data: 0.1005 (0.0001 -- 1.1090)  max mem: 16413
Epoch: [191] Total time: 0:00:50 (0.9434 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7374 (1.7240)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5874 (7.8342)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8897 (0.8897)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0109 (2.0109 -- 2.0109)  data: 1.8198 (1.8198 -- 1.8198)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5707 (0.6478)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3392 (0.0224 -- 2.0109)  data: 0.1821 (0.0001 -- 1.8198)  max mem: 16413
Val: Total time: 0:00:03 (0.3393 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [192]  [ 0/54]  eta: 0:07:48  lr: 0.000000  min_lr: 0.000000  loss: 1.8731 (1.8731)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5549 (11.5549)  time: 8.6738 (8.6738 -- 8.6738)  data: 6.7455 (6.7455 -- 6.7455)  max mem: 16413
Epoch: [192]  [20/54]  eta: 0:00:41  lr: 0.000000  min_lr: 0.000000  loss: 1.5019 (1.5307)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3308 (7.7289)  time: 0.8538 (0.5273 -- 4.9970)  data: 0.3055 (0.0003 -- 4.4760)  max mem: 16413
Epoch: [192]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6378 (1.5684)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3796 (8.0597)  time: 0.8062 (0.5249 -- 2.3956)  data: 0.1709 (0.0004 -- 1.8761)  max mem: 16413
[2023-10-23 22:59:51,290] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:59:51,290] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 22:59:51,290] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 22:59:51,290] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [192]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5570 (1.5839)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7457 (8.2144)  time: 0.6677 (0.4951 -- 1.9534)  data: 0.0731 (0.0001 -- 1.1631)  max mem: 16413
Epoch: [192] Total time: 0:00:49 (0.9218 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5570 (1.6328)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7457 (8.2144)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8904 (0.8904)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0438 (2.0438 -- 2.0438)  data: 1.8600 (1.8600 -- 1.8600)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5706 (0.6481)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3424 (0.0223 -- 2.0438)  data: 0.1861 (0.0001 -- 1.8600)  max mem: 16413
Val: Total time: 0:00:03 (0.3425 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [193]  [ 0/54]  eta: 0:07:44  lr: 0.000000  min_lr: 0.000000  loss: 1.2167 (1.2167)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6258 (8.6258)  time: 8.5950 (8.5950 -- 8.5950)  data: 8.0746 (8.0746 -- 8.0746)  max mem: 16413
[2023-10-23 23:00:06,132] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10426
[2023-10-23 23:00:06,132] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10426
[2023-10-23 23:00:06,132] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:00:06,132] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:00:06,132] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [193]  [20/54]  eta: 0:00:43  lr: 0.000000  min_lr: 0.000000  loss: 1.4683 (1.6325)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5239 (8.8724)  time: 0.9242 (0.5097 -- 5.7218)  data: 0.3883 (0.0003 -- 5.2011)  max mem: 16413
Epoch: [193]  [40/54]  eta: 0:00:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8417 (1.6815)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0025 (8.1105)  time: 0.8969 (0.5194 -- 2.8793)  data: 0.3511 (0.0002 -- 2.3562)  max mem: 16413
Epoch: [193]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7770 (1.7101)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2954 (8.2804)  time: 0.6367 (0.4954 -- 2.8728)  data: 0.1178 (0.0001 -- 2.3434)  max mem: 16413
Epoch: [193] Total time: 0:00:51 (0.9547 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7770 (1.6929)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2954 (8.2804)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8907 (0.8907)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0338 (2.0338 -- 2.0338)  data: 1.8538 (1.8538 -- 1.8538)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5706 (0.6479)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3414 (0.0223 -- 2.0338)  data: 0.1855 (0.0001 -- 1.8538)  max mem: 16413
Val: Total time: 0:00:03 (0.3416 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [194]  [ 0/54]  eta: 0:08:14  lr: 0.000000  min_lr: 0.000000  loss: 1.9144 (1.9144)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8591 (7.8591)  time: 9.1589 (9.1589 -- 9.1589)  data: 8.6154 (8.6154 -- 8.6154)  max mem: 16413
Epoch: [194]  [20/54]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.6386 (1.6629)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9936 (8.7921)  time: 0.7655 (0.5200 -- 3.2563)  data: 0.1856 (0.0005 -- 2.5271)  max mem: 16413
Epoch: [194]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.8340 (1.7056)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1174 (8.8044)  time: 0.9414 (0.5244 -- 3.7581)  data: 0.1428 (0.0004 -- 2.1235)  max mem: 16413
Epoch: [194]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8403 (1.7173)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5784 (8.4180)  time: 0.6193 (0.4954 -- 1.8018)  data: 0.0380 (0.0001 -- 0.6947)  max mem: 16413
Epoch: [194] Total time: 0:00:50 (0.9378 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8403 (1.7516)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5784 (8.4180)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8899 (0.8899)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9911 (1.9911 -- 1.9911)  data: 1.7682 (1.7682 -- 1.7682)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5707 (0.6479)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3371 (0.0224 -- 1.9911)  data: 0.1769 (0.0001 -- 1.7682)  max mem: 16413
Val: Total time: 0:00:03 (0.3372 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [195]  [ 0/54]  eta: 0:06:23  lr: 0.000000  min_lr: 0.000000  loss: 1.5798 (1.5798)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7260 (3.7260)  time: 7.1099 (7.1099 -- 7.1099)  data: 5.8111 (5.8111 -- 5.8111)  max mem: 16413
Epoch: [195]  [20/54]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.5044 (1.5738)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7432 (7.8891)  time: 0.8618 (0.5188 -- 4.1970)  data: 0.3032 (0.0003 -- 3.4106)  max mem: 16413
[2023-10-23 23:02:14,063] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:02:14,063] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:02:14,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:02:14,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:02:23,495] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10565
[2023-10-23 23:02:23,495] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10565
[2023-10-23 23:02:23,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:02:23,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:02:23,495] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [195]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.4291 (1.5610)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7809 (7.9962)  time: 0.9140 (0.5223 -- 3.5487)  data: 0.0815 (0.0002 -- 1.5976)  max mem: 16413
Epoch: [195]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6796 (1.6012)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7338 (8.1179)  time: 0.6666 (0.4957 -- 1.9769)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [195] Total time: 0:00:50 (0.9444 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6796 (1.6623)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7338 (8.1179)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8899 (0.8899)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9841 (1.9841 -- 1.9841)  data: 1.7630 (1.7630 -- 1.7630)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5708 (0.6477)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3364 (0.0223 -- 1.9841)  data: 0.1764 (0.0001 -- 1.7630)  max mem: 16413
Val: Total time: 0:00:03 (0.3365 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [196]  [ 0/54]  eta: 0:07:05  lr: 0.000000  min_lr: 0.000000  loss: 1.6449 (1.6449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2621 (9.2621)  time: 7.8834 (7.8834 -- 7.8834)  data: 6.0590 (6.0590 -- 6.0590)  max mem: 16413
[2023-10-23 23:02:54,877] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10593
[2023-10-23 23:02:54,877] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10593
[2023-10-23 23:02:54,877] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 23:02:54,877] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 23:02:54,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [196]  [20/54]  eta: 0:00:40  lr: 0.000000  min_lr: 0.000000  loss: 1.6505 (1.6523)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9689 (7.9952)  time: 0.8572 (0.5101 -- 3.6762)  data: 0.0047 (0.0002 -- 0.0646)  max mem: 16413
Epoch: [196]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7992 (1.6884)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3683 (7.6679)  time: 0.8566 (0.5393 -- 3.1008)  data: 0.1765 (0.0006 -- 1.6359)  max mem: 16413
Epoch: [196]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7992 (1.6951)  loss_scale: 8192.0000 (9557.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9642 (7.6893)  time: 0.6838 (0.4955 -- 1.6838)  data: 0.1626 (0.0001 -- 1.1649)  max mem: 16413
Epoch: [196] Total time: 0:00:50 (0.9285 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7992 (1.7018)  loss_scale: 8192.0000 (9557.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9642 (7.6893)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8899 (0.8899)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0119 (2.0119 -- 2.0119)  data: 1.8244 (1.8244 -- 1.8244)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5708 (0.6477)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3401 (0.0224 -- 2.0119)  data: 0.1825 (0.0001 -- 1.8244)  max mem: 16413
Val: Total time: 0:00:03 (0.3402 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [197]  [ 0/54]  eta: 0:08:17  lr: 0.000000  min_lr: 0.000000  loss: 1.1332 (1.1332)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8688 (7.8688)  time: 9.2063 (9.2063 -- 9.2063)  data: 4.9454 (4.9454 -- 4.9454)  max mem: 16413
Epoch: [197]  [20/54]  eta: 0:00:43  lr: 0.000000  min_lr: 0.000000  loss: 1.5390 (1.6018)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9004 (7.7749)  time: 0.8947 (0.5117 -- 4.6611)  data: 0.0012 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [197]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7436 (1.6962)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2568 (8.0071)  time: 0.8356 (0.5163 -- 3.0848)  data: 0.0016 (0.0001 -- 0.0070)  max mem: 16413
Epoch: [197]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5435 (1.6758)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0123 (7.8674)  time: 0.7258 (0.4942 -- 3.0848)  data: 0.0008 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [197] Total time: 0:00:51 (0.9611 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5435 (1.7005)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0123 (7.8674)
Val:  [ 0/10]  eta: 0:00:19  loss: 0.8896 (0.8896)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 1.9806 (1.9806 -- 1.9806)  data: 1.7796 (1.7796 -- 1.7796)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5708 (0.6476)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3360 (0.0223 -- 1.9806)  data: 0.1780 (0.0001 -- 1.7796)  max mem: 16413
Val: Total time: 0:00:03 (0.3361 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [198]  [ 0/54]  eta: 0:06:35  lr: 0.000000  min_lr: 0.000000  loss: 1.5083 (1.5083)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9619 (10.9619)  time: 7.3305 (7.3305 -- 7.3305)  data: 6.8125 (6.8125 -- 6.8125)  max mem: 16413
Epoch: [198]  [20/54]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.7353 (1.8251)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7035 (7.4729)  time: 0.8501 (0.5241 -- 2.6030)  data: 0.0751 (0.0009 -- 0.7692)  max mem: 16413
[2023-10-23 23:05:00,757] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:05:00,758] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 23:05:00,762] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:05:00,763] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [198]  [40/54]  eta: 0:00:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6050 (1.7431)  loss_scale: 16384.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4546 (7.6149)  time: 0.9697 (0.5082 -- 4.4426)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [198]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5446 (1.6965)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0364 (7.9513)  time: 0.5955 (0.4960 -- 2.0606)  data: 0.0007 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [198] Total time: 0:00:50 (0.9319 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5446 (1.7288)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0364 (7.9513)
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8899 (0.8899)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0298 (2.0298 -- 2.0298)  data: 1.8402 (1.8402 -- 1.8402)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5709 (0.6477)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3405 (0.0222 -- 2.0298)  data: 0.1841 (0.0001 -- 1.8402)  max mem: 16413
Val: Total time: 0:00:03 (0.3406 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Epoch: [199]  [ 0/54]  eta: 0:07:17  lr: 0.000000  min_lr: 0.000000  loss: 1.8900 (1.8900)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3222 (8.3222)  time: 8.0973 (8.0973 -- 8.0973)  data: 7.5437 (7.5437 -- 7.5437)  max mem: 16413
Epoch: [199]  [20/54]  eta: 0:00:43  lr: 0.000000  min_lr: 0.000000  loss: 1.7392 (1.7938)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2072 (8.0209)  time: 0.9260 (0.5284 -- 5.4472)  data: 0.3787 (0.0005 -- 4.9166)  max mem: 16413
Epoch: [199]  [40/54]  eta: 0:00:15  lr: 0.000000  min_lr: 0.000000  loss: 1.6975 (1.7493)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0330 (8.2226)  time: 0.8867 (0.5166 -- 4.5220)  data: 0.3501 (0.0002 -- 3.9919)  max mem: 16413
Epoch: [199]  [53/54]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6695 (1.7105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7507 (8.3852)  time: 0.5543 (0.4958 -- 1.2272)  data: 0.0360 (0.0002 -- 0.7105)  max mem: 16413
Epoch: [199] Total time: 0:00:50 (0.9440 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6695 (1.6957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7507 (8.3852)
[2023-10-23 23:06:12,696] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-10-23 23:06:12,699] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-199/mp_rank_00_model_states.pt
[2023-10-23 23:06:12,699] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-199/mp_rank_00_model_states.pt...
[2023-10-23 23:06:12,699] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-10-23 23:06:13,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_rear/checkpoint-199/mp_rank_00_model_states.pt.
[2023-10-23 23:06:13,661] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/10]  eta: 0:00:20  loss: 0.8894 (0.8894)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0198 (2.0198 -- 2.0198)  data: 1.8323 (1.8323 -- 1.8323)  max mem: 16413
Val:  [ 9/10]  eta: 0:00:00  loss: 0.5709 (0.6477)  acc1: 77.7778 (81.7073)  acc5: 100.0000 (95.1220)  time: 0.3407 (0.0228 -- 2.0198)  data: 0.1834 (0.0001 -- 1.8323)  max mem: 16413
Val: Total time: 0:00:03 (0.3408 s / it)
* Acc@1 82.927 Acc@5 94.512 loss 0.783
Accuracy of the network on the 163 val images: 82.93%
Max accuracy: 84.15%
Test:  [  0/408]  eta: 0:30:14  loss: 0.1645 (0.1645)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.4469 (4.4469 -- 4.4469)  data: 4.1627 (4.1627 -- 4.1627)  max mem: 16413
Test:  [ 10/408]  eta: 0:05:32  loss: 0.5253 (0.7193)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (98.4848)  time: 0.8361 (0.1370 -- 4.4469)  data: 0.6750 (0.0006 -- 4.1627)  max mem: 16413
Test:  [ 20/408]  eta: 0:04:25  loss: 0.7672 (0.8210)  acc1: 83.3333 (78.5714)  acc5: 100.0000 (96.0317)  time: 0.4952 (0.1370 -- 2.6465)  data: 0.3430 (0.0004 -- 2.5044)  max mem: 16413
Test:  [ 30/408]  eta: 0:04:00  loss: 0.6340 (0.7285)  acc1: 83.3333 (81.7204)  acc5: 100.0000 (97.3118)  time: 0.5248 (0.1379 -- 3.9666)  data: 0.3719 (0.0003 -- 3.8209)  max mem: 16413
Test:  [ 40/408]  eta: 0:04:39  loss: 0.5920 (0.7548)  acc1: 83.3333 (81.3008)  acc5: 100.0000 (96.7480)  time: 0.8369 (0.1339 -- 6.3239)  data: 0.6799 (0.0003 -- 6.1829)  max mem: 16413
Test:  [ 50/408]  eta: 0:04:15  loss: 0.6423 (0.7452)  acc1: 83.3333 (80.7190)  acc5: 100.0000 (96.7320)  time: 0.8341 (0.1325 -- 6.3239)  data: 0.6320 (0.0003 -- 6.1829)  max mem: 16413
Test:  [ 60/408]  eta: 0:03:44  loss: 0.5926 (0.7211)  acc1: 83.3333 (81.4208)  acc5: 100.0000 (97.2678)  time: 0.4089 (0.1325 -- 3.0163)  data: 0.2162 (0.0003 -- 2.8724)  max mem: 16413
Test:  [ 70/408]  eta: 0:03:46  loss: 0.4835 (0.7258)  acc1: 83.3333 (80.9859)  acc5: 100.0000 (96.9484)  time: 0.5573 (0.1377 -- 3.2213)  data: 0.3796 (0.0004 -- 3.0589)  max mem: 16413
Test:  [ 80/408]  eta: 0:03:46  loss: 0.4715 (0.7099)  acc1: 83.3333 (81.4815)  acc5: 100.0000 (96.9136)  time: 0.8303 (0.1383 -- 3.9280)  data: 0.6470 (0.0006 -- 3.7837)  max mem: 16413
Test:  [ 90/408]  eta: 0:03:31  loss: 0.4715 (0.7046)  acc1: 83.3333 (81.8681)  acc5: 100.0000 (97.0696)  time: 0.6535 (0.1263 -- 3.9280)  data: 0.5051 (0.0003 -- 3.7837)  max mem: 16413
Test:  [100/408]  eta: 0:03:23  loss: 0.5424 (0.6976)  acc1: 83.3333 (82.0132)  acc5: 100.0000 (97.1947)  time: 0.5396 (0.1263 -- 4.1984)  data: 0.3485 (0.0003 -- 3.6443)  max mem: 16413
Test:  [110/408]  eta: 0:03:14  loss: 0.5291 (0.6858)  acc1: 83.3333 (82.5826)  acc5: 100.0000 (97.1471)  time: 0.5919 (0.1331 -- 4.1984)  data: 0.3999 (0.0004 -- 3.6443)  max mem: 16413
Test:  [120/408]  eta: 0:03:03  loss: 0.5291 (0.6876)  acc1: 83.3333 (82.5069)  acc5: 100.0000 (97.1074)  time: 0.5187 (0.1352 -- 2.5969)  data: 0.3769 (0.0004 -- 2.4570)  max mem: 16413
Test:  [130/408]  eta: 0:03:09  loss: 0.3968 (0.6680)  acc1: 83.3333 (83.0789)  acc5: 100.0000 (97.3282)  time: 0.8340 (0.1352 -- 4.3532)  data: 0.6806 (0.0004 -- 4.1823)  max mem: 16413
Test:  [140/408]  eta: 0:02:58  loss: 0.4366 (0.6664)  acc1: 83.3333 (83.2151)  acc5: 100.0000 (97.3995)  time: 0.8330 (0.1383 -- 4.3532)  data: 0.6494 (0.0006 -- 4.1823)  max mem: 16413
Test:  [150/408]  eta: 0:02:46  loss: 0.5335 (0.6637)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (97.3510)  time: 0.4078 (0.1302 -- 1.3465)  data: 0.2363 (0.0004 -- 1.1915)  max mem: 16413
Test:  [160/408]  eta: 0:02:45  loss: 0.4699 (0.6558)  acc1: 83.3333 (83.5404)  acc5: 100.0000 (97.4120)  time: 0.6717 (0.1302 -- 4.3411)  data: 0.5093 (0.0004 -- 4.2043)  max mem: 16413
Test:  [170/408]  eta: 0:02:38  loss: 0.4380 (0.6541)  acc1: 83.3333 (83.7232)  acc5: 100.0000 (97.2710)  time: 0.8155 (0.1316 -- 4.3411)  data: 0.6029 (0.0004 -- 4.2043)  max mem: 16413
Test:  [180/408]  eta: 0:02:31  loss: 0.6190 (0.6541)  acc1: 83.3333 (83.5175)  acc5: 100.0000 (97.2376)  time: 0.6466 (0.1320 -- 3.2431)  data: 0.4575 (0.0004 -- 3.1010)  max mem: 16413
Test:  [190/408]  eta: 0:02:24  loss: 0.6226 (0.6549)  acc1: 83.3333 (83.5951)  acc5: 100.0000 (97.1204)  time: 0.6491 (0.1320 -- 2.7868)  data: 0.5054 (0.0005 -- 2.6574)  max mem: 16413
Test:  [200/408]  eta: 0:02:15  loss: 0.6372 (0.6533)  acc1: 83.3333 (83.7479)  acc5: 100.0000 (97.0149)  time: 0.5472 (0.1302 -- 2.7821)  data: 0.4002 (0.0006 -- 2.6142)  max mem: 16413
Test:  [210/408]  eta: 0:02:07  loss: 0.6372 (0.6532)  acc1: 83.3333 (83.7283)  acc5: 100.0000 (96.9194)  time: 0.4632 (0.1302 -- 2.8309)  data: 0.3193 (0.0006 -- 2.6896)  max mem: 16413
Test:  [220/408]  eta: 0:02:00  loss: 0.6642 (0.6572)  acc1: 83.3333 (83.7104)  acc5: 100.0000 (96.8326)  time: 0.5054 (0.1369 -- 2.8309)  data: 0.3636 (0.0006 -- 2.6896)  max mem: 16413
Test:  [230/408]  eta: 0:01:52  loss: 0.6313 (0.6598)  acc1: 83.3333 (83.4055)  acc5: 100.0000 (96.6811)  time: 0.4829 (0.1353 -- 2.8842)  data: 0.3425 (0.0006 -- 2.7399)  max mem: 16413
Test:  [240/408]  eta: 0:01:45  loss: 0.6076 (0.6597)  acc1: 83.3333 (83.4716)  acc5: 100.0000 (96.6805)  time: 0.4937 (0.1346 -- 2.8842)  data: 0.3488 (0.0006 -- 2.7399)  max mem: 16413
Test:  [250/408]  eta: 0:01:38  loss: 0.5744 (0.6599)  acc1: 83.3333 (83.4661)  acc5: 100.0000 (96.6135)  time: 0.5402 (0.1338 -- 2.3770)  data: 0.3934 (0.0006 -- 2.2280)  max mem: 16413
Test:  [260/408]  eta: 0:01:32  loss: 0.5269 (0.6613)  acc1: 83.3333 (83.4610)  acc5: 100.0000 (96.4879)  time: 0.5816 (0.1338 -- 3.6380)  data: 0.4357 (0.0007 -- 3.5044)  max mem: 16413
Test:  [270/408]  eta: 0:01:24  loss: 0.5269 (0.6652)  acc1: 83.3333 (83.3948)  acc5: 100.0000 (96.3715)  time: 0.5274 (0.1344 -- 3.6380)  data: 0.3835 (0.0004 -- 3.5044)  max mem: 16413
Test:  [280/408]  eta: 0:01:17  loss: 0.4673 (0.6668)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (96.2633)  time: 0.4203 (0.1344 -- 2.6059)  data: 0.2754 (0.0004 -- 2.4624)  max mem: 16413
Test:  [290/408]  eta: 0:01:10  loss: 0.4303 (0.6633)  acc1: 83.3333 (83.3906)  acc5: 100.0000 (96.3345)  time: 0.4053 (0.1363 -- 2.6059)  data: 0.2550 (0.0006 -- 2.4624)  max mem: 16413
Test:  [300/408]  eta: 0:01:06  loss: 0.5558 (0.6714)  acc1: 83.3333 (83.1672)  acc5: 100.0000 (96.1794)  time: 0.6655 (0.1369 -- 4.3840)  data: 0.5117 (0.0005 -- 4.2366)  max mem: 16413
Test:  [310/408]  eta: 0:00:59  loss: 0.6697 (0.6765)  acc1: 83.3333 (83.0118)  acc5: 100.0000 (96.0343)  time: 0.7102 (0.1339 -- 4.3840)  data: 0.5593 (0.0005 -- 4.2366)  max mem: 16413
Test:  [320/408]  eta: 0:00:53  loss: 0.6090 (0.6819)  acc1: 83.3333 (82.8141)  acc5: 100.0000 (95.9502)  time: 0.5532 (0.1339 -- 2.7503)  data: 0.4025 (0.0007 -- 2.5802)  max mem: 16413
Test:  [330/408]  eta: 0:00:47  loss: 0.4293 (0.6814)  acc1: 83.3333 (82.8298)  acc5: 100.0000 (95.9718)  time: 0.5953 (0.1414 -- 2.9128)  data: 0.4409 (0.0006 -- 2.7714)  max mem: 16413
Test:  [340/408]  eta: 0:00:41  loss: 0.5248 (0.6857)  acc1: 83.3333 (82.7957)  acc5: 100.0000 (95.8456)  time: 0.6158 (0.1390 -- 3.1341)  data: 0.4612 (0.0006 -- 2.9965)  max mem: 16413
Test:  [350/408]  eta: 0:00:35  loss: 0.6419 (0.6934)  acc1: 83.3333 (82.5736)  acc5: 100.0000 (95.6790)  time: 0.5673 (0.1351 -- 3.3200)  data: 0.4148 (0.0006 -- 3.1697)  max mem: 16413
Test:  [360/408]  eta: 0:00:28  loss: 0.7687 (0.6972)  acc1: 83.3333 (82.5485)  acc5: 100.0000 (95.6602)  time: 0.4479 (0.1351 -- 3.3200)  data: 0.2993 (0.0006 -- 3.1697)  max mem: 16413
Test:  [370/408]  eta: 0:00:22  loss: 0.6489 (0.6948)  acc1: 83.3333 (82.6146)  acc5: 100.0000 (95.6424)  time: 0.4194 (0.1338 -- 1.8240)  data: 0.2761 (0.0005 -- 1.6433)  max mem: 16413
Test:  [380/408]  eta: 0:00:16  loss: 0.5530 (0.7050)  acc1: 83.3333 (82.3272)  acc5: 100.0000 (95.3631)  time: 0.5313 (0.1338 -- 1.9660)  data: 0.3921 (0.0003 -- 1.8246)  max mem: 16413
Test:  [390/408]  eta: 0:00:10  loss: 0.8540 (0.7113)  acc1: 66.6667 (82.1398)  acc5: 100.0000 (95.3112)  time: 0.6206 (0.1363 -- 3.9264)  data: 0.4811 (0.0003 -- 3.7869)  max mem: 16413
Test:  [400/408]  eta: 0:00:04  loss: 0.8297 (0.7166)  acc1: 83.3333 (81.9618)  acc5: 100.0000 (95.0956)  time: 0.4128 (0.1220 -- 3.9264)  data: 0.2759 (0.0001 -- 3.7869)  max mem: 16413
Test:  [407/408]  eta: 0:00:00  loss: 0.5177 (0.7191)  acc1: 83.3333 (81.9223)  acc5: 100.0000 (95.0511)  time: 0.2378 (0.0801 -- 1.0585)  data: 0.1110 (0.0001 -- 0.9294)  max mem: 16413
Test: Total time: 0:03:57 (0.5832 s / it)
* Acc@1 81.779 Acc@5 95.092 loss 0.721
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 4890 test videos: Top-1: 87.73%, Top-5: 99.39%
Training time 3:05:32
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2023-10-23 23:10:25,268] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-23 23:10:25,306] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast/annotations/Right_side_window', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, img_diff_json_path=None, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sample_mode='normal', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=10, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f8f937bef10>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 629
Number of training training per epoch = 52
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-10-23 23:10:30,903] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-10-23 23:10:30,903] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-10-23 23:10:30,971] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-10-23 23:10:30,971] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-10-23 23:10:31,082] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.417311429977417 seconds
[2023-10-23 23:10:32,190] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-10-23 23:10:32,195] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-10-23 23:10:32,195] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-10-23 23:10:32,215] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-10-23 23:10:32,215] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-23 23:10:32,215] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-10-23 23:10:32,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 23:10:32,216] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-10-23 23:10:32,216] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   amp_params ................... False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8e88d87b20>
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   dump_state ................... False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-10-23 23:10:32,217] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   pld_params ................... False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-10-23 23:10:32,218] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   world_size ................... 2
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-23 23:10:32,219] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-10-23 23:10:32,219] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 260
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [ 0/52]  eta: 0:09:37  lr: 0.000000  min_lr: 0.000000  loss: 2.7733 (2.7733)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 11.1089 (11.1089 -- 11.1089)  data: 7.1008 (7.1008 -- 7.1008)  max mem: 16413
Epoch: [0]  [20/52]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 2.7727 (2.7728)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5081 (1.5302)  time: 0.6467 (0.5152 -- 1.8256)  data: 0.0307 (0.0002 -- 0.4569)  max mem: 16413
Epoch: [0]  [40/52]  eta: 0:00:12  lr: 0.000007  min_lr: 0.000000  loss: 2.7726 (2.7727)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3645 (1.4586)  time: 0.8804 (0.4926 -- 2.6629)  data: 0.0535 (0.0003 -- 0.6436)  max mem: 16413
Epoch: [0]  [51/52]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4493 (1.4800)  time: 0.8284 (0.4824 -- 2.3077)  data: 0.0429 (0.0002 -- 0.6436)  max mem: 16413
Epoch: [0] Total time: 0:00:48 (0.9415 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4493 (1.4800)
Val:  [0/9]  eta: 0:00:23  loss: 2.7708 (2.7708)  acc1: 22.2222 (22.2222)  acc5: 66.6667 (66.6667)  time: 2.5573 (2.5573 -- 2.5573)  data: 2.2851 (2.2851 -- 2.2851)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.7713 (2.7712)  acc1: 22.2222 (21.5190)  acc5: 55.5556 (55.6962)  time: 0.4373 (0.1652 -- 2.5573)  data: 0.2540 (0.0001 -- 2.2851)  max mem: 16413
Val: Total time: 0:00:03 (0.4374 s / it)
* Acc@1 21.519 Acc@5 55.696 loss 2.771
Accuracy of the network on the 158 val images: 21.52%
[2023-10-23 23:11:25,157] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-10-23 23:11:25,162] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:11:25,163] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:11:25,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:11:26,102] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:11:26,102] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 21.52%
Epoch: [1]  [ 0/52]  eta: 0:06:24  lr: 0.000009  min_lr: 0.000000  loss: 2.7716 (2.7716)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.1370 (1.1370)  time: 7.3922 (7.3922 -- 7.3922)  data: 6.8605 (6.8605 -- 6.8605)  max mem: 16413
Epoch: [1]  [20/52]  eta: 0:00:35  lr: 0.000013  min_lr: 0.000000  loss: 2.7718 (2.7719)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4373 (1.4733)  time: 0.7933 (0.5079 -- 2.9368)  data: 0.2037 (0.0003 -- 2.4060)  max mem: 16413
Epoch: [1]  [40/52]  eta: 0:00:11  lr: 0.000017  min_lr: 0.000000  loss: 2.7706 (2.7712)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4163 (1.4444)  time: 0.8766 (0.5129 -- 3.9524)  data: 0.3008 (0.0004 -- 3.4246)  max mem: 16413
Epoch: [1]  [51/52]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.7704 (2.7711)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4808 (1.4610)  time: 0.7243 (0.4893 -- 3.3434)  data: 0.1714 (0.0002 -- 2.8469)  max mem: 16413
Epoch: [1] Total time: 0:00:49 (0.9455 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.7704 (2.7712)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4808 (1.4610)
Val:  [0/9]  eta: 0:00:18  loss: 2.7626 (2.7626)  acc1: 22.2222 (22.2222)  acc5: 55.5556 (55.5556)  time: 2.0987 (2.0987 -- 2.0987)  data: 1.9113 (1.9113 -- 1.9113)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.7634 (2.7648)  acc1: 22.2222 (24.0506)  acc5: 55.5556 (60.7595)  time: 0.3786 (0.1317 -- 2.0987)  data: 0.2124 (0.0001 -- 1.9113)  max mem: 16413
Val: Total time: 0:00:03 (0.3787 s / it)
* Acc@1 24.051 Acc@5 58.861 loss 2.765
Accuracy of the network on the 158 val images: 24.05%
[2023-10-23 23:12:18,680] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:12:18,682] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:12:18,682] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:12:18,682] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:12:19,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:12:19,996] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 24.05%
Epoch: [2]  [ 0/52]  eta: 0:05:22  lr: 0.000019  min_lr: 0.000000  loss: 2.7668 (2.7668)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5426 (1.5426)  time: 6.2033 (6.2033 -- 6.2033)  data: 5.6573 (5.6573 -- 5.6573)  max mem: 16413
Epoch: [2]  [20/52]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 2.7668 (2.7667)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4374 (1.4853)  time: 0.9042 (0.5199 -- 3.1301)  data: 0.3717 (0.0007 -- 2.6284)  max mem: 16413
[2023-10-23 23:12:46,517] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:12:46,518] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-10-23 23:12:46,519] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:12:46,520] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [2]  [40/52]  eta: 0:00:12  lr: 0.000026  min_lr: 0.000001  loss: 2.7625 (2.7643)  loss_scale: 256.0000 (181.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5869 (1.5328)  time: 0.8579 (0.5128 -- 4.2201)  data: 0.3120 (0.0003 -- 3.7159)  max mem: 16413
Epoch: [2]  [51/52]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 2.7572 (2.7623)  loss_scale: 256.0000 (196.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4913 (1.5230)  time: 0.7633 (0.4899 -- 3.0310)  data: 0.2467 (0.0002 -- 2.4970)  max mem: 16413
Epoch: [2] Total time: 0:00:49 (0.9488 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 2.7572 (2.7616)  loss_scale: 256.0000 (196.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4913 (1.5230)
Val:  [0/9]  eta: 0:00:18  loss: 2.7131 (2.7131)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0218 (2.0218 -- 2.0218)  data: 1.8386 (1.8386 -- 1.8386)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.7337 (2.7290)  acc1: 33.3333 (37.9747)  acc5: 85.7143 (86.0759)  time: 0.3718 (0.1326 -- 2.0218)  data: 0.2044 (0.0001 -- 1.8386)  max mem: 16413
Val: Total time: 0:00:03 (0.3719 s / it)
* Acc@1 33.544 Acc@5 84.177 loss 2.730
Accuracy of the network on the 158 val images: 33.54%
[2023-10-23 23:13:12,691] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:13:12,693] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:13:12,693] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:13:12,693] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:13:13,962] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:13:13,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 33.54%
Epoch: [3]  [ 0/52]  eta: 0:06:48  lr: 0.000028  min_lr: 0.000001  loss: 2.7624 (2.7624)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3666 (1.3666)  time: 7.8643 (7.8643 -- 7.8643)  data: 5.9943 (5.9943 -- 5.9943)  max mem: 16413
Epoch: [3]  [20/52]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000001  loss: 2.7456 (2.7447)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5196 (1.5419)  time: 0.8235 (0.5303 -- 3.0055)  data: 0.1294 (0.0007 -- 2.2353)  max mem: 16413
Epoch: [3]  [40/52]  eta: 0:00:12  lr: 0.000035  min_lr: 0.000001  loss: 2.7270 (2.7349)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5800 (1.5728)  time: 0.8338 (0.5101 -- 2.5716)  data: 0.1964 (0.0003 -- 2.0388)  max mem: 16413
Epoch: [3]  [51/52]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 2.7255 (2.7322)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5641 (1.5801)  time: 0.7460 (0.4930 -- 2.5716)  data: 0.1033 (0.0001 -- 2.0388)  max mem: 16413
Epoch: [3] Total time: 0:00:47 (0.9157 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 2.7255 (2.7309)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5641 (1.5801)
Val:  [0/9]  eta: 0:00:18  loss: 2.6063 (2.6063)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0565 (2.0565 -- 2.0565)  data: 1.8779 (1.8779 -- 1.8779)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.6508 (2.6450)  acc1: 42.8571 (37.9747)  acc5: 77.7778 (82.2785)  time: 0.3755 (0.1334 -- 2.0565)  data: 0.2087 (0.0001 -- 1.8779)  max mem: 16413
Val: Total time: 0:00:03 (0.3756 s / it)
* Acc@1 32.278 Acc@5 80.380 loss 2.650
Accuracy of the network on the 158 val images: 32.28%
Max accuracy: 33.54%
Epoch: [4]  [ 0/52]  eta: 0:06:31  lr: 0.000038  min_lr: 0.000001  loss: 2.7084 (2.7084)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5383 (1.5383)  time: 7.5287 (7.5287 -- 7.5287)  data: 6.8418 (6.8418 -- 6.8418)  max mem: 16413
Epoch: [4]  [20/52]  eta: 0:00:38  lr: 0.000041  min_lr: 0.000001  loss: 2.6898 (2.6916)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6548 (1.6514)  time: 0.8818 (0.5326 -- 3.1485)  data: 0.3140 (0.0002 -- 2.6247)  max mem: 16413
Epoch: [4]  [40/52]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000001  loss: 2.6383 (2.6673)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7194 (1.7018)  time: 0.7723 (0.5120 -- 2.9498)  data: 0.1717 (0.0007 -- 2.4204)  max mem: 16413
[2023-10-23 23:14:50,360] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:14:50,360] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-10-23 23:14:50,360] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:14:50,360] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [4]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.6430 (2.6595)  loss_scale: 256.0000 (275.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8157 (1.7252)  time: 0.6785 (0.4930 -- 1.5488)  data: 0.1026 (0.0001 -- 0.6915)  max mem: 16413
Epoch: [4] Total time: 0:00:47 (0.9103 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.6430 (2.6667)  loss_scale: 256.0000 (275.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8157 (1.7252)
Val:  [0/9]  eta: 0:00:18  loss: 2.4711 (2.4711)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.0254 (2.0254 -- 2.0254)  data: 1.8413 (1.8413 -- 1.8413)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.5443 (2.5282)  acc1: 44.4444 (41.7722)  acc5: 88.8889 (87.3418)  time: 0.3724 (0.1335 -- 2.0254)  data: 0.2047 (0.0001 -- 1.8413)  max mem: 16413
Val: Total time: 0:00:03 (0.3725 s / it)
* Acc@1 37.975 Acc@5 79.747 loss 2.533
Accuracy of the network on the 158 val images: 37.97%
[2023-10-23 23:14:55,663] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:14:55,665] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:14:55,665] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:14:55,665] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:14:57,072] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:14:57,072] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.97%
Epoch: [5]  [ 0/52]  eta: 0:05:38  lr: 0.000047  min_lr: 0.000001  loss: 2.6749 (2.6749)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8321 (1.8321)  time: 6.5165 (6.5165 -- 6.5165)  data: 5.9005 (5.9005 -- 5.9005)  max mem: 16413
Epoch: [5]  [20/52]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000001  loss: 2.6342 (2.6276)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8016 (1.8992)  time: 0.9234 (0.5131 -- 4.2287)  data: 0.3822 (0.0007 -- 3.6959)  max mem: 16413
Epoch: [5]  [40/52]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000001  loss: 2.6016 (2.6091)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0161 (1.9407)  time: 0.8494 (0.5202 -- 4.5772)  data: 0.2963 (0.0003 -- 4.0633)  max mem: 16413
Epoch: [5]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.6044 (2.6099)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0464 (1.9770)  time: 0.6315 (0.4945 -- 2.3295)  data: 0.1075 (0.0001 -- 1.8318)  max mem: 16413
Epoch: [5] Total time: 0:00:47 (0.9199 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.6044 (2.6214)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0464 (1.9770)
Val:  [0/9]  eta: 0:00:18  loss: 2.3483 (2.3483)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.0916 (2.0916 -- 2.0916)  data: 1.9189 (1.9189 -- 1.9189)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.4460 (2.4266)  acc1: 42.8571 (40.5063)  acc5: 88.8889 (86.0760)  time: 0.3800 (0.1336 -- 2.0916)  data: 0.2133 (0.0001 -- 1.9189)  max mem: 16413
Val: Total time: 0:00:03 (0.3801 s / it)
* Acc@1 36.709 Acc@5 84.810 loss 2.431
Accuracy of the network on the 158 val images: 36.71%
Max accuracy: 37.97%
Epoch: [6]  [ 0/52]  eta: 0:05:31  lr: 0.000047  min_lr: 0.000001  loss: 2.5842 (2.5842)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5860 (2.5860)  time: 6.3812 (6.3812 -- 6.3812)  data: 5.2139 (5.2139 -- 5.2139)  max mem: 16413
Epoch: [6]  [20/52]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.6219 (2.6028)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0275 (2.1134)  time: 0.8730 (0.5095 -- 3.7908)  data: 0.3260 (0.0007 -- 3.2697)  max mem: 16413
Epoch: [6]  [40/52]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000001  loss: 2.5784 (2.5807)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1129 (2.2071)  time: 0.8475 (0.5225 -- 4.1343)  data: 0.2069 (0.0002 -- 3.5772)  max mem: 16413
Epoch: [6]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.5421 (2.5697)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0860 (2.2375)  time: 0.7393 (0.4925 -- 2.2348)  data: 0.0272 (0.0002 -- 0.3326)  max mem: 16413
Epoch: [6] Total time: 0:00:48 (0.9283 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.5421 (2.5723)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0860 (2.2375)
Val:  [0/9]  eta: 0:00:18  loss: 2.2432 (2.2432)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.1107 (2.1107 -- 2.1107)  data: 1.9258 (1.9258 -- 1.9258)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.3643 (2.3333)  acc1: 33.3333 (40.5063)  acc5: 88.8889 (87.3418)  time: 0.3821 (0.1328 -- 2.1107)  data: 0.2140 (0.0001 -- 1.9258)  max mem: 16413
Val: Total time: 0:00:03 (0.3822 s / it)
* Acc@1 37.975 Acc@5 81.646 loss 2.345
Accuracy of the network on the 158 val images: 37.97%
Max accuracy: 37.97%
Epoch: [7]  [ 0/52]  eta: 0:06:15  lr: 0.000047  min_lr: 0.000001  loss: 2.4569 (2.4569)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4655 (2.4655)  time: 7.2123 (7.2123 -- 7.2123)  data: 6.6939 (6.6939 -- 6.6939)  max mem: 16413
[2023-10-23 23:17:03,348] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:17:03,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-10-23 23:17:03,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:17:03,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [7]  [20/52]  eta: 0:00:35  lr: 0.000047  min_lr: 0.000001  loss: 2.4945 (2.5007)  loss_scale: 512.0000 (536.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1650 (2.4608)  time: 0.8025 (0.5291 -- 2.6245)  data: 0.1658 (0.0007 -- 2.1040)  max mem: 16413
Epoch: [7]  [40/52]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000001  loss: 2.5026 (2.5025)  loss_scale: 1024.0000 (774.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3185 (2.4295)  time: 0.8935 (0.5342 -- 3.4232)  data: 0.2926 (0.0004 -- 2.8728)  max mem: 16413
Epoch: [7]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.5026 (2.5046)  loss_scale: 1024.0000 (827.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3185 (2.4184)  time: 0.7159 (0.4942 -- 2.1648)  data: 0.1296 (0.0001 -- 1.6351)  max mem: 16413
Epoch: [7] Total time: 0:00:46 (0.9030 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.5026 (2.5145)  loss_scale: 1024.0000 (827.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3185 (2.4184)
Val:  [0/9]  eta: 0:00:19  loss: 2.1608 (2.1608)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.1518 (2.1518 -- 2.1518)  data: 1.9732 (1.9732 -- 1.9732)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.2605 (2.2586)  acc1: 44.4444 (44.3038)  acc5: 88.8889 (88.6076)  time: 0.3848 (0.1327 -- 2.1518)  data: 0.2193 (0.0001 -- 1.9732)  max mem: 16413
Val: Total time: 0:00:03 (0.3849 s / it)
* Acc@1 41.139 Acc@5 86.076 loss 2.269
Accuracy of the network on the 158 val images: 41.14%
[2023-10-23 23:17:30,474] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:17:30,476] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:17:30,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:17:30,476] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:17:31,620] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:17:31,620] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.14%
Epoch: [8]  [ 0/52]  eta: 0:07:03  lr: 0.000047  min_lr: 0.000001  loss: 2.4456 (2.4456)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2742 (2.2742)  time: 8.1515 (8.1515 -- 8.1515)  data: 7.6033 (7.6033 -- 7.6033)  max mem: 16413
Epoch: [8]  [20/52]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.5481 (2.5341)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7926 (2.9076)  time: 0.7801 (0.5186 -- 2.4471)  data: 0.2333 (0.0003 -- 1.9176)  max mem: 16413
Epoch: [8]  [40/52]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000001  loss: 2.4702 (2.5042)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6867 (2.8590)  time: 0.9900 (0.5290 -- 4.3856)  data: 0.4447 (0.0003 -- 3.8697)  max mem: 16413
Epoch: [8]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4314 (2.4818)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6867 (2.8255)  time: 0.8112 (0.4925 -- 3.8183)  data: 0.2899 (0.0000 -- 3.3227)  max mem: 16413
Epoch: [8] Total time: 0:00:49 (0.9586 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4314 (2.4728)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6867 (2.8255)
Val:  [0/9]  eta: 0:00:18  loss: 2.0840 (2.0840)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.0076 (2.0076 -- 2.0076)  data: 1.8225 (1.8225 -- 1.8225)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.1672 (2.1776)  acc1: 33.3333 (41.7722)  acc5: 88.8889 (88.6076)  time: 0.3690 (0.1326 -- 2.0076)  data: 0.2026 (0.0001 -- 1.8225)  max mem: 16413
Val: Total time: 0:00:03 (0.3692 s / it)
* Acc@1 43.671 Acc@5 87.975 loss 2.188
Accuracy of the network on the 158 val images: 43.67%
[2023-10-23 23:18:24,835] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:18:24,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:18:24,837] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:18:24,837] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:18:26,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:18:26,248] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.67%
Epoch: [9]  [ 0/52]  eta: 0:06:33  lr: 0.000047  min_lr: 0.000001  loss: 2.4163 (2.4163)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6135 (2.6135)  time: 7.5622 (7.5622 -- 7.5622)  data: 7.0232 (7.0232 -- 7.0232)  max mem: 16413
Epoch: [9]  [20/52]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.4924 (2.4873)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7888 (2.9819)  time: 0.8148 (0.5360 -- 3.5861)  data: 0.2050 (0.0002 -- 3.0118)  max mem: 16413
Epoch: [9]  [40/52]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000001  loss: 2.4580 (2.4646)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0385 (3.1537)  time: 0.9782 (0.5232 -- 4.6171)  data: 0.4247 (0.0003 -- 4.0773)  max mem: 16413
[2023-10-23 23:19:11,662] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:19:11,662] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-10-23 23:19:11,663] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:19:11,663] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [9]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4655 (2.4627)  loss_scale: 1024.0000 (1181.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8773 (3.1488)  time: 0.8758 (0.4929 -- 4.6171)  data: 0.3566 (0.0002 -- 4.0773)  max mem: 16413
Epoch: [9] Total time: 0:00:49 (0.9489 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4655 (2.4774)  loss_scale: 1024.0000 (1181.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8773 (3.1488)
Val:  [0/9]  eta: 0:00:18  loss: 1.9686 (1.9686)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.0889 (2.0889 -- 2.0889)  data: 1.9113 (1.9113 -- 1.9113)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.0924 (2.1083)  acc1: 44.4444 (49.3671)  acc5: 88.8889 (89.8734)  time: 0.3792 (0.1331 -- 2.0889)  data: 0.2124 (0.0001 -- 1.9113)  max mem: 16413
Val: Total time: 0:00:03 (0.3793 s / it)
* Acc@1 46.203 Acc@5 88.608 loss 2.116
Accuracy of the network on the 158 val images: 46.20%
[2023-10-23 23:19:19,016] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:19:19,018] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:19:19,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:19:19,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:19:20,418] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:19:20,418] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.20%
Epoch: [10]  [ 0/52]  eta: 0:06:07  lr: 0.000047  min_lr: 0.000001  loss: 2.4093 (2.4093)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6223 (2.6223)  time: 7.0702 (7.0702 -- 7.0702)  data: 6.4611 (6.4611 -- 6.4611)  max mem: 16413
Epoch: [10]  [20/52]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000001  loss: 2.4525 (2.3955)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0235 (3.0253)  time: 0.8686 (0.5202 -- 5.0213)  data: 0.0705 (0.0003 -- 1.3361)  max mem: 16413
Epoch: [10]  [40/52]  eta: 0:00:13  lr: 0.000047  min_lr: 0.000001  loss: 2.4847 (2.4137)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6508 (3.3303)  time: 1.0216 (0.5147 -- 4.2063)  data: 0.0594 (0.0003 -- 0.6299)  max mem: 16413
Epoch: [10]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.5016 (2.4331)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7195 (3.4995)  time: 0.8205 (0.4933 -- 4.2063)  data: 0.0273 (0.0001 -- 0.5192)  max mem: 16413
Epoch: [10] Total time: 0:00:50 (0.9759 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.5016 (2.4180)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7195 (3.4995)
Val:  [0/9]  eta: 0:00:19  loss: 1.8550 (1.8550)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.1346 (2.1346 -- 2.1346)  data: 1.9539 (1.9539 -- 1.9539)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 2.0160 (2.0466)  acc1: 44.4444 (49.3671)  acc5: 88.8889 (89.8734)  time: 0.3837 (0.1329 -- 2.1346)  data: 0.2172 (0.0001 -- 1.9539)  max mem: 16413
Val: Total time: 0:00:03 (0.3838 s / it)
* Acc@1 43.038 Acc@5 88.608 loss 2.054
Accuracy of the network on the 158 val images: 43.04%
Max accuracy: 46.20%
Epoch: [11]  [ 0/52]  eta: 0:05:53  lr: 0.000047  min_lr: 0.000001  loss: 2.3742 (2.3742)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1941 (3.1941)  time: 6.7907 (6.7907 -- 6.7907)  data: 4.6492 (4.6492 -- 4.6492)  max mem: 16413
Epoch: [11]  [20/52]  eta: 0:00:41  lr: 0.000047  min_lr: 0.000001  loss: 2.4404 (2.4316)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4570 (3.7505)  time: 1.0219 (0.5135 -- 5.4033)  data: 0.0969 (0.0006 -- 0.7748)  max mem: 16413
Epoch: [11]  [40/52]  eta: 0:00:11  lr: 0.000047  min_lr: 0.000001  loss: 2.4401 (2.4383)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6937 (3.7184)  time: 0.6529 (0.5254 -- 1.9044)  data: 0.0016 (0.0005 -- 0.0053)  max mem: 16413
Epoch: [11]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4742 (2.4349)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3994 (3.6632)  time: 0.7137 (0.4929 -- 2.8292)  data: 0.0008 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [11] Total time: 0:00:48 (0.9295 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4742 (2.4264)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3994 (3.6632)
Val:  [0/9]  eta: 0:00:19  loss: 1.8152 (1.8152)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1261 (2.1261 -- 2.1261)  data: 1.9472 (1.9472 -- 1.9472)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.9781 (1.9884)  acc1: 55.5556 (51.8987)  acc5: 88.8889 (92.4051)  time: 0.3820 (0.1326 -- 2.1261)  data: 0.2164 (0.0001 -- 1.9472)  max mem: 16413
Val: Total time: 0:00:03 (0.3821 s / it)
* Acc@1 50.633 Acc@5 91.139 loss 1.985
Accuracy of the network on the 158 val images: 50.63%
[2023-10-23 23:21:06,405] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:21:06,406] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:21:06,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:21:06,406] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:21:07,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:21:07,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.63%
Epoch: [12]  [ 0/52]  eta: 0:06:43  lr: 0.000047  min_lr: 0.000001  loss: 2.5998 (2.5998)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5605 (4.5605)  time: 7.7667 (7.7667 -- 7.7667)  data: 7.2445 (7.2445 -- 7.2445)  max mem: 16413
[2023-10-23 23:21:28,275] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:21:28,275] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-10-23 23:21:28,275] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:21:28,276] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [12]  [20/52]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000001  loss: 2.3376 (2.3812)  loss_scale: 2048.0000 (2535.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5005 (3.5595)  time: 0.8677 (0.5307 -- 3.2498)  data: 0.2642 (0.0003 -- 2.7242)  max mem: 16413
Epoch: [12]  [40/52]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000001  loss: 2.2876 (2.3544)  loss_scale: 4096.0000 (3296.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4992 (3.6071)  time: 0.8522 (0.5200 -- 3.0303)  data: 0.0534 (0.0003 -- 0.5742)  max mem: 16413
Epoch: [12]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4213 (2.3643)  loss_scale: 4096.0000 (3465.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7583 (3.7643)  time: 0.7720 (0.4925 -- 3.0303)  data: 0.0310 (0.0002 -- 0.5742)  max mem: 16413
Epoch: [12] Total time: 0:00:49 (0.9509 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4213 (2.3612)  loss_scale: 4096.0000 (3465.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7583 (3.7643)
Val:  [0/9]  eta: 0:00:18  loss: 1.7352 (1.7352)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.0982 (2.0982 -- 2.0982)  data: 1.9253 (1.9253 -- 1.9253)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.8955 (1.9155)  acc1: 44.4444 (48.1013)  acc5: 88.8889 (91.1392)  time: 0.3801 (0.1324 -- 2.0982)  data: 0.2140 (0.0001 -- 1.9253)  max mem: 16413
Val: Total time: 0:00:03 (0.3802 s / it)
* Acc@1 48.734 Acc@5 89.241 loss 1.924
Accuracy of the network on the 158 val images: 48.73%
Max accuracy: 50.63%
Epoch: [13]  [ 0/52]  eta: 0:06:15  lr: 0.000047  min_lr: 0.000001  loss: 2.2365 (2.2365)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0836 (4.0836)  time: 7.2248 (7.2248 -- 7.2248)  data: 6.7003 (6.7003 -- 6.7003)  max mem: 16413
Epoch: [13]  [20/52]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.3344 (2.2953)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6489 (3.9009)  time: 0.8378 (0.5221 -- 4.4234)  data: 0.2738 (0.0004 -- 3.8980)  max mem: 16413
Epoch: [13]  [40/52]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000001  loss: 2.3742 (2.3164)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9927 (3.9853)  time: 0.8863 (0.5179 -- 2.6314)  data: 0.2426 (0.0004 -- 1.5683)  max mem: 16413
Epoch: [13]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.2185 (2.3098)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8949 (4.1118)  time: 0.7862 (0.4929 -- 2.6314)  data: 0.1683 (0.0001 -- 1.6423)  max mem: 16413
Epoch: [13] Total time: 0:00:48 (0.9398 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.2185 (2.3179)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8949 (4.1118)
Val:  [0/9]  eta: 0:00:18  loss: 1.7117 (1.7117)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.0879 (2.0879 -- 2.0879)  data: 1.9159 (1.9159 -- 1.9159)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.8434 (1.8565)  acc1: 55.5556 (48.1013)  acc5: 88.8889 (92.4051)  time: 0.3786 (0.1325 -- 2.0879)  data: 0.2129 (0.0001 -- 1.9159)  max mem: 16413
Val: Total time: 0:00:03 (0.3787 s / it)
* Acc@1 51.266 Acc@5 91.139 loss 1.865
Accuracy of the network on the 158 val images: 51.27%
[2023-10-23 23:22:52,923] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:22:52,925] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:22:52,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:22:52,925] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:22:54,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:22:54,314] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 51.27%
Epoch: [14]  [ 0/52]  eta: 0:07:03  lr: 0.000047  min_lr: 0.000001  loss: 2.6261 (2.6261)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0853 (4.0853)  time: 8.1494 (8.1494 -- 8.1494)  data: 7.6364 (7.6364 -- 7.6364)  max mem: 16413
Epoch: [14]  [20/52]  eta: 0:00:43  lr: 0.000047  min_lr: 0.000001  loss: 2.3156 (2.3138)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9118 (3.9784)  time: 1.0232 (0.5150 -- 5.6998)  data: 0.4747 (0.0003 -- 5.1371)  max mem: 16413
[2023-10-23 23:23:36,921] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:23:36,922] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-10-23 23:23:36,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:23:36,923] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [14]  [40/52]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000001  loss: 2.3030 (2.3310)  loss_scale: 4096.0000 (4195.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0063 (4.0979)  time: 0.6999 (0.5219 -- 3.4165)  data: 0.1496 (0.0003 -- 2.8886)  max mem: 16413
Epoch: [14]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3587 (2.3148)  loss_scale: 8192.0000 (5041.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2541 (4.0742)  time: 0.6861 (0.4972 -- 3.4165)  data: 0.1584 (0.0002 -- 2.8886)  max mem: 16413
Epoch: [14] Total time: 0:00:48 (0.9307 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3587 (2.3166)  loss_scale: 8192.0000 (5041.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2541 (4.0742)
Val:  [0/9]  eta: 0:00:17  loss: 1.6211 (1.6211)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 1.9797 (1.9797 -- 1.9797)  data: 1.8040 (1.8040 -- 1.8040)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.7834 (1.8200)  acc1: 55.5556 (54.4304)  acc5: 100.0000 (93.6709)  time: 0.3662 (0.1324 -- 1.9797)  data: 0.2005 (0.0001 -- 1.8040)  max mem: 16413
Val: Total time: 0:00:03 (0.3664 s / it)
* Acc@1 53.797 Acc@5 94.304 loss 1.821
Accuracy of the network on the 158 val images: 53.80%
[2023-10-23 23:23:46,131] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:23:46,133] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:23:46,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:23:46,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:23:47,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:23:47,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 53.80%
Epoch: [15]  [ 0/52]  eta: 0:05:51  lr: 0.000047  min_lr: 0.000001  loss: 2.1586 (2.1586)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4016 (2.4016)  time: 6.7619 (6.7619 -- 6.7619)  data: 6.1889 (6.1889 -- 6.1889)  max mem: 16413
Epoch: [15]  [20/52]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000001  loss: 2.3235 (2.3022)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6587 (3.9983)  time: 0.9432 (0.5253 -- 2.1120)  data: 0.2239 (0.0005 -- 1.5682)  max mem: 16413
Epoch: [15]  [40/52]  eta: 0:00:12  lr: 0.000047  min_lr: 0.000001  loss: 2.2324 (2.2562)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9335 (4.5521)  time: 0.7689 (0.5234 -- 3.5130)  data: 0.2205 (0.0003 -- 2.9741)  max mem: 16413
Epoch: [15]  [51/52]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.3141 (2.2671)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6650 (4.5505)  time: 0.7169 (0.4938 -- 3.0317)  data: 0.1979 (0.0001 -- 2.5397)  max mem: 16413
Epoch: [15] Total time: 0:00:49 (0.9432 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.3141 (2.2978)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6650 (4.5505)
Val:  [0/9]  eta: 0:00:18  loss: 1.5879 (1.5879)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0934 (2.0934 -- 2.0934)  data: 1.9166 (1.9166 -- 1.9166)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.7642 (1.7810)  acc1: 57.1429 (56.9620)  acc5: 88.8889 (91.1392)  time: 0.3791 (0.1333 -- 2.0934)  data: 0.2130 (0.0001 -- 1.9166)  max mem: 16413
Val: Total time: 0:00:03 (0.3792 s / it)
* Acc@1 56.962 Acc@5 93.038 loss 1.773
Accuracy of the network on the 158 val images: 56.96%
[2023-10-23 23:24:39,978] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:24:39,979] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:24:39,980] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:24:39,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:24:41,432] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:24:41,432] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 56.96%
Epoch: [16]  [ 0/52]  eta: 0:08:05  lr: 0.000047  min_lr: 0.000001  loss: 2.1477 (2.1477)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0917 (6.0917)  time: 9.3312 (9.3312 -- 9.3312)  data: 8.8164 (8.8164 -- 8.8164)  max mem: 16413
Epoch: [16]  [20/52]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.1766 (2.2194)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4477 (4.7234)  time: 0.8326 (0.5116 -- 3.0370)  data: 0.1611 (0.0001 -- 2.4724)  max mem: 16413
Epoch: [16]  [40/52]  eta: 0:00:11  lr: 0.000046  min_lr: 0.000001  loss: 2.2752 (2.2778)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9144 (5.0515)  time: 0.7448 (0.5179 -- 3.0731)  data: 0.0017 (0.0003 -- 0.0049)  max mem: 16413
Epoch: [16]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2752 (2.2749)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6937 (5.0719)  time: 0.6968 (0.4939 -- 2.0169)  data: 0.0199 (0.0002 -- 0.3786)  max mem: 16413
Epoch: [16] Total time: 0:00:48 (0.9336 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2752 (2.2854)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6937 (5.0719)
Val:  [0/9]  eta: 0:00:17  loss: 1.5029 (1.5029)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 1.9761 (1.9761 -- 1.9761)  data: 1.7825 (1.7825 -- 1.7825)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.6740 (1.7310)  acc1: 55.5556 (56.9620)  acc5: 100.0000 (92.4051)  time: 0.3655 (0.1326 -- 1.9761)  data: 0.1981 (0.0001 -- 1.7825)  max mem: 16413
Val: Total time: 0:00:03 (0.3656 s / it)
* Acc@1 56.329 Acc@5 91.772 loss 1.736
Accuracy of the network on the 158 val images: 56.33%
Max accuracy: 56.96%
Epoch: [17]  [ 0/52]  eta: 0:06:58  lr: 0.000046  min_lr: 0.000001  loss: 2.0212 (2.0212)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1213 (4.1213)  time: 8.0435 (8.0435 -- 8.0435)  data: 7.4821 (7.4821 -- 7.4821)  max mem: 16413
[2023-10-23 23:25:50,292] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:25:50,292] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-10-23 23:25:50,294] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:25:50,295] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [17]  [20/52]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.1587 (2.2128)  loss_scale: 8192.0000 (11702.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0123 (4.4300)  time: 0.8013 (0.5275 -- 3.1413)  data: 0.2048 (0.0003 -- 2.6128)  max mem: 16413
Epoch: [17]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.2838 (2.2370)  loss_scale: 16384.0000 (13986.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1797 (4.8673)  time: 0.9240 (0.5266 -- 4.0737)  data: 0.3762 (0.0003 -- 3.5533)  max mem: 16413
Epoch: [17]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1996 (2.2201)  loss_scale: 16384.0000 (14493.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8878 (4.9222)  time: 0.7191 (0.4957 -- 3.8003)  data: 0.1972 (0.0001 -- 3.2071)  max mem: 16413
Epoch: [17] Total time: 0:00:48 (0.9250 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1996 (2.2324)  loss_scale: 16384.0000 (14493.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8878 (4.9222)
Val:  [0/9]  eta: 0:00:19  loss: 1.4854 (1.4854)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 2.1306 (2.1306 -- 2.1306)  data: 1.9532 (1.9532 -- 1.9532)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.6168 (1.6765)  acc1: 44.4444 (51.8987)  acc5: 88.8889 (91.1392)  time: 0.3830 (0.1331 -- 2.1306)  data: 0.2171 (0.0001 -- 1.9532)  max mem: 16413
Val: Total time: 0:00:03 (0.3831 s / it)
* Acc@1 55.696 Acc@5 93.038 loss 1.664
Accuracy of the network on the 158 val images: 55.70%
Max accuracy: 56.96%
Epoch: [18]  [ 0/52]  eta: 0:06:52  lr: 0.000046  min_lr: 0.000001  loss: 2.4599 (2.4599)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9458 (3.9458)  time: 7.9388 (7.9388 -- 7.9388)  data: 7.4081 (7.4081 -- 7.4081)  max mem: 16413
Epoch: [18]  [20/52]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 2.1828 (2.1772)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2075 (5.6475)  time: 0.8331 (0.5357 -- 3.5959)  data: 0.2002 (0.0003 -- 3.0492)  max mem: 16413
Epoch: [18]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.3236 (2.2267)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0471 (5.3148)  time: 0.8456 (0.5236 -- 2.9384)  data: 0.2474 (0.0003 -- 2.4092)  max mem: 16413
Epoch: [18]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2564 (2.2049)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9831 (5.4486)  time: 0.6857 (0.4959 -- 2.9384)  data: 0.1657 (0.0002 -- 2.4092)  max mem: 16413
Epoch: [18] Total time: 0:00:47 (0.9189 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2564 (2.2171)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9831 (5.4486)
Val:  [0/9]  eta: 0:00:18  loss: 1.4580 (1.4580)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 2.0701 (2.0701 -- 2.0701)  data: 1.8951 (1.8951 -- 1.8951)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.5961 (1.6505)  acc1: 55.5556 (54.4304)  acc5: 88.8889 (91.1392)  time: 0.3765 (0.1329 -- 2.0701)  data: 0.2106 (0.0001 -- 1.8951)  max mem: 16413
Val: Total time: 0:00:03 (0.3766 s / it)
* Acc@1 55.696 Acc@5 93.038 loss 1.637
Accuracy of the network on the 158 val images: 55.70%
Max accuracy: 56.96%
Epoch: [19]  [ 0/52]  eta: 0:06:38  lr: 0.000046  min_lr: 0.000001  loss: 2.3607 (2.3607)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7524 (4.7524)  time: 7.6577 (7.6577 -- 7.6577)  data: 5.2776 (5.2776 -- 5.2776)  max mem: 16413
[2023-10-23 23:27:32,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.099105386073283e-06, 1.099105386073283e-06, 1.4654738480977107e-06, 1.4654738480977107e-06, 1.9539651307969477e-06, 1.9539651307969477e-06, 2.605286841062597e-06, 2.605286841062597e-06, 3.4737157880834624e-06, 3.4737157880834624e-06, 4.63162105077795e-06, 4.63162105077795e-06, 6.1754947343706e-06, 6.1754947343706e-06, 8.2339929791608e-06, 8.2339929791608e-06, 1.0978657305547733e-05, 1.0978657305547733e-05, 1.4638209740730312e-05, 1.4638209740730312e-05, 1.9517612987640414e-05, 1.9517612987640414e-05, 2.6023483983520553e-05, 2.6023483983520553e-05, 3.4697978644694075e-05, 3.4697978644694075e-05, 4.626397152625876e-05, 4.626397152625876e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 23:27:32,471] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=19.7084881803857, CurrSamplesPerSec=21.543223288005443, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [19]  [20/52]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.2308 (2.2498)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9044 (5.9096)  time: 0.8304 (0.5312 -- 3.5494)  data: 0.1294 (0.0004 -- 1.3921)  max mem: 16413
[2023-10-23 23:27:55,873] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:27:55,873] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:27:55,874] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-10-23 23:27:55,874] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [19]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.1722 (2.2126)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8671 (5.4746)  time: 0.9286 (0.5190 -- 4.4530)  data: 0.0020 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [19]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2405 (2.2325)  loss_scale: 32768.0000 (21425.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0916 (5.4714)  time: 0.7618 (0.4958 -- 4.4530)  data: 0.0011 (0.0001 -- 0.0048)  max mem: 16413
Epoch: [19] Total time: 0:00:48 (0.9306 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2405 (2.2109)  loss_scale: 32768.0000 (21425.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0916 (5.4714)
[2023-10-23 23:28:04,509] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-10-23 23:28:04,511] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-19/mp_rank_00_model_states.pt
[2023-10-23 23:28:04,511] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-19/mp_rank_00_model_states.pt...
[2023-10-23 23:28:04,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-10-23 23:28:05,553] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-19/mp_rank_00_model_states.pt.
[2023-10-23 23:28:05,554] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [0/9]  eta: 0:00:17  loss: 1.4455 (1.4455)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 1.9877 (1.9877 -- 1.9877)  data: 1.8029 (1.8029 -- 1.8029)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.5718 (1.5815)  acc1: 44.4444 (53.1646)  acc5: 88.8889 (92.4051)  time: 0.3687 (0.1332 -- 1.9877)  data: 0.2004 (0.0001 -- 1.8029)  max mem: 16413
Val: Total time: 0:00:03 (0.3689 s / it)
* Acc@1 56.962 Acc@5 93.671 loss 1.579
Accuracy of the network on the 158 val images: 56.96%
[2023-10-23 23:28:08,875] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:28:08,877] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:28:08,877] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:28:08,877] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:28:10,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:28:10,315] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 56.96%
Epoch: [20]  [ 0/52]  eta: 0:06:25  lr: 0.000046  min_lr: 0.000001  loss: 2.1344 (2.1344)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0998 (6.0998)  time: 7.4216 (7.4216 -- 7.4216)  data: 6.8998 (6.8998 -- 6.8998)  max mem: 16413
Epoch: [20]  [20/52]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 2.2128 (2.1827)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3606 (5.6623)  time: 0.8490 (0.5293 -- 4.1775)  data: 0.3052 (0.0003 -- 3.6393)  max mem: 16413
Epoch: [20]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.1749 (2.1635)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0581 (5.4118)  time: 0.8724 (0.5322 -- 3.4235)  data: 0.3270 (0.0002 -- 2.8936)  max mem: 16413
Epoch: [20]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1333 (2.1461)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4924 (5.3982)  time: 0.7513 (0.4945 -- 3.3536)  data: 0.2295 (0.0001 -- 2.8299)  max mem: 16413
Epoch: [20] Total time: 0:00:49 (0.9452 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1333 (2.1666)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4924 (5.3982)
Val:  [0/9]  eta: 0:00:18  loss: 1.4106 (1.4106)  acc1: 44.4444 (44.4444)  acc5: 88.8889 (88.8889)  time: 2.0997 (2.0997 -- 2.0997)  data: 1.9297 (1.9297 -- 1.9297)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.5120 (1.5385)  acc1: 44.4444 (54.4304)  acc5: 88.8889 (92.4051)  time: 0.3788 (0.1318 -- 2.0997)  data: 0.2145 (0.0001 -- 1.9297)  max mem: 16413
Val: Total time: 0:00:03 (0.3789 s / it)
* Acc@1 60.127 Acc@5 92.405 loss 1.539
Accuracy of the network on the 158 val images: 60.13%
[2023-10-23 23:29:02,882] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:29:02,883] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:29:02,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:29:02,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:29:04,339] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:29:04,340] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 60.13%
Epoch: [21]  [ 0/52]  eta: 0:05:40  lr: 0.000046  min_lr: 0.000001  loss: 1.8694 (1.8694)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0965 (5.0965)  time: 6.5502 (6.5502 -- 6.5502)  data: 5.8965 (5.8965 -- 5.8965)  max mem: 16413
Epoch: [21]  [20/52]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 2.1530 (2.1717)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8538 (5.2077)  time: 0.8921 (0.5325 -- 3.6451)  data: 0.3021 (0.0008 -- 3.1232)  max mem: 16413
Epoch: [21]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.3861 (2.2548)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5435 (5.9811)  time: 0.9561 (0.5132 -- 4.4709)  data: 0.2703 (0.0003 -- 2.4491)  max mem: 16413
Epoch: [21]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.3376 (2.2667)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5009 (6.0448)  time: 0.7799 (0.4942 -- 4.4709)  data: 0.1815 (0.0001 -- 2.4491)  max mem: 16413
Epoch: [21] Total time: 0:00:49 (0.9484 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.3376 (2.2050)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5009 (6.0448)
Val:  [0/9]  eta: 0:00:18  loss: 1.3515 (1.3515)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.0660 (2.0660 -- 2.0660)  data: 1.8901 (1.8901 -- 1.8901)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.4560 (1.4868)  acc1: 66.6667 (62.0253)  acc5: 88.8889 (92.4051)  time: 0.3748 (0.1319 -- 2.0660)  data: 0.2101 (0.0001 -- 1.8901)  max mem: 16413
Val: Total time: 0:00:03 (0.3750 s / it)
* Acc@1 64.557 Acc@5 92.405 loss 1.495
Accuracy of the network on the 158 val images: 64.56%
[2023-10-23 23:29:57,037] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:29:57,038] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:29:57,038] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:29:57,038] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:29:58,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:29:58,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.56%
Epoch: [22]  [ 0/52]  eta: 0:07:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1903 (2.1903)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4840 (6.4840)  time: 8.0943 (8.0943 -- 8.0943)  data: 5.2422 (5.2422 -- 5.2422)  max mem: 16413
[2023-10-23 23:30:13,309] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:30:13,309] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-10-23 23:30:13,311] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:30:13,311] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-10-23 23:30:17,074] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1159
[2023-10-23 23:30:17,074] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1159
[2023-10-23 23:30:17,074] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-10-23 23:30:17,074] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-10-23 23:30:17,074] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
Epoch: [22]  [20/52]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000001  loss: 2.1387 (2.0871)  loss_scale: 32768.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3710 (6.3964)  time: 0.9191 (0.5166 -- 3.3729)  data: 0.0015 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [22]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.2408 (2.1651)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9590 (6.0860)  time: 0.8692 (0.5033 -- 4.0914)  data: 0.0009 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [22]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2578 (2.1906)  loss_scale: 32768.0000 (37179.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4776 (5.9744)  time: 0.7493 (0.4939 -- 4.0914)  data: 0.0006 (0.0001 -- 0.0015)  max mem: 16413
Epoch: [22] Total time: 0:00:50 (0.9731 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2578 (2.1878)  loss_scale: 32768.0000 (37179.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4776 (5.9744)
Val:  [0/9]  eta: 0:00:18  loss: 1.3689 (1.3689)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.0300 (2.0300 -- 2.0300)  data: 1.8594 (1.8594 -- 1.8594)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.4233 (1.4996)  acc1: 55.5556 (56.9620)  acc5: 88.8889 (93.6709)  time: 0.3728 (0.1334 -- 2.0300)  data: 0.2067 (0.0001 -- 1.8594)  max mem: 16413
Val: Total time: 0:00:03 (0.3730 s / it)
* Acc@1 61.392 Acc@5 94.304 loss 1.487
Accuracy of the network on the 158 val images: 61.39%
Max accuracy: 64.56%
Epoch: [23]  [ 0/52]  eta: 0:05:16  lr: 0.000046  min_lr: 0.000001  loss: 2.1382 (2.1382)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3878 (4.3878)  time: 6.0905 (6.0905 -- 6.0905)  data: 5.5524 (5.5524 -- 5.5524)  max mem: 16413
Epoch: [23]  [20/52]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.0176 (2.0786)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5987 (6.3018)  time: 1.0043 (0.5292 -- 3.7970)  data: 0.0017 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [23]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.1038 (2.1023)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1262 (6.3051)  time: 0.7950 (0.5168 -- 2.2059)  data: 0.0012 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [23]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1038 (2.1230)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0131 (6.0449)  time: 0.7011 (0.4962 -- 2.2059)  data: 0.0006 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [23] Total time: 0:00:48 (0.9235 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1038 (2.1714)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0131 (6.0449)
Val:  [0/9]  eta: 0:00:17  loss: 1.3372 (1.3372)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.9678 (1.9678 -- 1.9678)  data: 1.7823 (1.7823 -- 1.7823)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.4496 (1.4804)  acc1: 55.5556 (56.9620)  acc5: 88.8889 (92.4051)  time: 0.3651 (0.1328 -- 1.9678)  data: 0.1981 (0.0001 -- 1.7823)  max mem: 16413
Val: Total time: 0:00:03 (0.3653 s / it)
* Acc@1 62.025 Acc@5 93.038 loss 1.472
Accuracy of the network on the 158 val images: 62.03%
Max accuracy: 64.56%
Epoch: [24]  [ 0/52]  eta: 0:06:20  lr: 0.000046  min_lr: 0.000001  loss: 1.8078 (1.8078)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9344 (4.9344)  time: 7.3091 (7.3091 -- 7.3091)  data: 6.7631 (6.7631 -- 6.7631)  max mem: 16413
Epoch: [24]  [20/52]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 2.1452 (2.1630)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2995 (6.4603)  time: 0.8694 (0.5296 -- 2.4697)  data: 0.2516 (0.0004 -- 1.9496)  max mem: 16413
[2023-10-23 23:32:25,335] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:32:25,335] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-10-23 23:32:25,335] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:32:25,335] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [24]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.2472 (2.1912)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0252 (6.3918)  time: 0.8557 (0.5273 -- 2.6732)  data: 0.2379 (0.0003 -- 2.1524)  max mem: 16413
[2023-10-23 23:32:26,974] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1289
[2023-10-23 23:32:26,974] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-10-23 23:32:26,974] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1289
[2023-10-23 23:32:26,974] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-10-23 23:32:26,974] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2648 (2.2136)  loss_scale: 32768.0000 (33398.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8226 (6.3586)  time: 0.7375 (0.4934 -- 2.6732)  data: 0.1883 (0.0002 -- 2.1524)  max mem: 16413
Epoch: [24] Total time: 0:00:48 (0.9401 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2648 (2.1818)  loss_scale: 32768.0000 (33398.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8226 (6.3586)
Val:  [0/9]  eta: 0:00:19  loss: 1.3053 (1.3053)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.1160 (2.1160 -- 2.1160)  data: 1.9433 (1.9433 -- 1.9433)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.3892 (1.4337)  acc1: 66.6667 (59.4937)  acc5: 88.8889 (92.4051)  time: 0.3815 (0.1323 -- 2.1160)  data: 0.2160 (0.0001 -- 1.9433)  max mem: 16413
Val: Total time: 0:00:03 (0.3816 s / it)
* Acc@1 64.557 Acc@5 93.671 loss 1.423
Accuracy of the network on the 158 val images: 64.56%
[2023-10-23 23:32:35,854] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:32:35,856] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:32:35,856] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:32:35,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:32:37,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:32:37,315] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.56%
Epoch: [25]  [ 0/52]  eta: 0:07:16  lr: 0.000046  min_lr: 0.000001  loss: 2.3407 (2.3407)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5456 (6.5456)  time: 8.3894 (8.3894 -- 8.3894)  data: 5.2403 (5.2403 -- 5.2403)  max mem: 16413
Epoch: [25]  [20/52]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000001  loss: 2.0531 (2.0807)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8110 (6.8694)  time: 0.8687 (0.5194 -- 2.8379)  data: 0.0900 (0.0003 -- 1.3730)  max mem: 16413
Epoch: [25]  [40/52]  eta: 0:00:12  lr: 0.000046  min_lr: 0.000001  loss: 2.1226 (2.0966)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5405 (6.5274)  time: 0.8951 (0.5031 -- 2.9343)  data: 0.0415 (0.0002 -- 0.8140)  max mem: 16413
Epoch: [25]  [51/52]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.2345 (2.1255)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5488 (6.4221)  time: 0.8083 (0.4971 -- 2.6411)  data: 0.0006 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [25] Total time: 0:00:51 (0.9835 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.2345 (2.1283)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5488 (6.4221)
Val:  [0/9]  eta: 0:00:17  loss: 1.2974 (1.2974)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 1.9658 (1.9658 -- 1.9658)  data: 1.7843 (1.7843 -- 1.7843)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.3512 (1.4233)  acc1: 55.5556 (58.2278)  acc5: 88.8889 (93.6709)  time: 0.3652 (0.1334 -- 1.9658)  data: 0.1983 (0.0001 -- 1.7843)  max mem: 16413
Val: Total time: 0:00:03 (0.3653 s / it)
* Acc@1 60.759 Acc@5 94.937 loss 1.396
Accuracy of the network on the 158 val images: 60.76%
Max accuracy: 64.56%
Epoch: [26]  [ 0/52]  eta: 0:06:01  lr: 0.000046  min_lr: 0.000001  loss: 1.9790 (1.9790)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9330 (7.9330)  time: 6.9464 (6.9464 -- 6.9464)  data: 6.2597 (6.2597 -- 6.2597)  max mem: 16413
Epoch: [26]  [20/52]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000001  loss: 2.2396 (2.2273)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1006 (6.6011)  time: 0.9061 (0.5286 -- 4.6761)  data: 0.3582 (0.0003 -- 4.1443)  max mem: 16413
Epoch: [26]  [40/52]  eta: 0:00:12  lr: 0.000045  min_lr: 0.000001  loss: 2.1553 (2.1826)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6559 (6.3585)  time: 0.8375 (0.5305 -- 2.7411)  data: 0.2324 (0.0003 -- 1.7475)  max mem: 16413
Epoch: [26]  [51/52]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.2227 (2.1812)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7765 (6.5588)  time: 0.7912 (0.4954 -- 2.7411)  data: 0.1445 (0.0001 -- 1.4560)  max mem: 16413
Epoch: [26] Total time: 0:00:48 (0.9368 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.2227 (2.1403)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7765 (6.5588)
Val:  [0/9]  eta: 0:00:19  loss: 1.2672 (1.2672)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.1229 (2.1229 -- 2.1229)  data: 1.9466 (1.9466 -- 1.9466)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.3531 (1.4011)  acc1: 55.5556 (62.0253)  acc5: 88.8889 (91.1392)  time: 0.3832 (0.1326 -- 2.1229)  data: 0.2164 (0.0001 -- 1.9466)  max mem: 16413
Val: Total time: 0:00:03 (0.3833 s / it)
* Acc@1 64.557 Acc@5 93.038 loss 1.377
Accuracy of the network on the 158 val images: 64.56%
Max accuracy: 64.56%
[2023-10-23 23:34:32,236] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1404
[2023-10-23 23:34:32,236] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1404
[2023-10-23 23:34:32,237] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:34:32,237] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:34:32,237] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [ 0/52]  eta: 0:07:05  lr: 0.000045  min_lr: 0.000001  loss: 2.6531 (2.6531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6394 (7.6394)  time: 8.1787 (8.1787 -- 8.1787)  data: 7.2662 (7.2662 -- 7.2662)  max mem: 16413
Epoch: [27]  [20/52]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000001  loss: 2.1034 (2.1657)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7797 (6.2672)  time: 0.8173 (0.5270 -- 4.7595)  data: 0.2383 (0.0002 -- 4.1711)  max mem: 16413
Epoch: [27]  [40/52]  eta: 0:00:12  lr: 0.000045  min_lr: 0.000001  loss: 2.1099 (2.1219)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5239 (6.1563)  time: 0.9285 (0.5182 -- 3.4610)  data: 0.3820 (0.0004 -- 2.9282)  max mem: 16413
Epoch: [27]  [51/52]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0742 (2.1175)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0195 (6.2595)  time: 0.8092 (0.4966 -- 3.4610)  data: 0.2871 (0.0002 -- 2.9282)  max mem: 16413
Epoch: [27] Total time: 0:00:48 (0.9400 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0742 (2.1439)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0195 (6.2595)
Val:  [0/9]  eta: 0:00:18  loss: 1.2597 (1.2597)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.0519 (2.0519 -- 2.0519)  data: 1.8599 (1.8599 -- 1.8599)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.2957 (1.3757)  acc1: 55.5556 (62.0253)  acc5: 88.8889 (92.4051)  time: 0.3740 (0.1323 -- 2.0519)  data: 0.2067 (0.0001 -- 1.8599)  max mem: 16413
Val: Total time: 0:00:03 (0.3741 s / it)
* Acc@1 66.456 Acc@5 93.671 loss 1.344
Accuracy of the network on the 158 val images: 66.46%
[2023-10-23 23:35:16,371] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:35:16,373] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:35:16,373] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:35:16,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:35:17,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:35:17,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.46%
Epoch: [28]  [ 0/52]  eta: 0:07:12  lr: 0.000045  min_lr: 0.000001  loss: 2.4195 (2.4195)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1424 (8.1424)  time: 8.3221 (8.3221 -- 8.3221)  data: 7.8014 (7.8014 -- 7.8014)  max mem: 16413
Epoch: [28]  [20/52]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.1331 (2.1046)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7586 (6.4522)  time: 0.7898 (0.5205 -- 3.3369)  data: 0.2462 (0.0003 -- 2.8149)  max mem: 16413
Epoch: [28]  [40/52]  eta: 0:00:12  lr: 0.000045  min_lr: 0.000001  loss: 1.9516 (2.0443)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7523 (6.5682)  time: 0.9045 (0.5084 -- 4.3504)  data: 0.3690 (0.0003 -- 3.8428)  max mem: 16413
Epoch: [28]  [51/52]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0783 (2.0673)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5968 (6.5986)  time: 0.7122 (0.4938 -- 2.8609)  data: 0.2002 (0.0001 -- 2.3187)  max mem: 16413
Epoch: [28] Total time: 0:00:48 (0.9280 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0783 (2.0667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5968 (6.5986)
Val:  [0/9]  eta: 0:00:18  loss: 1.1987 (1.1987)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.1027 (2.1027 -- 2.1027)  data: 1.9266 (1.9266 -- 1.9266)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.2389 (1.3443)  acc1: 57.1429 (62.0253)  acc5: 88.8889 (93.6709)  time: 0.3814 (0.1322 -- 2.1027)  data: 0.2141 (0.0001 -- 1.9266)  max mem: 16413
Val: Total time: 0:00:03 (0.3815 s / it)
* Acc@1 67.722 Acc@5 93.671 loss 1.320
Accuracy of the network on the 158 val images: 67.72%
[2023-10-23 23:36:09,228] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:36:09,229] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:36:09,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:36:09,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:36:10,647] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:36:10,648] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.72%
Epoch: [29]  [ 0/52]  eta: 0:05:39  lr: 0.000045  min_lr: 0.000001  loss: 1.6278 (1.6278)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0906 (8.0906)  time: 6.5302 (6.5302 -- 6.5302)  data: 4.9724 (4.9724 -- 4.9724)  max mem: 16413
Epoch: [29]  [20/52]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.1352 (2.0998)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9279 (6.7436)  time: 0.8866 (0.5243 -- 3.1472)  data: 0.1334 (0.0007 -- 1.2642)  max mem: 16413
[2023-10-23 23:36:41,497] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:36:41,497] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:36:41,499] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:36:41,500] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [40/52]  eta: 0:00:12  lr: 0.000045  min_lr: 0.000001  loss: 2.1156 (2.0939)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2700 (6.9112)  time: 0.8922 (0.5172 -- 4.3735)  data: 0.0018 (0.0002 -- 0.0073)  max mem: 16413
Epoch: [29]  [51/52]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0871 (2.1197)  loss_scale: 32768.0000 (24891.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7522 (7.0221)  time: 0.6986 (0.4995 -- 3.6155)  data: 0.0150 (0.0001 -- 0.2840)  max mem: 16413
Epoch: [29] Total time: 0:00:48 (0.9264 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0871 (2.1305)  loss_scale: 32768.0000 (24891.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7522 (7.0221)
Val:  [0/9]  eta: 0:00:18  loss: 1.2022 (1.2022)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.0426 (2.0426 -- 2.0426)  data: 1.8695 (1.8695 -- 1.8695)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.2174 (1.3187)  acc1: 55.5556 (63.2911)  acc5: 88.8889 (92.4051)  time: 0.3732 (0.1333 -- 2.0426)  data: 0.2078 (0.0001 -- 1.8695)  max mem: 16413
Val: Total time: 0:00:03 (0.3733 s / it)
* Acc@1 65.823 Acc@5 92.405 loss 1.311
Accuracy of the network on the 158 val images: 65.82%
Max accuracy: 67.72%
Epoch: [30]  [ 0/52]  eta: 0:05:21  lr: 0.000045  min_lr: 0.000001  loss: 1.2064 (1.2064)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5870 (8.5870)  time: 6.1786 (6.1786 -- 6.1786)  data: 5.1186 (5.1186 -- 5.1186)  max mem: 16413
Epoch: [30]  [20/52]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000001  loss: 2.0969 (2.0025)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6460 (6.5253)  time: 0.9659 (0.5348 -- 2.9638)  data: 0.4162 (0.0007 -- 2.3973)  max mem: 16413
[2023-10-23 23:37:35,123] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1587
[2023-10-23 23:37:35,123] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1587
[2023-10-23 23:37:35,123] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:37:35,123] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:37:35,123] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 23:37:36,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1590
[2023-10-23 23:37:36,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1590
[2023-10-23 23:37:36,690] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 23:37:36,690] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-23 23:37:36,691] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [30]  [40/52]  eta: 0:00:12  lr: 0.000045  min_lr: 0.000001  loss: 2.1628 (2.0757)  loss_scale: 8192.0000 (24975.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1248 (6.9318)  time: 0.7761 (0.4919 -- 4.1310)  data: 0.2335 (0.0003 -- 3.6002)  max mem: 16413
Epoch: [30]  [51/52]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.1486 (2.0978)  loss_scale: 8192.0000 (21425.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6588 (6.9216)  time: 0.6629 (0.4965 -- 2.2201)  data: 0.1386 (0.0003 -- 1.7112)  max mem: 16413
Epoch: [30] Total time: 0:00:48 (0.9289 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.1486 (2.0915)  loss_scale: 8192.0000 (21425.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6588 (6.9216)
Val:  [0/9]  eta: 0:00:18  loss: 1.1310 (1.1310)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0920 (2.0920 -- 2.0920)  data: 1.9179 (1.9179 -- 1.9179)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.2854 (1.3350)  acc1: 66.6667 (63.2911)  acc5: 88.8889 (91.1392)  time: 0.3802 (0.1330 -- 2.0920)  data: 0.2132 (0.0001 -- 1.9179)  max mem: 16413
Val: Total time: 0:00:03 (0.3803 s / it)
* Acc@1 65.190 Acc@5 93.038 loss 1.309
Accuracy of the network on the 158 val images: 65.19%
Max accuracy: 67.72%
Epoch: [31]  [ 0/52]  eta: 0:06:13  lr: 0.000045  min_lr: 0.000001  loss: 1.9333 (1.9333)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7546 (6.7546)  time: 7.1867 (7.1867 -- 7.1867)  data: 6.6081 (6.6081 -- 6.6081)  max mem: 16413
Epoch: [31]  [20/52]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000001  loss: 2.1553 (2.0904)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5620 (5.7092)  time: 0.8634 (0.5121 -- 2.6778)  data: 0.1018 (0.0008 -- 1.2167)  max mem: 16413
Epoch: [31]  [40/52]  eta: 0:00:12  lr: 0.000045  min_lr: 0.000001  loss: 1.9859 (2.0557)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6990 (6.2054)  time: 0.9095 (0.5197 -- 3.9924)  data: 0.3212 (0.0004 -- 3.4723)  max mem: 16413
Epoch: [31]  [51/52]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.8895 (2.0447)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7074 (6.4228)  time: 0.8212 (0.4942 -- 3.9924)  data: 0.3055 (0.0001 -- 3.4723)  max mem: 16413
Epoch: [31] Total time: 0:00:49 (0.9562 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.8895 (2.0599)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7074 (6.4228)
Val:  [0/9]  eta: 0:00:19  loss: 1.1616 (1.1616)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.1416 (2.1416 -- 2.1416)  data: 1.9688 (1.9688 -- 1.9688)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.2041 (1.3010)  acc1: 66.6667 (68.3544)  acc5: 88.8889 (93.6709)  time: 0.3843 (0.1347 -- 2.1416)  data: 0.2188 (0.0001 -- 1.9688)  max mem: 16413
Val: Total time: 0:00:03 (0.3844 s / it)
* Acc@1 67.089 Acc@5 94.937 loss 1.279
Accuracy of the network on the 158 val images: 67.09%
Max accuracy: 67.72%
Epoch: [32]  [ 0/52]  eta: 0:06:42  lr: 0.000045  min_lr: 0.000001  loss: 1.9065 (1.9065)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4655 (7.4655)  time: 7.7406 (7.7406 -- 7.7406)  data: 7.1817 (7.1817 -- 7.1817)  max mem: 16413
Epoch: [32]  [20/52]  eta: 0:00:34  lr: 0.000045  min_lr: 0.000001  loss: 2.1438 (2.0142)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6459 (7.0153)  time: 0.7604 (0.5410 -- 2.5848)  data: 0.1578 (0.0004 -- 2.0524)  max mem: 16413
Epoch: [32]  [40/52]  eta: 0:00:11  lr: 0.000045  min_lr: 0.000001  loss: 2.1361 (2.0487)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9030 (7.0437)  time: 0.8901 (0.5210 -- 2.9652)  data: 0.0124 (0.0004 -- 0.2107)  max mem: 16413
Epoch: [32]  [51/52]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9451 (2.0258)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3114 (6.8523)  time: 0.6472 (0.4938 -- 1.3377)  data: 0.0012 (0.0002 -- 0.0042)  max mem: 16413
Epoch: [32] Total time: 0:00:47 (0.9187 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9451 (2.0800)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3114 (6.8523)
Val:  [0/9]  eta: 0:00:19  loss: 1.1450 (1.1450)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.1279 (2.1279 -- 2.1279)  data: 1.9434 (1.9434 -- 1.9434)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.2910 (1.2727)  acc1: 66.6667 (65.8228)  acc5: 88.8889 (92.4051)  time: 0.3831 (0.1327 -- 2.1279)  data: 0.2160 (0.0001 -- 1.9434)  max mem: 16413
Val: Total time: 0:00:03 (0.3833 s / it)
* Acc@1 67.722 Acc@5 93.671 loss 1.246
Accuracy of the network on the 158 val images: 67.72%
Max accuracy: 67.72%
Epoch: [33]  [ 0/52]  eta: 0:05:08  lr: 0.000045  min_lr: 0.000001  loss: 2.1841 (2.1841)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5543 (5.5543)  time: 5.9354 (5.9354 -- 5.9354)  data: 5.1857 (5.1857 -- 5.1857)  max mem: 16413
[2023-10-23 23:39:47,356] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:39:47,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-23 23:39:47,359] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:39:47,359] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [33]  [20/52]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 2.0194 (2.0129)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8382 (6.7868)  time: 0.8881 (0.5294 -- 2.1286)  data: 0.0123 (0.0003 -- 0.2185)  max mem: 16413
Epoch: [33]  [40/52]  eta: 0:00:12  lr: 0.000044  min_lr: 0.000001  loss: 1.8974 (1.9921)  loss_scale: 16384.0000 (15784.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7118 (6.9152)  time: 0.8663 (0.5219 -- 3.1333)  data: 0.0074 (0.0003 -- 0.1180)  max mem: 16413
Epoch: [33]  [51/52]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8842 (1.9940)  loss_scale: 16384.0000 (15911.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7118 (6.8969)  time: 0.7720 (0.4957 -- 3.1333)  data: 0.0067 (0.0002 -- 0.1180)  max mem: 16413
Epoch: [33] Total time: 0:00:47 (0.9137 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.8842 (1.9976)  loss_scale: 16384.0000 (15911.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7118 (6.8969)
Val:  [0/9]  eta: 0:00:19  loss: 1.1865 (1.1865)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.1526 (2.1526 -- 2.1526)  data: 1.9769 (1.9769 -- 1.9769)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.2025 (1.2500)  acc1: 55.5556 (62.0253)  acc5: 88.8889 (93.6709)  time: 0.3858 (0.1330 -- 2.1526)  data: 0.2197 (0.0001 -- 1.9769)  max mem: 16413
Val: Total time: 0:00:03 (0.3859 s / it)
* Acc@1 65.190 Acc@5 95.570 loss 1.233
Accuracy of the network on the 158 val images: 65.19%
Max accuracy: 67.72%
Epoch: [34]  [ 0/52]  eta: 0:07:32  lr: 0.000044  min_lr: 0.000001  loss: 2.3961 (2.3961)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1632 (8.1632)  time: 8.6927 (8.6927 -- 8.6927)  data: 8.1487 (8.1487 -- 8.1487)  max mem: 16413
Epoch: [34]  [20/52]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000001  loss: 2.0265 (2.0710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9489 (7.4449)  time: 0.8266 (0.5160 -- 3.9381)  data: 0.2808 (0.0003 -- 3.3915)  max mem: 16413
Epoch: [34]  [40/52]  eta: 0:00:12  lr: 0.000044  min_lr: 0.000001  loss: 2.1157 (2.0920)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9543 (7.5964)  time: 0.8626 (0.5273 -- 3.0681)  data: 0.2778 (0.0004 -- 2.5299)  max mem: 16413
Epoch: [34]  [51/52]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.1143 (2.0929)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1853 (7.6595)  time: 0.7533 (0.4984 -- 2.0647)  data: 0.1952 (0.0001 -- 1.5305)  max mem: 16413
Epoch: [34] Total time: 0:00:48 (0.9404 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.1143 (2.0693)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1853 (7.6595)
Val:  [0/9]  eta: 0:00:18  loss: 1.1590 (1.1590)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.0255 (2.0255 -- 2.0255)  data: 1.8426 (1.8426 -- 1.8426)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.1590 (1.2365)  acc1: 66.6667 (65.8228)  acc5: 88.8889 (93.6709)  time: 0.3731 (0.1334 -- 2.0255)  data: 0.2048 (0.0001 -- 1.8426)  max mem: 16413
Val: Total time: 0:00:03 (0.3732 s / it)
* Acc@1 68.354 Acc@5 94.937 loss 1.205
Accuracy of the network on the 158 val images: 68.35%
[2023-10-23 23:41:21,638] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:41:21,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:41:21,640] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:41:21,640] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:41:23,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:41:23,027] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.35%
Epoch: [35]  [ 0/52]  eta: 0:06:35  lr: 0.000044  min_lr: 0.000001  loss: 2.4042 (2.4042)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4709 (6.4709)  time: 7.5998 (7.5998 -- 7.5998)  data: 4.2145 (4.2145 -- 4.2145)  max mem: 16413
Epoch: [35]  [20/52]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000001  loss: 2.0812 (2.1709)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1560 (7.2833)  time: 0.8953 (0.5216 -- 3.8636)  data: 0.0022 (0.0003 -- 0.0118)  max mem: 16413
[2023-10-23 23:41:54,113] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:41:54,113] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:41:54,113] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:41:54,113] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [40/52]  eta: 0:00:11  lr: 0.000044  min_lr: 0.000001  loss: 1.8285 (2.0532)  loss_scale: 32768.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1702 (7.1684)  time: 0.7534 (0.5357 -- 2.1575)  data: 0.0549 (0.0004 -- 0.8492)  max mem: 16413
Epoch: [35]  [51/52]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.9940 (2.0467)  loss_scale: 32768.0000 (24260.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2148 (7.2165)  time: 0.7442 (0.4943 -- 1.7818)  data: 0.0499 (0.0001 -- 0.8492)  max mem: 16413
Epoch: [35] Total time: 0:00:48 (0.9268 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.9940 (2.0082)  loss_scale: 32768.0000 (24260.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2148 (7.2165)
Val:  [0/9]  eta: 0:00:19  loss: 1.0953 (1.0953)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.1202 (2.1202 -- 2.1202)  data: 1.9411 (1.9411 -- 1.9411)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.1093 (1.2024)  acc1: 66.6667 (67.0886)  acc5: 88.8889 (93.6709)  time: 0.3825 (0.1325 -- 2.1202)  data: 0.2157 (0.0001 -- 1.9411)  max mem: 16413
Val: Total time: 0:00:03 (0.3826 s / it)
* Acc@1 68.987 Acc@5 94.937 loss 1.161
Accuracy of the network on the 158 val images: 68.99%
[2023-10-23 23:42:14,671] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:42:14,672] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:42:14,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:42:14,672] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:42:15,958] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:42:15,959] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.99%
Epoch: [36]  [ 0/52]  eta: 0:06:04  lr: 0.000044  min_lr: 0.000001  loss: 1.9996 (1.9996)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1782 (7.1782)  time: 7.0053 (7.0053 -- 7.0053)  data: 5.5755 (5.5755 -- 5.5755)  max mem: 16413
Epoch: [36]  [20/52]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 2.0091 (2.0038)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7706 (6.7822)  time: 0.8387 (0.5306 -- 4.2793)  data: 0.0216 (0.0002 -- 0.3930)  max mem: 16413
[2023-10-23 23:42:44,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1899
[2023-10-23 23:42:44,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1899
[2023-10-23 23:42:44,506] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:42:44,506] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:42:44,506] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [36]  [40/52]  eta: 0:00:12  lr: 0.000044  min_lr: 0.000001  loss: 2.0773 (2.0314)  loss_scale: 16384.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7622 (6.9271)  time: 0.8682 (0.5198 -- 2.8068)  data: 0.1337 (0.0002 -- 1.6187)  max mem: 16413
Epoch: [36]  [51/52]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.0439 (2.0201)  loss_scale: 16384.0000 (24891.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4942 (7.0479)  time: 0.6823 (0.4954 -- 2.1668)  data: 0.0818 (0.0001 -- 1.6187)  max mem: 16413
Epoch: [36] Total time: 0:00:47 (0.9167 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.0439 (2.0266)  loss_scale: 16384.0000 (24891.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4942 (7.0479)
Val:  [0/9]  eta: 0:00:19  loss: 1.1122 (1.1122)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.1719 (2.1719 -- 2.1719)  data: 2.0002 (2.0002 -- 2.0002)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.1564 (1.2204)  acc1: 66.6667 (65.8228)  acc5: 88.8889 (93.6709)  time: 0.3876 (0.1336 -- 2.1719)  data: 0.2223 (0.0001 -- 2.0002)  max mem: 16413
Val: Total time: 0:00:03 (0.3877 s / it)
* Acc@1 68.987 Acc@5 94.937 loss 1.175
Accuracy of the network on the 158 val images: 68.99%
[2023-10-23 23:43:07,123] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:43:07,125] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:43:07,125] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:43:07,125] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:43:08,238] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:43:08,239] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.99%
Epoch: [37]  [ 0/52]  eta: 0:06:14  lr: 0.000044  min_lr: 0.000001  loss: 1.8850 (1.8850)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4424 (5.4424)  time: 7.1985 (7.1985 -- 7.1985)  data: 5.3775 (5.3775 -- 5.3775)  max mem: 16413
Epoch: [37]  [20/52]  eta: 0:00:35  lr: 0.000044  min_lr: 0.000001  loss: 1.9560 (1.9714)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5475 (7.3572)  time: 0.8206 (0.5258 -- 3.7464)  data: 0.1966 (0.0006 -- 2.1936)  max mem: 16413
Epoch: [37]  [40/52]  eta: 0:00:12  lr: 0.000044  min_lr: 0.000001  loss: 1.8915 (1.9286)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4349 (6.8955)  time: 0.9036 (0.5139 -- 3.8440)  data: 0.0016 (0.0002 -- 0.0055)  max mem: 16413
Epoch: [37]  [51/52]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.1427 (1.9860)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8019 (7.3061)  time: 0.7303 (0.4948 -- 2.9477)  data: 0.0009 (0.0001 -- 0.0040)  max mem: 16413
Epoch: [37] Total time: 0:00:47 (0.9133 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.1427 (1.9768)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8019 (7.3061)
Val:  [0/9]  eta: 0:00:19  loss: 1.1107 (1.1107)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.1505 (2.1505 -- 2.1505)  data: 1.9728 (1.9728 -- 1.9728)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.1107 (1.1715)  acc1: 66.6667 (67.0886)  acc5: 88.8889 (92.4051)  time: 0.3873 (0.1333 -- 2.1505)  data: 0.2193 (0.0001 -- 1.9728)  max mem: 16413
Val: Total time: 0:00:03 (0.3875 s / it)
* Acc@1 68.987 Acc@5 94.304 loss 1.133
Accuracy of the network on the 158 val images: 68.99%
Max accuracy: 68.99%
Epoch: [38]  [ 0/52]  eta: 0:06:18  lr: 0.000044  min_lr: 0.000001  loss: 2.7067 (2.7067)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8750 (7.8750)  time: 7.2830 (7.2830 -- 7.2830)  data: 6.7550 (6.7550 -- 6.7550)  max mem: 16413
Epoch: [38]  [20/52]  eta: 0:00:35  lr: 0.000044  min_lr: 0.000001  loss: 2.0385 (2.0142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5741 (7.7812)  time: 0.8170 (0.5390 -- 3.0495)  data: 0.2651 (0.0005 -- 2.5072)  max mem: 16413
[2023-10-23 23:44:24,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=6, lr=[1.0348201674939081e-06, 1.0348201674939081e-06, 1.3797602233252108e-06, 1.3797602233252108e-06, 1.8396802977669478e-06, 1.8396802977669478e-06, 2.452907063689264e-06, 2.452907063689264e-06, 3.2705427515856848e-06, 3.2705427515856848e-06, 4.360723668780913e-06, 4.360723668780913e-06, 5.814298225041218e-06, 5.814298225041218e-06, 7.75239763338829e-06, 7.75239763338829e-06, 1.0336530177851054e-05, 1.0336530177851054e-05, 1.3782040237134738e-05, 1.3782040237134738e-05, 1.8376053649512985e-05, 1.8376053649512985e-05, 2.4501404866017314e-05, 2.4501404866017314e-05, 3.266853982135642e-05, 3.266853982135642e-05, 4.355805309514189e-05, 4.355805309514189e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-23 23:44:24,565] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=18.778261786630438, CurrSamplesPerSec=22.61994876634758, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [38]  [40/52]  eta: 0:00:12  lr: 0.000043  min_lr: 0.000001  loss: 2.2131 (2.0793)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8313 (7.4863)  time: 0.9549 (0.5268 -- 3.9048)  data: 0.4136 (0.0003 -- 3.3738)  max mem: 16413
Epoch: [38]  [51/52]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.1494 (2.1142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0815 (7.2611)  time: 0.7652 (0.4964 -- 3.9048)  data: 0.2499 (0.0001 -- 3.3738)  max mem: 16413
Epoch: [38] Total time: 0:00:48 (0.9280 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.1494 (2.0606)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0815 (7.2611)
Val:  [0/9]  eta: 0:00:19  loss: 1.0821 (1.0821)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.1553 (2.1553 -- 2.1553)  data: 1.9786 (1.9786 -- 1.9786)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0821 (1.1924)  acc1: 66.6667 (65.8228)  acc5: 88.8889 (92.4051)  time: 0.3861 (0.1334 -- 2.1553)  data: 0.2199 (0.0001 -- 1.9786)  max mem: 16413
Val: Total time: 0:00:03 (0.3862 s / it)
* Acc@1 68.354 Acc@5 94.304 loss 1.141
Accuracy of the network on the 158 val images: 68.35%
Max accuracy: 68.99%
[2023-10-23 23:44:57,280] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:44:57,281] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:44:57,281] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:44:57,281] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [ 0/52]  eta: 0:05:27  lr: 0.000043  min_lr: 0.000001  loss: 2.2555 (2.2555)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2955 (6.2955)  time: 6.2925 (6.2925 -- 6.2925)  data: 5.7652 (5.7652 -- 5.7652)  max mem: 16413
Epoch: [39]  [20/52]  eta: 0:00:34  lr: 0.000043  min_lr: 0.000001  loss: 2.0348 (2.0440)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3702 (7.2589)  time: 0.8324 (0.5291 -- 3.6547)  data: 0.2782 (0.0006 -- 3.1092)  max mem: 16413
[2023-10-23 23:45:29,845] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2065
[2023-10-23 23:45:29,845] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2065
[2023-10-23 23:45:29,845] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:45:29,845] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:45:29,846] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [40/52]  eta: 0:00:12  lr: 0.000043  min_lr: 0.000001  loss: 1.9836 (2.0056)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4116 (7.5067)  time: 0.9460 (0.5271 -- 3.8674)  data: 0.2879 (0.0004 -- 3.3025)  max mem: 16413
Epoch: [39]  [51/52]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.0692 (2.0163)  loss_scale: 16384.0000 (28041.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6680 (7.2646)  time: 0.8125 (0.4955 -- 2.7232)  data: 0.1566 (0.0001 -- 1.3394)  max mem: 16413
Epoch: [39] Total time: 0:00:48 (0.9330 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.0692 (1.9742)  loss_scale: 16384.0000 (28041.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6680 (7.2646)
[2023-10-23 23:45:39,519] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-10-23 23:45:39,520] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-39/mp_rank_00_model_states.pt
[2023-10-23 23:45:39,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-39/mp_rank_00_model_states.pt...
[2023-10-23 23:45:39,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-10-23 23:45:40,503] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-39/mp_rank_00_model_states.pt.
[2023-10-23 23:45:40,503] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [0/9]  eta: 0:00:18  loss: 1.1060 (1.1060)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.0976 (2.0976 -- 2.0976)  data: 1.9238 (1.9238 -- 1.9238)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.1293 (1.1684)  acc1: 57.1429 (62.0253)  acc5: 88.8889 (92.4051)  time: 0.3784 (0.1326 -- 2.0976)  data: 0.2138 (0.0001 -- 1.9238)  max mem: 16413
Val: Total time: 0:00:03 (0.3785 s / it)
* Acc@1 68.987 Acc@5 94.937 loss 1.131
Accuracy of the network on the 158 val images: 68.99%
Max accuracy: 68.99%
Epoch: [40]  [ 0/52]  eta: 0:05:33  lr: 0.000043  min_lr: 0.000001  loss: 1.7169 (1.7169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2373 (7.2373)  time: 6.4078 (6.4078 -- 6.4078)  data: 5.8659 (5.8659 -- 5.8659)  max mem: 16413
Epoch: [40]  [20/52]  eta: 0:00:38  lr: 0.000043  min_lr: 0.000001  loss: 1.9479 (1.9719)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9012 (7.6875)  time: 0.9529 (0.5311 -- 2.8312)  data: 0.1974 (0.0006 -- 1.9369)  max mem: 16413
Epoch: [40]  [40/52]  eta: 0:00:11  lr: 0.000043  min_lr: 0.000001  loss: 2.0166 (1.9685)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4422 (7.2952)  time: 0.7410 (0.5280 -- 1.9149)  data: 0.0015 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [40]  [51/52]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 2.0166 (1.9975)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3930 (7.2655)  time: 0.7508 (0.4944 -- 1.9149)  data: 0.0008 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [40] Total time: 0:00:47 (0.9184 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 2.0166 (2.0261)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3930 (7.2655)
Val:  [0/9]  eta: 0:00:19  loss: 1.1008 (1.1008)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.1273 (2.1273 -- 2.1273)  data: 1.9490 (1.9490 -- 1.9490)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.2374 (1.1827)  acc1: 66.6667 (67.0886)  acc5: 88.8889 (91.1392)  time: 0.3834 (0.1329 -- 2.1273)  data: 0.2166 (0.0001 -- 1.9490)  max mem: 16413
Val: Total time: 0:00:03 (0.3835 s / it)
* Acc@1 69.620 Acc@5 93.671 loss 1.130
Accuracy of the network on the 158 val images: 69.62%
[2023-10-23 23:46:35,140] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:46:35,142] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:46:35,142] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:46:35,142] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:46:36,497] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:46:36,497] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.62%
Epoch: [41]  [ 0/52]  eta: 0:06:50  lr: 0.000043  min_lr: 0.000001  loss: 2.4675 (2.4675)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5441 (6.5441)  time: 7.8865 (7.8865 -- 7.8865)  data: 7.3644 (7.3644 -- 7.3644)  max mem: 16413
Epoch: [41]  [20/52]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000001  loss: 2.0368 (2.0500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2061 (7.2151)  time: 0.8402 (0.5211 -- 3.9057)  data: 0.2897 (0.0001 -- 3.3471)  max mem: 16413
Epoch: [41]  [40/52]  eta: 0:00:12  lr: 0.000043  min_lr: 0.000001  loss: 1.9214 (2.0164)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3841 (7.4310)  time: 0.9296 (0.5343 -- 3.7155)  data: 0.3847 (0.0003 -- 3.1927)  max mem: 16413
Epoch: [41]  [51/52]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9214 (2.0000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6181 (7.3983)  time: 0.7661 (0.4963 -- 2.7937)  data: 0.2442 (0.0001 -- 2.2846)  max mem: 16413
Epoch: [41] Total time: 0:00:49 (0.9463 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9214 (2.0135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6181 (7.3983)
Val:  [0/9]  eta: 0:00:18  loss: 1.0412 (1.0412)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.0386 (2.0386 -- 2.0386)  data: 1.8554 (1.8554 -- 1.8554)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0412 (1.1348)  acc1: 71.4286 (70.8861)  acc5: 88.8889 (93.6709)  time: 0.3732 (0.1331 -- 2.0386)  data: 0.2062 (0.0001 -- 1.8554)  max mem: 16413
Val: Total time: 0:00:03 (0.3733 s / it)
* Acc@1 71.519 Acc@5 95.570 loss 1.085
Accuracy of the network on the 158 val images: 71.52%
[2023-10-23 23:47:29,117] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:47:29,119] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:47:29,119] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:47:29,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:47:30,449] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:47:30,449] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.52%
Epoch: [42]  [ 0/52]  eta: 0:06:35  lr: 0.000043  min_lr: 0.000001  loss: 2.3415 (2.3415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2318 (10.2318)  time: 7.5980 (7.5980 -- 7.5980)  data: 7.0631 (7.0631 -- 7.0631)  max mem: 16413
[2023-10-23 23:47:47,028] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:47:47,028] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:47:47,028] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:47:47,028] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [42]  [20/52]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000001  loss: 1.9701 (2.0231)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4275 (7.4444)  time: 0.8292 (0.5210 -- 4.0016)  data: 0.2712 (0.0005 -- 3.4622)  max mem: 16413
Epoch: [42]  [40/52]  eta: 0:00:12  lr: 0.000043  min_lr: 0.000001  loss: 2.0435 (2.0171)  loss_scale: 32768.0000 (28771.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9296 (7.4144)  time: 0.9666 (0.5219 -- 4.5274)  data: 0.2157 (0.0004 -- 3.9854)  max mem: 16413
Epoch: [42]  [51/52]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9956 (2.0029)  loss_scale: 32768.0000 (29617.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9459 (7.2341)  time: 0.7416 (0.4955 -- 2.9953)  data: 0.0156 (0.0001 -- 0.2953)  max mem: 16413
Epoch: [42] Total time: 0:00:49 (0.9434 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9956 (2.0006)  loss_scale: 32768.0000 (29617.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9459 (7.2341)
Val:  [0/9]  eta: 0:00:18  loss: 1.0034 (1.0034)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0591 (2.0591 -- 2.0591)  data: 1.8809 (1.8809 -- 1.8809)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0816 (1.1601)  acc1: 66.6667 (69.6203)  acc5: 100.0000 (94.9367)  time: 0.3743 (0.1328 -- 2.0591)  data: 0.2091 (0.0001 -- 1.8809)  max mem: 16413
Val: Total time: 0:00:03 (0.3745 s / it)
* Acc@1 72.152 Acc@5 95.570 loss 1.090
Accuracy of the network on the 158 val images: 72.15%
[2023-10-23 23:48:22,883] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:48:22,885] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:48:22,885] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:48:22,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:48:23,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:48:23,993] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 72.15%
Epoch: [43]  [ 0/52]  eta: 0:07:17  lr: 0.000043  min_lr: 0.000001  loss: 1.7913 (1.7913)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0319 (5.0319)  time: 8.4125 (8.4125 -- 8.4125)  data: 7.8916 (7.8916 -- 7.8916)  max mem: 16413
Epoch: [43]  [20/52]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000001  loss: 1.9944 (1.9487)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1117 (7.3950)  time: 0.8214 (0.5303 -- 2.9662)  data: 0.1853 (0.0006 -- 2.4488)  max mem: 16413
Epoch: [43]  [40/52]  eta: 0:00:12  lr: 0.000042  min_lr: 0.000001  loss: 1.9910 (1.9739)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2250 (7.5134)  time: 0.9698 (0.5227 -- 4.9773)  data: 0.4263 (0.0004 -- 4.4358)  max mem: 16413
Epoch: [43]  [51/52]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.9821 (1.9771)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2359 (7.3974)  time: 0.7592 (0.4957 -- 2.7634)  data: 0.2426 (0.0001 -- 2.2678)  max mem: 16413
Epoch: [43] Total time: 0:00:50 (0.9716 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.9821 (1.9998)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2359 (7.3974)
Val:  [0/9]  eta: 0:00:19  loss: 0.9662 (0.9662)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1500 (2.1500 -- 2.1500)  data: 1.9717 (1.9717 -- 1.9717)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0676 (1.1429)  acc1: 77.7778 (73.4177)  acc5: 88.8889 (93.6709)  time: 0.3862 (0.1323 -- 2.1500)  data: 0.2192 (0.0001 -- 1.9717)  max mem: 16413
Val: Total time: 0:00:03 (0.3864 s / it)
* Acc@1 74.051 Acc@5 94.937 loss 1.080
Accuracy of the network on the 158 val images: 74.05%
[2023-10-23 23:49:18,001] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:49:18,003] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:49:18,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:49:18,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:49:19,391] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:49:19,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.05%
Epoch: [44]  [ 0/52]  eta: 0:07:20  lr: 0.000042  min_lr: 0.000001  loss: 2.4123 (2.4123)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2040 (6.2040)  time: 8.4670 (8.4670 -- 8.4670)  data: 7.9511 (7.9511 -- 7.9511)  max mem: 16413
Epoch: [44]  [20/52]  eta: 0:00:41  lr: 0.000042  min_lr: 0.000001  loss: 2.0127 (1.9963)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3242 (7.3918)  time: 0.9507 (0.5117 -- 3.8880)  data: 0.0013 (0.0002 -- 0.0027)  max mem: 16413
[2023-10-23 23:50:00,566] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:50:00,567] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-10-23 23:50:00,613] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:50:00,613] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-10-23 23:50:01,127] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2323
[2023-10-23 23:50:01,128] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-10-23 23:50:01,128] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2323
[2023-10-23 23:50:01,128] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-10-23 23:50:01,128] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-10-23 23:50:01,658] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2324
[2023-10-23 23:50:01,658] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2324
[2023-10-23 23:50:01,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:50:01,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:50:01,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [44]  [40/52]  eta: 0:00:12  lr: 0.000042  min_lr: 0.000001  loss: 1.9216 (1.9775)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8938 (7.1741)  time: 0.8448 (0.5004 -- 4.1989)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [44]  [51/52]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 2.1111 (1.9924)  loss_scale: 16384.0000 (28356.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8938 (7.3474)  time: 0.7941 (0.4943 -- 4.1989)  data: 0.0007 (0.0001 -- 0.0021)  max mem: 16413
Epoch: [44] Total time: 0:00:51 (0.9953 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 2.1111 (1.9768)  loss_scale: 16384.0000 (28356.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8938 (7.3474)
Val:  [0/9]  eta: 0:00:19  loss: 0.9702 (0.9702)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.1147 (2.1147 -- 2.1147)  data: 1.9341 (1.9341 -- 1.9341)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0184 (1.1204)  acc1: 66.6667 (68.3544)  acc5: 88.8889 (92.4051)  time: 0.3809 (0.1327 -- 2.1147)  data: 0.2150 (0.0001 -- 1.9341)  max mem: 16413
Val: Total time: 0:00:03 (0.3811 s / it)
* Acc@1 72.785 Acc@5 94.937 loss 1.052
Accuracy of the network on the 158 val images: 72.78%
Max accuracy: 74.05%
Epoch: [45]  [ 0/52]  eta: 0:05:29  lr: 0.000042  min_lr: 0.000001  loss: 2.3937 (2.3937)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0762 (9.0762)  time: 6.3456 (6.3456 -- 6.3456)  data: 5.4562 (5.4562 -- 5.4562)  max mem: 16413
Epoch: [45]  [20/52]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000001  loss: 1.9841 (1.9992)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6208 (7.8937)  time: 0.9202 (0.5260 -- 2.8142)  data: 0.2831 (0.0004 -- 2.2982)  max mem: 16413
Epoch: [45]  [40/52]  eta: 0:00:12  lr: 0.000042  min_lr: 0.000001  loss: 2.0974 (2.0478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7207 (7.8136)  time: 0.8340 (0.5206 -- 2.2832)  data: 0.1287 (0.0003 -- 1.1715)  max mem: 16413
Epoch: [45]  [51/52]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 2.0160 (2.0418)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4515 (7.8129)  time: 0.7908 (0.4944 -- 2.2832)  data: 0.1558 (0.0001 -- 1.1715)  max mem: 16413
Epoch: [45] Total time: 0:00:48 (0.9280 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 2.0160 (2.0223)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4515 (7.8129)
Val:  [0/9]  eta: 0:00:18  loss: 0.9851 (0.9851)  acc1: 88.8889 (88.8889)  acc5: 88.8889 (88.8889)  time: 2.1029 (2.1029 -- 2.1029)  data: 1.9267 (1.9267 -- 1.9267)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0620 (1.1153)  acc1: 66.6667 (72.1519)  acc5: 88.8889 (92.4051)  time: 0.3809 (0.1333 -- 2.1029)  data: 0.2142 (0.0001 -- 1.9267)  max mem: 16413
Val: Total time: 0:00:03 (0.3810 s / it)
* Acc@1 73.418 Acc@5 93.671 loss 1.054
Accuracy of the network on the 158 val images: 73.42%
Max accuracy: 74.05%
Epoch: [46]  [ 0/52]  eta: 0:06:29  lr: 0.000042  min_lr: 0.000001  loss: 2.0569 (2.0569)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5549 (6.5549)  time: 7.4930 (7.4930 -- 7.4930)  data: 6.9605 (6.9605 -- 6.9605)  max mem: 16413
Epoch: [46]  [20/52]  eta: 0:00:39  lr: 0.000042  min_lr: 0.000001  loss: 2.0041 (1.9976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0212 (6.4504)  time: 0.9195 (0.5222 -- 3.0598)  data: 0.3265 (0.0002 -- 2.4905)  max mem: 16413
Epoch: [46]  [40/52]  eta: 0:00:12  lr: 0.000042  min_lr: 0.000001  loss: 2.0255 (1.9746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4431 (7.2664)  time: 0.7719 (0.5185 -- 2.8045)  data: 0.1114 (0.0002 -- 1.0858)  max mem: 16413
Epoch: [46]  [51/52]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.9956 (1.9796)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1668 (7.4618)  time: 0.7923 (0.4965 -- 2.8045)  data: 0.0559 (0.0001 -- 0.9993)  max mem: 16413
Epoch: [46] Total time: 0:00:48 (0.9406 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.9956 (1.9603)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1668 (7.4618)
Val:  [0/9]  eta: 0:00:19  loss: 1.0112 (1.0112)  acc1: 66.6667 (66.6667)  acc5: 88.8889 (88.8889)  time: 2.1366 (2.1366 -- 2.1366)  data: 1.9673 (1.9673 -- 1.9673)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0221 (1.1030)  acc1: 66.6667 (67.0886)  acc5: 88.8889 (93.6709)  time: 0.3840 (0.1334 -- 2.1366)  data: 0.2187 (0.0001 -- 1.9673)  max mem: 16413
Val: Total time: 0:00:03 (0.3842 s / it)
* Acc@1 72.152 Acc@5 94.937 loss 1.043
Accuracy of the network on the 158 val images: 72.15%
Max accuracy: 74.05%
Epoch: [47]  [ 0/52]  eta: 0:08:16  lr: 0.000042  min_lr: 0.000001  loss: 1.8229 (1.8229)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7407 (6.7407)  time: 9.5532 (9.5532 -- 9.5532)  data: 9.0065 (9.0065 -- 9.0065)  max mem: 16413
[2023-10-23 23:52:15,058] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:52:15,059] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:52:15,060] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:52:15,060] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [47]  [20/52]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000001  loss: 1.8539 (1.9016)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9575 (7.2742)  time: 0.7647 (0.5299 -- 2.8668)  data: 0.2156 (0.0005 -- 2.3397)  max mem: 16413
[2023-10-23 23:52:29,221] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2469
[2023-10-23 23:52:29,221] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:52:29,221] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2469
[2023-10-23 23:52:29,222] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:52:29,222] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [40/52]  eta: 0:00:12  lr: 0.000042  min_lr: 0.000001  loss: 1.9354 (1.9279)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3109 (7.5360)  time: 0.9536 (0.5215 -- 3.5491)  data: 0.2514 (0.0003 -- 2.4846)  max mem: 16413
Epoch: [47]  [51/52]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.9800 (1.9475)  loss_scale: 16384.0000 (21425.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3996 (7.6797)  time: 0.7795 (0.4941 -- 3.3286)  data: 0.1261 (0.0001 -- 2.2695)  max mem: 16413
Epoch: [47] Total time: 0:00:49 (0.9522 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.9800 (1.9719)  loss_scale: 16384.0000 (21425.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3996 (7.6797)
Val:  [0/9]  eta: 0:00:19  loss: 0.9376 (0.9376)  acc1: 88.8889 (88.8889)  acc5: 88.8889 (88.8889)  time: 2.1687 (2.1687 -- 2.1687)  data: 1.9915 (1.9915 -- 1.9915)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.1327 (1.1043)  acc1: 77.7778 (72.1519)  acc5: 88.8889 (91.1392)  time: 0.3872 (0.1330 -- 2.1687)  data: 0.2213 (0.0001 -- 1.9915)  max mem: 16413
Val: Total time: 0:00:03 (0.3873 s / it)
* Acc@1 74.051 Acc@5 93.671 loss 1.049
Accuracy of the network on the 158 val images: 74.05%
[2023-10-23 23:52:51,649] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:52:51,651] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:52:51,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:52:51,651] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:52:53,088] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:52:53,088] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.05%
Epoch: [48]  [ 0/52]  eta: 0:05:40  lr: 0.000041  min_lr: 0.000001  loss: 1.5155 (1.5155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6644 (8.6644)  time: 6.5424 (6.5424 -- 6.5424)  data: 4.5279 (4.5279 -- 4.5279)  max mem: 16413
Epoch: [48]  [20/52]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000001  loss: 2.0322 (2.0023)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6595 (7.6243)  time: 0.9085 (0.5298 -- 3.7289)  data: 0.1264 (0.0003 -- 1.4149)  max mem: 16413
Epoch: [48]  [40/52]  eta: 0:00:12  lr: 0.000041  min_lr: 0.000001  loss: 2.0457 (2.0362)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3682 (7.5864)  time: 0.8801 (0.5129 -- 3.5595)  data: 0.1549 (0.0003 -- 1.5484)  max mem: 16413
Epoch: [48]  [51/52]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 2.0209 (1.9963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1992 (7.9235)  time: 0.7667 (0.4928 -- 3.5595)  data: 0.0766 (0.0001 -- 0.8104)  max mem: 16413
Epoch: [48] Total time: 0:00:48 (0.9409 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 2.0209 (1.9768)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1992 (7.9235)
Val:  [0/9]  eta: 0:00:19  loss: 0.9476 (0.9476)  acc1: 88.8889 (88.8889)  acc5: 88.8889 (88.8889)  time: 2.1252 (2.1252 -- 2.1252)  data: 1.9413 (1.9413 -- 1.9413)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0573 (1.0923)  acc1: 66.6667 (69.6203)  acc5: 88.8889 (92.4051)  time: 0.3826 (0.1333 -- 2.1252)  data: 0.2158 (0.0001 -- 1.9413)  max mem: 16413
Val: Total time: 0:00:03 (0.3827 s / it)
* Acc@1 74.051 Acc@5 94.937 loss 1.029
Accuracy of the network on the 158 val images: 74.05%
[2023-10-23 23:53:45,464] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:53:45,465] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:53:45,465] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:53:45,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:53:46,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:53:46,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.05%
Epoch: [49]  [ 0/52]  eta: 0:06:29  lr: 0.000041  min_lr: 0.000001  loss: 1.8762 (1.8762)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1196 (7.1196)  time: 7.4882 (7.4882 -- 7.4882)  data: 6.9478 (6.9478 -- 6.9478)  max mem: 16413
Epoch: [49]  [20/52]  eta: 0:00:39  lr: 0.000041  min_lr: 0.000001  loss: 2.0995 (2.1218)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5742 (8.2189)  time: 0.9182 (0.5283 -- 3.2101)  data: 0.0223 (0.0003 -- 0.4243)  max mem: 16413
Epoch: [49]  [40/52]  eta: 0:00:12  lr: 0.000041  min_lr: 0.000001  loss: 2.0528 (2.0728)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9387 (8.1699)  time: 0.8570 (0.5250 -- 3.1437)  data: 0.0681 (0.0003 -- 1.3393)  max mem: 16413
[2023-10-23 23:54:36,738] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:54:36,738] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:54:36,738] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:54:36,738] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [49]  [51/52]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.9871 (2.0363)  loss_scale: 16384.0000 (17014.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0182 (8.0152)  time: 0.7911 (0.4949 -- 2.8328)  data: 0.1559 (0.0001 -- 1.7651)  max mem: 16413
Epoch: [49] Total time: 0:00:50 (0.9667 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.9871 (2.0014)  loss_scale: 16384.0000 (17014.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0182 (8.0152)
Val:  [0/9]  eta: 0:00:18  loss: 0.9337 (0.9337)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0290 (2.0290 -- 2.0290)  data: 1.8497 (1.8497 -- 1.8497)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.9914 (1.0917)  acc1: 66.6667 (69.6203)  acc5: 100.0000 (94.9367)  time: 0.3720 (0.1334 -- 2.0290)  data: 0.2056 (0.0001 -- 1.8497)  max mem: 16413
Val: Total time: 0:00:03 (0.3721 s / it)
* Acc@1 75.316 Acc@5 94.937 loss 1.023
Accuracy of the network on the 158 val images: 75.32%
[2023-10-23 23:54:40,699] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:54:40,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:54:40,701] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:54:40,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:54:42,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:54:42,201] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.32%
Epoch: [50]  [ 0/52]  eta: 0:05:50  lr: 0.000041  min_lr: 0.000001  loss: 2.0877 (2.0877)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1617 (7.1617)  time: 6.7463 (6.7463 -- 6.7463)  data: 5.3208 (5.3208 -- 5.3208)  max mem: 16413
Epoch: [50]  [20/52]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000001  loss: 1.8517 (1.9006)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8686 (7.8306)  time: 0.8561 (0.5337 -- 2.6075)  data: 0.1402 (0.0005 -- 1.8904)  max mem: 16413
Epoch: [50]  [40/52]  eta: 0:00:12  lr: 0.000041  min_lr: 0.000001  loss: 2.1051 (1.9860)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9290 (7.5521)  time: 0.8645 (0.5280 -- 2.5111)  data: 0.2169 (0.0004 -- 1.9958)  max mem: 16413
Epoch: [50]  [51/52]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 2.0084 (1.9787)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0318 (7.5034)  time: 0.7489 (0.4995 -- 2.5111)  data: 0.1921 (0.0002 -- 1.9958)  max mem: 16413
Epoch: [50] Total time: 0:00:47 (0.9096 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 2.0084 (2.0111)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0318 (7.5034)
Val:  [0/9]  eta: 0:00:19  loss: 0.9899 (0.9899)  acc1: 77.7778 (77.7778)  acc5: 88.8889 (88.8889)  time: 2.1415 (2.1415 -- 2.1415)  data: 1.9676 (1.9676 -- 1.9676)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.9899 (1.0723)  acc1: 66.6667 (68.3544)  acc5: 88.8889 (93.6709)  time: 0.3848 (0.1330 -- 2.1415)  data: 0.2187 (0.0001 -- 1.9676)  max mem: 16413
Val: Total time: 0:00:03 (0.3849 s / it)
* Acc@1 71.519 Acc@5 94.937 loss 1.031
Accuracy of the network on the 158 val images: 71.52%
Max accuracy: 75.32%
Epoch: [51]  [ 0/52]  eta: 0:06:06  lr: 0.000041  min_lr: 0.000001  loss: 1.9772 (1.9772)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1331 (5.1331)  time: 7.0444 (7.0444 -- 7.0444)  data: 6.4898 (6.4898 -- 6.4898)  max mem: 16413
Epoch: [51]  [20/52]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000001  loss: 2.2141 (2.1417)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9639 (7.6415)  time: 0.8335 (0.5251 -- 2.1497)  data: 0.1897 (0.0004 -- 0.8183)  max mem: 16413
Epoch: [51]  [40/52]  eta: 0:00:12  lr: 0.000041  min_lr: 0.000001  loss: 2.1080 (2.1133)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5976 (8.0479)  time: 0.9119 (0.5211 -- 2.7703)  data: 0.1616 (0.0003 -- 2.2193)  max mem: 16413
Epoch: [51]  [51/52]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 2.0983 (2.1060)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5976 (7.9839)  time: 0.7177 (0.4990 -- 1.8970)  data: 0.0704 (0.0001 -- 0.9885)  max mem: 16413
Epoch: [51] Total time: 0:00:48 (0.9319 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 2.0983 (2.0304)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5976 (7.9839)
Val:  [0/9]  eta: 0:00:19  loss: 0.9385 (0.9385)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1114 (2.1114 -- 2.1114)  data: 1.9328 (1.9328 -- 1.9328)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 1.0824 (1.0961)  acc1: 66.6667 (68.3544)  acc5: 100.0000 (94.9367)  time: 0.3813 (0.1329 -- 2.1114)  data: 0.2148 (0.0001 -- 1.9328)  max mem: 16413
Val: Total time: 0:00:03 (0.3815 s / it)
* Acc@1 72.785 Acc@5 94.937 loss 1.028
Accuracy of the network on the 158 val images: 72.78%
Max accuracy: 75.32%
Epoch: [52]  [ 0/52]  eta: 0:04:55  lr: 0.000040  min_lr: 0.000001  loss: 2.4021 (2.4021)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0440 (9.0440)  time: 5.6895 (5.6895 -- 5.6895)  data: 4.6856 (4.6856 -- 4.6856)  max mem: 16413
[2023-10-23 23:56:35,815] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2711
[2023-10-23 23:56:35,815] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:56:35,815] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2711
[2023-10-23 23:56:35,816] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-23 23:56:35,816] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [52]  [20/52]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000001  loss: 2.0304 (1.9981)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1180 (8.6356)  time: 0.9307 (0.5226 -- 2.8685)  data: 0.3388 (0.0006 -- 2.3218)  max mem: 16413
Epoch: [52]  [40/52]  eta: 0:00:12  lr: 0.000040  min_lr: 0.000001  loss: 1.9921 (2.0070)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4045 (8.1914)  time: 0.8470 (0.5172 -- 2.3752)  data: 0.2018 (0.0003 -- 1.8538)  max mem: 16413
Epoch: [52]  [51/52]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.8819 (1.9960)  loss_scale: 16384.0000 (18589.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4015 (8.0727)  time: 0.7003 (0.4963 -- 1.7946)  data: 0.0246 (0.0002 -- 0.4765)  max mem: 16413
Epoch: [52] Total time: 0:00:49 (0.9450 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.8819 (2.0485)  loss_scale: 16384.0000 (18589.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4015 (8.0727)
Val:  [0/9]  eta: 0:00:18  loss: 0.9332 (0.9332)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0917 (2.0917 -- 2.0917)  data: 1.9198 (1.9198 -- 1.9198)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.9332 (1.0618)  acc1: 71.4286 (70.8861)  acc5: 100.0000 (93.6709)  time: 0.3811 (0.1338 -- 2.0917)  data: 0.2134 (0.0001 -- 1.9198)  max mem: 16413
Val: Total time: 0:00:03 (0.3812 s / it)
* Acc@1 72.785 Acc@5 94.937 loss 1.014
Accuracy of the network on the 158 val images: 72.78%
Max accuracy: 75.32%
Epoch: [53]  [ 0/52]  eta: 0:06:48  lr: 0.000040  min_lr: 0.000001  loss: 1.8838 (1.8838)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9702 (6.9702)  time: 7.8610 (7.8610 -- 7.8610)  data: 5.0674 (5.0674 -- 5.0674)  max mem: 16413
Epoch: [53]  [20/52]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000001  loss: 2.0642 (2.0589)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7866 (7.2455)  time: 0.8411 (0.5250 -- 3.4847)  data: 0.0072 (0.0007 -- 0.1146)  max mem: 16413
Epoch: [53]  [40/52]  eta: 0:00:12  lr: 0.000040  min_lr: 0.000001  loss: 1.8106 (1.9482)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5716 (7.2254)  time: 0.8777 (0.5286 -- 3.5505)  data: 0.0022 (0.0002 -- 0.0156)  max mem: 16413
Epoch: [53]  [51/52]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.7873 (1.9390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1960 (7.3712)  time: 0.7694 (0.4972 -- 3.5505)  data: 0.0016 (0.0001 -- 0.0156)  max mem: 16413
Epoch: [53] Total time: 0:00:48 (0.9237 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.7873 (1.9175)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1960 (7.3712)
Val:  [0/9]  eta: 0:00:18  loss: 0.9467 (0.9467)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0757 (2.0757 -- 2.0757)  data: 1.8907 (1.8907 -- 1.8907)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.9467 (1.0503)  acc1: 66.6667 (68.3544)  acc5: 100.0000 (93.6709)  time: 0.3769 (0.1333 -- 2.0757)  data: 0.2102 (0.0001 -- 1.8907)  max mem: 16413
Val: Total time: 0:00:03 (0.3770 s / it)
* Acc@1 72.785 Acc@5 94.937 loss 0.984
Accuracy of the network on the 158 val images: 72.78%
Max accuracy: 75.32%
Epoch: [54]  [ 0/52]  eta: 0:06:46  lr: 0.000040  min_lr: 0.000001  loss: 1.7455 (1.7455)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4001 (5.4001)  time: 7.8188 (7.8188 -- 7.8188)  data: 5.1263 (5.1263 -- 5.1263)  max mem: 16413
Epoch: [54]  [20/52]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000001  loss: 1.9867 (1.8896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1283 (7.3848)  time: 0.8255 (0.5161 -- 2.5916)  data: 0.1912 (0.0003 -- 1.1011)  max mem: 16413
[2023-10-23 23:58:43,446] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:58:43,446] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-23 23:58:43,446] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-23 23:58:43,446] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [54]  [40/52]  eta: 0:00:12  lr: 0.000040  min_lr: 0.000001  loss: 1.7692 (1.8395)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5726 (7.5660)  time: 0.8561 (0.5188 -- 3.1567)  data: 0.2764 (0.0006 -- 2.5946)  max mem: 16413
Epoch: [54]  [51/52]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.7692 (1.8315)  loss_scale: 32768.0000 (22685.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5224 (7.6025)  time: 0.7023 (0.4943 -- 3.1567)  data: 0.1453 (0.0002 -- 2.5946)  max mem: 16413
Epoch: [54] Total time: 0:00:47 (0.9107 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.7692 (1.9123)  loss_scale: 32768.0000 (22685.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5224 (7.6025)
Val:  [0/9]  eta: 0:00:18  loss: 0.9403 (0.9403)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0429 (2.0429 -- 2.0429)  data: 1.8663 (1.8663 -- 1.8663)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.9403 (1.0412)  acc1: 66.6667 (69.6203)  acc5: 100.0000 (94.9367)  time: 0.3738 (0.1333 -- 2.0429)  data: 0.2075 (0.0001 -- 1.8663)  max mem: 16413
Val: Total time: 0:00:03 (0.3739 s / it)
* Acc@1 72.785 Acc@5 95.570 loss 0.977
Accuracy of the network on the 158 val images: 72.78%
Max accuracy: 75.32%
Epoch: [55]  [ 0/52]  eta: 0:06:42  lr: 0.000040  min_lr: 0.000001  loss: 1.0828 (1.0828)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0339 (9.0339)  time: 7.7328 (7.7328 -- 7.7328)  data: 7.1802 (7.1802 -- 7.1802)  max mem: 16413
Epoch: [55]  [20/52]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000001  loss: 1.9033 (1.9099)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4779 (7.6491)  time: 0.8033 (0.5244 -- 2.1325)  data: 0.1822 (0.0004 -- 1.5813)  max mem: 16413
Epoch: [55]  [40/52]  eta: 0:00:12  lr: 0.000039  min_lr: 0.000001  loss: 1.8398 (1.8939)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7760 (7.3847)  time: 0.8720 (0.5360 -- 2.5060)  data: 0.0869 (0.0002 -- 0.8917)  max mem: 16413
[2023-10-23 23:59:43,177] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2902
[2023-10-23 23:59:43,177] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2902
[2023-10-23 23:59:43,177] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:59:43,177] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-23 23:59:43,177] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [55]  [51/52]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.9595 (1.8991)  loss_scale: 16384.0000 (29617.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4488 (7.3634)  time: 0.6952 (0.4947 -- 1.6989)  data: 0.0361 (0.0001 -- 0.3751)  max mem: 16413
Epoch: [55] Total time: 0:00:47 (0.9225 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.9595 (1.9656)  loss_scale: 16384.0000 (29617.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4488 (7.3634)
Val:  [0/9]  eta: 0:00:18  loss: 0.8729 (0.8729)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0640 (2.0640 -- 2.0640)  data: 1.8931 (1.8931 -- 1.8931)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8970 (1.0360)  acc1: 66.6667 (73.4177)  acc5: 100.0000 (93.6709)  time: 0.3768 (0.1331 -- 2.0640)  data: 0.2104 (0.0001 -- 1.8931)  max mem: 16413
Val: Total time: 0:00:03 (0.3770 s / it)
* Acc@1 75.949 Acc@5 94.937 loss 0.978
Accuracy of the network on the 158 val images: 75.95%
[2023-10-23 23:59:51,122] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-23 23:59:51,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-23 23:59:51,123] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-23 23:59:51,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-23 23:59:52,534] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-23 23:59:52,535] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.95%
Epoch: [56]  [ 0/52]  eta: 0:07:46  lr: 0.000039  min_lr: 0.000001  loss: 1.8019 (1.8019)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6608 (8.6608)  time: 8.9715 (8.9715 -- 8.9715)  data: 8.4460 (8.4460 -- 8.4460)  max mem: 16413
Epoch: [56]  [20/52]  eta: 0:00:40  lr: 0.000039  min_lr: 0.000001  loss: 1.8992 (1.8738)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3164 (7.7910)  time: 0.8790 (0.5317 -- 4.5558)  data: 0.3232 (0.0002 -- 4.0241)  max mem: 16413
Epoch: [56]  [40/52]  eta: 0:00:12  lr: 0.000039  min_lr: 0.000001  loss: 2.0720 (1.9374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6809 (8.4371)  time: 0.8311 (0.5048 -- 4.4677)  data: 0.2894 (0.0003 -- 3.9410)  max mem: 16413
Epoch: [56]  [51/52]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.8359 (1.9280)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0959 (8.2732)  time: 0.7514 (0.4958 -- 4.4677)  data: 0.2385 (0.0001 -- 3.9410)  max mem: 16413
Epoch: [56] Total time: 0:00:48 (0.9369 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.8359 (1.9203)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0959 (8.2732)
Val:  [0/9]  eta: 0:00:18  loss: 0.8569 (0.8569)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0777 (2.0777 -- 2.0777)  data: 1.9032 (1.9032 -- 1.9032)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.9143 (1.0331)  acc1: 66.6667 (73.4177)  acc5: 100.0000 (96.2025)  time: 0.3771 (0.1331 -- 2.0777)  data: 0.2115 (0.0001 -- 1.9032)  max mem: 16413
Val: Total time: 0:00:03 (0.3772 s / it)
* Acc@1 76.582 Acc@5 96.203 loss 0.975
Accuracy of the network on the 158 val images: 76.58%
[2023-10-24 00:00:44,658] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:00:44,659] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:00:44,659] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:00:44,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:00:45,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:00:45,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.58%
Epoch: [57]  [ 0/52]  eta: 0:06:29  lr: 0.000039  min_lr: 0.000001  loss: 1.8828 (1.8828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5931 (6.5931)  time: 7.4930 (7.4930 -- 7.4930)  data: 6.6761 (6.6761 -- 6.6761)  max mem: 16413
Epoch: [57]  [20/52]  eta: 0:00:39  lr: 0.000039  min_lr: 0.000001  loss: 2.0177 (1.9284)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8750 (8.1183)  time: 0.9090 (0.5259 -- 4.2534)  data: 0.3602 (0.0005 -- 3.7263)  max mem: 16413
[2023-10-24 00:01:25,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[9.250699452763599e-07, 9.250699452763599e-07, 1.2334265937018133e-06, 1.2334265937018133e-06, 1.6445687916024177e-06, 1.6445687916024177e-06, 2.1927583888032234e-06, 2.1927583888032234e-06, 2.9236778517376313e-06, 2.9236778517376313e-06, 3.898237135650176e-06, 3.898237135650176e-06, 5.197649514200233e-06, 5.197649514200233e-06, 6.930199352266978e-06, 6.930199352266978e-06, 9.240265803022638e-06, 9.240265803022638e-06, 1.2320354404030183e-05, 1.2320354404030183e-05, 1.6427139205373578e-05, 1.6427139205373578e-05, 2.190285227383144e-05, 2.190285227383144e-05, 2.920380303177525e-05, 2.920380303177525e-05, 3.8938404042367e-05, 3.8938404042367e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-24 00:01:25,191] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=18.71120929288503, CurrSamplesPerSec=23.89965607085045, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [57]  [40/52]  eta: 0:00:12  lr: 0.000039  min_lr: 0.000001  loss: 2.0650 (1.9844)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4233 (8.6500)  time: 0.8136 (0.5156 -- 2.3112)  data: 0.0584 (0.0005 -- 0.6547)  max mem: 16413
Epoch: [57]  [51/52]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 2.0053 (1.9804)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9215 (8.5012)  time: 0.7535 (0.4955 -- 2.3112)  data: 0.0387 (0.0001 -- 0.6547)  max mem: 16413
Epoch: [57] Total time: 0:00:48 (0.9368 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 2.0053 (1.9504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9215 (8.5012)
Val:  [0/9]  eta: 0:00:18  loss: 0.8397 (0.8397)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1084 (2.1084 -- 2.1084)  data: 1.9211 (1.9211 -- 1.9211)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8464 (1.0040)  acc1: 71.4286 (74.6835)  acc5: 100.0000 (93.6709)  time: 0.3809 (0.1325 -- 2.1084)  data: 0.2135 (0.0001 -- 1.9211)  max mem: 16413
Val: Total time: 0:00:03 (0.3810 s / it)
* Acc@1 75.949 Acc@5 94.937 loss 0.948
Accuracy of the network on the 158 val images: 75.95%
Max accuracy: 76.58%
Epoch: [58]  [ 0/52]  eta: 0:07:47  lr: 0.000039  min_lr: 0.000001  loss: 1.9847 (1.9847)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5503 (8.5503)  time: 8.9937 (8.9937 -- 8.9937)  data: 8.4427 (8.4427 -- 8.4427)  max mem: 16413
[2023-10-24 00:01:58,918] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:01:58,918] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:01:58,921] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:01:58,921] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:02:03,167] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3035
[2023-10-24 00:02:03,167] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3035
[2023-10-24 00:02:03,208] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:02:03,208] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:02:03,209] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [20/52]  eta: 0:00:39  lr: 0.000039  min_lr: 0.000001  loss: 1.8235 (1.9048)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1839 (8.3772)  time: 0.8336 (0.5236 -- 4.2104)  data: 0.2922 (0.0003 -- 3.7013)  max mem: 16413
Epoch: [58]  [40/52]  eta: 0:00:12  lr: 0.000039  min_lr: 0.000001  loss: 2.0456 (1.9539)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8502 (8.7570)  time: 0.8182 (0.5256 -- 2.1298)  data: 0.2714 (0.0003 -- 1.5758)  max mem: 16413
Epoch: [58]  [51/52]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.9159 (1.9317)  loss_scale: 16384.0000 (17644.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5733 (8.5505)  time: 0.7408 (0.4938 -- 2.0930)  data: 0.2197 (0.0002 -- 1.5498)  max mem: 16413
Epoch: [58] Total time: 0:00:49 (0.9434 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.9159 (1.9200)  loss_scale: 16384.0000 (17644.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5733 (8.5505)
Val:  [0/9]  eta: 0:00:18  loss: 0.8460 (0.8460)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0641 (2.0641 -- 2.0641)  data: 1.8867 (1.8867 -- 1.8867)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8483 (1.0004)  acc1: 71.4286 (72.1519)  acc5: 100.0000 (94.9367)  time: 0.3764 (0.1326 -- 2.0641)  data: 0.2097 (0.0001 -- 1.8867)  max mem: 16413
Val: Total time: 0:00:03 (0.3765 s / it)
* Acc@1 75.949 Acc@5 95.570 loss 0.934
Accuracy of the network on the 158 val images: 75.95%
Max accuracy: 76.58%
Epoch: [59]  [ 0/52]  eta: 0:06:57  lr: 0.000039  min_lr: 0.000001  loss: 2.2606 (2.2606)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4426 (6.4426)  time: 8.0302 (8.0302 -- 8.0302)  data: 7.4899 (7.4899 -- 7.4899)  max mem: 16413
Epoch: [59]  [20/52]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000001  loss: 1.9518 (1.9493)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6314 (7.9861)  time: 0.8042 (0.5273 -- 3.0328)  data: 0.2296 (0.0007 -- 2.5137)  max mem: 16413
Epoch: [59]  [40/52]  eta: 0:00:12  lr: 0.000038  min_lr: 0.000001  loss: 2.0514 (2.0060)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6185 (8.0463)  time: 0.8770 (0.5240 -- 3.0243)  data: 0.3298 (0.0001 -- 2.4872)  max mem: 16413
Epoch: [59]  [51/52]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 2.1629 (2.0262)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6060 (7.9637)  time: 0.7235 (0.4968 -- 2.1006)  data: 0.1970 (0.0002 -- 1.5808)  max mem: 16413
Epoch: [59] Total time: 0:00:47 (0.9163 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 2.1629 (1.9815)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6060 (7.9637)
[2023-10-24 00:03:18,190] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-10-24 00:03:18,191] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-59/mp_rank_00_model_states.pt
[2023-10-24 00:03:18,191] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-59/mp_rank_00_model_states.pt...
[2023-10-24 00:03:18,191] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-10-24 00:03:19,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-59/mp_rank_00_model_states.pt.
[2023-10-24 00:03:19,065] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [0/9]  eta: 0:00:19  loss: 0.8264 (0.8264)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1629 (2.1629 -- 2.1629)  data: 1.9797 (1.9797 -- 1.9797)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8968 (1.0118)  acc1: 66.6667 (72.1519)  acc5: 100.0000 (94.9367)  time: 0.3874 (0.1323 -- 2.1629)  data: 0.2200 (0.0001 -- 1.9797)  max mem: 16413
Val: Total time: 0:00:03 (0.3875 s / it)
* Acc@1 74.684 Acc@5 95.570 loss 0.952
Accuracy of the network on the 158 val images: 74.68%
Max accuracy: 76.58%
Epoch: [60]  [ 0/52]  eta: 0:05:58  lr: 0.000038  min_lr: 0.000001  loss: 1.5338 (1.5338)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4770 (7.4770)  time: 6.8945 (6.8945 -- 6.8945)  data: 6.2826 (6.2826 -- 6.2826)  max mem: 16413
Epoch: [60]  [20/52]  eta: 0:00:40  lr: 0.000038  min_lr: 0.000001  loss: 1.9090 (1.8681)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7232 (7.7553)  time: 0.9809 (0.5258 -- 4.9214)  data: 0.4336 (0.0003 -- 4.3933)  max mem: 16413
Epoch: [60]  [40/52]  eta: 0:00:12  lr: 0.000038  min_lr: 0.000001  loss: 2.0022 (1.9003)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5057 (7.7238)  time: 0.8339 (0.5122 -- 3.0846)  data: 0.2954 (0.0003 -- 2.5668)  max mem: 16413
[2023-10-24 00:04:07,890] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:04:07,890] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:04:07,890] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:04:07,890] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:04:09,874] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3168
[2023-10-24 00:04:09,874] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3168
[2023-10-24 00:04:09,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:04:09,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:04:09,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [60]  [51/52]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.8623 (1.9022)  loss_scale: 16384.0000 (17644.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0256 (7.7978)  time: 0.7093 (0.4821 -- 3.0846)  data: 0.1967 (0.0001 -- 2.5668)  max mem: 16413
Epoch: [60] Total time: 0:00:48 (0.9389 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.8623 (1.9303)  loss_scale: 16384.0000 (17644.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0256 (7.7978)
Val:  [0/9]  eta: 0:00:18  loss: 0.8316 (0.8316)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0673 (2.0673 -- 2.0673)  data: 1.8847 (1.8847 -- 1.8847)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8488 (1.0042)  acc1: 71.4286 (72.1519)  acc5: 100.0000 (96.2025)  time: 0.3761 (0.1334 -- 2.0673)  data: 0.2095 (0.0001 -- 1.8847)  max mem: 16413
Val: Total time: 0:00:03 (0.3763 s / it)
* Acc@1 76.582 Acc@5 96.203 loss 0.941
Accuracy of the network on the 158 val images: 76.58%
[2023-10-24 00:04:14,765] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:04:14,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:04:14,767] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:04:14,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:04:16,139] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:04:16,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.58%
Epoch: [61]  [ 0/52]  eta: 0:06:44  lr: 0.000038  min_lr: 0.000001  loss: 1.8985 (1.8985)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2091 (8.2091)  time: 7.7742 (7.7742 -- 7.7742)  data: 7.2105 (7.2105 -- 7.2105)  max mem: 16413
Epoch: [61]  [20/52]  eta: 0:00:35  lr: 0.000038  min_lr: 0.000001  loss: 1.9074 (1.9764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3512 (8.1471)  time: 0.7811 (0.5095 -- 3.3575)  data: 0.2172 (0.0006 -- 2.8325)  max mem: 16413
Epoch: [61]  [40/52]  eta: 0:00:12  lr: 0.000038  min_lr: 0.000001  loss: 1.9118 (1.9393)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5349 (8.1324)  time: 0.9649 (0.5262 -- 4.4380)  data: 0.4209 (0.0004 -- 3.8950)  max mem: 16413
Epoch: [61]  [51/52]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.8927 (1.9365)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9429 (8.1618)  time: 0.7899 (0.4942 -- 4.4380)  data: 0.2742 (0.0001 -- 3.8950)  max mem: 16413
Epoch: [61] Total time: 0:00:48 (0.9277 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.8927 (1.9742)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9429 (8.1618)
Val:  [0/9]  eta: 0:00:19  loss: 0.9045 (0.9045)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1597 (2.1597 -- 2.1597)  data: 1.9804 (1.9804 -- 1.9804)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.9045 (0.9996)  acc1: 71.4286 (70.8861)  acc5: 100.0000 (94.9367)  time: 0.3867 (0.1333 -- 2.1597)  data: 0.2201 (0.0001 -- 1.9804)  max mem: 16413
Val: Total time: 0:00:03 (0.3869 s / it)
* Acc@1 73.418 Acc@5 95.570 loss 0.951
Accuracy of the network on the 158 val images: 73.42%
Max accuracy: 76.58%
Epoch: [62]  [ 0/52]  eta: 0:07:21  lr: 0.000038  min_lr: 0.000001  loss: 2.2319 (2.2319)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8708 (10.8708)  time: 8.4916 (8.4916 -- 8.4916)  data: 7.9610 (7.9610 -- 7.9610)  max mem: 16413
Epoch: [62]  [20/52]  eta: 0:00:39  lr: 0.000038  min_lr: 0.000001  loss: 1.9802 (2.0402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8253 (8.8042)  time: 0.8640 (0.5295 -- 3.7677)  data: 0.2633 (0.0001 -- 3.2373)  max mem: 16413
Epoch: [62]  [40/52]  eta: 0:00:12  lr: 0.000037  min_lr: 0.000001  loss: 1.8256 (1.9522)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5143 (8.6193)  time: 0.8281 (0.5121 -- 2.9851)  data: 0.0261 (0.0004 -- 0.4963)  max mem: 16413
Epoch: [62]  [51/52]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.8105 (1.9276)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5143 (8.6274)  time: 0.7330 (0.4939 -- 2.9851)  data: 0.0008 (0.0001 -- 0.0035)  max mem: 16413
Epoch: [62] Total time: 0:00:48 (0.9345 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.8105 (1.9008)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5143 (8.6274)
Val:  [0/9]  eta: 0:00:19  loss: 0.8687 (0.8687)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1296 (2.1296 -- 2.1296)  data: 1.9592 (1.9592 -- 1.9592)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8873 (1.0155)  acc1: 66.6667 (72.1519)  acc5: 100.0000 (96.2025)  time: 0.3834 (0.1326 -- 2.1296)  data: 0.2178 (0.0001 -- 1.9592)  max mem: 16413
Val: Total time: 0:00:03 (0.3835 s / it)
* Acc@1 74.684 Acc@5 96.203 loss 0.954
Accuracy of the network on the 158 val images: 74.68%
Max accuracy: 76.58%
Epoch: [63]  [ 0/52]  eta: 0:08:14  lr: 0.000037  min_lr: 0.000001  loss: 2.1871 (2.1871)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0474 (8.0474)  time: 9.5053 (9.5053 -- 9.5053)  data: 8.9666 (8.9666 -- 8.9666)  max mem: 16413
Epoch: [63]  [20/52]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000001  loss: 1.9921 (1.9331)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0449 (8.6910)  time: 0.7306 (0.5124 -- 3.9884)  data: 0.1806 (0.0004 -- 3.4703)  max mem: 16413
[2023-10-24 00:06:24,645] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:06:24,645] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:06:24,645] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:06:24,645] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:06:36,880] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3310
[2023-10-24 00:06:36,880] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3310
[2023-10-24 00:06:36,880] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:06:36,881] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-24 00:06:36,881] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [63]  [40/52]  eta: 0:00:12  lr: 0.000037  min_lr: 0.000001  loss: 1.8425 (1.9134)  loss_scale: 32768.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1261 (8.2882)  time: 0.8945 (0.5348 -- 3.3791)  data: 0.1754 (0.0006 -- 1.8388)  max mem: 16413
Epoch: [63]  [51/52]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.9769 (1.8915)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3268 (8.4173)  time: 0.7194 (0.4956 -- 2.3489)  data: 0.1223 (0.0001 -- 1.8388)  max mem: 16413
Epoch: [63] Total time: 0:00:47 (0.9150 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.9769 (1.8562)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3268 (8.4173)
Val:  [0/9]  eta: 0:00:19  loss: 0.8584 (0.8584)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1812 (2.1812 -- 2.1812)  data: 2.0034 (2.0034 -- 2.0034)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8860 (1.0072)  acc1: 66.6667 (72.1519)  acc5: 100.0000 (94.9367)  time: 0.3893 (0.1326 -- 2.1812)  data: 0.2227 (0.0001 -- 2.0034)  max mem: 16413
Val: Total time: 0:00:03 (0.3894 s / it)
* Acc@1 75.316 Acc@5 95.570 loss 0.938
Accuracy of the network on the 158 val images: 75.32%
Max accuracy: 76.58%
Epoch: [64]  [ 0/52]  eta: 0:07:25  lr: 0.000037  min_lr: 0.000001  loss: 2.4473 (2.4473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2612 (6.2612)  time: 8.5643 (8.5643 -- 8.5643)  data: 5.6593 (5.6593 -- 5.6593)  max mem: 16413
Epoch: [64]  [20/52]  eta: 0:00:35  lr: 0.000037  min_lr: 0.000001  loss: 1.8839 (1.8337)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5510 (8.2230)  time: 0.7406 (0.5276 -- 2.6070)  data: 0.0017 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [64]  [40/52]  eta: 0:00:12  lr: 0.000037  min_lr: 0.000001  loss: 1.8535 (1.8902)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8986 (8.6487)  time: 0.9637 (0.5316 -- 4.4039)  data: 0.0595 (0.0003 -- 0.5844)  max mem: 16413
Epoch: [64]  [51/52]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 2.1316 (1.9009)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3564 (8.6393)  time: 0.7424 (0.4957 -- 3.8815)  data: 0.0298 (0.0001 -- 0.5842)  max mem: 16413
Epoch: [64] Total time: 0:00:48 (0.9261 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 2.1316 (1.9329)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3564 (8.6393)
Val:  [0/9]  eta: 0:00:18  loss: 0.7734 (0.7734)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0528 (2.0528 -- 2.0528)  data: 1.8813 (1.8813 -- 1.8813)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8672 (0.9981)  acc1: 66.6667 (73.4177)  acc5: 100.0000 (94.9367)  time: 0.3752 (0.1332 -- 2.0528)  data: 0.2091 (0.0001 -- 1.8813)  max mem: 16413
Val: Total time: 0:00:03 (0.3754 s / it)
* Acc@1 77.848 Acc@5 95.570 loss 0.930
Accuracy of the network on the 158 val images: 77.85%
[2023-10-24 00:07:42,620] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:07:42,622] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:07:42,622] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:07:42,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:07:43,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:07:43,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.85%
Epoch: [65]  [ 0/52]  eta: 0:07:36  lr: 0.000037  min_lr: 0.000001  loss: 2.2634 (2.2634)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0221 (7.0221)  time: 8.7707 (8.7707 -- 8.7707)  data: 4.6593 (4.6593 -- 4.6593)  max mem: 16413
Epoch: [65]  [20/52]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000001  loss: 1.7796 (1.8786)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0105 (8.3216)  time: 0.8038 (0.5248 -- 3.3554)  data: 0.0707 (0.0002 -- 1.0441)  max mem: 16413
Epoch: [65]  [40/52]  eta: 0:00:12  lr: 0.000037  min_lr: 0.000001  loss: 1.9302 (1.8945)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8426 (8.3613)  time: 0.8475 (0.5247 -- 4.1028)  data: 0.0543 (0.0003 -- 0.6331)  max mem: 16413
Epoch: [65]  [51/52]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.8629 (1.8787)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5019 (8.1359)  time: 0.6530 (0.4943 -- 2.4599)  data: 0.0235 (0.0001 -- 0.4354)  max mem: 16413
Epoch: [65] Total time: 0:00:47 (0.9119 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.8629 (1.9218)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5019 (8.1359)
Val:  [0/9]  eta: 0:00:19  loss: 0.8149 (0.8149)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1479 (2.1479 -- 2.1479)  data: 1.9679 (1.9679 -- 1.9679)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.9039 (1.0019)  acc1: 71.4286 (73.4177)  acc5: 100.0000 (96.2025)  time: 0.3861 (0.1334 -- 2.1479)  data: 0.2187 (0.0001 -- 1.9679)  max mem: 16413
Val: Total time: 0:00:03 (0.3862 s / it)
* Acc@1 76.582 Acc@5 95.570 loss 0.944
Accuracy of the network on the 158 val images: 76.58%
Max accuracy: 77.85%
Epoch: [66]  [ 0/52]  eta: 0:06:42  lr: 0.000036  min_lr: 0.000001  loss: 1.9144 (1.9144)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0120 (7.0120)  time: 7.7347 (7.7347 -- 7.7347)  data: 7.1999 (7.1999 -- 7.1999)  max mem: 16413
[2023-10-24 00:08:46,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:08:46,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:08:46,351] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:08:46,352] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:08:51,801] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3444
[2023-10-24 00:08:51,801] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3444
[2023-10-24 00:08:51,801] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:08:51,801] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:08:51,802] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [20/52]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000001  loss: 1.8581 (1.8444)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4483 (7.0588)  time: 0.8382 (0.5144 -- 3.2370)  data: 0.2411 (0.0003 -- 2.5770)  max mem: 16413
Epoch: [66]  [40/52]  eta: 0:00:12  lr: 0.000036  min_lr: 0.000001  loss: 1.9528 (1.8959)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5007 (8.1567)  time: 0.8850 (0.5287 -- 2.6273)  data: 0.3069 (0.0005 -- 2.0816)  max mem: 16413
Epoch: [66]  [51/52]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.9528 (1.8997)  loss_scale: 16384.0000 (17959.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4117 (8.1670)  time: 0.7578 (0.4953 -- 2.5795)  data: 0.2024 (0.0001 -- 2.0403)  max mem: 16413
Epoch: [66] Total time: 0:00:48 (0.9302 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.9528 (1.8458)  loss_scale: 16384.0000 (17959.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4117 (8.1670)
Val:  [0/9]  eta: 0:00:18  loss: 0.7396 (0.7396)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0267 (2.0267 -- 2.0267)  data: 1.8465 (1.8465 -- 1.8465)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8912 (0.9949)  acc1: 71.4286 (73.4177)  acc5: 100.0000 (94.9367)  time: 0.3713 (0.1324 -- 2.0267)  data: 0.2052 (0.0001 -- 1.8465)  max mem: 16413
Val: Total time: 0:00:03 (0.3715 s / it)
* Acc@1 77.848 Acc@5 94.304 loss 0.923
Accuracy of the network on the 158 val images: 77.85%
[2023-10-24 00:09:26,521] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:09:26,523] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:09:26,523] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:09:26,523] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:09:27,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:09:27,742] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.85%
Epoch: [67]  [ 0/52]  eta: 0:06:59  lr: 0.000036  min_lr: 0.000001  loss: 1.9241 (1.9241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7481 (7.7481)  time: 8.0602 (8.0602 -- 8.0602)  data: 6.0551 (6.0551 -- 6.0551)  max mem: 16413
Epoch: [67]  [20/52]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000001  loss: 1.7583 (1.7867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9940 (8.5241)  time: 0.8208 (0.5352 -- 3.1183)  data: 0.0018 (0.0002 -- 0.0063)  max mem: 16413
Epoch: [67]  [40/52]  eta: 0:00:12  lr: 0.000036  min_lr: 0.000001  loss: 1.8123 (1.7993)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4736 (8.3078)  time: 0.8753 (0.5068 -- 3.6030)  data: 0.2796 (0.0003 -- 3.0845)  max mem: 16413
Epoch: [67]  [51/52]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.7515 (1.8077)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7475 (8.0657)  time: 0.6956 (0.4944 -- 3.0014)  data: 0.1776 (0.0002 -- 2.4849)  max mem: 16413
Epoch: [67] Total time: 0:00:48 (0.9348 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.7515 (1.8749)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7475 (8.0657)
Val:  [0/9]  eta: 0:00:18  loss: 0.7496 (0.7496)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1083 (2.1083 -- 2.1083)  data: 1.9270 (1.9270 -- 1.9270)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8769 (0.9830)  acc1: 71.4286 (73.4177)  acc5: 100.0000 (94.9367)  time: 0.3805 (0.1334 -- 2.1083)  data: 0.2142 (0.0001 -- 1.9270)  max mem: 16413
Val: Total time: 0:00:03 (0.3806 s / it)
* Acc@1 76.582 Acc@5 95.570 loss 0.903
Accuracy of the network on the 158 val images: 76.58%
Max accuracy: 77.85%
Epoch: [68]  [ 0/52]  eta: 0:06:44  lr: 0.000036  min_lr: 0.000001  loss: 2.0107 (2.0107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3826 (7.3826)  time: 7.7776 (7.7776 -- 7.7776)  data: 7.2386 (7.2386 -- 7.2386)  max mem: 16413
Epoch: [68]  [20/52]  eta: 0:00:39  lr: 0.000036  min_lr: 0.000001  loss: 1.7979 (1.8218)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8234 (8.4472)  time: 0.9089 (0.5102 -- 4.0457)  data: 0.3042 (0.0010 -- 3.4963)  max mem: 16413
[2023-10-24 00:11:02,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:11:02,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:11:02,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:11:02,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [68]  [40/52]  eta: 0:00:12  lr: 0.000036  min_lr: 0.000001  loss: 2.0138 (1.9113)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9215 (8.9158)  time: 0.9052 (0.5133 -- 4.3335)  data: 0.3488 (0.0002 -- 3.8074)  max mem: 16413
Epoch: [68]  [51/52]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8844 (1.9033)  loss_scale: 32768.0000 (21110.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9058 (8.8297)  time: 0.7504 (0.4959 -- 4.3335)  data: 0.2335 (0.0001 -- 3.8074)  max mem: 16413
Epoch: [68] Total time: 0:00:50 (0.9684 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8844 (1.9040)  loss_scale: 32768.0000 (21110.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9058 (8.8297)
Val:  [0/9]  eta: 0:00:18  loss: 0.7131 (0.7131)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0016 (2.0016 -- 2.0016)  data: 1.8108 (1.8108 -- 1.8108)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8165 (0.9612)  acc1: 71.4286 (72.1519)  acc5: 100.0000 (94.9367)  time: 0.3697 (0.1333 -- 2.0016)  data: 0.2013 (0.0001 -- 1.8108)  max mem: 16413
Val: Total time: 0:00:03 (0.3699 s / it)
* Acc@1 77.848 Acc@5 95.570 loss 0.900
Accuracy of the network on the 158 val images: 77.85%
Max accuracy: 77.85%
Epoch: [69]  [ 0/52]  eta: 0:07:12  lr: 0.000035  min_lr: 0.000001  loss: 2.4071 (2.4071)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1668 (6.1668)  time: 8.3151 (8.3151 -- 8.3151)  data: 7.7892 (7.7892 -- 7.7892)  max mem: 16413
[2023-10-24 00:11:22,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3590
[2023-10-24 00:11:22,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3590
[2023-10-24 00:11:22,932] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:11:22,932] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:11:22,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [69]  [20/52]  eta: 0:00:38  lr: 0.000035  min_lr: 0.000001  loss: 1.7331 (1.7307)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3652 (7.6990)  time: 0.8538 (0.5292 -- 3.6321)  data: 0.2350 (0.0003 -- 3.0019)  max mem: 16413
Epoch: [69]  [40/52]  eta: 0:00:12  lr: 0.000035  min_lr: 0.000001  loss: 2.0003 (1.8676)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1353 (8.1168)  time: 0.8501 (0.5096 -- 2.8530)  data: 0.1428 (0.0002 -- 1.5415)  max mem: 16413
Epoch: [69]  [51/52]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 2.0243 (1.8821)  loss_scale: 16384.0000 (17014.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0439 (8.1868)  time: 0.7828 (0.4943 -- 2.6525)  data: 0.0804 (0.0002 -- 1.5415)  max mem: 16413
Epoch: [69] Total time: 0:00:49 (0.9504 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 2.0243 (1.8984)  loss_scale: 16384.0000 (17014.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0439 (8.1868)
Val:  [0/9]  eta: 0:00:19  loss: 0.7173 (0.7173)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1613 (2.1613 -- 2.1613)  data: 1.9802 (1.9802 -- 1.9802)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8373 (0.9740)  acc1: 66.6667 (72.1519)  acc5: 100.0000 (94.9367)  time: 0.3882 (0.1337 -- 2.1613)  data: 0.2201 (0.0001 -- 1.9802)  max mem: 16413
Val: Total time: 0:00:03 (0.3883 s / it)
* Acc@1 78.481 Acc@5 95.570 loss 0.897
Accuracy of the network on the 158 val images: 78.48%
[2023-10-24 00:12:06,470] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:12:06,472] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:12:06,472] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:12:06,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:12:08,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:12:08,053] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.48%
Epoch: [70]  [ 0/52]  eta: 0:06:32  lr: 0.000035  min_lr: 0.000001  loss: 2.2817 (2.2817)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2913 (6.2913)  time: 7.5465 (7.5465 -- 7.5465)  data: 6.9910 (6.9910 -- 6.9910)  max mem: 16413
Epoch: [70]  [20/52]  eta: 0:00:38  lr: 0.000035  min_lr: 0.000001  loss: 1.9501 (2.0046)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2363 (8.5283)  time: 0.8980 (0.5360 -- 5.0320)  data: 0.3472 (0.0004 -- 4.4814)  max mem: 16413
Epoch: [70]  [40/52]  eta: 0:00:12  lr: 0.000035  min_lr: 0.000001  loss: 1.9410 (1.9871)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9135 (8.0638)  time: 0.8060 (0.5085 -- 2.9356)  data: 0.2274 (0.0004 -- 2.4000)  max mem: 16413
Epoch: [70]  [51/52]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.9482 (1.9943)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8373 (7.9880)  time: 0.8291 (0.4946 -- 2.9356)  data: 0.2864 (0.0002 -- 2.4000)  max mem: 16413
Epoch: [70] Total time: 0:00:48 (0.9378 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.9482 (1.9390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8373 (7.9880)
Val:  [0/9]  eta: 0:00:19  loss: 0.6985 (0.6985)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1216 (2.1216 -- 2.1216)  data: 1.9520 (1.9520 -- 1.9520)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8217 (0.9588)  acc1: 66.6667 (73.4177)  acc5: 88.8889 (93.6709)  time: 0.3837 (0.1326 -- 2.1216)  data: 0.2170 (0.0001 -- 1.9520)  max mem: 16413
Val: Total time: 0:00:03 (0.3838 s / it)
* Acc@1 79.114 Acc@5 94.937 loss 0.890
Accuracy of the network on the 158 val images: 79.11%
[2023-10-24 00:13:00,284] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:13:00,286] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:13:00,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:13:00,286] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:13:01,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:13:01,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.11%
Epoch: [71]  [ 0/52]  eta: 0:08:10  lr: 0.000035  min_lr: 0.000001  loss: 1.4797 (1.4797)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9871 (6.9871)  time: 9.4283 (9.4283 -- 9.4283)  data: 8.9151 (8.9151 -- 8.9151)  max mem: 16413
Epoch: [71]  [20/52]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000001  loss: 1.8647 (1.7988)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2608 (8.4380)  time: 0.7507 (0.5337 -- 3.1856)  data: 0.1814 (0.0003 -- 2.6542)  max mem: 16413
[2023-10-24 00:13:32,418] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:13:32,418] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:13:32,419] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:13:32,419] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [71]  [40/52]  eta: 0:00:12  lr: 0.000035  min_lr: 0.000001  loss: 1.9602 (1.8975)  loss_scale: 32768.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6423 (8.2339)  time: 0.8896 (0.5242 -- 2.9859)  data: 0.3159 (0.0003 -- 2.4545)  max mem: 16413
Epoch: [71]  [51/52]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 2.0428 (1.9344)  loss_scale: 32768.0000 (24260.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9422 (8.2703)  time: 0.8072 (0.4945 -- 2.9859)  data: 0.2153 (0.0001 -- 2.4545)  max mem: 16413
Epoch: [71] Total time: 0:00:49 (0.9437 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 2.0428 (1.9325)  loss_scale: 32768.0000 (24260.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9422 (8.2703)
Val:  [0/9]  eta: 0:00:19  loss: 0.7384 (0.7384)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1199 (2.1199 -- 2.1199)  data: 1.9436 (1.9436 -- 1.9436)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7384 (0.9424)  acc1: 77.7778 (74.6835)  acc5: 100.0000 (94.9367)  time: 0.3818 (0.1330 -- 2.1199)  data: 0.2160 (0.0001 -- 1.9436)  max mem: 16413
Val: Total time: 0:00:03 (0.3819 s / it)
* Acc@1 77.848 Acc@5 95.570 loss 0.875
Accuracy of the network on the 158 val images: 77.85%
Max accuracy: 79.11%
Epoch: [72]  [ 0/52]  eta: 0:08:05  lr: 0.000035  min_lr: 0.000001  loss: 1.8096 (1.8096)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4615 (10.4615)  time: 9.3387 (9.3387 -- 9.3387)  data: 6.0933 (6.0933 -- 6.0933)  max mem: 16413
[2023-10-24 00:14:12,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3758
[2023-10-24 00:14:12,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3758
[2023-10-24 00:14:12,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:14:12,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:14:12,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [72]  [20/52]  eta: 0:00:35  lr: 0.000034  min_lr: 0.000001  loss: 1.9920 (1.9743)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4936 (8.0014)  time: 0.6952 (0.5348 -- 1.6189)  data: 0.0552 (0.0008 -- 0.9308)  max mem: 16413
Epoch: [72]  [40/52]  eta: 0:00:12  lr: 0.000034  min_lr: 0.000001  loss: 1.8247 (1.9012)  loss_scale: 16384.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3689 (8.0455)  time: 0.9191 (0.5273 -- 3.1498)  data: 0.1166 (0.0004 -- 0.9216)  max mem: 16413
Epoch: [72]  [51/52]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.8247 (1.9094)  loss_scale: 16384.0000 (20795.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7703 (8.1606)  time: 0.7997 (0.4953 -- 3.1498)  data: 0.0910 (0.0002 -- 1.0067)  max mem: 16413
Epoch: [72] Total time: 0:00:48 (0.9267 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.8247 (1.9036)  loss_scale: 16384.0000 (20795.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7703 (8.1606)
Val:  [0/9]  eta: 0:00:18  loss: 0.6982 (0.6982)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0545 (2.0545 -- 2.0545)  data: 1.8678 (1.8678 -- 1.8678)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8410 (0.9491)  acc1: 77.7778 (74.6835)  acc5: 100.0000 (93.6709)  time: 0.3756 (0.1331 -- 2.0545)  data: 0.2076 (0.0001 -- 1.8678)  max mem: 16413
Val: Total time: 0:00:03 (0.3758 s / it)
* Acc@1 79.747 Acc@5 94.937 loss 0.876
Accuracy of the network on the 158 val images: 79.75%
[2023-10-24 00:14:45,792] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:14:45,793] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:14:45,793] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:14:45,793] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:14:47,177] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:14:47,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.75%
Epoch: [73]  [ 0/52]  eta: 0:05:53  lr: 0.000034  min_lr: 0.000001  loss: 2.3365 (2.3365)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8855 (6.8855)  time: 6.8056 (6.8056 -- 6.8056)  data: 6.2637 (6.2637 -- 6.2637)  max mem: 16413
Epoch: [73]  [20/52]  eta: 0:00:39  lr: 0.000034  min_lr: 0.000001  loss: 1.7992 (1.8152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5266 (8.3143)  time: 0.9547 (0.5247 -- 4.6206)  data: 0.3665 (0.0004 -- 4.1028)  max mem: 16413
Epoch: [73]  [40/52]  eta: 0:00:13  lr: 0.000034  min_lr: 0.000001  loss: 1.9801 (1.8940)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5130 (7.9942)  time: 0.9642 (0.5209 -- 4.5966)  data: 0.4311 (0.0003 -- 4.0719)  max mem: 16413
Epoch: [73]  [51/52]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.8451 (1.8779)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6154 (8.1007)  time: 0.7421 (0.4951 -- 2.8864)  data: 0.2291 (0.0001 -- 2.3665)  max mem: 16413
Epoch: [73] Total time: 0:00:50 (0.9759 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.8451 (1.8907)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6154 (8.1007)
Val:  [0/9]  eta: 0:00:19  loss: 0.7280 (0.7280)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1566 (2.1566 -- 2.1566)  data: 1.9759 (1.9759 -- 1.9759)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8011 (0.9407)  acc1: 77.7778 (73.4177)  acc5: 100.0000 (94.9367)  time: 0.3857 (0.1325 -- 2.1566)  data: 0.2196 (0.0001 -- 1.9759)  max mem: 16413
Val: Total time: 0:00:03 (0.3858 s / it)
* Acc@1 77.215 Acc@5 95.570 loss 0.863
Accuracy of the network on the 158 val images: 77.22%
Max accuracy: 79.75%
Epoch: [74]  [ 0/52]  eta: 0:07:06  lr: 0.000034  min_lr: 0.000001  loss: 2.1957 (2.1957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9549 (9.9549)  time: 8.2040 (8.2040 -- 8.2040)  data: 7.6859 (7.6859 -- 7.6859)  max mem: 16413
Epoch: [74]  [20/52]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000001  loss: 1.6761 (1.8155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1275 (9.0199)  time: 0.7888 (0.5274 -- 2.5865)  data: 0.2117 (0.0004 -- 2.0723)  max mem: 16413
[2023-10-24 00:16:20,542] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:16:20,542] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:16:20,542] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:16:20,543] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [74]  [40/52]  eta: 0:00:12  lr: 0.000034  min_lr: 0.000001  loss: 1.7649 (1.8045)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4028 (8.4841)  time: 0.8538 (0.5287 -- 2.3891)  data: 0.2734 (0.0004 -- 1.8205)  max mem: 16413
[2023-10-24 00:16:25,355] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3892
[2023-10-24 00:16:25,355] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3892
[2023-10-24 00:16:25,355] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:16:25,355] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:16:25,355] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [74]  [51/52]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.8271 (1.8193)  loss_scale: 16384.0000 (17959.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7594 (8.5043)  time: 0.6976 (0.4939 -- 1.9162)  data: 0.1111 (0.0002 -- 1.4031)  max mem: 16413
Epoch: [74] Total time: 0:00:47 (0.9202 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.8271 (1.8430)  loss_scale: 16384.0000 (17959.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7594 (8.5043)
Val:  [0/9]  eta: 0:00:18  loss: 0.6912 (0.6912)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0703 (2.0703 -- 2.0703)  data: 1.8928 (1.8928 -- 1.8928)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8371 (0.9490)  acc1: 66.6667 (73.4177)  acc5: 100.0000 (93.6709)  time: 0.3764 (0.1330 -- 2.0703)  data: 0.2104 (0.0001 -- 1.8928)  max mem: 16413
Val: Total time: 0:00:03 (0.3765 s / it)
* Acc@1 79.114 Acc@5 94.937 loss 0.871
Accuracy of the network on the 158 val images: 79.11%
Max accuracy: 79.75%
Epoch: [75]  [ 0/52]  eta: 0:07:20  lr: 0.000033  min_lr: 0.000001  loss: 1.9932 (1.9932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4277 (8.4277)  time: 8.4676 (8.4676 -- 8.4676)  data: 7.8897 (7.8897 -- 7.8897)  max mem: 16413
Epoch: [75]  [20/52]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000001  loss: 1.9535 (1.8839)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5880 (9.2303)  time: 0.7994 (0.5211 -- 3.9962)  data: 0.2530 (0.0004 -- 3.4488)  max mem: 16413
Epoch: [75]  [40/52]  eta: 0:00:12  lr: 0.000033  min_lr: 0.000001  loss: 1.9067 (1.8310)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7214 (9.0275)  time: 0.9250 (0.5154 -- 3.1826)  data: 0.3404 (0.0001 -- 2.6418)  max mem: 16413
Epoch: [75]  [51/52]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.6594 (1.8065)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9100 (8.6923)  time: 0.8010 (0.4930 -- 3.1826)  data: 0.2794 (0.0001 -- 2.6418)  max mem: 16413
Epoch: [75] Total time: 0:00:49 (0.9465 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.6594 (1.8589)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9100 (8.6923)
Val:  [0/9]  eta: 0:00:19  loss: 0.6821 (0.6821)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1734 (2.1734 -- 2.1734)  data: 2.0020 (2.0020 -- 2.0020)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7776 (0.9420)  acc1: 77.7778 (75.9494)  acc5: 100.0000 (93.6709)  time: 0.3874 (0.1327 -- 2.1734)  data: 0.2225 (0.0001 -- 2.0020)  max mem: 16413
Val: Total time: 0:00:03 (0.3875 s / it)
* Acc@1 79.747 Acc@5 94.937 loss 0.867
Accuracy of the network on the 158 val images: 79.75%
[2023-10-24 00:17:25,359] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:17:25,361] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:17:25,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:17:25,361] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:17:26,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:17:26,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.75%
Epoch: [76]  [ 0/52]  eta: 0:07:15  lr: 0.000033  min_lr: 0.000001  loss: 2.0465 (2.0465)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7654 (7.7654)  time: 8.3774 (8.3774 -- 8.3774)  data: 7.8610 (7.8610 -- 7.8610)  max mem: 16413
Epoch: [76]  [20/52]  eta: 0:00:38  lr: 0.000033  min_lr: 0.000001  loss: 2.0962 (2.0227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5763 (8.3111)  time: 0.8386 (0.5297 -- 3.2167)  data: 0.2446 (0.0002 -- 2.6640)  max mem: 16413
Epoch: [76]  [40/52]  eta: 0:00:11  lr: 0.000033  min_lr: 0.000001  loss: 1.8350 (1.9435)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5194 (8.3324)  time: 0.7532 (0.5231 -- 1.8007)  data: 0.1119 (0.0005 -- 1.2611)  max mem: 16413
[2023-10-24 00:18:12,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=19, lr=[7.803055903427459e-07, 7.803055903427459e-07, 1.0404074537903278e-06, 1.0404074537903278e-06, 1.3872099383871037e-06, 1.3872099383871037e-06, 1.849613251182805e-06, 1.849613251182805e-06, 2.466151001577073e-06, 2.466151001577073e-06, 3.2882013354360974e-06, 3.2882013354360974e-06, 4.38426844724813e-06, 4.38426844724813e-06, 5.845691262997507e-06, 5.845691262997507e-06, 7.794255017330009e-06, 7.794255017330009e-06, 1.039234002310668e-05, 1.039234002310668e-05, 1.3856453364142239e-05, 1.3856453364142239e-05, 1.8475271152189652e-05, 1.8475271152189652e-05, 2.46336948695862e-05, 2.46336948695862e-05, 3.28449264927816e-05, 3.28449264927816e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-24 00:18:12,485] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=19.013899210602883, CurrSamplesPerSec=24.65358415521502, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [76]  [51/52]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.8481 (1.9197)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5194 (8.4009)  time: 0.7370 (0.4979 -- 2.4463)  data: 0.0836 (0.0002 -- 0.7118)  max mem: 16413
Epoch: [76] Total time: 0:00:47 (0.9179 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.8481 (1.9262)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5194 (8.4009)
Val:  [0/9]  eta: 0:00:19  loss: 0.6883 (0.6883)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1789 (2.1789 -- 2.1789)  data: 2.0075 (2.0075 -- 2.0075)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.8104 (0.9349)  acc1: 77.7778 (74.6835)  acc5: 100.0000 (93.6709)  time: 0.3879 (0.1326 -- 2.1789)  data: 0.2231 (0.0001 -- 2.0075)  max mem: 16413
Val: Total time: 0:00:03 (0.3880 s / it)
* Acc@1 79.114 Acc@5 94.937 loss 0.870
Accuracy of the network on the 158 val images: 79.11%
Max accuracy: 79.75%
Epoch: [77]  [ 0/52]  eta: 0:06:43  lr: 0.000033  min_lr: 0.000001  loss: 2.1389 (2.1389)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5713 (6.5713)  time: 7.7563 (7.7563 -- 7.7563)  data: 5.1304 (5.1304 -- 5.1304)  max mem: 16413
[2023-10-24 00:18:40,266] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:18:40,266] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:18:40,266] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:18:40,266] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [77]  [20/52]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 1.8797 (1.8984)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7621 (7.7829)  time: 0.8088 (0.5334 -- 2.6620)  data: 0.0140 (0.0007 -- 0.2466)  max mem: 16413
Epoch: [77]  [40/52]  eta: 0:00:12  lr: 0.000033  min_lr: 0.000001  loss: 1.9677 (1.9408)  loss_scale: 32768.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0422 (8.3559)  time: 0.9224 (0.5297 -- 3.2137)  data: 0.0219 (0.0004 -- 0.3069)  max mem: 16413
Epoch: [77]  [51/52]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.9784 (1.9541)  loss_scale: 32768.0000 (27411.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9774 (8.4530)  time: 0.7562 (0.4972 -- 2.7748)  data: 0.0184 (0.0002 -- 0.3069)  max mem: 16413
Epoch: [77] Total time: 0:00:49 (0.9614 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.9784 (1.9180)  loss_scale: 32768.0000 (27411.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9774 (8.4530)
Val:  [0/9]  eta: 0:00:19  loss: 0.6475 (0.6475)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1620 (2.1620 -- 2.1620)  data: 1.9844 (1.9844 -- 1.9844)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7701 (0.9359)  acc1: 66.6667 (73.4177)  acc5: 100.0000 (93.6709)  time: 0.3868 (0.1336 -- 2.1620)  data: 0.2206 (0.0001 -- 1.9844)  max mem: 16413
Val: Total time: 0:00:03 (0.3870 s / it)
* Acc@1 77.215 Acc@5 94.937 loss 0.870
Accuracy of the network on the 158 val images: 77.22%
Max accuracy: 79.75%
Epoch: [78]  [ 0/52]  eta: 0:06:16  lr: 0.000032  min_lr: 0.000001  loss: 2.2798 (2.2798)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8481 (8.8481)  time: 7.2316 (7.2316 -- 7.2316)  data: 6.1381 (6.1381 -- 6.1381)  max mem: 16413
Epoch: [78]  [20/52]  eta: 0:00:35  lr: 0.000032  min_lr: 0.000001  loss: 1.7353 (1.7754)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6777 (9.1964)  time: 0.8170 (0.5233 -- 2.3644)  data: 0.1827 (0.0005 -- 1.5781)  max mem: 16413
[2023-10-24 00:19:47,623] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4090
[2023-10-24 00:19:47,623] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4090
[2023-10-24 00:19:47,623] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:19:47,623] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:19:47,624] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [78]  [40/52]  eta: 0:00:11  lr: 0.000032  min_lr: 0.000001  loss: 1.9137 (1.8159)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2580 (8.5719)  time: 0.8671 (0.5314 -- 2.7019)  data: 0.1951 (0.0003 -- 2.1594)  max mem: 16413
Epoch: [78]  [51/52]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.8632 (1.8185)  loss_scale: 16384.0000 (27096.6154)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2580 (8.4417)  time: 0.7537 (0.4944 -- 2.0196)  data: 0.0591 (0.0002 -- 1.1675)  max mem: 16413
Epoch: [78] Total time: 0:00:47 (0.9217 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.8632 (1.8901)  loss_scale: 16384.0000 (27096.6154)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2580 (8.4417)
Val:  [0/9]  eta: 0:00:19  loss: 0.6836 (0.6836)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1729 (2.1729 -- 2.1729)  data: 1.9947 (1.9947 -- 1.9947)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7898 (0.9242)  acc1: 77.7778 (75.9494)  acc5: 100.0000 (93.6709)  time: 0.3872 (0.1328 -- 2.1729)  data: 0.2217 (0.0001 -- 1.9947)  max mem: 16413
Val: Total time: 0:00:03 (0.3873 s / it)
* Acc@1 79.114 Acc@5 94.937 loss 0.848
Accuracy of the network on the 158 val images: 79.11%
Max accuracy: 79.75%
Epoch: [79]  [ 0/52]  eta: 0:06:35  lr: 0.000032  min_lr: 0.000001  loss: 2.1516 (2.1516)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3822 (5.3822)  time: 7.6098 (7.6098 -- 7.6098)  data: 6.6724 (6.6724 -- 6.6724)  max mem: 16413
Epoch: [79]  [20/52]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 1.7504 (1.8234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0808 (8.4491)  time: 0.8231 (0.5207 -- 2.6640)  data: 0.1069 (0.0004 -- 1.4526)  max mem: 16413
Epoch: [79]  [40/52]  eta: 0:00:12  lr: 0.000032  min_lr: 0.000001  loss: 1.8539 (1.8208)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1623 (8.5165)  time: 0.8717 (0.5326 -- 2.3520)  data: 0.2376 (0.0003 -- 1.8227)  max mem: 16413
Epoch: [79]  [51/52]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.7900 (1.8304)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1204 (8.6827)  time: 0.6855 (0.4954 -- 2.3520)  data: 0.1580 (0.0002 -- 1.8227)  max mem: 16413
Epoch: [79] Total time: 0:00:47 (0.9097 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.7900 (1.8728)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1204 (8.6827)
[2023-10-24 00:20:50,194] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-10-24 00:20:50,195] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-79/mp_rank_00_model_states.pt
[2023-10-24 00:20:50,195] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-10-24 00:20:50,195] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-79/mp_rank_00_model_states.pt...
[2023-10-24 00:20:51,224] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-79/mp_rank_00_model_states.pt.
[2023-10-24 00:20:51,225] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [0/9]  eta: 0:00:18  loss: 0.6770 (0.6770)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0525 (2.0525 -- 2.0525)  data: 1.8574 (1.8574 -- 1.8574)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7796 (0.9308)  acc1: 77.7778 (74.6835)  acc5: 100.0000 (93.6709)  time: 0.3747 (0.1330 -- 2.0525)  data: 0.2065 (0.0001 -- 1.8574)  max mem: 16413
Val: Total time: 0:00:03 (0.3748 s / it)
* Acc@1 79.114 Acc@5 94.937 loss 0.847
Accuracy of the network on the 158 val images: 79.11%
Max accuracy: 79.75%
Epoch: [80]  [ 0/52]  eta: 0:06:43  lr: 0.000032  min_lr: 0.000001  loss: 1.2319 (1.2319)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3514 (7.3514)  time: 7.7541 (7.7541 -- 7.7541)  data: 7.2111 (7.2111 -- 7.2111)  max mem: 16413
[2023-10-24 00:21:13,337] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4174
[2023-10-24 00:21:13,337] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4174
[2023-10-24 00:21:13,337] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 00:21:13,337] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 00:21:13,337] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [80]  [20/52]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000001  loss: 1.9456 (1.8986)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0382 (7.9246)  time: 0.8281 (0.5215 -- 2.5858)  data: 0.1472 (0.0002 -- 2.0395)  max mem: 16413
Epoch: [80]  [40/52]  eta: 0:00:12  lr: 0.000031  min_lr: 0.000001  loss: 1.7486 (1.8487)  loss_scale: 8192.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4501 (8.0895)  time: 0.8557 (0.5225 -- 3.8650)  data: 0.2838 (0.0003 -- 3.3156)  max mem: 16413
Epoch: [80]  [51/52]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.8591 (1.8816)  loss_scale: 8192.0000 (10397.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2268 (7.9228)  time: 0.7941 (0.4942 -- 3.8650)  data: 0.2709 (0.0001 -- 3.3156)  max mem: 16413
Epoch: [80] Total time: 0:00:48 (0.9366 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.8591 (1.8991)  loss_scale: 8192.0000 (10397.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2268 (7.9228)
Val:  [0/9]  eta: 0:00:18  loss: 0.6534 (0.6534)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0644 (2.0644 -- 2.0644)  data: 1.8882 (1.8882 -- 1.8882)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7589 (0.9233)  acc1: 77.7778 (75.9494)  acc5: 100.0000 (92.4051)  time: 0.3756 (0.1330 -- 2.0644)  data: 0.2099 (0.0001 -- 1.8882)  max mem: 16413
Val: Total time: 0:00:03 (0.3757 s / it)
* Acc@1 78.481 Acc@5 94.304 loss 0.852
Accuracy of the network on the 158 val images: 78.48%
Max accuracy: 79.75%
Epoch: [81]  [ 0/52]  eta: 0:07:35  lr: 0.000031  min_lr: 0.000001  loss: 1.6284 (1.6284)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4000 (6.4000)  time: 8.7570 (8.7570 -- 8.7570)  data: 8.2061 (8.2061 -- 8.2061)  max mem: 16413
Epoch: [81]  [20/52]  eta: 0:00:39  lr: 0.000031  min_lr: 0.000001  loss: 1.8746 (1.8956)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1845 (9.6426)  time: 0.8493 (0.5226 -- 3.6026)  data: 0.2312 (0.0004 -- 3.0766)  max mem: 16413
Epoch: [81]  [40/52]  eta: 0:00:12  lr: 0.000031  min_lr: 0.000001  loss: 1.8485 (1.8761)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4218 (9.3889)  time: 0.8657 (0.5095 -- 3.0265)  data: 0.2252 (0.0001 -- 2.4737)  max mem: 16413
Epoch: [81]  [51/52]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.8485 (1.8607)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5597 (9.2557)  time: 0.7589 (0.4956 -- 3.0265)  data: 0.1547 (0.0001 -- 2.4737)  max mem: 16413
Epoch: [81] Total time: 0:00:49 (0.9471 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.8485 (1.8571)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5597 (9.2557)
Val:  [0/9]  eta: 0:00:18  loss: 0.6136 (0.6136)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1015 (2.1015 -- 2.1015)  data: 1.9229 (1.9229 -- 1.9229)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7401 (0.9061)  acc1: 77.7778 (75.9494)  acc5: 100.0000 (93.6709)  time: 0.3808 (0.1327 -- 2.1015)  data: 0.2137 (0.0001 -- 1.9229)  max mem: 16413
Val: Total time: 0:00:03 (0.3809 s / it)
* Acc@1 79.747 Acc@5 94.937 loss 0.834
Accuracy of the network on the 158 val images: 79.75%
Max accuracy: 79.75%
Epoch: [82]  [ 0/52]  eta: 0:05:22  lr: 0.000031  min_lr: 0.000001  loss: 2.1180 (2.1180)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6702 (6.6702)  time: 6.2057 (6.2057 -- 6.2057)  data: 5.6521 (5.6521 -- 5.6521)  max mem: 16413
Epoch: [82]  [20/52]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.7616 (1.8759)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4708 (8.7138)  time: 0.8862 (0.5251 -- 4.1028)  data: 0.0018 (0.0006 -- 0.0055)  max mem: 16413
[2023-10-24 00:23:19,328] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:23:19,328] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 00:23:19,330] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:23:19,330] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [82]  [40/52]  eta: 0:00:11  lr: 0.000031  min_lr: 0.000001  loss: 1.8087 (1.8699)  loss_scale: 8192.0000 (8591.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7188 (8.4284)  time: 0.8284 (0.5310 -- 2.5826)  data: 0.0016 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [82]  [51/52]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.8312 (1.8804)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0541 (8.4276)  time: 0.8235 (0.4932 -- 3.3477)  data: 0.0009 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [82] Total time: 0:00:50 (0.9620 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.8312 (1.8623)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0541 (8.4276)
Val:  [0/9]  eta: 0:00:19  loss: 0.5718 (0.5718)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1246 (2.1246 -- 2.1246)  data: 1.9495 (1.9495 -- 1.9495)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7077 (0.8857)  acc1: 77.7778 (75.9494)  acc5: 100.0000 (94.9367)  time: 0.3825 (0.1333 -- 2.1246)  data: 0.2167 (0.0001 -- 1.9495)  max mem: 16413
Val: Total time: 0:00:03 (0.3826 s / it)
* Acc@1 79.747 Acc@5 95.570 loss 0.830
Accuracy of the network on the 158 val images: 79.75%
Max accuracy: 79.75%
Epoch: [83]  [ 0/52]  eta: 0:06:31  lr: 0.000031  min_lr: 0.000001  loss: 2.3932 (2.3932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8379 (4.8379)  time: 7.5354 (7.5354 -- 7.5354)  data: 5.6787 (5.6787 -- 5.6787)  max mem: 16413
Epoch: [83]  [20/52]  eta: 0:00:38  lr: 0.000031  min_lr: 0.000001  loss: 2.0058 (1.9977)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7872 (8.0626)  time: 0.8908 (0.5304 -- 3.4481)  data: 0.3096 (0.0004 -- 2.9098)  max mem: 16413
Epoch: [83]  [40/52]  eta: 0:00:12  lr: 0.000030  min_lr: 0.000001  loss: 1.8775 (1.9417)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6444 (8.2761)  time: 0.7953 (0.5272 -- 3.4235)  data: 0.2485 (0.0002 -- 2.9095)  max mem: 16413
Epoch: [83]  [51/52]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9730 (1.9268)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1591 (8.2880)  time: 0.6693 (0.4937 -- 1.5979)  data: 0.1466 (0.0001 -- 1.0801)  max mem: 16413
Epoch: [83] Total time: 0:00:47 (0.9179 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9730 (1.9150)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1591 (8.2880)
Val:  [0/9]  eta: 0:00:19  loss: 0.6543 (0.6543)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1639 (2.1639 -- 2.1639)  data: 1.9908 (1.9908 -- 1.9908)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7289 (0.8898)  acc1: 77.7778 (74.6835)  acc5: 100.0000 (94.9367)  time: 0.3878 (0.1367 -- 2.1639)  data: 0.2213 (0.0001 -- 1.9908)  max mem: 16413
Val: Total time: 0:00:03 (0.3879 s / it)
* Acc@1 79.114 Acc@5 95.570 loss 0.831
Accuracy of the network on the 158 val images: 79.11%
Max accuracy: 79.75%
Epoch: [84]  [ 0/52]  eta: 0:06:04  lr: 0.000030  min_lr: 0.000001  loss: 2.0081 (2.0081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.3985 (13.3985)  time: 7.0039 (7.0039 -- 7.0039)  data: 6.4176 (6.4176 -- 6.4176)  max mem: 16413
Epoch: [84]  [20/52]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000001  loss: 1.8238 (1.8873)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8653 (9.1645)  time: 0.8677 (0.5285 -- 2.6758)  data: 0.1913 (0.0002 -- 2.1354)  max mem: 16413
Epoch: [84]  [40/52]  eta: 0:00:12  lr: 0.000030  min_lr: 0.000001  loss: 1.9706 (1.9349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3902 (8.5463)  time: 0.8887 (0.5371 -- 3.5503)  data: 0.0960 (0.0002 -- 1.0718)  max mem: 16413
Epoch: [84]  [51/52]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9143 (1.9015)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6040 (8.3770)  time: 0.7005 (0.4950 -- 2.3859)  data: 0.0415 (0.0001 -- 0.8101)  max mem: 16413
Epoch: [84] Total time: 0:00:48 (0.9321 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9143 (1.8598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6040 (8.3770)
Val:  [0/9]  eta: 0:00:18  loss: 0.6241 (0.6241)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0685 (2.0685 -- 2.0685)  data: 1.8883 (1.8883 -- 1.8883)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7403 (0.8862)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (93.6709)  time: 0.3765 (0.1320 -- 2.0685)  data: 0.2099 (0.0001 -- 1.8883)  max mem: 16413
Val: Total time: 0:00:03 (0.3766 s / it)
* Acc@1 81.013 Acc@5 94.937 loss 0.826
Accuracy of the network on the 158 val images: 81.01%
[2023-10-24 00:25:15,937] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:25:15,939] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:25:15,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:25:15,939] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:25:17,345] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:25:17,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.01%
Epoch: [85]  [ 0/52]  eta: 0:08:33  lr: 0.000030  min_lr: 0.000001  loss: 1.8664 (1.8664)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8676 (5.8676)  time: 9.8840 (9.8840 -- 9.8840)  data: 9.3507 (9.3507 -- 9.3507)  max mem: 16413
[2023-10-24 00:25:36,415] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:25:36,416] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:25:36,419] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:25:36,420] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [85]  [20/52]  eta: 0:00:39  lr: 0.000030  min_lr: 0.000001  loss: 1.8520 (1.8248)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3850 (7.3762)  time: 0.7914 (0.5185 -- 3.6553)  data: 0.2421 (0.0003 -- 3.1189)  max mem: 16413
[2023-10-24 00:25:51,611] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4449
[2023-10-24 00:25:51,612] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:25:51,612] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4449
[2023-10-24 00:25:51,612] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-24 00:25:51,612] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [85]  [40/52]  eta: 0:00:12  lr: 0.000030  min_lr: 0.000001  loss: 1.8366 (1.8095)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0329 (8.2271)  time: 0.8914 (0.5225 -- 4.1442)  data: 0.3488 (0.0003 -- 3.6012)  max mem: 16413
Epoch: [85]  [51/52]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9405 (1.8641)  loss_scale: 16384.0000 (22055.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0329 (8.4818)  time: 0.6945 (0.4957 -- 3.1012)  data: 0.1780 (0.0000 -- 2.5721)  max mem: 16413
Epoch: [85] Total time: 0:00:49 (0.9472 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9405 (1.8688)  loss_scale: 16384.0000 (22055.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0329 (8.4818)
Val:  [0/9]  eta: 0:00:19  loss: 0.6788 (0.6788)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1439 (2.1439 -- 2.1439)  data: 1.9679 (1.9679 -- 1.9679)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7013 (0.8822)  acc1: 66.6667 (73.4177)  acc5: 100.0000 (93.6709)  time: 0.3850 (0.1331 -- 2.1439)  data: 0.2187 (0.0001 -- 1.9679)  max mem: 16413
Val: Total time: 0:00:03 (0.3851 s / it)
* Acc@1 77.215 Acc@5 95.570 loss 0.812
Accuracy of the network on the 158 val images: 77.22%
Max accuracy: 81.01%
Epoch: [86]  [ 0/52]  eta: 0:05:20  lr: 0.000030  min_lr: 0.000001  loss: 2.2346 (2.2346)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6517 (5.6517)  time: 6.1603 (6.1603 -- 6.1603)  data: 5.6301 (5.6301 -- 5.6301)  max mem: 16413
Epoch: [86]  [20/52]  eta: 0:00:40  lr: 0.000029  min_lr: 0.000001  loss: 1.9119 (1.8640)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5118 (8.3927)  time: 1.0157 (0.5317 -- 3.3081)  data: 0.1858 (0.0004 -- 2.5139)  max mem: 16413
Epoch: [86]  [40/52]  eta: 0:00:12  lr: 0.000029  min_lr: 0.000001  loss: 2.0385 (1.9500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1432 (8.9033)  time: 0.7500 (0.5179 -- 2.2969)  data: 0.1550 (0.0003 -- 1.7395)  max mem: 16413
Epoch: [86]  [51/52]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 2.0653 (1.9439)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5986 (8.8645)  time: 0.7391 (0.4962 -- 2.2969)  data: 0.0883 (0.0001 -- 1.7395)  max mem: 16413
Epoch: [86] Total time: 0:00:48 (0.9351 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 2.0653 (1.8788)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5986 (8.8645)
Val:  [0/9]  eta: 0:00:18  loss: 0.6982 (0.6982)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1092 (2.1092 -- 2.1092)  data: 1.9372 (1.9372 -- 1.9372)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6982 (0.8771)  acc1: 77.7778 (75.9494)  acc5: 100.0000 (93.6709)  time: 0.3824 (0.1331 -- 2.1092)  data: 0.2153 (0.0001 -- 1.9372)  max mem: 16413
Val: Total time: 0:00:03 (0.3825 s / it)
* Acc@1 79.747 Acc@5 94.937 loss 0.825
Accuracy of the network on the 158 val images: 79.75%
Max accuracy: 81.01%
Epoch: [87]  [ 0/52]  eta: 0:06:58  lr: 0.000029  min_lr: 0.000001  loss: 1.2191 (1.2191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8316 (6.8316)  time: 8.0511 (8.0511 -- 8.0511)  data: 7.5227 (7.5227 -- 7.5227)  max mem: 16413
Epoch: [87]  [20/52]  eta: 0:00:38  lr: 0.000029  min_lr: 0.000001  loss: 1.7325 (1.7598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1538 (8.8971)  time: 0.8455 (0.5153 -- 4.3550)  data: 0.2105 (0.0004 -- 3.8238)  max mem: 16413
Epoch: [87]  [40/52]  eta: 0:00:12  lr: 0.000029  min_lr: 0.000001  loss: 2.0093 (1.8728)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5878 (9.1300)  time: 0.8256 (0.5159 -- 2.6629)  data: 0.0894 (0.0004 -- 1.4555)  max mem: 16413
Epoch: [87]  [51/52]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.8859 (1.8652)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0688 (8.8941)  time: 0.7280 (0.4977 -- 2.4682)  data: 0.1018 (0.0002 -- 1.4555)  max mem: 16413
Epoch: [87] Total time: 0:00:47 (0.9224 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.8859 (1.8870)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0688 (8.8941)
Val:  [0/9]  eta: 0:00:19  loss: 0.6621 (0.6621)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1135 (2.1135 -- 2.1135)  data: 1.9371 (1.9371 -- 1.9371)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7095 (0.8894)  acc1: 66.6667 (73.4177)  acc5: 100.0000 (93.6709)  time: 0.3810 (0.1332 -- 2.1135)  data: 0.2153 (0.0001 -- 1.9371)  max mem: 16413
Val: Total time: 0:00:03 (0.3811 s / it)
* Acc@1 77.215 Acc@5 94.937 loss 0.833
Accuracy of the network on the 158 val images: 77.22%
Max accuracy: 81.01%
Epoch: [88]  [ 0/52]  eta: 0:06:20  lr: 0.000029  min_lr: 0.000001  loss: 2.0781 (2.0781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8590 (7.8590)  time: 7.3268 (7.3268 -- 7.3268)  data: 6.1033 (6.1033 -- 6.1033)  max mem: 16413
[2023-10-24 00:28:01,972] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:28:01,973] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:28:01,974] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:28:01,974] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [88]  [20/52]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000001  loss: 1.8391 (1.8731)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6895 (9.0726)  time: 0.8350 (0.5399 -- 3.2815)  data: 0.1394 (0.0004 -- 2.7537)  max mem: 16413
Epoch: [88]  [40/52]  eta: 0:00:12  lr: 0.000029  min_lr: 0.000001  loss: 1.7300 (1.8342)  loss_scale: 32768.0000 (31968.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6117 (8.9070)  time: 0.8844 (0.5302 -- 2.0140)  data: 0.2850 (0.0001 -- 1.4752)  max mem: 16413
[2023-10-24 00:28:35,791] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4617
[2023-10-24 00:28:35,791] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4617
[2023-10-24 00:28:35,791] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:28:35,791] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:28:35,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [88]  [51/52]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.9499 (1.8461)  loss_scale: 16384.0000 (28672.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5986 (8.8324)  time: 0.7888 (0.4938 -- 1.9801)  data: 0.1450 (0.0001 -- 1.1643)  max mem: 16413
Epoch: [88] Total time: 0:00:48 (0.9381 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.9499 (1.8725)  loss_scale: 16384.0000 (28672.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5986 (8.8324)
Val:  [0/9]  eta: 0:00:18  loss: 0.5953 (0.5953)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0475 (2.0475 -- 2.0475)  data: 1.8561 (1.8561 -- 1.8561)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6256 (0.8684)  acc1: 88.8889 (78.4810)  acc5: 100.0000 (94.9367)  time: 0.3744 (0.1336 -- 2.0475)  data: 0.2063 (0.0001 -- 1.8561)  max mem: 16413
Val: Total time: 0:00:03 (0.3746 s / it)
* Acc@1 79.747 Acc@5 95.570 loss 0.811
Accuracy of the network on the 158 val images: 79.75%
Max accuracy: 81.01%
Epoch: [89]  [ 0/52]  eta: 0:06:54  lr: 0.000029  min_lr: 0.000001  loss: 1.6662 (1.6662)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6538 (9.6538)  time: 7.9661 (7.9661 -- 7.9661)  data: 5.9823 (5.9823 -- 5.9823)  max mem: 16413
Epoch: [89]  [20/52]  eta: 0:00:37  lr: 0.000028  min_lr: 0.000001  loss: 1.8589 (1.8363)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2808 (8.3550)  time: 0.8176 (0.5286 -- 3.2079)  data: 0.0342 (0.0003 -- 0.3728)  max mem: 16413
Epoch: [89]  [40/52]  eta: 0:00:12  lr: 0.000028  min_lr: 0.000001  loss: 2.0505 (1.9226)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1038 (8.3682)  time: 0.9125 (0.5227 -- 3.4131)  data: 0.2999 (0.0005 -- 2.9090)  max mem: 16413
Epoch: [89]  [51/52]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.9004 (1.8948)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7968 (8.4022)  time: 0.7822 (0.4946 -- 3.4131)  data: 0.2596 (0.0001 -- 2.9090)  max mem: 16413
Epoch: [89] Total time: 0:00:48 (0.9244 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.9004 (1.9039)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7968 (8.4022)
Val:  [0/9]  eta: 0:00:19  loss: 0.6127 (0.6127)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1488 (2.1488 -- 2.1488)  data: 1.9741 (1.9741 -- 1.9741)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6824 (0.8856)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (93.6709)  time: 0.3860 (0.1331 -- 2.1488)  data: 0.2194 (0.0001 -- 1.9741)  max mem: 16413
Val: Total time: 0:00:03 (0.3862 s / it)
* Acc@1 81.013 Acc@5 94.937 loss 0.813
Accuracy of the network on the 158 val images: 81.01%
[2023-10-24 00:29:37,273] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:29:37,274] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:29:37,274] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:29:37,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:29:38,701] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:29:38,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.01%
Epoch: [90]  [ 0/52]  eta: 0:08:12  lr: 0.000028  min_lr: 0.000001  loss: 1.6672 (1.6672)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5668 (5.5668)  time: 9.4631 (9.4631 -- 9.4631)  data: 5.9290 (5.9290 -- 5.9290)  max mem: 16413
Epoch: [90]  [20/52]  eta: 0:00:38  lr: 0.000028  min_lr: 0.000001  loss: 1.9750 (1.9395)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0852 (9.0007)  time: 0.7836 (0.5191 -- 3.8000)  data: 0.0311 (0.0004 -- 0.5966)  max mem: 16413
Epoch: [90]  [40/52]  eta: 0:00:12  lr: 0.000028  min_lr: 0.000001  loss: 1.9733 (1.9417)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3065 (8.6685)  time: 0.9139 (0.5230 -- 4.5246)  data: 0.0872 (0.0001 -- 1.6483)  max mem: 16413
Epoch: [90]  [51/52]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.8436 (1.8791)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3849 (8.7422)  time: 0.7660 (0.4936 -- 2.6160)  data: 0.0041 (0.0002 -- 0.0635)  max mem: 16413
Epoch: [90] Total time: 0:00:50 (0.9697 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.8436 (1.8546)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3849 (8.7422)
Val:  [0/9]  eta: 0:00:18  loss: 0.6109 (0.6109)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0038 (2.0038 -- 2.0038)  data: 1.8207 (1.8207 -- 1.8207)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6621 (0.8749)  acc1: 88.8889 (78.4810)  acc5: 100.0000 (93.6709)  time: 0.3689 (0.1329 -- 2.0038)  data: 0.2025 (0.0001 -- 1.8207)  max mem: 16413
Val: Total time: 0:00:03 (0.3690 s / it)
* Acc@1 82.911 Acc@5 95.570 loss 0.798
Accuracy of the network on the 158 val images: 82.91%
[2023-10-24 00:30:32,537] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:30:32,539] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:30:32,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:30:32,539] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:30:33,950] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:30:33,950] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.91%
Epoch: [91]  [ 0/52]  eta: 0:06:18  lr: 0.000028  min_lr: 0.000001  loss: 2.4277 (2.4277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4027 (8.4027)  time: 7.2855 (7.2855 -- 7.2855)  data: 6.7687 (6.7687 -- 6.7687)  max mem: 16413
[2023-10-24 00:30:52,633] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:30:52,633] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:30:52,633] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:30:52,633] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [91]  [20/52]  eta: 0:00:38  lr: 0.000028  min_lr: 0.000001  loss: 1.8965 (1.9469)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4943 (8.3076)  time: 0.9065 (0.5269 -- 3.7605)  data: 0.2019 (0.0004 -- 2.7558)  max mem: 16413
Epoch: [91]  [40/52]  eta: 0:00:13  lr: 0.000027  min_lr: 0.000001  loss: 1.9195 (1.9067)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0000 (8.2913)  time: 1.0139 (0.5229 -- 4.6220)  data: 0.4746 (0.0003 -- 4.0840)  max mem: 16413
Epoch: [91]  [51/52]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.8641 (1.9049)  loss_scale: 32768.0000 (28356.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2377 (8.2392)  time: 0.8181 (0.4932 -- 4.6220)  data: 0.3028 (0.0001 -- 4.0840)  max mem: 16413
Epoch: [91] Total time: 0:00:51 (0.9846 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.8641 (1.8693)  loss_scale: 32768.0000 (28356.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2377 (8.2392)
Val:  [0/9]  eta: 0:00:18  loss: 0.5906 (0.5906)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0572 (2.0572 -- 2.0572)  data: 1.8711 (1.8711 -- 1.8711)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6950 (0.8632)  acc1: 88.8889 (79.7468)  acc5: 100.0000 (93.6709)  time: 0.3752 (0.1332 -- 2.0572)  data: 0.2080 (0.0001 -- 1.8711)  max mem: 16413
Val: Total time: 0:00:03 (0.3753 s / it)
* Acc@1 84.177 Acc@5 94.937 loss 0.792
Accuracy of the network on the 158 val images: 84.18%
[2023-10-24 00:31:28,554] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 00:31:28,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 00:31:28,555] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 00:31:28,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 00:31:29,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 00:31:29,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.18%
Epoch: [92]  [ 0/52]  eta: 0:06:41  lr: 0.000027  min_lr: 0.000001  loss: 1.3058 (1.3058)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1008 (9.1008)  time: 7.7262 (7.7262 -- 7.7262)  data: 7.2167 (7.2167 -- 7.2167)  max mem: 16413
Epoch: [92]  [20/52]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000001  loss: 2.0258 (1.9300)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8465 (8.6089)  time: 0.8579 (0.5183 -- 3.8275)  data: 0.3094 (0.0002 -- 3.2974)  max mem: 16413
[2023-10-24 00:31:56,503] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4807
[2023-10-24 00:31:56,503] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4807
[2023-10-24 00:31:56,504] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:31:56,504] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:31:56,504] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [92]  [40/52]  eta: 0:00:12  lr: 0.000027  min_lr: 0.000001  loss: 1.8818 (1.8755)  loss_scale: 16384.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8750 (8.4458)  time: 0.8781 (0.5277 -- 3.2469)  data: 0.3291 (0.0004 -- 2.7289)  max mem: 16413
Epoch: [92]  [51/52]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.9007 (1.8709)  loss_scale: 16384.0000 (23630.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2726 (8.1639)  time: 0.6809 (0.4944 -- 2.2612)  data: 0.1549 (0.0002 -- 1.7438)  max mem: 16413
Epoch: [92] Total time: 0:00:48 (0.9239 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.9007 (1.8664)  loss_scale: 16384.0000 (23630.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2726 (8.1639)
Val:  [0/9]  eta: 0:00:19  loss: 0.5812 (0.5812)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1227 (2.1227 -- 2.1227)  data: 1.9450 (1.9450 -- 1.9450)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6834 (0.8549)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (92.4051)  time: 0.3831 (0.1387 -- 2.1227)  data: 0.2162 (0.0001 -- 1.9450)  max mem: 16413
Val: Total time: 0:00:03 (0.3832 s / it)
* Acc@1 82.278 Acc@5 94.304 loss 0.798
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [93]  [ 0/52]  eta: 0:06:23  lr: 0.000027  min_lr: 0.000001  loss: 1.9672 (1.9672)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5640 (6.5640)  time: 7.3747 (7.3747 -- 7.3747)  data: 6.2659 (6.2659 -- 6.2659)  max mem: 16413
Epoch: [93]  [20/52]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000001  loss: 1.7545 (1.8689)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8172 (8.1921)  time: 0.8546 (0.5269 -- 4.2114)  data: 0.1703 (0.0006 -- 1.8267)  max mem: 16413
Epoch: [93]  [40/52]  eta: 0:00:11  lr: 0.000027  min_lr: 0.000001  loss: 1.6764 (1.7797)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7090 (8.2024)  time: 0.7722 (0.5108 -- 2.8061)  data: 0.2032 (0.0004 -- 2.2345)  max mem: 16413
Epoch: [93]  [51/52]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.9128 (1.8205)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0916 (8.1785)  time: 0.7646 (0.4959 -- 2.8061)  data: 0.1871 (0.0002 -- 2.2345)  max mem: 16413
Epoch: [93] Total time: 0:00:47 (0.9101 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.9128 (1.8418)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0916 (8.1785)
Val:  [0/9]  eta: 0:00:19  loss: 0.5585 (0.5585)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1182 (2.1182 -- 2.1182)  data: 1.9391 (1.9391 -- 1.9391)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6870 (0.8455)  acc1: 85.7143 (77.2152)  acc5: 100.0000 (93.6709)  time: 0.3818 (0.1335 -- 2.1182)  data: 0.2155 (0.0001 -- 1.9391)  max mem: 16413
Val: Total time: 0:00:03 (0.3819 s / it)
* Acc@1 82.911 Acc@5 95.570 loss 0.778
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 84.18%
Epoch: [94]  [ 0/52]  eta: 0:06:41  lr: 0.000027  min_lr: 0.000001  loss: 2.0975 (2.0975)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9687 (5.9687)  time: 7.7212 (7.7212 -- 7.7212)  data: 6.1330 (6.1330 -- 6.1330)  max mem: 16413
Epoch: [94]  [20/52]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000001  loss: 1.8795 (1.9612)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6418 (8.3815)  time: 0.8360 (0.5246 -- 3.4358)  data: 0.1978 (0.0002 -- 2.8905)  max mem: 16413
Epoch: [94]  [40/52]  eta: 0:00:12  lr: 0.000026  min_lr: 0.000001  loss: 1.8121 (1.8649)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0066 (8.1305)  time: 0.9831 (0.5227 -- 5.1968)  data: 0.4415 (0.0004 -- 4.6918)  max mem: 16413
[2023-10-24 00:34:00,346] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:34:00,346] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:34:00,347] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:34:00,347] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [94]  [51/52]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.8347 (1.8841)  loss_scale: 16384.0000 (17644.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6341 (8.1779)  time: 0.8201 (0.4954 -- 5.1968)  data: 0.3080 (0.0001 -- 4.6918)  max mem: 16413
Epoch: [94] Total time: 0:00:49 (0.9541 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.8347 (1.8338)  loss_scale: 16384.0000 (17644.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6341 (8.1779)
Val:  [0/9]  eta: 0:00:19  loss: 0.6129 (0.6129)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1773 (2.1773 -- 2.1773)  data: 1.9952 (1.9952 -- 1.9952)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7077 (0.8775)  acc1: 77.7778 (75.9494)  acc5: 100.0000 (94.9367)  time: 0.3889 (0.1341 -- 2.1773)  data: 0.2218 (0.0001 -- 1.9952)  max mem: 16413
Val: Total time: 0:00:03 (0.3890 s / it)
* Acc@1 82.278 Acc@5 96.203 loss 0.798
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [95]  [ 0/52]  eta: 0:05:42  lr: 0.000026  min_lr: 0.000001  loss: 1.9478 (1.9478)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3114 (10.3114)  time: 6.5808 (6.5808 -- 6.5808)  data: 5.5580 (5.5580 -- 5.5580)  max mem: 16413
Epoch: [95]  [20/52]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000001  loss: 1.8628 (1.8741)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7337 (8.3945)  time: 0.8857 (0.5290 -- 2.5105)  data: 0.2651 (0.0005 -- 1.9651)  max mem: 16413
[2023-10-24 00:34:45,246] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4977
[2023-10-24 00:34:45,246] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4977
[2023-10-24 00:34:45,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:34:45,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:34:45,247] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [95]  [40/52]  eta: 0:00:12  lr: 0.000026  min_lr: 0.000001  loss: 2.0204 (1.8564)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9171 (8.5801)  time: 0.8588 (0.5154 -- 2.7561)  data: 0.0448 (0.0003 -- 0.6899)  max mem: 16413
Epoch: [95]  [51/52]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 2.0725 (1.8834)  loss_scale: 16384.0000 (28041.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5502 (8.4836)  time: 0.7735 (0.4960 -- 2.7561)  data: 0.0043 (0.0002 -- 0.0733)  max mem: 16413
Epoch: [95] Total time: 0:00:49 (0.9571 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 2.0725 (1.8658)  loss_scale: 16384.0000 (28041.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5502 (8.4836)
Val:  [0/9]  eta: 0:00:19  loss: 0.6295 (0.6295)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.2071 (2.2071 -- 2.2071)  data: 2.0295 (2.0295 -- 2.0295)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7063 (0.8681)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (93.6709)  time: 0.3919 (0.1334 -- 2.2071)  data: 0.2256 (0.0001 -- 2.0295)  max mem: 16413
Val: Total time: 0:00:03 (0.3920 s / it)
* Acc@1 82.278 Acc@5 95.570 loss 0.794
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [96]  [ 0/52]  eta: 0:06:41  lr: 0.000026  min_lr: 0.000001  loss: 2.0084 (2.0084)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5788 (9.5788)  time: 7.7119 (7.7119 -- 7.7119)  data: 7.1885 (7.1885 -- 7.1885)  max mem: 16413
[2023-10-24 00:35:10,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=25, lr=[6.143121644174455e-07, 6.143121644174455e-07, 8.190828858899274e-07, 8.190828858899274e-07, 1.0921105145199032e-06, 1.0921105145199032e-06, 1.4561473526932043e-06, 1.4561473526932043e-06, 1.941529803590939e-06, 1.941529803590939e-06, 2.5887064047879188e-06, 2.5887064047879188e-06, 3.451608539717225e-06, 3.451608539717225e-06, 4.602144719622966e-06, 4.602144719622966e-06, 6.136192959497289e-06, 6.136192959497289e-06, 8.181590612663051e-06, 8.181590612663051e-06, 1.0908787483550736e-05, 1.0908787483550736e-05, 1.4545049978067648e-05, 1.4545049978067648e-05, 1.939339997075686e-05, 1.939339997075686e-05, 2.5857866627675817e-05, 2.5857866627675817e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-24 00:35:10,210] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=18.90498055785398, CurrSamplesPerSec=22.02583235271336, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [96]  [20/52]  eta: 0:00:38  lr: 0.000026  min_lr: 0.000001  loss: 1.8293 (1.8734)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0539 (8.0745)  time: 0.8619 (0.5237 -- 4.2277)  data: 0.1420 (0.0005 -- 2.3843)  max mem: 16413
Epoch: [96]  [40/52]  eta: 0:00:12  lr: 0.000026  min_lr: 0.000001  loss: 1.7774 (1.8581)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8256 (7.9503)  time: 0.9349 (0.5233 -- 3.6236)  data: 0.0858 (0.0004 -- 0.9706)  max mem: 16413
Epoch: [96]  [51/52]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.7368 (1.8335)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6871 (8.0802)  time: 0.8143 (0.4954 -- 3.6236)  data: 0.0366 (0.0001 -- 0.7074)  max mem: 16413
Epoch: [96] Total time: 0:00:49 (0.9528 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.7368 (1.8242)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6871 (8.0802)
Val:  [0/9]  eta: 0:00:18  loss: 0.6172 (0.6172)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0777 (2.0777 -- 2.0777)  data: 1.8970 (1.8970 -- 1.8970)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7018 (0.8756)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (94.9367)  time: 0.3783 (0.1329 -- 2.0777)  data: 0.2109 (0.0001 -- 1.8970)  max mem: 16413
Val: Total time: 0:00:03 (0.3784 s / it)
* Acc@1 81.646 Acc@5 96.203 loss 0.800
Accuracy of the network on the 158 val images: 81.65%
Max accuracy: 84.18%
Epoch: [97]  [ 0/52]  eta: 0:06:34  lr: 0.000026  min_lr: 0.000001  loss: 1.9617 (1.9617)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4844 (5.4844)  time: 7.5800 (7.5800 -- 7.5800)  data: 6.9891 (6.9891 -- 6.9891)  max mem: 16413
Epoch: [97]  [20/52]  eta: 0:00:39  lr: 0.000025  min_lr: 0.000001  loss: 1.7639 (1.7706)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4700 (8.9150)  time: 0.9213 (0.5175 -- 2.6967)  data: 0.1765 (0.0005 -- 2.1753)  max mem: 16413
Epoch: [97]  [40/52]  eta: 0:00:12  lr: 0.000025  min_lr: 0.000001  loss: 1.8143 (1.7560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7222 (8.8526)  time: 0.8228 (0.5362 -- 2.6883)  data: 0.0328 (0.0004 -- 0.6019)  max mem: 16413
Epoch: [97]  [51/52]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.8637 (1.7624)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1808 (8.7575)  time: 0.7583 (0.4942 -- 2.5324)  data: 0.0014 (0.0001 -- 0.0127)  max mem: 16413
Epoch: [97] Total time: 0:00:50 (0.9623 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.8637 (1.8613)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1808 (8.7575)
Val:  [0/9]  eta: 0:00:18  loss: 0.6010 (0.6010)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0398 (2.0398 -- 2.0398)  data: 1.8664 (1.8664 -- 1.8664)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6698 (0.8592)  acc1: 88.8889 (79.7468)  acc5: 100.0000 (92.4051)  time: 0.3730 (0.1326 -- 2.0398)  data: 0.2075 (0.0001 -- 1.8664)  max mem: 16413
Val: Total time: 0:00:03 (0.3731 s / it)
* Acc@1 82.278 Acc@5 94.937 loss 0.792
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [98]  [ 0/52]  eta: 0:07:02  lr: 0.000025  min_lr: 0.000001  loss: 1.8983 (1.8983)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1277 (5.1277)  time: 8.1260 (8.1260 -- 8.1260)  data: 7.5731 (7.5731 -- 7.5731)  max mem: 16413
[2023-10-24 00:37:00,770] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:37:00,770] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:37:00,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:37:00,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [98]  [20/52]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000001  loss: 1.7866 (1.8347)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9146 (8.3119)  time: 0.8280 (0.5197 -- 3.7560)  data: 0.2280 (0.0002 -- 3.2224)  max mem: 16413
Epoch: [98]  [40/52]  eta: 0:00:12  lr: 0.000025  min_lr: 0.000001  loss: 1.7910 (1.8466)  loss_scale: 32768.0000 (28771.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5896 (8.0794)  time: 0.8448 (0.5206 -- 3.7226)  data: 0.2376 (0.0004 -- 3.1981)  max mem: 16413
[2023-10-24 00:37:30,640] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5142
[2023-10-24 00:37:30,640] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5142
[2023-10-24 00:37:30,640] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:37:30,640] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:37:30,640] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [98]  [51/52]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.9044 (1.8738)  loss_scale: 32768.0000 (27726.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9252 (8.3221)  time: 0.7119 (0.4849 -- 2.4542)  data: 0.1252 (0.0002 -- 0.9854)  max mem: 16413
Epoch: [98] Total time: 0:00:48 (0.9253 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.9044 (1.8390)  loss_scale: 32768.0000 (27726.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9252 (8.3221)
Val:  [0/9]  eta: 0:00:18  loss: 0.5601 (0.5601)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0947 (2.0947 -- 2.0947)  data: 1.9155 (1.9155 -- 1.9155)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6615 (0.8594)  acc1: 77.7778 (81.0127)  acc5: 100.0000 (92.4051)  time: 0.3799 (0.1333 -- 2.0947)  data: 0.2129 (0.0001 -- 1.9155)  max mem: 16413
Val: Total time: 0:00:03 (0.3800 s / it)
* Acc@1 82.911 Acc@5 94.937 loss 0.799
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 84.18%
Epoch: [99]  [ 0/52]  eta: 0:07:04  lr: 0.000025  min_lr: 0.000001  loss: 1.6820 (1.6820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.7888 (12.7888)  time: 8.1690 (8.1690 -- 8.1690)  data: 7.6356 (7.6356 -- 7.6356)  max mem: 16413
Epoch: [99]  [20/52]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000001  loss: 2.0020 (1.8878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5757 (8.4413)  time: 0.8142 (0.5291 -- 3.2437)  data: 0.2601 (0.0007 -- 2.6855)  max mem: 16413
Epoch: [99]  [40/52]  eta: 0:00:12  lr: 0.000024  min_lr: 0.000001  loss: 1.8243 (1.8800)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3597 (9.0078)  time: 0.8880 (0.5284 -- 3.0099)  data: 0.3386 (0.0004 -- 2.4483)  max mem: 16413
Epoch: [99]  [51/52]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.7970 (1.9002)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0389 (8.9890)  time: 0.7541 (0.4955 -- 2.2101)  data: 0.2252 (0.0001 -- 1.7004)  max mem: 16413
Epoch: [99] Total time: 0:00:48 (0.9243 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.7970 (1.8703)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0389 (8.9890)
[2023-10-24 00:38:24,659] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-10-24 00:38:24,660] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-99/mp_rank_00_model_states.pt
[2023-10-24 00:38:24,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-10-24 00:38:24,660] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-99/mp_rank_00_model_states.pt...
[2023-10-24 00:38:25,630] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-99/mp_rank_00_model_states.pt.
[2023-10-24 00:38:25,631] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [0/9]  eta: 0:00:18  loss: 0.6204 (0.6204)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0873 (2.0873 -- 2.0873)  data: 1.9112 (1.9112 -- 1.9112)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6327 (0.8518)  acc1: 77.7778 (81.0127)  acc5: 100.0000 (94.9367)  time: 0.3785 (0.1329 -- 2.0873)  data: 0.2124 (0.0001 -- 1.9112)  max mem: 16413
Val: Total time: 0:00:03 (0.3786 s / it)
* Acc@1 81.646 Acc@5 95.570 loss 0.799
Accuracy of the network on the 158 val images: 81.65%
Max accuracy: 84.18%
Epoch: [100]  [ 0/52]  eta: 0:06:17  lr: 0.000024  min_lr: 0.000001  loss: 2.4185 (2.4185)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6441 (7.6441)  time: 7.2670 (7.2670 -- 7.2670)  data: 6.2070 (6.2070 -- 6.2070)  max mem: 16413
Epoch: [100]  [20/52]  eta: 0:00:40  lr: 0.000024  min_lr: 0.000001  loss: 2.0510 (2.0176)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3201 (8.5351)  time: 0.9566 (0.5140 -- 6.0117)  data: 0.0687 (0.0001 -- 1.3454)  max mem: 16413
Epoch: [100]  [40/52]  eta: 0:00:12  lr: 0.000024  min_lr: 0.000001  loss: 1.9559 (1.9426)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7426 (8.3260)  time: 0.8352 (0.5208 -- 3.6111)  data: 0.0307 (0.0003 -- 0.5858)  max mem: 16413
Epoch: [100]  [51/52]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.9559 (1.9270)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7438 (8.4621)  time: 0.7300 (0.4943 -- 3.6111)  data: 0.0301 (0.0001 -- 0.5858)  max mem: 16413
Epoch: [100] Total time: 0:00:48 (0.9362 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.9559 (1.8792)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7438 (8.4621)
Val:  [0/9]  eta: 0:00:18  loss: 0.6624 (0.6624)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0755 (2.0755 -- 2.0755)  data: 1.9019 (1.9019 -- 1.9019)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7339 (0.8876)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (96.2025)  time: 0.3780 (0.1334 -- 2.0755)  data: 0.2114 (0.0001 -- 1.9019)  max mem: 16413
Val: Total time: 0:00:03 (0.3781 s / it)
* Acc@1 80.380 Acc@5 96.203 loss 0.813
Accuracy of the network on the 158 val images: 80.38%
Max accuracy: 84.18%
Epoch: [101]  [ 0/52]  eta: 0:06:57  lr: 0.000024  min_lr: 0.000001  loss: 1.5895 (1.5895)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8163 (8.8163)  time: 8.0200 (8.0200 -- 8.0200)  data: 5.3586 (5.3586 -- 5.3586)  max mem: 16413
[2023-10-24 00:39:45,457] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:39:45,457] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:39:45,457] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:39:45,457] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [101]  [20/52]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000001  loss: 1.8403 (1.8948)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4785 (8.8355)  time: 0.8426 (0.5275 -- 3.9015)  data: 0.0018 (0.0004 -- 0.0073)  max mem: 16413
Epoch: [101]  [40/52]  eta: 0:00:12  lr: 0.000024  min_lr: 0.000001  loss: 1.8877 (1.8673)  loss_scale: 32768.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9362 (9.1074)  time: 0.9101 (0.5333 -- 3.3399)  data: 0.0026 (0.0005 -- 0.0126)  max mem: 16413
[2023-10-24 00:40:05,730] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5295
[2023-10-24 00:40:05,730] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5295
[2023-10-24 00:40:05,730] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:40:05,730] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:40:05,730] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [101]  [51/52]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.9502 (1.8917)  loss_scale: 32768.0000 (23945.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5059 (8.9019)  time: 0.7453 (0.4863 -- 3.1507)  data: 0.0011 (0.0001 -- 0.0057)  max mem: 16413
Epoch: [101] Total time: 0:00:48 (0.9357 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.9502 (1.8528)  loss_scale: 32768.0000 (23945.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5059 (8.9019)
Val:  [0/9]  eta: 0:00:19  loss: 0.5974 (0.5974)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1412 (2.1412 -- 2.1412)  data: 1.9601 (1.9601 -- 1.9601)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6783 (0.8736)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (93.6709)  time: 0.3844 (0.1328 -- 2.1412)  data: 0.2179 (0.0001 -- 1.9601)  max mem: 16413
Val: Total time: 0:00:03 (0.3845 s / it)
* Acc@1 82.278 Acc@5 95.570 loss 0.806
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [102]  [ 0/52]  eta: 0:07:17  lr: 0.000024  min_lr: 0.000001  loss: 1.8030 (1.8030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9540 (7.9540)  time: 8.4221 (8.4221 -- 8.4221)  data: 7.9106 (7.9106 -- 7.9106)  max mem: 16413
Epoch: [102]  [20/52]  eta: 0:00:40  lr: 0.000024  min_lr: 0.000001  loss: 1.8179 (1.7626)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0353 (8.7906)  time: 0.8973 (0.5271 -- 3.5784)  data: 0.2406 (0.0006 -- 3.0454)  max mem: 16413
Epoch: [102]  [40/52]  eta: 0:00:12  lr: 0.000023  min_lr: 0.000001  loss: 1.8014 (1.7823)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4365 (8.6975)  time: 0.8445 (0.5320 -- 2.5277)  data: 0.2613 (0.0003 -- 1.9600)  max mem: 16413
Epoch: [102]  [51/52]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9535 (1.8259)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6660 (8.4163)  time: 0.8315 (0.5005 -- 2.5277)  data: 0.1763 (0.0002 -- 1.9600)  max mem: 16413
Epoch: [102] Total time: 0:00:50 (0.9769 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9535 (1.8032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6660 (8.4163)
Val:  [0/9]  eta: 0:00:21  loss: 0.5375 (0.5375)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3621 (2.3621 -- 2.3621)  data: 2.1669 (2.1669 -- 2.1669)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6398 (0.8549)  acc1: 85.7143 (79.7468)  acc5: 100.0000 (94.9367)  time: 0.4252 (0.1460 -- 2.3621)  data: 0.2408 (0.0001 -- 2.1669)  max mem: 16413
Val: Total time: 0:00:03 (0.4253 s / it)
* Acc@1 82.911 Acc@5 95.570 loss 0.784
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 84.18%
Epoch: [103]  [ 0/52]  eta: 0:08:42  lr: 0.000023  min_lr: 0.000001  loss: 2.0952 (2.0952)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9703 (6.9703)  time: 10.0400 (10.0400 -- 10.0400)  data: 9.4969 (9.4969 -- 9.4969)  max mem: 16413
Epoch: [103]  [20/52]  eta: 0:00:42  lr: 0.000023  min_lr: 0.000001  loss: 1.8069 (1.8136)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0973 (8.7220)  time: 0.8923 (0.5306 -- 3.2561)  data: 0.3373 (0.0010 -- 2.7295)  max mem: 16413
Epoch: [103]  [40/52]  eta: 0:00:12  lr: 0.000023  min_lr: 0.000001  loss: 1.8043 (1.8282)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9483 (8.8179)  time: 0.8127 (0.5250 -- 2.1616)  data: 0.2396 (0.0007 -- 1.6347)  max mem: 16413
Epoch: [103]  [51/52]  eta: 0:00:01  lr: 0.000023  min_lr: 0.000001  loss: 1.9482 (1.8325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7872 (8.8669)  time: 0.7614 (0.4969 -- 1.9123)  data: 0.2104 (0.0002 -- 1.3766)  max mem: 16413
Epoch: [103] Total time: 0:00:52 (1.0019 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9482 (1.8317)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7872 (8.8669)
Val:  [0/9]  eta: 0:00:19  loss: 0.5619 (0.5619)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1841 (2.1841 -- 2.1841)  data: 2.0130 (2.0130 -- 2.0130)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7184 (0.8722)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (94.9367)  time: 0.3898 (0.1328 -- 2.1841)  data: 0.2237 (0.0001 -- 2.0130)  max mem: 16413
Val: Total time: 0:00:03 (0.3900 s / it)
* Acc@1 82.278 Acc@5 95.570 loss 0.791
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [104]  [ 0/52]  eta: 0:05:05  lr: 0.000023  min_lr: 0.000001  loss: 1.8177 (1.8177)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1837 (6.1837)  time: 5.8733 (5.8733 -- 5.8733)  data: 5.3392 (5.3392 -- 5.3392)  max mem: 16413
[2023-10-24 00:42:24,108] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:42:24,108] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:42:24,108] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:42:24,108] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [104]  [20/52]  eta: 0:00:39  lr: 0.000023  min_lr: 0.000001  loss: 1.8384 (1.7748)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9095 (8.1708)  time: 0.9926 (0.5368 -- 3.4639)  data: 0.0018 (0.0005 -- 0.0045)  max mem: 16413
Epoch: [104]  [40/52]  eta: 0:00:12  lr: 0.000023  min_lr: 0.000001  loss: 1.9104 (1.8679)  loss_scale: 32768.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9213 (8.8170)  time: 0.8183 (0.5174 -- 3.8293)  data: 0.0018 (0.0002 -- 0.0092)  max mem: 16413
Epoch: [104]  [51/52]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0096 (1.8577)  loss_scale: 32768.0000 (27726.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0685 (8.9103)  time: 0.7419 (0.4963 -- 2.7883)  data: 0.0011 (0.0001 -- 0.0092)  max mem: 16413
Epoch: [104] Total time: 0:00:49 (0.9599 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0096 (1.8352)  loss_scale: 32768.0000 (27726.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0685 (8.9103)
Val:  [0/9]  eta: 0:00:19  loss: 0.6249 (0.6249)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1569 (2.1569 -- 2.1569)  data: 1.9776 (1.9776 -- 1.9776)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7708 (0.8926)  acc1: 71.4286 (74.6835)  acc5: 100.0000 (94.9367)  time: 0.3872 (0.1327 -- 2.1569)  data: 0.2198 (0.0001 -- 1.9776)  max mem: 16413
Val: Total time: 0:00:03 (0.3873 s / it)
* Acc@1 79.747 Acc@5 95.570 loss 0.809
Accuracy of the network on the 158 val images: 79.75%
Max accuracy: 84.18%
Epoch: [105]  [ 0/52]  eta: 0:06:05  lr: 0.000023  min_lr: 0.000001  loss: 1.6359 (1.6359)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7916 (8.7916)  time: 7.0306 (7.0306 -- 7.0306)  data: 6.4684 (6.4684 -- 6.4684)  max mem: 16413
[2023-10-24 00:43:21,184] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5477
[2023-10-24 00:43:21,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:43:21,184] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5477
[2023-10-24 00:43:21,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-24 00:43:21,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [105]  [20/52]  eta: 0:00:39  lr: 0.000022  min_lr: 0.000001  loss: 1.8963 (1.8643)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5711 (8.4509)  time: 0.9434 (0.5165 -- 2.6471)  data: 0.1755 (0.0004 -- 2.0626)  max mem: 16413
Epoch: [105]  [40/52]  eta: 0:00:12  lr: 0.000022  min_lr: 0.000001  loss: 1.7561 (1.8373)  loss_scale: 16384.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8953 (8.6291)  time: 0.9011 (0.5086 -- 3.3103)  data: 0.1198 (0.0004 -- 1.7297)  max mem: 16413
Epoch: [105]  [51/52]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8220 (1.8402)  loss_scale: 16384.0000 (21740.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3458 (8.8399)  time: 0.7841 (0.4950 -- 2.5565)  data: 0.0873 (0.0001 -- 1.7297)  max mem: 16413
Epoch: [105] Total time: 0:00:50 (0.9802 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.8220 (1.8386)  loss_scale: 16384.0000 (21740.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3458 (8.8399)
Val:  [0/9]  eta: 0:00:18  loss: 0.5637 (0.5637)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0583 (2.0583 -- 2.0583)  data: 1.8840 (1.8840 -- 1.8840)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6979 (0.8864)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (94.9367)  time: 0.3750 (0.1328 -- 2.0583)  data: 0.2094 (0.0001 -- 1.8840)  max mem: 16413
Val: Total time: 0:00:03 (0.3752 s / it)
* Acc@1 81.646 Acc@5 95.570 loss 0.793
Accuracy of the network on the 158 val images: 81.65%
Max accuracy: 84.18%
Epoch: [106]  [ 0/52]  eta: 0:06:44  lr: 0.000022  min_lr: 0.000001  loss: 1.4809 (1.4809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7358 (9.7358)  time: 7.7831 (7.7831 -- 7.7831)  data: 6.6885 (6.6885 -- 6.6885)  max mem: 16413
[2023-10-24 00:44:15,302] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5529
[2023-10-24 00:44:15,302] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5529
[2023-10-24 00:44:15,303] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 00:44:15,306] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-10-24 00:44:15,303] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [106]  [20/52]  eta: 0:00:39  lr: 0.000022  min_lr: 0.000001  loss: 1.8670 (1.8331)  loss_scale: 16384.0000 (14823.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3424 (8.2809)  time: 0.8949 (0.5223 -- 2.8182)  data: 0.1367 (0.0004 -- 1.5954)  max mem: 16413
Epoch: [106]  [40/52]  eta: 0:00:12  lr: 0.000022  min_lr: 0.000001  loss: 1.8117 (1.8307)  loss_scale: 8192.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9165 (8.5449)  time: 0.9044 (0.5000 -- 4.5694)  data: 0.0114 (0.0004 -- 0.1875)  max mem: 16413
Epoch: [106]  [51/52]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9349 (1.8131)  loss_scale: 8192.0000 (10870.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4582 (8.9489)  time: 0.7873 (0.4954 -- 4.5694)  data: 0.0099 (0.0001 -- 0.1875)  max mem: 16413
Epoch: [106] Total time: 0:00:50 (0.9736 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9349 (1.8103)  loss_scale: 8192.0000 (10870.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4582 (8.9489)
Val:  [0/9]  eta: 0:00:19  loss: 0.5434 (0.5434)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1292 (2.1292 -- 2.1292)  data: 1.9514 (1.9514 -- 1.9514)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7010 (0.8822)  acc1: 88.8889 (82.2785)  acc5: 100.0000 (94.9367)  time: 0.3828 (0.1328 -- 2.1292)  data: 0.2169 (0.0001 -- 1.9514)  max mem: 16413
Val: Total time: 0:00:03 (0.3829 s / it)
* Acc@1 81.646 Acc@5 95.570 loss 0.790
Accuracy of the network on the 158 val images: 81.65%
Max accuracy: 84.18%
Epoch: [107]  [ 0/52]  eta: 0:05:37  lr: 0.000022  min_lr: 0.000001  loss: 1.6960 (1.6960)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2554 (8.2554)  time: 6.4828 (6.4828 -- 6.4828)  data: 5.9235 (5.9235 -- 5.9235)  max mem: 16413
Epoch: [107]  [20/52]  eta: 0:00:38  lr: 0.000022  min_lr: 0.000001  loss: 1.7293 (1.6652)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5879 (8.8630)  time: 0.9262 (0.5255 -- 3.6711)  data: 0.3699 (0.0005 -- 3.1511)  max mem: 16413
Epoch: [107]  [40/52]  eta: 0:00:12  lr: 0.000021  min_lr: 0.000001  loss: 1.9210 (1.7342)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5616 (8.8328)  time: 0.9266 (0.5094 -- 3.7692)  data: 0.3898 (0.0004 -- 3.2362)  max mem: 16413
Epoch: [107]  [51/52]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.7316 (1.7311)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6612 (8.9320)  time: 0.8425 (0.4971 -- 2.8885)  data: 0.3240 (0.0001 -- 2.3553)  max mem: 16413
Epoch: [107] Total time: 0:00:51 (0.9811 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.7316 (1.8166)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6612 (8.9320)
Val:  [0/9]  eta: 0:00:19  loss: 0.5641 (0.5641)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1277 (2.1277 -- 2.1277)  data: 1.9559 (1.9559 -- 1.9559)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6809 (0.8641)  acc1: 88.8889 (79.7468)  acc5: 100.0000 (96.2025)  time: 0.3818 (0.1323 -- 2.1277)  data: 0.2174 (0.0001 -- 1.9559)  max mem: 16413
Val: Total time: 0:00:03 (0.3819 s / it)
* Acc@1 82.278 Acc@5 96.203 loss 0.788
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [108]  [ 0/52]  eta: 0:06:33  lr: 0.000021  min_lr: 0.000001  loss: 1.6562 (1.6562)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0905 (13.0905)  time: 7.5662 (7.5662 -- 7.5662)  data: 6.1985 (6.1985 -- 6.1985)  max mem: 16413
Epoch: [108]  [20/52]  eta: 0:00:35  lr: 0.000021  min_lr: 0.000001  loss: 1.7211 (1.8255)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7371 (8.9079)  time: 0.7927 (0.5263 -- 2.9242)  data: 0.0678 (0.0003 -- 0.4860)  max mem: 16413
Epoch: [108]  [40/52]  eta: 0:00:12  lr: 0.000021  min_lr: 0.000001  loss: 1.7267 (1.8035)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8276 (8.9803)  time: 0.8813 (0.5221 -- 3.6846)  data: 0.2083 (0.0005 -- 3.1468)  max mem: 16413
[2023-10-24 00:46:23,770] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:46:23,770] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:46:23,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 00:46:23,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [108]  [51/52]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.7862 (1.7915)  loss_scale: 8192.0000 (9767.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5340 (9.0581)  time: 0.7224 (0.4940 -- 2.4167)  data: 0.1822 (0.0002 -- 1.9017)  max mem: 16413
Epoch: [108] Total time: 0:00:49 (0.9476 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.7862 (1.7882)  loss_scale: 8192.0000 (9767.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5340 (9.0581)
Val:  [0/9]  eta: 0:00:19  loss: 0.5829 (0.5829)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1317 (2.1317 -- 2.1317)  data: 1.9540 (1.9540 -- 1.9540)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7338 (0.8732)  acc1: 85.7143 (79.7468)  acc5: 100.0000 (94.9367)  time: 0.3824 (0.1322 -- 2.1317)  data: 0.2172 (0.0001 -- 1.9540)  max mem: 16413
Val: Total time: 0:00:03 (0.3825 s / it)
* Acc@1 83.544 Acc@5 95.570 loss 0.787
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 84.18%
Epoch: [109]  [ 0/52]  eta: 0:06:16  lr: 0.000021  min_lr: 0.000000  loss: 1.6504 (1.6504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8709 (9.8709)  time: 7.2355 (7.2355 -- 7.2355)  data: 6.0567 (6.0567 -- 6.0567)  max mem: 16413
Epoch: [109]  [20/52]  eta: 0:00:40  lr: 0.000021  min_lr: 0.000000  loss: 1.9759 (1.9698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2204 (8.7473)  time: 0.9581 (0.5194 -- 5.2617)  data: 0.4069 (0.0003 -- 4.7176)  max mem: 16413
Epoch: [109]  [40/52]  eta: 0:00:12  lr: 0.000021  min_lr: 0.000000  loss: 1.8614 (1.9046)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8026 (8.4431)  time: 0.8950 (0.5240 -- 3.4384)  data: 0.3565 (0.0004 -- 2.9095)  max mem: 16413
Epoch: [109]  [51/52]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.7935 (1.8724)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3850 (8.9669)  time: 0.7386 (0.4943 -- 3.4384)  data: 0.2223 (0.0001 -- 2.9095)  max mem: 16413
Epoch: [109] Total time: 0:00:49 (0.9613 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.7935 (1.8270)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3850 (8.9669)
Val:  [0/9]  eta: 0:00:18  loss: 0.5621 (0.5621)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0559 (2.0559 -- 2.0559)  data: 1.8646 (1.8646 -- 1.8646)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6732 (0.8672)  acc1: 88.8889 (79.7468)  acc5: 100.0000 (94.9367)  time: 0.3752 (0.1330 -- 2.0559)  data: 0.2073 (0.0001 -- 1.8646)  max mem: 16413
Val: Total time: 0:00:03 (0.3753 s / it)
* Acc@1 82.911 Acc@5 95.570 loss 0.782
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 84.18%
Epoch: [110]  [ 0/52]  eta: 0:06:34  lr: 0.000021  min_lr: 0.000000  loss: 1.9953 (1.9953)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4719 (8.4719)  time: 7.5839 (7.5839 -- 7.5839)  data: 5.4673 (5.4673 -- 5.4673)  max mem: 16413
Epoch: [110]  [20/52]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.6644 (1.7459)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7933 (8.6512)  time: 0.8302 (0.5277 -- 2.4996)  data: 0.1398 (0.0003 -- 1.3221)  max mem: 16413
Epoch: [110]  [40/52]  eta: 0:00:12  lr: 0.000020  min_lr: 0.000000  loss: 1.8711 (1.8235)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5327 (8.9599)  time: 0.8722 (0.5213 -- 2.2043)  data: 0.2576 (0.0003 -- 1.6498)  max mem: 16413
Epoch: [110]  [51/52]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.8974 (1.8450)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5147 (8.9792)  time: 0.7505 (0.4931 -- 2.1106)  data: 0.2257 (0.0001 -- 1.5812)  max mem: 16413
Epoch: [110] Total time: 0:00:48 (0.9402 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.8974 (1.8362)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5147 (8.9792)
Val:  [0/9]  eta: 0:00:18  loss: 0.5688 (0.5688)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0576 (2.0576 -- 2.0576)  data: 1.8807 (1.8807 -- 1.8807)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6626 (0.8625)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (96.2025)  time: 0.3750 (0.1333 -- 2.0576)  data: 0.2090 (0.0001 -- 1.8807)  max mem: 16413
Val: Total time: 0:00:03 (0.3752 s / it)
* Acc@1 82.278 Acc@5 96.203 loss 0.782
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [111]  [ 0/52]  eta: 0:07:45  lr: 0.000020  min_lr: 0.000000  loss: 1.5184 (1.5184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.0948 (14.0948)  time: 8.9532 (8.9532 -- 8.9532)  data: 6.5953 (6.5953 -- 6.5953)  max mem: 16413
[2023-10-24 00:48:37,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:48:37,539] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:48:37,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:48:37,539] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:48:39,957] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5789
[2023-10-24 00:48:39,957] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5789
[2023-10-24 00:48:39,957] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:48:39,957] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:48:39,957] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [111]  [20/52]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000000  loss: 1.6777 (1.7050)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4738 (8.6524)  time: 0.7987 (0.5226 -- 2.1133)  data: 0.1378 (0.0006 -- 1.1905)  max mem: 16413
Epoch: [111]  [40/52]  eta: 0:00:12  lr: 0.000020  min_lr: 0.000000  loss: 1.9793 (1.7913)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4311 (8.8106)  time: 0.8726 (0.5207 -- 2.1060)  data: 0.1827 (0.0004 -- 1.4463)  max mem: 16413
Epoch: [111]  [51/52]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.0413 (1.8434)  loss_scale: 16384.0000 (17329.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3563 (8.9624)  time: 0.7598 (0.5012 -- 1.7509)  data: 0.1213 (0.0002 -- 1.1385)  max mem: 16413
Epoch: [111] Total time: 0:00:49 (0.9452 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.0413 (1.8062)  loss_scale: 16384.0000 (17329.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3563 (8.9624)
Val:  [0/9]  eta: 0:00:19  loss: 0.5302 (0.5302)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1259 (2.1259 -- 2.1259)  data: 1.9451 (1.9451 -- 1.9451)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7700 (0.8769)  acc1: 77.7778 (79.7468)  acc5: 100.0000 (94.9367)  time: 0.3841 (0.1327 -- 2.1259)  data: 0.2162 (0.0001 -- 1.9451)  max mem: 16413
Val: Total time: 0:00:03 (0.3842 s / it)
* Acc@1 82.911 Acc@5 95.570 loss 0.784
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 84.18%
Epoch: [112]  [ 0/52]  eta: 0:07:15  lr: 0.000020  min_lr: 0.000000  loss: 1.7531 (1.7531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5906 (9.5906)  time: 8.3819 (8.3819 -- 8.3819)  data: 7.8532 (7.8532 -- 7.8532)  max mem: 16413
Epoch: [112]  [20/52]  eta: 0:00:41  lr: 0.000020  min_lr: 0.000000  loss: 1.7996 (1.7618)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9953 (8.4222)  time: 0.9346 (0.5171 -- 4.6391)  data: 0.3735 (0.0003 -- 4.1018)  max mem: 16413
Epoch: [112]  [40/52]  eta: 0:00:12  lr: 0.000020  min_lr: 0.000000  loss: 1.7937 (1.8286)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8006 (8.3919)  time: 0.7753 (0.5193 -- 2.2684)  data: 0.2349 (0.0004 -- 1.7591)  max mem: 16413
Epoch: [112]  [51/52]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9403 (1.8544)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8635 (8.5431)  time: 0.7196 (0.4972 -- 2.2684)  data: 0.1955 (0.0001 -- 1.7591)  max mem: 16413
Epoch: [112] Total time: 0:00:48 (0.9352 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9403 (1.8482)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8635 (8.5431)
Val:  [0/9]  eta: 0:00:18  loss: 0.5033 (0.5033)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0922 (2.0922 -- 2.0922)  data: 1.9136 (1.9136 -- 1.9136)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6968 (0.8535)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (96.2025)  time: 0.3804 (0.1339 -- 2.0922)  data: 0.2127 (0.0001 -- 1.9136)  max mem: 16413
Val: Total time: 0:00:03 (0.3805 s / it)
* Acc@1 80.380 Acc@5 96.203 loss 0.772
Accuracy of the network on the 158 val images: 80.38%
Max accuracy: 84.18%
Epoch: [113]  [ 0/52]  eta: 0:06:26  lr: 0.000020  min_lr: 0.000000  loss: 1.1652 (1.1652)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7997 (7.7997)  time: 7.4329 (7.4329 -- 7.4329)  data: 6.7797 (6.7797 -- 6.7797)  max mem: 16413
[2023-10-24 00:50:25,637] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5894
[2023-10-24 00:50:25,637] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5894
[2023-10-24 00:50:25,637] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 00:50:25,637] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 00:50:25,637] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [113]  [20/52]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.7396 (1.8218)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2477 (8.7145)  time: 0.8168 (0.5055 -- 2.7504)  data: 0.2623 (0.0007 -- 2.2439)  max mem: 16413
Epoch: [113]  [40/52]  eta: 0:00:12  lr: 0.000019  min_lr: 0.000000  loss: 1.8168 (1.8348)  loss_scale: 8192.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6477 (8.7033)  time: 0.9252 (0.5097 -- 3.5916)  data: 0.3804 (0.0004 -- 3.0656)  max mem: 16413
Epoch: [113]  [51/52]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.8168 (1.8255)  loss_scale: 8192.0000 (11027.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5566 (8.8236)  time: 0.7534 (0.4979 -- 3.5916)  data: 0.2355 (0.0001 -- 3.0656)  max mem: 16413
Epoch: [113] Total time: 0:00:49 (0.9509 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.8168 (1.8122)  loss_scale: 8192.0000 (11027.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5566 (8.8236)
Val:  [0/9]  eta: 0:00:19  loss: 0.5089 (0.5089)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1655 (2.1655 -- 2.1655)  data: 1.9863 (1.9863 -- 1.9863)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7269 (0.8641)  acc1: 71.4286 (77.2152)  acc5: 100.0000 (96.2025)  time: 0.3884 (0.1333 -- 2.1655)  data: 0.2208 (0.0001 -- 1.9863)  max mem: 16413
Val: Total time: 0:00:03 (0.3885 s / it)
* Acc@1 81.013 Acc@5 96.203 loss 0.768
Accuracy of the network on the 158 val images: 81.01%
Max accuracy: 84.18%
Epoch: [114]  [ 0/52]  eta: 0:07:44  lr: 0.000019  min_lr: 0.000000  loss: 2.0361 (2.0361)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2404 (4.2404)  time: 8.9345 (8.9345 -- 8.9345)  data: 8.3906 (8.3906 -- 8.3906)  max mem: 16413
Epoch: [114]  [20/52]  eta: 0:00:40  lr: 0.000019  min_lr: 0.000000  loss: 1.9424 (1.9429)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1685 (8.6547)  time: 0.8752 (0.5243 -- 3.3155)  data: 0.0896 (0.0003 -- 1.7584)  max mem: 16413
Epoch: [114]  [40/52]  eta: 0:00:13  lr: 0.000019  min_lr: 0.000000  loss: 1.8939 (1.9023)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6483 (8.9699)  time: 0.9311 (0.5234 -- 2.7975)  data: 0.0019 (0.0003 -- 0.0147)  max mem: 16413
Epoch: [114]  [51/52]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.8636 (1.8775)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3789 (8.7661)  time: 0.7319 (0.4950 -- 2.7975)  data: 0.0014 (0.0001 -- 0.0147)  max mem: 16413
Epoch: [114] Total time: 0:00:50 (0.9731 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.8636 (1.8908)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3789 (8.7661)
Val:  [0/9]  eta: 0:00:19  loss: 0.5043 (0.5043)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1700 (2.1700 -- 2.1700)  data: 1.9942 (1.9942 -- 1.9942)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7516 (0.8643)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (97.4684)  time: 0.3873 (0.1330 -- 2.1700)  data: 0.2217 (0.0001 -- 1.9942)  max mem: 16413
Val: Total time: 0:00:03 (0.3874 s / it)
* Acc@1 81.013 Acc@5 96.835 loss 0.777
Accuracy of the network on the 158 val images: 81.01%
Max accuracy: 84.18%
Epoch: [115]  [ 0/52]  eta: 0:06:03  lr: 0.000019  min_lr: 0.000000  loss: 1.8717 (1.8717)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3242 (9.3242)  time: 6.9970 (6.9970 -- 6.9970)  data: 6.4068 (6.4068 -- 6.4068)  max mem: 16413
[2023-10-24 00:52:13,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=31, lr=[4.4289624932974196e-07, 4.4289624932974196e-07, 5.905283324396559e-07, 5.905283324396559e-07, 7.873711099195413e-07, 7.873711099195413e-07, 1.0498281465593883e-06, 1.0498281465593883e-06, 1.3997708620791843e-06, 1.3997708620791843e-06, 1.8663611494389126e-06, 1.8663611494389126e-06, 2.4884815325852167e-06, 2.4884815325852167e-06, 3.317975376780289e-06, 3.317975376780289e-06, 4.423967169040385e-06, 4.423967169040385e-06, 5.898622892053847e-06, 5.898622892053847e-06, 7.864830522738462e-06, 7.864830522738462e-06, 1.0486440696984618e-05, 1.0486440696984618e-05, 1.3981920929312824e-05, 1.3981920929312824e-05, 1.8642561239083764e-05, 1.8642561239083764e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-24 00:52:13,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=18.786773235645125, CurrSamplesPerSec=21.86382838607833, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [115]  [20/52]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8067 (1.7474)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0609 (8.2709)  time: 0.8573 (0.5313 -- 3.7051)  data: 0.1846 (0.0003 -- 2.8793)  max mem: 16413
Epoch: [115]  [40/52]  eta: 0:00:12  lr: 0.000018  min_lr: 0.000000  loss: 1.8066 (1.7798)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6077 (8.6794)  time: 0.9792 (0.5243 -- 4.0427)  data: 0.0482 (0.0002 -- 0.5163)  max mem: 16413
[2023-10-24 00:52:35,267] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:52:35,267] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 00:52:35,269] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:52:35,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [115]  [51/52]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7039 (1.7938)  loss_scale: 8192.0000 (9609.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6612 (8.8352)  time: 0.8230 (0.4993 -- 3.6502)  data: 0.0475 (0.0002 -- 0.5163)  max mem: 16413
Epoch: [115] Total time: 0:00:50 (0.9653 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7039 (1.8418)  loss_scale: 8192.0000 (9609.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6612 (8.8352)
Val:  [0/9]  eta: 0:00:18  loss: 0.5016 (0.5016)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0896 (2.0896 -- 2.0896)  data: 1.9096 (1.9096 -- 1.9096)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7345 (0.8564)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (96.2025)  time: 0.3788 (0.1331 -- 2.0896)  data: 0.2122 (0.0001 -- 1.9096)  max mem: 16413
Val: Total time: 0:00:03 (0.3790 s / it)
* Acc@1 81.646 Acc@5 96.203 loss 0.768
Accuracy of the network on the 158 val images: 81.65%
Max accuracy: 84.18%
Epoch: [116]  [ 0/52]  eta: 0:07:50  lr: 0.000018  min_lr: 0.000000  loss: 1.4531 (1.4531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4149 (9.4149)  time: 9.0471 (9.0471 -- 9.0471)  data: 7.1753 (7.1753 -- 7.1753)  max mem: 16413
Epoch: [116]  [20/52]  eta: 0:00:38  lr: 0.000018  min_lr: 0.000000  loss: 1.7391 (1.7435)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0703 (8.4673)  time: 0.8026 (0.5232 -- 3.3278)  data: 0.2050 (0.0004 -- 2.3387)  max mem: 16413
Epoch: [116]  [40/52]  eta: 0:00:12  lr: 0.000018  min_lr: 0.000000  loss: 1.8554 (1.7944)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5807 (8.4554)  time: 0.9146 (0.4980 -- 4.0453)  data: 0.0974 (0.0003 -- 1.5085)  max mem: 16413
Epoch: [116]  [51/52]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9327 (1.8478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5807 (8.3843)  time: 0.7037 (0.4948 -- 4.0453)  data: 0.0216 (0.0001 -- 0.4197)  max mem: 16413
Epoch: [116] Total time: 0:00:48 (0.9408 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9327 (1.8478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5807 (8.3843)
Val:  [0/9]  eta: 0:00:19  loss: 0.4923 (0.4923)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2036 (2.2036 -- 2.2036)  data: 2.0335 (2.0335 -- 2.0335)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7993 (0.8687)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (96.2025)  time: 0.3934 (0.1333 -- 2.2036)  data: 0.2260 (0.0001 -- 2.0335)  max mem: 16413
Val: Total time: 0:00:03 (0.3935 s / it)
* Acc@1 81.646 Acc@5 96.203 loss 0.776
Accuracy of the network on the 158 val images: 81.65%
Max accuracy: 84.18%
Epoch: [117]  [ 0/52]  eta: 0:05:23  lr: 0.000018  min_lr: 0.000000  loss: 1.3813 (1.3813)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7203 (6.7203)  time: 6.2287 (6.2287 -- 6.2287)  data: 4.6261 (4.6261 -- 4.6261)  max mem: 16413
Epoch: [117]  [20/52]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.7814 (1.7773)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8449 (8.9475)  time: 0.9332 (0.5308 -- 2.3534)  data: 0.1877 (0.0003 -- 1.7759)  max mem: 16413
Epoch: [117]  [40/52]  eta: 0:00:12  lr: 0.000018  min_lr: 0.000000  loss: 1.8473 (1.7824)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2591 (8.6256)  time: 0.9533 (0.5259 -- 4.1453)  data: 0.3119 (0.0002 -- 3.6111)  max mem: 16413
Epoch: [117]  [51/52]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9097 (1.7915)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0253 (8.8846)  time: 0.7334 (0.4986 -- 3.1529)  data: 0.1309 (0.0001 -- 2.5961)  max mem: 16413
Epoch: [117] Total time: 0:00:49 (0.9531 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9097 (1.8133)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0253 (8.8846)
Val:  [0/9]  eta: 0:00:18  loss: 0.5226 (0.5226)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0218 (2.0218 -- 2.0218)  data: 1.8241 (1.8241 -- 1.8241)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7666 (0.8590)  acc1: 77.7778 (75.9494)  acc5: 100.0000 (97.4684)  time: 0.3726 (0.1328 -- 2.0218)  data: 0.2028 (0.0001 -- 1.8241)  max mem: 16413
Val: Total time: 0:00:03 (0.3727 s / it)
* Acc@1 81.013 Acc@5 96.835 loss 0.770
Accuracy of the network on the 158 val images: 81.01%
Max accuracy: 84.18%
Epoch: [118]  [ 0/52]  eta: 0:06:12  lr: 0.000018  min_lr: 0.000000  loss: 1.1882 (1.1882)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8050 (10.8051)  time: 7.1543 (7.1543 -- 7.1543)  data: 6.5813 (6.5813 -- 6.5813)  max mem: 16413
[2023-10-24 00:54:48,138] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:54:48,138] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 00:54:48,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:54:48,140] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [118]  [20/52]  eta: 0:00:40  lr: 0.000018  min_lr: 0.000000  loss: 1.7766 (1.6918)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1149 (8.5257)  time: 0.9734 (0.5143 -- 3.6474)  data: 0.1112 (0.0004 -- 1.1828)  max mem: 16413
Epoch: [118]  [40/52]  eta: 0:00:12  lr: 0.000017  min_lr: 0.000000  loss: 1.7436 (1.7355)  loss_scale: 32768.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9339 (8.3806)  time: 0.7759 (0.5291 -- 3.0787)  data: 0.0103 (0.0001 -- 0.1546)  max mem: 16413
[2023-10-24 00:55:18,861] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6186
[2023-10-24 00:55:18,861] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6186
[2023-10-24 00:55:18,861] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:55:18,861] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 00:55:18,861] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [118]  [51/52]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8949 (1.7704)  loss_scale: 32768.0000 (27411.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1466 (8.4739)  time: 0.7466 (0.4857 -- 2.9605)  data: 0.0096 (0.0001 -- 0.1546)  max mem: 16413
Epoch: [118] Total time: 0:00:50 (0.9664 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8949 (1.7854)  loss_scale: 32768.0000 (27411.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1466 (8.4739)
Val:  [0/9]  eta: 0:00:19  loss: 0.5709 (0.5709)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1414 (2.1414 -- 2.1414)  data: 1.9653 (1.9653 -- 1.9653)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7755 (0.8625)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (97.4684)  time: 0.3843 (0.1329 -- 2.1414)  data: 0.2184 (0.0001 -- 1.9653)  max mem: 16413
Val: Total time: 0:00:03 (0.3844 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.772
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 84.18%
Epoch: [119]  [ 0/52]  eta: 0:05:37  lr: 0.000017  min_lr: 0.000000  loss: 2.2889 (2.2889)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0970 (8.0970)  time: 6.4966 (6.4966 -- 6.4966)  data: 5.9566 (5.9566 -- 5.9566)  max mem: 16413
Epoch: [119]  [20/52]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.9010 (1.9012)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6522 (7.9080)  time: 0.9214 (0.5146 -- 3.7517)  data: 0.3771 (0.0006 -- 3.2007)  max mem: 16413
Epoch: [119]  [40/52]  eta: 0:00:11  lr: 0.000017  min_lr: 0.000000  loss: 1.6853 (1.8152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6509 (7.8535)  time: 0.7983 (0.5096 -- 3.7143)  data: 0.2235 (0.0003 -- 3.1645)  max mem: 16413
Epoch: [119]  [51/52]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.6549 (1.8072)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9547 (7.9519)  time: 0.7282 (0.4987 -- 1.9752)  data: 0.1373 (0.0002 -- 1.4555)  max mem: 16413
Epoch: [119] Total time: 0:00:48 (0.9389 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.6549 (1.8367)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9547 (7.9519)
[2023-10-24 00:56:11,668] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-10-24 00:56:11,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-10-24 00:56:11,672] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-119/mp_rank_00_model_states.pt
[2023-10-24 00:56:11,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-119/mp_rank_00_model_states.pt...
[2023-10-24 00:56:12,770] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-119/mp_rank_00_model_states.pt.
[2023-10-24 00:56:12,770] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [0/9]  eta: 0:00:17  loss: 0.5902 (0.5902)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 1.9650 (1.9650 -- 1.9650)  data: 1.7832 (1.7832 -- 1.7832)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7117 (0.8523)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (97.4684)  time: 0.3648 (0.1341 -- 1.9650)  data: 0.1982 (0.0001 -- 1.7832)  max mem: 16413
Val: Total time: 0:00:03 (0.3649 s / it)
* Acc@1 82.278 Acc@5 97.468 loss 0.765
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [120]  [ 0/52]  eta: 0:06:33  lr: 0.000017  min_lr: 0.000000  loss: 1.8313 (1.8313)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3671 (6.3671)  time: 7.5692 (7.5692 -- 7.5692)  data: 5.5168 (5.5168 -- 5.5168)  max mem: 16413
[2023-10-24 00:56:30,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6248
[2023-10-24 00:56:30,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 00:56:30,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6248
[2023-10-24 00:56:30,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 00:56:30,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [120]  [20/52]  eta: 0:00:35  lr: 0.000017  min_lr: 0.000000  loss: 1.8790 (1.9191)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0153 (8.1329)  time: 0.7863 (0.5259 -- 3.1315)  data: 0.0714 (0.0008 -- 1.2085)  max mem: 16413
Epoch: [120]  [40/52]  eta: 0:00:12  lr: 0.000017  min_lr: 0.000000  loss: 1.7874 (1.8754)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7746 (8.7959)  time: 0.9306 (0.5202 -- 4.4528)  data: 0.0733 (0.0003 -- 1.4307)  max mem: 16413
Epoch: [120]  [51/52]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8420 (1.8708)  loss_scale: 8192.0000 (9452.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7746 (8.8760)  time: 0.7297 (0.4952 -- 2.7214)  data: 0.0016 (0.0002 -- 0.0123)  max mem: 16413
Epoch: [120] Total time: 0:00:47 (0.9196 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8420 (1.8608)  loss_scale: 8192.0000 (9452.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7746 (8.8760)
Val:  [0/9]  eta: 0:00:19  loss: 0.5718 (0.5718)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1716 (2.1716 -- 2.1716)  data: 1.9816 (1.9816 -- 1.9816)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7006 (0.8544)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (96.2025)  time: 0.3885 (0.1341 -- 2.1716)  data: 0.2203 (0.0001 -- 1.9816)  max mem: 16413
Val: Total time: 0:00:03 (0.3887 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.765
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 84.18%
Epoch: [121]  [ 0/52]  eta: 0:05:03  lr: 0.000017  min_lr: 0.000000  loss: 1.9497 (1.9497)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5031 (7.5031)  time: 5.8293 (5.8293 -- 5.8293)  data: 5.2784 (5.2784 -- 5.2784)  max mem: 16413
Epoch: [121]  [20/52]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 2.0253 (1.9518)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5449 (8.1985)  time: 0.9062 (0.5224 -- 2.6786)  data: 0.3635 (0.0004 -- 2.1234)  max mem: 16413
Epoch: [121]  [40/52]  eta: 0:00:11  lr: 0.000016  min_lr: 0.000000  loss: 1.9131 (1.9442)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6772 (8.7592)  time: 0.8370 (0.5309 -- 2.5719)  data: 0.2153 (0.0003 -- 1.8073)  max mem: 16413
Epoch: [121]  [51/52]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.8900 (1.9317)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3007 (8.5812)  time: 0.7632 (0.5014 -- 2.5719)  data: 0.1201 (0.0002 -- 1.2247)  max mem: 16413
Epoch: [121] Total time: 0:00:48 (0.9296 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.8900 (1.8655)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3007 (8.5812)
Val:  [0/9]  eta: 0:00:18  loss: 0.5309 (0.5309)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0580 (2.0580 -- 2.0580)  data: 1.8745 (1.8745 -- 1.8745)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6797 (0.8435)  acc1: 85.7143 (78.4810)  acc5: 100.0000 (96.2025)  time: 0.3753 (0.1333 -- 2.0580)  data: 0.2084 (0.0001 -- 1.8745)  max mem: 16413
Val: Total time: 0:00:03 (0.3754 s / it)
* Acc@1 82.278 Acc@5 96.835 loss 0.761
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [122]  [ 0/52]  eta: 0:06:24  lr: 0.000016  min_lr: 0.000000  loss: 1.8726 (1.8726)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7134 (11.7134)  time: 7.3866 (7.3866 -- 7.3866)  data: 6.8642 (6.8642 -- 6.8642)  max mem: 16413
Epoch: [122]  [20/52]  eta: 0:00:38  lr: 0.000016  min_lr: 0.000000  loss: 1.7494 (1.7557)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1460 (8.7303)  time: 0.8982 (0.5277 -- 3.7249)  data: 0.2584 (0.0007 -- 2.5691)  max mem: 16413
[2023-10-24 00:58:37,098] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:58:37,098] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 00:58:37,099] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 00:58:37,099] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [122]  [40/52]  eta: 0:00:12  lr: 0.000016  min_lr: 0.000000  loss: 1.9128 (1.8065)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1288 (8.6467)  time: 0.8167 (0.5281 -- 3.6407)  data: 0.2412 (0.0003 -- 2.4462)  max mem: 16413
Epoch: [122]  [51/52]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9421 (1.8356)  loss_scale: 16384.0000 (11185.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8264 (8.5888)  time: 0.7203 (0.4943 -- 2.8610)  data: 0.1671 (0.0002 -- 2.3542)  max mem: 16413
Epoch: [122] Total time: 0:00:48 (0.9392 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9421 (1.8245)  loss_scale: 16384.0000 (11185.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8264 (8.5888)
Val:  [0/9]  eta: 0:00:18  loss: 0.5482 (0.5482)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0600 (2.0600 -- 2.0600)  data: 1.8866 (1.8866 -- 1.8866)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7424 (0.8563)  acc1: 77.7778 (77.2152)  acc5: 100.0000 (96.2025)  time: 0.3777 (0.1332 -- 2.0600)  data: 0.2097 (0.0001 -- 1.8866)  max mem: 16413
Val: Total time: 0:00:03 (0.3778 s / it)
* Acc@1 82.278 Acc@5 96.835 loss 0.761
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [123]  [ 0/52]  eta: 0:07:48  lr: 0.000016  min_lr: 0.000000  loss: 1.9330 (1.9330)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8579 (5.8579)  time: 9.0098 (9.0098 -- 9.0098)  data: 5.8523 (5.8523 -- 5.8523)  max mem: 16413
Epoch: [123]  [20/52]  eta: 0:00:39  lr: 0.000016  min_lr: 0.000000  loss: 1.7822 (1.8537)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3596 (8.9324)  time: 0.8351 (0.5181 -- 2.9621)  data: 0.1677 (0.0004 -- 2.3324)  max mem: 16413
Epoch: [123]  [40/52]  eta: 0:00:12  lr: 0.000016  min_lr: 0.000000  loss: 1.7254 (1.8247)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7069 (8.7729)  time: 0.8794 (0.5228 -- 3.4884)  data: 0.0469 (0.0002 -- 0.5640)  max mem: 16413
Epoch: [123]  [51/52]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.6191 (1.7794)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6009 (8.5564)  time: 0.8092 (0.4972 -- 3.4884)  data: 0.0465 (0.0002 -- 0.5640)  max mem: 16413
Epoch: [123] Total time: 0:00:49 (0.9476 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.6191 (1.8314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6009 (8.5564)
Val:  [0/9]  eta: 0:00:18  loss: 0.5611 (0.5611)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0827 (2.0827 -- 2.0827)  data: 1.9025 (1.9025 -- 1.9025)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6941 (0.8525)  acc1: 77.7778 (79.7468)  acc5: 100.0000 (96.2025)  time: 0.3784 (0.1335 -- 2.0827)  data: 0.2115 (0.0001 -- 1.9025)  max mem: 16413
Val: Total time: 0:00:03 (0.3785 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.765
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 84.18%
Epoch: [124]  [ 0/52]  eta: 0:05:55  lr: 0.000016  min_lr: 0.000000  loss: 2.0139 (2.0139)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8247 (6.8247)  time: 6.8323 (6.8323 -- 6.8323)  data: 5.2666 (5.2666 -- 5.2666)  max mem: 16413
Epoch: [124]  [20/52]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.5879 (1.6884)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0894 (8.4461)  time: 0.8685 (0.5200 -- 3.9318)  data: 0.1957 (0.0003 -- 2.2613)  max mem: 16413
Epoch: [124]  [40/52]  eta: 0:00:12  lr: 0.000015  min_lr: 0.000000  loss: 1.6279 (1.6619)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6879 (8.6305)  time: 0.9348 (0.5329 -- 3.6511)  data: 0.3790 (0.0003 -- 3.1014)  max mem: 16413
Epoch: [124]  [51/52]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.5766 (1.6712)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5955 (8.7454)  time: 0.8056 (0.4946 -- 3.6511)  data: 0.2827 (0.0001 -- 3.1014)  max mem: 16413
Epoch: [124] Total time: 0:00:48 (0.9317 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.5766 (1.7345)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5955 (8.7454)
Val:  [0/9]  eta: 0:00:19  loss: 0.5649 (0.5649)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1133 (2.1133 -- 2.1133)  data: 1.9385 (1.9385 -- 1.9385)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7385 (0.8472)  acc1: 77.7778 (79.7468)  acc5: 100.0000 (97.4684)  time: 0.3809 (0.1333 -- 2.1133)  data: 0.2155 (0.0001 -- 1.9385)  max mem: 16413
Val: Total time: 0:00:03 (0.3810 s / it)
* Acc@1 81.646 Acc@5 97.468 loss 0.762
Accuracy of the network on the 158 val images: 81.65%
Max accuracy: 84.18%
Epoch: [125]  [ 0/52]  eta: 0:06:04  lr: 0.000015  min_lr: 0.000000  loss: 1.8659 (1.8659)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9869 (8.9869)  time: 7.0019 (7.0019 -- 7.0019)  data: 6.4353 (6.4353 -- 6.4353)  max mem: 16413
[2023-10-24 01:00:46,926] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:00:46,926] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:00:46,926] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:00:46,926] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [125]  [20/52]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.9369 (1.8740)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7109 (9.0050)  time: 0.8368 (0.5196 -- 3.0375)  data: 0.1916 (0.0002 -- 2.5226)  max mem: 16413
[2023-10-24 01:01:07,276] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6529
[2023-10-24 01:01:07,276] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6529
[2023-10-24 01:01:07,276] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:01:07,276] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:01:07,276] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [125]  [40/52]  eta: 0:00:12  lr: 0.000015  min_lr: 0.000000  loss: 1.8675 (1.8702)  loss_scale: 16384.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4480 (9.2960)  time: 0.8872 (0.5170 -- 3.0534)  data: 0.3033 (0.0004 -- 2.3309)  max mem: 16413
Epoch: [125]  [51/52]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.9034 (1.8751)  loss_scale: 16384.0000 (23945.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4480 (9.1396)  time: 0.7801 (0.4957 -- 2.8758)  data: 0.2193 (0.0002 -- 2.3309)  max mem: 16413
Epoch: [125] Total time: 0:00:47 (0.9213 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.9034 (1.8886)  loss_scale: 16384.0000 (23945.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4480 (9.1396)
Val:  [0/9]  eta: 0:00:19  loss: 0.5872 (0.5872)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1706 (2.1706 -- 2.1706)  data: 2.0021 (2.0021 -- 2.0021)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7902 (0.8634)  acc1: 77.7778 (79.7468)  acc5: 100.0000 (97.4684)  time: 0.3874 (0.1331 -- 2.1706)  data: 0.2225 (0.0001 -- 2.0021)  max mem: 16413
Val: Total time: 0:00:03 (0.3875 s / it)
* Acc@1 82.278 Acc@5 97.468 loss 0.772
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 84.18%
Epoch: [126]  [ 0/52]  eta: 0:07:11  lr: 0.000015  min_lr: 0.000000  loss: 1.7548 (1.7548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3680 (8.3680)  time: 8.2973 (8.2973 -- 8.2973)  data: 7.5334 (7.5334 -- 7.5334)  max mem: 16413
Epoch: [126]  [20/52]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.6940 (1.7508)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7390 (8.8216)  time: 0.8173 (0.5307 -- 2.6229)  data: 0.2025 (0.0008 -- 2.0350)  max mem: 16413
Epoch: [126]  [40/52]  eta: 0:00:12  lr: 0.000015  min_lr: 0.000000  loss: 1.7443 (1.7586)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6190 (8.7691)  time: 0.9110 (0.5284 -- 2.5500)  data: 0.2231 (0.0005 -- 2.0177)  max mem: 16413
Epoch: [126]  [51/52]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7183 (1.7687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7968 (8.9376)  time: 0.7675 (0.4930 -- 2.5500)  data: 0.2332 (0.0001 -- 2.0177)  max mem: 16413
Epoch: [126] Total time: 0:00:49 (0.9483 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7183 (1.8069)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7968 (8.9376)
Val:  [0/9]  eta: 0:00:18  loss: 0.5911 (0.5911)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0522 (2.0522 -- 2.0522)  data: 1.8804 (1.8804 -- 1.8804)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7687 (0.8713)  acc1: 77.7778 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3749 (0.1332 -- 2.0522)  data: 0.2090 (0.0001 -- 1.8804)  max mem: 16413
Val: Total time: 0:00:03 (0.3750 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.777
Accuracy of the network on the 158 val images: 84.18%
[2023-10-24 01:02:20,230] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 01:02:20,232] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 01:02:20,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 01:02:20,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 01:02:21,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 01:02:21,735] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.18%
Epoch: [127]  [ 0/52]  eta: 0:06:45  lr: 0.000014  min_lr: 0.000000  loss: 1.4203 (1.4203)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5515 (11.5515)  time: 7.7899 (7.7899 -- 7.7899)  data: 6.3815 (6.3815 -- 6.3815)  max mem: 16413
Epoch: [127]  [20/52]  eta: 0:00:38  lr: 0.000014  min_lr: 0.000000  loss: 1.8723 (1.8647)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9836 (8.5482)  time: 0.8781 (0.5346 -- 2.4771)  data: 0.0052 (0.0004 -- 0.0706)  max mem: 16413
Epoch: [127]  [40/52]  eta: 0:00:12  lr: 0.000014  min_lr: 0.000000  loss: 1.9029 (1.8771)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2615 (8.7211)  time: 0.8370 (0.5226 -- 2.8402)  data: 0.0017 (0.0005 -- 0.0046)  max mem: 16413
Epoch: [127]  [51/52]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.8229 (1.8495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5587 (8.5418)  time: 0.7166 (0.4971 -- 2.1598)  data: 0.0010 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [127] Total time: 0:00:48 (0.9241 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.8229 (1.8639)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5587 (8.5418)
Val:  [0/9]  eta: 0:00:19  loss: 0.5651 (0.5651)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1639 (2.1639 -- 2.1639)  data: 1.9915 (1.9915 -- 1.9915)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6789 (0.8529)  acc1: 85.7143 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3877 (0.1348 -- 2.1639)  data: 0.2214 (0.0001 -- 1.9915)  max mem: 16413
Val: Total time: 0:00:03 (0.3878 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.758
Accuracy of the network on the 158 val images: 84.81%
[2023-10-24 01:03:13,284] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 01:03:13,286] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 01:03:13,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 01:03:13,286] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 01:03:14,888] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 01:03:14,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.81%
Epoch: [128]  [ 0/52]  eta: 0:06:21  lr: 0.000014  min_lr: 0.000000  loss: 1.5322 (1.5322)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8420 (4.8420)  time: 7.3378 (7.3378 -- 7.3378)  data: 6.8038 (6.8038 -- 6.8038)  max mem: 16413
[2023-10-24 01:03:23,494] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:03:23,494] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:03:23,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:03:23,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [128]  [20/52]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.7251 (1.6846)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3489 (7.3976)  time: 0.8237 (0.5234 -- 3.3402)  data: 0.1381 (0.0002 -- 1.9715)  max mem: 16413
[2023-10-24 01:03:51,700] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6690
[2023-10-24 01:03:51,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:03:51,700] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6690
[2023-10-24 01:03:51,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:03:51,700] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [128]  [40/52]  eta: 0:00:12  lr: 0.000014  min_lr: 0.000000  loss: 1.7947 (1.7537)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1237 (8.3367)  time: 0.8992 (0.5236 -- 3.2603)  data: 0.2249 (0.0006 -- 1.5085)  max mem: 16413
Epoch: [128]  [51/52]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.8826 (1.7971)  loss_scale: 16384.0000 (26466.4615)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6929 (8.3649)  time: 0.7754 (0.4955 -- 3.1126)  data: 0.1490 (0.0001 -- 1.5039)  max mem: 16413
Epoch: [128] Total time: 0:00:48 (0.9254 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.8826 (1.7826)  loss_scale: 16384.0000 (26466.4615)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6929 (8.3649)
Val:  [0/9]  eta: 0:00:19  loss: 0.5702 (0.5702)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1407 (2.1407 -- 2.1407)  data: 1.9615 (1.9615 -- 1.9615)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6579 (0.8391)  acc1: 85.7143 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3869 (0.1330 -- 2.1407)  data: 0.2180 (0.0001 -- 1.9615)  max mem: 16413
Val: Total time: 0:00:03 (0.3870 s / it)
* Acc@1 85.443 Acc@5 96.835 loss 0.752
Accuracy of the network on the 158 val images: 85.44%
[2023-10-24 01:04:06,502] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 01:04:06,504] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 01:04:06,504] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 01:04:06,504] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 01:04:07,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 01:04:07,954] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.44%
Epoch: [129]  [ 0/52]  eta: 0:07:26  lr: 0.000014  min_lr: 0.000000  loss: 1.6633 (1.6633)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0582 (7.0582)  time: 8.5921 (8.5921 -- 8.5921)  data: 5.2063 (5.2063 -- 5.2063)  max mem: 16413
Epoch: [129]  [20/52]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 1.8815 (1.8150)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6549 (8.7259)  time: 0.8071 (0.5275 -- 3.9941)  data: 0.0017 (0.0005 -- 0.0057)  max mem: 16413
Epoch: [129]  [40/52]  eta: 0:00:12  lr: 0.000014  min_lr: 0.000000  loss: 1.7035 (1.7666)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2166 (8.6132)  time: 0.8144 (0.5215 -- 3.2769)  data: 0.0500 (0.0004 -- 0.5058)  max mem: 16413
Epoch: [129]  [51/52]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8377 (1.7994)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2285 (8.7527)  time: 0.7752 (0.4958 -- 3.2769)  data: 0.0816 (0.0002 -- 0.5690)  max mem: 16413
Epoch: [129] Total time: 0:00:48 (0.9385 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8377 (1.8145)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2285 (8.7527)
Val:  [0/9]  eta: 0:00:19  loss: 0.6045 (0.6045)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1254 (2.1254 -- 2.1254)  data: 1.9528 (1.9528 -- 1.9528)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6709 (0.8466)  acc1: 85.7143 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3830 (0.1329 -- 2.1254)  data: 0.2171 (0.0001 -- 1.9528)  max mem: 16413
Val: Total time: 0:00:03 (0.3832 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.755
Accuracy of the network on the 158 val images: 84.81%
Max accuracy: 85.44%
Epoch: [130]  [ 0/52]  eta: 0:08:43  lr: 0.000013  min_lr: 0.000000  loss: 2.0529 (2.0529)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0941 (7.0941)  time: 10.0723 (10.0723 -- 10.0723)  data: 5.3149 (5.3149 -- 5.3149)  max mem: 16413
Epoch: [130]  [20/52]  eta: 0:00:40  lr: 0.000013  min_lr: 0.000000  loss: 1.8514 (1.8668)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6459 (8.8293)  time: 0.8321 (0.5230 -- 3.5503)  data: 0.0019 (0.0003 -- 0.0158)  max mem: 16413
Epoch: [130]  [40/52]  eta: 0:00:13  lr: 0.000013  min_lr: 0.000000  loss: 1.8405 (1.8366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1710 (8.7252)  time: 0.9227 (0.5044 -- 4.8913)  data: 0.0008 (0.0003 -- 0.0021)  max mem: 16413
Epoch: [130]  [51/52]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7296 (1.8347)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6720 (8.6015)  time: 0.7289 (0.4953 -- 4.8913)  data: 0.0004 (0.0001 -- 0.0015)  max mem: 16413
Epoch: [130] Total time: 0:00:50 (0.9745 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.7296 (1.8654)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6720 (8.6015)
Val:  [0/9]  eta: 0:00:19  loss: 0.5738 (0.5738)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1716 (2.1716 -- 2.1716)  data: 1.9909 (1.9909 -- 1.9909)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6907 (0.8489)  acc1: 85.7143 (79.7468)  acc5: 100.0000 (96.2025)  time: 0.3886 (0.1330 -- 2.1716)  data: 0.2213 (0.0001 -- 1.9909)  max mem: 16413
Val: Total time: 0:00:03 (0.3887 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.748
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [131]  [ 0/52]  eta: 0:06:29  lr: 0.000013  min_lr: 0.000000  loss: 1.1802 (1.1802)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2608 (7.2608)  time: 7.4910 (7.4910 -- 7.4910)  data: 5.6509 (5.6509 -- 5.6509)  max mem: 16413
[2023-10-24 01:06:06,079] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:06:06,079] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:06:06,080] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:06:06,080] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [131]  [20/52]  eta: 0:00:35  lr: 0.000013  min_lr: 0.000000  loss: 1.6745 (1.6913)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4944 (7.9726)  time: 0.7838 (0.5303 -- 3.1707)  data: 0.1469 (0.0008 -- 1.3168)  max mem: 16413
Epoch: [131]  [40/52]  eta: 0:00:11  lr: 0.000013  min_lr: 0.000000  loss: 1.7645 (1.7688)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2968 (8.4513)  time: 0.8800 (0.5309 -- 2.6972)  data: 0.2450 (0.0002 -- 2.1461)  max mem: 16413
Epoch: [131]  [51/52]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7061 (1.7590)  loss_scale: 32768.0000 (30562.4615)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9075 (8.4967)  time: 0.8269 (0.4951 -- 2.9097)  data: 0.0614 (0.0002 -- 1.0135)  max mem: 16413
Epoch: [131] Total time: 0:00:49 (0.9512 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.7061 (1.7843)  loss_scale: 32768.0000 (30562.4615)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9075 (8.4967)
Val:  [0/9]  eta: 0:00:19  loss: 0.5364 (0.5364)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1589 (2.1589 -- 2.1589)  data: 1.9837 (1.9837 -- 1.9837)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7088 (0.8384)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (96.2025)  time: 0.3856 (0.1326 -- 2.1589)  data: 0.2205 (0.0001 -- 1.9837)  max mem: 16413
Val: Total time: 0:00:03 (0.3857 s / it)
* Acc@1 82.278 Acc@5 96.835 loss 0.747
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 85.44%
Epoch: [132]  [ 0/52]  eta: 0:06:18  lr: 0.000013  min_lr: 0.000000  loss: 2.0057 (2.0057)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3042 (10.3042)  time: 7.2743 (7.2743 -- 7.2743)  data: 6.7617 (6.7617 -- 6.7617)  max mem: 16413
[2023-10-24 01:07:05,232] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6875
[2023-10-24 01:07:05,232] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6875
[2023-10-24 01:07:05,232] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:07:05,232] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:07:05,233] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [132]  [20/52]  eta: 0:00:39  lr: 0.000013  min_lr: 0.000000  loss: 1.6266 (1.6920)  loss_scale: 16384.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1593 (8.5165)  time: 0.9171 (0.5091 -- 4.6602)  data: 0.2998 (0.0005 -- 4.1091)  max mem: 16413
Epoch: [132]  [40/52]  eta: 0:00:12  lr: 0.000012  min_lr: 0.000000  loss: 1.6159 (1.6823)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7360 (8.1207)  time: 0.8510 (0.5201 -- 2.8157)  data: 0.2012 (0.0002 -- 2.2635)  max mem: 16413
Epoch: [132]  [51/52]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.5623 (1.6781)  loss_scale: 16384.0000 (19849.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8776 (8.2816)  time: 0.7091 (0.4986 -- 2.8157)  data: 0.1799 (0.0001 -- 2.2635)  max mem: 16413
Epoch: [132] Total time: 0:00:48 (0.9397 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.5623 (1.7679)  loss_scale: 16384.0000 (19849.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8776 (8.2816)
Val:  [0/9]  eta: 0:00:18  loss: 0.5677 (0.5677)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0718 (2.0718 -- 2.0718)  data: 1.8936 (1.8936 -- 1.8936)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6954 (0.8433)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (96.2025)  time: 0.3767 (0.1327 -- 2.0718)  data: 0.2105 (0.0001 -- 1.8936)  max mem: 16413
Val: Total time: 0:00:03 (0.3768 s / it)
* Acc@1 82.278 Acc@5 96.835 loss 0.753
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 85.44%
Epoch: [133]  [ 0/52]  eta: 0:05:02  lr: 0.000012  min_lr: 0.000000  loss: 1.8783 (1.8783)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6301 (6.6301)  time: 5.8233 (5.8233 -- 5.8233)  data: 5.3042 (5.3042 -- 5.3042)  max mem: 16413
Epoch: [133]  [20/52]  eta: 0:00:41  lr: 0.000012  min_lr: 0.000000  loss: 1.9294 (1.8739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8742 (8.9868)  time: 1.0640 (0.5327 -- 4.3694)  data: 0.2251 (0.0006 -- 1.9122)  max mem: 16413
Epoch: [133]  [40/52]  eta: 0:00:12  lr: 0.000012  min_lr: 0.000000  loss: 1.7328 (1.8216)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9010 (9.3026)  time: 0.8478 (0.5043 -- 3.8086)  data: 0.0015 (0.0003 -- 0.0049)  max mem: 16413
Epoch: [133]  [51/52]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.6657 (1.8074)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0108 (9.0122)  time: 0.7144 (0.4956 -- 3.3209)  data: 0.0008 (0.0001 -- 0.0041)  max mem: 16413
Epoch: [133] Total time: 0:00:50 (0.9764 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.6657 (1.8393)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0108 (9.0122)
Val:  [0/9]  eta: 0:00:18  loss: 0.5600 (0.5600)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0212 (2.0212 -- 2.0212)  data: 1.8469 (1.8469 -- 1.8469)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7398 (0.8514)  acc1: 77.7778 (78.4810)  acc5: 100.0000 (96.2025)  time: 0.3709 (0.1325 -- 2.0212)  data: 0.2053 (0.0001 -- 1.8469)  max mem: 16413
Val: Total time: 0:00:03 (0.3711 s / it)
* Acc@1 82.278 Acc@5 96.835 loss 0.759
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 85.44%
Epoch: [134]  [ 0/52]  eta: 0:06:48  lr: 0.000012  min_lr: 0.000000  loss: 1.7678 (1.7678)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3794 (10.3794)  time: 7.8617 (7.8617 -- 7.8617)  data: 4.1954 (4.1954 -- 4.1954)  max mem: 16413
Epoch: [134]  [20/52]  eta: 0:00:42  lr: 0.000012  min_lr: 0.000000  loss: 1.8335 (1.7972)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6092 (8.7383)  time: 0.9941 (0.5226 -- 3.7546)  data: 0.3097 (0.0004 -- 3.2090)  max mem: 16413
[2023-10-24 01:09:11,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=36, lr=[2.8238077879566366e-07, 2.8238077879566366e-07, 3.765077050608849e-07, 3.765077050608849e-07, 5.020102734145132e-07, 5.020102734145132e-07, 6.69347031219351e-07, 6.69347031219351e-07, 8.924627082924679e-07, 8.924627082924679e-07, 1.1899502777232905e-06, 1.1899502777232905e-06, 1.5866003702977207e-06, 1.5866003702977207e-06, 2.115467160396961e-06, 2.115467160396961e-06, 2.8206228805292814e-06, 2.8206228805292814e-06, 3.760830507372375e-06, 3.760830507372375e-06, 5.0144406764965e-06, 5.0144406764965e-06, 6.685920901995333e-06, 6.685920901995333e-06, 8.914561202660445e-06, 8.914561202660445e-06, 1.188608160354726e-05, 1.188608160354726e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-24 01:09:11,363] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=18.600729105751633, CurrSamplesPerSec=22.2777281798944, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-10-24 01:09:15,822] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:09:15,823] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:09:15,825] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:09:15,825] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [134]  [40/52]  eta: 0:00:12  lr: 0.000012  min_lr: 0.000000  loss: 1.5678 (1.7076)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5690 (8.8024)  time: 0.8211 (0.5070 -- 4.3616)  data: 0.2796 (0.0003 -- 3.8539)  max mem: 16413
[2023-10-24 01:09:21,833] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7014
[2023-10-24 01:09:21,833] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7014
[2023-10-24 01:09:21,833] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:09:21,833] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:09:21,833] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [51/52]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.8316 (1.7395)  loss_scale: 16384.0000 (19534.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5690 (8.7903)  time: 0.6481 (0.4830 -- 2.2501)  data: 0.1316 (0.0001 -- 1.7134)  max mem: 16413
Epoch: [134] Total time: 0:00:50 (0.9724 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.8316 (1.7352)  loss_scale: 16384.0000 (19534.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5690 (8.7903)
Val:  [0/9]  eta: 0:00:19  loss: 0.5580 (0.5580)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1927 (2.1927 -- 2.1927)  data: 2.0211 (2.0211 -- 2.0211)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.7256 (0.8458)  acc1: 77.7778 (79.7468)  acc5: 100.0000 (96.2025)  time: 0.3892 (0.1324 -- 2.1927)  data: 0.2246 (0.0001 -- 2.0211)  max mem: 16413
Val: Total time: 0:00:03 (0.3893 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.752
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [135]  [ 0/52]  eta: 0:06:17  lr: 0.000012  min_lr: 0.000000  loss: 2.1564 (2.1564)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5427 (9.5427)  time: 7.2531 (7.2531 -- 7.2531)  data: 6.7094 (6.7094 -- 6.7094)  max mem: 16413
[2023-10-24 01:09:37,036] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7023
[2023-10-24 01:09:37,036] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7023
[2023-10-24 01:09:37,036] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:09:37,036] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:09:37,036] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [135]  [20/52]  eta: 0:00:35  lr: 0.000012  min_lr: 0.000000  loss: 1.7946 (1.8041)  loss_scale: 8192.0000 (9362.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2856 (8.1975)  time: 0.8035 (0.5116 -- 2.9130)  data: 0.1631 (0.0004 -- 1.5546)  max mem: 16413
Epoch: [135]  [40/52]  eta: 0:00:12  lr: 0.000012  min_lr: 0.000000  loss: 1.7341 (1.7954)  loss_scale: 8192.0000 (8791.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6356 (8.0824)  time: 0.9631 (0.5060 -- 3.0181)  data: 0.0381 (0.0003 -- 0.7336)  max mem: 16413
Epoch: [135]  [51/52]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 2.0065 (1.8198)  loss_scale: 8192.0000 (8664.6154)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3764 (8.0614)  time: 0.7179 (0.4943 -- 3.0181)  data: 0.0007 (0.0001 -- 0.0017)  max mem: 16413
Epoch: [135] Total time: 0:00:48 (0.9370 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 2.0065 (1.8194)  loss_scale: 8192.0000 (8664.6154)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3764 (8.0614)
Val:  [0/9]  eta: 0:00:19  loss: 0.5311 (0.5311)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1417 (2.1417 -- 2.1417)  data: 1.9611 (1.9611 -- 1.9611)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6819 (0.8308)  acc1: 85.7143 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3836 (0.1321 -- 2.1417)  data: 0.2180 (0.0001 -- 1.9611)  max mem: 16413
Val: Total time: 0:00:03 (0.3837 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.744
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [136]  [ 0/52]  eta: 0:06:43  lr: 0.000011  min_lr: 0.000000  loss: 2.2917 (2.2917)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9594 (13.9594)  time: 7.7553 (7.7553 -- 7.7553)  data: 4.8082 (4.8082 -- 4.8082)  max mem: 16413
Epoch: [136]  [20/52]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.8110 (1.8709)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6777 (9.3876)  time: 0.8232 (0.5241 -- 3.6443)  data: 0.1361 (0.0009 -- 1.1858)  max mem: 16413
Epoch: [136]  [40/52]  eta: 0:00:12  lr: 0.000011  min_lr: 0.000000  loss: 1.7915 (1.8665)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3155 (9.5121)  time: 0.9073 (0.5195 -- 2.3637)  data: 0.3680 (0.0004 -- 1.8254)  max mem: 16413
Epoch: [136]  [51/52]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.6538 (1.8282)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2615 (9.1643)  time: 0.7890 (0.4957 -- 2.3637)  data: 0.2728 (0.0002 -- 1.8254)  max mem: 16413
Epoch: [136] Total time: 0:00:49 (0.9425 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.6538 (1.8424)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2615 (9.1643)
Val:  [0/9]  eta: 0:00:19  loss: 0.5391 (0.5391)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1445 (2.1445 -- 2.1445)  data: 1.9590 (1.9590 -- 1.9590)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6958 (0.8413)  acc1: 77.7778 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3841 (0.1326 -- 2.1445)  data: 0.2177 (0.0001 -- 1.9590)  max mem: 16413
Val: Total time: 0:00:03 (0.3843 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.758
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [137]  [ 0/52]  eta: 0:06:55  lr: 0.000011  min_lr: 0.000000  loss: 1.9360 (1.9360)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2588 (8.2588)  time: 7.9844 (7.9844 -- 7.9844)  data: 7.4504 (7.4504 -- 7.4504)  max mem: 16413
Epoch: [137]  [20/52]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.9841 (1.9388)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7422 (7.9427)  time: 0.8111 (0.5137 -- 3.0386)  data: 0.2592 (0.0003 -- 2.4917)  max mem: 16413
[2023-10-24 01:11:44,414] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:11:44,414] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:11:44,415] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 01:11:44,415] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [137]  [40/52]  eta: 0:00:12  lr: 0.000011  min_lr: 0.000000  loss: 1.9447 (1.9316)  loss_scale: 16384.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3647 (8.0248)  time: 0.8993 (0.5263 -- 2.5598)  data: 0.3568 (0.0003 -- 2.0188)  max mem: 16413
Epoch: [137]  [51/52]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.8858 (1.9178)  loss_scale: 16384.0000 (11972.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1633 (8.2276)  time: 0.7117 (0.4958 -- 2.5598)  data: 0.1867 (0.0002 -- 2.0188)  max mem: 16413
Epoch: [137] Total time: 0:00:47 (0.9193 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.8858 (1.7998)  loss_scale: 16384.0000 (11972.9231)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1633 (8.2276)
Val:  [0/9]  eta: 0:00:19  loss: 0.5352 (0.5352)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1176 (2.1176 -- 2.1176)  data: 1.9430 (1.9430 -- 1.9430)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6826 (0.8299)  acc1: 85.7143 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3811 (0.1327 -- 2.1176)  data: 0.2160 (0.0001 -- 1.9430)  max mem: 16413
Val: Total time: 0:00:03 (0.3812 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.756
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [138]  [ 0/52]  eta: 0:05:14  lr: 0.000011  min_lr: 0.000000  loss: 2.0852 (2.0852)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6865 (9.6865)  time: 6.0449 (6.0449 -- 6.0449)  data: 5.5250 (5.5250 -- 5.5250)  max mem: 16413
Epoch: [138]  [20/52]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.8070 (1.7854)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3191 (8.4128)  time: 0.9283 (0.5274 -- 2.8772)  data: 0.2240 (0.0004 -- 1.5092)  max mem: 16413
Epoch: [138]  [40/52]  eta: 0:00:12  lr: 0.000011  min_lr: 0.000000  loss: 1.6909 (1.7895)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4441 (8.5541)  time: 0.9110 (0.5200 -- 3.4379)  data: 0.3635 (0.0004 -- 2.9328)  max mem: 16413
Epoch: [138]  [51/52]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7011 (1.7995)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1331 (8.6528)  time: 0.7553 (0.4952 -- 3.4379)  data: 0.2360 (0.0002 -- 2.9328)  max mem: 16413
Epoch: [138] Total time: 0:00:48 (0.9303 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7011 (1.7725)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1331 (8.6528)
Val:  [0/9]  eta: 0:00:18  loss: 0.5180 (0.5180)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0452 (2.0452 -- 2.0452)  data: 1.8722 (1.8722 -- 1.8722)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6862 (0.8346)  acc1: 85.7143 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3737 (0.1327 -- 2.0452)  data: 0.2081 (0.0001 -- 1.8722)  max mem: 16413
Val: Total time: 0:00:03 (0.3738 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.747
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [139]  [ 0/52]  eta: 0:06:20  lr: 0.000010  min_lr: 0.000000  loss: 1.4044 (1.4044)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2651 (5.2651)  time: 7.3138 (7.3138 -- 7.3138)  data: 6.5972 (6.5972 -- 6.5972)  max mem: 16413
Epoch: [139]  [20/52]  eta: 0:00:35  lr: 0.000010  min_lr: 0.000000  loss: 2.0110 (1.8363)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1125 (8.2099)  time: 0.7923 (0.5365 -- 2.5330)  data: 0.0023 (0.0006 -- 0.0055)  max mem: 16413
Epoch: [139]  [40/52]  eta: 0:00:11  lr: 0.000010  min_lr: 0.000000  loss: 2.1117 (1.9460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6552 (8.3550)  time: 0.8898 (0.5275 -- 3.4030)  data: 0.2261 (0.0004 -- 1.4924)  max mem: 16413
Epoch: [139]  [51/52]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 2.1299 (1.9355)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6552 (8.4087)  time: 0.7428 (0.4983 -- 3.4030)  data: 0.1530 (0.0002 -- 1.4924)  max mem: 16413
Epoch: [139] Total time: 0:00:47 (0.9062 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 2.1299 (1.8564)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6552 (8.4087)
[2023-10-24 01:13:42,646] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-10-24 01:13:42,648] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-139/mp_rank_00_model_states.pt
[2023-10-24 01:13:42,648] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-139/mp_rank_00_model_states.pt...
[2023-10-24 01:13:42,648] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-10-24 01:13:43,663] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-139/mp_rank_00_model_states.pt.
[2023-10-24 01:13:43,663] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [0/9]  eta: 0:00:19  loss: 0.5245 (0.5245)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1493 (2.1493 -- 2.1493)  data: 1.9677 (1.9677 -- 1.9677)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6744 (0.8344)  acc1: 85.7143 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3859 (0.1324 -- 2.1493)  data: 0.2187 (0.0001 -- 1.9677)  max mem: 16413
Val: Total time: 0:00:03 (0.3861 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.753
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
[2023-10-24 01:13:55,172] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:13:55,173] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:13:55,173] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:13:55,173] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [140]  [ 0/52]  eta: 0:06:58  lr: 0.000010  min_lr: 0.000000  loss: 1.1373 (1.1373)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0120 (11.0120)  time: 8.0461 (8.0461 -- 8.0461)  data: 6.4527 (6.4527 -- 6.4527)  max mem: 16413
Epoch: [140]  [20/52]  eta: 0:00:38  lr: 0.000010  min_lr: 0.000000  loss: 1.7563 (1.7318)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9001 (8.5979)  time: 0.8653 (0.5323 -- 3.0307)  data: 0.0839 (0.0008 -- 0.7230)  max mem: 16413
Epoch: [140]  [40/52]  eta: 0:00:12  lr: 0.000010  min_lr: 0.000000  loss: 1.7978 (1.7790)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3435 (8.3372)  time: 0.9011 (0.5157 -- 3.4576)  data: 0.0821 (0.0004 -- 1.0880)  max mem: 16413
Epoch: [140]  [51/52]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7978 (1.7754)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1722 (8.4889)  time: 0.7222 (0.4977 -- 3.4576)  data: 0.0777 (0.0001 -- 1.0880)  max mem: 16413
Epoch: [140] Total time: 0:00:49 (0.9428 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7978 (1.7501)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1722 (8.4889)
Val:  [0/9]  eta: 0:00:19  loss: 0.5154 (0.5154)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1585 (2.1585 -- 2.1585)  data: 1.9727 (1.9727 -- 1.9727)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6809 (0.8308)  acc1: 85.7143 (79.7468)  acc5: 100.0000 (96.2025)  time: 0.3872 (0.1334 -- 2.1585)  data: 0.2193 (0.0001 -- 1.9727)  max mem: 16413
Val: Total time: 0:00:03 (0.3873 s / it)
* Acc@1 82.278 Acc@5 96.835 loss 0.748
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 85.44%
Epoch: [141]  [ 0/52]  eta: 0:07:03  lr: 0.000010  min_lr: 0.000000  loss: 1.8618 (1.8618)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6610 (6.6610)  time: 8.1518 (8.1518 -- 8.1518)  data: 6.9072 (6.9072 -- 6.9072)  max mem: 16413
Epoch: [141]  [20/52]  eta: 0:00:40  lr: 0.000010  min_lr: 0.000000  loss: 1.8172 (1.8308)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4668 (8.8761)  time: 0.9226 (0.5198 -- 4.6829)  data: 0.0613 (0.0003 -- 1.1969)  max mem: 16413
Epoch: [141]  [40/52]  eta: 0:00:12  lr: 0.000010  min_lr: 0.000000  loss: 1.8375 (1.7884)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4431 (8.9549)  time: 0.8628 (0.5010 -- 4.3547)  data: 0.0009 (0.0002 -- 0.0052)  max mem: 16413
[2023-10-24 01:15:25,011] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7375
[2023-10-24 01:15:25,011] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7375
[2023-10-24 01:15:25,011] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:15:25,011] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:15:25,011] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [141]  [51/52]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8358 (1.7736)  loss_scale: 32768.0000 (29932.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9069 (8.8765)  time: 0.7013 (0.4812 -- 4.3547)  data: 0.0005 (0.0001 -- 0.0012)  max mem: 16413
Epoch: [141] Total time: 0:00:49 (0.9490 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.8358 (1.7856)  loss_scale: 32768.0000 (29932.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9069 (8.8765)
Val:  [0/9]  eta: 0:00:19  loss: 0.5145 (0.5145)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1629 (2.1629 -- 2.1629)  data: 1.9860 (1.9860 -- 1.9860)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6676 (0.8290)  acc1: 88.8889 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3864 (0.1326 -- 2.1629)  data: 0.2207 (0.0001 -- 1.9860)  max mem: 16413
Val: Total time: 0:00:03 (0.3865 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.746
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [142]  [ 0/52]  eta: 0:06:40  lr: 0.000010  min_lr: 0.000000  loss: 2.0449 (2.0449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0688 (12.0688)  time: 7.6971 (7.6971 -- 7.6971)  data: 3.5688 (3.5688 -- 3.5688)  max mem: 16413
Epoch: [142]  [20/52]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.6184 (1.7168)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3594 (8.6929)  time: 0.8366 (0.5300 -- 3.0729)  data: 0.0026 (0.0003 -- 0.0138)  max mem: 16413
Epoch: [142]  [40/52]  eta: 0:00:12  lr: 0.000009  min_lr: 0.000000  loss: 1.8098 (1.7577)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3814 (8.9047)  time: 0.9329 (0.5028 -- 3.3603)  data: 0.0237 (0.0001 -- 0.4538)  max mem: 16413
Epoch: [142]  [51/52]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7322 (1.7340)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7855 (8.6608)  time: 0.7934 (0.4963 -- 3.3603)  data: 0.0007 (0.0001 -- 0.0021)  max mem: 16413
Epoch: [142] Total time: 0:00:49 (0.9523 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7322 (1.7618)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7855 (8.6608)
Val:  [0/9]  eta: 0:00:18  loss: 0.5023 (0.5023)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0555 (2.0555 -- 2.0555)  data: 1.8748 (1.8748 -- 1.8748)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6652 (0.8305)  acc1: 88.8889 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3764 (0.1331 -- 2.0555)  data: 0.2084 (0.0001 -- 1.8748)  max mem: 16413
Val: Total time: 0:00:03 (0.3766 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.744
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [143]  [ 0/52]  eta: 0:06:47  lr: 0.000009  min_lr: 0.000000  loss: 1.6639 (1.6639)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1028 (7.1028)  time: 7.8422 (7.8422 -- 7.8422)  data: 7.2992 (7.2992 -- 7.2992)  max mem: 16413
Epoch: [143]  [20/52]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.6813 (1.6907)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9999 (8.3846)  time: 0.8340 (0.5198 -- 2.8134)  data: 0.2629 (0.0006 -- 2.2927)  max mem: 16413
Epoch: [143]  [40/52]  eta: 0:00:12  lr: 0.000009  min_lr: 0.000000  loss: 1.8009 (1.7471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0785 (8.7730)  time: 0.8290 (0.5230 -- 2.4212)  data: 0.1774 (0.0004 -- 1.8466)  max mem: 16413
Epoch: [143]  [51/52]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8009 (1.7199)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4575 (8.6810)  time: 0.7124 (0.5005 -- 2.0274)  data: 0.1053 (0.0002 -- 1.0080)  max mem: 16413
Epoch: [143] Total time: 0:00:47 (0.9085 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.8009 (1.7455)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4575 (8.6810)
Val:  [0/9]  eta: 0:00:18  loss: 0.5129 (0.5129)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0440 (2.0440 -- 2.0440)  data: 1.8547 (1.8547 -- 1.8547)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6803 (0.8265)  acc1: 88.8889 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3755 (0.1331 -- 2.0440)  data: 0.2062 (0.0001 -- 1.8547)  max mem: 16413
Val: Total time: 0:00:03 (0.3756 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.735
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [144]  [ 0/52]  eta: 0:07:22  lr: 0.000009  min_lr: 0.000000  loss: 1.8402 (1.8402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8489 (9.8489)  time: 8.5135 (8.5135 -- 8.5135)  data: 6.7217 (6.7217 -- 6.7217)  max mem: 16413
[2023-10-24 01:17:38,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:17:38,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:17:38,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:17:38,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [144]  [20/52]  eta: 0:00:39  lr: 0.000009  min_lr: 0.000000  loss: 1.6088 (1.7303)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7873 (7.9576)  time: 0.8552 (0.5250 -- 3.6011)  data: 0.1698 (0.0008 -- 2.1591)  max mem: 16413
[2023-10-24 01:17:57,450] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7524
[2023-10-24 01:17:57,450] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7524
[2023-10-24 01:17:57,450] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:17:57,450] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:17:57,450] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [144]  [40/52]  eta: 0:00:12  lr: 0.000009  min_lr: 0.000000  loss: 1.8526 (1.7942)  loss_scale: 32768.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5418 (8.7423)  time: 0.9177 (0.5091 -- 4.2287)  data: 0.3572 (0.0002 -- 3.7124)  max mem: 16413
Epoch: [144]  [51/52]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7254 (1.7848)  loss_scale: 16384.0000 (22685.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6080 (8.6439)  time: 0.7628 (0.4951 -- 4.2287)  data: 0.2119 (0.0001 -- 3.7124)  max mem: 16413
Epoch: [144] Total time: 0:00:49 (0.9577 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7254 (1.8548)  loss_scale: 16384.0000 (22685.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6080 (8.6439)
Val:  [0/9]  eta: 0:00:19  loss: 0.5382 (0.5382)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1560 (2.1560 -- 2.1560)  data: 1.9832 (1.9832 -- 1.9832)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6872 (0.8319)  acc1: 85.7143 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3868 (0.1336 -- 2.1560)  data: 0.2204 (0.0001 -- 1.9832)  max mem: 16413
Val: Total time: 0:00:03 (0.3869 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.742
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [145]  [ 0/52]  eta: 0:05:21  lr: 0.000009  min_lr: 0.000000  loss: 1.9577 (1.9577)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7556 (9.7556)  time: 6.1852 (6.1852 -- 6.1852)  data: 5.4159 (5.4159 -- 5.4159)  max mem: 16413
Epoch: [145]  [20/52]  eta: 0:00:38  lr: 0.000009  min_lr: 0.000000  loss: 1.7534 (1.7529)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9234 (9.7606)  time: 0.9613 (0.5270 -- 3.7180)  data: 0.0020 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [145]  [40/52]  eta: 0:00:12  lr: 0.000008  min_lr: 0.000000  loss: 1.8377 (1.8005)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9731 (9.4857)  time: 0.8903 (0.5079 -- 4.9594)  data: 0.0011 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [145]  [51/52]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.9512 (1.8328)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3213 (9.9719)  time: 0.7735 (0.4955 -- 3.1773)  data: 0.0007 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [145] Total time: 0:00:51 (0.9855 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.9512 (1.8330)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3213 (9.9719)
Val:  [0/9]  eta: 0:00:18  loss: 0.5433 (0.5433)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0531 (2.0531 -- 2.0531)  data: 1.8824 (1.8824 -- 1.8824)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6897 (0.8326)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3746 (0.1335 -- 2.0531)  data: 0.2092 (0.0001 -- 1.8824)  max mem: 16413
Val: Total time: 0:00:03 (0.3747 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.741
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [146]  [ 0/52]  eta: 0:06:57  lr: 0.000008  min_lr: 0.000000  loss: 1.8366 (1.8366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7257 (6.7257)  time: 8.0380 (8.0380 -- 8.0380)  data: 7.4423 (7.4423 -- 7.4423)  max mem: 16413
Epoch: [146]  [20/52]  eta: 0:00:38  lr: 0.000008  min_lr: 0.000000  loss: 1.8056 (1.7809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4339 (8.3158)  time: 0.8706 (0.5354 -- 4.6095)  data: 0.3259 (0.0007 -- 4.0988)  max mem: 16413
Epoch: [146]  [40/52]  eta: 0:00:12  lr: 0.000008  min_lr: 0.000000  loss: 1.8457 (1.8101)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6128 (8.4482)  time: 0.8992 (0.5089 -- 3.6921)  data: 0.3605 (0.0003 -- 3.1608)  max mem: 16413
Epoch: [146]  [51/52]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6498 (1.7915)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6128 (8.5058)  time: 0.7734 (0.4942 -- 3.3075)  data: 0.2567 (0.0001 -- 2.7935)  max mem: 16413
Epoch: [146] Total time: 0:00:50 (0.9637 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6498 (1.7939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6128 (8.5058)
Val:  [0/9]  eta: 0:00:18  loss: 0.5453 (0.5453)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0493 (2.0493 -- 2.0493)  data: 1.8751 (1.8751 -- 1.8751)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6616 (0.8350)  acc1: 77.7778 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3738 (0.1328 -- 2.0493)  data: 0.2084 (0.0001 -- 1.8751)  max mem: 16413
Val: Total time: 0:00:03 (0.3739 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.748
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [147]  [ 0/52]  eta: 0:07:46  lr: 0.000008  min_lr: 0.000000  loss: 1.6778 (1.6778)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0548 (6.0548)  time: 8.9783 (8.9783 -- 8.9783)  data: 4.7468 (4.7468 -- 4.7468)  max mem: 16413
[2023-10-24 01:20:13,134] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:20:13,134] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:20:13,135] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:20:13,135] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [147]  [20/52]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.7969 (1.9117)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0520 (8.0600)  time: 0.7716 (0.5406 -- 2.7697)  data: 0.0017 (0.0005 -- 0.0045)  max mem: 16413
Epoch: [147]  [40/52]  eta: 0:00:12  lr: 0.000008  min_lr: 0.000000  loss: 1.7954 (1.8560)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5793 (8.2870)  time: 0.8780 (0.5140 -- 2.8974)  data: 0.0054 (0.0002 -- 0.0808)  max mem: 16413
Epoch: [147]  [51/52]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.8201 (1.8623)  loss_scale: 32768.0000 (29932.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5542 (8.1918)  time: 0.7461 (0.4970 -- 1.7700)  data: 0.0229 (0.0001 -- 0.3641)  max mem: 16413
Epoch: [147] Total time: 0:00:48 (0.9322 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.8201 (1.8383)  loss_scale: 32768.0000 (29932.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5542 (8.1918)
Val:  [0/9]  eta: 0:00:18  loss: 0.5559 (0.5559)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0325 (2.0325 -- 2.0325)  data: 1.8505 (1.8505 -- 1.8505)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6658 (0.8362)  acc1: 77.7778 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3730 (0.1327 -- 2.0325)  data: 0.2063 (0.0001 -- 1.8505)  max mem: 16413
Val: Total time: 0:00:03 (0.3731 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.748
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [148]  [ 0/52]  eta: 0:07:41  lr: 0.000008  min_lr: 0.000000  loss: 1.5658 (1.5658)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4404 (6.4404)  time: 8.8727 (8.8727 -- 8.8727)  data: 8.3174 (8.3174 -- 8.3174)  max mem: 16413
Epoch: [148]  [20/52]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9016 (1.7676)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6731 (9.0343)  time: 0.7767 (0.5182 -- 3.9946)  data: 0.2243 (0.0002 -- 3.4446)  max mem: 16413
[2023-10-24 01:21:22,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7726
[2023-10-24 01:21:22,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7726
[2023-10-24 01:21:22,498] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:21:22,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:21:22,499] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [148]  [40/52]  eta: 0:00:12  lr: 0.000008  min_lr: 0.000000  loss: 1.6810 (1.7262)  loss_scale: 16384.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6394 (8.8568)  time: 0.9181 (0.5144 -- 3.7546)  data: 0.3566 (0.0004 -- 2.9328)  max mem: 16413
Epoch: [148]  [51/52]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7152 (1.7395)  loss_scale: 16384.0000 (25836.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8628 (8.9026)  time: 0.7966 (0.5014 -- 2.0109)  data: 0.2683 (0.0002 -- 1.4885)  max mem: 16413
Epoch: [148] Total time: 0:00:49 (0.9531 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7152 (1.7774)  loss_scale: 16384.0000 (25836.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8628 (8.9026)
Val:  [0/9]  eta: 0:00:19  loss: 0.5478 (0.5478)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1912 (2.1912 -- 2.1912)  data: 2.0202 (2.0202 -- 2.0202)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6838 (0.8362)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3897 (0.1328 -- 2.1912)  data: 0.2245 (0.0001 -- 2.0202)  max mem: 16413
Val: Total time: 0:00:03 (0.3898 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.747
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [149]  [ 0/52]  eta: 0:06:26  lr: 0.000008  min_lr: 0.000000  loss: 1.8345 (1.8345)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7505 (6.7505)  time: 7.4305 (7.4305 -- 7.4305)  data: 5.0026 (5.0026 -- 5.0026)  max mem: 16413
Epoch: [149]  [20/52]  eta: 0:00:40  lr: 0.000007  min_lr: 0.000000  loss: 1.7655 (1.7984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7921 (8.7754)  time: 0.9680 (0.5246 -- 3.9437)  data: 0.0122 (0.0004 -- 0.2134)  max mem: 16413
Epoch: [149]  [40/52]  eta: 0:00:12  lr: 0.000007  min_lr: 0.000000  loss: 1.8114 (1.7806)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3745 (8.8227)  time: 0.7763 (0.5237 -- 2.5364)  data: 0.0012 (0.0003 -- 0.0022)  max mem: 16413
Epoch: [149]  [51/52]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7568 (1.7412)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4737 (9.1870)  time: 0.7624 (0.4973 -- 2.5269)  data: 0.0033 (0.0002 -- 0.0515)  max mem: 16413
Epoch: [149] Total time: 0:00:50 (0.9634 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7568 (1.7798)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4737 (9.1870)
Val:  [0/9]  eta: 0:00:18  loss: 0.5390 (0.5390)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0662 (2.0662 -- 2.0662)  data: 1.8931 (1.8931 -- 1.8931)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6779 (0.8370)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3764 (0.1332 -- 2.0662)  data: 0.2104 (0.0001 -- 1.8931)  max mem: 16413
Val: Total time: 0:00:03 (0.3765 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.742
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [150]  [ 0/52]  eta: 0:07:05  lr: 0.000007  min_lr: 0.000000  loss: 2.1600 (2.1600)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6851 (9.6851)  time: 8.1795 (8.1795 -- 8.1795)  data: 7.6508 (7.6508 -- 7.6508)  max mem: 16413
Epoch: [150]  [20/52]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.9789 (1.8908)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3974 (8.7934)  time: 0.8329 (0.5266 -- 3.9061)  data: 0.2898 (0.0003 -- 3.3640)  max mem: 16413
Epoch: [150]  [40/52]  eta: 0:00:11  lr: 0.000007  min_lr: 0.000000  loss: 1.6035 (1.7746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3367 (9.3126)  time: 0.8065 (0.5287 -- 2.0183)  data: 0.1008 (0.0007 -- 0.9955)  max mem: 16413
Epoch: [150]  [51/52]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7419 (1.7882)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1951 (9.0802)  time: 0.6897 (0.4982 -- 1.5155)  data: 0.1426 (0.0002 -- 0.9955)  max mem: 16413
Epoch: [150] Total time: 0:00:48 (0.9276 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7419 (1.7963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1951 (9.0802)
Val:  [0/9]  eta: 0:00:18  loss: 0.5267 (0.5267)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0338 (2.0338 -- 2.0338)  data: 1.8594 (1.8594 -- 1.8594)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6692 (0.8356)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3723 (0.1330 -- 2.0338)  data: 0.2067 (0.0001 -- 1.8594)  max mem: 16413
Val: Total time: 0:00:03 (0.3724 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.743
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [151]  [ 0/52]  eta: 0:06:19  lr: 0.000007  min_lr: 0.000000  loss: 1.7834 (1.7834)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6656 (8.6656)  time: 7.3049 (7.3049 -- 7.3049)  data: 6.7444 (6.7444 -- 6.7444)  max mem: 16413
[2023-10-24 01:23:36,610] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:23:36,610] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:23:36,612] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:23:36,612] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:23:44,032] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7864
[2023-10-24 01:23:44,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7864
[2023-10-24 01:23:44,033] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:23:44,033] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:23:44,033] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [151]  [20/52]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.8135 (1.8098)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9720 (8.4113)  time: 0.8249 (0.5287 -- 2.6324)  data: 0.2320 (0.0005 -- 2.1051)  max mem: 16413
Epoch: [151]  [40/52]  eta: 0:00:12  lr: 0.000007  min_lr: 0.000000  loss: 1.7290 (1.7767)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6313 (7.9703)  time: 0.8696 (0.5357 -- 3.1550)  data: 0.3163 (0.0005 -- 2.6144)  max mem: 16413
Epoch: [151]  [51/52]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8520 (1.8216)  loss_scale: 16384.0000 (19219.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0900 (8.0619)  time: 0.7897 (0.5025 -- 3.1550)  data: 0.2136 (0.0002 -- 2.6144)  max mem: 16413
Epoch: [151] Total time: 0:00:47 (0.9198 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8520 (1.8218)  loss_scale: 16384.0000 (19219.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0900 (8.0619)
Val:  [0/9]  eta: 0:00:19  loss: 0.5162 (0.5162)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1634 (2.1634 -- 2.1634)  data: 1.9927 (1.9927 -- 1.9927)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6692 (0.8368)  acc1: 85.7143 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3868 (0.1334 -- 2.1634)  data: 0.2215 (0.0001 -- 1.9927)  max mem: 16413
Val: Total time: 0:00:03 (0.3869 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.743
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [152]  [ 0/52]  eta: 0:06:40  lr: 0.000007  min_lr: 0.000000  loss: 2.2151 (2.2151)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0698 (11.0698)  time: 7.7109 (7.7109 -- 7.7109)  data: 7.1582 (7.1582 -- 7.1582)  max mem: 16413
Epoch: [152]  [20/52]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.7931 (1.8088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2363 (9.6575)  time: 0.8007 (0.5270 -- 4.7870)  data: 0.2167 (0.0004 -- 4.2447)  max mem: 16413
Epoch: [152]  [40/52]  eta: 0:00:12  lr: 0.000007  min_lr: 0.000000  loss: 1.7808 (1.7933)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4241 (9.3094)  time: 0.9023 (0.5195 -- 3.0245)  data: 0.1296 (0.0004 -- 2.1192)  max mem: 16413
Epoch: [152]  [51/52]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7664 (1.7710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1044 (9.0868)  time: 0.7096 (0.4976 -- 3.0245)  data: 0.0232 (0.0001 -- 0.3809)  max mem: 16413
Epoch: [152] Total time: 0:00:47 (0.9151 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7664 (1.8124)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1044 (9.0868)
Val:  [0/9]  eta: 0:00:19  loss: 0.5306 (0.5306)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1420 (2.1420 -- 2.1420)  data: 1.9588 (1.9588 -- 1.9588)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6662 (0.8316)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3846 (0.1332 -- 2.1420)  data: 0.2177 (0.0001 -- 1.9588)  max mem: 16413
Val: Total time: 0:00:03 (0.3847 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.744
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [153]  [ 0/52]  eta: 0:06:30  lr: 0.000006  min_lr: 0.000000  loss: 2.4518 (2.4518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8969 (7.8969)  time: 7.5076 (7.5076 -- 7.5076)  data: 6.9684 (6.9684 -- 6.9684)  max mem: 16413
Epoch: [153]  [20/52]  eta: 0:00:38  lr: 0.000006  min_lr: 0.000000  loss: 1.7558 (1.8090)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4663 (8.5069)  time: 0.8772 (0.5257 -- 3.5339)  data: 0.1588 (0.0003 -- 1.6885)  max mem: 16413
[2023-10-24 01:25:50,086] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:25:50,087] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:25:50,089] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:25:50,090] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [153]  [40/52]  eta: 0:00:12  lr: 0.000006  min_lr: 0.000000  loss: 1.8736 (1.8088)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1224 (8.3784)  time: 0.8318 (0.5200 -- 2.8607)  data: 0.0659 (0.0003 -- 0.8419)  max mem: 16413
[2023-10-24 01:25:55,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=42, lr=[1.4805070103360887e-07, 1.4805070103360887e-07, 1.974009347114785e-07, 1.974009347114785e-07, 2.632012462819713e-07, 2.632012462819713e-07, 3.509349950426284e-07, 3.509349950426284e-07, 4.6791332672350453e-07, 4.6791332672350453e-07, 6.238844356313394e-07, 6.238844356313394e-07, 8.318459141751192e-07, 8.318459141751192e-07, 1.1091278855668255e-06, 1.1091278855668255e-06, 1.4788371807557674e-06, 1.4788371807557674e-06, 1.9717829076743567e-06, 1.9717829076743567e-06, 2.629043876899142e-06, 2.629043876899142e-06, 3.505391835865523e-06, 3.505391835865523e-06, 4.67385578115403e-06, 4.67385578115403e-06, 6.231807708205374e-06, 6.231807708205374e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-24 01:25:55,210] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=18.514956777161782, CurrSamplesPerSec=24.38704800888429, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [153]  [51/52]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.9183 (1.7968)  loss_scale: 32768.0000 (21110.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7028 (8.2404)  time: 0.6852 (0.4969 -- 1.9804)  data: 0.0962 (0.0002 -- 1.4704)  max mem: 16413
Epoch: [153] Total time: 0:00:49 (0.9611 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.9183 (1.8282)  loss_scale: 32768.0000 (21110.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7028 (8.2404)
Val:  [0/9]  eta: 0:00:18  loss: 0.5371 (0.5371)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.0847 (2.0847 -- 2.0847)  data: 1.9147 (1.9147 -- 1.9147)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6499 (0.8380)  acc1: 77.7778 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3787 (0.1337 -- 2.0847)  data: 0.2128 (0.0001 -- 1.9147)  max mem: 16413
Val: Total time: 0:00:03 (0.3789 s / it)
* Acc@1 82.278 Acc@5 96.835 loss 0.750
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 85.44%
Epoch: [154]  [ 0/52]  eta: 0:06:28  lr: 0.000006  min_lr: 0.000000  loss: 1.3090 (1.3090)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5220 (5.5220)  time: 7.4770 (7.4770 -- 7.4770)  data: 6.8877 (6.8877 -- 6.8877)  max mem: 16413
Epoch: [154]  [20/52]  eta: 0:00:38  lr: 0.000006  min_lr: 0.000000  loss: 1.8211 (1.7864)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6990 (8.0974)  time: 0.8998 (0.5262 -- 4.5716)  data: 0.3502 (0.0009 -- 4.0436)  max mem: 16413
Epoch: [154]  [40/52]  eta: 0:00:12  lr: 0.000006  min_lr: 0.000000  loss: 1.5053 (1.7594)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3057 (8.0027)  time: 0.8297 (0.5139 -- 3.1055)  data: 0.2941 (0.0002 -- 2.5666)  max mem: 16413
Epoch: [154]  [51/52]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6423 (1.7469)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0705 (8.3283)  time: 0.7587 (0.4979 -- 2.9485)  data: 0.2430 (0.0002 -- 2.4229)  max mem: 16413
Epoch: [154] Total time: 0:00:49 (0.9459 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6423 (1.7349)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0705 (8.3283)
Val:  [0/9]  eta: 0:00:19  loss: 0.5404 (0.5404)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.1700 (2.1700 -- 2.1700)  data: 1.9930 (1.9930 -- 1.9930)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6492 (0.8344)  acc1: 77.7778 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3875 (0.1331 -- 2.1700)  data: 0.2215 (0.0001 -- 1.9930)  max mem: 16413
Val: Total time: 0:00:03 (0.3876 s / it)
* Acc@1 82.278 Acc@5 96.835 loss 0.749
Accuracy of the network on the 158 val images: 82.28%
Max accuracy: 85.44%
Epoch: [155]  [ 0/52]  eta: 0:06:57  lr: 0.000006  min_lr: 0.000000  loss: 1.6171 (1.6171)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6733 (8.6733)  time: 8.0241 (8.0241 -- 8.0241)  data: 7.4959 (7.4959 -- 7.4959)  max mem: 16413
[2023-10-24 01:27:20,419] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8077
[2023-10-24 01:27:20,419] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8077
[2023-10-24 01:27:20,419] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:27:20,419] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:27:20,419] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [155]  [20/52]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000000  loss: 2.0318 (1.9012)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1509 (8.7660)  time: 0.8939 (0.5245 -- 4.2565)  data: 0.3497 (0.0002 -- 3.7409)  max mem: 16413
Epoch: [155]  [40/52]  eta: 0:00:13  lr: 0.000006  min_lr: 0.000000  loss: 1.8003 (1.8277)  loss_scale: 16384.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9981 (9.0972)  time: 0.9328 (0.5165 -- 4.5176)  data: 0.3919 (0.0004 -- 3.9863)  max mem: 16413
Epoch: [155]  [51/52]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6716 (1.8084)  loss_scale: 16384.0000 (21740.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8123 (9.1800)  time: 0.7073 (0.4960 -- 3.2777)  data: 0.1920 (0.0001 -- 2.7656)  max mem: 16413
Epoch: [155] Total time: 0:00:50 (0.9632 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6716 (1.8061)  loss_scale: 16384.0000 (21740.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8123 (9.1800)
Val:  [0/9]  eta: 0:00:19  loss: 0.5345 (0.5345)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1276 (2.1276 -- 2.1276)  data: 1.9467 (1.9467 -- 1.9467)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6538 (0.8268)  acc1: 88.8889 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3834 (0.1340 -- 2.1276)  data: 0.2164 (0.0001 -- 1.9467)  max mem: 16413
Val: Total time: 0:00:03 (0.3835 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.740
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [156]  [ 0/52]  eta: 0:07:20  lr: 0.000006  min_lr: 0.000000  loss: 2.2763 (2.2763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1158 (10.1158)  time: 8.4677 (8.4677 -- 8.4677)  data: 5.6402 (5.6402 -- 5.6402)  max mem: 16413
Epoch: [156]  [20/52]  eta: 0:00:38  lr: 0.000006  min_lr: 0.000000  loss: 1.8662 (1.8360)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2173 (9.0426)  time: 0.8504 (0.5261 -- 4.4019)  data: 0.0018 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [156]  [40/52]  eta: 0:00:13  lr: 0.000005  min_lr: 0.000000  loss: 1.7790 (1.8004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6975 (8.6712)  time: 0.9708 (0.5107 -- 4.3376)  data: 0.0011 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [156]  [51/52]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8732 (1.7838)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6551 (8.7117)  time: 0.8607 (0.4952 -- 4.3376)  data: 0.0007 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [156] Total time: 0:00:51 (0.9972 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8732 (1.7861)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6551 (8.7117)
Val:  [0/9]  eta: 0:00:19  loss: 0.5280 (0.5280)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1136 (2.1136 -- 2.1136)  data: 1.9337 (1.9337 -- 1.9337)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6532 (0.8289)  acc1: 85.7143 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3813 (0.1324 -- 2.1136)  data: 0.2149 (0.0001 -- 1.9337)  max mem: 16413
Val: Total time: 0:00:03 (0.3814 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.739
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [157]  [ 0/52]  eta: 0:07:22  lr: 0.000005  min_lr: 0.000000  loss: 2.0337 (2.0337)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3767 (8.3767)  time: 8.5021 (8.5021 -- 8.5021)  data: 7.9538 (7.9538 -- 7.9538)  max mem: 16413
Epoch: [157]  [20/52]  eta: 0:00:39  lr: 0.000005  min_lr: 0.000000  loss: 1.7459 (1.8400)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7324 (8.6168)  time: 0.8593 (0.5291 -- 4.2143)  data: 0.2972 (0.0005 -- 3.7126)  max mem: 16413
Epoch: [157]  [40/52]  eta: 0:00:12  lr: 0.000005  min_lr: 0.000000  loss: 1.8657 (1.7899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4940 (8.3919)  time: 0.9214 (0.5180 -- 4.0954)  data: 0.1800 (0.0003 -- 2.1383)  max mem: 16413
[2023-10-24 01:29:30,157] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:29:30,157] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:29:30,157] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:29:30,157] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:29:31,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8208
[2023-10-24 01:29:31,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:29:31,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8208
[2023-10-24 01:29:31,146] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-24 01:29:31,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [157]  [51/52]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8509 (1.7897)  loss_scale: 16384.0000 (17014.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1708 (8.5027)  time: 0.7965 (0.4803 -- 4.0954)  data: 0.0722 (0.0002 -- 1.4336)  max mem: 16413
Epoch: [157] Total time: 0:00:49 (0.9591 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8509 (1.7839)  loss_scale: 16384.0000 (17014.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1708 (8.5027)
Val:  [0/9]  eta: 0:00:18  loss: 0.5277 (0.5277)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1066 (2.1066 -- 2.1066)  data: 1.9334 (1.9334 -- 1.9334)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6710 (0.8262)  acc1: 85.7143 (81.0127)  acc5: 100.0000 (96.2025)  time: 0.3820 (0.1369 -- 2.1066)  data: 0.2149 (0.0001 -- 1.9334)  max mem: 16413
Val: Total time: 0:00:03 (0.3821 s / it)
* Acc@1 82.911 Acc@5 96.835 loss 0.736
Accuracy of the network on the 158 val images: 82.91%
Max accuracy: 85.44%
Epoch: [158]  [ 0/52]  eta: 0:06:14  lr: 0.000005  min_lr: 0.000000  loss: 1.8471 (1.8471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5145 (10.5145)  time: 7.1928 (7.1928 -- 7.1928)  data: 4.2992 (4.2992 -- 4.2992)  max mem: 16413
Epoch: [158]  [20/52]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.6847 (1.6947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1831 (8.1475)  time: 0.8807 (0.5342 -- 2.2745)  data: 0.0855 (0.0006 -- 1.3314)  max mem: 16413
[2023-10-24 01:30:16,675] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8252
[2023-10-24 01:30:16,675] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8252
[2023-10-24 01:30:16,675] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:30:16,675] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:30:16,675] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [158]  [40/52]  eta: 0:00:12  lr: 0.000005  min_lr: 0.000000  loss: 1.7155 (1.7306)  loss_scale: 16384.0000 (15384.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9215 (8.6336)  time: 0.8358 (0.5270 -- 1.7604)  data: 0.1162 (0.0003 -- 1.0729)  max mem: 16413
Epoch: [158]  [51/52]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7267 (1.7563)  loss_scale: 8192.0000 (13863.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5610 (8.7821)  time: 0.7482 (0.4966 -- 1.7604)  data: 0.0916 (0.0001 -- 1.0729)  max mem: 16413
Epoch: [158] Total time: 0:00:48 (0.9287 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7267 (1.8011)  loss_scale: 8192.0000 (13863.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5610 (8.7821)
Val:  [0/9]  eta: 0:00:19  loss: 0.5132 (0.5132)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1673 (2.1673 -- 2.1673)  data: 1.9896 (1.9896 -- 1.9896)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6726 (0.8254)  acc1: 85.7143 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3872 (0.1325 -- 2.1673)  data: 0.2211 (0.0001 -- 1.9896)  max mem: 16413
Val: Total time: 0:00:03 (0.3873 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.737
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [159]  [ 0/52]  eta: 0:08:38  lr: 0.000005  min_lr: 0.000000  loss: 1.5719 (1.5719)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0486 (9.0486)  time: 9.9714 (9.9714 -- 9.9714)  data: 6.0417 (6.0417 -- 6.0417)  max mem: 16413
Epoch: [159]  [20/52]  eta: 0:00:39  lr: 0.000005  min_lr: 0.000000  loss: 1.9149 (1.8414)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2653 (9.2556)  time: 0.7936 (0.5179 -- 4.2787)  data: 0.0014 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [159]  [40/52]  eta: 0:00:12  lr: 0.000005  min_lr: 0.000000  loss: 1.8229 (1.8099)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1798 (9.2745)  time: 0.9067 (0.5238 -- 3.4947)  data: 0.0411 (0.0002 -- 0.7889)  max mem: 16413
Epoch: [159]  [51/52]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.6852 (1.7970)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2234 (9.2508)  time: 0.7372 (0.4971 -- 2.8197)  data: 0.0007 (0.0001 -- 0.0017)  max mem: 16413
Epoch: [159] Total time: 0:00:49 (0.9566 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.6852 (1.7812)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2234 (9.2508)
[2023-10-24 01:31:19,864] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-10-24 01:31:19,867] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-10-24 01:31:19,869] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-159/mp_rank_00_model_states.pt
[2023-10-24 01:31:19,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-159/mp_rank_00_model_states.pt...
[2023-10-24 01:31:20,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-159/mp_rank_00_model_states.pt.
[2023-10-24 01:31:20,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [0/9]  eta: 0:00:19  loss: 0.4976 (0.4976)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1231 (2.1231 -- 2.1231)  data: 1.9455 (1.9455 -- 1.9455)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6528 (0.8226)  acc1: 88.8889 (84.8101)  acc5: 100.0000 (96.2025)  time: 0.3834 (0.1347 -- 2.1231)  data: 0.2163 (0.0001 -- 1.9455)  max mem: 16413
Val: Total time: 0:00:03 (0.3836 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.733
Accuracy of the network on the 158 val images: 84.81%
Max accuracy: 85.44%
Epoch: [160]  [ 0/52]  eta: 0:07:35  lr: 0.000005  min_lr: 0.000000  loss: 2.1784 (2.1784)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5715 (10.5715)  time: 8.7640 (8.7640 -- 8.7640)  data: 5.5467 (5.5467 -- 5.5467)  max mem: 16413
Epoch: [160]  [20/52]  eta: 0:00:35  lr: 0.000005  min_lr: 0.000000  loss: 1.8212 (1.7387)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0300 (8.8868)  time: 0.7244 (0.5239 -- 2.8293)  data: 0.0016 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [160]  [40/52]  eta: 0:00:12  lr: 0.000005  min_lr: 0.000000  loss: 1.7122 (1.7458)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1823 (8.8732)  time: 0.8895 (0.5387 -- 3.1393)  data: 0.1303 (0.0007 -- 1.6693)  max mem: 16413
Epoch: [160]  [51/52]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7811 (1.7629)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3295 (9.1774)  time: 0.7287 (0.5001 -- 3.1393)  data: 0.1001 (0.0001 -- 1.1058)  max mem: 16413
Epoch: [160] Total time: 0:00:47 (0.9205 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7811 (1.7873)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3295 (9.1774)
Val:  [0/9]  eta: 0:00:19  loss: 0.5020 (0.5020)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1996 (2.1996 -- 2.1996)  data: 2.0171 (2.0171 -- 2.0171)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6481 (0.8202)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3906 (0.1325 -- 2.1996)  data: 0.2242 (0.0001 -- 2.0171)  max mem: 16413
Val: Total time: 0:00:03 (0.3907 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.732
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [161]  [ 0/52]  eta: 0:06:56  lr: 0.000005  min_lr: 0.000000  loss: 2.2113 (2.2113)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2555 (12.2555)  time: 8.0134 (8.0134 -- 8.0134)  data: 7.4895 (7.4895 -- 7.4895)  max mem: 16413
[2023-10-24 01:32:31,629] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:32:31,629] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 01:32:31,631] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:32:31,631] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [161]  [20/52]  eta: 0:00:39  lr: 0.000004  min_lr: 0.000000  loss: 1.7800 (1.8531)  loss_scale: 16384.0000 (12873.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6956 (8.2393)  time: 0.9037 (0.5183 -- 4.6621)  data: 0.3612 (0.0003 -- 4.1361)  max mem: 16413
Epoch: [161]  [40/52]  eta: 0:00:13  lr: 0.000004  min_lr: 0.000000  loss: 1.7780 (1.8517)  loss_scale: 16384.0000 (14585.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6206 (8.7962)  time: 0.9242 (0.5171 -- 4.3333)  data: 0.3846 (0.0004 -- 3.8204)  max mem: 16413
Epoch: [161]  [51/52]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7624 (1.8229)  loss_scale: 16384.0000 (14966.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8224 (8.8432)  time: 0.7649 (0.4942 -- 4.3333)  data: 0.2486 (0.0001 -- 3.8204)  max mem: 16413
Epoch: [161] Total time: 0:00:50 (0.9636 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7624 (1.7560)  loss_scale: 16384.0000 (14966.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8224 (8.8432)
Val:  [0/9]  eta: 0:00:19  loss: 0.5039 (0.5039)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1473 (2.1473 -- 2.1473)  data: 1.9698 (1.9698 -- 1.9698)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6466 (0.8183)  acc1: 88.8889 (84.8101)  acc5: 100.0000 (96.2025)  time: 0.3854 (0.1329 -- 2.1473)  data: 0.2190 (0.0001 -- 1.9698)  max mem: 16413
Val: Total time: 0:00:03 (0.3855 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.730
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [162]  [ 0/52]  eta: 0:07:13  lr: 0.000004  min_lr: 0.000000  loss: 2.0875 (2.0875)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1923 (8.1923)  time: 8.3444 (8.3444 -- 8.3444)  data: 7.8315 (7.8315 -- 7.8315)  max mem: 16413
[2023-10-24 01:33:33,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8442
[2023-10-24 01:33:33,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8442
[2023-10-24 01:33:33,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:33:33,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:33:33,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [162]  [20/52]  eta: 0:00:39  lr: 0.000004  min_lr: 0.000000  loss: 1.8405 (1.8060)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6189 (9.0494)  time: 0.8663 (0.5161 -- 4.3579)  data: 0.3255 (0.0004 -- 3.8088)  max mem: 16413
Epoch: [162]  [40/52]  eta: 0:00:12  lr: 0.000004  min_lr: 0.000000  loss: 1.7515 (1.7705)  loss_scale: 8192.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2629 (9.1034)  time: 0.8973 (0.5141 -- 4.3887)  data: 0.3515 (0.0004 -- 3.8595)  max mem: 16413
Epoch: [162]  [51/52]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6560 (1.7409)  loss_scale: 8192.0000 (11027.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7985 (8.8686)  time: 0.6748 (0.4947 -- 3.5950)  data: 0.1575 (0.0001 -- 3.0773)  max mem: 16413
Epoch: [162] Total time: 0:00:49 (0.9454 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6560 (1.8049)  loss_scale: 8192.0000 (11027.6923)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7985 (8.8686)
Val:  [0/9]  eta: 0:00:19  loss: 0.5130 (0.5130)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1327 (2.1327 -- 2.1327)  data: 1.9651 (1.9651 -- 1.9651)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6475 (0.8206)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3828 (0.1326 -- 2.1327)  data: 0.2184 (0.0001 -- 1.9651)  max mem: 16413
Val: Total time: 0:00:03 (0.3829 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.732
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [163]  [ 0/52]  eta: 0:08:10  lr: 0.000004  min_lr: 0.000000  loss: 1.4677 (1.4677)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5131 (7.5131)  time: 9.4276 (9.4276 -- 9.4276)  data: 8.8918 (8.8918 -- 8.8918)  max mem: 16413
Epoch: [163]  [20/52]  eta: 0:00:40  lr: 0.000004  min_lr: 0.000000  loss: 1.9026 (1.9109)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7894 (8.8808)  time: 0.8662 (0.5298 -- 2.6476)  data: 0.1266 (0.0004 -- 1.9597)  max mem: 16413
Epoch: [163]  [40/52]  eta: 0:00:12  lr: 0.000004  min_lr: 0.000000  loss: 1.8432 (1.8900)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7648 (9.0555)  time: 0.8280 (0.5216 -- 2.6758)  data: 0.0835 (0.0002 -- 0.8857)  max mem: 16413
Epoch: [163]  [51/52]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7327 (1.8944)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4344 (8.8505)  time: 0.7019 (0.4944 -- 2.3872)  data: 0.0834 (0.0002 -- 0.8857)  max mem: 16413
Epoch: [163] Total time: 0:00:49 (0.9446 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7327 (1.8499)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4344 (8.8505)
Val:  [0/9]  eta: 0:00:19  loss: 0.5100 (0.5100)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2087 (2.2087 -- 2.2087)  data: 2.0264 (2.0264 -- 2.0264)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6525 (0.8203)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3921 (0.1330 -- 2.2087)  data: 0.2252 (0.0001 -- 2.0264)  max mem: 16413
Val: Total time: 0:00:03 (0.3923 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.731
Accuracy of the network on the 158 val images: 84.81%
Max accuracy: 85.44%
Epoch: [164]  [ 0/52]  eta: 0:07:24  lr: 0.000004  min_lr: 0.000000  loss: 2.3541 (2.3541)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3310 (8.3310)  time: 8.5475 (8.5475 -- 8.5475)  data: 8.0451 (8.0451 -- 8.0451)  max mem: 16413
Epoch: [164]  [20/52]  eta: 0:00:38  lr: 0.000004  min_lr: 0.000000  loss: 1.8147 (1.8929)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7943 (7.8273)  time: 0.8261 (0.5250 -- 3.3377)  data: 0.2807 (0.0001 -- 2.7915)  max mem: 16413
Epoch: [164]  [40/52]  eta: 0:00:12  lr: 0.000004  min_lr: 0.000000  loss: 1.7707 (1.8461)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7333 (8.5201)  time: 0.8956 (0.5197 -- 3.1179)  data: 0.3125 (0.0004 -- 2.5957)  max mem: 16413
[2023-10-24 01:35:39,134] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:35:39,134] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 01:35:39,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:35:39,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [164]  [51/52]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8430 (1.8335)  loss_scale: 8192.0000 (9609.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3256 (8.7544)  time: 0.7487 (0.4956 -- 2.9376)  data: 0.1884 (0.0001 -- 2.3972)  max mem: 16413
Epoch: [164] Total time: 0:00:48 (0.9355 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8430 (1.8032)  loss_scale: 8192.0000 (9609.8462)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3256 (8.7544)
Val:  [0/9]  eta: 0:00:18  loss: 0.5051 (0.5051)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0448 (2.0448 -- 2.0448)  data: 1.8742 (1.8742 -- 1.8742)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6520 (0.8222)  acc1: 77.7778 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3737 (0.1328 -- 2.0448)  data: 0.2083 (0.0001 -- 1.8742)  max mem: 16413
Val: Total time: 0:00:03 (0.3738 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.730
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [165]  [ 0/52]  eta: 0:08:46  lr: 0.000004  min_lr: 0.000000  loss: 2.0621 (2.0621)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2633 (7.2633)  time: 10.1329 (10.1329 -- 10.1329)  data: 9.6216 (9.6216 -- 9.6216)  max mem: 16413
Epoch: [165]  [20/52]  eta: 0:00:41  lr: 0.000004  min_lr: 0.000000  loss: 1.7409 (1.8129)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9864 (9.4959)  time: 0.8675 (0.5123 -- 4.9524)  data: 0.3292 (0.0003 -- 4.4515)  max mem: 16413
Epoch: [165]  [40/52]  eta: 0:00:12  lr: 0.000004  min_lr: 0.000000  loss: 1.7297 (1.7955)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0886 (8.8039)  time: 0.8070 (0.5227 -- 2.3272)  data: 0.2657 (0.0004 -- 1.8164)  max mem: 16413
Epoch: [165]  [51/52]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.9323 (1.8329)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0027 (8.6885)  time: 0.6930 (0.4952 -- 2.2987)  data: 0.1744 (0.0001 -- 1.7849)  max mem: 16413
Epoch: [165] Total time: 0:00:49 (0.9457 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.9323 (1.8166)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0027 (8.6885)
Val:  [0/9]  eta: 0:00:19  loss: 0.5063 (0.5063)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1906 (2.1906 -- 2.1906)  data: 2.0057 (2.0057 -- 2.0057)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6525 (0.8224)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3902 (0.1327 -- 2.1906)  data: 0.2229 (0.0001 -- 2.0057)  max mem: 16413
Val: Total time: 0:00:03 (0.3903 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.731
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [166]  [ 0/52]  eta: 0:05:50  lr: 0.000003  min_lr: 0.000000  loss: 0.9523 (0.9523)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9841 (4.9841)  time: 6.7438 (6.7438 -- 6.7438)  data: 6.1593 (6.1593 -- 6.1593)  max mem: 16413
Epoch: [166]  [20/52]  eta: 0:00:38  lr: 0.000003  min_lr: 0.000000  loss: 1.8422 (1.7333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8471 (8.6852)  time: 0.9319 (0.5216 -- 3.2035)  data: 0.2555 (0.0008 -- 2.6848)  max mem: 16413
Epoch: [166]  [40/52]  eta: 0:00:12  lr: 0.000003  min_lr: 0.000000  loss: 1.6502 (1.7196)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6302 (8.5044)  time: 0.9466 (0.5057 -- 4.1373)  data: 0.2323 (0.0003 -- 3.1590)  max mem: 16413
Epoch: [166]  [51/52]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7147 (1.7319)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8109 (8.5331)  time: 0.7748 (0.4952 -- 4.1373)  data: 0.0740 (0.0001 -- 1.4712)  max mem: 16413
Epoch: [166] Total time: 0:00:50 (0.9625 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7147 (1.7435)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8109 (8.5331)
Val:  [0/9]  eta: 0:00:19  loss: 0.5115 (0.5115)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1286 (2.1286 -- 2.1286)  data: 1.9487 (1.9487 -- 1.9487)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6563 (0.8232)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3833 (0.1335 -- 2.1286)  data: 0.2166 (0.0001 -- 1.9487)  max mem: 16413
Val: Total time: 0:00:03 (0.3834 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.732
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [167]  [ 0/52]  eta: 0:06:04  lr: 0.000003  min_lr: 0.000000  loss: 1.7037 (1.7037)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1603 (10.1603)  time: 7.0020 (7.0020 -- 7.0020)  data: 6.4655 (6.4655 -- 6.4655)  max mem: 16413
[2023-10-24 01:37:49,856] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8694
[2023-10-24 01:37:49,856] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:37:49,856] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8694
[2023-10-24 01:37:49,856] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:37:49,856] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [167]  [20/52]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000000  loss: 2.0100 (1.8963)  loss_scale: 8192.0000 (12092.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3836 (8.6223)  time: 0.9329 (0.5332 -- 2.3916)  data: 0.1942 (0.0002 -- 1.3028)  max mem: 16413
Epoch: [167]  [40/52]  eta: 0:00:12  lr: 0.000003  min_lr: 0.000000  loss: 1.7741 (1.8333)  loss_scale: 8192.0000 (10190.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0948 (8.4505)  time: 0.8062 (0.5219 -- 1.8021)  data: 0.1199 (0.0005 -- 1.1250)  max mem: 16413
Epoch: [167]  [51/52]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7577 (1.8056)  loss_scale: 8192.0000 (9767.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0948 (8.5999)  time: 0.7617 (0.4951 -- 1.8021)  data: 0.1491 (0.0001 -- 1.1250)  max mem: 16413
Epoch: [167] Total time: 0:00:48 (0.9275 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7577 (1.8197)  loss_scale: 8192.0000 (9767.3846)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0948 (8.5999)
Val:  [0/9]  eta: 0:00:19  loss: 0.5147 (0.5147)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1888 (2.1888 -- 2.1888)  data: 2.0191 (2.0191 -- 2.0191)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6478 (0.8226)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3890 (0.1330 -- 2.1888)  data: 0.2244 (0.0001 -- 2.0191)  max mem: 16413
Val: Total time: 0:00:03 (0.3891 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.732
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [168]  [ 0/52]  eta: 0:06:35  lr: 0.000003  min_lr: 0.000000  loss: 2.0223 (2.0223)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3617 (8.3617)  time: 7.5992 (7.5992 -- 7.5992)  data: 7.0592 (7.0592 -- 7.0592)  max mem: 16413
Epoch: [168]  [20/52]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000000  loss: 1.9309 (1.9285)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8220 (8.9346)  time: 0.9264 (0.5193 -- 3.4187)  data: 0.1712 (0.0004 -- 2.1074)  max mem: 16413
Epoch: [168]  [40/52]  eta: 0:00:12  lr: 0.000003  min_lr: 0.000000  loss: 1.7734 (1.8649)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9873 (9.0352)  time: 0.8648 (0.5171 -- 3.2059)  data: 0.0013 (0.0002 -- 0.0069)  max mem: 16413
Epoch: [168]  [51/52]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7531 (1.8639)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9368 (8.9580)  time: 0.6887 (0.4965 -- 2.7978)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [168] Total time: 0:00:50 (0.9628 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7531 (1.8331)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9368 (8.9580)
Val:  [0/9]  eta: 0:00:19  loss: 0.5282 (0.5282)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1804 (2.1804 -- 2.1804)  data: 1.9993 (1.9993 -- 1.9993)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6479 (0.8254)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3895 (0.1330 -- 2.1804)  data: 0.2222 (0.0001 -- 1.9993)  max mem: 16413
Val: Total time: 0:00:03 (0.3896 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.734
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [169]  [ 0/52]  eta: 0:06:45  lr: 0.000003  min_lr: 0.000000  loss: 2.0554 (2.0554)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1042 (8.1042)  time: 7.7913 (7.7913 -- 7.7913)  data: 5.7032 (5.7032 -- 5.7032)  max mem: 16413
Epoch: [169]  [20/52]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000000  loss: 1.7803 (1.8553)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6912 (8.4675)  time: 0.9101 (0.5325 -- 3.2692)  data: 0.1764 (0.0002 -- 2.2334)  max mem: 16413
[2023-10-24 01:39:59,749] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:39:59,749] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:39:59,750] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 01:39:59,750] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [169]  [40/52]  eta: 0:00:12  lr: 0.000003  min_lr: 0.000000  loss: 1.8631 (1.8678)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5895 (8.1738)  time: 0.9109 (0.5128 -- 4.3000)  data: 0.3587 (0.0003 -- 3.7899)  max mem: 16413
Epoch: [169]  [51/52]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7942 (1.8510)  loss_scale: 16384.0000 (10870.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7588 (8.0119)  time: 0.7963 (0.4946 -- 4.3000)  data: 0.2675 (0.0002 -- 3.7899)  max mem: 16413
Epoch: [169] Total time: 0:00:51 (0.9845 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7942 (1.8217)  loss_scale: 16384.0000 (10870.1538)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7588 (8.0119)
Val:  [0/9]  eta: 0:00:19  loss: 0.5311 (0.5311)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1991 (2.1991 -- 2.1991)  data: 2.0182 (2.0182 -- 2.0182)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6569 (0.8279)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3927 (0.1322 -- 2.1991)  data: 0.2243 (0.0001 -- 2.0182)  max mem: 16413
Val: Total time: 0:00:03 (0.3928 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.735
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [170]  [ 0/52]  eta: 0:06:40  lr: 0.000003  min_lr: 0.000000  loss: 2.5809 (2.5809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0690 (8.0690)  time: 7.6936 (7.6936 -- 7.6936)  data: 7.1521 (7.1521 -- 7.1521)  max mem: 16413
Epoch: [170]  [20/52]  eta: 0:00:38  lr: 0.000003  min_lr: 0.000000  loss: 1.8684 (1.9204)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6780 (9.2340)  time: 0.8914 (0.5266 -- 2.7884)  data: 0.3516 (0.0003 -- 2.2385)  max mem: 16413
Epoch: [170]  [40/52]  eta: 0:00:13  lr: 0.000003  min_lr: 0.000000  loss: 1.8708 (1.8551)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0953 (8.7735)  time: 0.9713 (0.5023 -- 4.7975)  data: 0.4380 (0.0002 -- 4.2804)  max mem: 16413
Epoch: [170]  [51/52]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8669 (1.8217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3153 (8.6508)  time: 0.7885 (0.4952 -- 4.7975)  data: 0.2786 (0.0001 -- 4.2804)  max mem: 16413
Epoch: [170] Total time: 0:00:50 (0.9704 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8669 (1.8305)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3153 (8.6508)
Val:  [0/9]  eta: 0:00:18  loss: 0.5359 (0.5359)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1023 (2.1023 -- 2.1023)  data: 1.9286 (1.9286 -- 1.9286)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6619 (0.8275)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3810 (0.1330 -- 2.1023)  data: 0.2144 (0.0001 -- 1.9286)  max mem: 16413
Val: Total time: 0:00:03 (0.3811 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.735
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [171]  [ 0/52]  eta: 0:07:17  lr: 0.000003  min_lr: 0.000000  loss: 1.3539 (1.3539)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8995 (10.8995)  time: 8.4042 (8.4042 -- 8.4042)  data: 7.2594 (7.2594 -- 7.2594)  max mem: 16413
Epoch: [171]  [20/52]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000000  loss: 1.6840 (1.6516)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3250 (8.8181)  time: 0.8475 (0.5209 -- 4.4295)  data: 0.1352 (0.0004 -- 2.4177)  max mem: 16413
Epoch: [171]  [40/52]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000000  loss: 1.6995 (1.7441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6175 (8.8413)  time: 0.8958 (0.5130 -- 3.7567)  data: 0.0489 (0.0002 -- 0.9592)  max mem: 16413
Epoch: [171]  [51/52]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7111 (1.7434)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6203 (8.9883)  time: 0.7298 (0.4941 -- 3.7567)  data: 0.0006 (0.0001 -- 0.0015)  max mem: 16413
Epoch: [171] Total time: 0:00:49 (0.9533 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7111 (1.8103)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6203 (8.9883)
Val:  [0/9]  eta: 0:00:20  loss: 0.5344 (0.5344)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2310 (2.2310 -- 2.2310)  data: 2.0526 (2.0526 -- 2.0526)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6618 (0.8282)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3966 (0.1324 -- 2.2310)  data: 0.2281 (0.0001 -- 2.0526)  max mem: 16413
Val: Total time: 0:00:03 (0.3967 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.734
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [172]  [ 0/52]  eta: 0:06:26  lr: 0.000002  min_lr: 0.000000  loss: 1.3321 (1.3321)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2904 (8.2904)  time: 7.4398 (7.4398 -- 7.4398)  data: 5.5816 (5.5816 -- 5.5816)  max mem: 16413
[2023-10-24 01:42:12,575] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:42:12,575] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:42:12,578] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:42:12,579] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:42:19,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8958
[2023-10-24 01:42:19,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8958
[2023-10-24 01:42:19,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:42:19,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:42:19,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [172]  [20/52]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 1.8924 (1.8817)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0597 (9.1312)  time: 0.9359 (0.5095 -- 2.7308)  data: 0.1236 (0.0003 -- 1.4262)  max mem: 16413
Epoch: [172]  [40/52]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000000  loss: 1.7409 (1.8334)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9776 (8.6715)  time: 0.8567 (0.4987 -- 4.2856)  data: 0.0036 (0.0001 -- 0.0488)  max mem: 16413
Epoch: [172]  [51/52]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8186 (1.8341)  loss_scale: 16384.0000 (18589.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7552 (8.7576)  time: 0.7465 (0.4962 -- 4.2856)  data: 0.0029 (0.0001 -- 0.0488)  max mem: 16413
Epoch: [172] Total time: 0:00:49 (0.9510 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8186 (1.7441)  loss_scale: 16384.0000 (18589.5385)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7552 (8.7576)
Val:  [0/9]  eta: 0:00:19  loss: 0.5339 (0.5339)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1473 (2.1473 -- 2.1473)  data: 1.9798 (1.9798 -- 1.9798)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6590 (0.8259)  acc1: 88.8889 (84.8101)  acc5: 100.0000 (96.2025)  time: 0.3851 (0.1325 -- 2.1473)  data: 0.2201 (0.0001 -- 1.9798)  max mem: 16413
Val: Total time: 0:00:03 (0.3852 s / it)
* Acc@1 85.443 Acc@5 96.835 loss 0.732
Accuracy of the network on the 158 val images: 85.44%
[2023-10-24 01:42:52,944] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-10-24 01:42:52,946] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt
[2023-10-24 01:42:52,946] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt...
[2023-10-24 01:42:52,946] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-10-24 01:42:54,367] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-best/mp_rank_00_model_states.pt.
[2023-10-24 01:42:54,368] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.44%
Epoch: [173]  [ 0/52]  eta: 0:06:32  lr: 0.000002  min_lr: 0.000000  loss: 1.6682 (1.6682)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8415 (7.8415)  time: 7.5456 (7.5456 -- 7.5456)  data: 5.6198 (5.6198 -- 5.6198)  max mem: 16413
[2023-10-24 01:43:03,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=48, lr=[5.269748266006533e-08, 5.269748266006533e-08, 7.026331021342043e-08, 7.026331021342043e-08, 9.368441361789391e-08, 9.368441361789391e-08, 1.249125514905252e-07, 1.249125514905252e-07, 1.665500686540336e-07, 1.665500686540336e-07, 2.2206675820537814e-07, 2.2206675820537814e-07, 2.960890109405042e-07, 2.960890109405042e-07, 3.9478534792067226e-07, 3.9478534792067226e-07, 5.263804638942297e-07, 5.263804638942297e-07, 7.018406185256396e-07, 7.018406185256396e-07, 9.357874913675195e-07, 9.357874913675195e-07, 1.2477166551566925e-06, 1.2477166551566925e-06, 1.6636222068755902e-06, 1.6636222068755902e-06, 2.218162942500787e-06, 2.218162942500787e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-24 01:43:03,543] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=18.517823044967873, CurrSamplesPerSec=22.316517939090716, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [173]  [20/52]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000000  loss: 1.7974 (1.7513)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7448 (8.7667)  time: 0.8800 (0.5236 -- 2.8396)  data: 0.2109 (0.0005 -- 1.7526)  max mem: 16413
Epoch: [173]  [40/52]  eta: 0:00:11  lr: 0.000002  min_lr: 0.000000  loss: 1.8175 (1.7611)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0058 (8.8369)  time: 0.7845 (0.5156 -- 2.0664)  data: 0.1362 (0.0005 -- 1.2857)  max mem: 16413
Epoch: [173]  [51/52]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6182 (1.7350)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1131 (8.9141)  time: 0.7492 (0.4942 -- 2.0664)  data: 0.0800 (0.0001 -- 1.2857)  max mem: 16413
Epoch: [173] Total time: 0:00:47 (0.9157 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6182 (1.7496)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1131 (8.9141)
Val:  [0/9]  eta: 0:00:18  loss: 0.5273 (0.5273)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0820 (2.0820 -- 2.0820)  data: 1.8985 (1.8985 -- 1.8985)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6546 (0.8261)  acc1: 88.8889 (84.8101)  acc5: 100.0000 (96.2025)  time: 0.3782 (0.1329 -- 2.0820)  data: 0.2110 (0.0001 -- 1.8985)  max mem: 16413
Val: Total time: 0:00:03 (0.3784 s / it)
* Acc@1 85.443 Acc@5 96.835 loss 0.731
Accuracy of the network on the 158 val images: 85.44%
Max accuracy: 85.44%
Epoch: [174]  [ 0/52]  eta: 0:06:00  lr: 0.000002  min_lr: 0.000000  loss: 1.2880 (1.2880)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7250 (9.7250)  time: 6.9356 (6.9356 -- 6.9356)  data: 6.0124 (6.0124 -- 6.0124)  max mem: 16413
Epoch: [174]  [20/52]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7371 (1.7767)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2600 (9.6262)  time: 0.8830 (0.5272 -- 2.9978)  data: 0.1751 (0.0005 -- 1.5357)  max mem: 16413
[2023-10-24 01:44:26,347] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:44:26,347] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:44:26,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:44:26,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [174]  [40/52]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000000  loss: 1.8169 (1.7768)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1783 (9.1359)  time: 0.8445 (0.5329 -- 2.7486)  data: 0.1955 (0.0004 -- 2.1858)  max mem: 16413
Epoch: [174]  [51/52]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7270 (1.7712)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2961 (9.1012)  time: 0.6646 (0.5004 -- 2.2532)  data: 0.1054 (0.0002 -- 1.6994)  max mem: 16413
Epoch: [174] Total time: 0:00:47 (0.9161 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7270 (1.7715)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2961 (9.1012)
Val:  [0/9]  eta: 0:00:18  loss: 0.5278 (0.5278)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0259 (2.0259 -- 2.0259)  data: 1.8433 (1.8433 -- 1.8433)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6506 (0.8256)  acc1: 85.7143 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3723 (0.1328 -- 2.0259)  data: 0.2049 (0.0001 -- 1.8433)  max mem: 16413
Val: Total time: 0:00:03 (0.3725 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.731
Accuracy of the network on the 158 val images: 84.81%
Max accuracy: 85.44%
Epoch: [175]  [ 0/52]  eta: 0:06:13  lr: 0.000002  min_lr: 0.000000  loss: 1.2141 (1.2141)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7404 (11.7404)  time: 7.1884 (7.1884 -- 7.1884)  data: 5.6706 (5.6706 -- 5.6706)  max mem: 16413
[2023-10-24 01:44:45,780] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9104
[2023-10-24 01:44:45,780] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:44:45,780] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9104
[2023-10-24 01:44:45,780] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-10-24 01:44:45,780] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [175]  [20/52]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9260 (1.7888)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6126 (8.1269)  time: 0.8628 (0.5219 -- 4.5627)  data: 0.0574 (0.0004 -- 0.8210)  max mem: 16413
Epoch: [175]  [40/52]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000000  loss: 1.8333 (1.7804)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8137 (8.6001)  time: 0.9517 (0.5092 -- 4.4578)  data: 0.1035 (0.0003 -- 1.1591)  max mem: 16413
Epoch: [175]  [51/52]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7568 (1.7719)  loss_scale: 16384.0000 (17644.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2234 (8.6101)  time: 0.8088 (0.4962 -- 4.4578)  data: 0.1019 (0.0001 -- 1.1591)  max mem: 16413
Epoch: [175] Total time: 0:00:49 (0.9529 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7568 (1.7990)  loss_scale: 16384.0000 (17644.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2234 (8.6101)
Val:  [0/9]  eta: 0:00:19  loss: 0.5266 (0.5266)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1722 (2.1722 -- 2.1722)  data: 1.9958 (1.9958 -- 1.9958)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6505 (0.8285)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3876 (0.1327 -- 2.1722)  data: 0.2218 (0.0001 -- 1.9958)  max mem: 16413
Val: Total time: 0:00:03 (0.3878 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.733
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [176]  [ 0/52]  eta: 0:06:04  lr: 0.000002  min_lr: 0.000000  loss: 1.9391 (1.9391)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9960 (8.9960)  time: 7.0043 (7.0043 -- 7.0043)  data: 5.7647 (5.7647 -- 5.7647)  max mem: 16413
Epoch: [176]  [20/52]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000000  loss: 1.7882 (1.8242)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9937 (9.0310)  time: 0.8992 (0.5188 -- 3.0497)  data: 0.2101 (0.0004 -- 1.8039)  max mem: 16413
Epoch: [176]  [40/52]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000000  loss: 1.5858 (1.7445)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0721 (9.1094)  time: 0.8359 (0.5243 -- 2.2642)  data: 0.0810 (0.0004 -- 0.8757)  max mem: 16413
Epoch: [176]  [51/52]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6390 (1.7570)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7056 (8.9112)  time: 0.7795 (0.5038 -- 2.2642)  data: 0.1022 (0.0001 -- 0.8757)  max mem: 16413
Epoch: [176] Total time: 0:00:47 (0.9202 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6390 (1.7807)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7056 (8.9112)
Val:  [0/9]  eta: 0:00:18  loss: 0.5236 (0.5236)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0661 (2.0661 -- 2.0661)  data: 1.8865 (1.8865 -- 1.8865)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6484 (0.8268)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3756 (0.1326 -- 2.0661)  data: 0.2097 (0.0001 -- 1.8865)  max mem: 16413
Val: Total time: 0:00:03 (0.3757 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.732
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [177]  [ 0/52]  eta: 0:07:59  lr: 0.000002  min_lr: 0.000000  loss: 2.2019 (2.2019)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7698 (13.7698)  time: 9.2294 (9.2294 -- 9.2294)  data: 6.3300 (6.3300 -- 6.3300)  max mem: 16413
Epoch: [177]  [20/52]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000000  loss: 1.9596 (1.9032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5200 (8.9669)  time: 0.8134 (0.5234 -- 3.6573)  data: 0.1631 (0.0001 -- 1.9547)  max mem: 16413
[2023-10-24 01:46:53,174] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:46:53,174] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:46:53,177] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:46:53,177] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [177]  [40/52]  eta: 0:00:12  lr: 0.000002  min_lr: 0.000000  loss: 1.7695 (1.8475)  loss_scale: 32768.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0648 (8.8802)  time: 0.8546 (0.5182 -- 3.2028)  data: 0.1343 (0.0001 -- 1.2228)  max mem: 16413
Epoch: [177]  [51/52]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8688 (1.8313)  loss_scale: 32768.0000 (23630.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2497 (8.6913)  time: 0.7433 (0.5001 -- 3.2028)  data: 0.0923 (0.0002 -- 1.2228)  max mem: 16413
Epoch: [177] Total time: 0:00:48 (0.9314 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8688 (1.8075)  loss_scale: 32768.0000 (23630.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2497 (8.6913)
Val:  [0/9]  eta: 0:00:19  loss: 0.5246 (0.5246)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1258 (2.1258 -- 2.1258)  data: 1.9505 (1.9505 -- 1.9505)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6473 (0.8245)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3833 (0.1330 -- 2.1258)  data: 0.2168 (0.0001 -- 1.9505)  max mem: 16413
Val: Total time: 0:00:03 (0.3834 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.730
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [178]  [ 0/52]  eta: 0:07:59  lr: 0.000002  min_lr: 0.000000  loss: 1.4072 (1.4072)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5653 (7.5653)  time: 9.2199 (9.2199 -- 9.2199)  data: 7.0639 (7.0639 -- 7.0639)  max mem: 16413
Epoch: [178]  [20/52]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 1.7191 (1.7198)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4550 (9.1505)  time: 0.8348 (0.5138 -- 3.7664)  data: 0.0170 (0.0005 -- 0.3114)  max mem: 16413
Epoch: [178]  [40/52]  eta: 0:00:13  lr: 0.000001  min_lr: 0.000000  loss: 1.8123 (1.7599)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0343 (8.9841)  time: 0.9330 (0.5269 -- 3.7439)  data: 0.2978 (0.0004 -- 3.2485)  max mem: 16413
Epoch: [178]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7610 (1.7571)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3536 (8.8548)  time: 0.8146 (0.4943 -- 3.7439)  data: 0.2947 (0.0001 -- 3.2485)  max mem: 16413
Epoch: [178] Total time: 0:00:50 (0.9713 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7610 (1.7905)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3536 (8.8548)
Val:  [0/9]  eta: 0:00:19  loss: 0.5283 (0.5283)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1422 (2.1422 -- 2.1422)  data: 1.9732 (1.9732 -- 1.9732)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6505 (0.8247)  acc1: 85.7143 (82.2785)  acc5: 100.0000 (96.2025)  time: 0.3855 (0.1330 -- 2.1422)  data: 0.2193 (0.0001 -- 1.9732)  max mem: 16413
Val: Total time: 0:00:03 (0.3856 s / it)
* Acc@1 83.544 Acc@5 96.835 loss 0.731
Accuracy of the network on the 158 val images: 83.54%
Max accuracy: 85.44%
Epoch: [179]  [ 0/52]  eta: 0:06:53  lr: 0.000001  min_lr: 0.000000  loss: 2.3185 (2.3185)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5942 (5.5942)  time: 7.9441 (7.9441 -- 7.9441)  data: 6.2815 (6.2815 -- 6.2815)  max mem: 16413
Epoch: [179]  [20/52]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.5777 (1.7040)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4591 (8.6292)  time: 0.8557 (0.5210 -- 4.5703)  data: 0.2545 (0.0003 -- 4.0092)  max mem: 16413
[2023-10-24 01:48:45,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9347
[2023-10-24 01:48:45,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9347
[2023-10-24 01:48:45,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:48:45,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:48:45,955] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [179]  [40/52]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 1.6068 (1.6821)  loss_scale: 32768.0000 (31968.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4604 (8.3078)  time: 0.9219 (0.5172 -- 4.1381)  data: 0.3765 (0.0003 -- 3.6050)  max mem: 16413
Epoch: [179]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7374 (1.7612)  loss_scale: 16384.0000 (28672.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3929 (8.4562)  time: 0.7831 (0.4970 -- 4.1381)  data: 0.2651 (0.0001 -- 3.6050)  max mem: 16413
Epoch: [179] Total time: 0:00:49 (0.9434 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7374 (1.8034)  loss_scale: 16384.0000 (28672.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3929 (8.4562)
[2023-10-24 01:48:55,651] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-10-24 01:48:55,653] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-179/mp_rank_00_model_states.pt
[2023-10-24 01:48:55,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-179/mp_rank_00_model_states.pt...
[2023-10-24 01:48:55,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-10-24 01:48:56,689] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-179/mp_rank_00_model_states.pt.
[2023-10-24 01:48:56,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [0/9]  eta: 0:00:19  loss: 0.5255 (0.5255)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1491 (2.1491 -- 2.1491)  data: 1.9794 (1.9794 -- 1.9794)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6521 (0.8234)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3853 (0.1326 -- 2.1491)  data: 0.2200 (0.0001 -- 1.9794)  max mem: 16413
Val: Total time: 0:00:03 (0.3855 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.730
Accuracy of the network on the 158 val images: 84.81%
Max accuracy: 85.44%
Epoch: [180]  [ 0/52]  eta: 0:05:37  lr: 0.000001  min_lr: 0.000000  loss: 1.9524 (1.9524)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4384 (6.4384)  time: 6.4926 (6.4926 -- 6.4926)  data: 5.7954 (5.7954 -- 5.7954)  max mem: 16413
Epoch: [180]  [20/52]  eta: 0:00:41  lr: 0.000001  min_lr: 0.000000  loss: 1.6884 (1.7413)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2583 (9.1294)  time: 1.0216 (0.5192 -- 4.3804)  data: 0.0383 (0.0003 -- 0.7377)  max mem: 16413
Epoch: [180]  [40/52]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 1.8253 (1.7442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5589 (8.8774)  time: 0.7582 (0.5369 -- 2.1491)  data: 0.0015 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [180]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.9196 (1.7846)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7742 (9.0370)  time: 0.6969 (0.4969 -- 1.7990)  data: 0.0007 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [180] Total time: 0:00:48 (0.9419 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.9196 (1.8054)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7742 (9.0370)
Val:  [0/9]  eta: 0:00:18  loss: 0.5234 (0.5234)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0933 (2.0933 -- 2.0933)  data: 1.9240 (1.9240 -- 1.9240)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6509 (0.8233)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3786 (0.1329 -- 2.0933)  data: 0.2139 (0.0001 -- 1.9240)  max mem: 16413
Val: Total time: 0:00:03 (0.3787 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.81%
Max accuracy: 85.44%
Epoch: [181]  [ 0/52]  eta: 0:05:32  lr: 0.000001  min_lr: 0.000000  loss: 1.4952 (1.4952)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3232 (7.3232)  time: 6.3878 (6.3878 -- 6.3878)  data: 5.8442 (5.8442 -- 5.8442)  max mem: 16413
Epoch: [181]  [20/52]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.9254 (1.8842)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7989 (9.1265)  time: 0.9456 (0.5297 -- 2.7401)  data: 0.3882 (0.0008 -- 2.2292)  max mem: 16413
Epoch: [181]  [40/52]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 1.7463 (1.7983)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4294 (8.9520)  time: 0.9174 (0.5274 -- 3.3661)  data: 0.0888 (0.0004 -- 1.2343)  max mem: 16413
Epoch: [181]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7228 (1.7989)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5334 (9.1789)  time: 0.7935 (0.4964 -- 3.3661)  data: 0.0267 (0.0002 -- 0.5231)  max mem: 16413
Epoch: [181] Total time: 0:00:49 (0.9485 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7228 (1.7820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5334 (9.1789)
Val:  [0/9]  eta: 0:00:18  loss: 0.5255 (0.5255)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0083 (2.0083 -- 2.0083)  data: 1.8270 (1.8270 -- 1.8270)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6493 (0.8226)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3701 (0.1329 -- 2.0083)  data: 0.2031 (0.0001 -- 1.8270)  max mem: 16413
Val: Total time: 0:00:03 (0.3702 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [182]  [ 0/52]  eta: 0:05:48  lr: 0.000001  min_lr: 0.000000  loss: 2.1560 (2.1560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8992 (12.8992)  time: 6.7077 (6.7077 -- 6.7077)  data: 6.1836 (6.1836 -- 6.1836)  max mem: 16413
[2023-10-24 01:51:03,632] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:51:03,632] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:51:03,632] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:51:03,633] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [182]  [20/52]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8879 (1.9069)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4289 (8.7954)  time: 0.8989 (0.5234 -- 2.9832)  data: 0.3550 (0.0009 -- 2.4664)  max mem: 16413
[2023-10-24 01:51:15,658] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9490
[2023-10-24 01:51:15,658] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9490
[2023-10-24 01:51:15,659] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:51:15,659] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:51:15,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [182]  [40/52]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 1.7846 (1.8472)  loss_scale: 16384.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8788 (8.9101)  time: 0.8637 (0.5179 -- 3.7082)  data: 0.3021 (0.0002 -- 3.1930)  max mem: 16413
Epoch: [182]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5063 (1.7943)  loss_scale: 16384.0000 (20795.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0454 (8.9486)  time: 0.8097 (0.4962 -- 3.7082)  data: 0.2926 (0.0002 -- 3.1930)  max mem: 16413
Epoch: [182] Total time: 0:00:49 (0.9475 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5063 (1.7802)  loss_scale: 16384.0000 (20795.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0454 (8.9486)
Val:  [0/9]  eta: 0:00:19  loss: 0.5252 (0.5252)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1652 (2.1652 -- 2.1652)  data: 1.9858 (1.9858 -- 1.9858)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6491 (0.8232)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3871 (0.1333 -- 2.1652)  data: 0.2207 (0.0001 -- 1.9858)  max mem: 16413
Val: Total time: 0:00:03 (0.3872 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [183]  [ 0/52]  eta: 0:06:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6603 (1.6603)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7495 (5.7495)  time: 7.2260 (7.2260 -- 7.2260)  data: 6.2220 (6.2220 -- 6.2220)  max mem: 16413
Epoch: [183]  [20/52]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8128 (1.8916)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2421 (8.5215)  time: 0.8518 (0.5112 -- 3.5996)  data: 0.2869 (0.0003 -- 3.0634)  max mem: 16413
Epoch: [183]  [40/52]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 1.7902 (1.8599)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7277 (8.7313)  time: 0.9424 (0.5086 -- 2.6780)  data: 0.0859 (0.0003 -- 1.6254)  max mem: 16413
Epoch: [183]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8448 (1.8336)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2129 (8.6706)  time: 0.7680 (0.4949 -- 2.6780)  data: 0.0043 (0.0002 -- 0.0744)  max mem: 16413
Epoch: [183] Total time: 0:00:49 (0.9495 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8448 (1.7644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2129 (8.6706)
Val:  [0/9]  eta: 0:00:19  loss: 0.5222 (0.5222)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1151 (2.1151 -- 2.1151)  data: 1.9352 (1.9352 -- 1.9352)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6493 (0.8216)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3815 (0.1334 -- 2.1151)  data: 0.2151 (0.0001 -- 1.9352)  max mem: 16413
Val: Total time: 0:00:03 (0.3817 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.728
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [184]  [ 0/52]  eta: 0:06:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6224 (1.6224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6747 (5.6747)  time: 7.9868 (7.9868 -- 7.9868)  data: 5.8835 (5.8835 -- 5.8835)  max mem: 16413
Epoch: [184]  [20/52]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.7025 (1.7737)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3864 (8.9480)  time: 0.8646 (0.5303 -- 2.7447)  data: 0.1549 (0.0004 -- 1.5981)  max mem: 16413
Epoch: [184]  [40/52]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 1.5598 (1.7363)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7127 (8.9412)  time: 0.8715 (0.5126 -- 4.6457)  data: 0.3190 (0.0003 -- 4.1139)  max mem: 16413
[2023-10-24 01:53:20,497] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:53:20,497] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 01:53:20,497] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:53:20,497] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [184]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7753 (1.7233)  loss_scale: 16384.0000 (16699.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7127 (9.0927)  time: 0.6940 (0.4941 -- 2.6828)  data: 0.1753 (0.0001 -- 2.1178)  max mem: 16413
Epoch: [184] Total time: 0:00:49 (0.9537 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7753 (1.7487)  loss_scale: 16384.0000 (16699.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7127 (9.0927)
Val:  [0/9]  eta: 0:00:18  loss: 0.5219 (0.5219)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0715 (2.0715 -- 2.0715)  data: 1.8873 (1.8873 -- 1.8873)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6488 (0.8215)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3765 (0.1329 -- 2.0715)  data: 0.2098 (0.0001 -- 1.8873)  max mem: 16413
Val: Total time: 0:00:03 (0.3766 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.728
Accuracy of the network on the 158 val images: 84.81%
Max accuracy: 85.44%
Epoch: [185]  [ 0/52]  eta: 0:04:37  lr: 0.000001  min_lr: 0.000000  loss: 1.9313 (1.9313)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5512 (4.5512)  time: 5.3435 (5.3435 -- 5.3435)  data: 4.8006 (4.8006 -- 4.8006)  max mem: 16413
Epoch: [185]  [20/52]  eta: 0:00:34  lr: 0.000001  min_lr: 0.000000  loss: 1.8073 (1.7777)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7601 (8.4297)  time: 0.8809 (0.5229 -- 2.4667)  data: 0.1140 (0.0006 -- 1.3967)  max mem: 16413
Epoch: [185]  [40/52]  eta: 0:00:11  lr: 0.000001  min_lr: 0.000000  loss: 1.7909 (1.7874)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6447 (8.5335)  time: 0.8725 (0.5201 -- 2.7749)  data: 0.0356 (0.0002 -- 0.4530)  max mem: 16413
Epoch: [185]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 2.0086 (1.8351)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1526 (8.7362)  time: 0.7097 (0.4954 -- 1.2997)  data: 0.0739 (0.0002 -- 0.7360)  max mem: 16413
Epoch: [185] Total time: 0:00:47 (0.9117 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 2.0086 (1.8482)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1526 (8.7362)
Val:  [0/9]  eta: 0:00:18  loss: 0.5209 (0.5209)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1019 (2.1019 -- 2.1019)  data: 1.9238 (1.9238 -- 1.9238)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6487 (0.8212)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3800 (0.1329 -- 2.1019)  data: 0.2138 (0.0001 -- 1.9238)  max mem: 16413
Val: Total time: 0:00:03 (0.3801 s / it)
* Acc@1 84.810 Acc@5 96.835 loss 0.728
Accuracy of the network on the 158 val images: 84.81%
Max accuracy: 85.44%
Epoch: [186]  [ 0/52]  eta: 0:07:03  lr: 0.000001  min_lr: 0.000000  loss: 1.5694 (1.5694)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4473 (9.4473)  time: 8.1411 (8.1411 -- 8.1411)  data: 6.1763 (6.1763 -- 6.1763)  max mem: 16413
Epoch: [186]  [20/52]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7157 (1.7705)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9981 (8.8406)  time: 0.7927 (0.5375 -- 3.0076)  data: 0.0020 (0.0003 -- 0.0054)  max mem: 16413
[2023-10-24 01:54:56,646] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9712
[2023-10-24 01:54:56,646] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9712
[2023-10-24 01:54:56,646] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:54:56,646] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 01:54:56,646] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [186]  [40/52]  eta: 0:00:12  lr: 0.000001  min_lr: 0.000000  loss: 1.8934 (1.7961)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5123 (8.6926)  time: 0.8939 (0.5403 -- 2.6278)  data: 0.1189 (0.0005 -- 1.8062)  max mem: 16413
Epoch: [186]  [51/52]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.9120 (1.8055)  loss_scale: 16384.0000 (28987.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2439 (8.6856)  time: 0.8774 (0.4944 -- 2.6278)  data: 0.1971 (0.0001 -- 1.8062)  max mem: 16413
Epoch: [186] Total time: 0:00:49 (0.9531 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.9120 (1.8246)  loss_scale: 16384.0000 (28987.0769)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2439 (8.6856)
Val:  [0/9]  eta: 0:00:19  loss: 0.5199 (0.5199)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1332 (2.1332 -- 2.1332)  data: 1.9556 (1.9556 -- 1.9556)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6490 (0.8215)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3833 (0.1327 -- 2.1332)  data: 0.2174 (0.0001 -- 1.9556)  max mem: 16413
Val: Total time: 0:00:03 (0.3834 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.728
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [187]  [ 0/52]  eta: 0:07:53  lr: 0.000001  min_lr: 0.000000  loss: 2.0449 (2.0449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0932 (6.0932)  time: 9.1084 (9.1084 -- 9.1084)  data: 8.5853 (8.5853 -- 8.5853)  max mem: 16413
Epoch: [187]  [20/52]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.6548 (1.7929)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2508 (8.6729)  time: 0.8087 (0.5096 -- 3.3067)  data: 0.2098 (0.0003 -- 2.7450)  max mem: 16413
Epoch: [187]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.5794 (1.7514)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1666 (8.4296)  time: 0.8079 (0.5189 -- 1.8792)  data: 0.1539 (0.0005 -- 1.3289)  max mem: 16413
Epoch: [187]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8700 (1.7619)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1697 (8.5440)  time: 0.7886 (0.4945 -- 2.4198)  data: 0.0770 (0.0001 -- 1.3289)  max mem: 16413
Epoch: [187] Total time: 0:00:48 (0.9404 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8700 (1.7869)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1697 (8.5440)
Val:  [0/9]  eta: 0:00:18  loss: 0.5200 (0.5200)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0868 (2.0868 -- 2.0868)  data: 1.9117 (1.9117 -- 1.9117)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6508 (0.8224)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3788 (0.1329 -- 2.0868)  data: 0.2125 (0.0001 -- 1.9117)  max mem: 16413
Val: Total time: 0:00:03 (0.3789 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [188]  [ 0/52]  eta: 0:05:03  lr: 0.000000  min_lr: 0.000000  loss: 1.5116 (1.5116)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3220 (5.3220)  time: 5.8270 (5.8270 -- 5.8270)  data: 5.2762 (5.2762 -- 5.2762)  max mem: 16413
Epoch: [188]  [20/52]  eta: 0:00:40  lr: 0.000000  min_lr: 0.000000  loss: 1.6715 (1.7427)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1720 (8.5856)  time: 1.0327 (0.5175 -- 4.4192)  data: 0.2063 (0.0006 -- 1.5319)  max mem: 16413
[2023-10-24 01:56:35,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9808
[2023-10-24 01:56:35,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9808
[2023-10-24 01:56:35,303] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:56:35,303] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 01:56:35,303] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [188]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.8170 (1.7885)  loss_scale: 16384.0000 (14585.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4749 (8.9969)  time: 0.8417 (0.4980 -- 3.5840)  data: 0.1451 (0.0003 -- 1.3357)  max mem: 16413
Epoch: [188]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7884 (1.7670)  loss_scale: 8192.0000 (13233.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4749 (9.1396)  time: 0.7900 (0.4945 -- 3.5840)  data: 0.0771 (0.0002 -- 0.9516)  max mem: 16413
Epoch: [188] Total time: 0:00:50 (0.9712 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7884 (1.7937)  loss_scale: 8192.0000 (13233.2308)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4749 (9.1396)
Val:  [0/9]  eta: 0:00:19  loss: 0.5201 (0.5201)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1354 (2.1354 -- 2.1354)  data: 1.9558 (1.9558 -- 1.9558)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6504 (0.8224)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3838 (0.1340 -- 2.1354)  data: 0.2174 (0.0001 -- 1.9558)  max mem: 16413
Val: Total time: 0:00:03 (0.3839 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [189]  [ 0/52]  eta: 0:06:52  lr: 0.000000  min_lr: 0.000000  loss: 1.0937 (1.0937)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3756 (7.3756)  time: 7.9233 (7.9233 -- 7.9233)  data: 4.6721 (4.6721 -- 4.6721)  max mem: 16413
Epoch: [189]  [20/52]  eta: 0:00:35  lr: 0.000000  min_lr: 0.000000  loss: 1.6762 (1.7004)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3008 (11.0522)  time: 0.7821 (0.5175 -- 2.8426)  data: 0.1954 (0.0005 -- 2.2858)  max mem: 16413
Epoch: [189]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.8965 (1.7468)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7187 (10.0431)  time: 0.9042 (0.5117 -- 3.0867)  data: 0.2242 (0.0002 -- 1.6880)  max mem: 16413
Epoch: [189]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8032 (1.7583)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8214 (9.6653)  time: 0.7827 (0.4941 -- 3.0867)  data: 0.1333 (0.0002 -- 1.5621)  max mem: 16413
Epoch: [189] Total time: 0:00:49 (0.9425 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8032 (1.7770)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8214 (9.6653)
Val:  [0/9]  eta: 0:00:18  loss: 0.5212 (0.5212)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0716 (2.0716 -- 2.0716)  data: 1.8814 (1.8814 -- 1.8814)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6492 (0.8220)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3781 (0.1333 -- 2.0716)  data: 0.2091 (0.0001 -- 1.8814)  max mem: 16413
Val: Total time: 0:00:03 (0.3782 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [190]  [ 0/52]  eta: 0:06:12  lr: 0.000000  min_lr: 0.000000  loss: 2.2307 (2.2307)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2558 (10.2558)  time: 7.1563 (7.1563 -- 7.1563)  data: 6.6288 (6.6288 -- 6.6288)  max mem: 16413
Epoch: [190]  [20/52]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7032 (1.7707)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9461 (8.7566)  time: 0.8507 (0.5256 -- 3.5914)  data: 0.3028 (0.0004 -- 3.0162)  max mem: 16413
Epoch: [190]  [40/52]  eta: 0:00:11  lr: 0.000000  min_lr: 0.000000  loss: 1.7914 (1.7755)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1270 (8.5314)  time: 0.7794 (0.5250 -- 2.6204)  data: 0.2158 (0.0004 -- 2.1088)  max mem: 16413
Epoch: [190]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.9173 (1.8036)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3570 (8.6449)  time: 0.7853 (0.4984 -- 2.6204)  data: 0.2621 (0.0002 -- 2.1088)  max mem: 16413
Epoch: [190] Total time: 0:00:48 (0.9240 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.9173 (1.8426)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3570 (8.6449)
Val:  [0/9]  eta: 0:00:19  loss: 0.5218 (0.5218)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1589 (2.1589 -- 2.1589)  data: 1.9887 (1.9887 -- 1.9887)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6492 (0.8221)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3858 (0.1329 -- 2.1589)  data: 0.2210 (0.0001 -- 1.9887)  max mem: 16413
Val: Total time: 0:00:03 (0.3859 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [191]  [ 0/52]  eta: 0:07:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7439 (1.7439)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2553 (10.2553)  time: 8.4388 (8.4388 -- 8.4388)  data: 6.5602 (6.5602 -- 6.5602)  max mem: 16413
[2023-10-24 01:58:49,220] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:58:49,220] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 01:58:49,220] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 01:58:49,220] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [191]  [20/52]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.9083 (1.8211)  loss_scale: 16384.0000 (14433.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8955 (10.1638)  time: 0.8268 (0.5216 -- 3.9333)  data: 0.1422 (0.0004 -- 1.9972)  max mem: 16413
Epoch: [191]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.8488 (1.8268)  loss_scale: 16384.0000 (15384.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0540 (9.4245)  time: 0.8847 (0.5269 -- 2.9888)  data: 0.3261 (0.0005 -- 2.4782)  max mem: 16413
Epoch: [191]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8976 (1.8461)  loss_scale: 16384.0000 (15596.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3104 (9.2822)  time: 0.6936 (0.4938 -- 2.9888)  data: 0.1791 (0.0001 -- 2.4782)  max mem: 16413
Epoch: [191] Total time: 0:00:48 (0.9304 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8976 (1.7626)  loss_scale: 16384.0000 (15596.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3104 (9.2822)
Val:  [0/9]  eta: 0:00:19  loss: 0.5211 (0.5211)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1467 (2.1467 -- 2.1467)  data: 1.9670 (1.9670 -- 1.9670)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6496 (0.8219)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3843 (0.1328 -- 2.1467)  data: 0.2186 (0.0001 -- 1.9670)  max mem: 16413
Val: Total time: 0:00:03 (0.3844 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [192]  [ 0/52]  eta: 0:06:22  lr: 0.000000  min_lr: 0.000000  loss: 1.7396 (1.7396)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6168 (8.6168)  time: 7.3572 (7.3572 -- 7.3572)  data: 6.8395 (6.8395 -- 6.8395)  max mem: 16413
[2023-10-24 01:59:48,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=53, lr=[5.401052235487838e-09, 5.401052235487838e-09, 7.20140298065045e-09, 7.20140298065045e-09, 9.601870640867267e-09, 9.601870640867267e-09, 1.2802494187823023e-08, 1.2802494187823023e-08, 1.7069992250430698e-08, 1.7069992250430698e-08, 2.275998966724093e-08, 2.275998966724093e-08, 3.0346652889654575e-08, 3.0346652889654575e-08, 4.046220385287277e-08, 4.046220385287277e-08, 5.3949605137163685e-08, 5.3949605137163685e-08, 7.193280684955158e-08, 7.193280684955158e-08, 9.591040913273544e-08, 9.591040913273544e-08, 1.2788054551031392e-07, 1.2788054551031392e-07, 1.705073940137519e-07, 1.705073940137519e-07, 2.2734319201833586e-07, 2.2734319201833586e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-10-24 01:59:48,975] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=18.48996905365304, CurrSamplesPerSec=21.511687434688117, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [192]  [20/52]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8250 (1.7634)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1475 (8.3270)  time: 0.8138 (0.5277 -- 2.2853)  data: 0.2572 (0.0005 -- 1.7661)  max mem: 16413
Epoch: [192]  [40/52]  eta: 0:00:11  lr: 0.000000  min_lr: 0.000000  loss: 1.9777 (1.8615)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4857 (8.5001)  time: 0.8622 (0.5256 -- 2.2123)  data: 0.1203 (0.0003 -- 1.6669)  max mem: 16413
Epoch: [192]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7820 (1.8410)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1497 (8.6078)  time: 0.7805 (0.4970 -- 2.2277)  data: 0.0062 (0.0003 -- 0.1098)  max mem: 16413
Epoch: [192] Total time: 0:00:48 (0.9344 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7820 (1.8243)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1497 (8.6078)
Val:  [0/9]  eta: 0:00:18  loss: 0.5218 (0.5218)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0881 (2.0881 -- 2.0881)  data: 1.9182 (1.9182 -- 1.9182)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6501 (0.8222)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3782 (0.1330 -- 2.0881)  data: 0.2132 (0.0001 -- 1.9182)  max mem: 16413
Val: Total time: 0:00:03 (0.3783 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [193]  [ 0/52]  eta: 0:08:01  lr: 0.000000  min_lr: 0.000000  loss: 2.2531 (2.2531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0966 (8.0966)  time: 9.2584 (9.2584 -- 9.2584)  data: 8.7107 (8.7107 -- 8.7107)  max mem: 16413
[2023-10-24 02:00:35,059] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10043
[2023-10-24 02:00:35,059] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10043
[2023-10-24 02:00:35,059] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 02:00:35,059] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 02:00:35,060] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [193]  [20/52]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.9272 (1.9250)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6898 (8.8008)  time: 0.8374 (0.5145 -- 3.8426)  data: 0.2999 (0.0003 -- 3.3255)  max mem: 16413
Epoch: [193]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.7878 (1.8693)  loss_scale: 8192.0000 (9590.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8898 (8.8393)  time: 0.8846 (0.5081 -- 4.3437)  data: 0.3530 (0.0002 -- 3.8232)  max mem: 16413
Epoch: [193]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7740 (1.8526)  loss_scale: 8192.0000 (9294.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3145 (8.6764)  time: 0.7015 (0.4947 -- 4.3437)  data: 0.1916 (0.0001 -- 3.8232)  max mem: 16413
Epoch: [193] Total time: 0:00:49 (0.9468 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7740 (1.8401)  loss_scale: 8192.0000 (9294.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3145 (8.6764)
Val:  [0/9]  eta: 0:00:19  loss: 0.5216 (0.5216)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1212 (2.1212 -- 2.1212)  data: 1.9442 (1.9442 -- 1.9442)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6507 (0.8223)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3817 (0.1326 -- 2.1212)  data: 0.2161 (0.0001 -- 1.9442)  max mem: 16413
Val: Total time: 0:00:03 (0.3818 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [194]  [ 0/52]  eta: 0:07:55  lr: 0.000000  min_lr: 0.000000  loss: 1.3811 (1.3811)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9521 (7.9521)  time: 9.1522 (9.1522 -- 9.1522)  data: 8.5883 (8.5883 -- 8.5883)  max mem: 16413
Epoch: [194]  [20/52]  eta: 0:00:41  lr: 0.000000  min_lr: 0.000000  loss: 1.7855 (1.7188)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6806 (8.7215)  time: 0.9124 (0.5284 -- 4.8758)  data: 0.3634 (0.0005 -- 4.3357)  max mem: 16413
Epoch: [194]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.8219 (1.7482)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8051 (8.5154)  time: 0.8070 (0.5285 -- 3.3193)  data: 0.2638 (0.0004 -- 2.7972)  max mem: 16413
Epoch: [194]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8375 (1.7888)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9627 (8.6554)  time: 0.7553 (0.4940 -- 3.3193)  data: 0.2371 (0.0001 -- 2.7972)  max mem: 16413
Epoch: [194] Total time: 0:00:50 (0.9657 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8375 (1.7873)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9627 (8.6554)
Val:  [0/9]  eta: 0:00:18  loss: 0.5215 (0.5215)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0562 (2.0562 -- 2.0562)  data: 1.8811 (1.8811 -- 1.8811)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6504 (0.8224)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3755 (0.1326 -- 2.0562)  data: 0.2091 (0.0001 -- 1.8811)  max mem: 16413
Val: Total time: 0:00:03 (0.3756 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [195]  [ 0/52]  eta: 0:07:32  lr: 0.000000  min_lr: 0.000000  loss: 1.5815 (1.5815)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6447 (8.6447)  time: 8.6949 (8.6949 -- 8.6949)  data: 4.8140 (4.8140 -- 4.8140)  max mem: 16413
Epoch: [195]  [20/52]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5605 (1.6482)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5007 (9.0586)  time: 0.7747 (0.5192 -- 4.8078)  data: 0.0013 (0.0003 -- 0.0025)  max mem: 16413
[2023-10-24 02:02:44,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 02:02:44,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-10-24 02:02:44,755] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 02:02:44,756] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [195]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.8767 (1.7441)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7063 (8.9236)  time: 0.9166 (0.5134 -- 4.7528)  data: 0.0449 (0.0004 -- 0.8657)  max mem: 16413
Epoch: [195]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.9727 (1.7638)  loss_scale: 16384.0000 (11342.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3203 (9.0413)  time: 0.7472 (0.4961 -- 2.0730)  data: 0.0479 (0.0002 -- 0.8657)  max mem: 16413
Epoch: [195] Total time: 0:00:49 (0.9493 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.9727 (1.7825)  loss_scale: 16384.0000 (11342.7692)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3203 (9.0413)
Val:  [0/9]  eta: 0:00:19  loss: 0.5214 (0.5214)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1573 (2.1573 -- 2.1573)  data: 1.9787 (1.9787 -- 1.9787)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6503 (0.8226)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3866 (0.1331 -- 2.1573)  data: 0.2199 (0.0001 -- 1.9787)  max mem: 16413
Val: Total time: 0:00:03 (0.3867 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [196]  [ 0/52]  eta: 0:06:31  lr: 0.000000  min_lr: 0.000000  loss: 2.2396 (2.2396)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3803 (9.3803)  time: 7.5264 (7.5264 -- 7.5264)  data: 7.0021 (7.0021 -- 7.0021)  max mem: 16413
Epoch: [196]  [20/52]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.9322 (1.9340)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2067 (9.5530)  time: 0.8978 (0.5243 -- 3.0433)  data: 0.1809 (0.0002 -- 2.4959)  max mem: 16413
Epoch: [196]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.7957 (1.8349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4966 (9.3309)  time: 0.8182 (0.5154 -- 2.9624)  data: 0.2252 (0.0001 -- 2.4258)  max mem: 16413
Epoch: [196]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8294 (1.8079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1678 (9.1403)  time: 0.7576 (0.4958 -- 2.9624)  data: 0.1336 (0.0001 -- 2.4258)  max mem: 16413
Epoch: [196] Total time: 0:00:48 (0.9335 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8294 (1.8144)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1678 (9.1403)
Val:  [0/9]  eta: 0:00:18  loss: 0.5212 (0.5212)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0871 (2.0871 -- 2.0871)  data: 1.8911 (1.8911 -- 1.8911)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6502 (0.8224)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3780 (0.1327 -- 2.0871)  data: 0.2102 (0.0001 -- 1.8911)  max mem: 16413
Val: Total time: 0:00:03 (0.3781 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [197]  [ 0/52]  eta: 0:06:57  lr: 0.000000  min_lr: 0.000000  loss: 2.3939 (2.3939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3180 (8.3180)  time: 8.0326 (8.0326 -- 8.0326)  data: 4.7748 (4.7748 -- 4.7748)  max mem: 16413
Epoch: [197]  [20/52]  eta: 0:00:35  lr: 0.000000  min_lr: 0.000000  loss: 1.9084 (1.9281)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9142 (8.3053)  time: 0.7719 (0.5335 -- 3.2905)  data: 0.0270 (0.0004 -- 0.5065)  max mem: 16413
Epoch: [197]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.8221 (1.8394)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5106 (8.5538)  time: 0.8953 (0.5247 -- 4.1186)  data: 0.0469 (0.0003 -- 0.8558)  max mem: 16413
Epoch: [197]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8698 (1.8348)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8591 (8.8171)  time: 0.8011 (0.4969 -- 4.1186)  data: 0.0300 (0.0002 -- 0.5840)  max mem: 16413
Epoch: [197] Total time: 0:00:47 (0.9149 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8698 (1.8539)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8591 (8.8171)
Val:  [0/9]  eta: 0:00:19  loss: 0.5221 (0.5221)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1667 (2.1667 -- 2.1667)  data: 1.9872 (1.9872 -- 1.9872)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6504 (0.8224)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3884 (0.1330 -- 2.1667)  data: 0.2209 (0.0001 -- 1.9872)  max mem: 16413
Val: Total time: 0:00:03 (0.3885 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [198]  [ 0/52]  eta: 0:06:54  lr: 0.000000  min_lr: 0.000000  loss: 1.4097 (1.4097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2315 (8.2315)  time: 7.9719 (7.9719 -- 7.9719)  data: 6.1914 (6.1914 -- 6.1914)  max mem: 16413
[2023-10-24 02:04:54,337] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 02:04:54,338] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-10-24 02:04:54,338] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-10-24 02:04:54,338] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [198]  [20/52]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.8634 (1.8468)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7839 (7.9947)  time: 0.9018 (0.5125 -- 4.6500)  data: 0.3514 (0.0003 -- 4.1188)  max mem: 16413
[2023-10-24 02:05:11,210] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10318
[2023-10-24 02:05:11,211] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 02:05:11,211] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10318
[2023-10-24 02:05:11,211] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-10-24 02:05:11,211] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [198]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.7912 (1.8626)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9924 (8.1976)  time: 0.8572 (0.5142 -- 2.9207)  data: 0.1385 (0.0004 -- 1.3880)  max mem: 16413
[2023-10-24 02:05:32,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10346
[2023-10-24 02:05:32,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10346
[2023-10-24 02:05:32,353] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 02:05:32,353] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-10-24 02:05:32,353] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [198]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.9838 (1.8927)  loss_scale: 16384.0000 (21740.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7544 (8.2833)  time: 0.7442 (0.4825 -- 2.9207)  data: 0.0914 (0.0001 -- 1.3880)  max mem: 16413
Epoch: [198] Total time: 0:00:48 (0.9367 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.9838 (1.8154)  loss_scale: 16384.0000 (21740.3077)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7544 (8.2833)
Val:  [0/9]  eta: 0:00:19  loss: 0.5212 (0.5212)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1163 (2.1163 -- 2.1163)  data: 1.9386 (1.9386 -- 1.9386)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6501 (0.8222)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3822 (0.1328 -- 2.1163)  data: 0.2155 (0.0001 -- 1.9386)  max mem: 16413
Val: Total time: 0:00:03 (0.3823 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Epoch: [199]  [ 0/52]  eta: 0:08:31  lr: 0.000000  min_lr: 0.000000  loss: 1.2565 (1.2565)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5058 (11.5058)  time: 9.8285 (9.8285 -- 9.8285)  data: 8.3596 (8.3596 -- 8.3596)  max mem: 16413
Epoch: [199]  [20/52]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7711 (1.7943)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8900 (8.2416)  time: 0.7159 (0.5267 -- 3.0336)  data: 0.1307 (0.0004 -- 2.0682)  max mem: 16413
Epoch: [199]  [40/52]  eta: 0:00:12  lr: 0.000000  min_lr: 0.000000  loss: 1.7367 (1.7472)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6360 (8.3687)  time: 0.9536 (0.5244 -- 4.1491)  data: 0.4121 (0.0003 -- 3.6119)  max mem: 16413
Epoch: [199]  [51/52]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8617 (1.7652)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8425 (8.7902)  time: 0.7564 (0.4957 -- 3.5452)  data: 0.2380 (0.0001 -- 3.0168)  max mem: 16413
Epoch: [199] Total time: 0:00:48 (0.9401 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8617 (1.7619)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8425 (8.7902)
[2023-10-24 02:06:25,190] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-10-24 02:06:25,192] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-199/mp_rank_00_model_states.pt
[2023-10-24 02:06:25,192] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-199/mp_rank_00_model_states.pt...
[2023-10-24 02:06:25,192] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-10-24 02:06:26,273] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output_right/checkpoint-199/mp_rank_00_model_states.pt.
[2023-10-24 02:06:26,273] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [0/9]  eta: 0:00:19  loss: 0.5215 (0.5215)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1916 (2.1916 -- 2.1916)  data: 2.0208 (2.0208 -- 2.0208)  max mem: 16413
Val:  [8/9]  eta: 0:00:00  loss: 0.6503 (0.8225)  acc1: 88.8889 (83.5443)  acc5: 100.0000 (96.2025)  time: 0.3912 (0.1338 -- 2.1916)  data: 0.2246 (0.0001 -- 2.0208)  max mem: 16413
Val: Total time: 0:00:03 (0.3914 s / it)
* Acc@1 84.177 Acc@5 96.835 loss 0.729
Accuracy of the network on the 158 val images: 84.18%
Max accuracy: 85.44%
Test:  [  0/395]  eta: 0:27:23  loss: 0.2919 (0.2919)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 4.1600 (4.1600 -- 4.1600)  data: 3.9147 (3.9147 -- 3.9147)  max mem: 16413
Test:  [ 10/395]  eta: 0:10:51  loss: 0.4569 (0.6679)  acc1: 83.3333 (81.8182)  acc5: 100.0000 (95.4545)  time: 1.6918 (0.1345 -- 6.6738)  data: 1.4470 (0.0006 -- 6.3909)  max mem: 16413
Test:  [ 20/395]  eta: 0:10:58  loss: 0.6002 (0.7110)  acc1: 83.3333 (80.1587)  acc5: 100.0000 (95.2381)  time: 1.6353 (0.1291 -- 12.3921)  data: 1.4348 (0.0005 -- 12.2618)  max mem: 16413
Test:  [ 30/395]  eta: 0:08:58  loss: 0.4919 (0.6903)  acc1: 83.3333 (81.1828)  acc5: 100.0000 (95.6989)  time: 1.3556 (0.1291 -- 12.3921)  data: 1.0797 (0.0004 -- 12.2618)  max mem: 16413
Test:  [ 40/395]  eta: 0:08:12  loss: 0.4074 (0.6368)  acc1: 83.3333 (82.9268)  acc5: 100.0000 (96.3415)  time: 1.0036 (0.1330 -- 7.1803)  data: 0.5944 (0.0004 -- 6.6831)  max mem: 16413
Test:  [ 50/395]  eta: 0:09:27  loss: 0.4206 (0.6465)  acc1: 83.3333 (82.6797)  acc5: 100.0000 (96.4052)  time: 1.9111 (0.1293 -- 14.6701)  data: 1.6229 (0.0003 -- 14.5497)  max mem: 16413
Test:  [ 60/395]  eta: 0:09:28  loss: 0.5281 (0.6500)  acc1: 83.3333 (82.7869)  acc5: 100.0000 (96.4481)  time: 2.3270 (0.1242 -- 14.6701)  data: 2.1446 (0.0003 -- 14.5497)  max mem: 16413
Test:  [ 70/395]  eta: 0:09:07  loss: 0.5628 (0.6505)  acc1: 83.3333 (82.6291)  acc5: 100.0000 (96.7136)  time: 1.7859 (0.1242 -- 10.8299)  data: 1.5402 (0.0001 -- 10.7013)  max mem: 16413
Test:  [ 80/395]  eta: 0:08:31  loss: 0.4811 (0.6352)  acc1: 83.3333 (83.1276)  acc5: 100.0000 (96.9136)  time: 1.4011 (0.1242 -- 10.2103)  data: 1.1738 (0.0001 -- 10.0845)  max mem: 16413
Test:  [ 90/395]  eta: 0:08:16  loss: 0.5710 (0.6397)  acc1: 83.3333 (82.9670)  acc5: 100.0000 (96.8864)  time: 1.4176 (0.1242 -- 12.0072)  data: 1.2243 (0.0002 -- 11.8499)  max mem: 16413
Test:  [100/395]  eta: 0:08:08  loss: 0.5710 (0.6355)  acc1: 83.3333 (83.1683)  acc5: 100.0000 (97.0297)  time: 1.7865 (0.1294 -- 12.9853)  data: 1.6128 (0.0002 -- 12.8471)  max mem: 16413
Test:  [110/395]  eta: 0:07:25  loss: 0.5158 (0.6288)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (97.1471)  time: 1.2664 (0.1205 -- 12.9853)  data: 1.0485 (0.0005 -- 12.8471)  max mem: 16413
Test:  [120/395]  eta: 0:07:35  loss: 0.6557 (0.6361)  acc1: 83.3333 (83.0579)  acc5: 100.0000 (96.9697)  time: 1.6570 (0.1205 -- 17.1282)  data: 1.3684 (0.0002 -- 16.9808)  max mem: 16413
Test:  [130/395]  eta: 0:07:05  loss: 0.7190 (0.6464)  acc1: 83.3333 (82.9517)  acc5: 100.0000 (96.6921)  time: 1.8403 (0.1226 -- 17.1282)  data: 1.6145 (0.0002 -- 16.9808)  max mem: 16413
Test:  [140/395]  eta: 0:07:09  loss: 0.5837 (0.6547)  acc1: 83.3333 (82.7423)  acc5: 100.0000 (96.5721)  time: 1.8500 (0.1269 -- 15.1028)  data: 1.4917 (0.0003 -- 14.9185)  max mem: 16413
Test:  [150/395]  eta: 0:06:55  loss: 0.5780 (0.6554)  acc1: 83.3333 (82.7815)  acc5: 100.0000 (96.4680)  time: 2.3006 (0.1350 -- 15.1028)  data: 1.8805 (0.0005 -- 14.9185)  max mem: 16413
Test:  [160/395]  eta: 0:06:38  loss: 0.5780 (0.6622)  acc1: 83.3333 (82.6087)  acc5: 100.0000 (96.2733)  time: 1.7809 (0.1350 -- 10.6289)  data: 1.5260 (0.0008 -- 10.4016)  max mem: 16413
Test:  [170/395]  eta: 0:06:25  loss: 0.8044 (0.6719)  acc1: 83.3333 (82.4561)  acc5: 100.0000 (96.1014)  time: 1.8258 (0.1273 -- 15.6046)  data: 1.6333 (0.0005 -- 15.4629)  max mem: 16413
Test:  [180/395]  eta: 0:06:10  loss: 0.6349 (0.6764)  acc1: 83.3333 (82.4125)  acc5: 100.0000 (96.1326)  time: 1.9595 (0.1273 -- 15.6046)  data: 1.7611 (0.0002 -- 15.4629)  max mem: 16413
Test:  [190/395]  eta: 0:05:47  loss: 0.6349 (0.6798)  acc1: 83.3333 (82.3735)  acc5: 100.0000 (96.0733)  time: 1.5722 (0.1290 -- 10.5674)  data: 1.3600 (0.0002 -- 10.4102)  max mem: 16413
Test:  [200/395]  eta: 0:05:26  loss: 0.6831 (0.6822)  acc1: 83.3333 (82.2554)  acc5: 100.0000 (96.0199)  time: 1.2266 (0.1373 -- 6.5222)  data: 0.9889 (0.0005 -- 5.9440)  max mem: 16413
Test:  [210/395]  eta: 0:05:11  loss: 0.6984 (0.6920)  acc1: 83.3333 (82.0695)  acc5: 100.0000 (95.8926)  time: 1.5466 (0.1331 -- 10.2064)  data: 1.3013 (0.0005 -- 10.0728)  max mem: 16413
Test:  [220/395]  eta: 0:04:59  loss: 0.6757 (0.6993)  acc1: 66.6667 (81.8250)  acc5: 100.0000 (95.7768)  time: 2.0558 (0.1276 -- 14.4930)  data: 1.8755 (0.0003 -- 14.3566)  max mem: 16413
Test:  [230/395]  eta: 0:04:44  loss: 0.6423 (0.7048)  acc1: 83.3333 (81.6017)  acc5: 100.0000 (95.7431)  time: 2.1774 (0.1276 -- 14.4930)  data: 1.9905 (0.0003 -- 14.3566)  max mem: 16413
Test:  [240/395]  eta: 0:04:25  loss: 0.6976 (0.7067)  acc1: 83.3333 (81.6044)  acc5: 100.0000 (95.8506)  time: 1.7266 (0.1319 -- 12.0107)  data: 1.4650 (0.0004 -- 11.8690)  max mem: 16413
Test:  [250/395]  eta: 0:04:02  loss: 0.8343 (0.7177)  acc1: 83.3333 (81.2749)  acc5: 100.0000 (95.6175)  time: 1.0923 (0.1294 -- 10.9757)  data: 0.8692 (0.0004 -- 10.8166)  max mem: 16413
Test:  [260/395]  eta: 0:03:44  loss: 0.8825 (0.7269)  acc1: 83.3333 (81.0345)  acc5: 100.0000 (95.5300)  time: 1.0943 (0.1294 -- 9.1528)  data: 0.8805 (0.0004 -- 8.9969)  max mem: 16413
Test:  [270/395]  eta: 0:03:29  loss: 0.8825 (0.7363)  acc1: 83.3333 (80.6888)  acc5: 100.0000 (95.3875)  time: 1.7134 (0.1292 -- 13.1512)  data: 1.5046 (0.0004 -- 13.0045)  max mem: 16413
Test:  [280/395]  eta: 0:03:14  loss: 0.8121 (0.7387)  acc1: 66.6667 (80.4864)  acc5: 100.0000 (95.3737)  time: 2.0982 (0.1292 -- 13.1512)  data: 1.9279 (0.0004 -- 13.0045)  max mem: 16413
Test:  [290/395]  eta: 0:02:56  loss: 0.8121 (0.7475)  acc1: 66.6667 (80.2978)  acc5: 100.0000 (95.0172)  time: 1.7809 (0.1368 -- 10.3722)  data: 1.4547 (0.0003 -- 10.0921)  max mem: 16413
Test:  [300/395]  eta: 0:02:41  loss: 0.9205 (0.7543)  acc1: 66.6667 (80.1772)  acc5: 83.3333 (94.7951)  time: 1.8145 (0.1340 -- 20.2642)  data: 1.4996 (0.0003 -- 20.0935)  max mem: 16413
Test:  [310/395]  eta: 0:02:24  loss: 0.9205 (0.7654)  acc1: 66.6667 (79.7428)  acc5: 100.0000 (94.5338)  time: 1.9807 (0.1340 -- 20.2642)  data: 1.7679 (0.0006 -- 20.0935)  max mem: 16413
Test:  [320/395]  eta: 0:02:05  loss: 1.0466 (0.7709)  acc1: 66.6667 (79.4912)  acc5: 83.3333 (94.3925)  time: 1.2453 (0.1308 -- 9.7607)  data: 1.0118 (0.0003 -- 9.3530)  max mem: 16413
Test:  [330/395]  eta: 0:01:50  loss: 0.9215 (0.7768)  acc1: 66.6667 (79.4058)  acc5: 100.0000 (94.2598)  time: 1.6469 (0.1308 -- 8.1088)  data: 1.4549 (0.0003 -- 7.9776)  max mem: 16413
Test:  [340/395]  eta: 0:01:34  loss: 0.8879 (0.7871)  acc1: 83.3333 (79.1300)  acc5: 100.0000 (94.0371)  time: 2.5000 (0.1191 -- 22.0495)  data: 2.2854 (0.0003 -- 21.8964)  max mem: 16413
Test:  [350/395]  eta: 0:01:17  loss: 0.9726 (0.7965)  acc1: 66.6667 (78.8224)  acc5: 83.3333 (93.8272)  time: 1.9854 (0.1168 -- 22.0495)  data: 1.7755 (0.0001 -- 21.8964)  max mem: 16413
Test:  [360/395]  eta: 0:01:00  loss: 0.9120 (0.8002)  acc1: 66.6667 (78.7627)  acc5: 83.3333 (93.7673)  time: 1.6043 (0.1168 -- 11.2410)  data: 1.4425 (0.0001 -- 11.1169)  max mem: 16413
Test:  [370/395]  eta: 0:00:43  loss: 0.9120 (0.8067)  acc1: 66.6667 (78.6164)  acc5: 83.3333 (93.6208)  time: 2.3663 (0.1200 -- 29.0777)  data: 2.2094 (0.0002 -- 28.9312)  max mem: 16413
Test:  [380/395]  eta: 0:00:26  loss: 1.0041 (0.8154)  acc1: 66.6667 (78.4777)  acc5: 83.3333 (93.3946)  time: 2.3398 (0.1163 -- 29.0777)  data: 2.2009 (0.0002 -- 28.9312)  max mem: 16413
Test:  [390/395]  eta: 0:00:08  loss: 1.0335 (0.8249)  acc1: 66.6667 (78.1756)  acc5: 83.3333 (93.2225)  time: 1.2237 (0.1163 -- 15.1976)  data: 1.0762 (0.0001 -- 15.0755)  max mem: 16413
Test:  [394/395]  eta: 0:00:01  loss: 1.0041 (0.8249)  acc1: 66.6667 (78.2279)  acc5: 83.3333 (93.1646)  time: 0.4741 (0.1102 -- 4.6676)  data: 0.3321 (0.0001 -- 4.3218)  max mem: 16413
Test: Total time: 0:11:15 (1.7099 s / it)
* Acc@1 79.620 Acc@5 95.127 loss 0.762
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 4740 test videos: Top-1: 88.61%, Top-5: 100.00%
Training time 3:07:16
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
