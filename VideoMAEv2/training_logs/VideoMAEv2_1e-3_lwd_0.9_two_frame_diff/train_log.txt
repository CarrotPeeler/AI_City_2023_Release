[2023-09-23 01:07:32,382] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-23 01:07:32,423] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, img_diff_json_path='/home/vislab-001/Jared/Envy_AI_City/data/img_diff_aicity_train.json', init_scale=0.001, input_size=224, layer_decay=0.9, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sample_mode='two_frame_difference', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc5a8bb64c0>
Mixup is activated!
[2023-09-23 01:07:38,598] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-23 01:07:38,598] [INFO] [comm.py:631:init_distributed] cdb=None
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.81
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.81
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.9
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.9
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-09-23 01:07:38,776] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-23 01:07:38,776] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-23 01:07:38,878] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.4164910316467285 seconds
[2023-09-23 01:07:40,079] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-23 01:07:40,083] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-09-23 01:07:40,083] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-09-23 01:07:40,098] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-23 01:07:40,098] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-23 01:07:40,098] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-23 01:07:40,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 01:07:40,098] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc4866a03a0>
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-23 01:07:40,099] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   world_size ................... 2
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-23 01:07:40,100] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-23 01:07:40,100] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:33:42  lr: 0.000000  min_lr: 0.000000  loss: 2.7732 (2.7732)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 12.6409 (12.6409 -- 12.6409)  data: 8.6854 (8.6854 -- 8.6854)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:53  lr: 0.000001  min_lr: 0.000000  loss: 2.7729 (2.7729)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5840 (1.6072)  time: 0.6726 (0.4979 -- 2.5446)  data: 0.1389 (0.0002 -- 2.0673)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:16  lr: 0.000002  min_lr: 0.000001  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4812 (1.5598)  time: 1.0247 (0.4933 -- 5.1639)  data: 0.4871 (0.0003 -- 4.6621)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:45  lr: 0.000004  min_lr: 0.000001  loss: 2.7724 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4700 (1.5469)  time: 0.8847 (0.4952 -- 3.2591)  data: 0.2407 (0.0003 -- 2.7641)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:21  lr: 0.000005  min_lr: 0.000001  loss: 2.7722 (2.7724)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5169 (1.5408)  time: 0.9037 (0.5095 -- 2.8887)  data: 0.3770 (0.0003 -- 2.3757)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:01:00  lr: 0.000006  min_lr: 0.000001  loss: 2.7718 (2.7723)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4858 (1.5321)  time: 0.9997 (0.5151 -- 3.5018)  data: 0.4685 (0.0003 -- 2.9876)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:40  lr: 0.000007  min_lr: 0.000002  loss: 2.7712 (2.7722)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5123 (1.5328)  time: 1.0004 (0.4986 -- 3.6943)  data: 0.4435 (0.0002 -- 3.1764)  max mem: 16413
[2023-09-23 01:09:50,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:09:50,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:09:50,376] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-09-23 01:09:50,376] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:19  lr: 0.000008  min_lr: 0.000002  loss: 2.7699 (2.7719)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4310 (1.5182)  time: 0.8428 (0.5142 -- 4.0106)  data: 0.2953 (0.0003 -- 3.5099)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 2.7695 (2.7716)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3792 (1.5061)  time: 0.7103 (0.4835 -- 2.0865)  data: 0.1423 (0.0002 -- 1.5796)  max mem: 16413
Epoch: [0] Total time: 0:02:32 (0.9558 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 2.7695 (2.7715)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3792 (1.5061)
Val:  [ 0/27]  eta: 0:01:24  loss: 2.7628 (2.7628)  acc1: 0.0000 (0.0000)  acc5: 88.8889 (88.8889)  time: 3.1396 (3.1396 -- 3.1396)  data: 2.7792 (2.7792 -- 2.7792)  max mem: 16413
Val:  [10/27]  eta: 0:00:09  loss: 2.7628 (2.7624)  acc1: 22.2222 (27.2727)  acc5: 77.7778 (82.8283)  time: 0.5413 (0.1745 -- 3.1396)  data: 0.3350 (0.0003 -- 2.7792)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7613 (2.7617)  acc1: 33.3333 (31.2169)  acc5: 77.7778 (84.1270)  time: 0.2610 (0.1667 -- 1.1096)  data: 0.0757 (0.0001 -- 0.8991)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7613 (2.7621)  acc1: 33.3333 (30.2905)  acc5: 77.7778 (82.5726)  time: 0.2801 (0.1662 -- 1.1096)  data: 0.1002 (0.0001 -- 0.8991)  max mem: 16413
Val: Total time: 0:00:09 (0.3653 s / it)
* Acc@1 31.535 Acc@5 83.817 loss 2.762
Accuracy of the network on the 482 val images: 31.54%
[2023-09-23 01:10:23,331] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-09-23 01:10:23,335] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:10:23,335] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:10:23,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:10:24,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:10:24,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 31.54%
Epoch: [1]  [  0/160]  eta: 0:19:40  lr: 0.000009  min_lr: 0.000002  loss: 2.7682 (2.7682)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6720 (1.6720)  time: 7.3786 (7.3786 -- 7.3786)  data: 6.4765 (6.4765 -- 6.4765)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:03:01  lr: 0.000011  min_lr: 0.000003  loss: 2.7652 (2.7651)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4654 (1.5064)  time: 0.9906 (0.5103 -- 2.9625)  data: 0.4618 (0.0004 -- 2.4426)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:13  lr: 0.000012  min_lr: 0.000003  loss: 2.7615 (2.7637)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5571 (1.5282)  time: 0.9135 (0.5091 -- 4.0284)  data: 0.3329 (0.0003 -- 3.5023)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:49  lr: 0.000013  min_lr: 0.000003  loss: 2.7559 (2.7612)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5434 (1.5473)  time: 1.0788 (0.5225 -- 3.6553)  data: 0.4076 (0.0004 -- 3.1474)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:24  lr: 0.000014  min_lr: 0.000004  loss: 2.7482 (2.7577)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5632 (1.5440)  time: 0.9489 (0.5032 -- 3.0430)  data: 0.1864 (0.0005 -- 1.8623)  max mem: 16413
[2023-09-23 01:12:04,838] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:12:04,838] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:12:04,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-09-23 01:12:04,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:01:02  lr: 0.000015  min_lr: 0.000004  loss: 2.7397 (2.7538)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5943 (1.5559)  time: 0.9273 (0.5180 -- 2.7319)  data: 0.2136 (0.0004 -- 1.8714)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:40  lr: 0.000016  min_lr: 0.000004  loss: 2.7203 (2.7487)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6294 (1.5667)  time: 0.8489 (0.5118 -- 4.2898)  data: 0.2573 (0.0001 -- 3.7621)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:19  lr: 0.000018  min_lr: 0.000004  loss: 2.7038 (2.7429)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7912 (1.5912)  time: 0.9367 (0.5193 -- 4.2900)  data: 0.3983 (0.0004 -- 3.7683)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 2.7061 (2.7390)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6923 (1.6004)  time: 0.6493 (0.4907 -- 2.8338)  data: 0.1360 (0.0002 -- 2.3250)  max mem: 16413
Epoch: [1] Total time: 0:02:32 (0.9548 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 2.7061 (2.7391)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6923 (1.6004)
Val:  [ 0/27]  eta: 0:01:07  loss: 2.6191 (2.6191)  acc1: 0.0000 (0.0000)  acc5: 88.8889 (88.8889)  time: 2.4895 (2.4895 -- 2.4895)  data: 2.2459 (2.2459 -- 2.2459)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 2.6352 (2.6299)  acc1: 44.4444 (40.4040)  acc5: 88.8889 (92.9293)  time: 0.4801 (0.1762 -- 2.4895)  data: 0.2835 (0.0004 -- 2.2459)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6270 (2.6257)  acc1: 44.4444 (41.2698)  acc5: 88.8889 (93.1217)  time: 0.2529 (0.1683 -- 1.0810)  data: 0.0656 (0.0001 -- 0.8633)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6270 (2.6299)  acc1: 44.4444 (39.0041)  acc5: 88.8889 (92.1162)  time: 0.2700 (0.1312 -- 1.0810)  data: 0.0905 (0.0001 -- 0.8633)  max mem: 16413
Val: Total time: 0:00:09 (0.3344 s / it)
* Acc@1 41.701 Acc@5 92.531 loss 2.630
Accuracy of the network on the 482 val images: 41.70%
[2023-09-23 01:13:06,415] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:13:06,417] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:13:06,417] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:13:06,417] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:13:07,796] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:13:07,797] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.70%
Epoch: [2]  [  0/160]  eta: 0:24:58  lr: 0.000019  min_lr: 0.000005  loss: 2.7276 (2.7276)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5795 (1.5795)  time: 9.3646 (9.3646 -- 9.3646)  data: 8.8253 (8.8253 -- 8.8253)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:03:10  lr: 0.000020  min_lr: 0.000005  loss: 2.6978 (2.6965)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6392 (1.6394)  time: 0.9582 (0.5127 -- 5.6768)  data: 0.4260 (0.0003 -- 5.1767)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:16  lr: 0.000021  min_lr: 0.000005  loss: 2.6716 (2.6826)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6348 (1.6780)  time: 0.9031 (0.5124 -- 4.8470)  data: 0.3657 (0.0003 -- 4.3375)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:49  lr: 0.000022  min_lr: 0.000006  loss: 2.6536 (2.6721)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7166 (1.7076)  time: 1.0122 (0.5128 -- 5.6009)  data: 0.4756 (0.0002 -- 5.1048)  max mem: 16413
[2023-09-23 01:14:17,583] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:14:17,584] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-09-23 01:14:17,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:14:17,585] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:23  lr: 0.000023  min_lr: 0.000006  loss: 2.6425 (2.6630)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7728 (1.7367)  time: 0.8781 (0.5105 -- 4.1050)  data: 0.3455 (0.0002 -- 3.5934)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:01:01  lr: 0.000025  min_lr: 0.000006  loss: 2.6226 (2.6561)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8254 (1.7526)  time: 0.9325 (0.5163 -- 3.3958)  data: 0.3928 (0.0003 -- 2.8602)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:40  lr: 0.000026  min_lr: 0.000007  loss: 2.6497 (2.6529)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8049 (1.7802)  time: 0.9434 (0.5038 -- 3.4075)  data: 0.4090 (0.0004 -- 2.8732)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:20  lr: 0.000027  min_lr: 0.000007  loss: 2.5901 (2.6435)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7813 (1.7917)  time: 1.0455 (0.5191 -- 4.1901)  data: 0.5030 (0.0002 -- 3.6445)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 2.5608 (2.6341)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7156 (1.8005)  time: 0.6435 (0.4904 -- 3.1458)  data: 0.1327 (0.0002 -- 2.6448)  max mem: 16413
Epoch: [2] Total time: 0:02:35 (0.9699 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 2.5608 (2.6345)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7156 (1.8005)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.4004 (2.4004)  acc1: 11.1111 (11.1111)  acc5: 77.7778 (77.7778)  time: 2.3559 (2.3559 -- 2.3559)  data: 2.1775 (2.1775 -- 2.1775)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 2.4013 (2.3953)  acc1: 33.3333 (35.3535)  acc5: 100.0000 (94.9495)  time: 0.4750 (0.1755 -- 2.3559)  data: 0.2842 (0.0002 -- 2.1775)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.3774 (2.3829)  acc1: 33.3333 (35.9788)  acc5: 100.0000 (96.8254)  time: 0.2613 (0.1665 -- 1.1472)  data: 0.0733 (0.0001 -- 0.9437)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.3774 (2.3922)  acc1: 33.3333 (34.8548)  acc5: 100.0000 (95.4357)  time: 0.2753 (0.1322 -- 1.1472)  data: 0.0973 (0.0001 -- 0.9437)  max mem: 16413
Val: Total time: 0:00:09 (0.3348 s / it)
* Acc@1 40.041 Acc@5 95.021 loss 2.389
Accuracy of the network on the 482 val images: 40.04%
Max accuracy: 41.70%
Epoch: [3]  [  0/160]  eta: 0:20:49  lr: 0.000028  min_lr: 0.000007  loss: 2.5785 (2.5785)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7061 (2.7061)  time: 7.8110 (7.8110 -- 7.8110)  data: 7.2442 (7.2442 -- 7.2442)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:57  lr: 0.000029  min_lr: 0.000007  loss: 2.5943 (2.5701)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0074 (2.1068)  time: 0.9388 (0.5246 -- 3.8517)  data: 0.2997 (0.0005 -- 3.1291)  max mem: 16413
[2023-09-23 01:16:30,756] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:16:30,757] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-09-23 01:16:30,759] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:16:30,759] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:17  lr: 0.000031  min_lr: 0.000008  loss: 2.5381 (2.5579)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9720 (2.0566)  time: 1.0168 (0.5145 -- 4.3548)  data: 0.2101 (0.0003 -- 1.4508)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:46  lr: 0.000032  min_lr: 0.000008  loss: 2.4837 (2.5339)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2355 (2.1494)  time: 0.9008 (0.5274 -- 3.6357)  data: 0.2679 (0.0005 -- 3.1212)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:22  lr: 0.000033  min_lr: 0.000008  loss: 2.4939 (2.5241)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1248 (2.1908)  time: 0.9203 (0.5111 -- 2.9268)  data: 0.3885 (0.0002 -- 2.4111)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:59  lr: 0.000034  min_lr: 0.000009  loss: 2.5346 (2.5245)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1436 (2.2121)  time: 0.8346 (0.5237 -- 2.9687)  data: 0.2930 (0.0003 -- 2.4623)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:39  lr: 0.000035  min_lr: 0.000009  loss: 2.4918 (2.5202)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5566 (2.2556)  time: 0.9947 (0.5117 -- 3.7107)  data: 0.4565 (0.0003 -- 3.1574)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:19  lr: 0.000036  min_lr: 0.000009  loss: 2.4871 (2.5144)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3730 (2.2793)  time: 0.8672 (0.5100 -- 3.6852)  data: 0.2363 (0.0002 -- 3.1726)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 2.4737 (2.5084)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3778 (2.3179)  time: 0.6875 (0.4944 -- 2.0070)  data: 0.0755 (0.0002 -- 1.4935)  max mem: 16413
Epoch: [3] Total time: 0:02:30 (0.9407 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 2.4737 (2.5036)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3778 (2.3179)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.1668 (2.1668)  acc1: 0.0000 (0.0000)  acc5: 100.0000 (100.0000)  time: 2.3882 (2.3882 -- 2.3882)  data: 2.2002 (2.2002 -- 2.2002)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 2.1631 (2.1354)  acc1: 33.3333 (35.3535)  acc5: 100.0000 (98.9899)  time: 0.4735 (0.1730 -- 2.3882)  data: 0.2843 (0.0002 -- 2.2002)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.1123 (2.1187)  acc1: 44.4444 (39.6825)  acc5: 100.0000 (97.3545)  time: 0.2605 (0.1681 -- 1.1192)  data: 0.0741 (0.0001 -- 0.9206)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.1123 (2.1309)  acc1: 44.4444 (38.1743)  acc5: 100.0000 (97.0954)  time: 0.2836 (0.1316 -- 1.1192)  data: 0.1054 (0.0001 -- 0.9206)  max mem: 16413
Val: Total time: 0:00:09 (0.3409 s / it)
* Acc@1 42.946 Acc@5 94.606 loss 2.136
Accuracy of the network on the 482 val images: 42.95%
[2023-09-23 01:18:32,209] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:18:32,210] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:18:32,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:18:32,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:18:33,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:18:33,543] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.95%
[2023-09-23 01:18:41,806] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:18:41,808] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-09-23 01:18:41,809] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:18:41,809] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:22:03  lr: 0.000038  min_lr: 0.000010  loss: 2.2225 (2.2225)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3605 (2.3605)  time: 8.2715 (8.2715 -- 8.2715)  data: 7.7416 (7.7416 -- 7.7416)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:56  lr: 0.000039  min_lr: 0.000010  loss: 2.4019 (2.3838)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4880 (2.5662)  time: 0.9074 (0.5215 -- 3.3069)  data: 0.1950 (0.0003 -- 2.6871)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:02:07  lr: 0.000040  min_lr: 0.000010  loss: 2.3865 (2.3931)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4043 (2.6046)  time: 0.8624 (0.5162 -- 2.4570)  data: 0.1379 (0.0003 -- 1.9122)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:41  lr: 0.000041  min_lr: 0.000010  loss: 2.3569 (2.3765)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3891 (2.5882)  time: 0.9120 (0.5186 -- 2.6406)  data: 0.0842 (0.0002 -- 0.8788)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:21  lr: 0.000042  min_lr: 0.000011  loss: 2.3435 (2.3669)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0447 (2.7349)  time: 1.0199 (0.5378 -- 2.5033)  data: 0.2000 (0.0008 -- 1.3913)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:59  lr: 0.000043  min_lr: 0.000011  loss: 2.3512 (2.3665)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9152 (2.8028)  time: 0.8806 (0.5290 -- 2.9944)  data: 0.1259 (0.0005 -- 1.0406)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:39  lr: 0.000045  min_lr: 0.000011  loss: 2.3053 (2.3533)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0102 (2.8618)  time: 0.9227 (0.5126 -- 2.1347)  data: 0.3063 (0.0003 -- 1.5730)  max mem: 16413
[2023-09-23 01:20:38,952] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:20:38,952] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-09-23 01:20:38,953] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:20:38,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 2.3771 (2.3569)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9315 (2.9315)  time: 0.9418 (0.5458 -- 2.2201)  data: 0.2630 (0.0004 -- 1.6622)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.3816 (2.3588)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0496 (2.9723)  time: 0.7979 (0.4977 -- 1.9556)  data: 0.1546 (0.0003 -- 1.4291)  max mem: 16413
Epoch: [4] Total time: 0:02:31 (0.9474 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.3816 (2.3636)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0496 (2.9723)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.9020 (1.9020)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3502 (2.3502 -- 2.3502)  data: 2.1610 (2.1610 -- 2.1610)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.8888 (1.8517)  acc1: 44.4444 (46.4646)  acc5: 100.0000 (97.9798)  time: 0.4659 (0.1860 -- 2.3502)  data: 0.2730 (0.0005 -- 2.1610)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8142 (1.8428)  acc1: 55.5556 (48.1481)  acc5: 100.0000 (97.3545)  time: 0.2624 (0.1674 -- 1.0041)  data: 0.0766 (0.0001 -- 0.8226)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.8142 (1.8735)  acc1: 44.4444 (44.8133)  acc5: 100.0000 (96.6805)  time: 0.2838 (0.1318 -- 1.0041)  data: 0.1076 (0.0001 -- 0.8226)  max mem: 16413
Val: Total time: 0:00:09 (0.3411 s / it)
* Acc@1 48.340 Acc@5 96.058 loss 1.874
Accuracy of the network on the 482 val images: 48.34%
[2023-09-23 01:21:14,589] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:21:14,591] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:21:14,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:21:14,591] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:21:15,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:21:15,913] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 48.34%
Epoch: [5]  [  0/160]  eta: 0:18:05  lr: 0.000047  min_lr: 0.000012  loss: 2.2813 (2.2813)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0868 (3.0868)  time: 6.7826 (6.7826 -- 6.7826)  data: 6.2454 (6.2454 -- 6.2454)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:03:03  lr: 0.000047  min_lr: 0.000012  loss: 2.2884 (2.2764)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2861 (3.4263)  time: 1.0377 (0.5293 -- 5.5479)  data: 0.1426 (0.0003 -- 1.6154)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000012  loss: 2.2464 (2.2513)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1299 (3.5316)  time: 0.7882 (0.5258 -- 2.5311)  data: 0.0577 (0.0003 -- 1.1161)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:43  lr: 0.000047  min_lr: 0.000012  loss: 2.2735 (2.2679)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0223 (3.6848)  time: 0.9976 (0.5186 -- 3.1915)  data: 0.0018 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:21  lr: 0.000047  min_lr: 0.000012  loss: 2.2353 (2.2616)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3119 (3.7945)  time: 0.9426 (0.5232 -- 3.7619)  data: 0.0044 (0.0003 -- 0.0603)  max mem: 16413
[2023-09-23 01:22:53,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:22:53,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-09-23 01:22:53,303] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:22:53,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:01:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1726 (2.2583)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4245 (3.8044)  time: 1.0089 (0.5137 -- 4.0643)  data: 0.0020 (0.0002 -- 0.0137)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 2.2058 (2.2650)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6287 (3.8422)  time: 0.8220 (0.5229 -- 3.6753)  data: 0.0017 (0.0003 -- 0.0053)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 2.2296 (2.2598)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1999 (3.7968)  time: 0.9331 (0.5094 -- 3.4896)  data: 0.0021 (0.0003 -- 0.0130)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0853 (2.2431)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8452 (3.8055)  time: 0.9538 (0.4952 -- 3.1982)  data: 0.0010 (0.0002 -- 0.0059)  max mem: 16413
Epoch: [5] Total time: 0:02:33 (0.9606 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0853 (2.2400)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8452 (3.8055)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.7340 (1.7340)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4322 (2.4322 -- 2.4322)  data: 2.2047 (2.2047 -- 2.2047)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 1.6328 (1.6367)  acc1: 55.5556 (47.4747)  acc5: 100.0000 (98.9899)  time: 0.4815 (0.1733 -- 2.4322)  data: 0.2903 (0.0004 -- 2.2047)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5843 (1.6187)  acc1: 55.5556 (47.6190)  acc5: 100.0000 (98.4127)  time: 0.2651 (0.1684 -- 1.1716)  data: 0.0797 (0.0001 -- 0.9818)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5891 (1.6585)  acc1: 44.4444 (45.2282)  acc5: 100.0000 (97.0954)  time: 0.3039 (0.1322 -- 1.1716)  data: 0.1258 (0.0001 -- 0.9818)  max mem: 16413
Val: Total time: 0:00:09 (0.3570 s / it)
* Acc@1 48.340 Acc@5 96.473 loss 1.655
Accuracy of the network on the 482 val images: 48.34%
[2023-09-23 01:23:59,536] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:23:59,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:23:59,538] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:23:59,538] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:24:00,930] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:24:00,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 48.34%
Epoch: [6]  [  0/160]  eta: 0:21:42  lr: 0.000047  min_lr: 0.000012  loss: 2.3966 (2.3966)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3024 (4.3024)  time: 8.1423 (8.1423 -- 8.1423)  data: 6.0357 (6.0357 -- 6.0357)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:03:03  lr: 0.000047  min_lr: 0.000012  loss: 2.2604 (2.2822)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4906 (3.8237)  time: 0.9701 (0.5184 -- 2.9892)  data: 0.3189 (0.0003 -- 1.7551)  max mem: 16413
[2023-09-23 01:24:45,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.1913801307802163e-05, 1.1913801307802163e-05, 1.3237557008669071e-05, 1.3237557008669071e-05, 1.4708396676298964e-05, 1.4708396676298964e-05, 1.634266297366552e-05, 1.634266297366552e-05, 1.8158514415183906e-05, 1.8158514415183906e-05, 2.017612712798212e-05, 2.017612712798212e-05, 2.2417919031091242e-05, 2.2417919031091242e-05, 2.490879892343471e-05, 2.490879892343471e-05, 2.7676443248260792e-05, 2.7676443248260792e-05, 3.0751603609178656e-05, 3.0751603609178656e-05, 3.416844845464296e-05, 3.416844845464296e-05, 3.7964942727381054e-05, 3.7964942727381054e-05, 4.218326969709006e-05, 4.218326969709006e-05, 4.68702996634334e-05, 4.68702996634334e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 01:24:45,957] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=18.686929444666717, CurrSamplesPerSec=21.56632832106511, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:02:17  lr: 0.000047  min_lr: 0.000012  loss: 2.2153 (2.2431)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5430 (4.4825)  time: 0.9686 (0.5271 -- 3.2150)  data: 0.4291 (0.0003 -- 2.6660)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:44  lr: 0.000047  min_lr: 0.000012  loss: 2.1697 (2.2191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8136 (4.4079)  time: 0.8266 (0.5239 -- 1.6919)  data: 0.2246 (0.0002 -- 1.1501)  max mem: 16413
[2023-09-23 01:25:08,512] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:25:08,512] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-09-23 01:25:08,512] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:25:08,512] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:21  lr: 0.000047  min_lr: 0.000012  loss: 2.1178 (2.2021)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2619 (4.4727)  time: 0.9553 (0.5258 -- 2.2728)  data: 0.1826 (0.0009 -- 1.3272)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:01:01  lr: 0.000047  min_lr: 0.000012  loss: 2.1934 (2.1981)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9085 (4.4786)  time: 1.0329 (0.5274 -- 4.7318)  data: 0.0013 (0.0001 -- 0.0043)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 2.1231 (2.1815)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4203 (4.4548)  time: 0.8498 (0.5249 -- 4.1658)  data: 0.0068 (0.0004 -- 0.1120)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:20  lr: 0.000047  min_lr: 0.000012  loss: 2.1771 (2.1800)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7345 (4.5702)  time: 1.0751 (0.5082 -- 5.1520)  data: 0.0013 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1630 (2.1805)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2129 (4.6312)  time: 0.7225 (0.4931 -- 3.9130)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [6] Total time: 0:02:35 (0.9728 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1630 (2.1865)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2129 (4.6312)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.7259 (1.7259)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3825 (2.3825 -- 2.3825)  data: 2.1955 (2.1955 -- 2.1955)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 1.4867 (1.5221)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (97.9798)  time: 0.4749 (0.1786 -- 2.3825)  data: 0.2827 (0.0002 -- 2.1955)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4185 (1.5033)  acc1: 55.5556 (53.4392)  acc5: 100.0000 (97.8836)  time: 0.2598 (0.1680 -- 1.0928)  data: 0.0722 (0.0001 -- 0.9057)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4412 (1.5442)  acc1: 55.5556 (52.2822)  acc5: 100.0000 (97.0954)  time: 0.2783 (0.1316 -- 1.0928)  data: 0.0995 (0.0001 -- 0.9057)  max mem: 16413
Val: Total time: 0:00:09 (0.3373 s / it)
* Acc@1 55.187 Acc@5 95.851 loss 1.531
Accuracy of the network on the 482 val images: 55.19%
[2023-09-23 01:26:46,323] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:26:46,325] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:26:46,325] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:26:46,325] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:26:47,755] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:26:47,756] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 55.19%
Epoch: [7]  [  0/160]  eta: 0:19:09  lr: 0.000047  min_lr: 0.000012  loss: 2.2511 (2.2511)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8175 (3.8175)  time: 7.1843 (7.1843 -- 7.1843)  data: 5.9016 (5.9016 -- 5.9016)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:59  lr: 0.000047  min_lr: 0.000012  loss: 2.1338 (2.1485)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4485 (4.7853)  time: 0.9904 (0.5168 -- 4.3412)  data: 0.4528 (0.0004 -- 3.8214)  max mem: 16413
[2023-09-23 01:27:26,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:27:26,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-09-23 01:27:26,771] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:27:26,772] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:02:10  lr: 0.000047  min_lr: 0.000012  loss: 2.1201 (2.1352)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8375 (4.9994)  time: 0.8731 (0.4963 -- 5.9441)  data: 0.3408 (0.0001 -- 5.4236)  max mem: 16413
Epoch: [7]  [ 60/160]  eta: 0:01:46  lr: 0.000047  min_lr: 0.000012  loss: 2.2380 (2.1551)  loss_scale: 65536.0000 (48346.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8211 (5.0467)  time: 1.0215 (0.5210 -- 5.4288)  data: 0.4853 (0.0002 -- 4.8961)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:19  lr: 0.000047  min_lr: 0.000012  loss: 2.2059 (2.1684)  loss_scale: 65536.0000 (52590.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0656 (4.9505)  time: 0.7746 (0.5249 -- 3.1433)  data: 0.2355 (0.0003 -- 2.6040)  max mem: 16413
[2023-09-23 01:28:08,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1201
[2023-09-23 01:28:08,666] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-09-23 01:28:08,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
[2023-09-23 01:28:08,666] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1201
[2023-09-23 01:28:08,666] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
Epoch: [7]  [100/160]  eta: 0:01:01  lr: 0.000047  min_lr: 0.000012  loss: 2.0941 (2.1507)  loss_scale: 32768.0000 (48665.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9229 (4.8712)  time: 1.1255 (0.5122 -- 4.9943)  data: 0.5843 (0.0003 -- 4.4847)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 2.1136 (2.1534)  loss_scale: 32768.0000 (46037.6860)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3965 (4.8387)  time: 0.7700 (0.5270 -- 4.4164)  data: 0.2301 (0.0002 -- 3.9076)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 2.1416 (2.1464)  loss_scale: 32768.0000 (44155.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9553 (4.8404)  time: 0.9800 (0.5154 -- 4.9729)  data: 0.4407 (0.0008 -- 4.4310)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1416 (2.1496)  loss_scale: 32768.0000 (42803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5801 (4.8337)  time: 0.6930 (0.4955 -- 3.5861)  data: 0.1794 (0.0002 -- 3.0875)  max mem: 16413
Epoch: [7] Total time: 0:02:31 (0.9453 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1416 (2.1426)  loss_scale: 32768.0000 (42803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5801 (4.8337)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.6678 (1.6678)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4809 (2.4809 -- 2.4809)  data: 2.2141 (2.2141 -- 2.2141)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 1.3084 (1.3551)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (98.9899)  time: 0.4727 (0.1774 -- 2.4809)  data: 0.2753 (0.0002 -- 2.2141)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2820 (1.3329)  acc1: 55.5556 (52.3810)  acc5: 100.0000 (99.4709)  time: 0.2532 (0.1687 -- 0.9919)  data: 0.0679 (0.0001 -- 0.8091)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3084 (1.3504)  acc1: 55.5556 (53.5270)  acc5: 100.0000 (99.1701)  time: 0.2813 (0.1322 -- 0.9919)  data: 0.1046 (0.0001 -- 0.8091)  max mem: 16413
Val: Total time: 0:00:09 (0.3430 s / it)
* Acc@1 56.846 Acc@5 98.755 loss 1.347
Accuracy of the network on the 482 val images: 56.85%
[2023-09-23 01:29:28,267] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:29:28,269] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:29:28,269] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:29:28,269] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:29:29,668] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:29:29,668] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 56.85%
Epoch: [8]  [  0/160]  eta: 0:21:25  lr: 0.000047  min_lr: 0.000012  loss: 1.9284 (1.9284)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5865 (5.5865)  time: 8.0331 (8.0331 -- 8.0331)  data: 6.3727 (6.3727 -- 6.3727)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:54  lr: 0.000047  min_lr: 0.000012  loss: 2.1884 (2.1429)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6942 (4.7768)  time: 0.9071 (0.5246 -- 4.0319)  data: 0.2053 (0.0002 -- 2.2560)  max mem: 16413
Epoch: [8]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000012  loss: 1.9819 (2.0675)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6369 (5.1071)  time: 0.8074 (0.5294 -- 3.8899)  data: 0.2598 (0.0004 -- 3.3526)  max mem: 16413
[2023-09-23 01:30:22,911] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:30:22,911] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:30:22,912] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:30:22,912] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [8]  [ 60/160]  eta: 0:01:43  lr: 0.000047  min_lr: 0.000012  loss: 2.1555 (2.0895)  loss_scale: 65536.0000 (38676.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6063 (5.0385)  time: 1.0490 (0.5212 -- 3.8658)  data: 0.5101 (0.0003 -- 3.3346)  max mem: 16413
[2023-09-23 01:30:43,052] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1351
[2023-09-23 01:30:43,052] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1351
[2023-09-23 01:30:43,053] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:30:43,053] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:30:43,053] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 80/160]  eta: 0:01:19  lr: 0.000047  min_lr: 0.000012  loss: 2.0905 (2.0901)  loss_scale: 32768.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0045 (5.0777)  time: 0.8629 (0.5131 -- 4.3221)  data: 0.3177 (0.0003 -- 3.7806)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:59  lr: 0.000047  min_lr: 0.000012  loss: 2.1809 (2.1017)  loss_scale: 32768.0000 (39581.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3803 (5.0306)  time: 0.9494 (0.5207 -- 3.8819)  data: 0.4041 (0.0004 -- 3.3405)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000012  loss: 2.0824 (2.1011)  loss_scale: 32768.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9525 (5.0942)  time: 0.8432 (0.5272 -- 4.6165)  data: 0.2943 (0.0007 -- 4.0826)  max mem: 16413
Epoch: [8]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 2.0294 (2.1076)  loss_scale: 32768.0000 (37648.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4511 (5.2019)  time: 1.0009 (0.5217 -- 5.2103)  data: 0.1352 (0.0004 -- 1.5280)  max mem: 16413
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1564 (2.1097)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7259 (5.1530)  time: 0.7539 (0.4939 -- 2.6719)  data: 0.0009 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [8] Total time: 0:02:30 (0.9437 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1564 (2.0928)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7259 (5.1530)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.4357 (1.4357)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4147 (2.4147 -- 2.4147)  data: 2.2272 (2.2272 -- 2.2272)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 1.2485 (1.2992)  acc1: 66.6667 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4723 (0.1761 -- 2.4147)  data: 0.2862 (0.0004 -- 2.2272)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2320 (1.2776)  acc1: 55.5556 (60.3175)  acc5: 100.0000 (98.4127)  time: 0.2586 (0.1685 -- 1.0990)  data: 0.0751 (0.0001 -- 0.9105)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2369 (1.3094)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (97.9253)  time: 0.2873 (0.1323 -- 1.0990)  data: 0.1106 (0.0001 -- 0.9105)  max mem: 16413
Val: Total time: 0:00:09 (0.3438 s / it)
* Acc@1 62.863 Acc@5 97.510 loss 1.286
Accuracy of the network on the 482 val images: 62.86%
[2023-09-23 01:32:10,039] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:32:10,041] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:32:10,041] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:32:10,041] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:32:11,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:32:11,438] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.86%
Epoch: [9]  [  0/160]  eta: 0:25:35  lr: 0.000047  min_lr: 0.000012  loss: 2.0155 (2.0155)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5144 (4.5144)  time: 9.5963 (9.5963 -- 9.5963)  data: 9.0238 (9.0238 -- 9.0238)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:02:49  lr: 0.000047  min_lr: 0.000012  loss: 2.0178 (2.0443)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9784 (5.8186)  time: 0.7902 (0.5276 -- 3.0498)  data: 0.2340 (0.0004 -- 2.3430)  max mem: 16413
[2023-09-23 01:32:57,819] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:32:57,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:32:57,824] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:32:57,824] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 40/160]  eta: 0:02:15  lr: 0.000047  min_lr: 0.000012  loss: 2.0313 (2.0557)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8746 (5.6890)  time: 1.0491 (0.5114 -- 5.6042)  data: 0.1664 (0.0004 -- 2.0918)  max mem: 16413
[2023-09-23 01:33:07,390] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1490
[2023-09-23 01:33:07,391] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:33:07,391] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1490
[2023-09-23 01:33:07,391] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:33:07,391] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 60/160]  eta: 0:01:43  lr: 0.000047  min_lr: 0.000012  loss: 2.0527 (2.0423)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7726 (5.4613)  time: 0.8440 (0.5121 -- 4.6697)  data: 0.0011 (0.0004 -- 0.0020)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:18  lr: 0.000047  min_lr: 0.000012  loss: 2.1101 (2.0422)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7461 (5.5505)  time: 0.8312 (0.5319 -- 2.9552)  data: 0.0649 (0.0005 -- 1.2728)  max mem: 16413
Epoch: [9]  [100/160]  eta: 0:00:59  lr: 0.000047  min_lr: 0.000012  loss: 2.0173 (2.0437)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1531 (5.5236)  time: 0.9855 (0.5138 -- 3.4877)  data: 0.1992 (0.0003 -- 2.9694)  max mem: 16413
Epoch: [9]  [120/160]  eta: 0:00:38  lr: 0.000047  min_lr: 0.000012  loss: 2.0580 (2.0452)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0827 (5.5278)  time: 0.8659 (0.5208 -- 4.3558)  data: 0.0019 (0.0003 -- 0.0079)  max mem: 16413
[2023-09-23 01:34:21,628] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1572
[2023-09-23 01:34:21,629] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:34:21,628] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1572
[2023-09-23 01:34:21,630] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:34:21,631] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [9]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 1.9518 (2.0362)  loss_scale: 32768.0000 (34046.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9161 (5.5076)  time: 1.0388 (0.5139 -- 4.0917)  data: 0.0078 (0.0004 -- 0.1381)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9433 (2.0333)  loss_scale: 16384.0000 (31948.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1360 (5.5280)  time: 0.6468 (0.4965 -- 2.7706)  data: 0.0009 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [9] Total time: 0:02:30 (0.9383 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9433 (2.0186)  loss_scale: 16384.0000 (31948.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1360 (5.5280)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.3580 (1.3580)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3762 (2.3762 -- 2.3762)  data: 2.1975 (2.1975 -- 2.1975)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 1.1998 (1.2242)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (96.9697)  time: 0.4917 (0.1774 -- 2.3762)  data: 0.2940 (0.0002 -- 2.1975)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1231 (1.1846)  acc1: 55.5556 (59.7884)  acc5: 100.0000 (98.4127)  time: 0.2642 (0.1685 -- 1.2495)  data: 0.0712 (0.0001 -- 1.0287)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1915 (1.2334)  acc1: 55.5556 (57.6763)  acc5: 100.0000 (97.5104)  time: 0.2890 (0.1323 -- 1.2495)  data: 0.1077 (0.0001 -- 1.0287)  max mem: 16413
Val: Total time: 0:00:09 (0.3475 s / it)
* Acc@1 60.788 Acc@5 96.888 loss 1.229
Accuracy of the network on the 482 val images: 60.79%
Max accuracy: 62.86%
Epoch: [10]  [  0/160]  eta: 0:17:28  lr: 0.000047  min_lr: 0.000012  loss: 2.1755 (2.1755)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8673 (3.8673)  time: 6.5532 (6.5532 -- 6.5532)  data: 6.0110 (6.0110 -- 6.0110)  max mem: 16413
Epoch: [10]  [ 20/160]  eta: 0:02:41  lr: 0.000047  min_lr: 0.000012  loss: 1.9067 (1.9538)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6440 (5.0994)  time: 0.8830 (0.5168 -- 3.5019)  data: 0.3352 (0.0009 -- 2.9809)  max mem: 16413
Epoch: [10]  [ 40/160]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000012  loss: 2.0667 (2.0142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9182 (5.0906)  time: 0.9544 (0.5276 -- 3.8465)  data: 0.1933 (0.0002 -- 1.3321)  max mem: 16413
Epoch: [10]  [ 60/160]  eta: 0:01:44  lr: 0.000047  min_lr: 0.000012  loss: 1.7767 (1.9549)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9084 (5.1395)  time: 1.0177 (0.5145 -- 2.6245)  data: 0.1899 (0.0004 -- 1.5011)  max mem: 16413
Epoch: [10]  [ 80/160]  eta: 0:01:20  lr: 0.000047  min_lr: 0.000012  loss: 1.8940 (1.9357)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8150 (5.2310)  time: 0.8718 (0.5290 -- 2.4464)  data: 0.2432 (0.0005 -- 1.9122)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:59  lr: 0.000047  min_lr: 0.000012  loss: 1.9964 (1.9587)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2872 (5.2964)  time: 0.9214 (0.5252 -- 2.8115)  data: 0.3164 (0.0006 -- 2.3065)  max mem: 16413
[2023-09-23 01:36:31,013] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:36:31,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 01:36:31,013] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:36:31,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 1.9239 (1.9494)  loss_scale: 32768.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3568 (5.3065)  time: 0.9308 (0.5206 -- 2.8917)  data: 0.3145 (0.0001 -- 2.3322)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 2.0459 (1.9645)  loss_scale: 32768.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4433 (5.3254)  time: 0.9676 (0.5120 -- 3.6431)  data: 0.2556 (0.0005 -- 3.1245)  max mem: 16413
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9970 (1.9693)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1420 (5.3485)  time: 0.7472 (0.4948 -- 3.0129)  data: 0.2262 (0.0001 -- 2.4824)  max mem: 16413
Epoch: [10] Total time: 0:02:31 (0.9495 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9970 (1.9911)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1420 (5.3485)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.5203 (1.5203)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4596 (2.4596 -- 2.4596)  data: 2.2617 (2.2617 -- 2.2617)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 1.0684 (1.1393)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (98.9899)  time: 0.4880 (0.1760 -- 2.4596)  data: 0.2988 (0.0003 -- 2.2617)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0606 (1.0865)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (99.4709)  time: 0.2591 (0.1691 -- 1.2118)  data: 0.0747 (0.0001 -- 1.0184)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0684 (1.1114)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (99.1701)  time: 0.2866 (0.1322 -- 1.2118)  data: 0.1100 (0.0001 -- 1.0184)  max mem: 16413
Val: Total time: 0:00:09 (0.3457 s / it)
* Acc@1 66.805 Acc@5 98.548 loss 1.133
Accuracy of the network on the 482 val images: 66.80%
[2023-09-23 01:37:32,214] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:37:32,216] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:37:32,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:37:32,216] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:37:33,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:37:33,790] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.80%
Epoch: [11]  [  0/160]  eta: 0:24:59  lr: 0.000047  min_lr: 0.000012  loss: 2.3462 (2.3462)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7988 (9.7988)  time: 9.3748 (9.3748 -- 9.3748)  data: 8.8636 (8.8636 -- 8.8636)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:54  lr: 0.000047  min_lr: 0.000012  loss: 2.0572 (2.1076)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0182 (5.2953)  time: 0.8431 (0.5161 -- 4.1997)  data: 0.2072 (0.0004 -- 2.2221)  max mem: 16413
Epoch: [11]  [ 40/160]  eta: 0:02:13  lr: 0.000047  min_lr: 0.000012  loss: 1.9038 (2.0345)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2590 (5.6223)  time: 0.9618 (0.5138 -- 3.3312)  data: 0.1745 (0.0004 -- 1.9286)  max mem: 16413
Epoch: [11]  [ 60/160]  eta: 0:01:44  lr: 0.000047  min_lr: 0.000012  loss: 1.9415 (2.0142)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2090 (5.6541)  time: 0.9254 (0.5203 -- 4.4903)  data: 0.1170 (0.0004 -- 1.5889)  max mem: 16413
[2023-09-23 01:38:46,063] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:38:46,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:38:46,064] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:38:46,064] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:38:55,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1840
[2023-09-23 01:38:55,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:38:55,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1840
[2023-09-23 01:38:55,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:38:55,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [11]  [ 80/160]  eta: 0:01:21  lr: 0.000047  min_lr: 0.000012  loss: 2.0861 (2.0308)  loss_scale: 65536.0000 (37217.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6339 (5.7050)  time: 0.9103 (0.5185 -- 3.9230)  data: 0.0014 (0.0005 -- 0.0036)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:01:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9804 (2.0200)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6807 (5.7906)  time: 0.9425 (0.5371 -- 3.1633)  data: 0.0217 (0.0004 -- 0.4043)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 1.8602 (2.0092)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9605 (5.8930)  time: 0.8637 (0.5183 -- 2.6615)  data: 0.0699 (0.0003 -- 1.0673)  max mem: 16413
Epoch: [11]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 1.9089 (1.9936)  loss_scale: 32768.0000 (35324.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7491 (5.7848)  time: 1.0339 (0.5255 -- 4.4539)  data: 0.0026 (0.0003 -- 0.0137)  max mem: 16413
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.7352 (1.9841)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7690 (5.7279)  time: 0.7296 (0.4950 -- 3.2499)  data: 0.0204 (0.0002 -- 0.3834)  max mem: 16413
Epoch: [11] Total time: 0:02:33 (0.9568 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.7352 (1.9524)  loss_scale: 32768.0000 (35020.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7690 (5.7279)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.3019 (1.3019)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3996 (2.3996 -- 2.3996)  data: 2.2038 (2.2038 -- 2.2038)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0352 (1.0717)  acc1: 55.5556 (62.6263)  acc5: 100.0000 (98.9899)  time: 0.4695 (0.1854 -- 2.3996)  data: 0.2752 (0.0002 -- 2.2038)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9671 (1.0233)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (99.4709)  time: 0.2553 (0.1680 -- 1.0014)  data: 0.0658 (0.0001 -- 0.8182)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9809 (1.0706)  acc1: 55.5556 (63.0705)  acc5: 100.0000 (98.7552)  time: 0.2770 (0.1319 -- 1.0014)  data: 0.0962 (0.0001 -- 0.8182)  max mem: 16413
Val: Total time: 0:00:09 (0.3368 s / it)
* Acc@1 66.598 Acc@5 98.133 loss 1.064
Accuracy of the network on the 482 val images: 66.60%
Max accuracy: 66.80%
Epoch: [12]  [  0/160]  eta: 0:21:18  lr: 0.000047  min_lr: 0.000012  loss: 1.1693 (1.1693)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4717 (3.4717)  time: 7.9908 (7.9908 -- 7.9908)  data: 5.4150 (5.4150 -- 5.4150)  max mem: 16413
Epoch: [12]  [ 20/160]  eta: 0:02:52  lr: 0.000047  min_lr: 0.000012  loss: 1.8269 (1.7607)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7527 (5.8023)  time: 0.8965 (0.5164 -- 3.5614)  data: 0.0667 (0.0004 -- 1.2902)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:02:08  lr: 0.000047  min_lr: 0.000012  loss: 1.8862 (1.8193)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8340 (5.8477)  time: 0.8924 (0.5221 -- 2.5382)  data: 0.0245 (0.0005 -- 0.2924)  max mem: 16413
[2023-09-23 01:41:08,751] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:41:08,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:41:08,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:41:08,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:41:15,993] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1977
[2023-09-23 01:41:15,993] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1977
[2023-09-23 01:41:15,993] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:41:15,993] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:41:15,993] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000012  loss: 1.8907 (1.8295)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2109 (5.8033)  time: 0.9426 (0.5158 -- 3.7436)  data: 0.0709 (0.0004 -- 0.6936)  max mem: 16413
[2023-09-23 01:41:35,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=6, lr=[1.1871674843346339e-05, 1.1871674843346339e-05, 1.3190749825940376e-05, 1.3190749825940376e-05, 1.4656388695489305e-05, 1.4656388695489305e-05, 1.628487632832145e-05, 1.628487632832145e-05, 1.8094307031468277e-05, 1.8094307031468277e-05, 2.010478559052031e-05, 2.010478559052031e-05, 2.2338650656133676e-05, 2.2338650656133676e-05, 2.4820722951259638e-05, 2.4820722951259638e-05, 2.7578581056955155e-05, 2.7578581056955155e-05, 3.064286784106128e-05, 3.064286784106128e-05, 3.4047630934512536e-05, 3.4047630934512536e-05, 3.783070103834726e-05, 3.783070103834726e-05, 4.203411226483028e-05, 4.203411226483028e-05, 4.670456918314476e-05, 4.670456918314476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 01:41:35,083] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=17.75375163341207, CurrSamplesPerSec=21.695386573008427, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:22  lr: 0.000047  min_lr: 0.000012  loss: 2.0253 (1.8805)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0626 (5.7124)  time: 1.0353 (0.5287 -- 4.4292)  data: 0.0619 (0.0004 -- 0.5873)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:59  lr: 0.000047  min_lr: 0.000012  loss: 2.1076 (1.9121)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9727 (5.7126)  time: 0.8744 (0.5163 -- 3.0499)  data: 0.0013 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [12]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 2.1049 (1.9322)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6815 (5.7527)  time: 0.8775 (0.5177 -- 2.8703)  data: 0.0026 (0.0003 -- 0.0163)  max mem: 16413
Epoch: [12]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 1.6523 (1.9025)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9938 (5.7904)  time: 0.8810 (0.5246 -- 3.4313)  data: 0.0015 (0.0005 -- 0.0058)  max mem: 16413
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0016 (1.9152)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5493 (5.8210)  time: 0.8020 (0.4965 -- 4.0799)  data: 0.0615 (0.0002 -- 1.2066)  max mem: 16413
Epoch: [12] Total time: 0:02:31 (0.9469 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0016 (1.9279)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5493 (5.8210)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.2161 (1.2161)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4628 (2.4628 -- 2.4628)  data: 2.2639 (2.2639 -- 2.2639)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.9844 (1.0375)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (98.9899)  time: 0.4772 (0.1799 -- 2.4628)  data: 0.2884 (0.0002 -- 2.2639)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9235 (0.9633)  acc1: 66.6667 (67.1958)  acc5: 100.0000 (99.4709)  time: 0.2536 (0.1689 -- 1.0956)  data: 0.0656 (0.0001 -- 0.9035)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9348 (1.0202)  acc1: 66.6667 (64.3154)  acc5: 100.0000 (98.3402)  time: 0.2770 (0.1323 -- 1.0956)  data: 0.0968 (0.0001 -- 0.9035)  max mem: 16413
Val: Total time: 0:00:09 (0.3403 s / it)
* Acc@1 67.842 Acc@5 97.718 loss 1.028
Accuracy of the network on the 482 val images: 67.84%
[2023-09-23 01:42:57,128] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:42:57,130] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:42:57,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:42:57,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:42:58,490] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:42:58,490] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.84%
Epoch: [13]  [  0/160]  eta: 0:21:16  lr: 0.000047  min_lr: 0.000012  loss: 1.5982 (1.5982)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9154 (3.9154)  time: 7.9776 (7.9776 -- 7.9776)  data: 7.4488 (7.4488 -- 7.4488)  max mem: 16413
Epoch: [13]  [ 20/160]  eta: 0:02:50  lr: 0.000047  min_lr: 0.000012  loss: 1.9925 (1.8694)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6843 (5.4321)  time: 0.8775 (0.5187 -- 3.0115)  data: 0.1685 (0.0003 -- 2.4834)  max mem: 16413
[2023-09-23 01:43:29,775] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:43:29,776] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:43:29,777] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:43:29,777] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:43:37,382] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2113
[2023-09-23 01:43:37,382] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:43:37,383] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2113
[2023-09-23 01:43:37,383] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:43:37,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 40/160]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000012  loss: 1.9900 (1.9393)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2966 (5.3595)  time: 0.8771 (0.5292 -- 2.6890)  data: 0.0481 (0.0006 -- 0.9244)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:44  lr: 0.000047  min_lr: 0.000012  loss: 1.9705 (1.9514)  loss_scale: 32768.0000 (36528.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3251 (5.3498)  time: 1.0461 (0.5297 -- 3.8312)  data: 0.0014 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:21  lr: 0.000047  min_lr: 0.000012  loss: 2.0168 (1.9587)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5264 (5.4798)  time: 0.9212 (0.5368 -- 3.1691)  data: 0.0014 (0.0005 -- 0.0023)  max mem: 16413
Epoch: [13]  [100/160]  eta: 0:01:02  lr: 0.000047  min_lr: 0.000012  loss: 1.7982 (1.9550)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7769 (5.5770)  time: 1.0987 (0.5079 -- 7.0790)  data: 0.0011 (0.0003 -- 0.0021)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 1.8812 (1.9477)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3907 (5.6984)  time: 0.8091 (0.5114 -- 4.2200)  data: 0.0012 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 1.7881 (1.9276)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7306 (5.7626)  time: 0.8989 (0.5271 -- 3.3405)  data: 0.0016 (0.0003 -- 0.0028)  max mem: 16413
[2023-09-23 01:45:27,532] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2233
[2023-09-23 01:45:27,532] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2233
[2023-09-23 01:45:27,532] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:45:27,532] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:45:27,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.7161 (1.9108)  loss_scale: 32768.0000 (33484.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9239 (5.8519)  time: 0.8018 (0.4843 -- 2.9730)  data: 0.0010 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [13] Total time: 0:02:34 (0.9628 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.7161 (1.9226)  loss_scale: 32768.0000 (33484.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9239 (5.8519)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.2531 (1.2531)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4161 (2.4161 -- 2.4161)  data: 2.2236 (2.2236 -- 2.2236)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9212 (0.9637)  acc1: 55.5556 (67.6768)  acc5: 100.0000 (97.9798)  time: 0.4640 (0.1764 -- 2.4161)  data: 0.2758 (0.0003 -- 2.2236)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8649 (0.9212)  acc1: 66.6667 (69.3122)  acc5: 100.0000 (98.4127)  time: 0.2533 (0.1685 -- 1.0085)  data: 0.0668 (0.0001 -- 0.8048)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8452 (0.9283)  acc1: 66.6667 (69.7095)  acc5: 100.0000 (98.7552)  time: 0.2797 (0.1327 -- 1.0085)  data: 0.1001 (0.0001 -- 0.8048)  max mem: 16413
Val: Total time: 0:00:09 (0.3397 s / it)
* Acc@1 70.332 Acc@5 98.548 loss 0.937
Accuracy of the network on the 482 val images: 70.33%
[2023-09-23 01:45:42,027] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:45:42,029] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:45:42,029] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:45:42,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:45:43,500] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:45:43,501] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.33%
Epoch: [14]  [  0/160]  eta: 0:20:08  lr: 0.000047  min_lr: 0.000012  loss: 2.1278 (2.1278)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1238 (5.1238)  time: 7.5536 (7.5536 -- 7.5536)  data: 6.9863 (6.9863 -- 6.9863)  max mem: 16413
Epoch: [14]  [ 20/160]  eta: 0:02:47  lr: 0.000047  min_lr: 0.000012  loss: 1.8838 (1.8812)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1443 (6.8122)  time: 0.8800 (0.5180 -- 3.8980)  data: 0.3434 (0.0004 -- 3.3764)  max mem: 16413
Epoch: [14]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000012  loss: 1.8225 (1.9001)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4253 (6.7594)  time: 0.9150 (0.5141 -- 4.2949)  data: 0.3508 (0.0003 -- 3.7615)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000012  loss: 1.7948 (1.8918)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1608 (6.4761)  time: 0.8661 (0.5175 -- 3.6236)  data: 0.2177 (0.0002 -- 2.9152)  max mem: 16413
Epoch: [14]  [ 80/160]  eta: 0:01:21  lr: 0.000047  min_lr: 0.000012  loss: 1.9072 (1.8939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3889 (6.2668)  time: 1.0699 (0.5194 -- 4.8983)  data: 0.2591 (0.0004 -- 2.2136)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:59  lr: 0.000047  min_lr: 0.000012  loss: 1.8376 (1.8920)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8593 (6.1770)  time: 0.9370 (0.5191 -- 5.7595)  data: 0.0498 (0.0003 -- 0.9665)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 1.5962 (1.8741)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8268 (6.1054)  time: 0.9767 (0.5149 -- 4.5317)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
[2023-09-23 01:47:45,061] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:47:45,062] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 01:47:45,064] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:47:45,065] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 1.8900 (1.8690)  loss_scale: 32768.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3724 (6.0076)  time: 0.9481 (0.5098 -- 6.5283)  data: 0.0011 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.8375 (1.8668)  loss_scale: 32768.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9352 (6.0635)  time: 0.5887 (0.4953 -- 1.5950)  data: 0.0007 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [14] Total time: 0:02:30 (0.9418 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.8375 (1.8960)  loss_scale: 32768.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9352 (6.0635)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.2395 (1.2395)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3924 (2.3924 -- 2.3924)  data: 2.2109 (2.2109 -- 2.2109)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9814 (0.9596)  acc1: 66.6667 (60.6061)  acc5: 100.0000 (98.9899)  time: 0.4615 (0.1758 -- 2.3924)  data: 0.2713 (0.0003 -- 2.2109)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8650 (0.9120)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (99.4709)  time: 0.2572 (0.1684 -- 0.9508)  data: 0.0700 (0.0001 -- 0.7555)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9339 (0.9374)  acc1: 66.6667 (61.8257)  acc5: 100.0000 (99.1701)  time: 0.2783 (0.1323 -- 0.9508)  data: 0.0998 (0.0001 -- 0.7555)  max mem: 16413
Val: Total time: 0:00:09 (0.3380 s / it)
* Acc@1 66.183 Acc@5 98.755 loss 0.916
Accuracy of the network on the 482 val images: 66.18%
Max accuracy: 70.33%
Epoch: [15]  [  0/160]  eta: 0:17:19  lr: 0.000047  min_lr: 0.000012  loss: 2.1808 (2.1808)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7538 (10.7538)  time: 6.4970 (6.4970 -- 6.4970)  data: 5.9807 (5.9807 -- 5.9807)  max mem: 16413
Epoch: [15]  [ 20/160]  eta: 0:02:50  lr: 0.000047  min_lr: 0.000012  loss: 1.9900 (1.9067)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0283 (6.0514)  time: 0.9560 (0.5248 -- 3.6275)  data: 0.1209 (0.0006 -- 1.2055)  max mem: 16413
Epoch: [15]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000012  loss: 1.9316 (1.9087)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5949 (6.0759)  time: 0.9010 (0.5195 -- 2.5888)  data: 0.1865 (0.0004 -- 2.0681)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:44  lr: 0.000047  min_lr: 0.000012  loss: 2.0460 (1.9421)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2226 (6.2139)  time: 1.0197 (0.5285 -- 3.8786)  data: 0.4448 (0.0004 -- 3.3571)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:20  lr: 0.000047  min_lr: 0.000012  loss: 1.7258 (1.8992)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5962 (6.1701)  time: 0.8739 (0.5189 -- 4.2478)  data: 0.3350 (0.0002 -- 3.7378)  max mem: 16413
[2023-09-23 01:49:58,599] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2490
[2023-09-23 01:49:58,599] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2490
[2023-09-23 01:49:58,599] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:49:58,599] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:49:58,600] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [100/160]  eta: 0:01:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0480 (1.9054)  loss_scale: 16384.0000 (30983.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1228 (6.2477)  time: 1.0451 (0.4983 -- 4.8959)  data: 0.5187 (0.0003 -- 4.3946)  max mem: 16413
Epoch: [15]  [120/160]  eta: 0:00:39  lr: 0.000047  min_lr: 0.000012  loss: 1.8750 (1.8980)  loss_scale: 16384.0000 (28570.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6645 (6.1781)  time: 0.8872 (0.5199 -- 4.1238)  data: 0.3468 (0.0003 -- 3.6143)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:19  lr: 0.000047  min_lr: 0.000012  loss: 2.0219 (1.8988)  loss_scale: 16384.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9538 (6.1797)  time: 0.9339 (0.5210 -- 3.7488)  data: 0.3929 (0.0003 -- 3.2288)  max mem: 16413
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9068 (1.9102)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2352 (6.1578)  time: 0.6884 (0.4937 -- 2.4422)  data: 0.1705 (0.0001 -- 1.9110)  max mem: 16413
Epoch: [15] Total time: 0:02:32 (0.9505 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9068 (1.8608)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2352 (6.1578)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.0825 (1.0825)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4580 (2.4580 -- 2.4580)  data: 2.2813 (2.2813 -- 2.2813)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.9118 (0.9364)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (97.9798)  time: 0.4799 (0.1753 -- 2.4580)  data: 0.2840 (0.0003 -- 2.2813)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8000 (0.8501)  acc1: 66.6667 (67.1958)  acc5: 100.0000 (98.9418)  time: 0.2585 (0.1677 -- 1.0588)  data: 0.0666 (0.0001 -- 0.8359)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8195 (0.8796)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (98.7552)  time: 0.2855 (0.1318 -- 1.0588)  data: 0.1038 (0.0001 -- 0.8359)  max mem: 16413
Val: Total time: 0:00:09 (0.3465 s / it)
* Acc@1 69.710 Acc@5 98.755 loss 0.879
Accuracy of the network on the 482 val images: 69.71%
Max accuracy: 70.33%
Epoch: [16]  [  0/160]  eta: 0:21:39  lr: 0.000047  min_lr: 0.000012  loss: 1.7181 (1.7181)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8461 (7.8461)  time: 8.1198 (8.1198 -- 8.1198)  data: 4.9313 (4.9313 -- 4.9313)  max mem: 16413
Epoch: [16]  [ 20/160]  eta: 0:02:58  lr: 0.000046  min_lr: 0.000012  loss: 1.8497 (1.8866)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9168 (6.9085)  time: 0.9349 (0.5151 -- 5.6330)  data: 0.0222 (0.0007 -- 0.4106)  max mem: 16413
Epoch: [16]  [ 40/160]  eta: 0:02:14  lr: 0.000046  min_lr: 0.000012  loss: 1.8723 (1.8522)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5555 (6.5699)  time: 0.9495 (0.5253 -- 4.6967)  data: 0.0014 (0.0004 -- 0.0030)  max mem: 16413
[2023-09-23 01:52:06,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:52:06,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:52:06,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 01:52:06,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000012  loss: 1.8115 (1.8390)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9557 (6.3976)  time: 0.8154 (0.5260 -- 2.7434)  data: 0.0018 (0.0003 -- 0.0051)  max mem: 16413
Epoch: [16]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9247 (1.8601)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5938 (6.4294)  time: 0.8529 (0.5264 -- 2.8266)  data: 0.1095 (0.0004 -- 0.9720)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:59  lr: 0.000046  min_lr: 0.000012  loss: 1.9036 (1.8568)  loss_scale: 32768.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3110 (6.4396)  time: 1.0819 (0.5250 -- 4.7759)  data: 0.2767 (0.0004 -- 2.3406)  max mem: 16413
Epoch: [16]  [120/160]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000012  loss: 1.9822 (1.8760)  loss_scale: 32768.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3771 (6.3568)  time: 0.8660 (0.5263 -- 4.0550)  data: 0.0015 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.9552 (1.8732)  loss_scale: 32768.0000 (25912.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8648 (6.3287)  time: 1.0031 (0.5062 -- 4.0072)  data: 0.2423 (0.0003 -- 2.9404)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0384 (1.8774)  loss_scale: 32768.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0934 (6.3723)  time: 0.7299 (0.4941 -- 4.2074)  data: 0.2164 (0.0001 -- 3.7013)  max mem: 16413
Epoch: [16] Total time: 0:02:32 (0.9519 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0384 (1.8788)  loss_scale: 32768.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0934 (6.3723)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.2062 (1.2062)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4647 (2.4647 -- 2.4647)  data: 2.2872 (2.2872 -- 2.2872)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8861 (0.9287)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4665 (0.1797 -- 2.4647)  data: 0.2753 (0.0002 -- 2.2872)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6993 (0.8430)  acc1: 66.6667 (68.2540)  acc5: 100.0000 (97.3545)  time: 0.2519 (0.1677 -- 0.9536)  data: 0.0622 (0.0001 -- 0.7362)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8609 (0.8763)  acc1: 66.6667 (66.8050)  acc5: 100.0000 (96.6805)  time: 0.2804 (0.1325 -- 0.9536)  data: 0.0985 (0.0001 -- 0.7362)  max mem: 16413
Val: Total time: 0:00:09 (0.3414 s / it)
* Acc@1 70.954 Acc@5 97.718 loss 0.839
Accuracy of the network on the 482 val images: 70.95%
[2023-09-23 01:53:46,519] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:53:46,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:53:46,521] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:53:46,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:53:48,058] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:53:48,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.95%
Epoch: [17]  [  0/160]  eta: 0:21:19  lr: 0.000046  min_lr: 0.000012  loss: 1.9009 (1.9009)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8552 (4.8552)  time: 7.9986 (7.9986 -- 7.9986)  data: 6.3158 (6.3158 -- 6.3158)  max mem: 16413
Epoch: [17]  [ 20/160]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000012  loss: 1.8313 (1.8315)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9088 (6.2823)  time: 0.9158 (0.5182 -- 3.5942)  data: 0.2290 (0.0004 -- 3.0773)  max mem: 16413
[2023-09-23 01:54:21,384] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:54:21,385] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:54:21,386] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:54:21,386] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:54:26,694] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2754
[2023-09-23 01:54:26,694] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2754
[2023-09-23 01:54:26,695] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:54:26,695] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:54:26,695] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 40/160]  eta: 0:02:12  lr: 0.000046  min_lr: 0.000012  loss: 1.6878 (1.8116)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2405 (5.8229)  time: 0.9447 (0.5068 -- 3.9041)  data: 0.2349 (0.0004 -- 2.4694)  max mem: 16413
Epoch: [17]  [ 60/160]  eta: 0:01:43  lr: 0.000046  min_lr: 0.000012  loss: 1.7364 (1.8045)  loss_scale: 32768.0000 (36528.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4339 (5.9745)  time: 0.9067 (0.5241 -- 5.3293)  data: 0.3435 (0.0003 -- 4.4335)  max mem: 16413
[2023-09-23 01:55:06,041] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2794
[2023-09-23 01:55:06,041] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:55:06,042] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 01:55:06,042] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2794
[2023-09-23 01:55:06,042] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [17]  [ 80/160]  eta: 0:01:23  lr: 0.000046  min_lr: 0.000012  loss: 1.7208 (1.8100)  loss_scale: 32768.0000 (34183.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1341 (6.0028)  time: 1.0389 (0.5234 -- 4.5174)  data: 0.0516 (0.0001 -- 0.6921)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000012  loss: 1.9183 (1.8228)  loss_scale: 16384.0000 (30659.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9431 (5.9330)  time: 0.7253 (0.5297 -- 2.3626)  data: 0.0023 (0.0007 -- 0.0120)  max mem: 16413
Epoch: [17]  [120/160]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000012  loss: 1.8314 (1.8265)  loss_scale: 16384.0000 (28299.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2585 (5.9880)  time: 1.0289 (0.5070 -- 5.5618)  data: 0.0011 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [17]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.5960 (1.8110)  loss_scale: 16384.0000 (26609.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9447 (5.9653)  time: 0.7923 (0.5349 -- 2.5470)  data: 0.0017 (0.0003 -- 0.0060)  max mem: 16413
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7838 (1.8081)  loss_scale: 16384.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8590 (6.0434)  time: 0.7967 (0.4940 -- 4.1362)  data: 0.0009 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [17] Total time: 0:02:30 (0.9405 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7838 (1.8238)  loss_scale: 16384.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8590 (6.0434)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.9140 (0.9140)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5508 (2.5508 -- 2.5508)  data: 2.3558 (2.3558 -- 2.3558)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.8559 (0.9072)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (96.9697)  time: 0.4856 (0.1725 -- 2.5508)  data: 0.2975 (0.0003 -- 2.3558)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7682 (0.8444)  acc1: 66.6667 (67.1958)  acc5: 100.0000 (97.8836)  time: 0.2605 (0.1692 -- 1.1019)  data: 0.0754 (0.0001 -- 0.9116)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7682 (0.8539)  acc1: 77.7778 (66.3900)  acc5: 100.0000 (97.9253)  time: 0.2865 (0.1328 -- 1.1019)  data: 0.1067 (0.0001 -- 0.9116)  max mem: 16413
Val: Total time: 0:00:09 (0.3479 s / it)
* Acc@1 69.502 Acc@5 98.548 loss 0.822
Accuracy of the network on the 482 val images: 69.50%
Max accuracy: 70.95%
Epoch: [18]  [  0/160]  eta: 0:26:03  lr: 0.000046  min_lr: 0.000012  loss: 1.8513 (1.8513)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3636 (4.3636)  time: 9.7698 (9.7698 -- 9.7698)  data: 6.2852 (6.2852 -- 6.2852)  max mem: 16413
Epoch: [18]  [ 20/160]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000012  loss: 1.7883 (1.8124)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3696 (6.9314)  time: 0.8309 (0.5148 -- 3.0496)  data: 0.0013 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:11  lr: 0.000046  min_lr: 0.000012  loss: 1.7978 (1.7896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3084 (5.9922)  time: 0.9220 (0.5274 -- 5.2588)  data: 0.0486 (0.0002 -- 0.5087)  max mem: 16413
[2023-09-23 01:57:16,936] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:57:16,936] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:57:16,937] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 01:57:16,937] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [ 60/160]  eta: 0:01:44  lr: 0.000046  min_lr: 0.000012  loss: 1.8874 (1.8364)  loss_scale: 32768.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9167 (5.9888)  time: 0.9563 (0.5278 -- 2.9682)  data: 0.0633 (0.0003 -- 0.6334)  max mem: 16413
Epoch: [18]  [ 80/160]  eta: 0:01:19  lr: 0.000046  min_lr: 0.000012  loss: 1.7539 (1.8211)  loss_scale: 32768.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0673 (6.0203)  time: 0.8215 (0.5337 -- 3.6648)  data: 0.0018 (0.0005 -- 0.0044)  max mem: 16413
Epoch: [18]  [100/160]  eta: 0:01:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9647 (1.8373)  loss_scale: 32768.0000 (25792.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3054 (6.1501)  time: 1.0782 (0.5239 -- 5.2135)  data: 0.0011 (0.0003 -- 0.0036)  max mem: 16413
[2023-09-23 01:58:25,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=11, lr=[1.1769696168306417e-05, 1.1769696168306417e-05, 1.307744018700713e-05, 1.307744018700713e-05, 1.4530489096674585e-05, 1.4530489096674585e-05, 1.6144987885193987e-05, 1.6144987885193987e-05, 1.7938875427993317e-05, 1.7938875427993317e-05, 1.9932083808881464e-05, 1.9932083808881464e-05, 2.214675978764607e-05, 2.214675978764607e-05, 2.4607510875162297e-05, 2.4607510875162297e-05, 2.734167875018033e-05, 2.734167875018033e-05, 3.037964305575592e-05, 3.037964305575592e-05, 3.3755158950839915e-05, 3.3755158950839915e-05, 3.75057321675999e-05, 3.75057321675999e-05, 4.167303574177767e-05, 4.167303574177767e-05, 4.630337304641963e-05, 4.630337304641963e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 01:58:25,162] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=16.879204428320207, CurrSamplesPerSec=22.563476696901244, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:38  lr: 0.000046  min_lr: 0.000012  loss: 1.9176 (1.8462)  loss_scale: 32768.0000 (26945.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0931 (6.2801)  time: 0.7819 (0.5250 -- 3.9623)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [18]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.9068 (1.8507)  loss_scale: 32768.0000 (27771.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8444 (6.3250)  time: 0.9557 (0.5270 -- 4.4490)  data: 0.0015 (0.0005 -- 0.0041)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8352 (1.8419)  loss_scale: 32768.0000 (28364.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1642 (6.3120)  time: 0.7020 (0.4929 -- 2.7789)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [18] Total time: 0:02:30 (0.9390 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8352 (1.8363)  loss_scale: 32768.0000 (28364.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1642 (6.3120)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.9068 (0.9068)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5610 (2.5610 -- 2.5610)  data: 2.3621 (2.3621 -- 2.3621)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.8047 (0.8607)  acc1: 55.5556 (63.6364)  acc5: 100.0000 (96.9697)  time: 0.4893 (0.1805 -- 2.5610)  data: 0.3013 (0.0001 -- 2.3621)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6594 (0.7704)  acc1: 77.7778 (70.3704)  acc5: 100.0000 (98.4127)  time: 0.2517 (0.1684 -- 1.1514)  data: 0.0669 (0.0001 -- 0.9472)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7067 (0.8111)  acc1: 77.7778 (68.8797)  acc5: 100.0000 (97.9253)  time: 0.2865 (0.1323 -- 1.1514)  data: 0.1080 (0.0001 -- 0.9472)  max mem: 16413
Val: Total time: 0:00:09 (0.3480 s / it)
* Acc@1 71.162 Acc@5 98.548 loss 0.795
Accuracy of the network on the 482 val images: 71.16%
[2023-09-23 01:59:07,735] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 01:59:07,736] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 01:59:07,736] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 01:59:07,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 01:59:09,268] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 01:59:09,269] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.16%
Epoch: [19]  [  0/160]  eta: 0:22:55  lr: 0.000046  min_lr: 0.000012  loss: 1.4781 (1.4781)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5852 (4.5852)  time: 8.5950 (8.5950 -- 8.5950)  data: 8.0770 (8.0770 -- 8.0770)  max mem: 16413
[2023-09-23 01:59:30,467] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:59:30,468] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 01:59:30,508] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:59:30,508] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 01:59:35,851] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3060
[2023-09-23 01:59:35,851] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3060
[2023-09-23 01:59:35,851] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:59:35,851] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 01:59:35,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 20/160]  eta: 0:02:57  lr: 0.000046  min_lr: 0.000012  loss: 1.8878 (1.7805)  loss_scale: 32768.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0026 (5.7305)  time: 0.8991 (0.5155 -- 4.0172)  data: 0.1205 (0.0002 -- 2.3700)  max mem: 16413
[2023-09-23 01:59:52,165] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3077
[2023-09-23 01:59:52,165] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3077
[2023-09-23 01:59:52,165] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:59:52,165] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 01:59:52,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [ 40/160]  eta: 0:02:10  lr: 0.000046  min_lr: 0.000012  loss: 1.6012 (1.7423)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6978 (6.1160)  time: 0.8958 (0.5168 -- 3.9400)  data: 0.1207 (0.0004 -- 1.2063)  max mem: 16413
Epoch: [19]  [ 60/160]  eta: 0:01:46  lr: 0.000046  min_lr: 0.000012  loss: 1.7781 (1.7949)  loss_scale: 16384.0000 (31156.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5699 (5.9250)  time: 1.0126 (0.5209 -- 4.9533)  data: 0.0021 (0.0004 -- 0.0143)  max mem: 16413
Epoch: [19]  [ 80/160]  eta: 0:01:20  lr: 0.000046  min_lr: 0.000012  loss: 1.7626 (1.7885)  loss_scale: 16384.0000 (27508.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9529 (6.0232)  time: 0.8513 (0.5224 -- 3.1774)  data: 0.0016 (0.0003 -- 0.0063)  max mem: 16413
Epoch: [19]  [100/160]  eta: 0:00:59  lr: 0.000046  min_lr: 0.000012  loss: 1.6632 (1.7726)  loss_scale: 16384.0000 (25305.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4215 (6.0967)  time: 0.9450 (0.5331 -- 2.7976)  data: 0.0017 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [19]  [120/160]  eta: 0:00:38  lr: 0.000046  min_lr: 0.000012  loss: 1.8419 (1.7971)  loss_scale: 16384.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7787 (6.2793)  time: 0.7838 (0.5290 -- 2.5850)  data: 0.0024 (0.0002 -- 0.0167)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.9139 (1.8164)  loss_scale: 16384.0000 (22774.9220)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8702 (6.3869)  time: 1.0992 (0.5259 -- 5.0397)  data: 0.0012 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8113 (1.8151)  loss_scale: 16384.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5609 (6.4401)  time: 0.7059 (0.4940 -- 2.4826)  data: 0.0005 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [19] Total time: 0:02:31 (0.9495 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8113 (1.8210)  loss_scale: 16384.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5609 (6.4401)
[2023-09-23 02:01:41,192] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-09-23 02:01:41,193] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-09-23 02:01:41,194] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-09-23 02:01:41,194] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-09-23 02:01:42,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-09-23 02:01:42,220] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:09  loss: 0.9744 (0.9744)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5557 (2.5557 -- 2.5557)  data: 2.3500 (2.3500 -- 2.3500)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.9112 (0.9067)  acc1: 55.5556 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4736 (0.1823 -- 2.5557)  data: 0.2766 (0.0003 -- 2.3500)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7796 (0.8235)  acc1: 66.6667 (65.6085)  acc5: 100.0000 (97.8836)  time: 0.2515 (0.1691 -- 0.8804)  data: 0.0615 (0.0001 -- 0.6863)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7960 (0.8928)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (97.5104)  time: 0.2722 (0.1318 -- 0.8804)  data: 0.0928 (0.0001 -- 0.6863)  max mem: 16413
Val: Total time: 0:00:09 (0.3407 s / it)
* Acc@1 68.257 Acc@5 98.133 loss 0.872
Accuracy of the network on the 482 val images: 68.26%
Max accuracy: 71.16%
Epoch: [20]  [  0/160]  eta: 0:16:23  lr: 0.000046  min_lr: 0.000012  loss: 1.8639 (1.8639)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8999 (4.8999)  time: 6.1452 (6.1452 -- 6.1452)  data: 5.5856 (5.5856 -- 5.5856)  max mem: 16413
[2023-09-23 02:02:03,321] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:02:03,321] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:02:03,322] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:02:03,322] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:02:11,288] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3214
[2023-09-23 02:02:11,288] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3214
[2023-09-23 02:02:11,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:02:11,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:02:11,288] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [ 20/160]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000012  loss: 1.6941 (1.7792)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4778 (5.7701)  time: 1.0072 (0.5086 -- 3.2062)  data: 0.3163 (0.0006 -- 2.6721)  max mem: 16413
Epoch: [20]  [ 40/160]  eta: 0:02:11  lr: 0.000046  min_lr: 0.000012  loss: 1.8067 (1.8153)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6125 (6.0809)  time: 0.9299 (0.5317 -- 4.4422)  data: 0.0381 (0.0004 -- 0.7124)  max mem: 16413
Epoch: [20]  [ 60/160]  eta: 0:01:44  lr: 0.000046  min_lr: 0.000012  loss: 1.7501 (1.8106)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4365 (6.3119)  time: 0.9447 (0.5038 -- 3.5730)  data: 0.3335 (0.0003 -- 3.0430)  max mem: 16413
Epoch: [20]  [ 80/160]  eta: 0:01:19  lr: 0.000046  min_lr: 0.000012  loss: 1.7123 (1.7888)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4433 (6.1348)  time: 0.8275 (0.5127 -- 3.2939)  data: 0.2148 (0.0003 -- 2.7669)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:01:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7692 (1.7861)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7864 (6.1316)  time: 1.0411 (0.5101 -- 4.2819)  data: 0.3788 (0.0003 -- 3.7424)  max mem: 16413
Epoch: [20]  [120/160]  eta: 0:00:38  lr: 0.000046  min_lr: 0.000012  loss: 1.9265 (1.7976)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0391 (6.1851)  time: 0.7996 (0.5350 -- 2.4907)  data: 0.1861 (0.0003 -- 1.9704)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.7515 (1.7882)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4066 (6.1399)  time: 1.0815 (0.5245 -- 4.5551)  data: 0.0019 (0.0004 -- 0.0079)  max mem: 16413
[2023-09-23 02:04:12,192] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:04:12,192] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:04:12,192] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:04:12,193] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8422 (1.7942)  loss_scale: 32768.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4731 (6.1080)  time: 0.6456 (0.4936 -- 2.8972)  data: 0.0006 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [20] Total time: 0:02:31 (0.9449 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8422 (1.8075)  loss_scale: 32768.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4731 (6.1080)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.8280 (0.8280)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4619 (2.4619 -- 2.4619)  data: 2.2715 (2.2715 -- 2.2715)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8280 (0.8636)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (96.9697)  time: 0.4622 (0.1778 -- 2.4619)  data: 0.2742 (0.0002 -- 2.2715)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6447 (0.7401)  acc1: 66.6667 (71.4286)  acc5: 100.0000 (98.4127)  time: 0.2514 (0.1678 -- 0.9408)  data: 0.0626 (0.0001 -- 0.7408)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7068 (0.7827)  acc1: 66.6667 (69.2946)  acc5: 100.0000 (97.9253)  time: 0.2789 (0.1330 -- 0.9408)  data: 0.0967 (0.0001 -- 0.7408)  max mem: 16413
Val: Total time: 0:00:09 (0.3391 s / it)
* Acc@1 72.822 Acc@5 98.340 loss 0.753
Accuracy of the network on the 482 val images: 72.82%
[2023-09-23 02:04:32,634] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:04:32,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:04:32,636] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:04:32,636] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:04:34,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:04:34,016] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 72.82%
Epoch: [21]  [  0/160]  eta: 0:23:05  lr: 0.000046  min_lr: 0.000012  loss: 2.1092 (2.1092)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8854 (5.8854)  time: 8.6619 (8.6619 -- 8.6619)  data: 7.2538 (7.2538 -- 7.2538)  max mem: 16413
Epoch: [21]  [ 20/160]  eta: 0:02:58  lr: 0.000046  min_lr: 0.000012  loss: 1.8217 (1.8990)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8442 (5.9965)  time: 0.9081 (0.5232 -- 4.8997)  data: 0.1938 (0.0003 -- 2.3068)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:21  lr: 0.000046  min_lr: 0.000012  loss: 1.7881 (1.8347)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2292 (6.2625)  time: 1.0710 (0.5075 -- 5.5124)  data: 0.5342 (0.0003 -- 4.9788)  max mem: 16413
Epoch: [21]  [ 60/160]  eta: 0:01:45  lr: 0.000046  min_lr: 0.000012  loss: 1.8282 (1.8337)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0152 (6.2586)  time: 0.7972 (0.5122 -- 3.5818)  data: 0.2549 (0.0003 -- 3.0649)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:22  lr: 0.000046  min_lr: 0.000012  loss: 1.7710 (1.8080)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9711 (6.2891)  time: 0.9491 (0.5352 -- 3.7496)  data: 0.4009 (0.0003 -- 3.1862)  max mem: 16413
Epoch: [21]  [100/160]  eta: 0:00:59  lr: 0.000046  min_lr: 0.000012  loss: 1.8786 (1.8156)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2366 (6.2575)  time: 0.8856 (0.5277 -- 4.1512)  data: 0.3386 (0.0003 -- 3.6170)  max mem: 16413
[2023-09-23 02:06:23,653] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:06:23,653] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:06:23,660] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:06:23,660] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:06:29,373] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3477
[2023-09-23 02:06:29,373] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3477
[2023-09-23 02:06:29,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:06:29,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:06:29,373] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [120/160]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000012  loss: 1.7364 (1.8036)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6013 (6.3274)  time: 0.9606 (0.5245 -- 3.6352)  data: 0.4182 (0.0002 -- 3.1217)  max mem: 16413
Epoch: [21]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.7745 (1.7999)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2612 (6.3889)  time: 0.8225 (0.5155 -- 4.1731)  data: 0.2799 (0.0002 -- 3.6467)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7555 (1.7957)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0332 (6.4434)  time: 0.6746 (0.4953 -- 1.9939)  data: 0.1410 (0.0002 -- 1.4889)  max mem: 16413
Epoch: [21] Total time: 0:02:29 (0.9345 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7555 (1.7893)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0332 (6.4434)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.8082 (0.8082)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4019 (2.4019 -- 2.4019)  data: 2.2159 (2.2159 -- 2.2159)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7568 (0.8025)  acc1: 66.6667 (71.7172)  acc5: 100.0000 (96.9697)  time: 0.4638 (0.1756 -- 2.4019)  data: 0.2752 (0.0003 -- 2.2159)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6512 (0.7308)  acc1: 66.6667 (74.6032)  acc5: 100.0000 (98.4127)  time: 0.2532 (0.1683 -- 0.9990)  data: 0.0670 (0.0001 -- 0.8052)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7272 (0.7683)  acc1: 66.6667 (71.7842)  acc5: 100.0000 (97.9253)  time: 0.2734 (0.1317 -- 0.9990)  data: 0.0950 (0.0001 -- 0.8052)  max mem: 16413
Val: Total time: 0:00:09 (0.3335 s / it)
* Acc@1 74.481 Acc@5 97.718 loss 0.749
Accuracy of the network on the 482 val images: 74.48%
[2023-09-23 02:07:12,903] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:07:12,905] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:07:12,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:07:12,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:07:14,312] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:07:14,312] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.48%
Epoch: [22]  [  0/160]  eta: 0:24:37  lr: 0.000046  min_lr: 0.000012  loss: 1.4896 (1.4896)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5017 (4.5017)  time: 9.2366 (9.2366 -- 9.2366)  data: 8.7156 (8.7156 -- 8.7156)  max mem: 16413
[2023-09-23 02:07:38,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3536
[2023-09-23 02:07:38,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3536
[2023-09-23 02:07:38,566] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:07:38,566] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:07:38,566] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [ 20/160]  eta: 0:02:56  lr: 0.000046  min_lr: 0.000012  loss: 1.7375 (1.7531)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7458 (6.6991)  time: 0.8618 (0.5032 -- 4.1899)  data: 0.3098 (0.0002 -- 3.4352)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000012  loss: 1.8724 (1.7779)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4727 (6.8655)  time: 0.8933 (0.5165 -- 3.3607)  data: 0.3531 (0.0002 -- 2.8211)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000012  loss: 1.9490 (1.8199)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3342 (6.7097)  time: 0.8706 (0.5170 -- 4.2892)  data: 0.1500 (0.0004 -- 1.4543)  max mem: 16413
Epoch: [22]  [ 80/160]  eta: 0:01:20  lr: 0.000046  min_lr: 0.000012  loss: 1.9161 (1.8215)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5653 (6.7042)  time: 0.9700 (0.5273 -- 2.8558)  data: 0.2258 (0.0002 -- 2.3270)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:59  lr: 0.000046  min_lr: 0.000012  loss: 1.8297 (1.8209)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7917 (6.5913)  time: 0.9631 (0.5160 -- 3.8653)  data: 0.2585 (0.0004 -- 2.7271)  max mem: 16413
Epoch: [22]  [120/160]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000012  loss: 1.7946 (1.8145)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1136 (6.5515)  time: 0.8997 (0.5169 -- 4.2000)  data: 0.0783 (0.0002 -- 1.2254)  max mem: 16413
Epoch: [22]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.8530 (1.8096)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6195 (6.5971)  time: 0.9506 (0.5260 -- 3.7563)  data: 0.3623 (0.0005 -- 3.2358)  max mem: 16413
[2023-09-23 02:09:37,404] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:09:37,405] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:09:37,406] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:09:37,406] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8438 (1.8110)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7891 (6.5772)  time: 0.6663 (0.4950 -- 3.4737)  data: 0.1492 (0.0002 -- 2.9505)  max mem: 16413
Epoch: [22] Total time: 0:02:30 (0.9391 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8438 (1.7850)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7891 (6.5772)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.8313 (0.8313)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4410 (2.4410 -- 2.4410)  data: 2.2230 (2.2230 -- 2.2230)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7427 (0.8143)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (96.9697)  time: 0.4660 (0.1780 -- 2.4410)  data: 0.2751 (0.0002 -- 2.2230)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6450 (0.7186)  acc1: 77.7778 (74.0741)  acc5: 100.0000 (98.4127)  time: 0.2547 (0.1696 -- 0.9914)  data: 0.0693 (0.0001 -- 0.7951)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6561 (0.7633)  acc1: 77.7778 (72.1992)  acc5: 100.0000 (98.3402)  time: 0.2800 (0.1340 -- 0.9914)  data: 0.1011 (0.0001 -- 0.7951)  max mem: 16413
Val: Total time: 0:00:09 (0.3401 s / it)
* Acc@1 74.689 Acc@5 98.548 loss 0.741
Accuracy of the network on the 482 val images: 74.69%
[2023-09-23 02:09:54,020] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:09:54,022] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:09:54,023] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:09:54,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:09:55,798] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:09:55,798] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.69%
Epoch: [23]  [  0/160]  eta: 0:22:00  lr: 0.000046  min_lr: 0.000012  loss: 2.2951 (2.2951)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1646 (9.1646)  time: 8.2557 (8.2557 -- 8.2557)  data: 7.7134 (7.7134 -- 7.7134)  max mem: 16413
[2023-09-23 02:10:06,226] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3684
[2023-09-23 02:10:06,226] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3684
[2023-09-23 02:10:06,227] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:10:06,227] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:10:06,227] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [ 20/160]  eta: 0:02:55  lr: 0.000046  min_lr: 0.000012  loss: 1.7193 (1.7562)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4806 (5.9120)  time: 0.9008 (0.5150 -- 4.3537)  data: 0.3514 (0.0002 -- 3.8093)  max mem: 16413
Epoch: [23]  [ 40/160]  eta: 0:02:11  lr: 0.000046  min_lr: 0.000012  loss: 1.7308 (1.7339)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5378 (6.3448)  time: 0.9344 (0.5249 -- 4.1053)  data: 0.3722 (0.0006 -- 3.5660)  max mem: 16413
Epoch: [23]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000012  loss: 1.9159 (1.7890)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5598 (6.4829)  time: 0.8244 (0.5253 -- 2.9778)  data: 0.2386 (0.0006 -- 2.4663)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:20  lr: 0.000046  min_lr: 0.000012  loss: 1.8227 (1.8036)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9557 (6.4631)  time: 0.9940 (0.5230 -- 5.1394)  data: 0.4439 (0.0006 -- 4.6010)  max mem: 16413
Epoch: [23]  [100/160]  eta: 0:00:59  lr: 0.000046  min_lr: 0.000012  loss: 1.6580 (1.7964)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1745 (6.4369)  time: 0.9632 (0.5145 -- 4.6903)  data: 0.4026 (0.0002 -- 4.1755)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000012  loss: 1.7623 (1.7894)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4217 (6.5235)  time: 1.0215 (0.5068 -- 5.1572)  data: 0.4851 (0.0002 -- 4.6413)  max mem: 16413
[2023-09-23 02:12:05,795] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:12:05,795] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:12:05,795] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:12:05,795] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.8371 (1.7973)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2065 (6.4567)  time: 0.7304 (0.5180 -- 2.4892)  data: 0.1929 (0.0003 -- 1.9337)  max mem: 16413
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8085 (1.7945)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4089 (6.3532)  time: 0.7952 (0.4963 -- 4.0103)  data: 0.2733 (0.0002 -- 3.4965)  max mem: 16413
Epoch: [23] Total time: 0:02:31 (0.9440 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8085 (1.7872)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4089 (6.3532)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.8790 (0.8790)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4929 (2.4929 -- 2.4929)  data: 2.2722 (2.2722 -- 2.2722)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.7500 (0.7906)  acc1: 66.6667 (67.6768)  acc5: 100.0000 (97.9798)  time: 0.4837 (0.1712 -- 2.4929)  data: 0.2937 (0.0003 -- 2.2722)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5724 (0.6919)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (98.9418)  time: 0.2566 (0.1679 -- 1.1330)  data: 0.0706 (0.0001 -- 0.9511)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5756 (0.7199)  acc1: 77.7778 (71.7842)  acc5: 100.0000 (99.1701)  time: 0.2808 (0.1321 -- 1.1330)  data: 0.1029 (0.0001 -- 0.9511)  max mem: 16413
Val: Total time: 0:00:09 (0.3427 s / it)
* Acc@1 74.689 Acc@5 98.963 loss 0.713
Accuracy of the network on the 482 val images: 74.69%
Max accuracy: 74.69%
Epoch: [24]  [  0/160]  eta: 0:21:48  lr: 0.000046  min_lr: 0.000012  loss: 1.6861 (1.6861)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6541 (4.6541)  time: 8.1773 (8.1773 -- 8.1773)  data: 6.5676 (6.5676 -- 6.5676)  max mem: 16413
Epoch: [24]  [ 20/160]  eta: 0:02:48  lr: 0.000046  min_lr: 0.000012  loss: 1.7397 (1.7185)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0785 (6.3947)  time: 0.8539 (0.5203 -- 3.3622)  data: 0.2130 (0.0006 -- 1.6842)  max mem: 16413
Epoch: [24]  [ 40/160]  eta: 0:02:12  lr: 0.000046  min_lr: 0.000012  loss: 1.6399 (1.6972)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9466 (6.4283)  time: 0.9993 (0.5183 -- 5.5767)  data: 0.4630 (0.0004 -- 5.0500)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:45  lr: 0.000046  min_lr: 0.000012  loss: 1.7239 (1.7075)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5027 (6.5566)  time: 0.9441 (0.5106 -- 4.7358)  data: 0.4048 (0.0002 -- 4.2220)  max mem: 16413
Epoch: [24]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000012  loss: 1.8381 (1.7205)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8707 (6.4561)  time: 0.7823 (0.5349 -- 2.1437)  data: 0.2327 (0.0003 -- 1.5979)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000012  loss: 1.4990 (1.7044)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7630 (6.5889)  time: 0.9514 (0.5226 -- 3.6055)  data: 0.3683 (0.0002 -- 3.0836)  max mem: 16413
[2023-09-23 02:14:15,472] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:14:15,472] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:14:15,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:14:15,473] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:14:22,648] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3949
[2023-09-23 02:14:22,648] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3949
[2023-09-23 02:14:22,648] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:14:22,648] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:14:22,648] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-23 02:14:25,865] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3951
[2023-09-23 02:14:25,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:14:25,865] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3951
[2023-09-23 02:14:25,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:14:25,866] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [120/160]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000012  loss: 1.7236 (1.7058)  loss_scale: 16384.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5346 (6.6135)  time: 0.9948 (0.5096 -- 2.6798)  data: 0.2161 (0.0002 -- 1.9742)  max mem: 16413
Epoch: [24]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.7896 (1.7236)  loss_scale: 16384.0000 (31141.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1487 (6.6383)  time: 0.9647 (0.5108 -- 4.7884)  data: 0.4026 (0.0003 -- 4.2718)  max mem: 16413
[2023-09-23 02:15:06,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=19, lr=[1.1608898360300739e-05, 1.1608898360300739e-05, 1.2898775955889711e-05, 1.2898775955889711e-05, 1.4331973284321898e-05, 1.4331973284321898e-05, 1.5924414760357666e-05, 1.5924414760357666e-05, 1.7693794178175183e-05, 1.7693794178175183e-05, 1.9659771309083537e-05, 1.9659771309083537e-05, 2.1844190343426154e-05, 2.1844190343426154e-05, 2.4271322603806833e-05, 2.4271322603806833e-05, 2.6968136226452038e-05, 2.6968136226452038e-05, 2.9964595807168928e-05, 2.9964595807168928e-05, 3.329399534129881e-05, 3.329399534129881e-05, 3.6993328156998675e-05, 3.6993328156998675e-05, 4.1103697952220754e-05, 4.1103697952220754e-05, 4.56707755024675e-05, 4.56707755024675e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 02:15:06,131] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=17.226829497908973, CurrSamplesPerSec=24.76263871641926, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7708 (1.7283)  loss_scale: 16384.0000 (29388.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8488 (6.6287)  time: 0.6279 (0.4942 -- 2.7095)  data: 0.1100 (0.0002 -- 2.1786)  max mem: 16413
Epoch: [24] Total time: 0:02:30 (0.9378 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7708 (1.7375)  loss_scale: 16384.0000 (29388.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8488 (6.6287)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.5715 (0.5715)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3552 (2.3552 -- 2.3552)  data: 2.1569 (2.1569 -- 2.1569)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6699 (0.7501)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (96.9697)  time: 0.4644 (0.1788 -- 2.3552)  data: 0.2739 (0.0003 -- 2.1569)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6420 (0.6917)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (98.4127)  time: 0.2551 (0.1681 -- 1.0441)  data: 0.0699 (0.0001 -- 0.8508)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6420 (0.7132)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (98.3402)  time: 0.2820 (0.1320 -- 1.0441)  data: 0.1048 (0.0001 -- 0.8508)  max mem: 16413
Val: Total time: 0:00:09 (0.3384 s / it)
* Acc@1 76.349 Acc@5 98.755 loss 0.708
Accuracy of the network on the 482 val images: 76.35%
[2023-09-23 02:15:15,605] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:15:15,607] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:15:15,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:15:15,607] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:15:16,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:15:16,999] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.35%
Epoch: [25]  [  0/160]  eta: 0:20:57  lr: 0.000046  min_lr: 0.000012  loss: 2.2877 (2.2877)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2001 (8.2001)  time: 7.8579 (7.8579 -- 7.8579)  data: 7.3325 (7.3325 -- 7.3325)  max mem: 16413
Epoch: [25]  [ 20/160]  eta: 0:02:45  lr: 0.000046  min_lr: 0.000012  loss: 1.7526 (1.7711)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6306 (7.5920)  time: 0.8480 (0.5182 -- 4.6221)  data: 0.3038 (0.0007 -- 4.1121)  max mem: 16413
Epoch: [25]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000012  loss: 1.8059 (1.7851)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0032 (7.2008)  time: 0.8996 (0.5267 -- 3.4889)  data: 0.3529 (0.0002 -- 2.9700)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:43  lr: 0.000046  min_lr: 0.000012  loss: 1.9560 (1.8213)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0156 (6.9107)  time: 1.0216 (0.5097 -- 2.6184)  data: 0.2225 (0.0004 -- 1.7261)  max mem: 16413
[2023-09-23 02:16:39,628] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:16:39,628] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:16:39,629] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:16:39,629] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 80/160]  eta: 0:01:21  lr: 0.000046  min_lr: 0.000012  loss: 1.5942 (1.7912)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7287 (6.6489)  time: 0.9691 (0.5185 -- 4.2356)  data: 0.0011 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [25]  [100/160]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000012  loss: 1.6281 (1.7658)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1398 (6.5495)  time: 0.8223 (0.5186 -- 2.7464)  data: 0.0018 (0.0005 -- 0.0046)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:39  lr: 0.000046  min_lr: 0.000012  loss: 1.6714 (1.7549)  loss_scale: 32768.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3712 (6.5022)  time: 1.0082 (0.5162 -- 4.2822)  data: 0.0019 (0.0005 -- 0.0049)  max mem: 16413
Epoch: [25]  [140/160]  eta: 0:00:19  lr: 0.000046  min_lr: 0.000012  loss: 1.6293 (1.7530)  loss_scale: 32768.0000 (23472.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0471 (6.5003)  time: 0.8573 (0.5240 -- 4.1043)  data: 0.0014 (0.0004 -- 0.0039)  max mem: 16413
[2023-09-23 02:17:42,206] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4150
[2023-09-23 02:17:42,206] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4150
[2023-09-23 02:17:42,207] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:17:42,207] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:17:42,207] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.6680 (1.7506)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5294 (6.5185)  time: 0.7269 (0.4865 -- 3.7625)  data: 0.0165 (0.0002 -- 0.3178)  max mem: 16413
Epoch: [25] Total time: 0:02:30 (0.9401 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.6680 (1.7572)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5294 (6.5185)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.6968 (0.6968)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4350 (2.4350 -- 2.4350)  data: 2.2512 (2.2512 -- 2.2512)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.6968 (0.7736)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (97.9798)  time: 0.4792 (0.1793 -- 2.4350)  data: 0.2936 (0.0004 -- 2.2512)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6116 (0.6796)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (98.4127)  time: 0.2562 (0.1683 -- 1.1537)  data: 0.0725 (0.0001 -- 0.9717)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6868 (0.7225)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (98.7552)  time: 0.2842 (0.1331 -- 1.1537)  data: 0.1079 (0.0001 -- 0.9717)  max mem: 16413
Val: Total time: 0:00:09 (0.3427 s / it)
* Acc@1 78.423 Acc@5 98.548 loss 0.710
Accuracy of the network on the 482 val images: 78.42%
[2023-09-23 02:17:56,920] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:17:56,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:17:56,922] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:17:56,922] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:17:58,432] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:17:58,432] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.42%
Epoch: [26]  [  0/160]  eta: 0:21:41  lr: 0.000046  min_lr: 0.000012  loss: 1.1446 (1.1446)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5403 (5.5403)  time: 8.1367 (8.1367 -- 8.1367)  data: 6.3258 (6.3258 -- 6.3258)  max mem: 16413
Epoch: [26]  [ 20/160]  eta: 0:02:56  lr: 0.000046  min_lr: 0.000012  loss: 1.9875 (1.8794)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2282 (7.1496)  time: 0.9194 (0.5174 -- 3.9327)  data: 0.1231 (0.0003 -- 1.0001)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:12  lr: 0.000046  min_lr: 0.000012  loss: 1.7226 (1.8079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1913 (7.0338)  time: 0.9326 (0.5139 -- 3.4197)  data: 0.1482 (0.0004 -- 2.1115)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:43  lr: 0.000046  min_lr: 0.000012  loss: 1.9322 (1.8249)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4172 (6.7929)  time: 0.8869 (0.5234 -- 3.3010)  data: 0.3471 (0.0004 -- 2.7866)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000012  loss: 1.6964 (1.8161)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8018 (6.6372)  time: 0.8535 (0.5271 -- 3.0763)  data: 0.3079 (0.0004 -- 2.5564)  max mem: 16413
Epoch: [26]  [100/160]  eta: 0:00:59  lr: 0.000045  min_lr: 0.000012  loss: 1.7116 (1.7932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7894 (6.5016)  time: 1.0186 (0.5194 -- 4.3320)  data: 0.4734 (0.0004 -- 3.7907)  max mem: 16413
[2023-09-23 02:19:55,700] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:19:55,700] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:19:55,701] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:19:55,701] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000012  loss: 1.8774 (1.7944)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1199 (6.5237)  time: 0.8704 (0.5166 -- 4.3842)  data: 0.2922 (0.0003 -- 3.8762)  max mem: 16413
Epoch: [26]  [140/160]  eta: 0:00:19  lr: 0.000045  min_lr: 0.000012  loss: 1.6533 (1.7824)  loss_scale: 32768.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4992 (6.6415)  time: 1.0474 (0.5088 -- 5.5042)  data: 0.4907 (0.0003 -- 4.9786)  max mem: 16413
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.8830 (1.7880)  loss_scale: 32768.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5176 (6.6350)  time: 0.5816 (0.4927 -- 1.5188)  data: 0.0644 (0.0002 -- 1.0204)  max mem: 16413
Epoch: [26] Total time: 0:02:29 (0.9368 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.8830 (1.7544)  loss_scale: 32768.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5176 (6.6350)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.8746 (0.8746)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4404 (2.4404 -- 2.4404)  data: 2.2531 (2.2531 -- 2.2531)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.6586 (0.7374)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (98.9899)  time: 0.4854 (0.1789 -- 2.4404)  data: 0.2918 (0.0003 -- 2.2531)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5525 (0.6354)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (99.4709)  time: 0.2603 (0.1682 -- 1.1598)  data: 0.0702 (0.0001 -- 0.9507)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5759 (0.6798)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (99.1701)  time: 0.2921 (0.1323 -- 1.1598)  data: 0.1102 (0.0001 -- 0.9507)  max mem: 16413
Val: Total time: 0:00:09 (0.3493 s / it)
* Acc@1 78.631 Acc@5 98.755 loss 0.677
Accuracy of the network on the 482 val images: 78.63%
[2023-09-23 02:20:37,749] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:20:37,751] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:20:37,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:20:37,751] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:20:39,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:20:39,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.63%
Epoch: [27]  [  0/160]  eta: 0:24:56  lr: 0.000045  min_lr: 0.000012  loss: 2.3161 (2.3161)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1400 (10.1400)  time: 9.3520 (9.3520 -- 9.3520)  data: 8.8231 (8.8231 -- 8.8231)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:58  lr: 0.000045  min_lr: 0.000012  loss: 1.6958 (1.7384)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3550 (6.7450)  time: 0.8691 (0.5215 -- 2.8951)  data: 0.3318 (0.0003 -- 2.3847)  max mem: 16413
[2023-09-23 02:21:19,723] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4355
[2023-09-23 02:21:19,723] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4355
[2023-09-23 02:21:19,723] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:21:19,723] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:21:19,723] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [ 40/160]  eta: 0:02:10  lr: 0.000045  min_lr: 0.000012  loss: 1.7645 (1.7369)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8748 (6.4575)  time: 0.8960 (0.5176 -- 2.7926)  data: 0.3239 (0.0002 -- 2.2649)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:41  lr: 0.000045  min_lr: 0.000012  loss: 1.7657 (1.7472)  loss_scale: 16384.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4708 (6.3372)  time: 0.8692 (0.5249 -- 2.7990)  data: 0.2716 (0.0005 -- 1.5119)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:20  lr: 0.000045  min_lr: 0.000012  loss: 1.8156 (1.7652)  loss_scale: 16384.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1478 (6.2952)  time: 0.9503 (0.5190 -- 3.6174)  data: 0.4072 (0.0003 -- 3.0962)  max mem: 16413
Epoch: [27]  [100/160]  eta: 0:01:00  lr: 0.000045  min_lr: 0.000012  loss: 1.9682 (1.7880)  loss_scale: 16384.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6174 (6.2767)  time: 1.0496 (0.5238 -- 4.1509)  data: 0.5033 (0.0004 -- 3.6243)  max mem: 16413
Epoch: [27]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000012  loss: 1.7495 (1.7858)  loss_scale: 16384.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9242 (6.3225)  time: 0.7876 (0.5125 -- 3.5177)  data: 0.2387 (0.0001 -- 3.0000)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:19  lr: 0.000045  min_lr: 0.000012  loss: 1.6312 (1.7659)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3400 (6.3931)  time: 0.9408 (0.5228 -- 4.2936)  data: 0.3971 (0.0003 -- 3.7712)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.8008 (1.7657)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2903 (6.4574)  time: 0.6379 (0.4950 -- 2.9166)  data: 0.1193 (0.0001 -- 2.3687)  max mem: 16413
Epoch: [27] Total time: 0:02:28 (0.9303 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.8008 (1.7587)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2903 (6.4574)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.7277 (0.7277)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4529 (2.4529 -- 2.4529)  data: 2.2578 (2.2578 -- 2.2578)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.6098 (0.7502)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (97.9798)  time: 0.4738 (0.1817 -- 2.4529)  data: 0.2821 (0.0002 -- 2.2578)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5685 (0.6345)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (98.9418)  time: 0.2552 (0.1685 -- 1.0229)  data: 0.0656 (0.0001 -- 0.8253)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5716 (0.6757)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (99.1701)  time: 0.2683 (0.1323 -- 1.0229)  data: 0.0859 (0.0001 -- 0.8253)  max mem: 16413
Val: Total time: 0:00:08 (0.3313 s / it)
* Acc@1 81.535 Acc@5 99.170 loss 0.645
Accuracy of the network on the 482 val images: 81.54%
[2023-09-23 02:23:17,816] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:23:17,817] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:23:17,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:23:17,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:23:19,218] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:23:19,218] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.54%
Epoch: [28]  [  0/160]  eta: 0:23:04  lr: 0.000045  min_lr: 0.000012  loss: 1.8661 (1.8661)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5212 (9.5212)  time: 8.6525 (8.6525 -- 8.6525)  data: 6.1648 (6.1648 -- 6.1648)  max mem: 16413
[2023-09-23 02:23:30,081] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:23:30,081] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:23:30,082] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:23:30,082] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 20/160]  eta: 0:02:47  lr: 0.000045  min_lr: 0.000012  loss: 1.6118 (1.6448)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8269 (6.2842)  time: 0.8232 (0.5220 -- 4.3207)  data: 0.0019 (0.0005 -- 0.0071)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:02:11  lr: 0.000045  min_lr: 0.000012  loss: 1.8068 (1.7184)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0067 (6.2358)  time: 0.9887 (0.5137 -- 3.8838)  data: 0.0067 (0.0003 -- 0.1085)  max mem: 16413
Epoch: [28]  [ 60/160]  eta: 0:01:41  lr: 0.000045  min_lr: 0.000011  loss: 1.6160 (1.7060)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8103 (6.4647)  time: 0.8555 (0.5231 -- 4.2098)  data: 0.0017 (0.0002 -- 0.0053)  max mem: 16413
Epoch: [28]  [ 80/160]  eta: 0:01:21  lr: 0.000045  min_lr: 0.000011  loss: 1.8011 (1.7206)  loss_scale: 32768.0000 (31958.9136)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6517 (6.5450)  time: 1.0098 (0.5242 -- 4.9761)  data: 0.0013 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [28]  [100/160]  eta: 0:00:59  lr: 0.000045  min_lr: 0.000011  loss: 1.9613 (1.7526)  loss_scale: 32768.0000 (32119.1287)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5963 (6.6660)  time: 0.9267 (0.5144 -- 5.2611)  data: 0.0016 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [28]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000011  loss: 1.6653 (1.7365)  loss_scale: 32768.0000 (32226.3802)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3993 (6.7161)  time: 0.8492 (0.5126 -- 3.7406)  data: 0.0012 (0.0003 -- 0.0029)  max mem: 16413
[2023-09-23 02:25:27,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:25:27,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:25:27,430] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:25:27,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:25:28,510] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4614
[2023-09-23 02:25:28,511] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:25:28,511] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4614
[2023-09-23 02:25:28,511] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:25:28,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-23 02:25:31,670] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4617
[2023-09-23 02:25:31,670] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4617
[2023-09-23 02:25:31,671] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:25:31,671] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:25:31,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [140/160]  eta: 0:00:19  lr: 0.000045  min_lr: 0.000011  loss: 1.7875 (1.7409)  loss_scale: 32768.0000 (32303.2057)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6463 (6.7896)  time: 0.8543 (0.5161 -- 3.8557)  data: 0.0017 (0.0005 -- 0.0042)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8313 (1.7528)  loss_scale: 16384.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1563 (6.8185)  time: 0.7579 (0.4931 -- 4.0984)  data: 0.0007 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [28] Total time: 0:02:29 (0.9341 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8313 (1.7497)  loss_scale: 16384.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1563 (6.8185)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.6480 (0.6480)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4285 (2.4285 -- 2.4285)  data: 2.2142 (2.2142 -- 2.2142)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6281 (0.6996)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4692 (0.1753 -- 2.4285)  data: 0.2797 (0.0003 -- 2.2142)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5524 (0.6273)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (98.4127)  time: 0.2574 (0.1686 -- 1.0552)  data: 0.0744 (0.0001 -- 0.8574)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6080 (0.6806)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (98.3402)  time: 0.2880 (0.1330 -- 1.0552)  data: 0.1116 (0.0001 -- 0.8574)  max mem: 16413
Val: Total time: 0:00:09 (0.3446 s / it)
* Acc@1 80.498 Acc@5 98.963 loss 0.637
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 81.54%
Epoch: [29]  [  0/160]  eta: 0:16:40  lr: 0.000045  min_lr: 0.000011  loss: 1.6484 (1.6484)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0880 (6.0880)  time: 6.2533 (6.2533 -- 6.2533)  data: 5.7044 (5.7044 -- 5.7044)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:46  lr: 0.000045  min_lr: 0.000011  loss: 1.7090 (1.7320)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0605 (6.0125)  time: 0.9371 (0.5201 -- 2.4598)  data: 0.1444 (0.0009 -- 1.4480)  max mem: 16413
Epoch: [29]  [ 40/160]  eta: 0:02:05  lr: 0.000045  min_lr: 0.000011  loss: 1.5697 (1.6805)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6946 (6.5494)  time: 0.8891 (0.5256 -- 2.6780)  data: 0.2966 (0.0005 -- 2.1712)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:42  lr: 0.000045  min_lr: 0.000011  loss: 1.9512 (1.7461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7514 (6.5225)  time: 1.0009 (0.5288 -- 3.2387)  data: 0.3597 (0.0010 -- 2.6917)  max mem: 16413
Epoch: [29]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000011  loss: 1.6926 (1.7296)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8541 (6.4682)  time: 0.8515 (0.5275 -- 3.2832)  data: 0.2874 (0.0005 -- 2.7307)  max mem: 16413
Epoch: [29]  [100/160]  eta: 0:00:59  lr: 0.000045  min_lr: 0.000011  loss: 1.7287 (1.7233)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1883 (6.5191)  time: 0.9890 (0.5244 -- 3.7412)  data: 0.4392 (0.0008 -- 3.2261)  max mem: 16413
[2023-09-23 02:27:44,385] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:27:44,385] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:27:44,386] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:27:44,386] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000011  loss: 1.6091 (1.7145)  loss_scale: 32768.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0791 (6.6195)  time: 0.8722 (0.5263 -- 3.8494)  data: 0.3244 (0.0001 -- 3.3322)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.5671 (1.6922)  loss_scale: 32768.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2390 (6.7081)  time: 0.8407 (0.5188 -- 3.0612)  data: 0.2232 (0.0005 -- 2.5430)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.6445 (1.6947)  loss_scale: 32768.0000 (21913.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6931 (6.6412)  time: 0.8516 (0.4954 -- 4.7591)  data: 0.0011 (0.0002 -- 0.0055)  max mem: 16413
Epoch: [29] Total time: 0:02:30 (0.9399 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.6445 (1.6885)  loss_scale: 32768.0000 (21913.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6931 (6.6412)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.5420 (0.5420)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4494 (2.4494 -- 2.4494)  data: 2.2577 (2.2577 -- 2.2577)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.5420 (0.7113)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4748 (0.1796 -- 2.4494)  data: 0.2833 (0.0003 -- 2.2577)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4610 (0.6056)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (98.9418)  time: 0.2574 (0.1681 -- 1.0530)  data: 0.0714 (0.0001 -- 0.8529)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5496 (0.6632)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (99.1701)  time: 0.2853 (0.1338 -- 1.0530)  data: 0.1064 (0.0001 -- 0.8529)  max mem: 16413
Val: Total time: 0:00:09 (0.3444 s / it)
* Acc@1 81.328 Acc@5 99.170 loss 0.630
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.54%
Epoch: [30]  [  0/160]  eta: 0:17:42  lr: 0.000045  min_lr: 0.000011  loss: 1.8012 (1.8012)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3330 (6.3330)  time: 6.6420 (6.6420 -- 6.6420)  data: 6.1165 (6.1165 -- 6.1165)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:02:45  lr: 0.000045  min_lr: 0.000011  loss: 1.7753 (1.7666)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8199 (6.1403)  time: 0.9090 (0.5213 -- 2.6560)  data: 0.2209 (0.0004 -- 2.1489)  max mem: 16413
Epoch: [30]  [ 40/160]  eta: 0:02:11  lr: 0.000045  min_lr: 0.000011  loss: 1.6500 (1.7512)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4908 (6.3497)  time: 0.9975 (0.5187 -- 2.9094)  data: 0.1445 (0.0002 -- 2.3926)  max mem: 16413
Epoch: [30]  [ 60/160]  eta: 0:01:42  lr: 0.000045  min_lr: 0.000011  loss: 1.4835 (1.6731)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7629 (6.3683)  time: 0.9016 (0.5246 -- 3.1045)  data: 0.0012 (0.0005 -- 0.0028)  max mem: 16413
[2023-09-23 02:29:54,811] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:29:54,812] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:29:54,812] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:29:54,812] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:29:55,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4876
[2023-09-23 02:29:55,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4876
[2023-09-23 02:29:55,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:29:55,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:29:55,866] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-23 02:29:57,546] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4879
[2023-09-23 02:29:57,546] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4879
[2023-09-23 02:29:57,546] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:29:57,546] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:29:57,546] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [ 80/160]  eta: 0:01:20  lr: 0.000045  min_lr: 0.000011  loss: 1.5697 (1.6610)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7159 (6.5115)  time: 0.9566 (0.5121 -- 2.8974)  data: 0.0127 (0.0003 -- 0.2284)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:59  lr: 0.000045  min_lr: 0.000011  loss: 1.5536 (1.6482)  loss_scale: 16384.0000 (29848.0792)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4600 (6.5151)  time: 0.8810 (0.5168 -- 3.1765)  data: 0.1628 (0.0002 -- 2.6152)  max mem: 16413
Epoch: [30]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000011  loss: 1.6870 (1.6500)  loss_scale: 16384.0000 (27622.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1221 (6.4649)  time: 0.8292 (0.5280 -- 5.4615)  data: 0.2819 (0.0003 -- 4.9493)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:19  lr: 0.000045  min_lr: 0.000011  loss: 1.7222 (1.6645)  loss_scale: 16384.0000 (26028.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4726 (6.5357)  time: 0.9859 (0.5345 -- 2.8400)  data: 0.2814 (0.0004 -- 2.3163)  max mem: 16413
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8533 (1.6760)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4405 (6.6655)  time: 0.7420 (0.4961 -- 2.7402)  data: 0.1020 (0.0002 -- 1.1060)  max mem: 16413
Epoch: [30] Total time: 0:02:30 (0.9386 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8533 (1.6771)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4405 (6.6655)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.5741 (0.5741)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.5378 (2.5378 -- 2.5378)  data: 2.3234 (2.3234 -- 2.3234)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.5741 (0.6879)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4889 (0.1752 -- 2.5378)  data: 0.2995 (0.0002 -- 2.3234)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5298 (0.5883)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.8836)  time: 0.2530 (0.1684 -- 1.1599)  data: 0.0698 (0.0001 -- 0.9665)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5569 (0.6340)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (97.9253)  time: 0.2857 (0.1323 -- 1.1599)  data: 0.1097 (0.0001 -- 0.9665)  max mem: 16413
Val: Total time: 0:00:09 (0.3474 s / it)
* Acc@1 82.988 Acc@5 98.548 loss 0.593
Accuracy of the network on the 482 val images: 82.99%
[2023-09-23 02:31:17,855] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:31:17,857] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:31:17,857] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:31:17,857] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:31:19,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:31:19,400] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.99%
Epoch: [31]  [  0/160]  eta: 0:24:40  lr: 0.000045  min_lr: 0.000011  loss: 1.3318 (1.3318)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9653 (7.9653)  time: 9.2503 (9.2503 -- 9.2503)  data: 8.7194 (8.7194 -- 8.7194)  max mem: 16413
Epoch: [31]  [ 20/160]  eta: 0:02:44  lr: 0.000045  min_lr: 0.000011  loss: 1.6918 (1.6689)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5727 (7.0800)  time: 0.7695 (0.5198 -- 3.1661)  data: 0.2255 (0.0003 -- 2.6158)  max mem: 16413
[2023-09-23 02:32:00,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=25, lr=[1.1390910354167532e-05, 1.1390910354167532e-05, 1.2656567060186148e-05, 1.2656567060186148e-05, 1.4062852289095717e-05, 1.4062852289095717e-05, 1.5625391432328577e-05, 1.5625391432328577e-05, 1.736154603592064e-05, 1.736154603592064e-05, 1.9290606706578487e-05, 1.9290606706578487e-05, 2.1434007451753873e-05, 2.1434007451753873e-05, 2.381556383528208e-05, 2.381556383528208e-05, 2.6461737594757868e-05, 2.6461737594757868e-05, 2.9401930660842074e-05, 2.9401930660842074e-05, 3.2668811845380086e-05, 3.2668811845380086e-05, 3.629867982820009e-05, 3.629867982820009e-05, 4.033186647577788e-05, 4.033186647577788e-05, 4.481318497308653e-05, 4.481318497308653e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 02:32:00,017] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=17.27345778183212, CurrSamplesPerSec=22.519027574950606, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:02:08  lr: 0.000045  min_lr: 0.000011  loss: 1.8131 (1.7357)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4421 (7.3714)  time: 0.9587 (0.5204 -- 3.2058)  data: 0.3980 (0.0007 -- 2.6844)  max mem: 16413
[2023-09-23 02:32:11,593] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:32:11,594] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:32:11,594] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:32:11,594] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 60/160]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000011  loss: 1.7586 (1.7168)  loss_scale: 32768.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0224 (7.3275)  time: 0.8666 (0.5268 -- 3.2273)  data: 0.3294 (0.0004 -- 2.7082)  max mem: 16413
[2023-09-23 02:32:28,173] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5029
[2023-09-23 02:32:28,173] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:32:28,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 02:32:28,173] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5029
[2023-09-23 02:32:28,173] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [31]  [ 80/160]  eta: 0:01:19  lr: 0.000045  min_lr: 0.000011  loss: 1.5670 (1.6888)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2582 (7.1453)  time: 0.9471 (0.5133 -- 2.5699)  data: 0.4050 (0.0005 -- 2.0351)  max mem: 16413
Epoch: [31]  [100/160]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000011  loss: 1.8003 (1.7014)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7827 (6.9588)  time: 0.9373 (0.5285 -- 2.4811)  data: 0.3898 (0.0009 -- 1.9344)  max mem: 16413
Epoch: [31]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000011  loss: 1.8047 (1.7138)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1887 (6.8974)  time: 0.8671 (0.5355 -- 3.1464)  data: 0.3175 (0.0004 -- 2.6151)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:19  lr: 0.000045  min_lr: 0.000011  loss: 1.6228 (1.7062)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1809 (6.7941)  time: 0.9648 (0.5307 -- 2.3890)  data: 0.4231 (0.0003 -- 1.8450)  max mem: 16413
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.7065 (1.7052)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1197 (6.7225)  time: 0.7567 (0.4948 -- 3.3743)  data: 0.2384 (0.0002 -- 2.8530)  max mem: 16413
Epoch: [31] Total time: 0:02:28 (0.9309 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.7065 (1.7162)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1197 (6.7225)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.6016 (0.6016)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.5165 (2.5165 -- 2.5165)  data: 2.3408 (2.3408 -- 2.3408)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.6016 (0.6924)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4741 (0.1804 -- 2.5165)  data: 0.2845 (0.0003 -- 2.3408)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5002 (0.5941)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (98.4127)  time: 0.2590 (0.1687 -- 0.9817)  data: 0.0715 (0.0001 -- 0.7839)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5439 (0.6561)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (98.3402)  time: 0.2788 (0.1326 -- 0.9817)  data: 0.0989 (0.0001 -- 0.7839)  max mem: 16413
Val: Total time: 0:00:09 (0.3417 s / it)
* Acc@1 82.158 Acc@5 98.963 loss 0.609
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [32]  [  0/160]  eta: 0:23:07  lr: 0.000045  min_lr: 0.000011  loss: 1.5460 (1.5460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1565 (5.1565)  time: 8.6723 (8.6723 -- 8.6723)  data: 7.8594 (7.8594 -- 7.8594)  max mem: 16413
Epoch: [32]  [ 20/160]  eta: 0:02:57  lr: 0.000045  min_lr: 0.000011  loss: 1.6303 (1.6848)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1139 (6.1970)  time: 0.8999 (0.5196 -- 4.1131)  data: 0.0954 (0.0004 -- 0.9886)  max mem: 16413
[2023-09-23 02:34:39,022] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:34:39,023] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:34:39,029] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:34:39,029] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 40/160]  eta: 0:02:10  lr: 0.000045  min_lr: 0.000011  loss: 1.7107 (1.6724)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1469 (5.9473)  time: 0.8935 (0.5174 -- 2.7480)  data: 0.0019 (0.0001 -- 0.0066)  max mem: 16413
[2023-09-23 02:34:45,846] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5165
[2023-09-23 02:34:45,846] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5165
[2023-09-23 02:34:45,846] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:34:45,846] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:34:45,846] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 60/160]  eta: 0:01:44  lr: 0.000045  min_lr: 0.000011  loss: 1.6124 (1.6737)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7501 (6.3707)  time: 0.9594 (0.5173 -- 3.0707)  data: 0.2254 (0.0005 -- 2.5514)  max mem: 16413
Epoch: [32]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000011  loss: 1.7289 (1.6792)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5028 (6.3939)  time: 0.7925 (0.5304 -- 3.0616)  data: 0.0181 (0.0003 -- 0.3353)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000011  loss: 1.7662 (1.6970)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0829 (6.4981)  time: 0.9411 (0.5220 -- 4.5170)  data: 0.0585 (0.0004 -- 0.6942)  max mem: 16413
Epoch: [32]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000011  loss: 1.8097 (1.7141)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1879 (6.6430)  time: 0.8685 (0.5237 -- 2.2605)  data: 0.0072 (0.0005 -- 0.0579)  max mem: 16413
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.6112 (1.7179)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5204 (6.7243)  time: 0.8981 (0.5262 -- 2.8865)  data: 0.0981 (0.0004 -- 1.0084)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8191 (1.7214)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3074 (6.6861)  time: 0.8291 (0.4962 -- 4.4299)  data: 0.0320 (0.0002 -- 0.4342)  max mem: 16413
Epoch: [32] Total time: 0:02:29 (0.9362 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8191 (1.6932)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3074 (6.6861)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5294 (0.5294)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4253 (2.4253 -- 2.4253)  data: 2.2374 (2.2374 -- 2.2374)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6035 (0.7455)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (94.9495)  time: 0.4668 (0.1775 -- 2.4253)  data: 0.2745 (0.0003 -- 2.2374)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5187 (0.6250)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2560 (0.1676 -- 0.9631)  data: 0.0665 (0.0001 -- 0.7767)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5550 (0.6696)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (97.5104)  time: 0.2833 (0.1324 -- 0.9631)  data: 0.1030 (0.0001 -- 0.7767)  max mem: 16413
Val: Total time: 0:00:09 (0.3430 s / it)
* Acc@1 81.950 Acc@5 98.755 loss 0.625
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.99%
Epoch: [33]  [  0/160]  eta: 0:20:35  lr: 0.000045  min_lr: 0.000011  loss: 2.3014 (2.3014)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4068 (7.4068)  time: 7.7193 (7.7193 -- 7.7193)  data: 7.1737 (7.1737 -- 7.1737)  max mem: 16413
[2023-09-23 02:36:55,617] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:36:55,617] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:36:55,617] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:36:55,617] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [ 20/160]  eta: 0:02:39  lr: 0.000045  min_lr: 0.000011  loss: 1.7533 (1.7318)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8081 (6.2135)  time: 0.8129 (0.5331 -- 3.7063)  data: 0.2641 (0.0004 -- 3.2069)  max mem: 16413
Epoch: [33]  [ 40/160]  eta: 0:02:03  lr: 0.000044  min_lr: 0.000011  loss: 1.6109 (1.6694)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9652 (7.0073)  time: 0.9121 (0.5287 -- 3.9267)  data: 0.3721 (0.0001 -- 3.4058)  max mem: 16413
Epoch: [33]  [ 60/160]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000011  loss: 1.7889 (1.7122)  loss_scale: 32768.0000 (29007.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9054 (6.7617)  time: 0.9934 (0.5252 -- 3.5582)  data: 0.3905 (0.0007 -- 3.0340)  max mem: 16413
Epoch: [33]  [ 80/160]  eta: 0:01:19  lr: 0.000044  min_lr: 0.000011  loss: 1.6637 (1.7028)  loss_scale: 32768.0000 (29936.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1438 (6.6280)  time: 0.9210 (0.5316 -- 3.7147)  data: 0.1330 (0.0005 -- 1.5516)  max mem: 16413
Epoch: [33]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 1.5033 (1.6749)  loss_scale: 32768.0000 (30496.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4660 (6.6014)  time: 0.8415 (0.5293 -- 3.3396)  data: 0.0938 (0.0005 -- 0.7589)  max mem: 16413
Epoch: [33]  [120/160]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000011  loss: 1.6923 (1.6830)  loss_scale: 32768.0000 (30872.3306)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1547 (6.5703)  time: 1.0171 (0.5266 -- 3.7499)  data: 0.0887 (0.0001 -- 0.7773)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:19  lr: 0.000044  min_lr: 0.000011  loss: 1.7219 (1.6724)  loss_scale: 32768.0000 (31141.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0089 (6.5805)  time: 0.8846 (0.5231 -- 4.2554)  data: 0.0012 (0.0003 -- 0.0041)  max mem: 16413
[2023-09-23 02:38:55,161] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:38:55,161] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:38:55,161] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:38:55,162] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 02:38:57,407] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5426
[2023-09-23 02:38:57,407] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:38:57,407] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5426
[2023-09-23 02:38:57,448] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 02:38:57,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6146 (1.6640)  loss_scale: 32768.0000 (32153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1555 (6.5645)  time: 0.7148 (0.4948 -- 2.6215)  data: 0.0007 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [33] Total time: 0:02:29 (0.9322 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6146 (1.6522)  loss_scale: 32768.0000 (32153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1555 (6.5645)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3452 (0.3452)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3800 (2.3800 -- 2.3800)  data: 2.1691 (2.1691 -- 2.1691)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.6021 (0.6902)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4746 (0.1786 -- 2.3800)  data: 0.2820 (0.0003 -- 2.1691)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4780 (0.5914)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (98.9418)  time: 0.2607 (0.1686 -- 1.1219)  data: 0.0739 (0.0001 -- 0.9157)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5264 (0.6269)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (99.1701)  time: 0.2894 (0.1325 -- 1.1219)  data: 0.1096 (0.0001 -- 0.9157)  max mem: 16413
Val: Total time: 0:00:09 (0.3443 s / it)
* Acc@1 82.573 Acc@5 98.963 loss 0.574
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [34]  [  0/160]  eta: 0:18:52  lr: 0.000044  min_lr: 0.000011  loss: 1.7899 (1.7899)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0005 (6.0005)  time: 7.0787 (7.0787 -- 7.0787)  data: 5.1699 (5.1699 -- 5.1699)  max mem: 16413
Epoch: [34]  [ 20/160]  eta: 0:02:40  lr: 0.000044  min_lr: 0.000011  loss: 1.7195 (1.6407)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6151 (5.9938)  time: 0.8500 (0.5179 -- 1.8151)  data: 0.2321 (0.0005 -- 1.2941)  max mem: 16413
[2023-09-23 02:39:56,803] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5475
[2023-09-23 02:39:56,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:39:56,803] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5475
[2023-09-23 02:39:56,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:39:56,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 40/160]  eta: 0:02:08  lr: 0.000044  min_lr: 0.000011  loss: 1.5682 (1.6289)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2239 (6.3709)  time: 0.9957 (0.5089 -- 4.7483)  data: 0.0598 (0.0004 -- 1.1752)  max mem: 16413
Epoch: [34]  [ 60/160]  eta: 0:01:43  lr: 0.000044  min_lr: 0.000011  loss: 1.7031 (1.6475)  loss_scale: 16384.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0431 (6.4643)  time: 0.9582 (0.5193 -- 4.3666)  data: 0.0014 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [34]  [ 80/160]  eta: 0:01:19  lr: 0.000044  min_lr: 0.000011  loss: 1.6188 (1.6608)  loss_scale: 16384.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7529 (6.4513)  time: 0.8855 (0.5195 -- 3.5275)  data: 0.0015 (0.0006 -- 0.0033)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:58  lr: 0.000044  min_lr: 0.000011  loss: 1.6784 (1.6522)  loss_scale: 16384.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3437 (6.5168)  time: 0.8557 (0.5189 -- 3.8283)  data: 0.0084 (0.0005 -- 0.1384)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000011  loss: 1.6651 (1.6473)  loss_scale: 16384.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7388 (6.4983)  time: 0.8892 (0.5152 -- 2.2094)  data: 0.1925 (0.0002 -- 1.6925)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:19  lr: 0.000044  min_lr: 0.000011  loss: 1.8004 (1.6612)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8291 (6.5098)  time: 1.0111 (0.5296 -- 4.3241)  data: 0.2780 (0.0003 -- 3.7882)  max mem: 16413
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.7370 (1.6732)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3649 (6.5613)  time: 0.7141 (0.4920 -- 1.9648)  data: 0.1967 (0.0002 -- 1.4506)  max mem: 16413
Epoch: [34] Total time: 0:02:29 (0.9361 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.7370 (1.6693)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3649 (6.5613)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.5625 (0.5625)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.5277 (2.5277 -- 2.5277)  data: 2.3332 (2.3332 -- 2.3332)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.5181 (0.5837)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4848 (0.1747 -- 2.5277)  data: 0.2941 (0.0003 -- 2.3332)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4394 (0.5084)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (99.4709)  time: 0.2567 (0.1688 -- 1.1067)  data: 0.0715 (0.0001 -- 0.8950)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4848 (0.5447)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (99.1701)  time: 0.2840 (0.1320 -- 1.1067)  data: 0.1054 (0.0001 -- 0.8950)  max mem: 16413
Val: Total time: 0:00:09 (0.3451 s / it)
* Acc@1 85.062 Acc@5 99.378 loss 0.509
Accuracy of the network on the 482 val images: 85.06%
[2023-09-23 02:41:55,272] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:41:55,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:41:55,274] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:41:55,274] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:41:56,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:41:56,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.06%
Epoch: [35]  [  0/160]  eta: 0:19:23  lr: 0.000044  min_lr: 0.000011  loss: 1.1835 (1.1835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2321 (6.2321)  time: 7.2704 (7.2704 -- 7.2704)  data: 6.0809 (6.0809 -- 6.0809)  max mem: 16413
[2023-09-23 02:42:06,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:42:06,775] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:42:06,775] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:42:06,776] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [ 20/160]  eta: 0:02:52  lr: 0.000044  min_lr: 0.000011  loss: 1.6283 (1.5726)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4956 (6.6267)  time: 0.9290 (0.5243 -- 2.9199)  data: 0.2840 (0.0008 -- 2.3832)  max mem: 16413
[2023-09-23 02:42:37,363] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5636
[2023-09-23 02:42:37,363] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5636
[2023-09-23 02:42:37,363] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:42:37,363] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:42:37,363] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 02:42:39,488] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5640
[2023-09-23 02:42:39,488] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5640
[2023-09-23 02:42:39,488] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 02:42:39,488] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 02:42:39,488] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [35]  [ 40/160]  eta: 0:02:05  lr: 0.000044  min_lr: 0.000011  loss: 1.7402 (1.6703)  loss_scale: 32768.0000 (28971.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9236 (7.0878)  time: 0.8445 (0.5045 -- 2.9360)  data: 0.2994 (0.0003 -- 2.4143)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:42  lr: 0.000044  min_lr: 0.000011  loss: 1.6585 (1.6748)  loss_scale: 8192.0000 (22158.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9912 (6.8217)  time: 0.9763 (0.5098 -- 3.8829)  data: 0.4155 (0.0004 -- 3.3605)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:18  lr: 0.000044  min_lr: 0.000011  loss: 1.5172 (1.6495)  loss_scale: 8192.0000 (18710.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1719 (6.8687)  time: 0.8417 (0.5292 -- 2.7626)  data: 0.2372 (0.0004 -- 2.2476)  max mem: 16413
Epoch: [35]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 1.7909 (1.6717)  loss_scale: 8192.0000 (16627.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4119 (6.8456)  time: 0.9238 (0.5312 -- 2.3332)  data: 0.2854 (0.0005 -- 1.2916)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:39  lr: 0.000044  min_lr: 0.000011  loss: 1.8366 (1.6807)  loss_scale: 8192.0000 (15233.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7393 (6.9492)  time: 1.0352 (0.5194 -- 4.3755)  data: 0.4448 (0.0008 -- 3.8522)  max mem: 16413
Epoch: [35]  [140/160]  eta: 0:00:19  lr: 0.000044  min_lr: 0.000011  loss: 1.6275 (1.6712)  loss_scale: 8192.0000 (14234.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5465 (6.9449)  time: 0.8043 (0.5293 -- 2.4855)  data: 0.2091 (0.0003 -- 1.9484)  max mem: 16413
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.7308 (1.6770)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9140 (7.2247)  time: 0.7921 (0.4929 -- 3.1878)  data: 0.1389 (0.0001 -- 1.4627)  max mem: 16413
Epoch: [35] Total time: 0:02:29 (0.9361 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.7308 (1.6526)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9140 (7.2247)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6618 (0.6618)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4717 (2.4717 -- 2.4717)  data: 2.2576 (2.2576 -- 2.2576)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.6261 (0.7322)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (97.9798)  time: 0.4966 (0.1779 -- 2.4717)  data: 0.3015 (0.0003 -- 2.2576)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4859 (0.6097)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (98.9418)  time: 0.2629 (0.1703 -- 1.2645)  data: 0.0731 (0.0001 -- 1.0536)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5153 (0.6598)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (98.7552)  time: 0.2933 (0.1323 -- 1.2645)  data: 0.1103 (0.0001 -- 1.0536)  max mem: 16413
Val: Total time: 0:00:09 (0.3506 s / it)
* Acc@1 82.780 Acc@5 99.170 loss 0.595
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 85.06%
Epoch: [36]  [  0/160]  eta: 0:24:16  lr: 0.000044  min_lr: 0.000011  loss: 1.9538 (1.9538)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6615 (7.6615)  time: 9.1018 (9.1018 -- 9.1018)  data: 5.3918 (5.3918 -- 5.3918)  max mem: 16413
[2023-09-23 02:44:54,099] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:44:54,100] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 02:44:54,100] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:44:54,101] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [36]  [ 20/160]  eta: 0:03:06  lr: 0.000044  min_lr: 0.000011  loss: 1.5219 (1.5908)  loss_scale: 16384.0000 (12873.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6147 (6.7086)  time: 0.9458 (0.5230 -- 4.6782)  data: 0.0413 (0.0002 -- 0.7996)  max mem: 16413
[2023-09-23 02:45:05,078] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5782
[2023-09-23 02:45:05,078] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 02:45:05,078] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5782
[2023-09-23 02:45:05,078] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 02:45:05,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [36]  [ 40/160]  eta: 0:02:15  lr: 0.000044  min_lr: 0.000011  loss: 1.6444 (1.6040)  loss_scale: 8192.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2274 (6.8966)  time: 0.9109 (0.5265 -- 3.3785)  data: 0.2472 (0.0001 -- 2.8583)  max mem: 16413
Epoch: [36]  [ 60/160]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000011  loss: 1.5496 (1.5657)  loss_scale: 8192.0000 (9937.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1914 (6.9865)  time: 0.7898 (0.5172 -- 3.0890)  data: 0.2474 (0.0003 -- 2.5761)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:19  lr: 0.000044  min_lr: 0.000011  loss: 1.7511 (1.6229)  loss_scale: 8192.0000 (9506.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3694 (7.1439)  time: 0.9111 (0.5238 -- 3.6991)  data: 0.1856 (0.0002 -- 2.6892)  max mem: 16413
Epoch: [36]  [100/160]  eta: 0:00:58  lr: 0.000044  min_lr: 0.000011  loss: 1.7034 (1.6453)  loss_scale: 8192.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1739 (7.2018)  time: 0.9072 (0.5078 -- 2.6830)  data: 0.0501 (0.0003 -- 0.9313)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000011  loss: 1.7594 (1.6590)  loss_scale: 8192.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7276 (7.1556)  time: 0.9031 (0.5281 -- 2.2478)  data: 0.1694 (0.0003 -- 1.7284)  max mem: 16413
Epoch: [36]  [140/160]  eta: 0:00:19  lr: 0.000044  min_lr: 0.000011  loss: 1.4978 (1.6544)  loss_scale: 8192.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4784 (7.1929)  time: 0.9485 (0.5226 -- 3.4349)  data: 0.3951 (0.0003 -- 2.8924)  max mem: 16413
[2023-09-23 02:47:00,318] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:47:00,318] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 02:47:00,319] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:47:00,319] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8190 (1.6667)  loss_scale: 8192.0000 (9318.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7863 (7.1690)  time: 0.7117 (0.4949 -- 2.9656)  data: 0.1737 (0.0002 -- 2.4501)  max mem: 16413
Epoch: [36] Total time: 0:02:29 (0.9322 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8190 (1.6516)  loss_scale: 8192.0000 (9318.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7863 (7.1690)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3923 (0.3923)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4455 (2.4455 -- 2.4455)  data: 2.2245 (2.2245 -- 2.2245)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4734 (0.6321)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4688 (0.1803 -- 2.4455)  data: 0.2782 (0.0002 -- 2.2245)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4722 (0.5338)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.9418)  time: 0.2496 (0.1684 -- 1.0266)  data: 0.0655 (0.0001 -- 0.8316)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5220 (0.5876)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (98.7552)  time: 0.2761 (0.1326 -- 1.0266)  data: 0.0994 (0.0001 -- 0.8316)  max mem: 16413
Val: Total time: 0:00:09 (0.3370 s / it)
* Acc@1 85.270 Acc@5 98.963 loss 0.572
Accuracy of the network on the 482 val images: 85.27%
[2023-09-23 02:47:14,655] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:47:14,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:47:14,657] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:47:14,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:47:16,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:47:16,146] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.27%
Epoch: [37]  [  0/160]  eta: 0:18:09  lr: 0.000044  min_lr: 0.000011  loss: 1.8801 (1.8801)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5449 (10.5449)  time: 6.8102 (6.8102 -- 6.8102)  data: 6.2913 (6.2913 -- 6.2913)  max mem: 16413
[2023-09-23 02:47:35,570] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5935
[2023-09-23 02:47:35,570] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 02:47:35,570] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5935
[2023-09-23 02:47:35,571] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-23 02:47:35,571] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [37]  [ 20/160]  eta: 0:02:47  lr: 0.000044  min_lr: 0.000011  loss: 1.6973 (1.6798)  loss_scale: 16384.0000 (14043.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9450 (7.2396)  time: 0.9134 (0.5216 -- 3.4704)  data: 0.1943 (0.0006 -- 2.4999)  max mem: 16413
Epoch: [37]  [ 40/160]  eta: 0:02:07  lr: 0.000044  min_lr: 0.000011  loss: 1.6227 (1.6699)  loss_scale: 8192.0000 (11189.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1102 (6.8595)  time: 0.9220 (0.5117 -- 3.3600)  data: 0.1650 (0.0002 -- 1.3705)  max mem: 16413
Epoch: [37]  [ 60/160]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000011  loss: 1.6666 (1.6963)  loss_scale: 8192.0000 (10206.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1051 (7.2202)  time: 0.9225 (0.5350 -- 2.6408)  data: 0.1289 (0.0003 -- 2.1339)  max mem: 16413
[2023-09-23 02:48:37,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=33, lr=[1.1117940440317657e-05, 1.1117940440317657e-05, 1.2353267155908508e-05, 1.2353267155908508e-05, 1.3725852395453895e-05, 1.3725852395453895e-05, 1.5250947106059886e-05, 1.5250947106059886e-05, 1.6945496784510984e-05, 1.6945496784510984e-05, 1.882832976056776e-05, 1.882832976056776e-05, 2.092036640063084e-05, 2.092036640063084e-05, 2.324485155625649e-05, 2.324485155625649e-05, 2.5827612840284987e-05, 2.5827612840284987e-05, 2.869734760031665e-05, 2.869734760031665e-05, 3.1885941778129613e-05, 3.1885941778129613e-05, 3.542882419792179e-05, 3.542882419792179e-05, 3.93653602199131e-05, 3.93653602199131e-05, 4.373928913323678e-05, 4.373928913323678e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 02:48:37,730] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=17.251812270245377, CurrSamplesPerSec=22.391664683232605, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:21  lr: 0.000044  min_lr: 0.000011  loss: 1.5611 (1.6775)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8883 (7.0979)  time: 1.0083 (0.5283 -- 3.8293)  data: 0.0016 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [37]  [100/160]  eta: 0:00:59  lr: 0.000044  min_lr: 0.000011  loss: 1.7243 (1.6737)  loss_scale: 8192.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2990 (7.0561)  time: 0.8659 (0.5348 -- 3.3685)  data: 0.0022 (0.0006 -- 0.0130)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000011  loss: 1.6421 (1.6677)  loss_scale: 8192.0000 (9207.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3378 (7.2340)  time: 0.8451 (0.5330 -- 2.6336)  data: 0.0657 (0.0004 -- 1.2881)  max mem: 16413
Epoch: [37]  [140/160]  eta: 0:00:19  lr: 0.000044  min_lr: 0.000011  loss: 1.5444 (1.6591)  loss_scale: 8192.0000 (9063.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2189 (7.2070)  time: 0.9769 (0.5287 -- 4.1784)  data: 0.0458 (0.0004 -- 0.8862)  max mem: 16413
[2023-09-23 02:49:35,956] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:49:35,956] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 02:49:35,956] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:49:35,998] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6056 (1.6564)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6646 (7.1484)  time: 0.7194 (0.4932 -- 1.8253)  data: 0.0720 (0.0002 -- 0.9729)  max mem: 16413
Epoch: [37] Total time: 0:02:29 (0.9359 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6056 (1.6608)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6646 (7.1484)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6065 (0.6065)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4524 (2.4524 -- 2.4524)  data: 2.2757 (2.2757 -- 2.2757)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5750 (0.6693)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (96.9697)  time: 0.4697 (0.1744 -- 2.4524)  data: 0.2791 (0.0002 -- 2.2757)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3991 (0.5500)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.8836)  time: 0.2514 (0.1697 -- 1.0093)  data: 0.0620 (0.0001 -- 0.7893)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4227 (0.5984)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2772 (0.1325 -- 1.0093)  data: 0.0944 (0.0001 -- 0.7893)  max mem: 16413
Val: Total time: 0:00:09 (0.3379 s / it)
* Acc@1 85.477 Acc@5 98.340 loss 0.559
Accuracy of the network on the 482 val images: 85.48%
[2023-09-23 02:49:55,578] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:49:55,580] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:49:55,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:49:55,580] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:49:56,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:49:56,977] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.48%
Epoch: [38]  [  0/160]  eta: 0:26:33  lr: 0.000044  min_lr: 0.000011  loss: 1.6831 (1.6831)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3595 (4.3595)  time: 9.9584 (9.9584 -- 9.9584)  data: 7.3214 (7.3214 -- 7.3214)  max mem: 16413
Epoch: [38]  [ 20/160]  eta: 0:03:01  lr: 0.000044  min_lr: 0.000011  loss: 1.6513 (1.6861)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3525 (6.1540)  time: 0.8641 (0.5219 -- 3.6034)  data: 0.1026 (0.0004 -- 1.4580)  max mem: 16413
Epoch: [38]  [ 40/160]  eta: 0:02:12  lr: 0.000044  min_lr: 0.000011  loss: 1.7035 (1.6868)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9681 (6.7562)  time: 0.9018 (0.5108 -- 3.2754)  data: 0.2562 (0.0004 -- 2.7138)  max mem: 16413
Epoch: [38]  [ 60/160]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000011  loss: 1.6045 (1.6552)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3084 (6.8142)  time: 0.8316 (0.5076 -- 3.2863)  data: 0.2239 (0.0005 -- 2.7545)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:19  lr: 0.000044  min_lr: 0.000011  loss: 1.6012 (1.6476)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4999 (7.0466)  time: 0.9221 (0.5248 -- 3.3513)  data: 0.2837 (0.0003 -- 2.8113)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:58  lr: 0.000044  min_lr: 0.000011  loss: 1.6835 (1.6585)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5276 (6.9839)  time: 0.8699 (0.5276 -- 3.5865)  data: 0.2822 (0.0003 -- 3.0672)  max mem: 16413
[2023-09-23 02:51:49,065] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:51:49,065] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:51:49,065] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:51:49,065] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [38]  [120/160]  eta: 0:00:39  lr: 0.000043  min_lr: 0.000011  loss: 1.7820 (1.6710)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6000 (7.0566)  time: 1.1086 (0.5245 -- 5.3459)  data: 0.5612 (0.0006 -- 4.8227)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:19  lr: 0.000043  min_lr: 0.000011  loss: 1.6443 (1.6571)  loss_scale: 32768.0000 (19753.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3687 (7.1555)  time: 0.7678 (0.5184 -- 2.7645)  data: 0.2204 (0.0003 -- 2.2339)  max mem: 16413
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.5976 (1.6550)  loss_scale: 32768.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3619 (7.1725)  time: 0.7401 (0.4933 -- 2.8256)  data: 0.2219 (0.0002 -- 2.3355)  max mem: 16413
Epoch: [38] Total time: 0:02:29 (0.9349 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.5976 (1.6272)  loss_scale: 32768.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3619 (7.1725)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2815 (0.2815)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5114 (2.5114 -- 2.5114)  data: 2.3133 (2.3133 -- 2.3133)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4065 (0.6307)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4990 (0.1783 -- 2.5114)  data: 0.2996 (0.0004 -- 2.3133)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3831 (0.5166)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.9418)  time: 0.2592 (0.1696 -- 1.2206)  data: 0.0638 (0.0001 -- 0.9719)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4065 (0.5693)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (99.1701)  time: 0.2789 (0.1323 -- 1.2206)  data: 0.0918 (0.0001 -- 0.9719)  max mem: 16413
Val: Total time: 0:00:09 (0.3426 s / it)
* Acc@1 84.440 Acc@5 99.378 loss 0.535
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.48%
Epoch: [39]  [  0/160]  eta: 0:21:06  lr: 0.000043  min_lr: 0.000011  loss: 1.8169 (1.8169)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5052 (5.5052)  time: 7.9176 (7.9176 -- 7.9176)  data: 7.3786 (7.3786 -- 7.3786)  max mem: 16413
[2023-09-23 02:52:47,684] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6245
[2023-09-23 02:52:47,684] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6245
[2023-09-23 02:52:47,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:52:47,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:52:47,685] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [ 20/160]  eta: 0:02:54  lr: 0.000043  min_lr: 0.000011  loss: 1.6433 (1.7045)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5285 (6.7317)  time: 0.9144 (0.5202 -- 3.1998)  data: 0.3678 (0.0005 -- 2.6824)  max mem: 16413
Epoch: [39]  [ 40/160]  eta: 0:02:10  lr: 0.000043  min_lr: 0.000011  loss: 1.5928 (1.6430)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4967 (6.8207)  time: 0.9200 (0.5154 -- 3.1492)  data: 0.3267 (0.0004 -- 2.5351)  max mem: 16413
Epoch: [39]  [ 60/160]  eta: 0:01:42  lr: 0.000043  min_lr: 0.000011  loss: 1.6388 (1.6435)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7823 (6.7439)  time: 0.9004 (0.5185 -- 3.2285)  data: 0.1429 (0.0004 -- 2.3124)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:20  lr: 0.000043  min_lr: 0.000011  loss: 1.7368 (1.6642)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2029 (6.8150)  time: 0.9238 (0.5221 -- 3.3204)  data: 0.0358 (0.0003 -- 0.4491)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:59  lr: 0.000043  min_lr: 0.000011  loss: 1.5010 (1.6550)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6975 (7.0337)  time: 0.9592 (0.5215 -- 3.4058)  data: 0.0011 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:39  lr: 0.000043  min_lr: 0.000011  loss: 1.8878 (1.6719)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9136 (7.1258)  time: 0.9881 (0.5204 -- 3.0360)  data: 0.0014 (0.0002 -- 0.0055)  max mem: 16413
[2023-09-23 02:54:45,764] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:54:45,764] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:54:45,765] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:54:45,766] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [140/160]  eta: 0:00:19  lr: 0.000043  min_lr: 0.000011  loss: 1.7093 (1.6739)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1511 (7.0317)  time: 0.7969 (0.5193 -- 2.5330)  data: 0.0016 (0.0005 -- 0.0031)  max mem: 16413
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.6476 (1.6700)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7420 (7.0235)  time: 0.7640 (0.4941 -- 2.8046)  data: 0.0080 (0.0002 -- 0.1434)  max mem: 16413
Epoch: [39] Total time: 0:02:30 (0.9421 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.6476 (1.6577)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7420 (7.0235)
[2023-09-23 02:55:06,558] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-09-23 02:55:06,560] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-09-23 02:55:06,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-09-23 02:55:06,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-09-23 02:55:07,441] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-09-23 02:55:07,442] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.9032 (0.9032)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.5364 (2.5364 -- 2.5364)  data: 2.3458 (2.3458 -- 2.3458)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.5705 (0.7144)  acc1: 77.7778 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4876 (0.1753 -- 2.5364)  data: 0.2994 (0.0003 -- 2.3458)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4220 (0.5509)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2633 (0.1707 -- 1.1469)  data: 0.0729 (0.0001 -- 0.9425)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4848 (0.5986)  acc1: 85.7143 (83.4025)  acc5: 100.0000 (98.3402)  time: 0.2884 (0.1329 -- 1.1469)  data: 0.1055 (0.0001 -- 0.9425)  max mem: 16413
Val: Total time: 0:00:09 (0.3496 s / it)
* Acc@1 84.647 Acc@5 98.340 loss 0.579
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.48%
Epoch: [40]  [  0/160]  eta: 0:21:26  lr: 0.000043  min_lr: 0.000011  loss: 1.0385 (1.0385)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9729 (7.9729)  time: 8.0392 (8.0392 -- 8.0392)  data: 7.4181 (7.4181 -- 7.4181)  max mem: 16413
Epoch: [40]  [ 20/160]  eta: 0:02:46  lr: 0.000043  min_lr: 0.000011  loss: 1.6996 (1.6681)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0708 (6.5000)  time: 0.8482 (0.5201 -- 3.1821)  data: 0.3185 (0.0005 -- 2.6409)  max mem: 16413
[2023-09-23 02:55:46,435] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6426
[2023-09-23 02:55:46,435] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6426
[2023-09-23 02:55:46,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:55:46,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:55:46,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [40]  [ 40/160]  eta: 0:02:05  lr: 0.000043  min_lr: 0.000011  loss: 1.6246 (1.6316)  loss_scale: 16384.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0083 (6.8487)  time: 0.8978 (0.5160 -- 2.8390)  data: 0.2800 (0.0003 -- 2.3313)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:42  lr: 0.000043  min_lr: 0.000011  loss: 1.4236 (1.5900)  loss_scale: 16384.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4985 (6.7088)  time: 0.9785 (0.5240 -- 3.4237)  data: 0.2467 (0.0007 -- 1.9995)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:20  lr: 0.000043  min_lr: 0.000011  loss: 1.4357 (1.5924)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1947 (6.6602)  time: 0.9324 (0.5169 -- 3.8015)  data: 0.1145 (0.0003 -- 2.1345)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:59  lr: 0.000043  min_lr: 0.000011  loss: 1.6074 (1.5895)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2648 (6.6554)  time: 0.9162 (0.5214 -- 2.3554)  data: 0.1877 (0.0003 -- 1.8548)  max mem: 16413
Epoch: [40]  [120/160]  eta: 0:00:39  lr: 0.000043  min_lr: 0.000011  loss: 1.7229 (1.6077)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1928 (6.8150)  time: 0.9848 (0.5285 -- 3.0669)  data: 0.3530 (0.0004 -- 2.5274)  max mem: 16413
Epoch: [40]  [140/160]  eta: 0:00:19  lr: 0.000043  min_lr: 0.000011  loss: 1.6595 (1.6268)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4401 (6.9376)  time: 0.8495 (0.5246 -- 2.7337)  data: 0.0013 (0.0002 -- 0.0024)  max mem: 16413
[2023-09-23 02:57:46,924] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:57:46,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 02:57:46,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 02:57:46,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.5043 (1.6239)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0830 (7.0371)  time: 0.8037 (0.4938 -- 3.8067)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [40] Total time: 0:02:31 (0.9489 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.5043 (1.6210)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0830 (7.0371)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4297 (0.4297)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4574 (2.4574 -- 2.4574)  data: 2.2718 (2.2718 -- 2.2718)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4833 (0.6358)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4696 (0.1745 -- 2.4574)  data: 0.2803 (0.0002 -- 2.2718)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3997 (0.4910)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.9418)  time: 0.2535 (0.1678 -- 1.0184)  data: 0.0668 (0.0001 -- 0.8059)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4359 (0.5380)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.7552)  time: 0.2850 (0.1330 -- 1.0184)  data: 0.1056 (0.0001 -- 0.8059)  max mem: 16413
Val: Total time: 0:00:09 (0.3441 s / it)
* Acc@1 86.515 Acc@5 99.170 loss 0.518
Accuracy of the network on the 482 val images: 86.51%
[2023-09-23 02:57:58,410] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 02:57:58,412] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 02:57:58,412] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 02:57:58,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 02:57:59,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 02:57:59,878] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.51%
Epoch: [41]  [  0/160]  eta: 0:20:59  lr: 0.000043  min_lr: 0.000011  loss: 2.1309 (2.1309)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3977 (8.3977)  time: 7.8728 (7.8728 -- 7.8728)  data: 7.3496 (7.3496 -- 7.3496)  max mem: 16413
[2023-09-23 02:58:18,372] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6571
[2023-09-23 02:58:18,372] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6571
[2023-09-23 02:58:18,372] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 02:58:18,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 02:58:18,372] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [41]  [ 20/160]  eta: 0:03:06  lr: 0.000043  min_lr: 0.000011  loss: 1.6287 (1.6578)  loss_scale: 16384.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5794 (7.2642)  time: 1.0051 (0.5157 -- 4.0768)  data: 0.4672 (0.0002 -- 3.5302)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:15  lr: 0.000043  min_lr: 0.000011  loss: 1.6968 (1.6708)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0391 (7.0239)  time: 0.9194 (0.5190 -- 2.5806)  data: 0.3800 (0.0002 -- 2.0496)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:46  lr: 0.000043  min_lr: 0.000011  loss: 1.6413 (1.6614)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6351 (7.0576)  time: 0.9259 (0.5115 -- 3.8922)  data: 0.3836 (0.0004 -- 3.3697)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:22  lr: 0.000043  min_lr: 0.000011  loss: 1.7092 (1.6514)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6208 (6.9629)  time: 0.9348 (0.5196 -- 4.6144)  data: 0.3853 (0.0004 -- 4.1005)  max mem: 16413
Epoch: [41]  [100/160]  eta: 0:01:00  lr: 0.000043  min_lr: 0.000011  loss: 1.5192 (1.6414)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4250 (6.9991)  time: 0.9211 (0.5222 -- 3.7040)  data: 0.3732 (0.0003 -- 3.1764)  max mem: 16413
Epoch: [41]  [120/160]  eta: 0:00:39  lr: 0.000043  min_lr: 0.000011  loss: 1.4272 (1.6169)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2514 (7.0703)  time: 0.8056 (0.5267 -- 3.7059)  data: 0.2598 (0.0004 -- 3.1981)  max mem: 16413
[2023-09-23 03:00:16,754] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:00:16,754] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:00:16,754] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:00:16,754] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [41]  [140/160]  eta: 0:00:19  lr: 0.000043  min_lr: 0.000011  loss: 1.5897 (1.6092)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3919 (7.0299)  time: 0.9349 (0.5248 -- 3.8291)  data: 0.3906 (0.0005 -- 3.3007)  max mem: 16413
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.7837 (1.6330)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7900 (7.0098)  time: 0.7575 (0.4946 -- 3.5488)  data: 0.2409 (0.0001 -- 3.0341)  max mem: 16413
Epoch: [41] Total time: 0:02:31 (0.9469 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.7837 (1.6119)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7900 (7.0098)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3090 (0.3090)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4011 (2.4011 -- 2.4011)  data: 2.2003 (2.2003 -- 2.2003)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3090 (0.5501)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4710 (0.1751 -- 2.4011)  data: 0.2828 (0.0003 -- 2.2003)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3977 (0.4656)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (100.0000)  time: 0.2553 (0.1684 -- 1.0960)  data: 0.0682 (0.0001 -- 0.9051)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4494 (0.5274)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (100.0000)  time: 0.2794 (0.1346 -- 1.0960)  data: 0.0992 (0.0001 -- 0.9051)  max mem: 16413
Val: Total time: 0:00:09 (0.3394 s / it)
* Acc@1 86.100 Acc@5 99.793 loss 0.487
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.51%
Epoch: [42]  [  0/160]  eta: 0:18:31  lr: 0.000043  min_lr: 0.000011  loss: 1.8519 (1.8519)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2693 (7.2693)  time: 6.9466 (6.9466 -- 6.9466)  data: 6.2678 (6.2678 -- 6.2678)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:53  lr: 0.000043  min_lr: 0.000011  loss: 1.5852 (1.6431)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7148 (6.0090)  time: 0.9514 (0.5280 -- 2.8228)  data: 0.0870 (0.0003 -- 1.3824)  max mem: 16413
[2023-09-23 03:01:13,388] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6746
[2023-09-23 03:01:13,388] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:01:13,388] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6746
[2023-09-23 03:01:13,388] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:01:13,389] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [42]  [ 40/160]  eta: 0:02:05  lr: 0.000043  min_lr: 0.000011  loss: 1.4789 (1.5833)  loss_scale: 16384.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2477 (6.3868)  time: 0.8516 (0.5138 -- 3.3460)  data: 0.0923 (0.0002 -- 1.8081)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:45  lr: 0.000043  min_lr: 0.000011  loss: 1.6816 (1.6146)  loss_scale: 16384.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1001 (6.5834)  time: 1.0547 (0.5174 -- 4.4372)  data: 0.0237 (0.0004 -- 0.4455)  max mem: 16413
Epoch: [42]  [ 80/160]  eta: 0:01:18  lr: 0.000043  min_lr: 0.000011  loss: 1.6778 (1.6343)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3094 (6.5809)  time: 0.7753 (0.5332 -- 2.4487)  data: 0.0588 (0.0002 -- 0.9973)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:59  lr: 0.000043  min_lr: 0.000011  loss: 1.5462 (1.6331)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8411 (6.8739)  time: 0.9944 (0.5249 -- 4.2240)  data: 0.4021 (0.0005 -- 3.6785)  max mem: 16413
Epoch: [42]  [120/160]  eta: 0:00:38  lr: 0.000043  min_lr: 0.000011  loss: 1.6021 (1.6322)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8951 (6.8158)  time: 0.8020 (0.5310 -- 3.6195)  data: 0.2246 (0.0006 -- 3.0846)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:19  lr: 0.000043  min_lr: 0.000011  loss: 1.4832 (1.6034)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9276 (7.0246)  time: 1.1364 (0.5015 -- 4.9732)  data: 0.6095 (0.0004 -- 4.4627)  max mem: 16413
[2023-09-23 03:03:09,745] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:03:09,745] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:03:09,745] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:03:09,745] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:03:10,234] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6876
[2023-09-23 03:03:10,234] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6876
[2023-09-23 03:03:10,234] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:03:10,234] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:03:10,234] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.4949 (1.5915)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1305 (7.0686)  time: 0.6555 (0.4793 -- 2.3862)  data: 0.1454 (0.0001 -- 1.8638)  max mem: 16413
Epoch: [42] Total time: 0:02:30 (0.9431 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.4949 (1.6096)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1305 (7.0686)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2494 (0.2494)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4071 (2.4071 -- 2.4071)  data: 2.2146 (2.2146 -- 2.2146)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4925 (0.6613)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4828 (0.1811 -- 2.4071)  data: 0.2939 (0.0002 -- 2.2146)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4158 (0.5272)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2620 (0.1679 -- 1.1983)  data: 0.0765 (0.0001 -- 1.0128)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4925 (0.5903)  acc1: 85.7143 (83.8174)  acc5: 100.0000 (98.7552)  time: 0.2858 (0.1322 -- 1.1983)  data: 0.1079 (0.0001 -- 1.0128)  max mem: 16413
Val: Total time: 0:00:09 (0.3430 s / it)
* Acc@1 86.100 Acc@5 99.170 loss 0.526
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.51%
Epoch: [43]  [  0/160]  eta: 0:19:43  lr: 0.000043  min_lr: 0.000011  loss: 1.9007 (1.9007)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4842 (7.4842)  time: 7.3960 (7.3960 -- 7.3960)  data: 6.8492 (6.8492 -- 6.8492)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:48  lr: 0.000043  min_lr: 0.000011  loss: 1.7008 (1.6833)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2012 (6.4747)  time: 0.8935 (0.5180 -- 2.9081)  data: 0.2899 (0.0006 -- 1.7164)  max mem: 16413
Epoch: [43]  [ 40/160]  eta: 0:02:09  lr: 0.000043  min_lr: 0.000011  loss: 1.5701 (1.5897)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7849 (6.7150)  time: 0.9511 (0.5190 -- 3.1545)  data: 0.3200 (0.0004 -- 2.6262)  max mem: 16413
Epoch: [43]  [ 60/160]  eta: 0:01:46  lr: 0.000043  min_lr: 0.000011  loss: 1.4955 (1.5896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5009 (6.7586)  time: 1.0408 (0.5273 -- 3.3888)  data: 0.1197 (0.0004 -- 2.2489)  max mem: 16413
Epoch: [43]  [ 80/160]  eta: 0:01:20  lr: 0.000043  min_lr: 0.000011  loss: 1.5009 (1.5766)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5961 (6.8290)  time: 0.8289 (0.5126 -- 3.0481)  data: 0.0013 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [43]  [100/160]  eta: 0:01:00  lr: 0.000042  min_lr: 0.000011  loss: 1.5294 (1.5836)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7111 (6.8470)  time: 1.0414 (0.5144 -- 4.8691)  data: 0.0013 (0.0002 -- 0.0038)  max mem: 16413
[2023-09-23 03:05:19,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=38, lr=[1.079275389402428e-05, 1.079275389402428e-05, 1.199194877113809e-05, 1.199194877113809e-05, 1.3324387523486763e-05, 1.3324387523486763e-05, 1.4804875026096405e-05, 1.4804875026096405e-05, 1.6449861140107117e-05, 1.6449861140107117e-05, 1.8277623489007908e-05, 1.8277623489007908e-05, 2.0308470543342117e-05, 2.0308470543342117e-05, 2.2564967270380128e-05, 2.2564967270380128e-05, 2.5072185855977923e-05, 2.5072185855977923e-05, 2.785798428441991e-05, 2.785798428441991e-05, 3.095331587157768e-05, 3.095331587157768e-05, 3.439257319064187e-05, 3.439257319064187e-05, 3.8213970211824294e-05, 3.8213970211824294e-05, 4.245996690202699e-05, 4.245996690202699e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 03:05:19,871] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=17.28483215826373, CurrSamplesPerSec=22.70953769353009, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:39  lr: 0.000042  min_lr: 0.000011  loss: 1.4676 (1.5624)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0461 (6.7743)  time: 0.8318 (0.5162 -- 5.3122)  data: 0.0012 (0.0002 -- 0.0028)  max mem: 16413
[2023-09-23 03:05:25,050] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:05:25,050] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:05:25,052] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:05:25,052] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [43]  [140/160]  eta: 0:00:19  lr: 0.000042  min_lr: 0.000011  loss: 1.4082 (1.5527)  loss_scale: 32768.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1394 (6.7595)  time: 0.9089 (0.5186 -- 2.9951)  data: 0.0021 (0.0002 -- 0.0111)  max mem: 16413
[2023-09-23 03:05:49,638] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7035
[2023-09-23 03:05:49,638] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7035
[2023-09-23 03:05:49,638] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:05:49,638] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:05:49,638] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.5819 (1.5557)  loss_scale: 32768.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7077 (6.7749)  time: 0.6802 (0.4936 -- 2.2992)  data: 0.0009 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [43] Total time: 0:02:30 (0.9398 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.5819 (1.5808)  loss_scale: 32768.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7077 (6.7749)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.5493 (0.5493)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4508 (2.4508 -- 2.4508)  data: 2.2533 (2.2533 -- 2.2533)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.5258 (0.6227)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4747 (0.1788 -- 2.4508)  data: 0.2823 (0.0002 -- 2.2533)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3128 (0.4828)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (99.4709)  time: 0.2639 (0.1680 -- 1.0609)  data: 0.0779 (0.0001 -- 0.8487)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4168 (0.5340)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (99.5851)  time: 0.2945 (0.1324 -- 1.0609)  data: 0.1174 (0.0001 -- 0.8487)  max mem: 16413
Val: Total time: 0:00:09 (0.3519 s / it)
* Acc@1 87.137 Acc@5 99.585 loss 0.481
Accuracy of the network on the 482 val images: 87.14%
[2023-09-23 03:06:01,130] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 03:06:01,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 03:06:01,131] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 03:06:01,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 03:06:02,741] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 03:06:02,741] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 87.14%
Epoch: [44]  [  0/160]  eta: 0:19:15  lr: 0.000042  min_lr: 0.000011  loss: 1.2252 (1.2252)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8423 (12.8423)  time: 7.2223 (7.2223 -- 7.2223)  data: 6.7078 (6.7078 -- 6.7078)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:03:02  lr: 0.000042  min_lr: 0.000011  loss: 1.7069 (1.5989)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9473 (7.3186)  time: 1.0058 (0.5224 -- 5.1590)  data: 0.2461 (0.0002 -- 2.4597)  max mem: 16413
Epoch: [44]  [ 40/160]  eta: 0:02:07  lr: 0.000042  min_lr: 0.000011  loss: 1.5110 (1.5626)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7360 (6.9676)  time: 0.8136 (0.5111 -- 2.2725)  data: 0.0014 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [44]  [ 60/160]  eta: 0:01:45  lr: 0.000042  min_lr: 0.000011  loss: 1.6861 (1.6045)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6971 (6.9431)  time: 1.0374 (0.5170 -- 4.9235)  data: 0.0013 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:21  lr: 0.000042  min_lr: 0.000011  loss: 1.6004 (1.5883)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5624 (6.9582)  time: 0.9228 (0.5125 -- 4.6892)  data: 0.0009 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [44]  [100/160]  eta: 0:00:59  lr: 0.000042  min_lr: 0.000011  loss: 1.6163 (1.5899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4956 (6.9668)  time: 0.8402 (0.5128 -- 3.7611)  data: 0.1321 (0.0004 -- 2.6126)  max mem: 16413
Epoch: [44]  [120/160]  eta: 0:00:38  lr: 0.000042  min_lr: 0.000011  loss: 1.6147 (1.6005)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3691 (7.0755)  time: 0.9006 (0.5205 -- 2.5839)  data: 0.0015 (0.0006 -- 0.0032)  max mem: 16413
[2023-09-23 03:08:03,378] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:08:03,378] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:08:03,379] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:08:03,379] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [140/160]  eta: 0:00:19  lr: 0.000042  min_lr: 0.000011  loss: 1.7133 (1.6093)  loss_scale: 32768.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5464 (7.2582)  time: 0.8643 (0.5235 -- 2.8141)  data: 0.0541 (0.0002 -- 0.6745)  max mem: 16413
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.6790 (1.6103)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1557 (7.2741)  time: 0.8023 (0.4972 -- 2.8088)  data: 0.0336 (0.0002 -- 0.6574)  max mem: 16413
Epoch: [44] Total time: 0:02:30 (0.9402 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.6790 (1.6191)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1557 (7.2741)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6750 (0.6750)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3639 (2.3639 -- 2.3639)  data: 2.1624 (2.1624 -- 2.1624)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4364 (0.5986)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4661 (0.1747 -- 2.3639)  data: 0.2790 (0.0003 -- 2.1624)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3685 (0.5120)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2565 (0.1683 -- 1.0989)  data: 0.0706 (0.0001 -- 0.9022)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4532 (0.5899)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (98.3402)  time: 0.2889 (0.1322 -- 1.0989)  data: 0.1091 (0.0001 -- 0.9022)  max mem: 16413
Val: Total time: 0:00:09 (0.3427 s / it)
* Acc@1 85.892 Acc@5 98.963 loss 0.516
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.14%
Epoch: [45]  [  0/160]  eta: 0:23:31  lr: 0.000042  min_lr: 0.000011  loss: 1.8545 (1.8545)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0346 (6.0346)  time: 8.8214 (8.8214 -- 8.8214)  data: 8.2971 (8.2971 -- 8.2971)  max mem: 16413
[2023-09-23 03:09:02,945] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7214
[2023-09-23 03:09:02,945] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7214
[2023-09-23 03:09:02,945] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:09:02,945] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:09:02,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [45]  [ 20/160]  eta: 0:02:51  lr: 0.000042  min_lr: 0.000011  loss: 1.6941 (1.6792)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5495 (6.9317)  time: 0.8488 (0.5184 -- 3.3254)  data: 0.3099 (0.0007 -- 2.7777)  max mem: 16413
Epoch: [45]  [ 40/160]  eta: 0:02:11  lr: 0.000042  min_lr: 0.000011  loss: 1.5709 (1.6376)  loss_scale: 16384.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8667 (6.9204)  time: 0.9580 (0.5209 -- 5.3780)  data: 0.4229 (0.0003 -- 4.8633)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:45  lr: 0.000042  min_lr: 0.000011  loss: 1.6028 (1.6240)  loss_scale: 16384.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8004 (6.9553)  time: 0.9664 (0.5091 -- 4.2757)  data: 0.4251 (0.0003 -- 3.7569)  max mem: 16413
Epoch: [45]  [ 80/160]  eta: 0:01:20  lr: 0.000042  min_lr: 0.000011  loss: 1.4705 (1.6045)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1296 (6.7513)  time: 0.8645 (0.5192 -- 4.0629)  data: 0.3264 (0.0003 -- 3.5455)  max mem: 16413
Epoch: [45]  [100/160]  eta: 0:00:59  lr: 0.000042  min_lr: 0.000011  loss: 1.7083 (1.6087)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8891 (6.7035)  time: 0.9393 (0.5228 -- 3.2903)  data: 0.3333 (0.0004 -- 2.7168)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:38  lr: 0.000042  min_lr: 0.000011  loss: 1.3743 (1.5858)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9035 (6.7203)  time: 0.8668 (0.5249 -- 2.8963)  data: 0.2358 (0.0005 -- 2.3723)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:19  lr: 0.000042  min_lr: 0.000011  loss: 1.5031 (1.5768)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6951 (6.7387)  time: 0.9774 (0.5206 -- 4.6947)  data: 0.4247 (0.0004 -- 4.1485)  max mem: 16413
[2023-09-23 03:11:01,527] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:11:01,527] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:11:01,528] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:11:01,528] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.6190 (1.5860)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4178 (6.7435)  time: 0.6944 (0.4936 -- 3.0323)  data: 0.1805 (0.0002 -- 2.5115)  max mem: 16413
Epoch: [45] Total time: 0:02:30 (0.9413 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.6190 (1.5673)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4178 (6.7435)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.4739 (0.4739)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5232 (2.5232 -- 2.5232)  data: 2.3295 (2.3295 -- 2.3295)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4739 (0.5872)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4747 (0.1742 -- 2.5232)  data: 0.2836 (0.0002 -- 2.3295)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3934 (0.5125)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.9418)  time: 0.2527 (0.1686 -- 0.9701)  data: 0.0653 (0.0001 -- 0.7832)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5079 (0.5466)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (99.1701)  time: 0.2850 (0.1330 -- 0.9791)  data: 0.1054 (0.0001 -- 0.8081)  max mem: 16413
Val: Total time: 0:00:09 (0.3469 s / it)
* Acc@1 87.137 Acc@5 99.378 loss 0.485
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.14%
Epoch: [46]  [  0/160]  eta: 0:20:53  lr: 0.000042  min_lr: 0.000011  loss: 1.6590 (1.6590)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3106 (7.3106)  time: 7.8366 (7.8366 -- 7.8366)  data: 7.3154 (7.3154 -- 7.3154)  max mem: 16413
[2023-09-23 03:11:50,061] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7379
[2023-09-23 03:11:50,061] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7379
[2023-09-23 03:11:50,061] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:11:50,062] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:11:50,062] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [46]  [ 20/160]  eta: 0:03:06  lr: 0.000042  min_lr: 0.000011  loss: 1.5124 (1.5666)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2182 (7.5349)  time: 1.0055 (0.5259 -- 3.0777)  data: 0.3335 (0.0004 -- 2.5374)  max mem: 16413
Epoch: [46]  [ 40/160]  eta: 0:02:09  lr: 0.000042  min_lr: 0.000011  loss: 1.5678 (1.5803)  loss_scale: 16384.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6400 (7.5287)  time: 0.8084 (0.5286 -- 3.3324)  data: 0.1000 (0.0005 -- 1.4077)  max mem: 16413
Epoch: [46]  [ 60/160]  eta: 0:01:45  lr: 0.000042  min_lr: 0.000011  loss: 1.6486 (1.5997)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0336 (7.4351)  time: 1.0032 (0.5215 -- 3.9276)  data: 0.1192 (0.0004 -- 1.1914)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:19  lr: 0.000042  min_lr: 0.000011  loss: 1.5360 (1.5835)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0175 (7.4308)  time: 0.8298 (0.5174 -- 2.6331)  data: 0.1586 (0.0001 -- 1.7090)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:59  lr: 0.000042  min_lr: 0.000011  loss: 1.4031 (1.5515)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2326 (7.3418)  time: 1.0029 (0.5129 -- 4.0945)  data: 0.4290 (0.0004 -- 3.5765)  max mem: 16413
Epoch: [46]  [120/160]  eta: 0:00:38  lr: 0.000042  min_lr: 0.000011  loss: 1.6780 (1.5707)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8647 (7.2731)  time: 0.7848 (0.5166 -- 3.3345)  data: 0.2007 (0.0005 -- 2.7882)  max mem: 16413
Epoch: [46]  [140/160]  eta: 0:00:19  lr: 0.000042  min_lr: 0.000011  loss: 1.4246 (1.5683)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0591 (7.2516)  time: 1.0034 (0.5250 -- 3.6213)  data: 0.4678 (0.0005 -- 3.1018)  max mem: 16413
[2023-09-23 03:13:46,230] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:13:46,230] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:13:46,231] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:13:46,231] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.5799 (1.5706)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7762 (7.2255)  time: 0.7211 (0.4943 -- 3.1353)  data: 0.1983 (0.0001 -- 2.5955)  max mem: 16413
Epoch: [46] Total time: 0:02:30 (0.9407 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.5799 (1.6010)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7762 (7.2255)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2384 (0.2384)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4740 (2.4740 -- 2.4740)  data: 2.3009 (2.3009 -- 2.3009)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4838 (0.5636)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (96.9697)  time: 0.4844 (0.1800 -- 2.4740)  data: 0.2922 (0.0003 -- 2.3009)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4924 (0.5083)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2529 (0.1684 -- 1.1231)  data: 0.0640 (0.0001 -- 0.9024)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5459 (0.5657)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2892 (0.1322 -- 1.1231)  data: 0.1082 (0.0001 -- 0.9024)  max mem: 16413
Val: Total time: 0:00:09 (0.3498 s / it)
* Acc@1 87.344 Acc@5 98.963 loss 0.523
Accuracy of the network on the 482 val images: 87.34%
[2023-09-23 03:14:02,617] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 03:14:02,619] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 03:14:02,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 03:14:02,619] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 03:14:04,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 03:14:04,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 87.34%
Epoch: [47]  [  0/160]  eta: 0:20:31  lr: 0.000042  min_lr: 0.000011  loss: 1.2280 (1.2280)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3287 (5.3287)  time: 7.6940 (7.6940 -- 7.6940)  data: 5.5472 (5.5472 -- 5.5472)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:58  lr: 0.000042  min_lr: 0.000011  loss: 1.5971 (1.5823)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8683 (6.3856)  time: 0.9564 (0.5192 -- 3.6847)  data: 0.3451 (0.0005 -- 3.1444)  max mem: 16413
Epoch: [47]  [ 40/160]  eta: 0:02:15  lr: 0.000042  min_lr: 0.000011  loss: 1.4178 (1.5078)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7546 (6.6046)  time: 0.9700 (0.5118 -- 3.6056)  data: 0.4339 (0.0005 -- 3.0926)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:43  lr: 0.000042  min_lr: 0.000011  loss: 1.7223 (1.5414)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7429 (6.5551)  time: 0.8606 (0.5239 -- 3.8429)  data: 0.3198 (0.0003 -- 3.3075)  max mem: 16413
[2023-09-23 03:15:16,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7590
[2023-09-23 03:15:16,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7590
[2023-09-23 03:15:16,623] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:15:16,623] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:15:16,623] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [ 80/160]  eta: 0:01:23  lr: 0.000042  min_lr: 0.000011  loss: 1.5110 (1.5548)  loss_scale: 16384.0000 (30543.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3693 (6.6221)  time: 1.0650 (0.5033 -- 4.9750)  data: 0.5321 (0.0003 -- 4.4597)  max mem: 16413
Epoch: [47]  [100/160]  eta: 0:00:59  lr: 0.000042  min_lr: 0.000011  loss: 1.5469 (1.5568)  loss_scale: 16384.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2813 (6.9039)  time: 0.7681 (0.5163 -- 3.8321)  data: 0.2208 (0.0002 -- 3.3204)  max mem: 16413
Epoch: [47]  [120/160]  eta: 0:00:40  lr: 0.000042  min_lr: 0.000011  loss: 1.6308 (1.5695)  loss_scale: 16384.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2660 (7.0410)  time: 1.1029 (0.5230 -- 5.8415)  data: 0.5585 (0.0003 -- 5.3412)  max mem: 16413
Epoch: [47]  [140/160]  eta: 0:00:19  lr: 0.000042  min_lr: 0.000011  loss: 1.6257 (1.5787)  loss_scale: 16384.0000 (24517.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6210 (7.0469)  time: 0.6970 (0.5129 -- 3.3204)  data: 0.1565 (0.0002 -- 2.8023)  max mem: 16413
[2023-09-23 03:16:27,145] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7667
[2023-09-23 03:16:27,145] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7667
[2023-09-23 03:16:27,145] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 03:16:27,145] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 03:16:27,145] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000011  loss: 1.5310 (1.5639)  loss_scale: 8192.0000 (22886.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6378 (7.0224)  time: 0.7172 (0.4935 -- 3.6238)  data: 0.1981 (0.0002 -- 3.0911)  max mem: 16413
Epoch: [47] Total time: 0:02:29 (0.9371 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000011  loss: 1.5310 (1.5766)  loss_scale: 8192.0000 (22886.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6378 (7.0224)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2100 (0.2100)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3782 (2.3782 -- 2.3782)  data: 2.1667 (2.1667 -- 2.1667)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3560 (0.5399)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.4815 (0.1785 -- 2.3782)  data: 0.2889 (0.0002 -- 2.1667)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3560 (0.5013)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2599 (0.1684 -- 1.1803)  data: 0.0718 (0.0001 -- 0.9969)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4207 (0.5393)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2816 (0.1323 -- 1.1803)  data: 0.1023 (0.0001 -- 0.9969)  max mem: 16413
Val: Total time: 0:00:09 (0.3394 s / it)
* Acc@1 87.344 Acc@5 98.963 loss 0.483
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 87.34%
Epoch: [48]  [  0/160]  eta: 0:21:53  lr: 0.000041  min_lr: 0.000011  loss: 1.4380 (1.4380)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8738 (5.8738)  time: 8.2083 (8.2083 -- 8.2083)  data: 6.0940 (6.0940 -- 6.0940)  max mem: 16413
Epoch: [48]  [ 20/160]  eta: 0:03:00  lr: 0.000041  min_lr: 0.000011  loss: 1.6759 (1.6810)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0047 (6.4398)  time: 0.9414 (0.5265 -- 2.5265)  data: 0.2357 (0.0002 -- 1.9927)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:11  lr: 0.000041  min_lr: 0.000011  loss: 1.4922 (1.5434)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4024 (7.0471)  time: 0.8931 (0.5116 -- 3.6599)  data: 0.2384 (0.0001 -- 3.1479)  max mem: 16413
Epoch: [48]  [ 60/160]  eta: 0:01:43  lr: 0.000041  min_lr: 0.000011  loss: 1.5182 (1.5517)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7219 (6.9757)  time: 0.9231 (0.5146 -- 3.6499)  data: 0.1182 (0.0004 -- 1.6989)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:22  lr: 0.000041  min_lr: 0.000011  loss: 1.5089 (1.5568)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1969 (6.9008)  time: 0.9960 (0.5328 -- 3.9526)  data: 0.0371 (0.0003 -- 0.7093)  max mem: 16413
Epoch: [48]  [100/160]  eta: 0:00:59  lr: 0.000041  min_lr: 0.000011  loss: 1.5341 (1.5545)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6636 (7.0324)  time: 0.8608 (0.5264 -- 3.2803)  data: 0.2589 (0.0004 -- 2.7595)  max mem: 16413
[2023-09-23 03:18:39,471] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:18:39,472] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 03:18:39,472] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:18:39,472] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [48]  [120/160]  eta: 0:00:39  lr: 0.000041  min_lr: 0.000010  loss: 1.5773 (1.5559)  loss_scale: 8192.0000 (8530.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7796 (7.0851)  time: 0.8790 (0.5149 -- 5.3880)  data: 0.3408 (0.0007 -- 4.8692)  max mem: 16413
Epoch: [48]  [140/160]  eta: 0:00:19  lr: 0.000041  min_lr: 0.000010  loss: 1.7364 (1.5783)  loss_scale: 16384.0000 (9644.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5121 (7.1448)  time: 1.0555 (0.5099 -- 5.5624)  data: 0.5202 (0.0003 -- 5.0378)  max mem: 16413
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.5399 (1.5720)  loss_scale: 16384.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3197 (7.1087)  time: 0.7201 (0.4925 -- 3.2768)  data: 0.2013 (0.0002 -- 2.7621)  max mem: 16413
Epoch: [48] Total time: 0:02:33 (0.9569 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.5399 (1.5679)  loss_scale: 16384.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3197 (7.1087)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.4678 (0.4678)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5097 (2.5097 -- 2.5097)  data: 2.3134 (2.3134 -- 2.3134)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3809 (0.6383)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (95.9596)  time: 0.4835 (0.1790 -- 2.5097)  data: 0.2950 (0.0002 -- 2.3134)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3809 (0.5656)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2609 (0.1683 -- 1.1226)  data: 0.0739 (0.0001 -- 0.9232)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5072 (0.5933)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (97.5104)  time: 0.2873 (0.1320 -- 1.1226)  data: 0.1063 (0.0001 -- 0.9232)  max mem: 16413
Val: Total time: 0:00:09 (0.3468 s / it)
* Acc@1 85.062 Acc@5 98.340 loss 0.534
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 87.34%
Epoch: [49]  [  0/160]  eta: 0:23:54  lr: 0.000041  min_lr: 0.000010  loss: 2.0471 (2.0471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7912 (6.7912)  time: 8.9660 (8.9660 -- 8.9660)  data: 8.4516 (8.4516 -- 8.4516)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:52  lr: 0.000041  min_lr: 0.000010  loss: 1.4988 (1.5882)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8095 (6.6773)  time: 0.8451 (0.5167 -- 3.9174)  data: 0.3048 (0.0003 -- 3.4000)  max mem: 16413
Epoch: [49]  [ 40/160]  eta: 0:02:17  lr: 0.000041  min_lr: 0.000010  loss: 1.4800 (1.5616)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1805 (6.5261)  time: 1.0533 (0.5211 -- 4.9427)  data: 0.5160 (0.0005 -- 4.4252)  max mem: 16413
Epoch: [49]  [ 60/160]  eta: 0:01:42  lr: 0.000041  min_lr: 0.000010  loss: 1.6606 (1.5863)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5474 (6.7231)  time: 0.7924 (0.5214 -- 3.8531)  data: 0.2545 (0.0002 -- 3.3350)  max mem: 16413
Epoch: [49]  [ 80/160]  eta: 0:01:21  lr: 0.000041  min_lr: 0.000010  loss: 1.6335 (1.5946)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3473 (6.8824)  time: 1.0096 (0.5227 -- 4.5357)  data: 0.4668 (0.0003 -- 3.9955)  max mem: 16413
[2023-09-23 03:20:51,525] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:20:51,525] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:20:51,525] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:20:51,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [49]  [100/160]  eta: 0:00:59  lr: 0.000041  min_lr: 0.000010  loss: 1.6802 (1.6046)  loss_scale: 32768.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0832 (6.8750)  time: 0.8265 (0.5161 -- 4.7465)  data: 0.2833 (0.0001 -- 4.2338)  max mem: 16413
[2023-09-23 03:21:06,184] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7941
[2023-09-23 03:21:06,184] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7941
[2023-09-23 03:21:06,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:21:06,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:21:06,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [49]  [120/160]  eta: 0:00:39  lr: 0.000041  min_lr: 0.000010  loss: 1.5766 (1.5896)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7571 (6.8935)  time: 1.0374 (0.5213 -- 3.9984)  data: 0.4335 (0.0004 -- 3.4490)  max mem: 16413
Epoch: [49]  [140/160]  eta: 0:00:19  lr: 0.000041  min_lr: 0.000010  loss: 1.4930 (1.5844)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7717 (6.9620)  time: 0.8800 (0.5148 -- 4.3600)  data: 0.3432 (0.0003 -- 3.8273)  max mem: 16413
[2023-09-23 03:21:56,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7993
[2023-09-23 03:21:56,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7993
[2023-09-23 03:21:56,102] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 03:21:56,102] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 03:21:56,102] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-23 03:21:59,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=45, lr=[1.0418644962272174e-05, 1.0418644962272174e-05, 1.1576272180302416e-05, 1.1576272180302416e-05, 1.286252464478046e-05, 1.286252464478046e-05, 1.4291694049756068e-05, 1.4291694049756068e-05, 1.587966005528452e-05, 1.587966005528452e-05, 1.764406672809391e-05, 1.764406672809391e-05, 1.960451858677101e-05, 1.960451858677101e-05, 2.1782798429745566e-05, 2.1782798429745566e-05, 2.4203109366383962e-05, 2.4203109366383962e-05, 2.689234374042662e-05, 2.689234374042662e-05, 2.988038193380736e-05, 2.988038193380736e-05, 3.3200424370897066e-05, 3.3200424370897066e-05, 3.688936041210785e-05, 3.688936041210785e-05, 4.0988178235675386e-05, 4.0988178235675386e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 03:21:59,077] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=17.48869800691787, CurrSamplesPerSec=24.938497129416803, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.5477 (1.5714)  loss_scale: 16384.0000 (17766.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4206 (6.9569)  time: 0.7801 (0.4786 -- 4.8064)  data: 0.2730 (0.0002 -- 4.3032)  max mem: 16413
Epoch: [49] Total time: 0:02:32 (0.9559 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.5477 (1.5992)  loss_scale: 16384.0000 (17766.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4206 (6.9569)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5619 (0.5619)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4415 (2.4415 -- 2.4415)  data: 2.2574 (2.2574 -- 2.2574)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4079 (0.5680)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (96.9697)  time: 0.4686 (0.1785 -- 2.4415)  data: 0.2726 (0.0005 -- 2.2574)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3602 (0.4786)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2515 (0.1684 -- 0.9501)  data: 0.0621 (0.0001 -- 0.7351)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4506 (0.5325)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2688 (0.1334 -- 0.9501)  data: 0.0889 (0.0001 -- 0.7351)  max mem: 16413
Val: Total time: 0:00:08 (0.3332 s / it)
* Acc@1 86.722 Acc@5 98.963 loss 0.497
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.34%
Epoch: [50]  [  0/160]  eta: 0:18:50  lr: 0.000041  min_lr: 0.000010  loss: 1.6145 (1.6145)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1015 (8.1015)  time: 7.0653 (7.0653 -- 7.0653)  data: 6.1407 (6.1407 -- 6.1407)  max mem: 16413
Epoch: [50]  [ 20/160]  eta: 0:03:02  lr: 0.000041  min_lr: 0.000010  loss: 1.6286 (1.5651)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3715 (6.5265)  time: 1.0121 (0.5337 -- 4.1579)  data: 0.3195 (0.0001 -- 3.6312)  max mem: 16413
Epoch: [50]  [ 40/160]  eta: 0:02:09  lr: 0.000041  min_lr: 0.000010  loss: 1.3494 (1.4972)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0361 (6.5394)  time: 0.8394 (0.5161 -- 4.1020)  data: 0.3003 (0.0003 -- 3.5816)  max mem: 16413
Epoch: [50]  [ 60/160]  eta: 0:01:45  lr: 0.000041  min_lr: 0.000010  loss: 1.3686 (1.4948)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3968 (6.9008)  time: 1.0208 (0.5250 -- 4.1261)  data: 0.4435 (0.0004 -- 3.5990)  max mem: 16413
Epoch: [50]  [ 80/160]  eta: 0:01:22  lr: 0.000041  min_lr: 0.000010  loss: 1.6274 (1.5096)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1185 (6.9001)  time: 0.9479 (0.5248 -- 4.4166)  data: 0.0675 (0.0003 -- 0.6532)  max mem: 16413
Epoch: [50]  [100/160]  eta: 0:01:01  lr: 0.000041  min_lr: 0.000010  loss: 1.5043 (1.5263)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9003 (7.0737)  time: 0.9965 (0.5189 -- 4.9401)  data: 0.0014 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:38  lr: 0.000041  min_lr: 0.000010  loss: 1.3961 (1.5133)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6108 (7.0349)  time: 0.6593 (0.5175 -- 1.8506)  data: 0.0469 (0.0005 -- 0.9062)  max mem: 16413
[2023-09-23 03:24:08,628] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:24:08,628] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:24:08,628] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 03:24:08,628] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [50]  [140/160]  eta: 0:00:19  lr: 0.000041  min_lr: 0.000010  loss: 1.5355 (1.5255)  loss_scale: 16384.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8752 (7.0416)  time: 0.9995 (0.5248 -- 3.9655)  data: 0.4576 (0.0004 -- 3.4520)  max mem: 16413
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.4702 (1.5309)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1894 (7.0515)  time: 0.7498 (0.4943 -- 1.6319)  data: 0.1793 (0.0002 -- 1.1246)  max mem: 16413
Epoch: [50] Total time: 0:02:31 (0.9440 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.4702 (1.5438)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1894 (7.0515)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3503 (0.3503)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4268 (2.4268 -- 2.4268)  data: 2.2199 (2.2199 -- 2.2199)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4047 (0.6185)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (96.9697)  time: 0.4802 (0.1750 -- 2.4268)  data: 0.2872 (0.0004 -- 2.2199)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2544 (0.4882)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2604 (0.1688 -- 1.1266)  data: 0.0737 (0.0001 -- 0.9337)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4047 (0.5547)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2957 (0.1336 -- 1.1266)  data: 0.1157 (0.0001 -- 0.9337)  max mem: 16413
Val: Total time: 0:00:09 (0.3508 s / it)
* Acc@1 85.892 Acc@5 98.548 loss 0.529
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.34%
Epoch: [51]  [  0/160]  eta: 0:20:52  lr: 0.000041  min_lr: 0.000010  loss: 1.4790 (1.4790)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2456 (8.2456)  time: 7.8257 (7.8257 -- 7.8257)  data: 7.2913 (7.2913 -- 7.2913)  max mem: 16413
Epoch: [51]  [ 20/160]  eta: 0:02:39  lr: 0.000041  min_lr: 0.000010  loss: 1.5358 (1.5828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5838 (6.8984)  time: 0.8064 (0.5206 -- 3.9475)  data: 0.2452 (0.0005 -- 3.4263)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:02:02  lr: 0.000041  min_lr: 0.000010  loss: 1.5144 (1.5599)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5429 (6.6392)  time: 0.9020 (0.5204 -- 1.9509)  data: 0.2524 (0.0004 -- 1.3192)  max mem: 16413
Epoch: [51]  [ 60/160]  eta: 0:01:40  lr: 0.000041  min_lr: 0.000010  loss: 1.5380 (1.5502)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2125 (6.9992)  time: 0.9720 (0.5259 -- 3.3126)  data: 0.3315 (0.0004 -- 2.7872)  max mem: 16413
Epoch: [51]  [ 80/160]  eta: 0:01:18  lr: 0.000041  min_lr: 0.000010  loss: 1.5260 (1.5566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7972 (7.0222)  time: 0.8995 (0.5251 -- 3.4487)  data: 0.3466 (0.0005 -- 2.9177)  max mem: 16413
[2023-09-23 03:26:19,085] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:26:19,085] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:26:19,085] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:26:19,086] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [51]  [100/160]  eta: 0:00:58  lr: 0.000041  min_lr: 0.000010  loss: 1.5392 (1.5601)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9648 (6.9876)  time: 0.9798 (0.5219 -- 3.1442)  data: 0.4369 (0.0006 -- 2.6178)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:38  lr: 0.000041  min_lr: 0.000010  loss: 1.6799 (1.5703)  loss_scale: 32768.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3779 (7.0924)  time: 0.8335 (0.5301 -- 3.0613)  data: 0.2838 (0.0001 -- 2.5446)  max mem: 16413
Epoch: [51]  [140/160]  eta: 0:00:19  lr: 0.000041  min_lr: 0.000010  loss: 1.4218 (1.5591)  loss_scale: 32768.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5113 (7.0741)  time: 0.9645 (0.5266 -- 4.2931)  data: 0.3992 (0.0002 -- 3.7771)  max mem: 16413
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.4980 (1.5541)  loss_scale: 32768.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9143 (6.9426)  time: 0.7352 (0.4938 -- 3.8933)  data: 0.2184 (0.0002 -- 3.3943)  max mem: 16413
Epoch: [51] Total time: 0:02:29 (0.9324 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.4980 (1.5626)  loss_scale: 32768.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9143 (6.9426)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2584 (0.2584)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3498 (2.3498 -- 2.3498)  data: 2.1495 (2.1495 -- 2.1495)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3732 (0.5907)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4670 (0.1765 -- 2.3498)  data: 0.2774 (0.0002 -- 2.1495)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4233 (0.5096)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.9418)  time: 0.2605 (0.1683 -- 1.0903)  data: 0.0729 (0.0001 -- 0.8878)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4674 (0.5772)  acc1: 85.7143 (82.5726)  acc5: 100.0000 (98.7552)  time: 0.2845 (0.1320 -- 1.0903)  data: 0.1039 (0.0001 -- 0.8878)  max mem: 16413
Val: Total time: 0:00:09 (0.3394 s / it)
* Acc@1 85.477 Acc@5 99.170 loss 0.501
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.34%
Epoch: [52]  [  0/160]  eta: 0:20:14  lr: 0.000040  min_lr: 0.000010  loss: 2.1348 (2.1348)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5951 (6.5951)  time: 7.5885 (7.5885 -- 7.5885)  data: 7.0719 (7.0719 -- 7.0719)  max mem: 16413
Epoch: [52]  [ 20/160]  eta: 0:02:50  lr: 0.000040  min_lr: 0.000010  loss: 1.4701 (1.5315)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2891 (6.8775)  time: 0.8963 (0.5253 -- 4.0113)  data: 0.3319 (0.0004 -- 3.4697)  max mem: 16413
[2023-09-23 03:28:08,691] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8355
[2023-09-23 03:28:08,692] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8355
[2023-09-23 03:28:08,692] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:28:08,692] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:28:08,692] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [52]  [ 40/160]  eta: 0:02:19  lr: 0.000040  min_lr: 0.000010  loss: 1.6586 (1.5726)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3926 (6.7893)  time: 1.1034 (0.5066 -- 4.8390)  data: 0.3293 (0.0004 -- 2.7632)  max mem: 16413
Epoch: [52]  [ 60/160]  eta: 0:01:42  lr: 0.000040  min_lr: 0.000010  loss: 1.6160 (1.5938)  loss_scale: 16384.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3253 (6.8146)  time: 0.7495 (0.5270 -- 3.4332)  data: 0.0636 (0.0003 -- 1.0570)  max mem: 16413
Epoch: [52]  [ 80/160]  eta: 0:01:20  lr: 0.000040  min_lr: 0.000010  loss: 1.5295 (1.5928)  loss_scale: 16384.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3655 (6.9932)  time: 0.9568 (0.5180 -- 3.6558)  data: 0.3143 (0.0002 -- 2.6092)  max mem: 16413
Epoch: [52]  [100/160]  eta: 0:00:58  lr: 0.000040  min_lr: 0.000010  loss: 1.4649 (1.5953)  loss_scale: 16384.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2744 (6.8496)  time: 0.8201 (0.5230 -- 4.1910)  data: 0.2788 (0.0003 -- 3.6646)  max mem: 16413
Epoch: [52]  [120/160]  eta: 0:00:38  lr: 0.000040  min_lr: 0.000010  loss: 1.4636 (1.5897)  loss_scale: 16384.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9039 (6.8935)  time: 0.9444 (0.5350 -- 2.7727)  data: 0.2529 (0.0003 -- 2.2291)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:19  lr: 0.000040  min_lr: 0.000010  loss: 1.5654 (1.5834)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9295 (6.7724)  time: 0.9093 (0.5250 -- 1.6500)  data: 0.2042 (0.0002 -- 0.9689)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.5654 (1.5762)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1651 (6.7495)  time: 0.7773 (0.4925 -- 2.9681)  data: 0.0897 (0.0002 -- 0.9689)  max mem: 16413
Epoch: [52] Total time: 0:02:29 (0.9330 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.5654 (1.5808)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1651 (6.7495)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2562 (0.2562)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5609 (2.5609 -- 2.5609)  data: 2.3211 (2.3211 -- 2.3211)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3710 (0.5891)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4817 (0.1794 -- 2.5609)  data: 0.2846 (0.0003 -- 2.3211)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3001 (0.4811)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2526 (0.1675 -- 1.0427)  data: 0.0618 (0.0001 -- 0.8034)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3943 (0.5262)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.7552)  time: 0.2755 (0.1331 -- 1.0427)  data: 0.0920 (0.0001 -- 0.8034)  max mem: 16413
Val: Total time: 0:00:09 (0.3408 s / it)
* Acc@1 87.137 Acc@5 99.170 loss 0.484
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.34%
Epoch: [53]  [  0/160]  eta: 0:21:47  lr: 0.000040  min_lr: 0.000010  loss: 1.6027 (1.6027)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5367 (5.5367)  time: 8.1716 (8.1716 -- 8.1716)  data: 7.1032 (7.1032 -- 7.1032)  max mem: 16413
[2023-09-23 03:30:16,432] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:30:16,432] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:30:16,433] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:30:16,433] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [53]  [ 20/160]  eta: 0:02:51  lr: 0.000040  min_lr: 0.000010  loss: 1.5254 (1.5721)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6266 (7.6014)  time: 0.8747 (0.5239 -- 3.7598)  data: 0.2076 (0.0002 -- 2.1785)  max mem: 16413
Epoch: [53]  [ 40/160]  eta: 0:02:14  lr: 0.000040  min_lr: 0.000010  loss: 1.5489 (1.5648)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6334 (7.2214)  time: 1.0134 (0.5246 -- 3.9614)  data: 0.0941 (0.0004 -- 0.9759)  max mem: 16413
Epoch: [53]  [ 60/160]  eta: 0:01:46  lr: 0.000040  min_lr: 0.000010  loss: 1.8043 (1.6142)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5677 (7.2155)  time: 0.9556 (0.5138 -- 4.2515)  data: 0.1386 (0.0004 -- 2.0856)  max mem: 16413
Epoch: [53]  [ 80/160]  eta: 0:01:22  lr: 0.000040  min_lr: 0.000010  loss: 1.5356 (1.5911)  loss_scale: 32768.0000 (31958.9136)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9203 (7.0862)  time: 0.9208 (0.5367 -- 3.1662)  data: 0.1174 (0.0002 -- 1.1929)  max mem: 16413
[2023-09-23 03:31:45,490] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8577
[2023-09-23 03:31:45,490] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8577
[2023-09-23 03:31:45,490] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:31:45,490] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:31:45,490] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [53]  [100/160]  eta: 0:01:00  lr: 0.000040  min_lr: 0.000010  loss: 1.6551 (1.6005)  loss_scale: 32768.0000 (31470.2574)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0179 (7.1858)  time: 0.8821 (0.5197 -- 4.3131)  data: 0.0090 (0.0002 -- 0.1402)  max mem: 16413
Epoch: [53]  [120/160]  eta: 0:00:39  lr: 0.000040  min_lr: 0.000010  loss: 1.5490 (1.5964)  loss_scale: 16384.0000 (28976.6612)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2602 (7.0773)  time: 0.9393 (0.5255 -- 4.2159)  data: 0.0017 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:19  lr: 0.000040  min_lr: 0.000010  loss: 1.6026 (1.5951)  loss_scale: 16384.0000 (27190.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6072 (7.0111)  time: 0.8018 (0.5174 -- 3.1571)  data: 0.0016 (0.0003 -- 0.0075)  max mem: 16413
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.6300 (1.5928)  loss_scale: 16384.0000 (25907.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8783 (7.0350)  time: 0.7606 (0.4922 -- 4.0063)  data: 0.0009 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [53] Total time: 0:02:30 (0.9412 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.6300 (1.5623)  loss_scale: 16384.0000 (25907.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8783 (7.0350)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2452 (0.2452)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4873 (2.4873 -- 2.4873)  data: 2.3056 (2.3056 -- 2.3056)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3345 (0.5023)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4968 (0.1792 -- 2.4873)  data: 0.3030 (0.0003 -- 2.3056)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3345 (0.4422)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2592 (0.1679 -- 1.1918)  data: 0.0696 (0.0001 -- 0.9954)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4135 (0.5098)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (99.1701)  time: 0.2867 (0.1324 -- 1.1918)  data: 0.1068 (0.0001 -- 0.9954)  max mem: 16413
Val: Total time: 0:00:09 (0.3492 s / it)
* Acc@1 86.100 Acc@5 99.170 loss 0.497
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.34%
Epoch: [54]  [  0/160]  eta: 0:20:25  lr: 0.000040  min_lr: 0.000010  loss: 2.1706 (2.1706)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8314 (6.8314)  time: 7.6612 (7.6612 -- 7.6612)  data: 7.1043 (7.1043 -- 7.1043)  max mem: 16413
Epoch: [54]  [ 20/160]  eta: 0:02:41  lr: 0.000040  min_lr: 0.000010  loss: 1.4409 (1.5342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4401 (6.7853)  time: 0.8259 (0.5329 -- 2.8889)  data: 0.2461 (0.0002 -- 2.2344)  max mem: 16413
Epoch: [54]  [ 40/160]  eta: 0:02:11  lr: 0.000040  min_lr: 0.000010  loss: 1.5174 (1.5891)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6869 (7.1016)  time: 1.0363 (0.5251 -- 2.5840)  data: 0.3617 (0.0001 -- 2.0488)  max mem: 16413
Epoch: [54]  [ 60/160]  eta: 0:01:42  lr: 0.000040  min_lr: 0.000010  loss: 1.5776 (1.5539)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7689 (6.9990)  time: 0.8819 (0.5270 -- 3.5096)  data: 0.3068 (0.0004 -- 2.9904)  max mem: 16413
[2023-09-23 03:33:54,143] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:33:54,143] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:33:54,144] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:33:54,144] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [54]  [ 80/160]  eta: 0:01:19  lr: 0.000040  min_lr: 0.000010  loss: 1.6138 (1.5732)  loss_scale: 32768.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4952 (6.7161)  time: 0.9160 (0.5191 -- 2.8225)  data: 0.1936 (0.0001 -- 2.2810)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:58  lr: 0.000040  min_lr: 0.000010  loss: 1.6025 (1.5777)  loss_scale: 32768.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5026 (6.7199)  time: 0.8677 (0.5284 -- 2.5882)  data: 0.1788 (0.0007 -- 1.3176)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:38  lr: 0.000040  min_lr: 0.000010  loss: 1.4970 (1.5841)  loss_scale: 32768.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8492 (6.6368)  time: 0.9439 (0.5227 -- 2.4656)  data: 0.1845 (0.0005 -- 1.9543)  max mem: 16413
[2023-09-23 03:34:59,450] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8777
[2023-09-23 03:34:59,450] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8777
[2023-09-23 03:34:59,450] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:34:59,450] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:34:59,450] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [54]  [140/160]  eta: 0:00:19  lr: 0.000040  min_lr: 0.000010  loss: 1.6108 (1.5906)  loss_scale: 32768.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5018 (6.9482)  time: 0.9010 (0.5179 -- 3.0208)  data: 0.2263 (0.0003 -- 2.5052)  max mem: 16413
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.5305 (1.5775)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4444 (6.9098)  time: 0.7612 (0.4949 -- 1.7096)  data: 0.1976 (0.0002 -- 1.1989)  max mem: 16413
Epoch: [54] Total time: 0:02:29 (0.9364 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.5305 (1.5683)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4444 (6.9098)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4471 (0.4471)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4188 (2.4188 -- 2.4188)  data: 2.2299 (2.2299 -- 2.2299)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2846 (0.5166)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (95.9596)  time: 0.4656 (0.1727 -- 2.4188)  data: 0.2789 (0.0002 -- 2.2299)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2846 (0.4538)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (97.3545)  time: 0.2523 (0.1688 -- 1.0284)  data: 0.0675 (0.0001 -- 0.8317)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3682 (0.5079)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.5104)  time: 0.2780 (0.1326 -- 1.0284)  data: 0.1003 (0.0001 -- 0.8317)  max mem: 16413
Val: Total time: 0:00:09 (0.3371 s / it)
* Acc@1 88.174 Acc@5 98.755 loss 0.454
Accuracy of the network on the 482 val images: 88.17%
[2023-09-23 03:35:25,272] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 03:35:25,274] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 03:35:25,274] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 03:35:25,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 03:35:26,658] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 03:35:26,659] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 88.17%
Epoch: [55]  [  0/160]  eta: 0:23:44  lr: 0.000040  min_lr: 0.000010  loss: 1.5647 (1.5647)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2306 (7.2306)  time: 8.9046 (8.9046 -- 8.9046)  data: 8.3670 (8.3670 -- 8.3670)  max mem: 16413
Epoch: [55]  [ 20/160]  eta: 0:02:44  lr: 0.000040  min_lr: 0.000010  loss: 1.4750 (1.4854)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6728 (7.4126)  time: 0.7891 (0.5247 -- 3.8959)  data: 0.2335 (0.0003 -- 3.3488)  max mem: 16413
Epoch: [55]  [ 40/160]  eta: 0:02:13  lr: 0.000040  min_lr: 0.000010  loss: 1.6389 (1.5675)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8194 (7.2794)  time: 1.0533 (0.5215 -- 4.3845)  data: 0.5046 (0.0002 -- 3.8417)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:42  lr: 0.000040  min_lr: 0.000010  loss: 1.6367 (1.5812)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6513 (7.4826)  time: 0.8433 (0.5138 -- 4.9952)  data: 0.3047 (0.0003 -- 4.4490)  max mem: 16413
[2023-09-23 03:36:45,328] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8878
[2023-09-23 03:36:45,329] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 03:36:45,329] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8878
[2023-09-23 03:36:45,329] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 03:36:45,329] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [55]  [ 80/160]  eta: 0:01:20  lr: 0.000040  min_lr: 0.000010  loss: 1.4130 (1.5431)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2985 (7.3077)  time: 0.9605 (0.5216 -- 2.9317)  data: 0.3840 (0.0003 -- 2.1986)  max mem: 16413
Epoch: [55]  [100/160]  eta: 0:00:59  lr: 0.000040  min_lr: 0.000010  loss: 1.3984 (1.5366)  loss_scale: 8192.0000 (14518.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4942 (7.2410)  time: 0.8851 (0.5171 -- 2.6346)  data: 0.2108 (0.0002 -- 1.7673)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:38  lr: 0.000039  min_lr: 0.000010  loss: 1.5266 (1.5477)  loss_scale: 8192.0000 (13472.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4393 (7.3345)  time: 0.9162 (0.5165 -- 3.5371)  data: 0.1352 (0.0003 -- 1.5066)  max mem: 16413
Epoch: [55]  [140/160]  eta: 0:00:19  lr: 0.000039  min_lr: 0.000010  loss: 1.7688 (1.5734)  loss_scale: 8192.0000 (12723.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7471 (7.3493)  time: 0.9615 (0.5175 -- 3.6861)  data: 0.0338 (0.0003 -- 0.6553)  max mem: 16413
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.5138 (1.5699)  loss_scale: 8192.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0348 (7.3246)  time: 0.6822 (0.4922 -- 2.6079)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [55] Total time: 0:02:30 (0.9388 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.5138 (1.5662)  loss_scale: 8192.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0348 (7.3246)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4735 (0.4735)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3576 (2.3576 -- 2.3576)  data: 2.1538 (2.1538 -- 2.1538)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4132 (0.5940)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4643 (0.1751 -- 2.3576)  data: 0.2716 (0.0002 -- 2.1538)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3687 (0.4940)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (99.4709)  time: 0.2542 (0.1688 -- 1.0170)  data: 0.0661 (0.0001 -- 0.8117)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4257 (0.5351)  acc1: 85.7143 (83.8174)  acc5: 100.0000 (99.1701)  time: 0.2822 (0.1348 -- 1.0170)  data: 0.1004 (0.0001 -- 0.8117)  max mem: 16413
Val: Total time: 0:00:09 (0.3404 s / it)
* Acc@1 86.722 Acc@5 99.585 loss 0.465
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 88.17%
Epoch: [56]  [  0/160]  eta: 0:22:59  lr: 0.000039  min_lr: 0.000010  loss: 1.4930 (1.4930)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6075 (4.6075)  time: 8.6224 (8.6224 -- 8.6224)  data: 6.5179 (6.5179 -- 6.5179)  max mem: 16413
Epoch: [56]  [ 20/160]  eta: 0:02:49  lr: 0.000039  min_lr: 0.000010  loss: 1.6013 (1.5601)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0324 (6.7614)  time: 0.8369 (0.5220 -- 4.2625)  data: 0.2277 (0.0004 -- 2.3633)  max mem: 16413
[2023-09-23 03:38:49,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[9.999403491949061e-06, 9.999403491949061e-06, 1.1110448324387846e-05, 1.1110448324387846e-05, 1.234494258265316e-05, 1.234494258265316e-05, 1.3716602869614624e-05, 1.3716602869614624e-05, 1.524066985512736e-05, 1.524066985512736e-05, 1.6934077616808177e-05, 1.6934077616808177e-05, 1.881564179645353e-05, 1.881564179645353e-05, 2.0906268662726143e-05, 2.0906268662726143e-05, 2.3229187403029046e-05, 2.3229187403029046e-05, 2.5810208225587826e-05, 2.5810208225587826e-05, 2.8678009139542034e-05, 2.8678009139542034e-05, 3.186445459949115e-05, 3.186445459949115e-05, 3.540494955499016e-05, 3.540494955499016e-05, 3.9338832838877956e-05, 3.9338832838877956e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 03:38:49,450] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=17.560813377179812, CurrSamplesPerSec=21.632872981843253, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:18  lr: 0.000039  min_lr: 0.000010  loss: 1.4820 (1.4970)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5072 (7.0784)  time: 1.0907 (0.5115 -- 4.5764)  data: 0.5373 (0.0006 -- 4.0426)  max mem: 16413
[2023-09-23 03:38:57,332] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:38:57,333] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 03:38:57,336] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:38:57,336] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [56]  [ 60/160]  eta: 0:01:43  lr: 0.000039  min_lr: 0.000010  loss: 1.5240 (1.4897)  loss_scale: 16384.0000 (10072.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7343 (7.0113)  time: 0.7835 (0.5200 -- 3.4353)  data: 0.2385 (0.0004 -- 2.9230)  max mem: 16413
Epoch: [56]  [ 80/160]  eta: 0:01:20  lr: 0.000039  min_lr: 0.000010  loss: 1.6528 (1.5021)  loss_scale: 16384.0000 (11630.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9653 (7.1784)  time: 0.9513 (0.5266 -- 3.8885)  data: 0.3600 (0.0003 -- 3.3410)  max mem: 16413
Epoch: [56]  [100/160]  eta: 0:00:59  lr: 0.000039  min_lr: 0.000010  loss: 1.3296 (1.4865)  loss_scale: 16384.0000 (12571.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4840 (7.1504)  time: 0.9256 (0.5241 -- 4.7672)  data: 0.3778 (0.0006 -- 4.2342)  max mem: 16413
Epoch: [56]  [120/160]  eta: 0:00:39  lr: 0.000039  min_lr: 0.000010  loss: 1.5610 (1.5055)  loss_scale: 16384.0000 (13201.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0242 (7.0703)  time: 0.9560 (0.5111 -- 4.6427)  data: 0.4156 (0.0003 -- 4.1104)  max mem: 16413
Epoch: [56]  [140/160]  eta: 0:00:19  lr: 0.000039  min_lr: 0.000010  loss: 1.3763 (1.5044)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7047 (7.0501)  time: 0.8213 (0.5149 -- 3.2777)  data: 0.2792 (0.0001 -- 2.7536)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.7000 (1.5201)  loss_scale: 16384.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0986 (6.9765)  time: 0.7775 (0.4949 -- 2.2282)  data: 0.1592 (0.0002 -- 1.2073)  max mem: 16413
Epoch: [56] Total time: 0:02:29 (0.9360 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.7000 (1.5066)  loss_scale: 16384.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0986 (6.9765)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2200 (0.2200)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5118 (2.5118 -- 2.5118)  data: 2.3229 (2.3229 -- 2.3229)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3652 (0.5542)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4859 (0.1763 -- 2.5118)  data: 0.2999 (0.0002 -- 2.3229)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3379 (0.4599)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (99.4709)  time: 0.2574 (0.1679 -- 1.1418)  data: 0.0720 (0.0001 -- 0.9587)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3652 (0.5213)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (99.1701)  time: 0.2947 (0.1323 -- 1.1418)  data: 0.1162 (0.0001 -- 0.9587)  max mem: 16413
Val: Total time: 0:00:09 (0.3532 s / it)
* Acc@1 86.722 Acc@5 99.378 loss 0.479
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 88.17%
Epoch: [57]  [  0/160]  eta: 0:23:59  lr: 0.000039  min_lr: 0.000010  loss: 1.5580 (1.5580)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0580 (7.0580)  time: 8.9973 (8.9973 -- 8.9973)  data: 6.8093 (6.8093 -- 6.8093)  max mem: 16413
[2023-09-23 03:41:05,531] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:41:05,531] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:41:05,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:41:05,535] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [ 20/160]  eta: 0:03:00  lr: 0.000039  min_lr: 0.000010  loss: 1.5741 (1.5651)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6492 (7.4021)  time: 0.9039 (0.5196 -- 5.0090)  data: 0.0733 (0.0002 -- 0.9897)  max mem: 16413
[2023-09-23 03:41:21,989] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9150
[2023-09-23 03:41:21,989] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9150
[2023-09-23 03:41:21,989] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:41:21,989] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:41:21,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [57]  [ 40/160]  eta: 0:02:12  lr: 0.000039  min_lr: 0.000010  loss: 1.4864 (1.5204)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1488 (6.9998)  time: 0.9182 (0.5129 -- 4.4512)  data: 0.0563 (0.0003 -- 1.0973)  max mem: 16413
Epoch: [57]  [ 60/160]  eta: 0:01:44  lr: 0.000039  min_lr: 0.000010  loss: 1.5254 (1.5039)  loss_scale: 16384.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1925 (6.8396)  time: 0.9220 (0.5217 -- 4.6637)  data: 0.3755 (0.0006 -- 4.1477)  max mem: 16413
Epoch: [57]  [ 80/160]  eta: 0:01:22  lr: 0.000039  min_lr: 0.000010  loss: 1.5022 (1.5214)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1424 (7.0496)  time: 0.9769 (0.5170 -- 3.9208)  data: 0.4400 (0.0002 -- 3.4021)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:59  lr: 0.000039  min_lr: 0.000010  loss: 1.6284 (1.5342)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6846 (7.0329)  time: 0.8217 (0.5152 -- 3.7194)  data: 0.1903 (0.0001 -- 2.4278)  max mem: 16413
Epoch: [57]  [120/160]  eta: 0:00:38  lr: 0.000039  min_lr: 0.000010  loss: 1.5383 (1.5320)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6067 (7.0324)  time: 0.8859 (0.5199 -- 4.3660)  data: 0.2418 (0.0004 -- 2.6252)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:19  lr: 0.000039  min_lr: 0.000010  loss: 1.6062 (1.5422)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1422 (6.9778)  time: 0.9092 (0.5246 -- 5.3888)  data: 0.3236 (0.0004 -- 4.8236)  max mem: 16413
[2023-09-23 03:43:15,621] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:43:15,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:43:15,621] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:43:15,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.6732 (1.5405)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8677 (6.9716)  time: 0.7357 (0.4957 -- 4.1387)  data: 0.2177 (0.0001 -- 3.6115)  max mem: 16413
Epoch: [57] Total time: 0:02:29 (0.9374 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.6732 (1.5472)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8677 (6.9716)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3107 (0.3107)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4724 (2.4724 -- 2.4724)  data: 2.2414 (2.2414 -- 2.2414)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.5159 (0.5942)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (100.0000)  time: 0.4803 (0.1787 -- 2.4724)  data: 0.2845 (0.0003 -- 2.2414)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5159 (0.5079)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (100.0000)  time: 0.2554 (0.1682 -- 1.0901)  data: 0.0672 (0.0001 -- 0.8741)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5427 (0.5479)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (99.5851)  time: 0.2777 (0.1329 -- 1.0901)  data: 0.0969 (0.0001 -- 0.8741)  max mem: 16413
Val: Total time: 0:00:09 (0.3397 s / it)
* Acc@1 85.892 Acc@5 99.585 loss 0.498
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 88.17%
Epoch: [58]  [  0/160]  eta: 0:20:50  lr: 0.000039  min_lr: 0.000010  loss: 1.1168 (1.1168)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4168 (5.4168)  time: 7.8165 (7.8165 -- 7.8165)  data: 7.2647 (7.2647 -- 7.2647)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:41  lr: 0.000039  min_lr: 0.000010  loss: 1.3049 (1.3637)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6063 (7.0341)  time: 0.8167 (0.5156 -- 3.6296)  data: 0.2191 (0.0006 -- 2.9897)  max mem: 16413
[2023-09-23 03:44:05,042] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9318
[2023-09-23 03:44:05,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:44:05,042] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9318
[2023-09-23 03:44:05,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:44:05,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [ 40/160]  eta: 0:02:03  lr: 0.000039  min_lr: 0.000010  loss: 1.5723 (1.4632)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8133 (7.2952)  time: 0.9060 (0.5231 -- 3.5524)  data: 0.2198 (0.0001 -- 2.1707)  max mem: 16413
Epoch: [58]  [ 60/160]  eta: 0:01:39  lr: 0.000039  min_lr: 0.000010  loss: 1.5365 (1.4612)  loss_scale: 16384.0000 (26590.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0965 (7.1474)  time: 0.9167 (0.5257 -- 2.6013)  data: 0.2806 (0.0004 -- 2.0758)  max mem: 16413
Epoch: [58]  [ 80/160]  eta: 0:01:18  lr: 0.000039  min_lr: 0.000010  loss: 1.4865 (1.4793)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1758 (7.1910)  time: 0.9645 (0.5220 -- 2.6373)  data: 0.1960 (0.0002 -- 2.1086)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:59  lr: 0.000039  min_lr: 0.000010  loss: 1.5924 (1.5014)  loss_scale: 16384.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7585 (7.1161)  time: 1.0211 (0.5198 -- 4.4128)  data: 0.2104 (0.0003 -- 2.3158)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:38  lr: 0.000039  min_lr: 0.000010  loss: 1.5414 (1.5112)  loss_scale: 16384.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0401 (7.2003)  time: 0.7554 (0.5249 -- 2.6049)  data: 0.0021 (0.0003 -- 0.0085)  max mem: 16413
Epoch: [58]  [140/160]  eta: 0:00:19  lr: 0.000039  min_lr: 0.000010  loss: 1.5433 (1.5203)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9447 (7.2242)  time: 1.0750 (0.5114 -- 4.7826)  data: 0.0013 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.6568 (1.5283)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0567 (7.2267)  time: 0.6327 (0.4927 -- 2.5187)  data: 0.0008 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [58] Total time: 0:02:29 (0.9316 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.6568 (1.5310)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0567 (7.2267)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2640 (0.2640)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4588 (2.4588 -- 2.4588)  data: 2.2631 (2.2631 -- 2.2631)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3080 (0.5054)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4827 (0.1790 -- 2.4588)  data: 0.2874 (0.0004 -- 2.2631)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3080 (0.4288)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (99.4709)  time: 0.2545 (0.1694 -- 1.0800)  data: 0.0635 (0.0001 -- 0.8912)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3160 (0.4858)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (99.1701)  time: 0.2763 (0.1322 -- 1.0800)  data: 0.0971 (0.0001 -- 0.8912)  max mem: 16413
Val: Total time: 0:00:09 (0.3411 s / it)
* Acc@1 86.722 Acc@5 99.585 loss 0.452
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 88.17%
Epoch: [59]  [  0/160]  eta: 0:18:47  lr: 0.000039  min_lr: 0.000010  loss: 1.8208 (1.8208)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6179 (8.6179)  time: 7.0498 (7.0498 -- 7.0498)  data: 6.5029 (6.5029 -- 6.5029)  max mem: 16413
[2023-09-23 03:46:16,036] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:46:16,036] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:46:16,039] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:46:16,039] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [59]  [ 20/160]  eta: 0:02:44  lr: 0.000039  min_lr: 0.000010  loss: 1.4043 (1.4818)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8206 (6.7609)  time: 0.8804 (0.5247 -- 2.4781)  data: 0.1422 (0.0003 -- 1.4357)  max mem: 16413
[2023-09-23 03:46:46,486] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9479
[2023-09-23 03:46:46,486] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9479
[2023-09-23 03:46:46,486] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:46:46,486] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:46:46,486] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [59]  [ 40/160]  eta: 0:02:09  lr: 0.000038  min_lr: 0.000010  loss: 1.5185 (1.5362)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5897 (6.7045)  time: 0.9733 (0.5155 -- 4.2827)  data: 0.0973 (0.0003 -- 1.5414)  max mem: 16413
Epoch: [59]  [ 60/160]  eta: 0:01:42  lr: 0.000038  min_lr: 0.000010  loss: 1.5061 (1.5496)  loss_scale: 16384.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8392 (7.0743)  time: 0.9307 (0.5103 -- 3.5266)  data: 0.0766 (0.0003 -- 1.2178)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:19  lr: 0.000038  min_lr: 0.000010  loss: 1.2794 (1.4904)  loss_scale: 16384.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1360 (7.0530)  time: 0.8998 (0.5200 -- 2.7632)  data: 0.1224 (0.0004 -- 1.1133)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:01:00  lr: 0.000038  min_lr: 0.000010  loss: 1.5865 (1.5103)  loss_scale: 16384.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1146 (7.1696)  time: 1.0411 (0.5180 -- 4.2984)  data: 0.0174 (0.0003 -- 0.3251)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:38  lr: 0.000038  min_lr: 0.000010  loss: 1.5294 (1.5085)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5737 (7.1965)  time: 0.7488 (0.5323 -- 2.8903)  data: 0.0103 (0.0007 -- 0.1551)  max mem: 16413
Epoch: [59]  [140/160]  eta: 0:00:19  lr: 0.000038  min_lr: 0.000010  loss: 1.5637 (1.5145)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8427 (7.3425)  time: 0.9128 (0.5301 -- 2.5452)  data: 0.0347 (0.0003 -- 0.5123)  max mem: 16413
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.5431 (1.5120)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0556 (7.2980)  time: 0.7414 (0.4925 -- 2.0814)  data: 0.0681 (0.0003 -- 1.3423)  max mem: 16413
Epoch: [59] Total time: 0:02:29 (0.9319 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.5431 (1.4809)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0556 (7.2980)
[2023-09-23 03:48:32,373] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-09-23 03:48:32,375] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-09-23 03:48:32,375] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-09-23 03:48:32,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-09-23 03:48:33,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-09-23 03:48:33,295] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3244 (0.3244)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3913 (2.3913 -- 2.3913)  data: 2.1950 (2.1950 -- 2.1950)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3248 (0.5161)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4800 (0.1745 -- 2.3913)  data: 0.2895 (0.0003 -- 2.1950)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3248 (0.4482)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (100.0000)  time: 0.2636 (0.1683 -- 1.1652)  data: 0.0783 (0.0001 -- 0.9847)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3396 (0.4803)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (99.5851)  time: 0.2832 (0.1323 -- 1.1652)  data: 0.1078 (0.0001 -- 0.9847)  max mem: 16413
Val: Total time: 0:00:09 (0.3422 s / it)
* Acc@1 87.137 Acc@5 99.378 loss 0.463
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 88.17%
Epoch: [60]  [  0/160]  eta: 0:18:38  lr: 0.000038  min_lr: 0.000010  loss: 1.7531 (1.7531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4916 (5.4916)  time: 6.9898 (6.9898 -- 6.9898)  data: 5.5584 (5.5584 -- 5.5584)  max mem: 16413
[2023-09-23 03:48:57,340] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:48:57,340] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:48:57,342] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:48:57,342] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [60]  [ 20/160]  eta: 0:02:54  lr: 0.000038  min_lr: 0.000010  loss: 1.5514 (1.5743)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2140 (7.7010)  time: 0.9577 (0.5274 -- 2.6901)  data: 0.2569 (0.0004 -- 1.5398)  max mem: 16413
[2023-09-23 03:49:15,091] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9626
[2023-09-23 03:49:15,091] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:49:15,091] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9626
[2023-09-23 03:49:15,091] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 03:49:15,092] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [60]  [ 40/160]  eta: 0:02:05  lr: 0.000038  min_lr: 0.000010  loss: 1.3763 (1.5581)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7823 (7.3278)  time: 0.8357 (0.5245 -- 3.7599)  data: 0.0012 (0.0001 -- 0.0040)  max mem: 16413
Epoch: [60]  [ 60/160]  eta: 0:01:41  lr: 0.000038  min_lr: 0.000010  loss: 1.4932 (1.5679)  loss_scale: 16384.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4143 (7.1316)  time: 0.9500 (0.5119 -- 3.9957)  data: 0.0012 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [60]  [ 80/160]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000010  loss: 1.6477 (1.5984)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5970 (7.0382)  time: 0.8862 (0.5200 -- 5.3816)  data: 0.0015 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:58  lr: 0.000038  min_lr: 0.000010  loss: 1.5420 (1.5847)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5188 (7.1593)  time: 0.9225 (0.5219 -- 3.8915)  data: 0.0024 (0.0008 -- 0.0092)  max mem: 16413
Epoch: [60]  [120/160]  eta: 0:00:38  lr: 0.000038  min_lr: 0.000010  loss: 1.5546 (1.5890)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4445 (7.1167)  time: 0.9227 (0.5125 -- 2.9844)  data: 0.3152 (0.0005 -- 2.4574)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:19  lr: 0.000038  min_lr: 0.000010  loss: 1.5733 (1.5830)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9774 (7.1024)  time: 0.8926 (0.5209 -- 2.1559)  data: 0.0520 (0.0004 -- 0.7723)  max mem: 16413
[2023-09-23 03:51:10,019] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:51:10,019] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:51:10,020] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:51:10,060] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.4968 (1.5724)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9305 (7.0365)  time: 0.7571 (0.4946 -- 2.0088)  data: 0.0010 (0.0001 -- 0.0058)  max mem: 16413
Epoch: [60] Total time: 0:02:28 (0.9312 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.4968 (1.5621)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9305 (7.0365)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4021 (0.4021)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3487 (2.3487 -- 2.3487)  data: 2.1402 (2.1402 -- 2.1402)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4737 (0.5828)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.4735 (0.1810 -- 2.3487)  data: 0.2772 (0.0005 -- 2.1402)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3355 (0.4639)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2594 (0.1691 -- 1.1149)  data: 0.0679 (0.0001 -- 0.9002)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4099 (0.5057)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2784 (0.1332 -- 1.1149)  data: 0.0954 (0.0001 -- 0.9002)  max mem: 16413
Val: Total time: 0:00:09 (0.3360 s / it)
* Acc@1 87.759 Acc@5 99.170 loss 0.468
Accuracy of the network on the 482 val images: 87.76%
Max accuracy: 88.17%
Epoch: [61]  [  0/160]  eta: 0:18:53  lr: 0.000038  min_lr: 0.000010  loss: 1.7418 (1.7418)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6357 (10.6357)  time: 7.0845 (7.0845 -- 7.0845)  data: 6.3097 (6.3097 -- 6.3097)  max mem: 16413
Epoch: [61]  [ 20/160]  eta: 0:03:03  lr: 0.000038  min_lr: 0.000010  loss: 1.2499 (1.3426)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3801 (7.0613)  time: 1.0237 (0.5162 -- 2.8312)  data: 0.3016 (0.0004 -- 2.3279)  max mem: 16413
[2023-09-23 03:52:05,478] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9800
[2023-09-23 03:52:05,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9800
[2023-09-23 03:52:05,478] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:52:05,478] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:52:05,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [61]  [ 40/160]  eta: 0:02:08  lr: 0.000038  min_lr: 0.000010  loss: 1.5387 (1.4507)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0447 (7.2821)  time: 0.8177 (0.5245 -- 2.1356)  data: 0.1221 (0.0004 -- 1.3772)  max mem: 16413
Epoch: [61]  [ 60/160]  eta: 0:01:42  lr: 0.000038  min_lr: 0.000010  loss: 1.4341 (1.4819)  loss_scale: 16384.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2977 (7.3322)  time: 0.9351 (0.5105 -- 3.1359)  data: 0.0208 (0.0004 -- 0.2190)  max mem: 16413
Epoch: [61]  [ 80/160]  eta: 0:01:19  lr: 0.000038  min_lr: 0.000010  loss: 1.5163 (1.4936)  loss_scale: 16384.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4711 (6.9919)  time: 0.8945 (0.5276 -- 3.3010)  data: 0.1330 (0.0005 -- 0.8436)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:59  lr: 0.000038  min_lr: 0.000010  loss: 1.4782 (1.5027)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4837 (6.8780)  time: 0.9689 (0.5247 -- 3.3386)  data: 0.3669 (0.0002 -- 2.8200)  max mem: 16413
Epoch: [61]  [120/160]  eta: 0:00:38  lr: 0.000038  min_lr: 0.000010  loss: 1.5325 (1.5077)  loss_scale: 16384.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1718 (6.9748)  time: 0.8347 (0.5220 -- 3.8065)  data: 0.2815 (0.0002 -- 3.2841)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:19  lr: 0.000038  min_lr: 0.000010  loss: 1.5242 (1.5183)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8830 (7.0762)  time: 1.0597 (0.5242 -- 4.8414)  data: 0.0746 (0.0005 -- 1.4641)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.5833 (1.5273)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7389 (7.0722)  time: 0.6759 (0.4933 -- 2.7301)  data: 0.0010 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [61] Total time: 0:02:30 (0.9422 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.5833 (1.5425)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7389 (7.0722)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3963 (0.3963)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5141 (2.5141 -- 2.5141)  data: 2.3066 (2.3066 -- 2.3066)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3375 (0.5942)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (100.0000)  time: 0.4734 (0.1782 -- 2.5141)  data: 0.2827 (0.0002 -- 2.3066)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3292 (0.4916)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (100.0000)  time: 0.2522 (0.1683 -- 0.9920)  data: 0.0671 (0.0001 -- 0.7978)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3489 (0.5150)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (100.0000)  time: 0.2765 (0.1321 -- 0.9920)  data: 0.0984 (0.0001 -- 0.7978)  max mem: 16413
Val: Total time: 0:00:09 (0.3395 s / it)
* Acc@1 87.344 Acc@5 99.585 loss 0.480
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 88.17%
Epoch: [62]  [  0/160]  eta: 0:21:48  lr: 0.000038  min_lr: 0.000010  loss: 1.5314 (1.5314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6428 (8.6428)  time: 8.1758 (8.1758 -- 8.1758)  data: 7.6580 (7.6580 -- 7.6580)  max mem: 16413
[2023-09-23 03:54:18,210] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:54:18,211] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 03:54:18,211] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:54:18,211] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [62]  [ 20/160]  eta: 0:03:01  lr: 0.000038  min_lr: 0.000010  loss: 1.5786 (1.5774)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1433 (6.7588)  time: 0.9549 (0.5253 -- 4.3736)  data: 0.2039 (0.0002 -- 3.7113)  max mem: 16413
Epoch: [62]  [ 40/160]  eta: 0:02:09  lr: 0.000038  min_lr: 0.000010  loss: 1.4986 (1.5181)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4407 (7.0957)  time: 0.8559 (0.5139 -- 4.3087)  data: 0.0014 (0.0001 -- 0.0041)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:42  lr: 0.000038  min_lr: 0.000010  loss: 1.4858 (1.5189)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7394 (6.7777)  time: 0.9089 (0.5207 -- 3.7394)  data: 0.1992 (0.0004 -- 3.2100)  max mem: 16413
[2023-09-23 03:55:20,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=54, lr=[9.539276537446474e-06, 9.539276537446474e-06, 1.0599196152718305e-05, 1.0599196152718305e-05, 1.1776884614131447e-05, 1.1776884614131447e-05, 1.3085427349034943e-05, 1.3085427349034943e-05, 1.4539363721149935e-05, 1.4539363721149935e-05, 1.6154848579055485e-05, 1.6154848579055485e-05, 1.794983175450609e-05, 1.794983175450609e-05, 1.9944257505006766e-05, 1.9944257505006766e-05, 2.2160286116674188e-05, 2.2160286116674188e-05, 2.4622540129637982e-05, 2.4622540129637982e-05, 2.7358377921819982e-05, 2.7358377921819982e-05, 3.0398197690911092e-05, 3.0398197690911092e-05, 3.377577521212343e-05, 3.377577521212343e-05, 3.752863912458159e-05, 3.752863912458159e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 03:55:20,232] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=17.4612450090485, CurrSamplesPerSec=22.01858635067333, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000010  loss: 1.4632 (1.5038)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1042 (6.8613)  time: 0.8361 (0.5298 -- 4.1530)  data: 0.0059 (0.0004 -- 0.0894)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:59  lr: 0.000037  min_lr: 0.000010  loss: 1.6742 (1.5264)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0725 (6.9611)  time: 1.0472 (0.5224 -- 4.3095)  data: 0.3057 (0.0006 -- 3.7805)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:38  lr: 0.000037  min_lr: 0.000010  loss: 1.7138 (1.5489)  loss_scale: 32768.0000 (31549.3554)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8293 (7.0021)  time: 0.8012 (0.5120 -- 2.1669)  data: 0.2039 (0.0005 -- 1.6380)  max mem: 16413
[2023-09-23 03:56:13,633] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:56:13,633] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 03:56:13,633] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:56:13,633] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [62]  [140/160]  eta: 0:00:19  lr: 0.000037  min_lr: 0.000010  loss: 1.5872 (1.5612)  loss_scale: 32768.0000 (32651.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8974 (6.9332)  time: 0.9536 (0.5253 -- 4.7695)  data: 0.4051 (0.0005 -- 4.2249)  max mem: 16413
[2023-09-23 03:56:18,073] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10061
[2023-09-23 03:56:18,073] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10061
[2023-09-23 03:56:18,073] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 03:56:18,073] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 03:56:18,074] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 1.5271 (1.5546)  loss_scale: 32768.0000 (32665.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3882 (6.9729)  time: 0.7019 (0.4970 -- 2.7726)  data: 0.0274 (0.0002 -- 0.5292)  max mem: 16413
Epoch: [62] Total time: 0:02:28 (0.9304 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 1.5271 (1.5138)  loss_scale: 32768.0000 (32665.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3882 (6.9729)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2716 (0.2716)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5658 (2.5658 -- 2.5658)  data: 2.3259 (2.3259 -- 2.3259)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3131 (0.5674)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4885 (0.1750 -- 2.5658)  data: 0.2925 (0.0004 -- 2.3259)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3131 (0.4683)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2556 (0.1707 -- 1.0996)  data: 0.0650 (0.0001 -- 0.8865)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3934 (0.5238)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.7552)  time: 0.2849 (0.1325 -- 1.0996)  data: 0.1010 (0.0001 -- 0.8865)  max mem: 16413
Val: Total time: 0:00:09 (0.3476 s / it)
* Acc@1 87.759 Acc@5 99.170 loss 0.471
Accuracy of the network on the 482 val images: 87.76%
Max accuracy: 88.17%
Epoch: [63]  [  0/160]  eta: 0:22:13  lr: 0.000037  min_lr: 0.000010  loss: 1.2393 (1.2393)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0019 (6.0019)  time: 8.3351 (8.3351 -- 8.3351)  data: 5.6089 (5.6089 -- 5.6089)  max mem: 16413
Epoch: [63]  [ 20/160]  eta: 0:02:46  lr: 0.000037  min_lr: 0.000009  loss: 1.5445 (1.4592)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6314 (5.9566)  time: 0.8302 (0.5242 -- 3.5661)  data: 0.1192 (0.0004 -- 2.0751)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:02:15  lr: 0.000037  min_lr: 0.000009  loss: 1.4775 (1.5106)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6144 (6.5773)  time: 1.0654 (0.5269 -- 5.1451)  data: 0.3287 (0.0002 -- 4.6197)  max mem: 16413
Epoch: [63]  [ 60/160]  eta: 0:01:42  lr: 0.000037  min_lr: 0.000009  loss: 1.4850 (1.5156)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5412 (6.6890)  time: 0.8093 (0.5203 -- 3.9250)  data: 0.0075 (0.0004 -- 0.1268)  max mem: 16413
Epoch: [63]  [ 80/160]  eta: 0:01:21  lr: 0.000037  min_lr: 0.000009  loss: 1.4860 (1.4946)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1433 (6.8897)  time: 0.9810 (0.5311 -- 4.6684)  data: 0.0019 (0.0005 -- 0.0054)  max mem: 16413
Epoch: [63]  [100/160]  eta: 0:00:59  lr: 0.000037  min_lr: 0.000009  loss: 1.5699 (1.5127)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9786 (6.9707)  time: 0.8637 (0.5126 -- 3.6466)  data: 0.0021 (0.0004 -- 0.0151)  max mem: 16413
[2023-09-23 03:58:27,168] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:58:27,168] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 03:58:27,169] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 03:58:27,169] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 03:58:34,168] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10198
[2023-09-23 03:58:34,168] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10198
[2023-09-23 03:58:34,169] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 03:58:34,169] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 03:58:34,169] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [63]  [120/160]  eta: 0:00:39  lr: 0.000037  min_lr: 0.000009  loss: 1.7335 (1.5270)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5392 (7.0532)  time: 1.0546 (0.5172 -- 5.4401)  data: 0.0355 (0.0001 -- 0.6864)  max mem: 16413
[2023-09-23 03:58:49,358] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10212
[2023-09-23 03:58:49,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10212
[2023-09-23 03:58:49,359] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:58:49,359] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 03:58:49,359] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [63]  [140/160]  eta: 0:00:19  lr: 0.000037  min_lr: 0.000009  loss: 1.7826 (1.5388)  loss_scale: 32768.0000 (33581.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6116 (7.2447)  time: 0.7706 (0.5112 -- 3.2022)  data: 0.0013 (0.0003 -- 0.0056)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.4653 (1.5253)  loss_scale: 16384.0000 (31539.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9168 (7.2080)  time: 0.7269 (0.4935 -- 4.6828)  data: 0.0012 (0.0002 -- 0.0056)  max mem: 16413
Epoch: [63] Total time: 0:02:29 (0.9364 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.4653 (1.5356)  loss_scale: 16384.0000 (31539.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9168 (7.2080)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1852 (0.1852)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4405 (2.4405 -- 2.4405)  data: 2.2510 (2.2510 -- 2.2510)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3213 (0.5760)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4759 (0.1759 -- 2.4405)  data: 0.2880 (0.0002 -- 2.2510)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3573 (0.4777)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (99.4709)  time: 0.2565 (0.1687 -- 1.1192)  data: 0.0716 (0.0001 -- 0.9129)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3682 (0.5212)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (99.1701)  time: 0.2908 (0.1331 -- 1.1192)  data: 0.1112 (0.0001 -- 0.9129)  max mem: 16413
Val: Total time: 0:00:09 (0.3465 s / it)
* Acc@1 87.967 Acc@5 99.170 loss 0.469
Accuracy of the network on the 482 val images: 87.97%
Max accuracy: 88.17%
Epoch: [64]  [  0/160]  eta: 0:21:10  lr: 0.000037  min_lr: 0.000009  loss: 1.2501 (1.2501)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2820 (7.2820)  time: 7.9385 (7.9385 -- 7.9385)  data: 5.6939 (5.6939 -- 5.6939)  max mem: 16413
Epoch: [64]  [ 20/160]  eta: 0:02:52  lr: 0.000037  min_lr: 0.000009  loss: 1.4924 (1.5912)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4629 (6.9645)  time: 0.8980 (0.5214 -- 3.0818)  data: 0.2219 (0.0004 -- 2.5615)  max mem: 16413
Epoch: [64]  [ 40/160]  eta: 0:02:12  lr: 0.000037  min_lr: 0.000009  loss: 1.6145 (1.5819)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2660 (6.8892)  time: 0.9658 (0.5200 -- 3.5239)  data: 0.2370 (0.0005 -- 2.2000)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:46  lr: 0.000037  min_lr: 0.000009  loss: 1.5575 (1.5748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7384 (7.0140)  time: 0.9806 (0.5083 -- 3.6570)  data: 0.4442 (0.0002 -- 3.1354)  max mem: 16413
Epoch: [64]  [ 80/160]  eta: 0:01:20  lr: 0.000037  min_lr: 0.000009  loss: 1.4072 (1.5385)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8701 (7.0556)  time: 0.8121 (0.5181 -- 3.4570)  data: 0.2723 (0.0003 -- 2.9094)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:58  lr: 0.000037  min_lr: 0.000009  loss: 1.4860 (1.5399)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6584 (7.0781)  time: 0.8937 (0.5174 -- 4.2751)  data: 0.3517 (0.0002 -- 3.7369)  max mem: 16413
[2023-09-23 04:00:58,463] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:00:58,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:00:58,463] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:00:58,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [64]  [120/160]  eta: 0:00:38  lr: 0.000037  min_lr: 0.000009  loss: 1.7415 (1.5686)  loss_scale: 32768.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8076 (7.0598)  time: 0.8488 (0.5139 -- 3.9125)  data: 0.3018 (0.0004 -- 3.3773)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.4091 (1.5473)  loss_scale: 32768.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1185 (6.9647)  time: 0.8958 (0.5334 -- 3.4877)  data: 0.3523 (0.0004 -- 2.9739)  max mem: 16413
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.3956 (1.5302)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9021 (6.9458)  time: 0.8051 (0.4953 -- 2.6903)  data: 0.2786 (0.0002 -- 2.1711)  max mem: 16413
Epoch: [64] Total time: 0:02:29 (0.9341 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.3956 (1.5252)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9021 (6.9458)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.0455 (1.0455)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4951 (2.4951 -- 2.4951)  data: 2.3061 (2.3061 -- 2.3061)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4351 (0.6028)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4617 (0.1775 -- 2.4951)  data: 0.2740 (0.0002 -- 2.3061)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3241 (0.4732)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (99.4709)  time: 0.2518 (0.1684 -- 0.8903)  data: 0.0663 (0.0001 -- 0.6897)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3935 (0.5143)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (99.1701)  time: 0.2811 (0.1328 -- 0.8951)  data: 0.1017 (0.0001 -- 0.7230)  max mem: 16413
Val: Total time: 0:00:09 (0.3421 s / it)
* Acc@1 86.100 Acc@5 99.378 loss 0.473
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 88.17%
Epoch: [65]  [  0/160]  eta: 0:18:17  lr: 0.000037  min_lr: 0.000009  loss: 0.9388 (0.9388)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1554 (6.1554)  time: 6.8615 (6.8615 -- 6.8615)  data: 6.3345 (6.3345 -- 6.3345)  max mem: 16413
Epoch: [65]  [ 20/160]  eta: 0:02:53  lr: 0.000037  min_lr: 0.000009  loss: 1.5907 (1.5067)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9257 (7.3045)  time: 0.9596 (0.5269 -- 2.4769)  data: 0.2746 (0.0008 -- 1.9489)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:14  lr: 0.000037  min_lr: 0.000009  loss: 1.2836 (1.4127)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8256 (7.0980)  time: 1.0021 (0.5204 -- 5.0309)  data: 0.1079 (0.0002 -- 1.3628)  max mem: 16413
[2023-09-23 04:02:59,444] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10457
[2023-09-23 04:02:59,444] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10457
[2023-09-23 04:02:59,444] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:02:59,444] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:02:59,444] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [65]  [ 60/160]  eta: 0:01:49  lr: 0.000037  min_lr: 0.000009  loss: 1.3619 (1.4196)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4099 (6.9846)  time: 1.0497 (0.5136 -- 5.1961)  data: 0.0011 (0.0004 -- 0.0023)  max mem: 16413
Epoch: [65]  [ 80/160]  eta: 0:01:22  lr: 0.000037  min_lr: 0.000009  loss: 1.7104 (1.4844)  loss_scale: 16384.0000 (27913.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8414 (7.0235)  time: 0.8296 (0.5139 -- 3.7963)  data: 0.0010 (0.0003 -- 0.0021)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:01:01  lr: 0.000037  min_lr: 0.000009  loss: 1.4364 (1.4806)  loss_scale: 16384.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4230 (7.0306)  time: 1.0329 (0.5131 -- 4.6470)  data: 0.0011 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [65]  [120/160]  eta: 0:00:40  lr: 0.000037  min_lr: 0.000009  loss: 1.6508 (1.4996)  loss_scale: 16384.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5805 (7.0753)  time: 0.8669 (0.5220 -- 3.8386)  data: 0.0010 (0.0003 -- 0.0017)  max mem: 16413
Epoch: [65]  [140/160]  eta: 0:00:19  lr: 0.000036  min_lr: 0.000009  loss: 1.2802 (1.4865)  loss_scale: 16384.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7878 (7.1007)  time: 0.9051 (0.5207 -- 4.3669)  data: 0.0012 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.6789 (1.5094)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7337 (7.0788)  time: 0.7026 (0.4945 -- 2.8453)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [65] Total time: 0:02:33 (0.9583 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.6789 (1.5203)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7337 (7.0788)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5843 (0.5843)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3708 (2.3708 -- 2.3708)  data: 2.1828 (2.1828 -- 2.1828)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4431 (0.5338)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4702 (0.1757 -- 2.3708)  data: 0.2807 (0.0002 -- 2.1828)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3114 (0.4482)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (99.4709)  time: 0.2597 (0.1688 -- 1.0830)  data: 0.0732 (0.0001 -- 0.8931)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3669 (0.5072)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (99.1701)  time: 0.2990 (0.1323 -- 1.1186)  data: 0.1201 (0.0001 -- 0.9484)  max mem: 16413
Val: Total time: 0:00:09 (0.3519 s / it)
* Acc@1 87.552 Acc@5 99.378 loss 0.453
Accuracy of the network on the 482 val images: 87.55%
Max accuracy: 88.17%
Epoch: [66]  [  0/160]  eta: 0:23:57  lr: 0.000036  min_lr: 0.000009  loss: 1.2486 (1.2486)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4478 (8.4478)  time: 8.9864 (8.9864 -- 8.9864)  data: 5.1655 (5.1655 -- 5.1655)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:52  lr: 0.000036  min_lr: 0.000009  loss: 1.4010 (1.4131)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9569 (7.4143)  time: 0.8439 (0.5277 -- 4.4554)  data: 0.0561 (0.0003 -- 1.0913)  max mem: 16413
[2023-09-23 04:05:12,548] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:05:12,548] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:05:12,550] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:05:12,550] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [66]  [ 40/160]  eta: 0:02:11  lr: 0.000036  min_lr: 0.000009  loss: 1.4326 (1.4327)  loss_scale: 32768.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3905 (7.2534)  time: 0.9516 (0.5229 -- 3.4337)  data: 0.0580 (0.0003 -- 0.7008)  max mem: 16413
Epoch: [66]  [ 60/160]  eta: 0:01:43  lr: 0.000036  min_lr: 0.000009  loss: 1.3701 (1.4465)  loss_scale: 32768.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9363 (7.0596)  time: 0.9249 (0.5201 -- 2.8210)  data: 0.3793 (0.0005 -- 2.3017)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:21  lr: 0.000036  min_lr: 0.000009  loss: 1.7037 (1.5163)  loss_scale: 32768.0000 (27508.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2748 (6.9735)  time: 0.9766 (0.5130 -- 3.7922)  data: 0.0410 (0.0003 -- 0.7916)  max mem: 16413
Epoch: [66]  [100/160]  eta: 0:00:59  lr: 0.000036  min_lr: 0.000009  loss: 1.3665 (1.4931)  loss_scale: 32768.0000 (28550.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2573 (6.9103)  time: 0.8924 (0.5138 -- 4.1076)  data: 0.0020 (0.0003 -- 0.0174)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:40  lr: 0.000036  min_lr: 0.000009  loss: 1.6203 (1.5113)  loss_scale: 32768.0000 (29247.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9478 (6.9175)  time: 1.0283 (0.5139 -- 5.4599)  data: 0.0013 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [66]  [140/160]  eta: 0:00:19  lr: 0.000036  min_lr: 0.000009  loss: 1.4980 (1.5166)  loss_scale: 32768.0000 (29746.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5591 (7.0179)  time: 0.8710 (0.5112 -- 4.2342)  data: 0.0009 (0.0003 -- 0.0025)  max mem: 16413
[2023-09-23 04:07:09,625] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10714
[2023-09-23 04:07:09,625] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10714
[2023-09-23 04:07:09,625] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:07:09,625] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:07:09,625] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.3253 (1.5002)  loss_scale: 32768.0000 (29491.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3377 (6.9975)  time: 0.6599 (0.4819 -- 1.8675)  data: 0.0008 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [66] Total time: 0:02:31 (0.9467 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.3253 (1.5398)  loss_scale: 32768.0000 (29491.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3377 (6.9975)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2182 (0.2182)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4718 (2.4718 -- 2.4718)  data: 2.2667 (2.2667 -- 2.2667)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2736 (0.4795)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4711 (0.1824 -- 2.4718)  data: 0.2780 (0.0005 -- 2.2667)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2736 (0.4201)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (99.4709)  time: 0.2525 (0.1678 -- 0.9881)  data: 0.0647 (0.0001 -- 0.7848)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3552 (0.4778)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (99.1701)  time: 0.2828 (0.1326 -- 0.9881)  data: 0.1025 (0.0001 -- 0.7848)  max mem: 16413
Val: Total time: 0:00:09 (0.3434 s / it)
* Acc@1 87.344 Acc@5 99.378 loss 0.463
Accuracy of the network on the 482 val images: 87.34%
Max accuracy: 88.17%
Epoch: [67]  [  0/160]  eta: 0:19:08  lr: 0.000036  min_lr: 0.000009  loss: 1.7790 (1.7790)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2437 (4.2437)  time: 7.1810 (7.1810 -- 7.1810)  data: 5.9351 (5.9351 -- 5.9351)  max mem: 16413
Epoch: [67]  [ 20/160]  eta: 0:02:47  lr: 0.000036  min_lr: 0.000009  loss: 1.5033 (1.5231)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9774 (6.9457)  time: 0.9009 (0.5299 -- 2.3134)  data: 0.1337 (0.0004 -- 1.5982)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:02:12  lr: 0.000036  min_lr: 0.000009  loss: 1.5491 (1.5526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6492 (6.9619)  time: 1.0007 (0.5216 -- 4.2179)  data: 0.0587 (0.0003 -- 1.1485)  max mem: 16413
Epoch: [67]  [ 60/160]  eta: 0:01:48  lr: 0.000036  min_lr: 0.000009  loss: 1.4329 (1.5102)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5848 (7.1217)  time: 1.0378 (0.5115 -- 4.8185)  data: 0.0016 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [67]  [ 80/160]  eta: 0:01:20  lr: 0.000036  min_lr: 0.000009  loss: 1.4467 (1.4970)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8970 (7.1826)  time: 0.7663 (0.5203 -- 3.8688)  data: 0.0015 (0.0001 -- 0.0060)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:58  lr: 0.000036  min_lr: 0.000009  loss: 1.5915 (1.5085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8380 (7.2004)  time: 0.8906 (0.5316 -- 2.7119)  data: 0.1122 (0.0004 -- 2.2026)  max mem: 16413
Epoch: [67]  [120/160]  eta: 0:00:38  lr: 0.000036  min_lr: 0.000009  loss: 1.5317 (1.5036)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1026 (7.1889)  time: 0.8903 (0.5211 -- 2.8107)  data: 0.1730 (0.0002 -- 1.8056)  max mem: 16413
[2023-09-23 04:09:21,563] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:09:21,563] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:09:21,563] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:09:21,563] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [67]  [140/160]  eta: 0:00:19  lr: 0.000036  min_lr: 0.000009  loss: 1.5094 (1.4954)  loss_scale: 32768.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2891 (7.2203)  time: 0.9697 (0.5088 -- 3.8281)  data: 0.0704 (0.0003 -- 1.3816)  max mem: 16413
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.5686 (1.4995)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7512 (7.2059)  time: 0.7410 (0.4945 -- 3.2779)  data: 0.0007 (0.0001 -- 0.0017)  max mem: 16413
Epoch: [67] Total time: 0:02:30 (0.9413 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.5686 (1.5102)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7512 (7.2059)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.5803 (0.5803)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3155 (2.3155 -- 2.3155)  data: 2.1210 (2.1210 -- 2.1210)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4343 (0.5982)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4577 (0.1769 -- 2.3155)  data: 0.2709 (0.0002 -- 2.1210)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4362 (0.5029)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2594 (0.1692 -- 1.0386)  data: 0.0718 (0.0001 -- 0.8539)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4576 (0.5460)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (99.5851)  time: 0.2883 (0.1323 -- 1.0386)  data: 0.1071 (0.0001 -- 0.8539)  max mem: 16413
Val: Total time: 0:00:09 (0.3406 s / it)
* Acc@1 86.929 Acc@5 99.378 loss 0.471
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 88.17%
Epoch: [68]  [  0/160]  eta: 0:23:32  lr: 0.000036  min_lr: 0.000009  loss: 1.4491 (1.4491)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5424 (5.5424)  time: 8.8307 (8.8307 -- 8.8307)  data: 6.3788 (6.3788 -- 6.3788)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:03:01  lr: 0.000036  min_lr: 0.000009  loss: 1.3430 (1.3635)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1837 (6.9997)  time: 0.9199 (0.5144 -- 3.4480)  data: 0.3063 (0.0003 -- 2.4554)  max mem: 16413
Epoch: [68]  [ 40/160]  eta: 0:02:11  lr: 0.000036  min_lr: 0.000009  loss: 1.5195 (1.4361)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2061 (7.0813)  time: 0.8842 (0.5247 -- 3.9308)  data: 0.3343 (0.0003 -- 3.4190)  max mem: 16413
Epoch: [68]  [ 60/160]  eta: 0:01:44  lr: 0.000036  min_lr: 0.000009  loss: 1.3217 (1.4329)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2217 (7.0649)  time: 0.9538 (0.5276 -- 3.6052)  data: 0.4054 (0.0004 -- 3.0761)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:21  lr: 0.000036  min_lr: 0.000009  loss: 1.5197 (1.4714)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8418 (6.9304)  time: 0.9048 (0.5186 -- 2.5245)  data: 0.1042 (0.0006 -- 1.3053)  max mem: 16413
[2023-09-23 04:11:33,270] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:11:33,270] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 04:11:33,271] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:11:33,271] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [68]  [100/160]  eta: 0:00:59  lr: 0.000036  min_lr: 0.000009  loss: 1.5359 (1.4827)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6025 (6.9215)  time: 0.9226 (0.5186 -- 4.1236)  data: 0.0698 (0.0003 -- 1.1304)  max mem: 16413
[2023-09-23 04:11:51,957] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10990
[2023-09-23 04:11:51,957] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10990
[2023-09-23 04:11:51,957] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 04:11:51,957] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 04:11:51,957] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-23 04:12:00,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=60, lr=[9.042925336597765e-06, 9.042925336597765e-06, 1.0047694818441962e-05, 1.0047694818441962e-05, 1.1164105353824401e-05, 1.1164105353824401e-05, 1.2404561504249335e-05, 1.2404561504249335e-05, 1.3782846115832595e-05, 1.3782846115832595e-05, 1.5314273462036215e-05, 1.5314273462036215e-05, 1.701585940226246e-05, 1.701585940226246e-05, 1.890651044695829e-05, 1.890651044695829e-05, 2.1007233829953654e-05, 2.1007233829953654e-05, 2.3341370922170724e-05, 2.3341370922170724e-05, 2.59348565801897e-05, 2.59348565801897e-05, 2.8816507311321884e-05, 2.8816507311321884e-05, 3.2018341457024315e-05, 3.2018341457024315e-05, 3.557593495224924e-05, 3.557593495224924e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 04:12:00,609] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=17.30072280774118, CurrSamplesPerSec=22.652536411421043, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:39  lr: 0.000036  min_lr: 0.000009  loss: 1.6126 (1.5003)  loss_scale: 32768.0000 (37913.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0775 (6.9914)  time: 0.9419 (0.5204 -- 4.3713)  data: 0.3213 (0.0003 -- 3.8585)  max mem: 16413
[2023-09-23 04:12:20,173] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11020
[2023-09-23 04:12:20,173] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:12:20,173] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11020
[2023-09-23 04:12:20,173] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:12:20,173] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [68]  [140/160]  eta: 0:00:19  lr: 0.000036  min_lr: 0.000009  loss: 1.6491 (1.5284)  loss_scale: 32768.0000 (37067.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2207 (7.1311)  time: 0.9512 (0.5218 -- 3.8396)  data: 0.4160 (0.0004 -- 3.2967)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.5429 (1.5263)  loss_scale: 16384.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5076 (7.1078)  time: 0.7061 (0.4945 -- 2.9887)  data: 0.1897 (0.0002 -- 2.4490)  max mem: 16413
Epoch: [68] Total time: 0:02:32 (0.9503 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.5429 (1.5218)  loss_scale: 16384.0000 (34611.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5076 (7.1078)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.4183 (0.4183)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5071 (2.5071 -- 2.5071)  data: 2.3094 (2.3094 -- 2.3094)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4278 (0.5596)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4805 (0.1735 -- 2.5071)  data: 0.2867 (0.0004 -- 2.3094)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3232 (0.4452)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (99.4709)  time: 0.2597 (0.1682 -- 1.0362)  data: 0.0712 (0.0001 -- 0.8337)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3240 (0.4873)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (99.1701)  time: 0.2835 (0.1330 -- 1.0362)  data: 0.1037 (0.0001 -- 0.8337)  max mem: 16413
Val: Total time: 0:00:09 (0.3461 s / it)
* Acc@1 88.382 Acc@5 99.378 loss 0.463
Accuracy of the network on the 482 val images: 88.38%
[2023-09-23 04:12:43,276] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 04:12:43,278] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 04:12:43,278] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 04:12:43,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 04:12:44,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 04:12:44,709] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 88.38%
Epoch: [69]  [  0/160]  eta: 0:19:33  lr: 0.000035  min_lr: 0.000009  loss: 1.8702 (1.8702)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9423 (7.9423)  time: 7.3327 (7.3327 -- 7.3327)  data: 6.7699 (6.7699 -- 6.7699)  max mem: 16413
Epoch: [69]  [ 20/160]  eta: 0:02:52  lr: 0.000035  min_lr: 0.000009  loss: 1.4371 (1.4663)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9621 (7.3398)  time: 0.9275 (0.5183 -- 3.0805)  data: 0.0017 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [69]  [ 40/160]  eta: 0:02:05  lr: 0.000035  min_lr: 0.000009  loss: 1.4372 (1.4548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5266 (7.1415)  time: 0.8580 (0.5134 -- 2.4025)  data: 0.0013 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [69]  [ 60/160]  eta: 0:01:45  lr: 0.000035  min_lr: 0.000009  loss: 1.6305 (1.5104)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5236 (7.0645)  time: 1.0543 (0.5261 -- 2.7399)  data: 0.0015 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:19  lr: 0.000035  min_lr: 0.000009  loss: 1.4501 (1.4875)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7841 (7.1268)  time: 0.8107 (0.5239 -- 3.2164)  data: 0.0014 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:59  lr: 0.000035  min_lr: 0.000009  loss: 1.5144 (1.5043)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5879 (7.1816)  time: 1.0045 (0.5170 -- 4.7574)  data: 0.0017 (0.0003 -- 0.0062)  max mem: 16413
[2023-09-23 04:14:32,947] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:14:32,947] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:14:32,948] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:14:32,949] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [69]  [120/160]  eta: 0:00:38  lr: 0.000035  min_lr: 0.000009  loss: 1.5772 (1.5204)  loss_scale: 32768.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3367 (7.2648)  time: 0.8672 (0.5197 -- 3.5358)  data: 0.0490 (0.0002 -- 0.9569)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:19  lr: 0.000035  min_lr: 0.000009  loss: 1.4952 (1.5189)  loss_scale: 32768.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6714 (7.2404)  time: 1.0410 (0.5098 -- 4.7715)  data: 0.0348 (0.0002 -- 0.6749)  max mem: 16413
[2023-09-23 04:15:04,372] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11182
[2023-09-23 04:15:04,372] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:15:04,372] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11182
[2023-09-23 04:15:04,372] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:15:04,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.5405 (1.5071)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4590 (7.2322)  time: 0.6221 (0.4925 -- 2.5758)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [69] Total time: 0:02:30 (0.9408 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.5405 (1.4960)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4590 (7.2322)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2494 (0.2494)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5106 (2.5106 -- 2.5106)  data: 2.3004 (2.3004 -- 2.3004)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2494 (0.4523)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4832 (0.1798 -- 2.5106)  data: 0.2879 (0.0002 -- 2.3004)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2171 (0.3737)  acc1: 88.8889 (91.5344)  acc5: 100.0000 (100.0000)  time: 0.2550 (0.1687 -- 1.1129)  data: 0.0639 (0.0001 -- 0.8606)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2330 (0.4382)  acc1: 88.8889 (88.3817)  acc5: 100.0000 (100.0000)  time: 0.2718 (0.1331 -- 1.1129)  data: 0.0888 (0.0001 -- 0.8606)  max mem: 16413
Val: Total time: 0:00:09 (0.3370 s / it)
* Acc@1 87.967 Acc@5 99.793 loss 0.431
Accuracy of the network on the 482 val images: 87.97%
Max accuracy: 88.38%
Epoch: [70]  [  0/160]  eta: 0:20:15  lr: 0.000035  min_lr: 0.000009  loss: 1.5649 (1.5649)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8462 (5.8462)  time: 7.6000 (7.6000 -- 7.6000)  data: 6.0565 (6.0565 -- 6.0565)  max mem: 16413
Epoch: [70]  [ 20/160]  eta: 0:02:46  lr: 0.000035  min_lr: 0.000009  loss: 1.3705 (1.4557)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2197 (6.6437)  time: 0.8705 (0.5101 -- 3.7967)  data: 0.2433 (0.0004 -- 3.2791)  max mem: 16413
Epoch: [70]  [ 40/160]  eta: 0:02:07  lr: 0.000035  min_lr: 0.000009  loss: 1.3255 (1.4295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7836 (6.9751)  time: 0.9203 (0.5133 -- 4.4675)  data: 0.0146 (0.0006 -- 0.1938)  max mem: 16413
Epoch: [70]  [ 60/160]  eta: 0:01:46  lr: 0.000035  min_lr: 0.000009  loss: 1.4537 (1.4408)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7256 (7.0417)  time: 1.0734 (0.5182 -- 5.8451)  data: 0.0882 (0.0005 -- 1.2268)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:20  lr: 0.000035  min_lr: 0.000009  loss: 1.5364 (1.4611)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3300 (7.3382)  time: 0.8131 (0.5227 -- 3.6566)  data: 0.0014 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [70]  [100/160]  eta: 0:00:59  lr: 0.000035  min_lr: 0.000009  loss: 1.6727 (1.4945)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2273 (7.2664)  time: 0.9090 (0.5302 -- 3.7463)  data: 0.0016 (0.0003 -- 0.0081)  max mem: 16413
[2023-09-23 04:17:13,742] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:17:13,743] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:17:13,744] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:17:13,744] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [70]  [120/160]  eta: 0:00:38  lr: 0.000035  min_lr: 0.000009  loss: 1.5734 (1.5152)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0902 (7.2409)  time: 0.8125 (0.5327 -- 3.8638)  data: 0.0014 (0.0002 -- 0.0027)  max mem: 16413
[2023-09-23 04:17:27,851] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11327
[2023-09-23 04:17:27,851] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11327
[2023-09-23 04:17:27,852] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:17:27,852] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:17:27,852] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [70]  [140/160]  eta: 0:00:19  lr: 0.000035  min_lr: 0.000009  loss: 1.6284 (1.5279)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9260 (7.0959)  time: 1.0769 (0.5185 -- 3.7304)  data: 0.0414 (0.0004 -- 0.8010)  max mem: 16413
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.6923 (1.5428)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2686 (7.0527)  time: 0.7027 (0.4952 -- 2.5437)  data: 0.0009 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [70] Total time: 0:02:30 (0.9419 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.6923 (1.5142)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2686 (7.0527)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1537 (0.1537)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4215 (2.4215 -- 2.4215)  data: 2.2334 (2.2334 -- 2.2334)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2573 (0.5331)  acc1: 100.0000 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4819 (0.1780 -- 2.4215)  data: 0.2915 (0.0002 -- 2.2334)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2266 (0.4409)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.2613 (0.1674 -- 1.1703)  data: 0.0741 (0.0001 -- 0.9684)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2780 (0.4987)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (100.0000)  time: 0.2763 (0.1323 -- 1.1703)  data: 0.0967 (0.0001 -- 0.9684)  max mem: 16413
Val: Total time: 0:00:09 (0.3365 s / it)
* Acc@1 88.589 Acc@5 99.585 loss 0.446
Accuracy of the network on the 482 val images: 88.59%
[2023-09-23 04:18:04,753] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 04:18:04,755] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 04:18:04,755] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 04:18:04,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 04:18:06,208] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 04:18:06,208] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 88.59%
Epoch: [71]  [  0/160]  eta: 0:21:49  lr: 0.000035  min_lr: 0.000009  loss: 1.1969 (1.1969)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8672 (6.8672)  time: 8.1841 (8.1841 -- 8.1841)  data: 6.8246 (6.8246 -- 6.8246)  max mem: 16413
Epoch: [71]  [ 20/160]  eta: 0:02:48  lr: 0.000035  min_lr: 0.000009  loss: 1.5423 (1.4201)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9188 (6.6742)  time: 0.8542 (0.5230 -- 3.8446)  data: 0.0395 (0.0009 -- 0.4303)  max mem: 16413
Epoch: [71]  [ 40/160]  eta: 0:02:05  lr: 0.000035  min_lr: 0.000009  loss: 1.4768 (1.4547)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3890 (6.7546)  time: 0.8872 (0.5228 -- 2.6835)  data: 0.1798 (0.0008 -- 2.1435)  max mem: 16413
Epoch: [71]  [ 60/160]  eta: 0:01:41  lr: 0.000035  min_lr: 0.000009  loss: 1.6527 (1.4994)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4465 (6.6962)  time: 0.9523 (0.5205 -- 3.0065)  data: 0.3111 (0.0004 -- 1.6939)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:19  lr: 0.000035  min_lr: 0.000009  loss: 1.4630 (1.4933)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0350 (6.7876)  time: 0.9185 (0.5199 -- 3.0643)  data: 0.3562 (0.0004 -- 2.5305)  max mem: 16413
[2023-09-23 04:19:41,731] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:19:41,731] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:19:41,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:19:41,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [71]  [100/160]  eta: 0:00:59  lr: 0.000035  min_lr: 0.000009  loss: 1.4867 (1.4967)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7713 (6.8703)  time: 1.0253 (0.5132 -- 4.6188)  data: 0.3722 (0.0003 -- 3.3359)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:38  lr: 0.000035  min_lr: 0.000009  loss: 1.4475 (1.5072)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8612 (6.9320)  time: 0.8283 (0.5226 -- 2.4895)  data: 0.2484 (0.0004 -- 1.9584)  max mem: 16413
Epoch: [71]  [140/160]  eta: 0:00:19  lr: 0.000035  min_lr: 0.000009  loss: 1.6848 (1.5363)  loss_scale: 32768.0000 (21612.9362)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2205 (6.8968)  time: 0.9307 (0.5176 -- 3.3390)  data: 0.3905 (0.0005 -- 2.8171)  max mem: 16413
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.7205 (1.5558)  loss_scale: 32768.0000 (22937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2083 (6.8615)  time: 0.8042 (0.4974 -- 4.6766)  data: 0.2789 (0.0002 -- 4.1285)  max mem: 16413
Epoch: [71] Total time: 0:02:31 (0.9481 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.7205 (1.5143)  loss_scale: 32768.0000 (22937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2083 (6.8615)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3938 (0.3938)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4817 (2.4817 -- 2.4817)  data: 2.2967 (2.2967 -- 2.2967)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3938 (0.4747)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4862 (0.1789 -- 2.4817)  data: 0.2995 (0.0004 -- 2.2967)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3296 (0.4062)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (99.4709)  time: 0.2572 (0.1684 -- 1.1694)  data: 0.0701 (0.0001 -- 0.9912)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4259 (0.4574)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (99.5851)  time: 0.2816 (0.1324 -- 1.1694)  data: 0.1025 (0.0001 -- 0.9912)  max mem: 16413
Val: Total time: 0:00:09 (0.3428 s / it)
* Acc@1 89.212 Acc@5 99.585 loss 0.416
Accuracy of the network on the 482 val images: 89.21%
[2023-09-23 04:20:47,282] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 04:20:47,284] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 04:20:47,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 04:20:47,284] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 04:20:48,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 04:20:48,697] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 89.21%
Epoch: [72]  [  0/160]  eta: 0:21:23  lr: 0.000035  min_lr: 0.000009  loss: 1.4102 (1.4102)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3086 (5.3086)  time: 8.0215 (8.0215 -- 8.0215)  data: 5.4368 (5.4368 -- 5.4368)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:03:06  lr: 0.000034  min_lr: 0.000009  loss: 1.5416 (1.5359)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2638 (7.6192)  time: 0.9946 (0.5302 -- 4.0238)  data: 0.1452 (0.0007 -- 2.0495)  max mem: 16413
Epoch: [72]  [ 40/160]  eta: 0:02:14  lr: 0.000034  min_lr: 0.000009  loss: 1.4791 (1.5133)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2202 (7.2143)  time: 0.9056 (0.5145 -- 3.5643)  data: 0.0421 (0.0002 -- 0.8177)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:43  lr: 0.000034  min_lr: 0.000009  loss: 1.3242 (1.4536)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1655 (6.9396)  time: 0.8535 (0.5287 -- 3.4984)  data: 0.0015 (0.0007 -- 0.0029)  max mem: 16413
[2023-09-23 04:21:54,016] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:21:54,016] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:21:54,016] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 04:21:54,016] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 04:21:59,653] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11587
[2023-09-23 04:21:59,653] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 04:21:59,654] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11587
[2023-09-23 04:21:59,654] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 04:21:59,654] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [72]  [ 80/160]  eta: 0:01:21  lr: 0.000034  min_lr: 0.000009  loss: 1.4850 (1.4469)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2529 (6.8429)  time: 0.9675 (0.5075 -- 5.1327)  data: 0.0932 (0.0002 -- 1.2388)  max mem: 16413
[2023-09-23 04:22:29,623] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11617
[2023-09-23 04:22:29,623] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11617
[2023-09-23 04:22:29,623] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:22:29,623] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:22:29,624] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [72]  [100/160]  eta: 0:01:00  lr: 0.000034  min_lr: 0.000009  loss: 1.4713 (1.4606)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4833 (6.6862)  time: 1.0034 (0.5151 -- 4.7160)  data: 0.0212 (0.0003 -- 0.2767)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:39  lr: 0.000034  min_lr: 0.000009  loss: 1.6075 (1.4735)  loss_scale: 16384.0000 (30330.7107)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8279 (6.7552)  time: 0.8593 (0.5225 -- 5.1799)  data: 0.0683 (0.0004 -- 1.3463)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:19  lr: 0.000034  min_lr: 0.000009  loss: 1.5440 (1.4843)  loss_scale: 16384.0000 (28352.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3911 (6.9071)  time: 1.0325 (0.5066 -- 4.6258)  data: 0.0015 (0.0003 -- 0.0070)  max mem: 16413
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.4895 (1.4874)  loss_scale: 16384.0000 (26931.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4555 (6.8705)  time: 0.6140 (0.4939 -- 2.1242)  data: 0.0007 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [72] Total time: 0:02:32 (0.9507 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.4895 (1.4888)  loss_scale: 16384.0000 (26931.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4555 (6.8705)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1497 (0.1497)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4936 (2.4936 -- 2.4936)  data: 2.2853 (2.2853 -- 2.2853)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2636 (0.4338)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4724 (0.1806 -- 2.4936)  data: 0.2809 (0.0002 -- 2.2853)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2636 (0.3710)  acc1: 88.8889 (92.0635)  acc5: 100.0000 (99.4709)  time: 0.2577 (0.1702 -- 0.9891)  data: 0.0695 (0.0001 -- 0.8003)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3220 (0.4639)  acc1: 88.8889 (88.3817)  acc5: 100.0000 (99.5851)  time: 0.2827 (0.1326 -- 0.9891)  data: 0.1035 (0.0001 -- 0.8003)  max mem: 16413
Val: Total time: 0:00:09 (0.3451 s / it)
* Acc@1 88.797 Acc@5 99.585 loss 0.423
Accuracy of the network on the 482 val images: 88.80%
Max accuracy: 89.21%
Epoch: [73]  [  0/160]  eta: 0:21:13  lr: 0.000034  min_lr: 0.000009  loss: 1.8984 (1.8984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7562 (5.7562)  time: 7.9583 (7.9583 -- 7.9583)  data: 7.3972 (7.3972 -- 7.3972)  max mem: 16413
Epoch: [73]  [ 20/160]  eta: 0:02:50  lr: 0.000034  min_lr: 0.000009  loss: 1.4845 (1.5316)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2589 (6.6492)  time: 0.8792 (0.5220 -- 4.0645)  data: 0.3405 (0.0003 -- 3.5413)  max mem: 16413
Epoch: [73]  [ 40/160]  eta: 0:02:22  lr: 0.000034  min_lr: 0.000009  loss: 1.4319 (1.4910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7092 (6.4907)  time: 1.1498 (0.5192 -- 5.0238)  data: 0.1471 (0.0002 -- 1.9992)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:41  lr: 0.000034  min_lr: 0.000009  loss: 1.4855 (1.4845)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1718 (6.5503)  time: 0.6548 (0.5042 -- 3.0461)  data: 0.0496 (0.0002 -- 0.9716)  max mem: 16413
[2023-09-23 04:24:39,380] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:24:39,380] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:24:39,380] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:24:39,381] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [73]  [ 80/160]  eta: 0:01:21  lr: 0.000034  min_lr: 0.000009  loss: 1.3915 (1.4800)  loss_scale: 32768.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9966 (6.5416)  time: 1.0550 (0.5144 -- 4.8311)  data: 0.0549 (0.0003 -- 0.7308)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:58  lr: 0.000034  min_lr: 0.000009  loss: 1.4993 (1.4930)  loss_scale: 32768.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8994 (6.6918)  time: 0.7502 (0.5270 -- 2.7703)  data: 0.0097 (0.0003 -- 0.1436)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:38  lr: 0.000034  min_lr: 0.000009  loss: 1.5331 (1.4921)  loss_scale: 32768.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1028 (6.6927)  time: 0.9548 (0.5255 -- 3.9918)  data: 0.1839 (0.0002 -- 2.6377)  max mem: 16413
Epoch: [73]  [140/160]  eta: 0:00:19  lr: 0.000034  min_lr: 0.000009  loss: 1.5653 (1.4949)  loss_scale: 32768.0000 (25098.8936)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2273 (6.7083)  time: 0.9571 (0.5166 -- 4.5299)  data: 0.0027 (0.0002 -- 0.0149)  max mem: 16413
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.4938 (1.4943)  loss_scale: 32768.0000 (26009.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1907 (6.7532)  time: 0.6624 (0.4945 -- 2.4192)  data: 0.0011 (0.0002 -- 0.0066)  max mem: 16413
Epoch: [73] Total time: 0:02:28 (0.9297 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.4938 (1.5073)  loss_scale: 32768.0000 (26009.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1907 (6.7532)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1487 (0.1487)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4107 (2.4107 -- 2.4107)  data: 2.1730 (2.1730 -- 2.1730)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2670 (0.4934)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4794 (0.1806 -- 2.4107)  data: 0.2859 (0.0005 -- 2.1730)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3392 (0.4394)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (99.4709)  time: 0.2540 (0.1696 -- 0.8992)  data: 0.0653 (0.0001 -- 0.7160)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3392 (0.4748)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (99.1701)  time: 0.2835 (0.1331 -- 0.9177)  data: 0.1025 (0.0001 -- 0.7465)  max mem: 16413
Val: Total time: 0:00:09 (0.3416 s / it)
* Acc@1 88.382 Acc@5 99.378 loss 0.434
Accuracy of the network on the 482 val images: 88.38%
Max accuracy: 89.21%
Epoch: [74]  [  0/160]  eta: 0:22:05  lr: 0.000034  min_lr: 0.000009  loss: 1.4647 (1.4647)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0471 (2.0471)  time: 8.2827 (8.2827 -- 8.2827)  data: 7.7338 (7.7338 -- 7.7338)  max mem: 16413
[2023-09-23 04:26:17,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11842
[2023-09-23 04:26:17,831] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:26:17,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11842
[2023-09-23 04:26:17,831] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:26:17,832] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [74]  [ 20/160]  eta: 0:02:50  lr: 0.000034  min_lr: 0.000009  loss: 1.4245 (1.4139)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6424 (5.8859)  time: 0.8665 (0.5134 -- 4.3466)  data: 0.3248 (0.0006 -- 3.8023)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:02:06  lr: 0.000034  min_lr: 0.000009  loss: 1.6040 (1.4459)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1625 (6.2084)  time: 0.8773 (0.5104 -- 2.4515)  data: 0.2763 (0.0002 -- 1.6764)  max mem: 16413
Epoch: [74]  [ 60/160]  eta: 0:01:39  lr: 0.000034  min_lr: 0.000009  loss: 1.5284 (1.4738)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4811 (6.3264)  time: 0.8896 (0.5261 -- 2.3943)  data: 0.3495 (0.0002 -- 1.8699)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:18  lr: 0.000034  min_lr: 0.000009  loss: 1.4682 (1.4757)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9376 (6.2645)  time: 0.9204 (0.5236 -- 2.3137)  data: 0.3790 (0.0004 -- 1.7963)  max mem: 16413
Epoch: [74]  [100/160]  eta: 0:00:59  lr: 0.000034  min_lr: 0.000009  loss: 1.4654 (1.4692)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6803 (6.3249)  time: 1.0320 (0.5244 -- 4.9778)  data: 0.4846 (0.0004 -- 4.4546)  max mem: 16413
[2023-09-23 04:27:55,891] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11948
[2023-09-23 04:27:55,891] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11948
[2023-09-23 04:27:55,891] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 04:27:55,891] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 04:27:55,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [74]  [120/160]  eta: 0:00:38  lr: 0.000034  min_lr: 0.000009  loss: 1.6698 (1.4910)  loss_scale: 8192.0000 (15774.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4076 (6.4216)  time: 0.8614 (0.5083 -- 3.7431)  data: 0.2886 (0.0004 -- 3.1922)  max mem: 16413
Epoch: [74]  [140/160]  eta: 0:00:19  lr: 0.000034  min_lr: 0.000009  loss: 1.6041 (1.5124)  loss_scale: 8192.0000 (14699.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5863 (6.4061)  time: 0.9238 (0.5271 -- 4.0018)  data: 0.3702 (0.0004 -- 3.4903)  max mem: 16413
[2023-09-23 04:28:38,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=67, lr=[8.515378090801247e-06, 8.515378090801247e-06, 9.461531212001385e-06, 9.461531212001385e-06, 1.0512812457779315e-05, 1.0512812457779315e-05, 1.1680902730865906e-05, 1.1680902730865906e-05, 1.297878081207323e-05, 1.297878081207323e-05, 1.4420867568970255e-05, 1.4420867568970255e-05, 1.6023186187744725e-05, 1.6023186187744725e-05, 1.780354020860525e-05, 1.780354020860525e-05, 1.9781711342894723e-05, 1.9781711342894723e-05, 2.1979679269883025e-05, 2.1979679269883025e-05, 2.4421865855425585e-05, 2.4421865855425585e-05, 2.7135406506028427e-05, 2.7135406506028427e-05, 3.0150451673364916e-05, 3.0150451673364916e-05, 3.350050185929435e-05, 3.350050185929435e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 04:28:38,223] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=17.201419823833827, CurrSamplesPerSec=24.930776637858273, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.4299 (1.5014)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3792 (6.5366)  time: 0.7283 (0.4934 -- 2.8300)  data: 0.2026 (0.0002 -- 2.2843)  max mem: 16413
Epoch: [74] Total time: 0:02:29 (0.9362 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.4299 (1.4923)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3792 (6.5366)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2882 (0.2882)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4202 (2.4202 -- 2.4202)  data: 2.2230 (2.2230 -- 2.2230)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2548 (0.4588)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.9798)  time: 0.4674 (0.1739 -- 2.4202)  data: 0.2789 (0.0003 -- 2.2230)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2548 (0.3985)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (98.9418)  time: 0.2536 (0.1682 -- 1.0513)  data: 0.0678 (0.0001 -- 0.8404)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2703 (0.4452)  acc1: 88.8889 (88.3817)  acc5: 100.0000 (98.7552)  time: 0.2790 (0.1324 -- 1.0513)  data: 0.1000 (0.0001 -- 0.8404)  max mem: 16413
Val: Total time: 0:00:09 (0.3377 s / it)
* Acc@1 88.589 Acc@5 99.170 loss 0.436
Accuracy of the network on the 482 val images: 88.59%
Max accuracy: 89.21%
Epoch: [75]  [  0/160]  eta: 0:22:29  lr: 0.000033  min_lr: 0.000009  loss: 1.5738 (1.5738)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8354 (6.8354)  time: 8.4333 (8.4333 -- 8.4333)  data: 7.1468 (7.1468 -- 7.1468)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:02:53  lr: 0.000033  min_lr: 0.000009  loss: 1.5366 (1.4752)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8710 (7.3987)  time: 0.8822 (0.5134 -- 4.0228)  data: 0.2111 (0.0003 -- 2.2822)  max mem: 16413
Epoch: [75]  [ 40/160]  eta: 0:02:11  lr: 0.000033  min_lr: 0.000008  loss: 1.5384 (1.4950)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4753 (7.3661)  time: 0.9476 (0.5167 -- 3.0717)  data: 0.3449 (0.0004 -- 2.5433)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:41  lr: 0.000033  min_lr: 0.000008  loss: 1.4805 (1.4825)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8409 (7.1718)  time: 0.8456 (0.5227 -- 3.5012)  data: 0.2491 (0.0004 -- 2.9577)  max mem: 16413
[2023-09-23 04:30:06,021] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:30:06,021] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 04:30:06,021] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:30:06,021] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [75]  [ 80/160]  eta: 0:01:21  lr: 0.000033  min_lr: 0.000008  loss: 1.5799 (1.4859)  loss_scale: 8192.0000 (8596.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0165 (7.2880)  time: 1.0336 (0.5151 -- 3.9214)  data: 0.1524 (0.0002 -- 2.2155)  max mem: 16413
Epoch: [75]  [100/160]  eta: 0:00:58  lr: 0.000033  min_lr: 0.000008  loss: 1.3989 (1.4699)  loss_scale: 16384.0000 (10138.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2276 (7.3484)  time: 0.8197 (0.5205 -- 2.3269)  data: 0.2345 (0.0005 -- 1.8179)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:38  lr: 0.000033  min_lr: 0.000008  loss: 1.5165 (1.4785)  loss_scale: 16384.0000 (11170.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2211 (7.3898)  time: 0.9302 (0.5254 -- 2.7459)  data: 0.2691 (0.0002 -- 2.2218)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:19  lr: 0.000033  min_lr: 0.000008  loss: 1.3124 (1.4629)  loss_scale: 16384.0000 (11910.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5484 (7.3620)  time: 0.9771 (0.5057 -- 3.9512)  data: 0.4022 (0.0002 -- 3.4413)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.4189 (1.4649)  loss_scale: 16384.0000 (12441.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2295 (7.3028)  time: 0.7883 (0.4920 -- 4.3721)  data: 0.2711 (0.0001 -- 3.8419)  max mem: 16413
Epoch: [75] Total time: 0:02:32 (0.9524 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.4189 (1.4779)  loss_scale: 16384.0000 (12441.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2295 (7.3028)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1475 (0.1475)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5546 (2.5546 -- 2.5546)  data: 2.3598 (2.3598 -- 2.3598)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2189 (0.3792)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4835 (0.1784 -- 2.5546)  data: 0.2920 (0.0004 -- 2.3598)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2437 (0.3559)  acc1: 88.8889 (91.5344)  acc5: 100.0000 (100.0000)  time: 0.2542 (0.1685 -- 1.0398)  data: 0.0686 (0.0001 -- 0.8469)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3018 (0.4199)  acc1: 88.8889 (89.2116)  acc5: 100.0000 (100.0000)  time: 0.2714 (0.1328 -- 1.0398)  data: 0.0934 (0.0001 -- 0.8469)  max mem: 16413
Val: Total time: 0:00:09 (0.3377 s / it)
* Acc@1 90.456 Acc@5 99.793 loss 0.412
Accuracy of the network on the 482 val images: 90.46%
[2023-09-23 04:31:29,375] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 04:31:29,377] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 04:31:29,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 04:31:29,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 04:31:30,800] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 04:31:30,800] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 90.46%
Epoch: [76]  [  0/160]  eta: 0:18:54  lr: 0.000033  min_lr: 0.000008  loss: 1.6758 (1.6758)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7922 (6.7922)  time: 7.0919 (7.0919 -- 7.0919)  data: 5.9126 (5.9126 -- 5.9126)  max mem: 16413
Epoch: [76]  [ 20/160]  eta: 0:02:55  lr: 0.000033  min_lr: 0.000008  loss: 1.4942 (1.5014)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5148 (7.0746)  time: 0.9647 (0.5190 -- 4.0656)  data: 0.1204 (0.0003 -- 0.9992)  max mem: 16413
Epoch: [76]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000008  loss: 1.2790 (1.4025)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8014 (7.1807)  time: 0.8597 (0.5219 -- 2.7477)  data: 0.0155 (0.0003 -- 0.2700)  max mem: 16413
[2023-09-23 04:32:18,241] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:32:18,241] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:32:18,243] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:32:18,244] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [76]  [ 60/160]  eta: 0:01:43  lr: 0.000033  min_lr: 0.000008  loss: 1.4292 (1.4364)  loss_scale: 32768.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6078 (7.1076)  time: 0.9740 (0.5265 -- 2.5231)  data: 0.0443 (0.0003 -- 0.8645)  max mem: 16413
[2023-09-23 04:32:40,148] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12228
[2023-09-23 04:32:40,149] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:32:40,149] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12228
[2023-09-23 04:32:40,149] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:32:40,149] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [76]  [ 80/160]  eta: 0:01:18  lr: 0.000033  min_lr: 0.000008  loss: 1.5361 (1.4773)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3041 (7.0886)  time: 0.8216 (0.5255 -- 3.2104)  data: 0.1031 (0.0004 -- 1.1126)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:59  lr: 0.000033  min_lr: 0.000008  loss: 1.4434 (1.4782)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2997 (7.2144)  time: 1.0340 (0.5296 -- 4.0655)  data: 0.2717 (0.0003 -- 3.5058)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:38  lr: 0.000033  min_lr: 0.000008  loss: 1.4652 (1.4804)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4131 (7.2504)  time: 0.8338 (0.5192 -- 3.7127)  data: 0.2682 (0.0003 -- 3.1977)  max mem: 16413
Epoch: [76]  [140/160]  eta: 0:00:19  lr: 0.000033  min_lr: 0.000008  loss: 1.4841 (1.4798)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4270 (7.1941)  time: 1.0459 (0.5342 -- 4.0828)  data: 0.4315 (0.0003 -- 3.5657)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.4234 (1.4743)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0446 (7.1278)  time: 0.6731 (0.4947 -- 2.2612)  data: 0.0223 (0.0001 -- 0.4361)  max mem: 16413
Epoch: [76] Total time: 0:02:29 (0.9360 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.4234 (1.4966)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0446 (7.1278)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1601 (0.1601)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4300 (2.4300 -- 2.4300)  data: 2.2396 (2.2396 -- 2.2396)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2632 (0.4862)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (96.9697)  time: 0.4804 (0.1817 -- 2.4300)  data: 0.2856 (0.0002 -- 2.2396)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2767 (0.4391)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2614 (0.1686 -- 1.0801)  data: 0.0717 (0.0001 -- 0.8961)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3400 (0.4662)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.7552)  time: 0.2779 (0.1328 -- 1.0801)  data: 0.0990 (0.0001 -- 0.8961)  max mem: 16413
Val: Total time: 0:00:09 (0.3404 s / it)
* Acc@1 88.589 Acc@5 98.963 loss 0.431
Accuracy of the network on the 482 val images: 88.59%
Max accuracy: 90.46%
Epoch: [77]  [  0/160]  eta: 0:22:11  lr: 0.000033  min_lr: 0.000008  loss: 1.7621 (1.7621)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5053 (5.5053)  time: 8.3206 (8.3206 -- 8.3206)  data: 7.7652 (7.7652 -- 7.7652)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:51  lr: 0.000033  min_lr: 0.000008  loss: 1.6024 (1.5100)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9854 (6.7191)  time: 0.8696 (0.5300 -- 4.2946)  data: 0.3219 (0.0004 -- 3.7586)  max mem: 16413
[2023-09-23 04:34:51,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:34:51,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:34:51,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:34:51,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [77]  [ 40/160]  eta: 0:02:14  lr: 0.000033  min_lr: 0.000008  loss: 1.5115 (1.5069)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1928 (7.2069)  time: 1.0179 (0.5184 -- 4.1974)  data: 0.4729 (0.0004 -- 3.7001)  max mem: 16413
Epoch: [77]  [ 60/160]  eta: 0:01:42  lr: 0.000033  min_lr: 0.000008  loss: 1.4312 (1.4427)  loss_scale: 32768.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1230 (6.8987)  time: 0.8082 (0.5197 -- 4.7628)  data: 0.2613 (0.0003 -- 4.2415)  max mem: 16413
Epoch: [77]  [ 80/160]  eta: 0:01:22  lr: 0.000033  min_lr: 0.000008  loss: 1.3377 (1.4446)  loss_scale: 32768.0000 (25283.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0409 (6.9881)  time: 1.0717 (0.5161 -- 4.3498)  data: 0.5335 (0.0003 -- 3.8374)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:59  lr: 0.000033  min_lr: 0.000008  loss: 1.3075 (1.4294)  loss_scale: 32768.0000 (26765.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0920 (7.2401)  time: 0.7972 (0.5370 -- 3.9501)  data: 0.2414 (0.0003 -- 3.4144)  max mem: 16413
Epoch: [77]  [120/160]  eta: 0:00:39  lr: 0.000033  min_lr: 0.000008  loss: 1.3969 (1.4191)  loss_scale: 32768.0000 (27758.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5555 (7.1184)  time: 1.0220 (0.5297 -- 4.0824)  data: 0.4775 (0.0003 -- 3.5715)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:19  lr: 0.000033  min_lr: 0.000008  loss: 1.5519 (1.4406)  loss_scale: 32768.0000 (28468.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0116 (7.1614)  time: 0.8694 (0.5159 -- 3.9070)  data: 0.3240 (0.0003 -- 3.3892)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.4441 (1.4531)  loss_scale: 32768.0000 (28979.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8973 (7.0770)  time: 0.6367 (0.4945 -- 1.5558)  data: 0.1058 (0.0001 -- 1.0652)  max mem: 16413
Epoch: [77] Total time: 0:02:29 (0.9354 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.4441 (1.4832)  loss_scale: 32768.0000 (28979.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8973 (7.0770)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2472 (0.2472)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4285 (2.4285 -- 2.4285)  data: 2.2519 (2.2519 -- 2.2519)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2907 (0.5009)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4706 (0.1749 -- 2.4285)  data: 0.2819 (0.0003 -- 2.2519)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2907 (0.4293)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (99.4709)  time: 0.2547 (0.1680 -- 1.0098)  data: 0.0671 (0.0001 -- 0.8218)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4255 (0.4799)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (99.5851)  time: 0.2885 (0.1342 -- 1.0098)  data: 0.1083 (0.0001 -- 0.8270)  max mem: 16413
Val: Total time: 0:00:09 (0.3457 s / it)
* Acc@1 89.004 Acc@5 99.378 loss 0.432
Accuracy of the network on the 482 val images: 89.00%
Max accuracy: 90.46%
Epoch: [78]  [  0/160]  eta: 0:23:23  lr: 0.000032  min_lr: 0.000008  loss: 1.3474 (1.3474)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2011 (7.2011)  time: 8.7694 (8.7694 -- 8.7694)  data: 7.8931 (7.8931 -- 7.8931)  max mem: 16413
[2023-09-23 04:37:00,336] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:37:00,336] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 04:37:00,336] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:37:00,336] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 04:37:01,415] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12487
[2023-09-23 04:37:01,416] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 04:37:01,416] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-23 04:37:01,416] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12487
[2023-09-23 04:37:01,417] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 04:37:14,925] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12500
[2023-09-23 04:37:14,925] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12500
[2023-09-23 04:37:14,925] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:37:14,925] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:37:14,925] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [78]  [ 20/160]  eta: 0:02:53  lr: 0.000032  min_lr: 0.000008  loss: 1.5928 (1.4525)  loss_scale: 32768.0000 (35108.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0476 (7.8225)  time: 0.8627 (0.5168 -- 3.4494)  data: 0.2339 (0.0004 -- 2.3023)  max mem: 16413
Epoch: [78]  [ 40/160]  eta: 0:02:11  lr: 0.000032  min_lr: 0.000008  loss: 1.4998 (1.5123)  loss_scale: 16384.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5106 (7.5293)  time: 0.9441 (0.5191 -- 4.4631)  data: 0.3784 (0.0003 -- 3.9463)  max mem: 16413
Epoch: [78]  [ 60/160]  eta: 0:01:43  lr: 0.000032  min_lr: 0.000008  loss: 1.5425 (1.5172)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0441 (7.3466)  time: 0.9147 (0.5167 -- 4.5275)  data: 0.2418 (0.0002 -- 2.4515)  max mem: 16413
Epoch: [78]  [ 80/160]  eta: 0:01:23  lr: 0.000032  min_lr: 0.000008  loss: 1.3305 (1.5187)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3691 (7.2080)  time: 1.0488 (0.5301 -- 4.8983)  data: 0.0411 (0.0002 -- 0.7882)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:59  lr: 0.000032  min_lr: 0.000008  loss: 1.5141 (1.5337)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5827 (7.1740)  time: 0.7575 (0.5148 -- 3.7094)  data: 0.0397 (0.0001 -- 0.7571)  max mem: 16413
[2023-09-23 04:38:28,740] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12581
[2023-09-23 04:38:28,740] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 04:38:28,741] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-23 04:38:28,754] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12581
[2023-09-23 04:38:28,754] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [78]  [120/160]  eta: 0:00:39  lr: 0.000032  min_lr: 0.000008  loss: 1.4555 (1.5257)  loss_scale: 8192.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2786 (6.9507)  time: 1.0786 (0.5024 -- 5.0866)  data: 0.0012 (0.0002 -- 0.0053)  max mem: 16413
Epoch: [78]  [140/160]  eta: 0:00:19  lr: 0.000032  min_lr: 0.000008  loss: 1.4073 (1.5093)  loss_scale: 8192.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7919 (6.8296)  time: 0.7343 (0.5153 -- 2.8696)  data: 0.0014 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.4075 (1.5148)  loss_scale: 8192.0000 (15820.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8679 (6.8580)  time: 0.7292 (0.4920 -- 2.9454)  data: 0.0009 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [78] Total time: 0:02:29 (0.9353 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.4075 (1.5043)  loss_scale: 8192.0000 (15820.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8679 (6.8580)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6139 (0.6139)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4546 (2.4546 -- 2.4546)  data: 2.2568 (2.2568 -- 2.2568)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2166 (0.4776)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (97.9798)  time: 0.4728 (0.1890 -- 2.4546)  data: 0.2775 (0.0002 -- 2.2568)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2532 (0.3905)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (98.9418)  time: 0.2558 (0.1685 -- 1.0023)  data: 0.0672 (0.0001 -- 0.7892)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3024 (0.4241)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (99.1701)  time: 0.2838 (0.1327 -- 1.0023)  data: 0.1039 (0.0001 -- 0.7892)  max mem: 16413
Val: Total time: 0:00:09 (0.3443 s / it)
* Acc@1 89.627 Acc@5 99.170 loss 0.430
Accuracy of the network on the 482 val images: 89.63%
Max accuracy: 90.46%
Epoch: [79]  [  0/160]  eta: 0:24:23  lr: 0.000032  min_lr: 0.000008  loss: 1.5828 (1.5828)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1183 (4.1183)  time: 9.1447 (9.1447 -- 9.1447)  data: 7.0713 (7.0713 -- 7.0713)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:49  lr: 0.000032  min_lr: 0.000008  loss: 1.5387 (1.5226)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1042 (6.1743)  time: 0.8175 (0.5122 -- 3.4636)  data: 0.0572 (0.0003 -- 0.7476)  max mem: 16413
Epoch: [79]  [ 40/160]  eta: 0:02:13  lr: 0.000032  min_lr: 0.000008  loss: 1.5099 (1.5134)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9447 (6.0307)  time: 1.0135 (0.5239 -- 4.0484)  data: 0.0830 (0.0004 -- 1.1800)  max mem: 16413
Epoch: [79]  [ 60/160]  eta: 0:01:45  lr: 0.000032  min_lr: 0.000008  loss: 1.6013 (1.5338)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9053 (6.3007)  time: 0.9164 (0.5214 -- 3.3549)  data: 0.0022 (0.0006 -- 0.0120)  max mem: 16413
[2023-09-23 04:40:39,849] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:40:39,849] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 04:40:39,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:40:39,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [79]  [ 80/160]  eta: 0:01:20  lr: 0.000032  min_lr: 0.000008  loss: 1.4814 (1.5229)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6991 (6.4550)  time: 0.8787 (0.5192 -- 4.7798)  data: 0.0015 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:59  lr: 0.000032  min_lr: 0.000008  loss: 1.5234 (1.5198)  loss_scale: 16384.0000 (10706.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7848 (6.6133)  time: 0.9169 (0.5255 -- 3.9329)  data: 0.0017 (0.0004 -- 0.0057)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:38  lr: 0.000032  min_lr: 0.000008  loss: 1.3295 (1.4955)  loss_scale: 16384.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1079 (6.9238)  time: 0.8937 (0.5231 -- 4.4895)  data: 0.0020 (0.0003 -- 0.0055)  max mem: 16413
[2023-09-23 04:41:36,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12772
[2023-09-23 04:41:36,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12772
[2023-09-23 04:41:36,932] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 04:41:36,933] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 04:41:36,933] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [79]  [140/160]  eta: 0:00:19  lr: 0.000032  min_lr: 0.000008  loss: 1.4848 (1.4958)  loss_scale: 16384.0000 (11794.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2280 (6.8554)  time: 0.8484 (0.5033 -- 3.2965)  data: 0.0024 (0.0003 -- 0.0157)  max mem: 16413
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.3471 (1.4902)  loss_scale: 8192.0000 (11366.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4559 (6.8828)  time: 0.7556 (0.4929 -- 2.9564)  data: 0.1426 (0.0002 -- 2.4572)  max mem: 16413
Epoch: [79] Total time: 0:02:29 (0.9339 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.3471 (1.4949)  loss_scale: 8192.0000 (11366.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4559 (6.8828)
[2023-09-23 04:41:57,337] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-09-23 04:41:57,338] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-09-23 04:41:57,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-09-23 04:41:57,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-09-23 04:41:58,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-09-23 04:41:58,409] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3514 (0.3514)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4800 (2.4800 -- 2.4800)  data: 2.2858 (2.2858 -- 2.2858)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3160 (0.5252)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (96.9697)  time: 0.4794 (0.1814 -- 2.4800)  data: 0.2807 (0.0002 -- 2.2858)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2358 (0.4240)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.4127)  time: 0.2581 (0.1684 -- 1.0081)  data: 0.0662 (0.0001 -- 0.7973)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2754 (0.4755)  acc1: 88.8889 (88.3817)  acc5: 100.0000 (98.7552)  time: 0.2882 (0.1325 -- 1.0081)  data: 0.1075 (0.0001 -- 0.8290)  max mem: 16413
Val: Total time: 0:00:09 (0.3502 s / it)
* Acc@1 89.627 Acc@5 99.170 loss 0.430
Accuracy of the network on the 482 val images: 89.63%
Max accuracy: 90.46%
Epoch: [80]  [  0/160]  eta: 0:22:01  lr: 0.000032  min_lr: 0.000008  loss: 1.5101 (1.5101)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7662 (6.7662)  time: 8.2595 (8.2595 -- 8.2595)  data: 7.7111 (7.7111 -- 7.7111)  max mem: 16413
Epoch: [80]  [ 20/160]  eta: 0:02:47  lr: 0.000032  min_lr: 0.000008  loss: 1.5291 (1.5140)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8087 (7.4147)  time: 0.8437 (0.5145 -- 3.3957)  data: 0.1851 (0.0006 -- 2.3864)  max mem: 16413
Epoch: [80]  [ 40/160]  eta: 0:02:10  lr: 0.000032  min_lr: 0.000008  loss: 1.4157 (1.4738)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5456 (7.4143)  time: 0.9730 (0.5210 -- 3.3383)  data: 0.1402 (0.0003 -- 1.5329)  max mem: 16413
Epoch: [80]  [ 60/160]  eta: 0:01:43  lr: 0.000032  min_lr: 0.000008  loss: 1.3835 (1.4766)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3477 (7.6344)  time: 0.9236 (0.5186 -- 3.9006)  data: 0.1010 (0.0002 -- 1.9834)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:19  lr: 0.000032  min_lr: 0.000008  loss: 1.5312 (1.4925)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7336 (7.5331)  time: 0.8733 (0.5180 -- 4.1738)  data: 0.3139 (0.0002 -- 3.6100)  max mem: 16413
Epoch: [80]  [100/160]  eta: 0:00:59  lr: 0.000032  min_lr: 0.000008  loss: 1.5900 (1.5099)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1533 (7.3705)  time: 1.0143 (0.5200 -- 3.9070)  data: 0.3789 (0.0003 -- 3.1399)  max mem: 16413
[2023-09-23 04:43:49,229] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:43:49,229] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 04:43:49,230] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:43:49,231] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [80]  [120/160]  eta: 0:00:39  lr: 0.000031  min_lr: 0.000008  loss: 1.6117 (1.5203)  loss_scale: 16384.0000 (9546.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7728 (7.4471)  time: 0.8670 (0.5228 -- 3.9253)  data: 0.2856 (0.0004 -- 3.3958)  max mem: 16413
Epoch: [80]  [140/160]  eta: 0:00:19  lr: 0.000031  min_lr: 0.000008  loss: 1.3961 (1.5056)  loss_scale: 16384.0000 (10515.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1305 (7.4298)  time: 0.9630 (0.5185 -- 3.8662)  data: 0.4236 (0.0004 -- 3.3558)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.5720 (1.5118)  loss_scale: 16384.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0096 (7.4133)  time: 0.6939 (0.4940 -- 2.9939)  data: 0.1715 (0.0002 -- 2.4670)  max mem: 16413
Epoch: [80] Total time: 0:02:30 (0.9424 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.5720 (1.4900)  loss_scale: 16384.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0096 (7.4133)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2160 (0.2160)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4965 (2.4965 -- 2.4965)  data: 2.3075 (2.3075 -- 2.3075)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2090 (0.4172)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (97.9798)  time: 0.4883 (0.1793 -- 2.4965)  data: 0.2975 (0.0002 -- 2.3075)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2475 (0.3659)  acc1: 88.8889 (92.5926)  acc5: 100.0000 (98.4127)  time: 0.2645 (0.1677 -- 1.1515)  data: 0.0754 (0.0001 -- 0.9599)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2880 (0.4128)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (98.7552)  time: 0.2945 (0.1320 -- 1.1515)  data: 0.1126 (0.0001 -- 0.9599)  max mem: 16413
Val: Total time: 0:00:09 (0.3524 s / it)
* Acc@1 89.834 Acc@5 99.170 loss 0.425
Accuracy of the network on the 482 val images: 89.83%
Max accuracy: 90.46%
Epoch: [81]  [  0/160]  eta: 0:20:15  lr: 0.000031  min_lr: 0.000008  loss: 2.1947 (2.1947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1385 (6.1385)  time: 7.5958 (7.5958 -- 7.5958)  data: 6.4883 (6.4883 -- 6.4883)  max mem: 16413
Epoch: [81]  [ 20/160]  eta: 0:02:51  lr: 0.000031  min_lr: 0.000008  loss: 1.5680 (1.5026)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6147 (7.4954)  time: 0.9063 (0.5114 -- 3.3488)  data: 0.3030 (0.0002 -- 2.1307)  max mem: 16413
[2023-09-23 04:45:29,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=72, lr=[7.961979027681322e-06, 7.961979027681322e-06, 8.846643364090358e-06, 8.846643364090358e-06, 9.829603737878174e-06, 9.829603737878174e-06, 1.0921781930975751e-05, 1.0921781930975751e-05, 1.2135313256639722e-05, 1.2135313256639722e-05, 1.3483681396266359e-05, 1.3483681396266359e-05, 1.498186821807373e-05, 1.498186821807373e-05, 1.6646520242304144e-05, 1.6646520242304144e-05, 1.849613360256016e-05, 1.849613360256016e-05, 2.0551259558400174e-05, 2.0551259558400174e-05, 2.2834732842666863e-05, 2.2834732842666863e-05, 2.537192538074096e-05, 2.537192538074096e-05, 2.8191028200823287e-05, 2.8191028200823287e-05, 3.132336466758143e-05, 3.132336466758143e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 04:45:29,403] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=17.231220604802616, CurrSamplesPerSec=22.931180463802452, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:03  lr: 0.000031  min_lr: 0.000008  loss: 1.1320 (1.3876)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8410 (7.0585)  time: 0.8255 (0.5303 -- 3.1001)  data: 0.2543 (0.0003 -- 2.0483)  max mem: 16413
Epoch: [81]  [ 60/160]  eta: 0:01:42  lr: 0.000031  min_lr: 0.000008  loss: 1.4587 (1.4422)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3583 (7.2739)  time: 1.0288 (0.5171 -- 3.3608)  data: 0.3667 (0.0004 -- 2.8431)  max mem: 16413
[2023-09-23 04:45:58,789] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:45:58,789] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:45:58,790] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:45:58,790] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [81]  [ 80/160]  eta: 0:01:19  lr: 0.000031  min_lr: 0.000008  loss: 1.5413 (1.4649)  loss_scale: 32768.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8885 (7.0228)  time: 0.8886 (0.5266 -- 2.7871)  data: 0.2638 (0.0003 -- 2.2525)  max mem: 16413
[2023-09-23 04:46:12,093] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13042
[2023-09-23 04:46:12,093] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:46:12,093] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13042
[2023-09-23 04:46:12,093] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 04:46:12,093] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [81]  [100/160]  eta: 0:00:59  lr: 0.000031  min_lr: 0.000008  loss: 1.4893 (1.4642)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9023 (6.9932)  time: 1.0046 (0.5270 -- 3.7024)  data: 0.0594 (0.0002 -- 0.6008)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:38  lr: 0.000031  min_lr: 0.000008  loss: 1.2157 (1.4351)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2304 (6.9820)  time: 0.8133 (0.5074 -- 5.6173)  data: 0.0015 (0.0002 -- 0.0110)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:19  lr: 0.000031  min_lr: 0.000008  loss: 1.5144 (1.4503)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3887 (7.0607)  time: 0.9807 (0.5283 -- 3.0957)  data: 0.0742 (0.0004 -- 0.5840)  max mem: 16413
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.5795 (1.4526)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1688 (7.1186)  time: 0.6817 (0.4929 -- 2.2699)  data: 0.0777 (0.0002 -- 1.5451)  max mem: 16413
Epoch: [81] Total time: 0:02:29 (0.9354 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.5795 (1.4659)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1688 (7.1186)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2174 (0.2174)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5673 (2.5673 -- 2.5673)  data: 2.3730 (2.3730 -- 2.3730)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3241 (0.4913)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (97.9798)  time: 0.4871 (0.1778 -- 2.5673)  data: 0.2982 (0.0003 -- 2.3730)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3102 (0.4367)  acc1: 88.8889 (91.5344)  acc5: 100.0000 (98.4127)  time: 0.2519 (0.1689 -- 1.0876)  data: 0.0651 (0.0001 -- 0.9005)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3517 (0.4725)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (98.3402)  time: 0.2777 (0.1328 -- 1.0876)  data: 0.0983 (0.0001 -- 0.9005)  max mem: 16413
Val: Total time: 0:00:09 (0.3427 s / it)
* Acc@1 89.627 Acc@5 98.963 loss 0.472
Accuracy of the network on the 482 val images: 89.63%
Max accuracy: 90.46%
Epoch: [82]  [  0/160]  eta: 0:23:22  lr: 0.000031  min_lr: 0.000008  loss: 1.6622 (1.6622)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3605 (5.3605)  time: 8.7682 (8.7682 -- 8.7682)  data: 6.4718 (6.4718 -- 6.4718)  max mem: 16413
Epoch: [82]  [ 20/160]  eta: 0:03:00  lr: 0.000031  min_lr: 0.000008  loss: 1.6458 (1.5828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0277 (7.0415)  time: 0.9118 (0.5256 -- 3.6685)  data: 0.2161 (0.0004 -- 1.3186)  max mem: 16413
Epoch: [82]  [ 40/160]  eta: 0:02:10  lr: 0.000031  min_lr: 0.000008  loss: 1.5261 (1.5459)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9092 (7.1973)  time: 0.8824 (0.5229 -- 2.5641)  data: 0.1326 (0.0002 -- 1.6082)  max mem: 16413
[2023-09-23 04:48:22,090] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:48:22,090] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:48:22,090] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:48:22,090] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:48:22,662] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13172
[2023-09-23 04:48:22,662] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:48:22,662] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13172
[2023-09-23 04:48:22,662] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 04:48:22,662] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [82]  [ 60/160]  eta: 0:01:44  lr: 0.000031  min_lr: 0.000008  loss: 1.5973 (1.5481)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4913 (7.4385)  time: 0.9579 (0.5252 -- 4.1887)  data: 0.0770 (0.0003 -- 1.4950)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:20  lr: 0.000031  min_lr: 0.000008  loss: 1.6014 (1.5385)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8463 (7.5485)  time: 0.8624 (0.5142 -- 3.2300)  data: 0.2073 (0.0003 -- 2.6944)  max mem: 16413
Epoch: [82]  [100/160]  eta: 0:00:58  lr: 0.000031  min_lr: 0.000008  loss: 1.5963 (1.5420)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1350 (7.5894)  time: 0.8999 (0.5166 -- 4.1449)  data: 0.3568 (0.0003 -- 3.6331)  max mem: 16413
[2023-09-23 04:49:15,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13231
[2023-09-23 04:49:15,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13231
[2023-09-23 04:49:15,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 04:49:15,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 04:49:15,955] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [82]  [120/160]  eta: 0:00:38  lr: 0.000031  min_lr: 0.000008  loss: 1.4344 (1.5200)  loss_scale: 8192.0000 (15842.3802)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8711 (7.5475)  time: 0.8915 (0.5115 -- 3.5505)  data: 0.2849 (0.0002 -- 3.0306)  max mem: 16413
Epoch: [82]  [140/160]  eta: 0:00:19  lr: 0.000031  min_lr: 0.000008  loss: 1.3350 (1.5046)  loss_scale: 8192.0000 (14757.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4980 (7.4538)  time: 0.9770 (0.5204 -- 3.7842)  data: 0.4433 (0.0003 -- 3.2671)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.4210 (1.4948)  loss_scale: 8192.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3102 (7.4081)  time: 0.7455 (0.4930 -- 3.9805)  data: 0.2225 (0.0001 -- 3.4420)  max mem: 16413
Epoch: [82] Total time: 0:02:30 (0.9426 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.4210 (1.4913)  loss_scale: 8192.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3102 (7.4081)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4478 (2.4478 -- 2.4478)  data: 2.2612 (2.2612 -- 2.2612)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1979 (0.4788)  acc1: 100.0000 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4735 (0.1755 -- 2.4478)  data: 0.2835 (0.0002 -- 2.2612)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3153 (0.4236)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (100.0000)  time: 0.2547 (0.1687 -- 1.0498)  data: 0.0679 (0.0001 -- 0.8519)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3507 (0.4750)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (100.0000)  time: 0.2871 (0.1321 -- 1.0498)  data: 0.1085 (0.0001 -- 0.8519)  max mem: 16413
Val: Total time: 0:00:09 (0.3459 s / it)
* Acc@1 89.212 Acc@5 100.000 loss 0.423
Accuracy of the network on the 482 val images: 89.21%
Max accuracy: 90.46%
Epoch: [83]  [  0/160]  eta: 0:23:20  lr: 0.000031  min_lr: 0.000008  loss: 1.6852 (1.6852)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7504 (4.7504)  time: 8.7543 (8.7543 -- 8.7543)  data: 7.8782 (7.8782 -- 7.8782)  max mem: 16413
Epoch: [83]  [ 20/160]  eta: 0:02:55  lr: 0.000031  min_lr: 0.000008  loss: 1.5120 (1.5891)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4117 (7.2223)  time: 0.8785 (0.5198 -- 4.3394)  data: 0.3480 (0.0005 -- 3.8263)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:02:17  lr: 0.000031  min_lr: 0.000008  loss: 1.3047 (1.4763)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6200 (7.2030)  time: 1.0306 (0.5104 -- 4.4135)  data: 0.4902 (0.0004 -- 3.8752)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:44  lr: 0.000031  min_lr: 0.000008  loss: 1.5022 (1.4927)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1844 (7.6686)  time: 0.8419 (0.5333 -- 2.0854)  data: 0.2958 (0.0003 -- 1.5604)  max mem: 16413
[2023-09-23 04:51:31,104] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:51:31,104] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 04:51:31,105] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:51:31,105] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [83]  [ 80/160]  eta: 0:01:22  lr: 0.000031  min_lr: 0.000008  loss: 1.3690 (1.5025)  loss_scale: 8192.0000 (8293.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8268 (7.5644)  time: 0.9765 (0.5284 -- 3.6613)  data: 0.4346 (0.0001 -- 3.1465)  max mem: 16413
Epoch: [83]  [100/160]  eta: 0:01:01  lr: 0.000030  min_lr: 0.000008  loss: 1.8005 (1.5360)  loss_scale: 16384.0000 (9895.2871)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5518 (7.5029)  time: 1.0144 (0.5167 -- 4.0985)  data: 0.4669 (0.0003 -- 3.5521)  max mem: 16413
Epoch: [83]  [120/160]  eta: 0:00:39  lr: 0.000030  min_lr: 0.000008  loss: 1.3792 (1.5105)  loss_scale: 16384.0000 (10967.8017)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8852 (7.4230)  time: 0.8461 (0.5120 -- 3.6967)  data: 0.3164 (0.0002 -- 3.1641)  max mem: 16413
Epoch: [83]  [140/160]  eta: 0:00:19  lr: 0.000030  min_lr: 0.000008  loss: 1.3216 (1.4926)  loss_scale: 16384.0000 (11736.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3777 (7.3036)  time: 0.9396 (0.5269 -- 3.8684)  data: 0.3922 (0.0004 -- 3.3525)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.3892 (1.4967)  loss_scale: 16384.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2325 (7.1612)  time: 0.6334 (0.4914 -- 1.5843)  data: 0.1108 (0.0002 -- 1.0693)  max mem: 16413
Epoch: [83] Total time: 0:02:31 (0.9467 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.3892 (1.4914)  loss_scale: 16384.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2325 (7.1612)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3600 (0.3600)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4220 (2.4220 -- 2.4220)  data: 2.2341 (2.2341 -- 2.2341)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3071 (0.4555)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4719 (0.1759 -- 2.4220)  data: 0.2834 (0.0002 -- 2.2341)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2514 (0.3735)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (100.0000)  time: 0.2578 (0.1676 -- 1.1073)  data: 0.0722 (0.0001 -- 0.8767)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2554 (0.4045)  acc1: 88.8889 (89.2116)  acc5: 100.0000 (100.0000)  time: 0.2797 (0.1328 -- 1.1073)  data: 0.1005 (0.0001 -- 0.8767)  max mem: 16413
Val: Total time: 0:00:09 (0.3381 s / it)
* Acc@1 89.834 Acc@5 99.793 loss 0.408
Accuracy of the network on the 482 val images: 89.83%
Max accuracy: 90.46%
Epoch: [84]  [  0/160]  eta: 0:22:43  lr: 0.000030  min_lr: 0.000008  loss: 1.2030 (1.2030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1954 (8.1954)  time: 8.5227 (8.5227 -- 8.5227)  data: 7.9891 (7.9891 -- 7.9891)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:56  lr: 0.000030  min_lr: 0.000008  loss: 1.4233 (1.3979)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4173 (6.3498)  time: 0.9012 (0.5196 -- 4.5659)  data: 0.3563 (0.0005 -- 4.0452)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:11  lr: 0.000030  min_lr: 0.000008  loss: 1.5101 (1.4702)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7420 (7.1242)  time: 0.9113 (0.5160 -- 3.2703)  data: 0.2678 (0.0002 -- 2.7225)  max mem: 16413
[2023-09-23 04:53:39,331] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:53:39,331] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:53:39,331] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:53:39,332] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [84]  [ 60/160]  eta: 0:01:43  lr: 0.000030  min_lr: 0.000008  loss: 1.6219 (1.4901)  loss_scale: 32768.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2973 (6.9435)  time: 0.9115 (0.5283 -- 2.3400)  data: 0.1369 (0.0003 -- 1.5525)  max mem: 16413
[2023-09-23 04:53:54,094] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13504
[2023-09-23 04:53:54,094] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:53:54,094] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13504
[2023-09-23 04:53:54,094] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:53:54,094] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [84]  [ 80/160]  eta: 0:01:19  lr: 0.000030  min_lr: 0.000008  loss: 1.2227 (1.4604)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8972 (6.7146)  time: 0.8625 (0.5118 -- 2.2005)  data: 0.0882 (0.0004 -- 1.1481)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:59  lr: 0.000030  min_lr: 0.000008  loss: 1.5168 (1.4645)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4600 (6.6917)  time: 0.9870 (0.5190 -- 3.7635)  data: 0.0119 (0.0004 -- 0.1953)  max mem: 16413
Epoch: [84]  [120/160]  eta: 0:00:39  lr: 0.000030  min_lr: 0.000008  loss: 1.3885 (1.4560)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2233 (6.7375)  time: 0.9810 (0.5092 -- 6.3831)  data: 0.0723 (0.0003 -- 1.1229)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:19  lr: 0.000030  min_lr: 0.000008  loss: 1.4314 (1.4453)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9096 (6.8849)  time: 0.8092 (0.5195 -- 3.1197)  data: 0.0022 (0.0004 -- 0.0179)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.3351 (1.4363)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8437 (6.8873)  time: 0.7632 (0.4935 -- 4.6358)  data: 0.0685 (0.0002 -- 1.1996)  max mem: 16413
Epoch: [84] Total time: 0:02:30 (0.9411 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.3351 (1.4589)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8437 (6.8873)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.5369 (0.5369)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4881 (2.4881 -- 2.4881)  data: 2.2787 (2.2787 -- 2.2787)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2134 (0.4970)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (96.9697)  time: 0.4779 (0.1810 -- 2.4881)  data: 0.2875 (0.0003 -- 2.2787)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2269 (0.4176)  acc1: 88.8889 (91.0053)  acc5: 100.0000 (98.4127)  time: 0.2540 (0.1686 -- 1.0701)  data: 0.0680 (0.0001 -- 0.8778)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3775 (0.4790)  acc1: 88.8889 (89.2116)  acc5: 100.0000 (98.7552)  time: 0.2775 (0.1329 -- 1.0701)  data: 0.0985 (0.0001 -- 0.8778)  max mem: 16413
Val: Total time: 0:00:09 (0.3394 s / it)
* Acc@1 89.212 Acc@5 99.378 loss 0.457
Accuracy of the network on the 482 val images: 89.21%
Max accuracy: 90.46%
Epoch: [85]  [  0/160]  eta: 0:24:29  lr: 0.000030  min_lr: 0.000008  loss: 0.7910 (0.7910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8051 (5.8051)  time: 9.1837 (9.1837 -- 9.1837)  data: 7.3355 (7.3355 -- 7.3355)  max mem: 16413
Epoch: [85]  [ 20/160]  eta: 0:02:56  lr: 0.000030  min_lr: 0.000008  loss: 1.5129 (1.4736)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4140 (7.3210)  time: 0.8675 (0.5169 -- 3.1136)  data: 0.1959 (0.0004 -- 2.6020)  max mem: 16413
[2023-09-23 04:56:06,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:56:06,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:56:06,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:56:06,888] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:56:12,517] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13640
[2023-09-23 04:56:12,518] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13640
[2023-09-23 04:56:12,559] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:56:12,559] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:56:12,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [ 40/160]  eta: 0:02:10  lr: 0.000030  min_lr: 0.000008  loss: 1.5557 (1.4409)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6169 (6.8609)  time: 0.8950 (0.5247 -- 2.7574)  data: 0.2576 (0.0004 -- 1.8659)  max mem: 16413
Epoch: [85]  [ 60/160]  eta: 0:01:44  lr: 0.000030  min_lr: 0.000008  loss: 1.3889 (1.4185)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7504 (6.8191)  time: 0.9674 (0.5118 -- 3.1730)  data: 0.0014 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [85]  [ 80/160]  eta: 0:01:20  lr: 0.000030  min_lr: 0.000008  loss: 1.6269 (1.4621)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2604 (6.9056)  time: 0.8625 (0.5283 -- 2.3274)  data: 0.0013 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:59  lr: 0.000030  min_lr: 0.000008  loss: 1.3331 (1.4518)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7068 (6.9427)  time: 0.9563 (0.5326 -- 3.2802)  data: 0.0013 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [85]  [120/160]  eta: 0:00:38  lr: 0.000030  min_lr: 0.000008  loss: 1.3912 (1.4530)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6228 (7.0582)  time: 0.8713 (0.5248 -- 4.7558)  data: 0.0016 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [85]  [140/160]  eta: 0:00:19  lr: 0.000030  min_lr: 0.000008  loss: 1.4964 (1.4539)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4521 (7.0291)  time: 1.0144 (0.5162 -- 4.1544)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.5986 (1.4716)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4776 (7.0234)  time: 0.6859 (0.4956 -- 3.9033)  data: 0.0005 (0.0001 -- 0.0013)  max mem: 16413
Epoch: [85] Total time: 0:02:31 (0.9443 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.5986 (1.4802)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4776 (7.0234)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2792 (0.2792)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4918 (2.4918 -- 2.4918)  data: 2.3076 (2.3076 -- 2.3076)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2475 (0.4372)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (97.9798)  time: 0.4775 (0.1754 -- 2.4918)  data: 0.2869 (0.0003 -- 2.3076)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2098 (0.3530)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (98.4127)  time: 0.2535 (0.1696 -- 1.0446)  data: 0.0668 (0.0001 -- 0.8441)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2749 (0.4206)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (98.7552)  time: 0.2787 (0.1325 -- 1.0446)  data: 0.0995 (0.0001 -- 0.8441)  max mem: 16413
Val: Total time: 0:00:09 (0.3408 s / it)
* Acc@1 88.174 Acc@5 99.170 loss 0.441
Accuracy of the network on the 482 val images: 88.17%
Max accuracy: 90.46%
Epoch: [86]  [  0/160]  eta: 0:19:38  lr: 0.000030  min_lr: 0.000008  loss: 1.2171 (1.2171)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0912 (8.0912)  time: 7.3686 (7.3686 -- 7.3686)  data: 6.1172 (6.1172 -- 6.1172)  max mem: 16413
[2023-09-23 04:58:25,081] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:58:25,082] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:58:25,081] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 04:58:25,082] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 04:58:28,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13775
[2023-09-23 04:58:28,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:58:28,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13775
[2023-09-23 04:58:28,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 04:58:28,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [86]  [ 20/160]  eta: 0:02:46  lr: 0.000030  min_lr: 0.000008  loss: 1.4284 (1.4861)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9748 (7.3524)  time: 0.8815 (0.5244 -- 4.6410)  data: 0.2155 (0.0003 -- 2.6227)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:15  lr: 0.000030  min_lr: 0.000008  loss: 1.3710 (1.4407)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9526 (7.2056)  time: 1.0577 (0.5352 -- 5.0589)  data: 0.1355 (0.0003 -- 1.5917)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:43  lr: 0.000029  min_lr: 0.000007  loss: 1.3634 (1.4347)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6706 (7.4715)  time: 0.8462 (0.5197 -- 3.6076)  data: 0.0197 (0.0009 -- 0.3607)  max mem: 16413
Epoch: [86]  [ 80/160]  eta: 0:01:22  lr: 0.000029  min_lr: 0.000007  loss: 1.5128 (1.4645)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8125 (7.3397)  time: 1.0340 (0.5188 -- 4.5096)  data: 0.0430 (0.0005 -- 0.8206)  max mem: 16413
Epoch: [86]  [100/160]  eta: 0:00:59  lr: 0.000029  min_lr: 0.000007  loss: 1.4642 (1.4489)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1555 (7.2779)  time: 0.7976 (0.5184 -- 4.7395)  data: 0.0014 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [86]  [120/160]  eta: 0:00:39  lr: 0.000029  min_lr: 0.000007  loss: 1.5540 (1.4607)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2820 (7.3321)  time: 0.9678 (0.5285 -- 3.6469)  data: 0.0122 (0.0003 -- 0.2159)  max mem: 16413
Epoch: [86]  [140/160]  eta: 0:00:19  lr: 0.000029  min_lr: 0.000007  loss: 1.5480 (1.4671)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3889 (7.3428)  time: 0.8596 (0.5265 -- 4.2455)  data: 0.0020 (0.0003 -- 0.0110)  max mem: 16413
[2023-09-23 05:00:29,656] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:00:29,656] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:00:29,656] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:00:29,657] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.4578 (1.4586)  loss_scale: 32768.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0032 (7.1967)  time: 0.7011 (0.4930 -- 3.0951)  data: 0.0008 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [86] Total time: 0:02:29 (0.9359 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.4578 (1.4487)  loss_scale: 32768.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0032 (7.1967)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3045 (0.3045)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4604 (2.4604 -- 2.4604)  data: 2.2718 (2.2718 -- 2.2718)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3045 (0.5219)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (97.9798)  time: 0.4770 (0.1787 -- 2.4604)  data: 0.2830 (0.0003 -- 2.2718)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2649 (0.4252)  acc1: 88.8889 (89.9471)  acc5: 100.0000 (98.9418)  time: 0.2544 (0.1719 -- 1.0201)  data: 0.0622 (0.0001 -- 0.8344)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4044 (0.4831)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2750 (0.1325 -- 1.0201)  data: 0.0925 (0.0001 -- 0.8344)  max mem: 16413
Val: Total time: 0:00:09 (0.3390 s / it)
* Acc@1 89.004 Acc@5 98.963 loss 0.459
Accuracy of the network on the 482 val images: 89.00%
Max accuracy: 90.46%
Epoch: [87]  [  0/160]  eta: 0:23:07  lr: 0.000029  min_lr: 0.000007  loss: 1.3696 (1.3696)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9550 (9.9550)  time: 8.6699 (8.6699 -- 8.6699)  data: 8.1199 (8.1199 -- 8.1199)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:57  lr: 0.000029  min_lr: 0.000007  loss: 1.4776 (1.4364)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6947 (6.9716)  time: 0.9002 (0.5232 -- 4.5530)  data: 0.3106 (0.0002 -- 4.0482)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:17  lr: 0.000029  min_lr: 0.000007  loss: 1.4132 (1.3965)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7942 (6.8268)  time: 1.0195 (0.5183 -- 4.8366)  data: 0.4755 (0.0003 -- 4.3274)  max mem: 16413
[2023-09-23 05:01:38,823] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13966
[2023-09-23 05:01:38,823] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13966
[2023-09-23 05:01:38,823] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:01:38,823] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:01:38,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [87]  [ 60/160]  eta: 0:01:48  lr: 0.000029  min_lr: 0.000007  loss: 1.3356 (1.4056)  loss_scale: 16384.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8197 (6.9029)  time: 0.9572 (0.4979 -- 4.4007)  data: 0.1496 (0.0003 -- 2.6380)  max mem: 16413
[2023-09-23 05:02:10,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=79, lr=[7.38833426229976e-06, 7.38833426229976e-06, 8.20926029144418e-06, 8.20926029144418e-06, 9.121400323826864e-06, 9.121400323826864e-06, 1.0134889248696515e-05, 1.0134889248696515e-05, 1.126098805410724e-05, 1.126098805410724e-05, 1.2512208949008044e-05, 1.2512208949008044e-05, 1.3902454387786715e-05, 1.3902454387786715e-05, 1.5447171541985237e-05, 1.5447171541985237e-05, 1.7163523935539154e-05, 1.7163523935539154e-05, 1.9070582150599057e-05, 1.9070582150599057e-05, 2.1189535722887845e-05, 2.1189535722887845e-05, 2.3543928580986492e-05, 2.3543928580986492e-05, 2.6159920645540545e-05, 2.6159920645540545e-05, 2.906657849504505e-05, 2.906657849504505e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 05:02:10,783] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=17.194661476484477, CurrSamplesPerSec=22.892704366300386, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:22  lr: 0.000029  min_lr: 0.000007  loss: 1.6509 (1.4559)  loss_scale: 16384.0000 (25688.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1513 (6.9044)  time: 0.8569 (0.5121 -- 5.2728)  data: 0.0626 (0.0001 -- 1.2343)  max mem: 16413
Epoch: [87]  [100/160]  eta: 0:01:00  lr: 0.000029  min_lr: 0.000007  loss: 1.3564 (1.4584)  loss_scale: 16384.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8632 (6.8000)  time: 0.8903 (0.5363 -- 3.9729)  data: 0.0185 (0.0003 -- 0.3365)  max mem: 16413
Epoch: [87]  [120/160]  eta: 0:00:39  lr: 0.000029  min_lr: 0.000007  loss: 1.5243 (1.4629)  loss_scale: 16384.0000 (22612.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5702 (6.8420)  time: 0.9321 (0.5243 -- 4.4596)  data: 0.0015 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [87]  [140/160]  eta: 0:00:20  lr: 0.000029  min_lr: 0.000007  loss: 1.3882 (1.4599)  loss_scale: 16384.0000 (21729.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1619 (6.8826)  time: 1.0925 (0.5293 -- 4.9928)  data: 0.0013 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.4988 (1.4640)  loss_scale: 16384.0000 (21094.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6486 (6.8145)  time: 0.6924 (0.4941 -- 3.3815)  data: 0.0005 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [87] Total time: 0:02:34 (0.9684 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.4988 (1.4417)  loss_scale: 16384.0000 (21094.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6486 (6.8145)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1158 (0.1158)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5001 (2.5001 -- 2.5001)  data: 2.2961 (2.2961 -- 2.2961)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2835 (0.5390)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4776 (0.1723 -- 2.5001)  data: 0.2919 (0.0004 -- 2.2961)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2272 (0.4136)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (99.4709)  time: 0.2516 (0.1698 -- 1.1022)  data: 0.0678 (0.0001 -- 0.9100)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2835 (0.4343)  acc1: 88.8889 (88.7967)  acc5: 100.0000 (99.1701)  time: 0.2845 (0.1323 -- 1.1022)  data: 0.1068 (0.0001 -- 0.9100)  max mem: 16413
Val: Total time: 0:00:09 (0.3444 s / it)
* Acc@1 90.249 Acc@5 99.378 loss 0.391
Accuracy of the network on the 482 val images: 90.25%
Max accuracy: 90.46%
Epoch: [88]  [  0/160]  eta: 0:19:22  lr: 0.000029  min_lr: 0.000007  loss: 1.3633 (1.3633)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9296 (3.9296)  time: 7.2674 (7.2674 -- 7.2674)  data: 6.7519 (6.7519 -- 6.7519)  max mem: 16413
[2023-09-23 05:03:43,387] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14087
[2023-09-23 05:03:43,387] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14087
[2023-09-23 05:03:43,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:03:43,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:03:43,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [88]  [ 20/160]  eta: 0:02:52  lr: 0.000029  min_lr: 0.000007  loss: 1.3975 (1.4442)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2858 (7.1602)  time: 0.9267 (0.5253 -- 3.1508)  data: 0.3785 (0.0004 -- 2.6118)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:02:07  lr: 0.000029  min_lr: 0.000007  loss: 1.2947 (1.3808)  loss_scale: 8192.0000 (9590.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7660 (6.9532)  time: 0.8887 (0.5144 -- 3.9146)  data: 0.3469 (0.0002 -- 3.3698)  max mem: 16413
Epoch: [88]  [ 60/160]  eta: 0:01:41  lr: 0.000029  min_lr: 0.000007  loss: 1.4754 (1.4262)  loss_scale: 8192.0000 (9132.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5119 (6.8296)  time: 0.9174 (0.5171 -- 3.1895)  data: 0.3736 (0.0007 -- 2.6585)  max mem: 16413
Epoch: [88]  [ 80/160]  eta: 0:01:19  lr: 0.000029  min_lr: 0.000007  loss: 1.5282 (1.4438)  loss_scale: 8192.0000 (8899.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5249 (6.8265)  time: 0.9150 (0.5255 -- 2.7587)  data: 0.2179 (0.0005 -- 2.2451)  max mem: 16413
Epoch: [88]  [100/160]  eta: 0:00:58  lr: 0.000029  min_lr: 0.000007  loss: 1.3275 (1.4349)  loss_scale: 8192.0000 (8759.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8296 (6.9278)  time: 0.9228 (0.5368 -- 2.3878)  data: 0.1165 (0.0004 -- 0.8179)  max mem: 16413
Epoch: [88]  [120/160]  eta: 0:00:38  lr: 0.000029  min_lr: 0.000007  loss: 1.5807 (1.4632)  loss_scale: 8192.0000 (8665.9174)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6953 (6.9444)  time: 0.8315 (0.5392 -- 3.6396)  data: 0.1053 (0.0005 -- 1.3040)  max mem: 16413
[2023-09-23 05:05:43,248] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:05:43,248] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 05:05:43,248] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:05:43,249] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [88]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.4108 (1.4500)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2255 (7.0405)  time: 0.9286 (0.5241 -- 2.8357)  data: 0.3628 (0.0003 -- 2.3244)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.5179 (1.4473)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9079 (7.0412)  time: 0.7564 (0.4959 -- 2.0333)  data: 0.1485 (0.0002 -- 1.5169)  max mem: 16413
Epoch: [88] Total time: 0:02:28 (0.9281 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.5179 (1.4456)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9079 (7.0412)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1423 (0.1423)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3358 (2.3358 -- 2.3358)  data: 2.1498 (2.1498 -- 2.1498)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2115 (0.4597)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (97.9798)  time: 0.4646 (0.1727 -- 2.3358)  data: 0.2752 (0.0002 -- 2.1498)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2245 (0.3637)  acc1: 88.8889 (91.5344)  acc5: 100.0000 (98.9418)  time: 0.2556 (0.1682 -- 1.0708)  data: 0.0669 (0.0001 -- 0.8643)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2436 (0.4250)  acc1: 88.8889 (89.2116)  acc5: 100.0000 (98.7552)  time: 0.2889 (0.1323 -- 1.0708)  data: 0.1077 (0.0001 -- 0.8643)  max mem: 16413
Val: Total time: 0:00:09 (0.3430 s / it)
* Acc@1 90.249 Acc@5 98.963 loss 0.415
Accuracy of the network on the 482 val images: 90.25%
Max accuracy: 90.46%
Epoch: [89]  [  0/160]  eta: 0:19:22  lr: 0.000029  min_lr: 0.000007  loss: 1.4497 (1.4497)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1377 (6.1377)  time: 7.2631 (7.2631 -- 7.2631)  data: 3.9948 (3.9948 -- 3.9948)  max mem: 16413
Epoch: [89]  [ 20/160]  eta: 0:02:58  lr: 0.000028  min_lr: 0.000007  loss: 1.5023 (1.5146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8264 (6.6925)  time: 0.9740 (0.5167 -- 3.6415)  data: 0.1347 (0.0004 -- 1.8676)  max mem: 16413
Epoch: [89]  [ 40/160]  eta: 0:02:07  lr: 0.000028  min_lr: 0.000007  loss: 1.4708 (1.5160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0927 (7.1614)  time: 0.8383 (0.5240 -- 4.3951)  data: 0.0166 (0.0003 -- 0.2887)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:45  lr: 0.000028  min_lr: 0.000007  loss: 1.4245 (1.5054)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8349 (7.0525)  time: 1.0343 (0.5192 -- 5.0417)  data: 0.1389 (0.0006 -- 1.7127)  max mem: 16413
Epoch: [89]  [ 80/160]  eta: 0:01:19  lr: 0.000028  min_lr: 0.000007  loss: 1.3630 (1.4869)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4861 (6.9164)  time: 0.7940 (0.5272 -- 2.9654)  data: 0.1367 (0.0003 -- 2.4086)  max mem: 16413
Epoch: [89]  [100/160]  eta: 0:00:59  lr: 0.000028  min_lr: 0.000007  loss: 1.4952 (1.4868)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9789 (7.0110)  time: 1.0443 (0.5180 -- 4.7242)  data: 0.0017 (0.0002 -- 0.0130)  max mem: 16413
[2023-09-23 05:07:53,359] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:07:53,359] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:07:53,359] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:07:53,359] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [89]  [120/160]  eta: 0:00:39  lr: 0.000028  min_lr: 0.000007  loss: 1.4238 (1.4789)  loss_scale: 32768.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3261 (6.9556)  time: 0.8806 (0.5294 -- 4.1428)  data: 0.0164 (0.0002 -- 0.2963)  max mem: 16413
[2023-09-23 05:08:20,449] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14371
[2023-09-23 05:08:20,449] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14371
[2023-09-23 05:08:20,449] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:08:20,450] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:08:20,450] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [89]  [140/160]  eta: 0:00:19  lr: 0.000028  min_lr: 0.000007  loss: 1.5771 (1.4899)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8621 (6.8636)  time: 0.9328 (0.5130 -- 3.6271)  data: 0.0016 (0.0003 -- 0.0048)  max mem: 16413
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.3611 (1.4855)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5411 (6.8481)  time: 0.7589 (0.4936 -- 4.3818)  data: 0.0007 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [89] Total time: 0:02:31 (0.9492 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.3611 (1.4946)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5411 (6.8481)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1253 (0.1253)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4311 (2.4311 -- 2.4311)  data: 2.2367 (2.2367 -- 2.2367)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1843 (0.4435)  acc1: 100.0000 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4710 (0.1793 -- 2.4311)  data: 0.2795 (0.0003 -- 2.2367)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2803 (0.3942)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (99.4709)  time: 0.2577 (0.1686 -- 1.0274)  data: 0.0694 (0.0001 -- 0.8299)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3570 (0.4438)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (99.5851)  time: 0.2799 (0.1325 -- 1.0274)  data: 0.1004 (0.0001 -- 0.8299)  max mem: 16413
Val: Total time: 0:00:09 (0.3406 s / it)
* Acc@1 88.589 Acc@5 99.378 loss 0.411
Accuracy of the network on the 482 val images: 88.59%
Max accuracy: 90.46%
Epoch: [90]  [  0/160]  eta: 0:22:50  lr: 0.000028  min_lr: 0.000007  loss: 1.3086 (1.3086)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9002 (5.9002)  time: 8.5634 (8.5634 -- 8.5634)  data: 8.0361 (8.0361 -- 8.0361)  max mem: 16413
Epoch: [90]  [ 20/160]  eta: 0:02:56  lr: 0.000028  min_lr: 0.000007  loss: 1.4571 (1.4957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7574 (6.9218)  time: 0.8959 (0.5169 -- 3.5710)  data: 0.3353 (0.0006 -- 3.0530)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:17  lr: 0.000028  min_lr: 0.000007  loss: 1.5749 (1.5451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4068 (7.1549)  time: 1.0200 (0.5167 -- 4.6114)  data: 0.4840 (0.0004 -- 4.0998)  max mem: 16413
Epoch: [90]  [ 60/160]  eta: 0:01:41  lr: 0.000028  min_lr: 0.000007  loss: 1.5491 (1.5273)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2279 (7.2456)  time: 0.7609 (0.5266 -- 2.9660)  data: 0.2161 (0.0002 -- 2.4511)  max mem: 16413
Epoch: [90]  [ 80/160]  eta: 0:01:21  lr: 0.000028  min_lr: 0.000007  loss: 1.5425 (1.5212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9334 (7.3934)  time: 1.0055 (0.5127 -- 3.5169)  data: 0.4306 (0.0002 -- 3.0068)  max mem: 16413
[2023-09-23 05:10:31,926] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:10:31,926] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:10:31,927] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:10:31,928] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [90]  [100/160]  eta: 0:00:59  lr: 0.000028  min_lr: 0.000007  loss: 1.5377 (1.5193)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4604 (7.3138)  time: 0.9169 (0.5150 -- 4.9755)  data: 0.3160 (0.0003 -- 4.4205)  max mem: 16413
Epoch: [90]  [120/160]  eta: 0:00:38  lr: 0.000028  min_lr: 0.000007  loss: 1.4728 (1.5210)  loss_scale: 32768.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3036 (7.1761)  time: 0.7849 (0.5158 -- 2.5665)  data: 0.0523 (0.0002 -- 0.8228)  max mem: 16413
Epoch: [90]  [140/160]  eta: 0:00:19  lr: 0.000028  min_lr: 0.000007  loss: 1.5156 (1.5154)  loss_scale: 32768.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2336 (7.1301)  time: 1.0315 (0.5219 -- 4.2565)  data: 0.0016 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.4821 (1.5110)  loss_scale: 32768.0000 (22528.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0690 (7.1583)  time: 0.6584 (0.4962 -- 2.2107)  data: 0.0014 (0.0002 -- 0.0124)  max mem: 16413
Epoch: [90] Total time: 0:02:29 (0.9344 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.4821 (1.4946)  loss_scale: 32768.0000 (22528.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0690 (7.1583)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1342 (0.1342)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3018 (2.3018 -- 2.3018)  data: 2.1242 (2.1242 -- 2.1242)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1969 (0.4347)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4692 (0.1778 -- 2.3018)  data: 0.2811 (0.0003 -- 2.1242)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1969 (0.3819)  acc1: 100.0000 (91.5344)  acc5: 100.0000 (99.4709)  time: 0.2628 (0.1685 -- 1.1803)  data: 0.0775 (0.0001 -- 0.9597)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2461 (0.4485)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (99.1701)  time: 0.2979 (0.1319 -- 1.1803)  data: 0.1187 (0.0001 -- 0.9597)  max mem: 16413
Val: Total time: 0:00:09 (0.3470 s / it)
* Acc@1 90.871 Acc@5 99.378 loss 0.405
Accuracy of the network on the 482 val images: 90.87%
[2023-09-23 05:11:30,267] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 05:11:30,269] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 05:11:30,270] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 05:11:30,270] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 05:11:31,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 05:11:31,761] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 90.87%
Epoch: [91]  [  0/160]  eta: 0:18:08  lr: 0.000028  min_lr: 0.000007  loss: 1.5078 (1.5078)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6273 (5.6273)  time: 6.8051 (6.8051 -- 6.8051)  data: 5.9809 (5.9809 -- 5.9809)  max mem: 16413
Epoch: [91]  [ 20/160]  eta: 0:03:03  lr: 0.000028  min_lr: 0.000007  loss: 1.3990 (1.4169)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1275 (6.5040)  time: 1.0367 (0.5221 -- 3.1991)  data: 0.3622 (0.0003 -- 2.6840)  max mem: 16413
Epoch: [91]  [ 40/160]  eta: 0:02:12  lr: 0.000028  min_lr: 0.000007  loss: 1.5577 (1.4499)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2175 (6.7934)  time: 0.8812 (0.5295 -- 3.0183)  data: 0.1417 (0.0004 -- 1.6206)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:46  lr: 0.000028  min_lr: 0.000007  loss: 1.5789 (1.4935)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9505 (6.8628)  time: 0.9836 (0.5099 -- 4.4677)  data: 0.4428 (0.0003 -- 3.9251)  max mem: 16413
[2023-09-23 05:12:46,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:12:46,539] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 05:12:46,540] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:12:46,540] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 05:12:56,310] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14640
[2023-09-23 05:12:56,310] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14640
[2023-09-23 05:12:56,311] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 05:12:56,311] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 05:12:56,311] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [91]  [ 80/160]  eta: 0:01:23  lr: 0.000028  min_lr: 0.000007  loss: 1.3521 (1.4472)  loss_scale: 65536.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5099 (6.5625)  time: 0.9849 (0.5052 -- 5.1653)  data: 0.4528 (0.0002 -- 4.6559)  max mem: 16413
[2023-09-23 05:13:01,609] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14646
[2023-09-23 05:13:01,609] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14646
[2023-09-23 05:13:01,609] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:13:01,609] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:13:01,609] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [91]  [100/160]  eta: 0:01:01  lr: 0.000028  min_lr: 0.000007  loss: 1.4478 (1.4356)  loss_scale: 16384.0000 (34227.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3401 (6.5117)  time: 0.9826 (0.5165 -- 5.5313)  data: 0.4448 (0.0003 -- 5.0171)  max mem: 16413
Epoch: [91]  [120/160]  eta: 0:00:40  lr: 0.000027  min_lr: 0.000007  loss: 1.3014 (1.4195)  loss_scale: 16384.0000 (31278.5455)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8721 (6.4167)  time: 0.8517 (0.5131 -- 3.8257)  data: 0.3136 (0.0003 -- 3.2992)  max mem: 16413
Epoch: [91]  [140/160]  eta: 0:00:19  lr: 0.000027  min_lr: 0.000007  loss: 1.3676 (1.4178)  loss_scale: 16384.0000 (29165.8440)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2106 (6.4232)  time: 0.9608 (0.5261 -- 4.0179)  data: 0.4198 (0.0003 -- 3.5041)  max mem: 16413
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.3411 (1.4121)  loss_scale: 16384.0000 (27648.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6909 (6.4783)  time: 0.6799 (0.4931 -- 3.6089)  data: 0.1601 (0.0002 -- 3.0872)  max mem: 16413
Epoch: [91] Total time: 0:02:33 (0.9595 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.3411 (1.4401)  loss_scale: 16384.0000 (27648.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6909 (6.4783)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1128 (0.1128)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3856 (2.3856 -- 2.3856)  data: 2.1963 (2.1963 -- 2.1963)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1569 (0.4502)  acc1: 100.0000 (90.9091)  acc5: 100.0000 (97.9798)  time: 0.4768 (0.1774 -- 2.3856)  data: 0.2853 (0.0003 -- 2.1963)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2336 (0.3883)  acc1: 100.0000 (91.5344)  acc5: 100.0000 (98.4127)  time: 0.2610 (0.1679 -- 1.1301)  data: 0.0721 (0.0001 -- 0.9336)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2964 (0.4415)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (98.7552)  time: 0.2923 (0.1327 -- 1.1301)  data: 0.1115 (0.0001 -- 0.9336)  max mem: 16413
Val: Total time: 0:00:09 (0.3475 s / it)
* Acc@1 89.834 Acc@5 99.378 loss 0.406
Accuracy of the network on the 482 val images: 89.83%
Max accuracy: 90.87%
Epoch: [92]  [  0/160]  eta: 0:23:32  lr: 0.000027  min_lr: 0.000007  loss: 1.0371 (1.0371)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0608 (6.0608)  time: 8.8306 (8.8306 -- 8.8306)  data: 6.9508 (6.9508 -- 6.9508)  max mem: 16413
Epoch: [92]  [ 20/160]  eta: 0:02:59  lr: 0.000027  min_lr: 0.000007  loss: 1.4315 (1.4362)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9813 (7.0756)  time: 0.9044 (0.5214 -- 4.7573)  data: 0.1591 (0.0004 -- 1.7515)  max mem: 16413
Epoch: [92]  [ 40/160]  eta: 0:02:13  lr: 0.000027  min_lr: 0.000007  loss: 1.5124 (1.4370)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5157 (6.8629)  time: 0.9320 (0.5269 -- 3.8122)  data: 0.3868 (0.0004 -- 3.2821)  max mem: 16413
[2023-09-23 05:15:12,924] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:15:12,924] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:15:12,924] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:15:12,924] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [92]  [ 60/160]  eta: 0:01:43  lr: 0.000027  min_lr: 0.000007  loss: 1.3129 (1.4088)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9627 (6.7751)  time: 0.8738 (0.5246 -- 4.3811)  data: 0.3282 (0.0005 -- 3.8518)  max mem: 16413
Epoch: [92]  [ 80/160]  eta: 0:01:19  lr: 0.000027  min_lr: 0.000007  loss: 1.5198 (1.4663)  loss_scale: 32768.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2234 (7.0635)  time: 0.8814 (0.5240 -- 2.7809)  data: 0.1619 (0.0002 -- 2.1385)  max mem: 16413
Epoch: [92]  [100/160]  eta: 0:01:00  lr: 0.000027  min_lr: 0.000007  loss: 1.2656 (1.4493)  loss_scale: 32768.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3988 (6.9299)  time: 1.0233 (0.5257 -- 3.1039)  data: 0.4876 (0.0004 -- 2.5901)  max mem: 16413
[2023-09-23 05:16:01,817] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14827
[2023-09-23 05:16:01,817] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14827
[2023-09-23 05:16:01,859] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:16:01,859] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:16:01,859] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [92]  [120/160]  eta: 0:00:38  lr: 0.000027  min_lr: 0.000007  loss: 1.4848 (1.4513)  loss_scale: 16384.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0089 (7.0311)  time: 0.8379 (0.5224 -- 4.2430)  data: 0.2826 (0.0004 -- 3.7408)  max mem: 16413
[2023-09-23 05:16:31,075] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14857
[2023-09-23 05:16:31,075] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:16:31,075] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14857
[2023-09-23 05:16:31,075] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:16:31,075] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [92]  [140/160]  eta: 0:00:19  lr: 0.000027  min_lr: 0.000007  loss: 1.5285 (1.4577)  loss_scale: 16384.0000 (22193.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5144 (7.0283)  time: 1.0030 (0.5184 -- 3.4328)  data: 0.4572 (0.0006 -- 2.9000)  max mem: 16413
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.5231 (1.4715)  loss_scale: 8192.0000 (20531.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0301 (6.9298)  time: 0.6627 (0.4939 -- 2.9785)  data: 0.1444 (0.0002 -- 2.4664)  max mem: 16413
Epoch: [92] Total time: 0:02:30 (0.9423 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.5231 (1.4582)  loss_scale: 8192.0000 (20531.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0301 (6.9298)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1571 (0.1571)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4228 (2.4228 -- 2.4228)  data: 2.2405 (2.2405 -- 2.2405)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1416 (0.3636)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (97.9798)  time: 0.4611 (0.1764 -- 2.4228)  data: 0.2721 (0.0003 -- 2.2405)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1624 (0.3186)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (98.4127)  time: 0.2534 (0.1684 -- 0.9416)  data: 0.0649 (0.0001 -- 0.7463)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2019 (0.4029)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (98.7552)  time: 0.2781 (0.1325 -- 0.9416)  data: 0.0989 (0.0001 -- 0.7463)  max mem: 16413
Val: Total time: 0:00:09 (0.3390 s / it)
* Acc@1 90.041 Acc@5 99.170 loss 0.402
Accuracy of the network on the 482 val images: 90.04%
Max accuracy: 90.87%
Epoch: [93]  [  0/160]  eta: 0:20:23  lr: 0.000027  min_lr: 0.000007  loss: 0.9893 (0.9893)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3952 (7.3952)  time: 7.6482 (7.6482 -- 7.6482)  data: 6.2267 (6.2267 -- 6.2267)  max mem: 16413
Epoch: [93]  [ 20/160]  eta: 0:02:55  lr: 0.000027  min_lr: 0.000007  loss: 1.5588 (1.4536)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8026 (6.5525)  time: 0.9306 (0.5182 -- 3.1880)  data: 0.1211 (0.0002 -- 1.4149)  max mem: 16413
Epoch: [93]  [ 40/160]  eta: 0:02:10  lr: 0.000027  min_lr: 0.000007  loss: 1.4044 (1.4257)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2727 (6.3935)  time: 0.9189 (0.5159 -- 2.6383)  data: 0.0144 (0.0005 -- 0.2566)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:40  lr: 0.000027  min_lr: 0.000007  loss: 1.4489 (1.4170)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5774 (6.6778)  time: 0.8227 (0.5256 -- 3.1748)  data: 0.0236 (0.0003 -- 0.4462)  max mem: 16413
Epoch: [93]  [ 80/160]  eta: 0:01:18  lr: 0.000027  min_lr: 0.000007  loss: 1.4030 (1.4198)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7561 (6.8783)  time: 0.8950 (0.5183 -- 3.4402)  data: 0.0015 (0.0004 -- 0.0065)  max mem: 16413
Epoch: [93]  [100/160]  eta: 0:00:59  lr: 0.000027  min_lr: 0.000007  loss: 1.4128 (1.4173)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2904 (6.7916)  time: 1.0351 (0.5261 -- 3.9036)  data: 0.0923 (0.0002 -- 1.6847)  max mem: 16413
[2023-09-23 05:18:39,246] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:18:39,246] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 05:18:39,247] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:18:39,247] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 05:18:50,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=85, lr=[6.800255005360772e-06, 6.800255005360772e-06, 7.555838894845303e-06, 7.555838894845303e-06, 8.395376549828112e-06, 8.395376549828112e-06, 9.328196166475682e-06, 9.328196166475682e-06, 1.0364662407195201e-05, 1.0364662407195201e-05, 1.1516291563550223e-05, 1.1516291563550223e-05, 1.2795879515055802e-05, 1.2795879515055802e-05, 1.4217643905617558e-05, 1.4217643905617558e-05, 1.5797382117352843e-05, 1.5797382117352843e-05, 1.7552646797058713e-05, 1.7552646797058713e-05, 1.9502940885620792e-05, 1.9502940885620792e-05, 2.1669934317356435e-05, 2.1669934317356435e-05, 2.4077704797062705e-05, 2.4077704797062705e-05, 2.675300533006967e-05, 2.675300533006967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 05:18:50,689] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=17.19857472523868, CurrSamplesPerSec=23.391888443911732, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [93]  [120/160]  eta: 0:00:38  lr: 0.000027  min_lr: 0.000007  loss: 1.5868 (1.4397)  loss_scale: 16384.0000 (9207.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7497 (6.8254)  time: 0.8358 (0.5181 -- 2.4027)  data: 0.0390 (0.0004 -- 0.7571)  max mem: 16413
[2023-09-23 05:19:04,975] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15015
[2023-09-23 05:19:04,975] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15015
[2023-09-23 05:19:04,975] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:19:04,975] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:19:04,975] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [93]  [140/160]  eta: 0:00:19  lr: 0.000027  min_lr: 0.000007  loss: 1.4825 (1.4506)  loss_scale: 16384.0000 (9876.8794)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7052 (7.0141)  time: 1.0792 (0.5268 -- 5.3184)  data: 0.0515 (0.0003 -- 0.6399)  max mem: 16413
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.3610 (1.4476)  loss_scale: 8192.0000 (9676.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4809 (6.9399)  time: 0.7099 (0.4934 -- 3.8733)  data: 0.0006 (0.0001 -- 0.0016)  max mem: 16413
Epoch: [93] Total time: 0:02:31 (0.9481 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.3610 (1.4530)  loss_scale: 8192.0000 (9676.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4809 (6.9399)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1178 (0.1178)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5199 (2.5199 -- 2.5199)  data: 2.3089 (2.3089 -- 2.3089)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1613 (0.4170)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4626 (0.1764 -- 2.5199)  data: 0.2721 (0.0003 -- 2.3089)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1613 (0.3433)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2488 (0.1679 -- 0.8750)  data: 0.0620 (0.0001 -- 0.6779)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2133 (0.4362)  acc1: 100.0000 (88.3817)  acc5: 100.0000 (100.0000)  time: 0.2818 (0.1321 -- 0.9791)  data: 0.1022 (0.0001 -- 0.8088)  max mem: 16413
Val: Total time: 0:00:09 (0.3440 s / it)
* Acc@1 89.627 Acc@5 99.585 loss 0.410
Accuracy of the network on the 482 val images: 89.63%
Max accuracy: 90.87%
Epoch: [94]  [  0/160]  eta: 0:23:30  lr: 0.000027  min_lr: 0.000007  loss: 1.5933 (1.5933)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2720 (7.2720)  time: 8.8160 (8.8160 -- 8.8160)  data: 6.9238 (6.9238 -- 6.9238)  max mem: 16413
Epoch: [94]  [ 20/160]  eta: 0:02:58  lr: 0.000027  min_lr: 0.000007  loss: 1.4700 (1.3918)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4193 (6.9114)  time: 0.8977 (0.5148 -- 4.8275)  data: 0.3536 (0.0001 -- 4.3084)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:02:10  lr: 0.000027  min_lr: 0.000007  loss: 1.3837 (1.4265)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0764 (7.1559)  time: 0.8896 (0.5228 -- 3.6459)  data: 0.3221 (0.0005 -- 3.1224)  max mem: 16413
Epoch: [94]  [ 60/160]  eta: 0:01:44  lr: 0.000027  min_lr: 0.000007  loss: 1.3513 (1.4153)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5075 (6.8978)  time: 0.9476 (0.5161 -- 3.2713)  data: 0.2751 (0.0002 -- 1.9685)  max mem: 16413
Epoch: [94]  [ 80/160]  eta: 0:01:20  lr: 0.000026  min_lr: 0.000007  loss: 1.3138 (1.4165)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1930 (7.0939)  time: 0.9018 (0.5150 -- 2.0414)  data: 0.2374 (0.0003 -- 1.5267)  max mem: 16413
Epoch: [94]  [100/160]  eta: 0:00:59  lr: 0.000026  min_lr: 0.000007  loss: 1.4284 (1.4225)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9601 (7.3092)  time: 0.9398 (0.5193 -- 5.0043)  data: 0.3918 (0.0004 -- 4.4611)  max mem: 16413
[2023-09-23 05:21:21,355] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:21:21,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 05:21:21,356] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:21:21,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [94]  [120/160]  eta: 0:00:39  lr: 0.000026  min_lr: 0.000007  loss: 1.4786 (1.4339)  loss_scale: 16384.0000 (9342.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4015 (7.2934)  time: 0.9807 (0.5039 -- 4.6952)  data: 0.4462 (0.0002 -- 4.1685)  max mem: 16413
Epoch: [94]  [140/160]  eta: 0:00:19  lr: 0.000026  min_lr: 0.000007  loss: 1.5118 (1.4498)  loss_scale: 16384.0000 (10341.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9796 (7.1987)  time: 0.8279 (0.5237 -- 2.7698)  data: 0.2837 (0.0005 -- 2.2441)  max mem: 16413
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.4288 (1.4453)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9799 (7.1557)  time: 0.6785 (0.4951 -- 1.9325)  data: 0.1566 (0.0002 -- 1.4178)  max mem: 16413
Epoch: [94] Total time: 0:02:29 (0.9351 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.4288 (1.4625)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9799 (7.1557)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1247 (0.1247)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4130 (2.4130 -- 2.4130)  data: 2.2331 (2.2331 -- 2.2331)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1989 (0.4332)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4631 (0.1794 -- 2.4130)  data: 0.2737 (0.0004 -- 2.2331)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2018 (0.4039)  acc1: 100.0000 (91.0053)  acc5: 100.0000 (99.4709)  time: 0.2552 (0.1685 -- 0.9603)  data: 0.0670 (0.0001 -- 0.7611)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3974 (0.4500)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (99.1701)  time: 0.2758 (0.1322 -- 0.9603)  data: 0.0945 (0.0001 -- 0.7611)  max mem: 16413
Val: Total time: 0:00:09 (0.3357 s / it)
* Acc@1 89.834 Acc@5 98.963 loss 0.423
Accuracy of the network on the 482 val images: 89.83%
Max accuracy: 90.87%
Epoch: [95]  [  0/160]  eta: 0:21:22  lr: 0.000026  min_lr: 0.000007  loss: 1.0619 (1.0619)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8904 (8.8904)  time: 8.0177 (8.0177 -- 8.0177)  data: 7.4588 (7.4588 -- 7.4588)  max mem: 16413
Epoch: [95]  [ 20/160]  eta: 0:02:47  lr: 0.000026  min_lr: 0.000007  loss: 1.6010 (1.5077)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7838 (6.1429)  time: 0.8572 (0.5163 -- 3.8146)  data: 0.3078 (0.0009 -- 3.2855)  max mem: 16413
Epoch: [95]  [ 40/160]  eta: 0:02:09  lr: 0.000026  min_lr: 0.000007  loss: 1.3930 (1.4666)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9857 (6.6818)  time: 0.9607 (0.5176 -- 3.6687)  data: 0.4205 (0.0002 -- 3.1453)  max mem: 16413
Epoch: [95]  [ 60/160]  eta: 0:01:45  lr: 0.000026  min_lr: 0.000007  loss: 1.4229 (1.4644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2534 (6.6537)  time: 0.9937 (0.5218 -- 4.2617)  data: 0.3963 (0.0003 -- 3.6930)  max mem: 16413
[2023-09-23 05:23:27,857] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:23:27,857] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:23:27,858] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:23:27,858] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [95]  [ 80/160]  eta: 0:01:18  lr: 0.000026  min_lr: 0.000007  loss: 1.5159 (1.4684)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4565 (6.7500)  time: 0.7708 (0.5140 -- 3.2170)  data: 0.2312 (0.0002 -- 2.6921)  max mem: 16413
[2023-09-23 05:23:46,602] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15292
[2023-09-23 05:23:46,602] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15292
[2023-09-23 05:23:46,602] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:23:46,602] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:23:46,602] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [95]  [100/160]  eta: 0:00:58  lr: 0.000026  min_lr: 0.000007  loss: 1.4299 (1.4667)  loss_scale: 32768.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3592 (6.9036)  time: 0.9264 (0.5206 -- 3.5836)  data: 0.2622 (0.0003 -- 3.0518)  max mem: 16413
Epoch: [95]  [120/160]  eta: 0:00:38  lr: 0.000026  min_lr: 0.000007  loss: 1.4460 (1.4542)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2450 (6.8315)  time: 0.8894 (0.5316 -- 3.1719)  data: 0.0828 (0.0004 -- 0.8806)  max mem: 16413
Epoch: [95]  [140/160]  eta: 0:00:19  lr: 0.000026  min_lr: 0.000007  loss: 1.3140 (1.4469)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6690 (6.7106)  time: 1.0756 (0.5187 -- 5.2795)  data: 0.0013 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.4027 (1.4441)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3056 (6.7402)  time: 0.6467 (0.4929 -- 1.9833)  data: 0.0006 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [95] Total time: 0:02:29 (0.9369 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.4027 (1.4553)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3056 (6.7402)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1361 (0.1361)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5448 (2.5448 -- 2.5448)  data: 2.3321 (2.3321 -- 2.3321)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2212 (0.4705)  acc1: 100.0000 (87.8788)  acc5: 100.0000 (97.9798)  time: 0.4739 (0.1738 -- 2.5448)  data: 0.2803 (0.0002 -- 2.3321)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2115 (0.3853)  acc1: 100.0000 (91.0053)  acc5: 100.0000 (98.9418)  time: 0.2519 (0.1683 -- 0.9595)  data: 0.0624 (0.0001 -- 0.7460)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2596 (0.4319)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (99.1701)  time: 0.2769 (0.1322 -- 0.9595)  data: 0.0950 (0.0001 -- 0.7460)  max mem: 16413
Val: Total time: 0:00:09 (0.3413 s / it)
* Acc@1 90.871 Acc@5 99.170 loss 0.410
Accuracy of the network on the 482 val images: 90.87%
[2023-09-23 05:24:54,334] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 05:24:54,336] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 05:24:54,336] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 05:24:54,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 05:24:55,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 05:24:55,739] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 90.87%
Epoch: [96]  [  0/160]  eta: 0:23:21  lr: 0.000026  min_lr: 0.000007  loss: 1.8659 (1.8659)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4772 (5.4772)  time: 8.7607 (8.7607 -- 8.7607)  data: 7.5725 (7.5725 -- 7.5725)  max mem: 16413
Epoch: [96]  [ 20/160]  eta: 0:02:47  lr: 0.000026  min_lr: 0.000007  loss: 1.6309 (1.5519)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6043 (7.3160)  time: 0.8188 (0.5169 -- 3.7342)  data: 0.2634 (0.0002 -- 2.8324)  max mem: 16413
Epoch: [96]  [ 40/160]  eta: 0:02:09  lr: 0.000026  min_lr: 0.000007  loss: 1.1425 (1.4458)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0470 (6.9426)  time: 0.9533 (0.5117 -- 3.6944)  data: 0.3540 (0.0004 -- 2.5405)  max mem: 16413
Epoch: [96]  [ 60/160]  eta: 0:01:45  lr: 0.000026  min_lr: 0.000007  loss: 1.4385 (1.4369)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8759 (6.9694)  time: 1.0185 (0.5207 -- 4.0375)  data: 0.4787 (0.0003 -- 3.5211)  max mem: 16413
[2023-09-23 05:26:00,856] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:26:00,856] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:26:00,857] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:26:00,857] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [96]  [ 80/160]  eta: 0:01:20  lr: 0.000026  min_lr: 0.000007  loss: 1.4448 (1.4340)  loss_scale: 32768.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4668 (7.0316)  time: 0.8427 (0.5169 -- 4.4106)  data: 0.2998 (0.0004 -- 3.8759)  max mem: 16413
Epoch: [96]  [100/160]  eta: 0:00:58  lr: 0.000026  min_lr: 0.000007  loss: 1.5259 (1.4352)  loss_scale: 32768.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1837 (6.9589)  time: 0.8584 (0.5238 -- 3.7425)  data: 0.3151 (0.0006 -- 3.2159)  max mem: 16413
Epoch: [96]  [120/160]  eta: 0:00:38  lr: 0.000026  min_lr: 0.000007  loss: 1.3851 (1.4172)  loss_scale: 32768.0000 (24508.2975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1915 (6.8530)  time: 0.8905 (0.5133 -- 3.8692)  data: 0.3152 (0.0002 -- 3.3457)  max mem: 16413
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.4076 (1.4236)  loss_scale: 32768.0000 (25679.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9120 (6.9089)  time: 0.8681 (0.5285 -- 3.6084)  data: 0.3219 (0.0004 -- 3.0632)  max mem: 16413
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000006  loss: 1.5939 (1.4443)  loss_scale: 32768.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7986 (6.9059)  time: 0.8074 (0.4947 -- 2.4929)  data: 0.2873 (0.0002 -- 1.9423)  max mem: 16413
Epoch: [96] Total time: 0:02:29 (0.9339 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000006  loss: 1.5939 (1.4458)  loss_scale: 32768.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7986 (6.9059)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1552 (0.1552)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4177 (2.4177 -- 2.4177)  data: 2.2279 (2.2279 -- 2.2279)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1604 (0.3801)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4676 (0.1774 -- 2.4177)  data: 0.2782 (0.0002 -- 2.2279)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2503 (0.3702)  acc1: 88.8889 (92.0635)  acc5: 100.0000 (99.4709)  time: 0.2590 (0.1705 -- 1.0202)  data: 0.0706 (0.0001 -- 0.8266)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3041 (0.4345)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (99.5851)  time: 0.2789 (0.1319 -- 1.0202)  data: 0.0977 (0.0001 -- 0.8266)  max mem: 16413
Val: Total time: 0:00:09 (0.3381 s / it)
* Acc@1 89.834 Acc@5 99.585 loss 0.452
Accuracy of the network on the 482 val images: 89.83%
Max accuracy: 90.87%
Epoch: [97]  [  0/160]  eta: 0:19:42  lr: 0.000026  min_lr: 0.000006  loss: 1.7212 (1.7212)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1046 (7.1046)  time: 7.3901 (7.3901 -- 7.3901)  data: 6.7204 (6.7204 -- 6.7204)  max mem: 16413
Epoch: [97]  [ 20/160]  eta: 0:02:39  lr: 0.000025  min_lr: 0.000006  loss: 1.5086 (1.4139)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5053 (7.4480)  time: 0.8274 (0.5147 -- 2.5411)  data: 0.2029 (0.0005 -- 2.0030)  max mem: 16413
[2023-09-23 05:28:07,835] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:28:07,836] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 05:28:07,836] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:28:07,836] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 05:28:15,438] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15558
[2023-09-23 05:28:15,438] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15558
[2023-09-23 05:28:15,439] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 05:28:15,439] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 05:28:15,439] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [97]  [ 40/160]  eta: 0:02:12  lr: 0.000025  min_lr: 0.000006  loss: 1.5156 (1.4892)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2210 (6.9764)  time: 1.0605 (0.5098 -- 3.8801)  data: 0.0764 (0.0002 -- 0.8212)  max mem: 16413
Epoch: [97]  [ 60/160]  eta: 0:01:40  lr: 0.000025  min_lr: 0.000006  loss: 1.5301 (1.4841)  loss_scale: 32768.0000 (37602.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1301 (7.0953)  time: 0.7972 (0.5219 -- 3.3183)  data: 0.0013 (0.0004 -- 0.0023)  max mem: 16413
Epoch: [97]  [ 80/160]  eta: 0:01:19  lr: 0.000025  min_lr: 0.000006  loss: 1.3952 (1.4685)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6863 (6.9626)  time: 0.9610 (0.5237 -- 3.9661)  data: 0.0014 (0.0004 -- 0.0027)  max mem: 16413
[2023-09-23 05:28:56,593] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15603
[2023-09-23 05:28:56,593] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15603
[2023-09-23 05:28:56,593] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:28:56,593] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:28:56,593] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [97]  [100/160]  eta: 0:00:58  lr: 0.000025  min_lr: 0.000006  loss: 1.3020 (1.4540)  loss_scale: 16384.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7823 (6.7989)  time: 0.8792 (0.5148 -- 2.7893)  data: 0.0538 (0.0003 -- 1.0371)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:39  lr: 0.000025  min_lr: 0.000006  loss: 1.4094 (1.4553)  loss_scale: 16384.0000 (30059.9008)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2372 (6.7218)  time: 1.0050 (0.5277 -- 3.2461)  data: 0.3223 (0.0002 -- 2.7172)  max mem: 16413
Epoch: [97]  [140/160]  eta: 0:00:19  lr: 0.000025  min_lr: 0.000006  loss: 1.4560 (1.4611)  loss_scale: 16384.0000 (28120.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5074 (6.8032)  time: 0.8761 (0.5071 -- 3.8865)  data: 0.2497 (0.0002 -- 3.3459)  max mem: 16413
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.4813 (1.4576)  loss_scale: 16384.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1380 (6.7549)  time: 0.7187 (0.4943 -- 3.2815)  data: 0.1977 (0.0002 -- 2.7532)  max mem: 16413
Epoch: [97] Total time: 0:02:29 (0.9337 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.4813 (1.4496)  loss_scale: 16384.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1380 (6.7549)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1412 (0.1412)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4985 (2.4985 -- 2.4985)  data: 2.3176 (2.3176 -- 2.3176)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1901 (0.4210)  acc1: 100.0000 (90.9091)  acc5: 100.0000 (97.9798)  time: 0.4867 (0.1771 -- 2.4985)  data: 0.2961 (0.0002 -- 2.3176)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2175 (0.3759)  acc1: 100.0000 (92.0635)  acc5: 100.0000 (98.9418)  time: 0.2576 (0.1687 -- 1.1455)  data: 0.0703 (0.0001 -- 0.9357)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2911 (0.4444)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (98.7552)  time: 0.2824 (0.1325 -- 1.1455)  data: 0.1028 (0.0001 -- 0.9357)  max mem: 16413
Val: Total time: 0:00:09 (0.3439 s / it)
* Acc@1 90.456 Acc@5 99.378 loss 0.413
Accuracy of the network on the 482 val images: 90.46%
Max accuracy: 90.87%
Epoch: [98]  [  0/160]  eta: 0:19:25  lr: 0.000025  min_lr: 0.000006  loss: 1.3476 (1.3476)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4325 (5.4325)  time: 7.2857 (7.2857 -- 7.2857)  data: 6.7553 (6.7553 -- 6.7553)  max mem: 16413
Epoch: [98]  [ 20/160]  eta: 0:02:55  lr: 0.000025  min_lr: 0.000006  loss: 1.4191 (1.4525)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4728 (6.3979)  time: 0.9490 (0.5295 -- 3.4633)  data: 0.2516 (0.0003 -- 1.5944)  max mem: 16413
Epoch: [98]  [ 40/160]  eta: 0:02:10  lr: 0.000025  min_lr: 0.000006  loss: 1.5427 (1.4964)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1713 (6.9021)  time: 0.9139 (0.5189 -- 3.0127)  data: 0.3744 (0.0004 -- 2.5069)  max mem: 16413
[2023-09-23 05:31:09,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:31:09,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:31:09,299] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:31:09,299] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [98]  [ 60/160]  eta: 0:01:44  lr: 0.000025  min_lr: 0.000006  loss: 1.3190 (1.4548)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2850 (7.0519)  time: 0.9539 (0.5188 -- 3.3876)  data: 0.4117 (0.0004 -- 2.8534)  max mem: 16413
Epoch: [98]  [ 80/160]  eta: 0:01:17  lr: 0.000025  min_lr: 0.000006  loss: 1.5025 (1.4644)  loss_scale: 32768.0000 (22249.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8310 (6.8456)  time: 0.7199 (0.5234 -- 2.6002)  data: 0.1780 (0.0002 -- 2.0696)  max mem: 16413
Epoch: [98]  [100/160]  eta: 0:00:58  lr: 0.000025  min_lr: 0.000006  loss: 1.3832 (1.4355)  loss_scale: 32768.0000 (24332.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3951 (6.7440)  time: 0.9862 (0.5192 -- 3.7414)  data: 0.1581 (0.0002 -- 1.3709)  max mem: 16413
Epoch: [98]  [120/160]  eta: 0:00:38  lr: 0.000025  min_lr: 0.000006  loss: 1.5071 (1.4382)  loss_scale: 32768.0000 (25726.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2336 (6.6888)  time: 0.8711 (0.5305 -- 3.8103)  data: 0.2706 (0.0005 -- 3.0850)  max mem: 16413
Epoch: [98]  [140/160]  eta: 0:00:19  lr: 0.000025  min_lr: 0.000006  loss: 1.5069 (1.4477)  loss_scale: 32768.0000 (26725.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7607 (6.7315)  time: 1.1051 (0.5193 -- 5.1652)  data: 0.3172 (0.0003 -- 2.9479)  max mem: 16413
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.3188 (1.4452)  loss_scale: 32768.0000 (27443.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8288 (6.8127)  time: 0.6130 (0.4943 -- 1.5817)  data: 0.0052 (0.0001 -- 0.0926)  max mem: 16413
Epoch: [98] Total time: 0:02:29 (0.9314 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.3188 (1.4369)  loss_scale: 32768.0000 (27443.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8288 (6.8127)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1853 (0.1853)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5057 (2.5057 -- 2.5057)  data: 2.2812 (2.2812 -- 2.2812)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2047 (0.5000)  acc1: 100.0000 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4951 (0.1805 -- 2.5057)  data: 0.2977 (0.0003 -- 2.2812)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2299 (0.4161)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (99.4709)  time: 0.2804 (0.1677 -- 1.2219)  data: 0.0908 (0.0001 -- 0.9857)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2919 (0.4735)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (99.1701)  time: 0.2807 (0.1320 -- 1.2219)  data: 0.0990 (0.0001 -- 0.9857)  max mem: 16413
Val: Total time: 0:00:09 (0.3429 s / it)
* Acc@1 90.041 Acc@5 99.170 loss 0.399
Accuracy of the network on the 482 val images: 90.04%
Max accuracy: 90.87%
Epoch: [99]  [  0/160]  eta: 0:21:22  lr: 0.000025  min_lr: 0.000006  loss: 1.7564 (1.7564)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8699 (3.8699)  time: 8.0150 (8.0150 -- 8.0150)  data: 7.4943 (7.4943 -- 7.4943)  max mem: 16413
[2023-09-23 05:33:18,593] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:33:18,593] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 05:33:18,593] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:33:18,594] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [99]  [ 20/160]  eta: 0:02:56  lr: 0.000025  min_lr: 0.000006  loss: 1.3691 (1.4004)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4792 (6.7947)  time: 0.9249 (0.5252 -- 3.3404)  data: 0.3451 (0.0001 -- 2.8140)  max mem: 16413
[2023-09-23 05:33:33,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15876
[2023-09-23 05:33:33,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15876
[2023-09-23 05:33:33,687] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 05:33:33,687] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 05:33:33,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [99]  [ 40/160]  eta: 0:02:08  lr: 0.000025  min_lr: 0.000006  loss: 1.5721 (1.4791)  loss_scale: 65536.0000 (45555.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2542 (6.4361)  time: 0.8630 (0.5123 -- 3.6862)  data: 0.3054 (0.0004 -- 3.1542)  max mem: 16413
Epoch: [99]  [ 60/160]  eta: 0:01:42  lr: 0.000025  min_lr: 0.000006  loss: 1.5556 (1.4757)  loss_scale: 32768.0000 (41362.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1385 (6.4866)  time: 0.9442 (0.5190 -- 2.9144)  data: 0.3023 (0.0004 -- 2.3946)  max mem: 16413
Epoch: [99]  [ 80/160]  eta: 0:01:20  lr: 0.000025  min_lr: 0.000006  loss: 1.2340 (1.4420)  loss_scale: 32768.0000 (39240.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8335 (6.7405)  time: 0.9272 (0.5087 -- 2.6688)  data: 0.3173 (0.0003 -- 1.8831)  max mem: 16413
Epoch: [99]  [100/160]  eta: 0:00:59  lr: 0.000025  min_lr: 0.000006  loss: 1.2884 (1.4201)  loss_scale: 32768.0000 (37958.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6274 (6.8401)  time: 0.9287 (0.5201 -- 3.8642)  data: 0.3092 (0.0002 -- 3.3194)  max mem: 16413
Epoch: [99]  [120/160]  eta: 0:00:40  lr: 0.000024  min_lr: 0.000006  loss: 1.2995 (1.4170)  loss_scale: 32768.0000 (37100.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7136 (6.8959)  time: 1.0994 (0.5153 -- 4.7063)  data: 0.5588 (0.0003 -- 4.1772)  max mem: 16413
Epoch: [99]  [140/160]  eta: 0:00:19  lr: 0.000024  min_lr: 0.000006  loss: 1.5616 (1.4235)  loss_scale: 32768.0000 (36486.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9455 (6.8397)  time: 0.8461 (0.5209 -- 3.6205)  data: 0.3030 (0.0001 -- 3.1053)  max mem: 16413
[2023-09-23 05:35:24,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=90, lr=[6.20369869372962e-06, 6.20369869372962e-06, 6.892998548588467e-06, 6.892998548588467e-06, 7.658887276209406e-06, 7.658887276209406e-06, 8.509874751343784e-06, 8.509874751343784e-06, 9.455416390381983e-06, 9.455416390381983e-06, 1.0506018211535537e-05, 1.0506018211535537e-05, 1.1673353568372818e-05, 1.1673353568372818e-05, 1.2970392853747574e-05, 1.2970392853747574e-05, 1.4411547615275083e-05, 1.4411547615275083e-05, 1.601283068363898e-05, 1.601283068363898e-05, 1.77920340929322e-05, 1.77920340929322e-05, 1.9768926769924666e-05, 1.9768926769924666e-05, 2.1965474188805186e-05, 2.1965474188805186e-05, 2.440608243200576e-05, 2.440608243200576e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 05:35:24,647] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=17.305621633630984, CurrSamplesPerSec=24.60636146260547, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.4022 (1.4167)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1217 (6.7852)  time: 0.7171 (0.4946 -- 2.8801)  data: 0.1959 (0.0001 -- 2.3901)  max mem: 16413
Epoch: [99] Total time: 0:02:32 (0.9535 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.4022 (1.4413)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1217 (6.7852)
[2023-09-23 05:35:24,650] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-09-23 05:35:24,652] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-09-23 05:35:24,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-09-23 05:35:24,652] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-09-23 05:35:25,618] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-09-23 05:35:25,618] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1282 (0.1282)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5484 (2.5484 -- 2.5484)  data: 2.3655 (2.3655 -- 2.3655)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2013 (0.4690)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (97.9798)  time: 0.4894 (0.1841 -- 2.5484)  data: 0.2962 (0.0002 -- 2.3655)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2338 (0.4091)  acc1: 100.0000 (91.0053)  acc5: 100.0000 (98.4127)  time: 0.2568 (0.1681 -- 1.0888)  data: 0.0680 (0.0001 -- 0.8882)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2714 (0.4481)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (98.7552)  time: 0.2923 (0.1323 -- 1.0888)  data: 0.1108 (0.0001 -- 0.8882)  max mem: 16413
Val: Total time: 0:00:09 (0.3531 s / it)
* Acc@1 90.871 Acc@5 98.963 loss 0.405
Accuracy of the network on the 482 val images: 90.87%
Max accuracy: 90.87%
Epoch: [100]  [  0/160]  eta: 0:21:03  lr: 0.000024  min_lr: 0.000006  loss: 1.9682 (1.9682)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6567 (4.6567)  time: 7.8939 (7.8939 -- 7.8939)  data: 7.3648 (7.3648 -- 7.3648)  max mem: 16413
[2023-09-23 05:35:45,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:35:45,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:35:45,706] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 05:35:45,706] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [100]  [ 20/160]  eta: 0:02:53  lr: 0.000024  min_lr: 0.000006  loss: 1.3398 (1.3940)  loss_scale: 65536.0000 (57734.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6414 (5.6384)  time: 0.9089 (0.5233 -- 5.0073)  data: 0.3723 (0.0008 -- 4.4925)  max mem: 16413
[2023-09-23 05:36:02,270] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16022
[2023-09-23 05:36:02,270] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 05:36:02,270] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-23 05:36:02,271] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16022
[2023-09-23 05:36:02,271] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [100]  [ 40/160]  eta: 0:02:19  lr: 0.000024  min_lr: 0.000006  loss: 1.3682 (1.4123)  loss_scale: 32768.0000 (46354.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3116 (6.4430)  time: 1.0839 (0.5133 -- 4.8600)  data: 0.5523 (0.0002 -- 4.3271)  max mem: 16413
Epoch: [100]  [ 60/160]  eta: 0:01:43  lr: 0.000024  min_lr: 0.000006  loss: 1.4091 (1.4051)  loss_scale: 32768.0000 (41900.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8298 (6.4990)  time: 0.7616 (0.5123 -- 2.8749)  data: 0.2158 (0.0004 -- 2.3632)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:19  lr: 0.000024  min_lr: 0.000006  loss: 1.4286 (1.4173)  loss_scale: 32768.0000 (39645.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5743 (6.6287)  time: 0.8872 (0.5239 -- 3.1528)  data: 0.3412 (0.0006 -- 2.6140)  max mem: 16413
[2023-09-23 05:37:07,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16092
[2023-09-23 05:37:07,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16092
[2023-09-23 05:37:07,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:37:07,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:37:07,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [100/160]  eta: 0:00:59  lr: 0.000024  min_lr: 0.000006  loss: 1.4591 (1.4258)  loss_scale: 32768.0000 (36823.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6324 (6.6523)  time: 1.0087 (0.5172 -- 4.4020)  data: 0.4261 (0.0007 -- 3.8692)  max mem: 16413
Epoch: [100]  [120/160]  eta: 0:00:39  lr: 0.000024  min_lr: 0.000006  loss: 1.3961 (1.4331)  loss_scale: 16384.0000 (33445.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7841 (6.6545)  time: 0.9310 (0.5226 -- 3.3985)  data: 0.2673 (0.0001 -- 1.7663)  max mem: 16413
[2023-09-23 05:37:49,184] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16136
[2023-09-23 05:37:49,185] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:37:49,185] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16136
[2023-09-23 05:37:49,185] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:37:49,185] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [100]  [140/160]  eta: 0:00:19  lr: 0.000024  min_lr: 0.000006  loss: 1.4790 (1.4378)  loss_scale: 16384.0000 (30734.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4574 (6.7261)  time: 0.8355 (0.5195 -- 4.9122)  data: 0.0980 (0.0004 -- 1.9336)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.4654 (1.4404)  loss_scale: 8192.0000 (28057.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7438 (6.7346)  time: 0.7075 (0.4932 -- 3.8207)  data: 0.0440 (0.0002 -- 0.8651)  max mem: 16413
Epoch: [100] Total time: 0:02:29 (0.9367 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.4654 (1.4566)  loss_scale: 8192.0000 (28057.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7438 (6.7346)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1543 (0.1543)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4958 (2.4958 -- 2.4958)  data: 2.2813 (2.2813 -- 2.2813)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1636 (0.3706)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4610 (0.1794 -- 2.4958)  data: 0.2697 (0.0002 -- 2.2813)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2128 (0.3497)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2502 (0.1688 -- 0.8713)  data: 0.0617 (0.0001 -- 0.6795)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2957 (0.3996)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2763 (0.1326 -- 0.8713)  data: 0.0962 (0.0001 -- 0.6949)  max mem: 16413
Val: Total time: 0:00:09 (0.3398 s / it)
* Acc@1 91.286 Acc@5 99.793 loss 0.400
Accuracy of the network on the 482 val images: 91.29%
[2023-09-23 05:38:14,706] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 05:38:14,708] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 05:38:14,708] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 05:38:14,708] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 05:38:16,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 05:38:16,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 91.29%
Epoch: [101]  [  0/160]  eta: 0:19:19  lr: 0.000024  min_lr: 0.000006  loss: 1.6149 (1.6149)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4962 (5.4962)  time: 7.2499 (7.2499 -- 7.2499)  data: 5.7433 (5.7433 -- 5.7433)  max mem: 16413
Epoch: [101]  [ 20/160]  eta: 0:02:43  lr: 0.000024  min_lr: 0.000006  loss: 1.2474 (1.2754)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6035 (6.0511)  time: 0.8666 (0.5201 -- 3.3373)  data: 0.0673 (0.0003 -- 0.9294)  max mem: 16413
Epoch: [101]  [ 40/160]  eta: 0:02:09  lr: 0.000024  min_lr: 0.000006  loss: 1.5473 (1.3562)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8132 (6.7869)  time: 0.9807 (0.5187 -- 4.7141)  data: 0.0644 (0.0004 -- 1.2538)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:46  lr: 0.000024  min_lr: 0.000006  loss: 1.3665 (1.3660)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8959 (6.8699)  time: 1.0446 (0.5167 -- 4.4014)  data: 0.0016 (0.0004 -- 0.0085)  max mem: 16413
Epoch: [101]  [ 80/160]  eta: 0:01:19  lr: 0.000024  min_lr: 0.000006  loss: 1.3169 (1.3819)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7272 (7.1623)  time: 0.7521 (0.5086 -- 4.1872)  data: 0.0018 (0.0002 -- 0.0069)  max mem: 16413
Epoch: [101]  [100/160]  eta: 0:00:59  lr: 0.000024  min_lr: 0.000006  loss: 1.4820 (1.3949)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5587 (7.2552)  time: 0.9938 (0.5219 -- 3.3264)  data: 0.0017 (0.0003 -- 0.0072)  max mem: 16413
[2023-09-23 05:39:59,239] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:39:59,239] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 05:39:59,241] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:39:59,241] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [101]  [120/160]  eta: 0:00:39  lr: 0.000024  min_lr: 0.000006  loss: 1.4208 (1.3990)  loss_scale: 16384.0000 (9275.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5543 (7.2851)  time: 0.9380 (0.5110 -- 4.3327)  data: 0.0013 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [101]  [140/160]  eta: 0:00:19  lr: 0.000024  min_lr: 0.000006  loss: 1.3418 (1.4019)  loss_scale: 16384.0000 (10283.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9247 (7.1057)  time: 0.9717 (0.5232 -- 4.2007)  data: 0.0011 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.5287 (1.4101)  loss_scale: 16384.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3033 (7.0398)  time: 0.6745 (0.4936 -- 2.5337)  data: 0.0005 (0.0002 -- 0.0011)  max mem: 16413
Epoch: [101] Total time: 0:02:31 (0.9449 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.5287 (1.4298)  loss_scale: 16384.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3033 (7.0398)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1605 (0.1605)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4894 (2.4894 -- 2.4894)  data: 2.2477 (2.2477 -- 2.2477)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2676 (0.3923)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4876 (0.1780 -- 2.4894)  data: 0.2851 (0.0005 -- 2.2477)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2237 (0.3402)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (99.4709)  time: 0.2531 (0.1687 -- 1.0713)  data: 0.0595 (0.0001 -- 0.8808)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3534 (0.4221)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (99.1701)  time: 0.2795 (0.1326 -- 1.0713)  data: 0.0959 (0.0001 -- 0.8808)  max mem: 16413
Val: Total time: 0:00:09 (0.3433 s / it)
* Acc@1 91.286 Acc@5 99.170 loss 0.397
Accuracy of the network on the 482 val images: 91.29%
Max accuracy: 91.29%
Epoch: [102]  [  0/160]  eta: 0:21:16  lr: 0.000024  min_lr: 0.000006  loss: 1.6545 (1.6545)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8013 (7.8013)  time: 7.9756 (7.9756 -- 7.9756)  data: 7.4289 (7.4289 -- 7.4289)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:51  lr: 0.000024  min_lr: 0.000006  loss: 1.4385 (1.4258)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4947 (5.9007)  time: 0.8912 (0.5221 -- 4.5700)  data: 0.2797 (0.0002 -- 3.2547)  max mem: 16413
Epoch: [102]  [ 40/160]  eta: 0:02:03  lr: 0.000024  min_lr: 0.000006  loss: 1.3157 (1.4152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1127 (6.4314)  time: 0.8191 (0.5176 -- 2.3905)  data: 0.0868 (0.0004 -- 1.1486)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:38  lr: 0.000024  min_lr: 0.000006  loss: 1.4766 (1.4098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6669 (6.4457)  time: 0.9101 (0.5305 -- 2.8472)  data: 0.2800 (0.0002 -- 2.1333)  max mem: 16413
[2023-09-23 05:42:08,770] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:42:08,770] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:42:08,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:42:08,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [102]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000006  loss: 1.3000 (1.4008)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7737 (6.4729)  time: 0.8833 (0.5250 -- 3.1225)  data: 0.1465 (0.0003 -- 1.1709)  max mem: 16413
Epoch: [102]  [100/160]  eta: 0:00:58  lr: 0.000023  min_lr: 0.000006  loss: 1.3855 (1.4058)  loss_scale: 32768.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8856 (6.5836)  time: 0.9882 (0.5299 -- 3.0458)  data: 0.1323 (0.0004 -- 1.7050)  max mem: 16413
Epoch: [102]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000006  loss: 1.4902 (1.4186)  loss_scale: 32768.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2873 (6.6960)  time: 0.8029 (0.5306 -- 3.8289)  data: 0.0019 (0.0003 -- 0.0074)  max mem: 16413
Epoch: [102]  [140/160]  eta: 0:00:19  lr: 0.000023  min_lr: 0.000006  loss: 1.4750 (1.4173)  loss_scale: 32768.0000 (24285.5035)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0883 (6.8611)  time: 1.1497 (0.5106 -- 3.9974)  data: 0.3378 (0.0005 -- 3.4382)  max mem: 16413
[2023-09-23 05:43:20,120] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16468
[2023-09-23 05:43:20,120] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16468
[2023-09-23 05:43:20,120] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:43:20,120] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:43:20,120] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.5195 (1.4253)  loss_scale: 16384.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6356 (6.8673)  time: 0.6761 (0.4811 -- 2.7572)  data: 0.0005 (0.0001 -- 0.0013)  max mem: 16413
Epoch: [102] Total time: 0:02:29 (0.9369 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.5195 (1.4235)  loss_scale: 16384.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6356 (6.8673)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1689 (0.1689)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4596 (2.4596 -- 2.4596)  data: 2.2726 (2.2726 -- 2.2726)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2542 (0.4173)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4908 (0.1813 -- 2.4596)  data: 0.3003 (0.0003 -- 2.2726)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2712 (0.3868)  acc1: 88.8889 (91.0053)  acc5: 100.0000 (98.9418)  time: 0.2574 (0.1684 -- 1.2405)  data: 0.0715 (0.0001 -- 1.0248)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4306 (0.4549)  acc1: 88.8889 (88.7967)  acc5: 100.0000 (99.1701)  time: 0.2845 (0.1322 -- 1.2405)  data: 0.1057 (0.0001 -- 1.0248)  max mem: 16413
Val: Total time: 0:00:09 (0.3437 s / it)
* Acc@1 89.834 Acc@5 99.378 loss 0.422
Accuracy of the network on the 482 val images: 89.83%
Max accuracy: 91.29%
Epoch: [103]  [  0/160]  eta: 0:18:48  lr: 0.000023  min_lr: 0.000006  loss: 1.6912 (1.6912)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0998 (9.0998)  time: 7.0544 (7.0544 -- 7.0544)  data: 5.3881 (5.3881 -- 5.3881)  max mem: 16413
Epoch: [103]  [ 20/160]  eta: 0:02:50  lr: 0.000023  min_lr: 0.000006  loss: 1.4706 (1.4016)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9867 (6.6617)  time: 0.9237 (0.5137 -- 3.1969)  data: 0.1143 (0.0002 -- 1.2865)  max mem: 16413
Epoch: [103]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000006  loss: 1.4005 (1.4512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7708 (6.9129)  time: 0.9101 (0.5245 -- 3.4696)  data: 0.2363 (0.0003 -- 2.9405)  max mem: 16413
Epoch: [103]  [ 60/160]  eta: 0:01:45  lr: 0.000023  min_lr: 0.000006  loss: 1.4318 (1.4662)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4974 (6.9644)  time: 1.0306 (0.5174 -- 5.5837)  data: 0.4932 (0.0004 -- 5.0668)  max mem: 16413
Epoch: [103]  [ 80/160]  eta: 0:01:21  lr: 0.000023  min_lr: 0.000006  loss: 1.2310 (1.4157)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6594 (6.9900)  time: 0.9008 (0.5027 -- 5.1353)  data: 0.3758 (0.0003 -- 4.6424)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:01:01  lr: 0.000023  min_lr: 0.000006  loss: 1.4181 (1.4265)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6265 (7.1175)  time: 1.0206 (0.5186 -- 4.1164)  data: 0.4791 (0.0004 -- 3.5884)  max mem: 16413
[2023-09-23 05:45:32,974] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:45:32,974] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:45:32,975] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:45:32,975] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [103]  [120/160]  eta: 0:00:39  lr: 0.000023  min_lr: 0.000006  loss: 1.3546 (1.4219)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5407 (7.2572)  time: 0.7794 (0.5219 -- 3.0908)  data: 0.2374 (0.0003 -- 2.5546)  max mem: 16413
[2023-09-23 05:45:38,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16603
[2023-09-23 05:45:38,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16603
[2023-09-23 05:45:38,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:45:38,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:45:38,355] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 05:45:48,558] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16616
[2023-09-23 05:45:48,558] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16616
[2023-09-23 05:45:48,558] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:45:48,559] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:45:48,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [103]  [140/160]  eta: 0:00:19  lr: 0.000023  min_lr: 0.000006  loss: 1.4829 (1.4216)  loss_scale: 16384.0000 (16790.6950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7827 (7.1727)  time: 1.0298 (0.5179 -- 5.0250)  data: 0.4890 (0.0004 -- 4.5204)  max mem: 16413
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.4405 (1.4309)  loss_scale: 8192.0000 (15769.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3796 (7.1131)  time: 0.7015 (0.4926 -- 3.0468)  data: 0.1915 (0.0002 -- 2.5290)  max mem: 16413
Epoch: [103] Total time: 0:02:32 (0.9530 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.4405 (1.4207)  loss_scale: 8192.0000 (15769.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3796 (7.1131)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3420 (0.3420)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4137 (2.4137 -- 2.4137)  data: 2.2149 (2.2149 -- 2.2149)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1944 (0.4694)  acc1: 100.0000 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4826 (0.1771 -- 2.4137)  data: 0.2903 (0.0003 -- 2.2149)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1835 (0.3737)  acc1: 100.0000 (91.0053)  acc5: 100.0000 (100.0000)  time: 0.2523 (0.1691 -- 1.1619)  data: 0.0645 (0.0001 -- 0.9731)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1944 (0.4189)  acc1: 100.0000 (89.6266)  acc5: 100.0000 (99.5851)  time: 0.2667 (0.1349 -- 1.1619)  data: 0.0884 (0.0001 -- 0.9731)  max mem: 16413
Val: Total time: 0:00:08 (0.3321 s / it)
* Acc@1 90.871 Acc@5 99.585 loss 0.393
Accuracy of the network on the 482 val images: 90.87%
Max accuracy: 91.29%
Epoch: [104]  [  0/160]  eta: 0:14:16  lr: 0.000023  min_lr: 0.000006  loss: 1.6317 (1.6317)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8386 (7.8386)  time: 5.3502 (5.3502 -- 5.3502)  data: 4.7771 (4.7771 -- 4.7771)  max mem: 16413
Epoch: [104]  [ 20/160]  eta: 0:02:50  lr: 0.000023  min_lr: 0.000006  loss: 1.2692 (1.3179)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7340 (6.9341)  time: 1.0131 (0.5297 -- 3.9130)  data: 0.4167 (0.0004 -- 3.3695)  max mem: 16413
Epoch: [104]  [ 40/160]  eta: 0:02:05  lr: 0.000023  min_lr: 0.000006  loss: 1.4183 (1.3507)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7658 (6.9017)  time: 0.8718 (0.5286 -- 3.5051)  data: 0.1480 (0.0006 -- 0.9723)  max mem: 16413
Epoch: [104]  [ 60/160]  eta: 0:01:42  lr: 0.000023  min_lr: 0.000006  loss: 1.5321 (1.3993)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6428 (6.9979)  time: 0.9693 (0.5263 -- 4.1561)  data: 0.2939 (0.0003 -- 3.6389)  max mem: 16413
Epoch: [104]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000006  loss: 1.3649 (1.3966)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5605 (7.0592)  time: 0.8655 (0.5312 -- 2.6816)  data: 0.0817 (0.0002 -- 1.6007)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:59  lr: 0.000023  min_lr: 0.000006  loss: 1.3768 (1.4034)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5129 (7.0170)  time: 0.9886 (0.5317 -- 3.7199)  data: 0.0014 (0.0003 -- 0.0028)  max mem: 16413
[2023-09-23 05:48:03,884] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:48:03,884] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 05:48:03,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:48:03,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [104]  [120/160]  eta: 0:00:38  lr: 0.000023  min_lr: 0.000006  loss: 1.5574 (1.4303)  loss_scale: 16384.0000 (9275.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9717 (7.0169)  time: 0.9167 (0.5216 -- 4.5377)  data: 0.0015 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [104]  [140/160]  eta: 0:00:19  lr: 0.000023  min_lr: 0.000006  loss: 1.6477 (1.4595)  loss_scale: 16384.0000 (10283.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8168 (7.0474)  time: 1.0958 (0.5065 -- 6.7126)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.2997 (1.4405)  loss_scale: 16384.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6029 (7.0445)  time: 0.5456 (0.4917 -- 0.9678)  data: 0.0006 (0.0001 -- 0.0015)  max mem: 16413
Epoch: [104] Total time: 0:02:30 (0.9386 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.2997 (1.4401)  loss_scale: 16384.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6029 (7.0445)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1227 (0.1227)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4163 (2.4163 -- 2.4163)  data: 2.2224 (2.2224 -- 2.2224)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2206 (0.4371)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4760 (0.1753 -- 2.4163)  data: 0.2873 (0.0002 -- 2.2224)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2313 (0.3748)  acc1: 100.0000 (92.0635)  acc5: 100.0000 (99.4709)  time: 0.2553 (0.1687 -- 1.1262)  data: 0.0680 (0.0001 -- 0.9333)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2727 (0.4200)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (99.1701)  time: 0.2872 (0.1328 -- 1.1262)  data: 0.1078 (0.0001 -- 0.9333)  max mem: 16413
Val: Total time: 0:00:09 (0.3446 s / it)
* Acc@1 91.494 Acc@5 99.170 loss 0.388
Accuracy of the network on the 482 val images: 91.49%
[2023-09-23 05:48:57,770] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 05:48:57,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 05:48:57,772] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 05:48:57,772] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 05:48:59,323] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 05:48:59,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 91.49%
Epoch: [105]  [  0/160]  eta: 0:23:52  lr: 0.000023  min_lr: 0.000006  loss: 1.5028 (1.5028)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6236 (5.6236)  time: 8.9546 (8.9546 -- 8.9546)  data: 6.7506 (6.7506 -- 6.7506)  max mem: 16413
Epoch: [105]  [ 20/160]  eta: 0:02:57  lr: 0.000022  min_lr: 0.000006  loss: 1.4180 (1.3600)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5052 (7.1443)  time: 0.8868 (0.5273 -- 4.9643)  data: 0.1905 (0.0002 -- 2.3053)  max mem: 16413
Epoch: [105]  [ 40/160]  eta: 0:02:16  lr: 0.000022  min_lr: 0.000006  loss: 1.5170 (1.4578)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5506 (7.0479)  time: 0.9957 (0.5296 -- 4.6609)  data: 0.1309 (0.0003 -- 1.1494)  max mem: 16413
Epoch: [105]  [ 60/160]  eta: 0:01:45  lr: 0.000022  min_lr: 0.000006  loss: 1.4725 (1.4598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7144 (7.0593)  time: 0.8967 (0.5228 -- 3.4217)  data: 0.1739 (0.0004 -- 1.5880)  max mem: 16413
[2023-09-23 05:50:15,208] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:50:15,209] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:50:15,209] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:50:15,209] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [105]  [ 80/160]  eta: 0:01:21  lr: 0.000022  min_lr: 0.000006  loss: 1.4971 (1.4552)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7502 (7.0967)  time: 0.9121 (0.5186 -- 3.2963)  data: 0.0470 (0.0003 -- 0.5636)  max mem: 16413
[2023-09-23 05:50:24,644] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16884
[2023-09-23 05:50:24,644] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16884
[2023-09-23 05:50:24,644] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:50:24,644] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:50:24,644] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [105]  [100/160]  eta: 0:01:00  lr: 0.000022  min_lr: 0.000006  loss: 1.3403 (1.4273)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7753 (6.9351)  time: 0.9594 (0.5262 -- 2.9605)  data: 0.2384 (0.0007 -- 2.3737)  max mem: 16413
Epoch: [105]  [120/160]  eta: 0:00:38  lr: 0.000022  min_lr: 0.000006  loss: 1.4710 (1.4330)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5707 (7.0676)  time: 0.7771 (0.5094 -- 3.5821)  data: 0.2313 (0.0001 -- 3.0518)  max mem: 16413
Epoch: [105]  [140/160]  eta: 0:00:19  lr: 0.000022  min_lr: 0.000006  loss: 1.4621 (1.4255)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8371 (7.0115)  time: 1.0470 (0.5228 -- 4.7075)  data: 0.5048 (0.0004 -- 4.1967)  max mem: 16413
[2023-09-23 05:51:24,465] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16951
[2023-09-23 05:51:24,465] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16951
[2023-09-23 05:51:24,465] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:51:24,465] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 05:51:24,465] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.6347 (1.4403)  loss_scale: 16384.0000 (17049.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8302 (7.1688)  time: 0.8098 (0.4942 -- 4.7075)  data: 0.2939 (0.0002 -- 4.1967)  max mem: 16413
Epoch: [105] Total time: 0:02:29 (0.9374 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.6347 (1.4320)  loss_scale: 16384.0000 (17049.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8302 (7.1688)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1296 (0.1296)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3020 (2.3020 -- 2.3020)  data: 2.1066 (2.1066 -- 2.1066)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1553 (0.4491)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (97.9798)  time: 0.4587 (0.1799 -- 2.3020)  data: 0.2680 (0.0004 -- 2.1066)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2573 (0.3964)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (98.9418)  time: 0.2584 (0.1691 -- 1.0421)  data: 0.0714 (0.0001 -- 0.8345)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3456 (0.4423)  acc1: 88.8889 (88.7967)  acc5: 100.0000 (99.1701)  time: 0.2864 (0.1328 -- 1.0421)  data: 0.1067 (0.0001 -- 0.8345)  max mem: 16413
Val: Total time: 0:00:09 (0.3395 s / it)
* Acc@1 90.456 Acc@5 99.378 loss 0.403
Accuracy of the network on the 482 val images: 90.46%
Max accuracy: 91.49%
Epoch: [106]  [  0/160]  eta: 0:24:31  lr: 0.000022  min_lr: 0.000006  loss: 1.2693 (1.2693)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3288 (10.3288)  time: 9.1994 (9.1994 -- 9.1994)  data: 5.7155 (5.7155 -- 5.7155)  max mem: 16413
Epoch: [106]  [ 20/160]  eta: 0:02:57  lr: 0.000022  min_lr: 0.000006  loss: 1.4093 (1.4804)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6357 (7.0049)  time: 0.8732 (0.5227 -- 4.0605)  data: 0.0023 (0.0003 -- 0.0165)  max mem: 16413
[2023-09-23 05:52:21,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=98, lr=[5.604708639631972e-06, 5.604708639631972e-06, 6.227454044035524e-06, 6.227454044035524e-06, 6.919393382261692e-06, 6.919393382261692e-06, 7.688214869179659e-06, 7.688214869179659e-06, 8.542460965755176e-06, 8.542460965755176e-06, 9.49162329528353e-06, 9.49162329528353e-06, 1.0546248105870587e-05, 1.0546248105870587e-05, 1.1718053450967318e-05, 1.1718053450967318e-05, 1.3020059389963686e-05, 1.3020059389963686e-05, 1.4466732655515206e-05, 1.4466732655515206e-05, 1.6074147395016896e-05, 1.6074147395016896e-05, 1.7860163772240996e-05, 1.7860163772240996e-05, 1.9844626413601107e-05, 1.9844626413601107e-05, 2.2049584904001228e-05, 2.2049584904001228e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 05:52:21,990] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=17.276422585106186, CurrSamplesPerSec=23.13302131817595, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:02:08  lr: 0.000022  min_lr: 0.000006  loss: 1.6282 (1.5041)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4815 (7.1728)  time: 0.8702 (0.5181 -- 4.0653)  data: 0.0014 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [106]  [ 60/160]  eta: 0:01:44  lr: 0.000022  min_lr: 0.000006  loss: 1.3570 (1.4663)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1424 (7.5897)  time: 0.9835 (0.5109 -- 2.8134)  data: 0.1382 (0.0005 -- 1.4315)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:21  lr: 0.000022  min_lr: 0.000006  loss: 1.3106 (1.4280)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1438 (7.3716)  time: 0.9471 (0.5121 -- 3.5495)  data: 0.0087 (0.0002 -- 0.1447)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:59  lr: 0.000022  min_lr: 0.000006  loss: 1.5106 (1.4254)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8502 (7.2398)  time: 0.8735 (0.5151 -- 3.1935)  data: 0.2081 (0.0003 -- 2.6644)  max mem: 16413
[2023-09-23 05:53:38,908] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:53:38,908] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 05:53:38,908] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:53:38,908] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [106]  [120/160]  eta: 0:00:39  lr: 0.000022  min_lr: 0.000006  loss: 1.4587 (1.4319)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8522 (7.1937)  time: 1.0058 (0.5299 -- 2.9228)  data: 0.1873 (0.0004 -- 2.1149)  max mem: 16413
Epoch: [106]  [140/160]  eta: 0:00:19  lr: 0.000022  min_lr: 0.000006  loss: 1.5301 (1.4420)  loss_scale: 16384.0000 (9412.0851)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9420 (7.0593)  time: 0.8597 (0.5156 -- 3.8181)  data: 0.0226 (0.0006 -- 0.4236)  max mem: 16413
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.5411 (1.4513)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3327 (7.0781)  time: 0.7170 (0.4918 -- 3.1341)  data: 0.0008 (0.0001 -- 0.0031)  max mem: 16413
Epoch: [106] Total time: 0:02:31 (0.9456 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.5411 (1.4470)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3327 (7.0781)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1422 (0.1422)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4658 (2.4658 -- 2.4658)  data: 2.2217 (2.2217 -- 2.2217)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1627 (0.3219)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4623 (0.1765 -- 2.4658)  data: 0.2678 (0.0002 -- 2.2217)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2122 (0.2797)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2473 (0.1690 -- 0.9255)  data: 0.0619 (0.0001 -- 0.7206)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2443 (0.3522)  acc1: 100.0000 (92.1162)  acc5: 100.0000 (99.5851)  time: 0.2797 (0.1331 -- 0.9604)  data: 0.1011 (0.0001 -- 0.7862)  max mem: 16413
Val: Total time: 0:00:09 (0.3402 s / it)
* Acc@1 92.739 Acc@5 99.585 loss 0.348
Accuracy of the network on the 482 val images: 92.74%
[2023-09-23 05:54:19,455] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 05:54:19,457] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 05:54:19,457] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 05:54:19,457] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 05:54:20,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 05:54:20,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 92.74%
Epoch: [107]  [  0/160]  eta: 0:20:16  lr: 0.000022  min_lr: 0.000006  loss: 1.6142 (1.6142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9873 (4.9873)  time: 7.6018 (7.6018 -- 7.6018)  data: 7.0579 (7.0579 -- 7.0579)  max mem: 16413
Epoch: [107]  [ 20/160]  eta: 0:03:05  lr: 0.000022  min_lr: 0.000006  loss: 1.5070 (1.4266)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0366 (6.9615)  time: 1.0080 (0.5125 -- 3.9276)  data: 0.2915 (0.0004 -- 3.3991)  max mem: 16413
Epoch: [107]  [ 40/160]  eta: 0:02:11  lr: 0.000022  min_lr: 0.000006  loss: 1.5441 (1.4597)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6542 (7.2155)  time: 0.8508 (0.5081 -- 4.2197)  data: 0.2348 (0.0001 -- 2.3583)  max mem: 16413
Epoch: [107]  [ 60/160]  eta: 0:01:47  lr: 0.000022  min_lr: 0.000005  loss: 1.4521 (1.4436)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0837 (7.1432)  time: 1.0293 (0.5253 -- 4.4367)  data: 0.3381 (0.0004 -- 3.3238)  max mem: 16413
Epoch: [107]  [ 80/160]  eta: 0:01:22  lr: 0.000022  min_lr: 0.000005  loss: 1.4359 (1.4392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5916 (7.1043)  time: 0.9290 (0.5061 -- 5.8205)  data: 0.3884 (0.0003 -- 5.3050)  max mem: 16413
[2023-09-23 05:55:50,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:55:50,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:55:50,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:55:50,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [107]  [100/160]  eta: 0:01:00  lr: 0.000022  min_lr: 0.000005  loss: 1.4036 (1.4307)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6117 (6.8559)  time: 0.9096 (0.5217 -- 4.3475)  data: 0.3648 (0.0002 -- 3.8007)  max mem: 16413
[2023-09-23 05:56:17,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17238
[2023-09-23 05:56:17,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:56:17,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 05:56:17,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17238
[2023-09-23 05:56:17,167] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [107]  [120/160]  eta: 0:00:39  lr: 0.000021  min_lr: 0.000005  loss: 1.3118 (1.4187)  loss_scale: 32768.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1867 (6.8393)  time: 0.8319 (0.5179 -- 2.7526)  data: 0.1924 (0.0004 -- 2.1912)  max mem: 16413
Epoch: [107]  [140/160]  eta: 0:00:19  lr: 0.000021  min_lr: 0.000005  loss: 1.6817 (1.4379)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3556 (6.8967)  time: 0.9658 (0.5294 -- 2.3367)  data: 0.0478 (0.0006 -- 0.9276)  max mem: 16413
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.3400 (1.4312)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5079 (6.7896)  time: 0.6891 (0.4954 -- 1.7979)  data: 0.0274 (0.0002 -- 0.5300)  max mem: 16413
Epoch: [107] Total time: 0:02:31 (0.9462 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.3400 (1.4309)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5079 (6.7896)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1541 (0.1541)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3920 (2.3920 -- 2.3920)  data: 2.1883 (2.1883 -- 2.1883)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1553 (0.3494)  acc1: 100.0000 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4591 (0.1804 -- 2.3920)  data: 0.2659 (0.0004 -- 2.1883)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1811 (0.3289)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2548 (0.1700 -- 0.9451)  data: 0.0658 (0.0001 -- 0.7307)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2300 (0.3880)  acc1: 100.0000 (90.4564)  acc5: 100.0000 (100.0000)  time: 0.2684 (0.1327 -- 0.9451)  data: 0.0880 (0.0001 -- 0.7307)  max mem: 16413
Val: Total time: 0:00:08 (0.3303 s / it)
* Acc@1 91.286 Acc@5 99.585 loss 0.369
Accuracy of the network on the 482 val images: 91.29%
Max accuracy: 92.74%
Epoch: [108]  [  0/160]  eta: 0:25:30  lr: 0.000021  min_lr: 0.000005  loss: 1.5551 (1.5551)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0687 (9.0687)  time: 9.5627 (9.5627 -- 9.5627)  data: 5.6255 (5.6255 -- 5.6255)  max mem: 16413
Epoch: [108]  [ 20/160]  eta: 0:02:55  lr: 0.000021  min_lr: 0.000005  loss: 1.3861 (1.4715)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1433 (7.7889)  time: 0.8386 (0.5157 -- 4.5473)  data: 0.0015 (0.0005 -- 0.0027)  max mem: 16413
Epoch: [108]  [ 40/160]  eta: 0:02:17  lr: 0.000021  min_lr: 0.000005  loss: 1.5447 (1.4844)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0031 (7.1537)  time: 1.0336 (0.5183 -- 4.3667)  data: 0.0013 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [108]  [ 60/160]  eta: 0:01:44  lr: 0.000021  min_lr: 0.000005  loss: 1.3804 (1.4715)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7776 (7.2746)  time: 0.8341 (0.5225 -- 4.2033)  data: 0.0258 (0.0003 -- 0.3152)  max mem: 16413
Epoch: [108]  [ 80/160]  eta: 0:01:23  lr: 0.000021  min_lr: 0.000005  loss: 1.2974 (1.4464)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2851 (7.1708)  time: 1.0623 (0.5280 -- 4.8075)  data: 0.0011 (0.0004 -- 0.0029)  max mem: 16413
[2023-09-23 05:58:30,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:58:30,477] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 05:58:30,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 05:58:30,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [108]  [100/160]  eta: 0:00:59  lr: 0.000021  min_lr: 0.000005  loss: 1.4855 (1.4446)  loss_scale: 32768.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5000 (7.1823)  time: 0.7656 (0.5227 -- 3.0448)  data: 0.0013 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:39  lr: 0.000021  min_lr: 0.000005  loss: 1.4610 (1.4499)  loss_scale: 32768.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3641 (7.2206)  time: 0.9015 (0.5173 -- 3.7917)  data: 0.0564 (0.0006 -- 1.0958)  max mem: 16413
Epoch: [108]  [140/160]  eta: 0:00:19  lr: 0.000021  min_lr: 0.000005  loss: 1.4348 (1.4612)  loss_scale: 32768.0000 (22658.7234)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5628 (7.1543)  time: 1.0505 (0.5288 -- 5.1565)  data: 0.0021 (0.0002 -- 0.0140)  max mem: 16413
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.5080 (1.4621)  loss_scale: 32768.0000 (23859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7508 (7.1347)  time: 0.5920 (0.4932 -- 1.7864)  data: 0.0006 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [108] Total time: 0:02:30 (0.9414 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.5080 (1.4501)  loss_scale: 32768.0000 (23859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7508 (7.1347)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1306 (0.1306)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3824 (2.3824 -- 2.3824)  data: 2.1932 (2.1932 -- 2.1932)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1775 (0.3858)  acc1: 100.0000 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4637 (0.1776 -- 2.3824)  data: 0.2772 (0.0004 -- 2.1932)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2059 (0.3346)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2589 (0.1683 -- 1.0364)  data: 0.0742 (0.0001 -- 0.8497)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2717 (0.4229)  acc1: 88.8889 (88.7967)  acc5: 100.0000 (100.0000)  time: 0.2920 (0.1350 -- 1.0364)  data: 0.1137 (0.0001 -- 0.8497)  max mem: 16413
Val: Total time: 0:00:09 (0.3462 s / it)
* Acc@1 90.041 Acc@5 99.585 loss 0.405
Accuracy of the network on the 482 val images: 90.04%
Max accuracy: 92.74%
Epoch: [109]  [  0/160]  eta: 0:24:10  lr: 0.000021  min_lr: 0.000005  loss: 2.0618 (2.0618)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4617 (7.4617)  time: 9.0647 (9.0647 -- 9.0647)  data: 8.5463 (8.5463 -- 8.5463)  max mem: 16413
[2023-09-23 05:59:54,004] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17446
[2023-09-23 05:59:54,004] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17446
[2023-09-23 05:59:54,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:59:54,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 05:59:54,004] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [109]  [ 20/160]  eta: 0:02:34  lr: 0.000021  min_lr: 0.000005  loss: 1.3671 (1.4218)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3630 (7.8001)  time: 0.7060 (0.5097 -- 2.3512)  data: 0.1451 (0.0004 -- 1.4706)  max mem: 16413
Epoch: [109]  [ 40/160]  eta: 0:02:12  lr: 0.000021  min_lr: 0.000005  loss: 1.5695 (1.4648)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2641 (7.3496)  time: 1.1039 (0.5271 -- 4.1181)  data: 0.0027 (0.0003 -- 0.0113)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:42  lr: 0.000021  min_lr: 0.000005  loss: 1.5142 (1.4601)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4238 (7.2749)  time: 0.8689 (0.5217 -- 4.3393)  data: 0.0013 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [109]  [ 80/160]  eta: 0:01:20  lr: 0.000021  min_lr: 0.000005  loss: 1.5123 (1.4669)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6208 (7.0994)  time: 0.9497 (0.5200 -- 3.3687)  data: 0.0905 (0.0002 -- 1.7706)  max mem: 16413
Epoch: [109]  [100/160]  eta: 0:00:58  lr: 0.000021  min_lr: 0.000005  loss: 1.4449 (1.4542)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1362 (7.1666)  time: 0.8123 (0.5214 -- 3.2555)  data: 0.0480 (0.0002 -- 0.5779)  max mem: 16413
Epoch: [109]  [120/160]  eta: 0:00:38  lr: 0.000021  min_lr: 0.000005  loss: 1.3004 (1.4413)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5906 (7.2142)  time: 0.9359 (0.5189 -- 4.0444)  data: 0.0216 (0.0004 -- 0.3983)  max mem: 16413
[2023-09-23 06:01:52,317] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:01:52,318] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:01:52,318] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:01:52,318] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [140/160]  eta: 0:00:19  lr: 0.000021  min_lr: 0.000005  loss: 1.5242 (1.4490)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3010 (7.2088)  time: 0.9129 (0.5361 -- 2.4551)  data: 0.0916 (0.0002 -- 1.4759)  max mem: 16413
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.4752 (1.4501)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8221 (7.2499)  time: 0.7809 (0.4966 -- 1.9411)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [109] Total time: 0:02:29 (0.9372 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.4752 (1.4234)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8221 (7.2499)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1133 (0.1133)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4794 (2.4794 -- 2.4794)  data: 2.2761 (2.2761 -- 2.2761)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1547 (0.3651)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4897 (0.1748 -- 2.4794)  data: 0.2965 (0.0003 -- 2.2761)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1939 (0.3366)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2591 (0.1686 -- 1.1869)  data: 0.0721 (0.0001 -- 0.9798)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2565 (0.3875)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (99.5851)  time: 0.2377 (0.1331 -- 0.7233)  data: 0.0593 (0.0001 -- 0.5546)  max mem: 16413
Val: Total time: 0:00:09 (0.3470 s / it)
* Acc@1 91.494 Acc@5 99.378 loss 0.388
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 92.74%
Epoch: [110]  [  0/160]  eta: 0:22:30  lr: 0.000021  min_lr: 0.000005  loss: 1.5704 (1.5704)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3359 (8.3359)  time: 8.4406 (8.4406 -- 8.4406)  data: 7.9116 (7.9116 -- 7.9116)  max mem: 16413
Epoch: [110]  [ 20/160]  eta: 0:03:00  lr: 0.000021  min_lr: 0.000005  loss: 1.3921 (1.3903)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4851 (6.6046)  time: 0.9286 (0.5221 -- 4.6161)  data: 0.2641 (0.0003 -- 3.4314)  max mem: 16413
[2023-09-23 06:02:55,048] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17628
[2023-09-23 06:02:55,048] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:02:55,048] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17628
[2023-09-23 06:02:55,049] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:02:55,049] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [110]  [ 40/160]  eta: 0:02:18  lr: 0.000021  min_lr: 0.000005  loss: 1.3115 (1.4009)  loss_scale: 16384.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8071 (6.8982)  time: 1.0122 (0.4984 -- 4.1063)  data: 0.1104 (0.0002 -- 1.6726)  max mem: 16413
Epoch: [110]  [ 60/160]  eta: 0:01:44  lr: 0.000020  min_lr: 0.000005  loss: 1.4045 (1.4197)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4782 (7.2480)  time: 0.8204 (0.5136 -- 3.9440)  data: 0.0015 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [110]  [ 80/160]  eta: 0:01:22  lr: 0.000020  min_lr: 0.000005  loss: 1.4699 (1.4324)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8055 (7.5349)  time: 0.9998 (0.5332 -- 3.9127)  data: 0.0020 (0.0006 -- 0.0058)  max mem: 16413
Epoch: [110]  [100/160]  eta: 0:01:00  lr: 0.000020  min_lr: 0.000005  loss: 1.2056 (1.4056)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9187 (7.4508)  time: 0.8691 (0.5165 -- 4.7068)  data: 0.0016 (0.0003 -- 0.0063)  max mem: 16413
[2023-09-23 06:04:15,581] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17716
[2023-09-23 06:04:15,581] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17716
[2023-09-23 06:04:15,582] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 06:04:15,582] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 06:04:15,582] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [110]  [120/160]  eta: 0:00:39  lr: 0.000020  min_lr: 0.000005  loss: 1.4454 (1.4067)  loss_scale: 16384.0000 (19836.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5399 (7.3454)  time: 0.9756 (0.5228 -- 4.3248)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [110]  [140/160]  eta: 0:00:19  lr: 0.000020  min_lr: 0.000005  loss: 1.3681 (1.4168)  loss_scale: 8192.0000 (18185.0780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7596 (7.3623)  time: 0.7502 (0.5178 -- 3.0128)  data: 0.0015 (0.0004 -- 0.0065)  max mem: 16413
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.6001 (1.4365)  loss_scale: 8192.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0716 (7.3168)  time: 0.7811 (0.4938 -- 2.6844)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [110] Total time: 0:02:30 (0.9417 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.6001 (1.4254)  loss_scale: 8192.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0716 (7.3168)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1640 (0.1640)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4172 (2.4172 -- 2.4172)  data: 2.2311 (2.2311 -- 2.2311)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1590 (0.3476)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (98.9899)  time: 0.4751 (0.1786 -- 2.4172)  data: 0.2848 (0.0003 -- 2.2311)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1590 (0.3424)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (98.9418)  time: 0.2559 (0.1684 -- 1.1264)  data: 0.0672 (0.0001 -- 0.8961)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3279 (0.4456)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (98.7552)  time: 0.2819 (0.1321 -- 1.1264)  data: 0.0998 (0.0001 -- 0.8961)  max mem: 16413
Val: Total time: 0:00:09 (0.3395 s / it)
* Acc@1 89.627 Acc@5 98.755 loss 0.431
Accuracy of the network on the 482 val images: 89.63%
Max accuracy: 92.74%
Epoch: [111]  [  0/160]  eta: 0:20:25  lr: 0.000020  min_lr: 0.000005  loss: 1.2138 (1.2138)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0932 (8.0932)  time: 7.6584 (7.6584 -- 7.6584)  data: 7.0131 (7.0131 -- 7.0131)  max mem: 16413
Epoch: [111]  [ 20/160]  eta: 0:02:54  lr: 0.000020  min_lr: 0.000005  loss: 1.3698 (1.3364)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2561 (7.0913)  time: 0.9251 (0.5137 -- 3.2090)  data: 0.0948 (0.0003 -- 1.0236)  max mem: 16413
Epoch: [111]  [ 40/160]  eta: 0:02:11  lr: 0.000020  min_lr: 0.000005  loss: 1.2900 (1.3709)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0186 (6.9411)  time: 0.9333 (0.5262 -- 2.4748)  data: 0.1675 (0.0007 -- 1.5799)  max mem: 16413
Epoch: [111]  [ 60/160]  eta: 0:01:45  lr: 0.000020  min_lr: 0.000005  loss: 1.3872 (1.3857)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2596 (7.0148)  time: 0.9847 (0.5318 -- 5.2830)  data: 0.0927 (0.0008 -- 1.8059)  max mem: 16413
Epoch: [111]  [ 80/160]  eta: 0:01:21  lr: 0.000020  min_lr: 0.000005  loss: 1.4351 (1.3835)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6727 (7.1194)  time: 0.9145 (0.5190 -- 3.4593)  data: 0.0015 (0.0005 -- 0.0030)  max mem: 16413
[2023-09-23 06:06:26,669] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:06:26,670] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 06:06:26,670] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:06:26,670] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [111]  [100/160]  eta: 0:00:58  lr: 0.000020  min_lr: 0.000005  loss: 1.3665 (1.3906)  loss_scale: 16384.0000 (9489.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5758 (6.9671)  time: 0.7412 (0.5325 -- 2.5701)  data: 0.0237 (0.0004 -- 0.4504)  max mem: 16413
Epoch: [111]  [120/160]  eta: 0:00:38  lr: 0.000020  min_lr: 0.000005  loss: 1.3644 (1.3887)  loss_scale: 16384.0000 (10629.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7517 (6.7941)  time: 0.9928 (0.5241 -- 3.0394)  data: 0.2507 (0.0003 -- 2.5126)  max mem: 16413
Epoch: [111]  [140/160]  eta: 0:00:19  lr: 0.000020  min_lr: 0.000005  loss: 1.4742 (1.4061)  loss_scale: 16384.0000 (11445.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2610 (6.8505)  time: 0.9147 (0.5219 -- 3.5812)  data: 0.1179 (0.0003 -- 1.2699)  max mem: 16413
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.4742 (1.4141)  loss_scale: 16384.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5759 (6.8402)  time: 0.7599 (0.4944 -- 4.5765)  data: 0.0106 (0.0002 -- 0.1982)  max mem: 16413
Epoch: [111] Total time: 0:02:30 (0.9405 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.4742 (1.4211)  loss_scale: 16384.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5759 (6.8402)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1415 (0.1415)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5216 (2.5216 -- 2.5216)  data: 2.2901 (2.2901 -- 2.2901)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1742 (0.4263)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4762 (0.1801 -- 2.5216)  data: 0.2834 (0.0003 -- 2.2901)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2046 (0.3654)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (99.4709)  time: 0.2572 (0.1682 -- 1.0178)  data: 0.0682 (0.0001 -- 0.8190)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2753 (0.4314)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (99.1701)  time: 0.2910 (0.1326 -- 1.0178)  data: 0.1083 (0.0001 -- 0.8190)  max mem: 16413
Val: Total time: 0:00:09 (0.3502 s / it)
* Acc@1 90.249 Acc@5 99.170 loss 0.405
Accuracy of the network on the 482 val images: 90.25%
Max accuracy: 92.74%
Epoch: [112]  [  0/160]  eta: 0:23:53  lr: 0.000020  min_lr: 0.000005  loss: 0.9764 (0.9764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5647 (4.5647)  time: 8.9604 (8.9604 -- 8.9604)  data: 8.4280 (8.4280 -- 8.4280)  max mem: 16413
Epoch: [112]  [ 20/160]  eta: 0:02:45  lr: 0.000020  min_lr: 0.000005  loss: 1.3667 (1.3592)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8892 (6.7803)  time: 0.7965 (0.5164 -- 2.8782)  data: 0.1273 (0.0005 -- 1.3753)  max mem: 16413
Epoch: [112]  [ 40/160]  eta: 0:02:08  lr: 0.000020  min_lr: 0.000005  loss: 1.3164 (1.3840)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6263 (7.0618)  time: 0.9515 (0.5186 -- 3.6964)  data: 0.2318 (0.0003 -- 3.1685)  max mem: 16413
[2023-09-23 06:08:38,137] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:08:38,137] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:08:38,137] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:08:38,137] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [112]  [ 60/160]  eta: 0:01:46  lr: 0.000020  min_lr: 0.000005  loss: 1.4129 (1.4070)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7498 (6.7822)  time: 1.0383 (0.5149 -- 4.9442)  data: 0.4396 (0.0002 -- 4.4508)  max mem: 16413
[2023-09-23 06:08:46,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17982
[2023-09-23 06:08:46,831] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:08:46,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17982
[2023-09-23 06:08:46,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 06:08:46,831] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:09:04,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=103, lr=[5.0093528099075036e-06, 5.0093528099075036e-06, 5.565947566563894e-06, 5.565947566563894e-06, 6.184386185070991e-06, 6.184386185070991e-06, 6.8715402056344355e-06, 6.8715402056344355e-06, 7.63504467292715e-06, 7.63504467292715e-06, 8.483382969919056e-06, 8.483382969919056e-06, 9.42598107768784e-06, 9.42598107768784e-06, 1.0473312308542044e-05, 1.0473312308542044e-05, 1.1637013676157826e-05, 1.1637013676157826e-05, 1.2930015195730916e-05, 1.2930015195730916e-05, 1.436668355081213e-05, 1.436668355081213e-05, 1.5962981723124588e-05, 1.5962981723124588e-05, 1.773664635902732e-05, 1.773664635902732e-05, 1.970738484336369e-05, 1.970738484336369e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 06:09:04,414] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=17.149106510287137, CurrSamplesPerSec=24.06582334647913, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [112]  [ 80/160]  eta: 0:01:22  lr: 0.000020  min_lr: 0.000005  loss: 1.2822 (1.3933)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8450 (6.7928)  time: 0.9580 (0.5032 -- 4.5312)  data: 0.1007 (0.0002 -- 1.9943)  max mem: 16413
Epoch: [112]  [100/160]  eta: 0:01:00  lr: 0.000020  min_lr: 0.000005  loss: 1.4959 (1.4104)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0534 (6.9454)  time: 0.9282 (0.5136 -- 4.0626)  data: 0.0173 (0.0002 -- 0.3123)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:39  lr: 0.000020  min_lr: 0.000005  loss: 1.4262 (1.4144)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1537 (7.0241)  time: 0.8456 (0.5248 -- 2.5641)  data: 0.1289 (0.0002 -- 1.5752)  max mem: 16413
Epoch: [112]  [140/160]  eta: 0:00:19  lr: 0.000020  min_lr: 0.000005  loss: 1.2831 (1.4039)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1543 (7.0545)  time: 0.9721 (0.5209 -- 4.4025)  data: 0.4277 (0.0003 -- 3.8636)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.5295 (1.4132)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5942 (7.0072)  time: 0.6372 (0.4947 -- 2.3773)  data: 0.1173 (0.0002 -- 1.8467)  max mem: 16413
Epoch: [112] Total time: 0:02:30 (0.9435 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.5295 (1.4061)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5942 (7.0072)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1472 (0.1472)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4321 (2.4321 -- 2.4321)  data: 2.2446 (2.2446 -- 2.2446)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1389 (0.3877)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4767 (0.1766 -- 2.4321)  data: 0.2864 (0.0002 -- 2.2446)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1389 (0.2832)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (99.4709)  time: 0.2573 (0.1692 -- 1.1085)  data: 0.0697 (0.0001 -- 0.9019)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1665 (0.3821)  acc1: 100.0000 (91.2863)  acc5: 100.0000 (99.1701)  time: 0.2729 (0.1333 -- 1.1085)  data: 0.0938 (0.0001 -- 0.9019)  max mem: 16413
Val: Total time: 0:00:09 (0.3349 s / it)
* Acc@1 91.079 Acc@5 99.585 loss 0.361
Accuracy of the network on the 482 val images: 91.08%
Max accuracy: 92.74%
Epoch: [113]  [  0/160]  eta: 0:28:26  lr: 0.000020  min_lr: 0.000005  loss: 1.2973 (1.2973)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8287 (4.8287)  time: 10.6626 (10.6626 -- 10.6626)  data: 4.8532 (4.8532 -- 4.8532)  max mem: 16413
Epoch: [113]  [ 20/160]  eta: 0:03:00  lr: 0.000019  min_lr: 0.000005  loss: 1.4850 (1.5435)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1486 (7.3816)  time: 0.8225 (0.5278 -- 2.7082)  data: 0.0023 (0.0003 -- 0.0099)  max mem: 16413
[2023-09-23 06:10:58,857] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:10:58,857] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:10:58,857] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:10:58,857] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:11:04,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18119
[2023-09-23 06:11:04,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18119
[2023-09-23 06:11:04,665] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:11:04,665] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:11:04,665] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [113]  [ 40/160]  eta: 0:02:13  lr: 0.000019  min_lr: 0.000005  loss: 1.4727 (1.4982)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7611 (6.9913)  time: 0.9197 (0.5103 -- 4.7878)  data: 0.0013 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [113]  [ 60/160]  eta: 0:01:47  lr: 0.000019  min_lr: 0.000005  loss: 1.4305 (1.4722)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5883 (6.9442)  time: 1.0000 (0.5173 -- 2.5773)  data: 0.1841 (0.0003 -- 1.4591)  max mem: 16413
Epoch: [113]  [ 80/160]  eta: 0:01:21  lr: 0.000019  min_lr: 0.000005  loss: 1.5853 (1.4817)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8621 (7.0781)  time: 0.8485 (0.5189 -- 2.4864)  data: 0.1216 (0.0003 -- 1.2801)  max mem: 16413
Epoch: [113]  [100/160]  eta: 0:01:00  lr: 0.000019  min_lr: 0.000005  loss: 1.5343 (1.4821)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4177 (7.0363)  time: 0.9274 (0.5144 -- 3.2128)  data: 0.1642 (0.0004 -- 2.7060)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:39  lr: 0.000019  min_lr: 0.000005  loss: 1.3496 (1.4642)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6250 (7.2069)  time: 0.9496 (0.5199 -- 3.8288)  data: 0.0847 (0.0001 -- 1.6724)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:19  lr: 0.000019  min_lr: 0.000005  loss: 1.4360 (1.4564)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2784 (7.1235)  time: 0.9305 (0.5298 -- 3.0472)  data: 0.2774 (0.0003 -- 2.5191)  max mem: 16413
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.6249 (1.4738)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6612 (7.1156)  time: 0.6696 (0.4948 -- 3.1864)  data: 0.0076 (0.0002 -- 0.1385)  max mem: 16413
Epoch: [113] Total time: 0:02:31 (0.9470 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.6249 (1.4587)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6612 (7.1156)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1555 (0.1555)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5289 (2.5289 -- 2.5289)  data: 2.3489 (2.3489 -- 2.3489)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1780 (0.3325)  acc1: 100.0000 (94.9495)  acc5: 100.0000 (100.0000)  time: 0.4946 (0.1765 -- 2.5289)  data: 0.3052 (0.0002 -- 2.3489)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1780 (0.3034)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (100.0000)  time: 0.2516 (0.1691 -- 1.1993)  data: 0.0645 (0.0001 -- 1.0045)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2610 (0.3761)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2822 (0.1357 -- 1.1993)  data: 0.1008 (0.0001 -- 1.0045)  max mem: 16413
Val: Total time: 0:00:09 (0.3443 s / it)
* Acc@1 91.494 Acc@5 100.000 loss 0.370
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 92.74%
Epoch: [114]  [  0/160]  eta: 0:18:05  lr: 0.000019  min_lr: 0.000005  loss: 1.4646 (1.4646)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4762 (7.4762)  time: 6.7845 (6.7845 -- 6.7845)  data: 6.2399 (6.2399 -- 6.2399)  max mem: 16413
[2023-09-23 06:13:17,528] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:13:17,528] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:13:17,529] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:13:17,529] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:13:27,287] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18259
[2023-09-23 06:13:27,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:13:27,287] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18259
[2023-09-23 06:13:27,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:13:27,288] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [114]  [ 20/160]  eta: 0:02:49  lr: 0.000019  min_lr: 0.000005  loss: 1.4260 (1.4303)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8781 (6.9421)  time: 0.9319 (0.5110 -- 3.5547)  data: 0.2685 (0.0003 -- 2.4745)  max mem: 16413
Epoch: [114]  [ 40/160]  eta: 0:02:12  lr: 0.000019  min_lr: 0.000005  loss: 1.4378 (1.4395)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6515 (6.7727)  time: 0.9949 (0.5312 -- 5.1204)  data: 0.0507 (0.0003 -- 0.9791)  max mem: 16413
Epoch: [114]  [ 60/160]  eta: 0:01:42  lr: 0.000019  min_lr: 0.000005  loss: 1.3970 (1.4378)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0719 (7.0022)  time: 0.8505 (0.5248 -- 2.7313)  data: 0.1119 (0.0002 -- 1.6178)  max mem: 16413
Epoch: [114]  [ 80/160]  eta: 0:01:18  lr: 0.000019  min_lr: 0.000005  loss: 1.3454 (1.4202)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8345 (7.1046)  time: 0.8826 (0.5215 -- 3.7667)  data: 0.0014 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:58  lr: 0.000019  min_lr: 0.000005  loss: 1.5313 (1.4389)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7664 (7.2828)  time: 0.9077 (0.5338 -- 3.7912)  data: 0.0326 (0.0003 -- 0.6163)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:39  lr: 0.000019  min_lr: 0.000005  loss: 1.3196 (1.4271)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5196 (7.0925)  time: 1.0054 (0.5203 -- 3.0860)  data: 0.0602 (0.0005 -- 1.0144)  max mem: 16413
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.5966 (1.4399)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0220 (7.0884)  time: 0.7699 (0.5270 -- 3.4930)  data: 0.0017 (0.0003 -- 0.0051)  max mem: 16413
[2023-09-23 06:15:22,611] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:15:22,612] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:15:22,612] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:15:22,612] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.5152 (1.4469)  loss_scale: 32768.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7797 (7.0885)  time: 0.7388 (0.4935 -- 2.4039)  data: 0.0394 (0.0004 -- 0.4737)  max mem: 16413
Epoch: [114] Total time: 0:02:27 (0.9244 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.5152 (1.4251)  loss_scale: 32768.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7797 (7.0885)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1307 (0.1307)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5152 (2.5152 -- 2.5152)  data: 2.3269 (2.3269 -- 2.3269)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1762 (0.3378)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4723 (0.1782 -- 2.5152)  data: 0.2832 (0.0002 -- 2.3269)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1997 (0.3326)  acc1: 88.8889 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2543 (0.1689 -- 0.9843)  data: 0.0674 (0.0001 -- 0.7837)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3772 (0.3901)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (100.0000)  time: 0.2937 (0.1329 -- 1.1201)  data: 0.1147 (0.0001 -- 0.9484)  max mem: 16413
Val: Total time: 0:00:09 (0.3531 s / it)
* Acc@1 91.701 Acc@5 99.585 loss 0.397
Accuracy of the network on the 482 val images: 91.70%
Max accuracy: 92.74%
Epoch: [115]  [  0/160]  eta: 0:18:32  lr: 0.000019  min_lr: 0.000005  loss: 1.3622 (1.3622)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4609 (6.4609)  time: 6.9552 (6.9552 -- 6.9552)  data: 6.2220 (6.2220 -- 6.2220)  max mem: 16413
[2023-09-23 06:15:51,211] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18405
[2023-09-23 06:15:51,211] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18405
[2023-09-23 06:15:51,211] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:15:51,211] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:15:51,212] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [115]  [ 20/160]  eta: 0:03:05  lr: 0.000019  min_lr: 0.000005  loss: 1.4707 (1.4244)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8910 (7.1345)  time: 1.0460 (0.5154 -- 3.2897)  data: 0.2694 (0.0003 -- 1.8409)  max mem: 16413
Epoch: [115]  [ 40/160]  eta: 0:02:10  lr: 0.000019  min_lr: 0.000005  loss: 1.3526 (1.3539)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1715 (6.9348)  time: 0.8378 (0.5230 -- 2.7499)  data: 0.2865 (0.0002 -- 2.2185)  max mem: 16413
Epoch: [115]  [ 60/160]  eta: 0:01:43  lr: 0.000019  min_lr: 0.000005  loss: 1.3436 (1.3370)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3583 (6.7913)  time: 0.9128 (0.5287 -- 4.1195)  data: 0.0628 (0.0003 -- 0.5241)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:19  lr: 0.000019  min_lr: 0.000005  loss: 1.5277 (1.3679)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6751 (6.8590)  time: 0.8760 (0.5184 -- 3.7122)  data: 0.3339 (0.0004 -- 3.1868)  max mem: 16413
Epoch: [115]  [100/160]  eta: 0:00:57  lr: 0.000019  min_lr: 0.000005  loss: 1.3614 (1.3671)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7449 (6.8375)  time: 0.8463 (0.5170 -- 3.8742)  data: 0.3080 (0.0003 -- 3.3638)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:38  lr: 0.000019  min_lr: 0.000005  loss: 1.4941 (1.3814)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9064 (6.8574)  time: 0.9148 (0.5263 -- 2.8359)  data: 0.2344 (0.0006 -- 1.4001)  max mem: 16413
[2023-09-23 06:17:49,772] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:17:49,773] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:17:49,773] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:17:49,773] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [115]  [140/160]  eta: 0:00:19  lr: 0.000018  min_lr: 0.000005  loss: 1.3512 (1.3826)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4747 (6.9110)  time: 1.0346 (0.5263 -- 2.9412)  data: 0.4918 (0.0008 -- 2.3914)  max mem: 16413
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.3154 (1.3798)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5256 (6.9189)  time: 0.6871 (0.4948 -- 2.4839)  data: 0.1657 (0.0001 -- 1.9466)  max mem: 16413
Epoch: [115] Total time: 0:02:29 (0.9351 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.3154 (1.3956)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5256 (6.9189)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1759 (0.1759)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3363 (2.3363 -- 2.3363)  data: 2.1526 (2.1526 -- 2.1526)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1733 (0.3571)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (98.9899)  time: 0.4669 (0.1782 -- 2.3363)  data: 0.2754 (0.0002 -- 2.1526)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1901 (0.3278)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (98.9418)  time: 0.2582 (0.1685 -- 1.0813)  data: 0.0694 (0.0001 -- 0.8702)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3151 (0.4002)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (99.1701)  time: 0.2941 (0.1327 -- 1.0813)  data: 0.1135 (0.0001 -- 0.8838)  max mem: 16413
Val: Total time: 0:00:09 (0.3486 s / it)
* Acc@1 90.664 Acc@5 98.963 loss 0.399
Accuracy of the network on the 482 val images: 90.66%
Max accuracy: 92.74%
Epoch: [116]  [  0/160]  eta: 0:18:56  lr: 0.000018  min_lr: 0.000005  loss: 1.4467 (1.4467)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8571 (5.8571)  time: 7.1007 (7.1007 -- 7.1007)  data: 6.0695 (6.0695 -- 6.0695)  max mem: 16413
Epoch: [116]  [ 20/160]  eta: 0:02:51  lr: 0.000018  min_lr: 0.000005  loss: 1.3361 (1.3706)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4039 (6.8156)  time: 0.9341 (0.5370 -- 2.8343)  data: 0.2087 (0.0004 -- 1.6991)  max mem: 16413
Epoch: [116]  [ 40/160]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000005  loss: 1.4315 (1.4048)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3062 (6.4588)  time: 0.8381 (0.5173 -- 3.6508)  data: 0.2595 (0.0002 -- 3.1210)  max mem: 16413
[2023-09-23 06:19:14,883] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18613
[2023-09-23 06:19:14,883] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18613
[2023-09-23 06:19:14,883] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:19:14,883] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:19:14,883] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [ 60/160]  eta: 0:01:42  lr: 0.000018  min_lr: 0.000005  loss: 1.3909 (1.4017)  loss_scale: 32768.0000 (30619.2787)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9892 (6.9710)  time: 0.9994 (0.5224 -- 5.1953)  data: 0.4212 (0.0004 -- 4.6603)  max mem: 16413
Epoch: [116]  [ 80/160]  eta: 0:01:18  lr: 0.000018  min_lr: 0.000005  loss: 1.2681 (1.3843)  loss_scale: 16384.0000 (27104.3951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6785 (6.9807)  time: 0.8671 (0.5260 -- 2.6594)  data: 0.2849 (0.0004 -- 2.1459)  max mem: 16413
Epoch: [116]  [100/160]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000005  loss: 1.4321 (1.4037)  loss_scale: 16384.0000 (24981.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5158 (7.1191)  time: 0.8794 (0.5281 -- 3.4325)  data: 0.2855 (0.0005 -- 2.8939)  max mem: 16413
Epoch: [116]  [120/160]  eta: 0:00:38  lr: 0.000018  min_lr: 0.000005  loss: 1.1760 (1.3806)  loss_scale: 16384.0000 (23560.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2370 (7.0216)  time: 0.9822 (0.5237 -- 5.2122)  data: 0.4368 (0.0002 -- 4.6606)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:19  lr: 0.000018  min_lr: 0.000005  loss: 1.3849 (1.3890)  loss_scale: 16384.0000 (22542.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4473 (6.8670)  time: 0.8769 (0.5298 -- 4.7857)  data: 0.3248 (0.0001 -- 4.2446)  max mem: 16413
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.2321 (1.3798)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9331 (6.9573)  time: 0.7735 (0.4936 -- 3.5259)  data: 0.2536 (0.0001 -- 3.0034)  max mem: 16413
Epoch: [116] Total time: 0:02:29 (0.9347 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.2321 (1.3960)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9331 (6.9573)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1149 (0.1149)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5280 (2.5280 -- 2.5280)  data: 2.3341 (2.3341 -- 2.3341)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1660 (0.4186)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4703 (0.1758 -- 2.5280)  data: 0.2827 (0.0002 -- 2.3341)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1660 (0.3374)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (99.4709)  time: 0.2531 (0.1677 -- 0.9652)  data: 0.0686 (0.0001 -- 0.7717)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2435 (0.3871)  acc1: 100.0000 (90.8714)  acc5: 100.0000 (99.5851)  time: 0.2759 (0.1336 -- 0.9652)  data: 0.0981 (0.0001 -- 0.7717)  max mem: 16413
Val: Total time: 0:00:09 (0.3398 s / it)
* Acc@1 92.531 Acc@5 99.378 loss 0.358
Accuracy of the network on the 482 val images: 92.53%
Max accuracy: 92.74%
Epoch: [117]  [  0/160]  eta: 0:20:26  lr: 0.000018  min_lr: 0.000005  loss: 1.8156 (1.8156)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8696 (8.8696)  time: 7.6631 (7.6631 -- 7.6631)  data: 7.0803 (7.0803 -- 7.0803)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:45  lr: 0.000018  min_lr: 0.000005  loss: 1.2257 (1.3279)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4491 (5.8633)  time: 0.8576 (0.5212 -- 3.3595)  data: 0.1302 (0.0006 -- 2.3127)  max mem: 16413
[2023-09-23 06:21:23,850] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:21:23,851] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:21:23,851] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:21:23,851] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:21:24,947] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18744
[2023-09-23 06:21:24,947] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:21:24,947] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 06:21:24,947] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18744
[2023-09-23 06:21:24,947] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [117]  [ 40/160]  eta: 0:02:06  lr: 0.000018  min_lr: 0.000005  loss: 1.2289 (1.3442)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8082 (6.2793)  time: 0.9285 (0.5239 -- 2.5098)  data: 0.1562 (0.0002 -- 1.9669)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:41  lr: 0.000018  min_lr: 0.000005  loss: 1.4449 (1.3783)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7438 (6.6335)  time: 0.9405 (0.5215 -- 3.8999)  data: 0.0597 (0.0005 -- 1.1692)  max mem: 16413
Epoch: [117]  [ 80/160]  eta: 0:01:21  lr: 0.000018  min_lr: 0.000005  loss: 1.4530 (1.3960)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4202 (6.4636)  time: 1.0039 (0.5300 -- 3.9594)  data: 0.0019 (0.0004 -- 0.0068)  max mem: 16413
Epoch: [117]  [100/160]  eta: 0:00:58  lr: 0.000018  min_lr: 0.000005  loss: 1.3277 (1.3998)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8920 (6.5962)  time: 0.7786 (0.5023 -- 3.4087)  data: 0.0243 (0.0002 -- 0.2858)  max mem: 16413
Epoch: [117]  [120/160]  eta: 0:00:38  lr: 0.000018  min_lr: 0.000005  loss: 1.3132 (1.3883)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1282 (6.7808)  time: 0.9262 (0.5311 -- 3.3000)  data: 0.3479 (0.0002 -- 2.7341)  max mem: 16413
Epoch: [117]  [140/160]  eta: 0:00:19  lr: 0.000018  min_lr: 0.000005  loss: 1.3092 (1.3845)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8914 (6.9006)  time: 0.9941 (0.5226 -- 4.8107)  data: 0.4200 (0.0003 -- 4.3141)  max mem: 16413
[2023-09-23 06:23:24,898] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:23:24,898] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:23:24,898] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:23:24,898] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000004  loss: 1.4654 (1.3895)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3513 (6.9510)  time: 0.7321 (0.4940 -- 4.2843)  data: 0.2119 (0.0002 -- 3.7698)  max mem: 16413
Epoch: [117] Total time: 0:02:30 (0.9399 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000004  loss: 1.4654 (1.3970)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3513 (6.9510)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1578 (0.1578)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5085 (2.5085 -- 2.5085)  data: 2.3129 (2.3129 -- 2.3129)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1578 (0.4013)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4767 (0.1791 -- 2.5085)  data: 0.2858 (0.0002 -- 2.3129)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1587 (0.3588)  acc1: 100.0000 (91.0053)  acc5: 100.0000 (99.4709)  time: 0.2587 (0.1680 -- 1.0112)  data: 0.0702 (0.0001 -- 0.8183)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2455 (0.4135)  acc1: 88.8889 (89.2116)  acc5: 100.0000 (99.5851)  time: 0.2682 (0.1329 -- 1.0112)  data: 0.0889 (0.0001 -- 0.8183)  max mem: 16413
Val: Total time: 0:00:09 (0.3349 s / it)
* Acc@1 90.871 Acc@5 99.378 loss 0.383
Accuracy of the network on the 482 val images: 90.87%
Max accuracy: 92.74%
Epoch: [118]  [  0/160]  eta: 0:20:25  lr: 0.000018  min_lr: 0.000004  loss: 0.7277 (0.7277)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9459 (8.9459)  time: 7.6582 (7.6582 -- 7.6582)  data: 7.0797 (7.0797 -- 7.0797)  max mem: 16413
[2023-09-23 06:23:53,636] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18889
[2023-09-23 06:23:53,636] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18889
[2023-09-23 06:23:53,636] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:23:53,636] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:23:53,636] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [118]  [ 20/160]  eta: 0:02:53  lr: 0.000018  min_lr: 0.000004  loss: 1.5242 (1.5186)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1628 (7.2351)  time: 0.9217 (0.5282 -- 3.1805)  data: 0.3206 (0.0009 -- 2.1034)  max mem: 16413
Epoch: [118]  [ 40/160]  eta: 0:02:09  lr: 0.000018  min_lr: 0.000004  loss: 1.2982 (1.4482)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6335 (7.5370)  time: 0.9162 (0.5152 -- 3.2059)  data: 0.2791 (0.0003 -- 2.6957)  max mem: 16413
Epoch: [118]  [ 60/160]  eta: 0:01:43  lr: 0.000018  min_lr: 0.000004  loss: 1.4256 (1.4204)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8498 (7.5942)  time: 0.9328 (0.5269 -- 4.3064)  data: 0.3931 (0.0005 -- 3.7867)  max mem: 16413
Epoch: [118]  [ 80/160]  eta: 0:01:18  lr: 0.000017  min_lr: 0.000004  loss: 1.3592 (1.4064)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9685 (7.3670)  time: 0.8343 (0.5219 -- 3.6484)  data: 0.2942 (0.0004 -- 3.1260)  max mem: 16413
Epoch: [118]  [100/160]  eta: 0:00:58  lr: 0.000017  min_lr: 0.000004  loss: 1.4536 (1.4022)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4030 (7.4591)  time: 0.9536 (0.5260 -- 4.6606)  data: 0.0970 (0.0009 -- 1.2944)  max mem: 16413
[2023-09-23 06:25:33,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=109, lr=[4.423662355504158e-06, 4.423662355504158e-06, 4.91518039500462e-06, 4.91518039500462e-06, 5.4613115500051325e-06, 5.4613115500051325e-06, 6.068123944450148e-06, 6.068123944450148e-06, 6.742359938277941e-06, 6.742359938277941e-06, 7.491511042531047e-06, 7.491511042531047e-06, 8.323901158367829e-06, 8.323901158367829e-06, 9.248779064853141e-06, 9.248779064853141e-06, 1.0276421183170158e-05, 1.0276421183170158e-05, 1.1418245759077952e-05, 1.1418245759077952e-05, 1.2686939732308837e-05, 1.2686939732308837e-05, 1.4096599702565375e-05, 1.4096599702565375e-05, 1.566288855840597e-05, 1.566288855840597e-05, 1.7403209509339967e-05, 1.7403209509339967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 06:25:33,777] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=17.161843807676377, CurrSamplesPerSec=23.25709126295779, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:38  lr: 0.000017  min_lr: 0.000004  loss: 1.5197 (1.4085)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2589 (7.5392)  time: 0.8946 (0.5085 -- 4.7865)  data: 0.1663 (0.0003 -- 1.7996)  max mem: 16413
[2023-09-23 06:25:52,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:25:52,569] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:25:52,569] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:25:52,569] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [118]  [140/160]  eta: 0:00:19  lr: 0.000017  min_lr: 0.000004  loss: 1.4031 (1.4131)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5373 (7.5200)  time: 0.9676 (0.5299 -- 4.4111)  data: 0.2227 (0.0004 -- 3.8047)  max mem: 16413
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.3635 (1.4036)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9554 (7.4543)  time: 0.7287 (0.4955 -- 3.5182)  data: 0.1514 (0.0002 -- 2.9907)  max mem: 16413
Epoch: [118] Total time: 0:02:30 (0.9382 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.3635 (1.4102)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9554 (7.4543)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1165 (0.1165)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5117 (2.5117 -- 2.5117)  data: 2.3265 (2.3265 -- 2.3265)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1526 (0.4013)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4794 (0.1786 -- 2.5117)  data: 0.2890 (0.0005 -- 2.3265)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1957 (0.3507)  acc1: 100.0000 (91.0053)  acc5: 100.0000 (99.4709)  time: 0.2547 (0.1679 -- 1.0501)  data: 0.0699 (0.0001 -- 0.8409)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2294 (0.3929)  acc1: 88.8889 (89.2116)  acc5: 100.0000 (99.5851)  time: 0.2770 (0.1329 -- 1.0501)  data: 0.0988 (0.0001 -- 0.8409)  max mem: 16413
Val: Total time: 0:00:09 (0.3397 s / it)
* Acc@1 90.664 Acc@5 99.378 loss 0.370
Accuracy of the network on the 482 val images: 90.66%
Max accuracy: 92.74%
Epoch: [119]  [  0/160]  eta: 0:20:07  lr: 0.000017  min_lr: 0.000004  loss: 1.5572 (1.5572)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7039 (8.7039)  time: 7.5498 (7.5498 -- 7.5498)  data: 6.4401 (6.4401 -- 6.4401)  max mem: 16413
[2023-09-23 06:26:41,253] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19058
[2023-09-23 06:26:41,254] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:26:41,254] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19058
[2023-09-23 06:26:41,254] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:26:41,255] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [119]  [ 20/160]  eta: 0:02:51  lr: 0.000017  min_lr: 0.000004  loss: 1.5032 (1.4441)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3426 (7.1347)  time: 0.9097 (0.5182 -- 4.3854)  data: 0.2597 (0.0004 -- 2.6059)  max mem: 16413
Epoch: [119]  [ 40/160]  eta: 0:02:19  lr: 0.000017  min_lr: 0.000004  loss: 1.5100 (1.4714)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9004 (7.6297)  time: 1.0890 (0.5207 -- 4.3379)  data: 0.0654 (0.0004 -- 1.2829)  max mem: 16413
Epoch: [119]  [ 60/160]  eta: 0:01:45  lr: 0.000017  min_lr: 0.000004  loss: 1.4035 (1.4610)  loss_scale: 16384.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0027 (7.4672)  time: 0.8541 (0.5065 -- 2.8895)  data: 0.0930 (0.0003 -- 1.8328)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:18  lr: 0.000017  min_lr: 0.000004  loss: 1.6022 (1.4790)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5121 (7.3140)  time: 0.7547 (0.5209 -- 1.8063)  data: 0.1098 (0.0001 -- 1.2603)  max mem: 16413
Epoch: [119]  [100/160]  eta: 0:01:00  lr: 0.000017  min_lr: 0.000004  loss: 1.2448 (1.4407)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5843 (7.1404)  time: 1.0929 (0.5106 -- 4.0417)  data: 0.1761 (0.0004 -- 3.4993)  max mem: 16413
Epoch: [119]  [120/160]  eta: 0:00:38  lr: 0.000017  min_lr: 0.000004  loss: 1.3565 (1.4311)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4333 (6.9874)  time: 0.7553 (0.5296 -- 3.9515)  data: 0.0016 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [119]  [140/160]  eta: 0:00:19  lr: 0.000017  min_lr: 0.000004  loss: 1.5557 (1.4495)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2881 (7.1117)  time: 0.9083 (0.5260 -- 3.1853)  data: 0.0020 (0.0005 -- 0.0047)  max mem: 16413
[2023-09-23 06:28:38,608] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:28:38,608] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:28:38,609] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:28:38,609] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.3301 (1.4437)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9527 (7.0572)  time: 0.7792 (0.4942 -- 3.0292)  data: 0.2260 (0.0002 -- 2.4804)  max mem: 16413
Epoch: [119] Total time: 0:02:29 (0.9367 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.3301 (1.4272)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9527 (7.0572)
[2023-09-23 06:28:46,910] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-09-23 06:28:46,912] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-09-23 06:28:46,912] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-09-23 06:28:46,912] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-09-23 06:28:48,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-09-23 06:28:48,134] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1342 (0.1342)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5652 (2.5652 -- 2.5652)  data: 2.3394 (2.3394 -- 2.3394)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1566 (0.3612)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4868 (0.1835 -- 2.5652)  data: 0.2903 (0.0005 -- 2.3394)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1617 (0.3107)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (99.4709)  time: 0.2605 (0.1683 -- 1.0538)  data: 0.0710 (0.0001 -- 0.8375)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2811 (0.4003)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (99.5851)  time: 0.2880 (0.1327 -- 1.0538)  data: 0.1067 (0.0001 -- 0.8375)  max mem: 16413
Val: Total time: 0:00:09 (0.3513 s / it)
* Acc@1 90.456 Acc@5 99.585 loss 0.400
Accuracy of the network on the 482 val images: 90.46%
Max accuracy: 92.74%
Epoch: [120]  [  0/160]  eta: 0:18:18  lr: 0.000017  min_lr: 0.000004  loss: 1.2853 (1.2853)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7349 (2.7349)  time: 6.8676 (6.8676 -- 6.8676)  data: 6.3074 (6.3074 -- 6.3074)  max mem: 16413
Epoch: [120]  [ 20/160]  eta: 0:02:54  lr: 0.000017  min_lr: 0.000004  loss: 1.5503 (1.5297)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7385 (6.8673)  time: 0.9666 (0.5319 -- 2.5439)  data: 0.4127 (0.0003 -- 2.0078)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:11  lr: 0.000017  min_lr: 0.000004  loss: 1.4964 (1.5058)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1021 (6.5924)  time: 0.9400 (0.5228 -- 2.8706)  data: 0.1881 (0.0007 -- 2.3394)  max mem: 16413
[2023-09-23 06:30:01,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19259
[2023-09-23 06:30:01,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19259
[2023-09-23 06:30:01,244] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:30:01,244] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:30:01,244] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [120]  [ 60/160]  eta: 0:01:45  lr: 0.000017  min_lr: 0.000004  loss: 1.4277 (1.4554)  loss_scale: 32768.0000 (32230.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9032 (6.8823)  time: 0.9583 (0.5282 -- 2.3995)  data: 0.1440 (0.0005 -- 1.3345)  max mem: 16413
Epoch: [120]  [ 80/160]  eta: 0:01:20  lr: 0.000017  min_lr: 0.000004  loss: 1.2019 (1.3975)  loss_scale: 16384.0000 (28318.0247)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4439 (6.9623)  time: 0.8598 (0.5233 -- 3.3008)  data: 0.0881 (0.0003 -- 1.1378)  max mem: 16413
Epoch: [120]  [100/160]  eta: 0:01:00  lr: 0.000017  min_lr: 0.000004  loss: 1.3943 (1.4153)  loss_scale: 16384.0000 (25954.8515)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7377 (7.0301)  time: 1.0310 (0.5085 -- 4.5030)  data: 0.0019 (0.0003 -- 0.0141)  max mem: 16413
Epoch: [120]  [120/160]  eta: 0:00:39  lr: 0.000017  min_lr: 0.000004  loss: 1.4685 (1.4259)  loss_scale: 16384.0000 (24372.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8282 (7.0688)  time: 0.8473 (0.5268 -- 3.5599)  data: 0.0014 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [120]  [140/160]  eta: 0:00:19  lr: 0.000017  min_lr: 0.000004  loss: 1.4739 (1.4301)  loss_scale: 16384.0000 (23239.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6727 (7.0405)  time: 1.0167 (0.5220 -- 4.5710)  data: 0.0015 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.5375 (1.4497)  loss_scale: 16384.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6353 (7.1268)  time: 0.6443 (0.4952 -- 2.7399)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [120] Total time: 0:02:31 (0.9477 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.5375 (1.4226)  loss_scale: 16384.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6353 (7.1268)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1288 (0.1288)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5106 (2.5106 -- 2.5106)  data: 2.3260 (2.3260 -- 2.3260)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1588 (0.2848)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4792 (0.1798 -- 2.5106)  data: 0.2906 (0.0003 -- 2.3260)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1935 (0.2748)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2567 (0.1688 -- 1.0705)  data: 0.0700 (0.0001 -- 0.8651)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2181 (0.3666)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (100.0000)  time: 0.2775 (0.1321 -- 1.0705)  data: 0.0981 (0.0001 -- 0.8651)  max mem: 16413
Val: Total time: 0:00:09 (0.3402 s / it)
* Acc@1 91.079 Acc@5 99.793 loss 0.379
Accuracy of the network on the 482 val images: 91.08%
Max accuracy: 92.74%
Epoch: [121]  [  0/160]  eta: 0:20:11  lr: 0.000017  min_lr: 0.000004  loss: 1.7582 (1.7582)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4843 (10.4843)  time: 7.5741 (7.5741 -- 7.5741)  data: 5.0978 (5.0978 -- 5.0978)  max mem: 16413
Epoch: [121]  [ 20/160]  eta: 0:03:02  lr: 0.000017  min_lr: 0.000004  loss: 1.3926 (1.4252)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2412 (7.3768)  time: 0.9888 (0.5197 -- 4.8352)  data: 0.0017 (0.0002 -- 0.0046)  max mem: 16413
[2023-09-23 06:32:11,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:32:11,864] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:32:11,865] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:32:11,865] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [121]  [ 40/160]  eta: 0:02:13  lr: 0.000016  min_lr: 0.000004  loss: 1.2555 (1.3682)  loss_scale: 32768.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3408 (6.9330)  time: 0.9084 (0.5217 -- 3.0234)  data: 0.0014 (0.0003 -- 0.0026)  max mem: 16413
[2023-09-23 06:32:39,809] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19417
[2023-09-23 06:32:39,809] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19417
[2023-09-23 06:32:39,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:32:39,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:32:39,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [121]  [ 60/160]  eta: 0:01:43  lr: 0.000016  min_lr: 0.000004  loss: 1.5944 (1.4284)  loss_scale: 32768.0000 (24173.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8228 (7.0298)  time: 0.8721 (0.5226 -- 4.0172)  data: 0.0016 (0.0003 -- 0.0053)  max mem: 16413
Epoch: [121]  [ 80/160]  eta: 0:01:22  lr: 0.000016  min_lr: 0.000004  loss: 1.4904 (1.4237)  loss_scale: 16384.0000 (22249.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7378 (6.9873)  time: 1.0105 (0.5135 -- 3.0507)  data: 0.0011 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [121]  [100/160]  eta: 0:01:00  lr: 0.000016  min_lr: 0.000004  loss: 1.3201 (1.4142)  loss_scale: 16384.0000 (21088.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7910 (7.0152)  time: 0.9506 (0.5130 -- 4.0633)  data: 0.0014 (0.0004 -- 0.0034)  max mem: 16413
[2023-09-23 06:33:21,160] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19461
[2023-09-23 06:33:21,160] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 06:33:21,160] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-23 06:33:21,160] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19461
[2023-09-23 06:33:21,161] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [121]  [120/160]  eta: 0:00:39  lr: 0.000016  min_lr: 0.000004  loss: 1.4063 (1.4156)  loss_scale: 8192.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2492 (6.9247)  time: 0.8173 (0.5075 -- 2.2625)  data: 0.0015 (0.0001 -- 0.0044)  max mem: 16413
Epoch: [121]  [140/160]  eta: 0:00:19  lr: 0.000016  min_lr: 0.000004  loss: 1.4519 (1.4207)  loss_scale: 8192.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0516 (6.9813)  time: 0.9739 (0.5275 -- 2.0010)  data: 0.1939 (0.0005 -- 1.4531)  max mem: 16413
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.4011 (1.4219)  loss_scale: 8192.0000 (16332.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6677 (7.1745)  time: 0.6651 (0.4930 -- 1.7089)  data: 0.0359 (0.0002 -- 0.7053)  max mem: 16413
Epoch: [121] Total time: 0:02:30 (0.9426 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.4011 (1.4252)  loss_scale: 8192.0000 (16332.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6677 (7.1745)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1380 (0.1380)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4873 (2.4873 -- 2.4873)  data: 2.2734 (2.2734 -- 2.2734)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1715 (0.3055)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4736 (0.1775 -- 2.4873)  data: 0.2819 (0.0002 -- 2.2734)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1787 (0.2939)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (100.0000)  time: 0.2576 (0.1677 -- 1.0196)  data: 0.0712 (0.0001 -- 0.8236)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2196 (0.3575)  acc1: 100.0000 (92.5311)  acc5: 100.0000 (100.0000)  time: 0.2897 (0.1326 -- 1.0196)  data: 0.1107 (0.0001 -- 0.8236)  max mem: 16413
Val: Total time: 0:00:09 (0.3486 s / it)
* Acc@1 91.286 Acc@5 99.585 loss 0.393
Accuracy of the network on the 482 val images: 91.29%
Max accuracy: 92.74%
Epoch: [122]  [  0/160]  eta: 0:24:17  lr: 0.000016  min_lr: 0.000004  loss: 1.7845 (1.7845)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6454 (7.6454)  time: 9.1087 (9.1087 -- 9.1087)  data: 8.5732 (8.5732 -- 8.5732)  max mem: 16413
Epoch: [122]  [ 20/160]  eta: 0:02:58  lr: 0.000016  min_lr: 0.000004  loss: 1.4079 (1.4263)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3478 (7.6317)  time: 0.8846 (0.5135 -- 4.6103)  data: 0.2601 (0.0003 -- 4.0433)  max mem: 16413
Epoch: [122]  [ 40/160]  eta: 0:02:21  lr: 0.000016  min_lr: 0.000004  loss: 1.4631 (1.4566)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0708 (7.3389)  time: 1.0741 (0.5086 -- 5.4340)  data: 0.2285 (0.0002 -- 2.7292)  max mem: 16413
Epoch: [122]  [ 60/160]  eta: 0:01:45  lr: 0.000016  min_lr: 0.000004  loss: 1.4698 (1.4566)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6915 (7.2494)  time: 0.7889 (0.5110 -- 3.5144)  data: 0.0013 (0.0002 -- 0.0050)  max mem: 16413
[2023-09-23 06:35:32,188] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:35:32,188] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:35:32,188] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 06:35:32,188] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [122]  [ 80/160]  eta: 0:01:21  lr: 0.000016  min_lr: 0.000004  loss: 1.3187 (1.4407)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3059 (7.1443)  time: 0.9228 (0.5214 -- 4.3636)  data: 0.1072 (0.0003 -- 2.1228)  max mem: 16413
Epoch: [122]  [100/160]  eta: 0:01:00  lr: 0.000016  min_lr: 0.000004  loss: 1.5958 (1.4578)  loss_scale: 16384.0000 (10706.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8731 (7.1308)  time: 0.9629 (0.5105 -- 4.0734)  data: 0.3753 (0.0004 -- 3.5464)  max mem: 16413
Epoch: [122]  [120/160]  eta: 0:00:38  lr: 0.000016  min_lr: 0.000004  loss: 1.4669 (1.4554)  loss_scale: 16384.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7987 (7.0378)  time: 0.7909 (0.5208 -- 3.0241)  data: 0.2426 (0.0004 -- 2.4926)  max mem: 16413
Epoch: [122]  [140/160]  eta: 0:00:19  lr: 0.000016  min_lr: 0.000004  loss: 1.3928 (1.4387)  loss_scale: 16384.0000 (12317.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6848 (7.0239)  time: 1.0821 (0.5303 -- 5.9902)  data: 0.5263 (0.0006 -- 5.4796)  max mem: 16413
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.3365 (1.4203)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1016 (7.0032)  time: 0.6700 (0.4916 -- 2.7228)  data: 0.1559 (0.0002 -- 2.2133)  max mem: 16413
Epoch: [122] Total time: 0:02:32 (0.9509 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.3365 (1.3977)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1016 (7.0032)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1113 (0.1113)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4892 (2.4892 -- 2.4892)  data: 2.2969 (2.2969 -- 2.2969)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1617 (0.3964)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4854 (0.1843 -- 2.4892)  data: 0.2919 (0.0002 -- 2.2969)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2952 (0.3650)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (100.0000)  time: 0.2562 (0.1686 -- 1.1135)  data: 0.0674 (0.0001 -- 0.9073)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3069 (0.3950)  acc1: 88.8889 (89.6266)  acc5: 100.0000 (100.0000)  time: 0.2842 (0.1325 -- 1.1135)  data: 0.1033 (0.0001 -- 0.9073)  max mem: 16413
Val: Total time: 0:00:09 (0.3458 s / it)
* Acc@1 91.079 Acc@5 99.793 loss 0.370
Accuracy of the network on the 482 val images: 91.08%
Max accuracy: 92.74%
Epoch: [123]  [  0/160]  eta: 0:19:40  lr: 0.000016  min_lr: 0.000004  loss: 0.9147 (0.9147)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5611 (2.5611)  time: 7.3783 (7.3783 -- 7.3783)  data: 6.8296 (6.8296 -- 6.8296)  max mem: 16413
Epoch: [123]  [ 20/160]  eta: 0:02:43  lr: 0.000016  min_lr: 0.000004  loss: 1.4329 (1.4028)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5127 (6.7526)  time: 0.8548 (0.5207 -- 2.9210)  data: 0.3052 (0.0005 -- 2.3725)  max mem: 16413
[2023-09-23 06:37:40,538] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:37:40,538] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:37:40,538] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:37:40,538] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [123]  [ 40/160]  eta: 0:02:07  lr: 0.000016  min_lr: 0.000004  loss: 1.3580 (1.4011)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7050 (7.0946)  time: 0.9569 (0.5231 -- 2.9985)  data: 0.2676 (0.0006 -- 2.4225)  max mem: 16413
[2023-09-23 06:37:48,469] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19726
[2023-09-23 06:37:48,469] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:37:48,472] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19726
[2023-09-23 06:37:48,472] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:37:48,472] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [123]  [ 60/160]  eta: 0:01:39  lr: 0.000016  min_lr: 0.000004  loss: 1.5761 (1.4489)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5683 (7.0077)  time: 0.8506 (0.5156 -- 1.8877)  data: 0.2316 (0.0001 -- 1.3529)  max mem: 16413
Epoch: [123]  [ 80/160]  eta: 0:01:17  lr: 0.000016  min_lr: 0.000004  loss: 1.5006 (1.4410)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7289 (6.9161)  time: 0.9016 (0.5155 -- 3.0295)  data: 0.3578 (0.0005 -- 2.4646)  max mem: 16413
Epoch: [123]  [100/160]  eta: 0:00:58  lr: 0.000016  min_lr: 0.000004  loss: 1.5593 (1.4413)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1266 (7.0419)  time: 0.9686 (0.5272 -- 2.7079)  data: 0.4288 (0.0003 -- 2.1842)  max mem: 16413
Epoch: [123]  [120/160]  eta: 0:00:38  lr: 0.000016  min_lr: 0.000004  loss: 1.4711 (1.4414)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2055 (6.9481)  time: 0.9123 (0.5089 -- 3.9161)  data: 0.2360 (0.0003 -- 1.7091)  max mem: 16413
Epoch: [123]  [140/160]  eta: 0:00:19  lr: 0.000016  min_lr: 0.000004  loss: 1.3550 (1.4283)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2884 (6.9662)  time: 0.9658 (0.5162 -- 3.6157)  data: 0.0012 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.3550 (1.4224)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6133 (6.9922)  time: 0.7269 (0.4993 -- 3.6267)  data: 0.0007 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [123] Total time: 0:02:29 (0.9350 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.3550 (1.4158)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6133 (6.9922)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1518 (0.1518)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4140 (2.4140 -- 2.4140)  data: 2.2355 (2.2355 -- 2.2355)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1315 (0.3174)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4707 (0.1837 -- 2.4140)  data: 0.2786 (0.0006 -- 2.2355)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1405 (0.2854)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2587 (0.1677 -- 1.0195)  data: 0.0710 (0.0001 -- 0.8199)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1552 (0.3513)  acc1: 100.0000 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2783 (0.1330 -- 1.0195)  data: 0.0994 (0.0001 -- 0.8199)  max mem: 16413
Val: Total time: 0:00:09 (0.3402 s / it)
* Acc@1 91.701 Acc@5 99.585 loss 0.361
Accuracy of the network on the 482 val images: 91.70%
Max accuracy: 92.74%
Epoch: [124]  [  0/160]  eta: 0:21:09  lr: 0.000016  min_lr: 0.000004  loss: 1.9772 (1.9772)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5539 (6.5539)  time: 7.9331 (7.9331 -- 7.9331)  data: 7.4003 (7.4003 -- 7.4003)  max mem: 16413
[2023-09-23 06:39:58,824] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:39:58,825] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:39:58,827] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:39:58,827] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [124]  [ 20/160]  eta: 0:02:46  lr: 0.000015  min_lr: 0.000004  loss: 1.3128 (1.3791)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5846 (7.0053)  time: 0.8523 (0.5193 -- 3.6312)  data: 0.3002 (0.0002 -- 3.0583)  max mem: 16413
Epoch: [124]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000004  loss: 1.4429 (1.3778)  loss_scale: 32768.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8391 (7.1230)  time: 0.8912 (0.5240 -- 3.2538)  data: 0.3035 (0.0002 -- 2.7107)  max mem: 16413
Epoch: [124]  [ 60/160]  eta: 0:01:41  lr: 0.000015  min_lr: 0.000004  loss: 1.4175 (1.4025)  loss_scale: 32768.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6289 (6.9713)  time: 0.9665 (0.5258 -- 2.5401)  data: 0.1048 (0.0003 -- 2.0201)  max mem: 16413
[2023-09-23 06:40:48,277] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19907
[2023-09-23 06:40:48,277] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:40:48,277] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19907
[2023-09-23 06:40:48,277] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:40:48,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [124]  [ 80/160]  eta: 0:01:18  lr: 0.000015  min_lr: 0.000004  loss: 1.3753 (1.4110)  loss_scale: 16384.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8354 (6.9234)  time: 0.8738 (0.5051 -- 3.3656)  data: 0.2984 (0.0002 -- 2.8373)  max mem: 16413
Epoch: [124]  [100/160]  eta: 0:00:58  lr: 0.000015  min_lr: 0.000004  loss: 1.4836 (1.4115)  loss_scale: 16384.0000 (24819.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9520 (6.9474)  time: 0.9544 (0.5194 -- 3.1384)  data: 0.4090 (0.0004 -- 2.5810)  max mem: 16413
Epoch: [124]  [120/160]  eta: 0:00:38  lr: 0.000015  min_lr: 0.000004  loss: 1.4117 (1.4116)  loss_scale: 16384.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8657 (6.9286)  time: 0.8923 (0.5267 -- 2.5590)  data: 0.1857 (0.0002 -- 2.0431)  max mem: 16413
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 1.3946 (1.4059)  loss_scale: 16384.0000 (22426.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6271 (6.9539)  time: 0.8687 (0.5224 -- 3.7879)  data: 0.0271 (0.0005 -- 0.4942)  max mem: 16413
[2023-09-23 06:42:04,099] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19992
[2023-09-23 06:42:04,099] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19992
[2023-09-23 06:42:04,099] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 06:42:04,099] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 06:42:04,099] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-23 06:42:09,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=116, lr=[3.853570513929506e-06, 3.853570513929506e-06, 4.281745015477229e-06, 4.281745015477229e-06, 4.757494461641365e-06, 4.757494461641365e-06, 5.286104957379294e-06, 5.286104957379294e-06, 5.87344995264366e-06, 5.87344995264366e-06, 6.5260555029374e-06, 6.5260555029374e-06, 7.251172781041555e-06, 7.251172781041555e-06, 8.056858645601727e-06, 8.056858645601727e-06, 8.952065161779698e-06, 8.952065161779698e-06, 9.946739068644107e-06, 9.946739068644107e-06, 1.1051932298493453e-05, 1.1051932298493453e-05, 1.2279924776103836e-05, 1.2279924776103836e-05, 1.3644360862337596e-05, 1.3644360862337596e-05, 1.5160400958152884e-05, 1.5160400958152884e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 06:42:09,442] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=17.12449875360295, CurrSamplesPerSec=24.55233481010003, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000004  loss: 1.2748 (1.4042)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9149 (6.9347)  time: 0.8261 (0.4898 -- 2.8120)  data: 0.0007 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [124] Total time: 0:02:29 (0.9371 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000004  loss: 1.2748 (1.3833)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9149 (6.9347)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1290 (0.1290)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5178 (2.5178 -- 2.5178)  data: 2.3185 (2.3185 -- 2.3185)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1520 (0.3580)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4864 (0.1799 -- 2.5178)  data: 0.2966 (0.0003 -- 2.3185)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1504 (0.3035)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2546 (0.1683 -- 1.1531)  data: 0.0683 (0.0001 -- 0.9388)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1644 (0.3756)  acc1: 100.0000 (91.2863)  acc5: 100.0000 (100.0000)  time: 0.2774 (0.1329 -- 1.1531)  data: 0.0982 (0.0001 -- 0.9388)  max mem: 16413
Val: Total time: 0:00:09 (0.3406 s / it)
* Acc@1 91.494 Acc@5 100.000 loss 0.366
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 92.74%
Epoch: [125]  [  0/160]  eta: 0:19:36  lr: 0.000015  min_lr: 0.000004  loss: 1.7693 (1.7693)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6333 (7.6333)  time: 7.3513 (7.3513 -- 7.3513)  data: 6.2942 (6.2942 -- 6.2942)  max mem: 16413
Epoch: [125]  [ 20/160]  eta: 0:02:51  lr: 0.000015  min_lr: 0.000004  loss: 1.4338 (1.4537)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1440 (6.9028)  time: 0.9201 (0.5276 -- 2.9154)  data: 0.2657 (0.0004 -- 1.7471)  max mem: 16413
Epoch: [125]  [ 40/160]  eta: 0:02:11  lr: 0.000015  min_lr: 0.000004  loss: 1.4732 (1.4237)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4590 (6.5743)  time: 0.9578 (0.5230 -- 3.2945)  data: 0.2651 (0.0004 -- 2.7621)  max mem: 16413
Epoch: [125]  [ 60/160]  eta: 0:01:44  lr: 0.000015  min_lr: 0.000004  loss: 1.4027 (1.4105)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8341 (6.6641)  time: 0.9292 (0.5261 -- 4.5684)  data: 0.0303 (0.0004 -- 0.5731)  max mem: 16413
Epoch: [125]  [ 80/160]  eta: 0:01:21  lr: 0.000015  min_lr: 0.000004  loss: 1.6523 (1.4710)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7367 (7.2497)  time: 0.9731 (0.5222 -- 3.7636)  data: 0.0888 (0.0003 -- 1.1774)  max mem: 16413
Epoch: [125]  [100/160]  eta: 0:01:00  lr: 0.000015  min_lr: 0.000004  loss: 1.4574 (1.4657)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8896 (7.1052)  time: 0.9157 (0.5051 -- 4.3188)  data: 0.3758 (0.0003 -- 3.8118)  max mem: 16413
Epoch: [125]  [120/160]  eta: 0:00:38  lr: 0.000015  min_lr: 0.000004  loss: 1.1995 (1.4365)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5978 (6.9727)  time: 0.8054 (0.5155 -- 3.5358)  data: 0.2581 (0.0004 -- 3.0200)  max mem: 16413
[2023-09-23 06:44:19,813] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:44:19,814] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 06:44:19,816] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:44:19,816] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [125]  [140/160]  eta: 0:00:19  lr: 0.000015  min_lr: 0.000004  loss: 1.4153 (1.4381)  loss_scale: 16384.0000 (9353.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9440 (6.8710)  time: 0.9985 (0.5186 -- 5.2718)  data: 0.3427 (0.0005 -- 4.7322)  max mem: 16413
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000004  loss: 1.3636 (1.4337)  loss_scale: 16384.0000 (10188.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0985 (6.9266)  time: 0.6927 (0.4935 -- 3.6823)  data: 0.1768 (0.0002 -- 3.1478)  max mem: 16413
Epoch: [125] Total time: 0:02:30 (0.9421 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000004  loss: 1.3636 (1.4097)  loss_scale: 16384.0000 (10188.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0985 (6.9266)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1554 (0.1554)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4086 (2.4086 -- 2.4086)  data: 2.1922 (2.1922 -- 2.1922)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1703 (0.3222)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4736 (0.1775 -- 2.4086)  data: 0.2798 (0.0005 -- 2.1922)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1500 (0.3127)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2608 (0.1682 -- 1.0815)  data: 0.0712 (0.0001 -- 0.8796)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2066 (0.3650)  acc1: 100.0000 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2812 (0.1332 -- 1.0815)  data: 0.0998 (0.0001 -- 0.8796)  max mem: 16413
Val: Total time: 0:00:09 (0.3402 s / it)
* Acc@1 91.079 Acc@5 99.793 loss 0.376
Accuracy of the network on the 482 val images: 91.08%
Max accuracy: 92.74%
Epoch: [126]  [  0/160]  eta: 0:19:20  lr: 0.000015  min_lr: 0.000004  loss: 1.5995 (1.5995)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2081 (4.2081)  time: 7.2535 (7.2535 -- 7.2535)  data: 6.7093 (6.7093 -- 6.7093)  max mem: 16413
Epoch: [126]  [ 20/160]  eta: 0:02:49  lr: 0.000015  min_lr: 0.000004  loss: 1.3664 (1.3593)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0269 (6.5858)  time: 0.9074 (0.5304 -- 3.2612)  data: 0.2481 (0.0002 -- 2.6899)  max mem: 16413
Epoch: [126]  [ 40/160]  eta: 0:02:06  lr: 0.000015  min_lr: 0.000004  loss: 1.5368 (1.3867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2760 (7.0111)  time: 0.8846 (0.5214 -- 2.5055)  data: 0.2913 (0.0002 -- 1.9568)  max mem: 16413
Epoch: [126]  [ 60/160]  eta: 0:01:44  lr: 0.000015  min_lr: 0.000004  loss: 1.3072 (1.3698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0447 (6.7967)  time: 1.0247 (0.5125 -- 4.0415)  data: 0.4754 (0.0005 -- 3.5115)  max mem: 16413
Epoch: [126]  [ 80/160]  eta: 0:01:21  lr: 0.000015  min_lr: 0.000004  loss: 1.3999 (1.3804)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5592 (6.7791)  time: 0.9260 (0.5221 -- 2.9817)  data: 0.3493 (0.0001 -- 2.4780)  max mem: 16413
[2023-09-23 06:46:29,630] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:46:29,630] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:46:29,630] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:46:29,630] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:46:31,841] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20253
[2023-09-23 06:46:31,842] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:46:31,842] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20253
[2023-09-23 06:46:31,842] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:46:31,842] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [126]  [100/160]  eta: 0:00:59  lr: 0.000015  min_lr: 0.000004  loss: 1.4614 (1.3774)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1698 (6.7573)  time: 0.8992 (0.5223 -- 4.0725)  data: 0.3524 (0.0002 -- 3.5439)  max mem: 16413
[2023-09-23 06:46:40,765] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20263
[2023-09-23 06:46:40,765] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20263
[2023-09-23 06:46:40,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 06:46:40,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 06:46:40,766] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [126]  [120/160]  eta: 0:00:39  lr: 0.000015  min_lr: 0.000004  loss: 1.4247 (1.3953)  loss_scale: 8192.0000 (15706.9752)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9334 (6.6663)  time: 0.9976 (0.5290 -- 3.8982)  data: 0.4505 (0.0002 -- 3.3360)  max mem: 16413
Epoch: [126]  [140/160]  eta: 0:00:19  lr: 0.000015  min_lr: 0.000004  loss: 1.5093 (1.4144)  loss_scale: 8192.0000 (14641.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1131 (6.7526)  time: 0.9966 (0.5213 -- 5.4068)  data: 0.1741 (0.0004 -- 1.9049)  max mem: 16413
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 1.2855 (1.4189)  loss_scale: 8192.0000 (13875.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3350 (6.7797)  time: 0.6388 (0.4936 -- 3.0312)  data: 0.0004 (0.0002 -- 0.0011)  max mem: 16413
Epoch: [126] Total time: 0:02:32 (0.9516 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 1.2855 (1.4070)  loss_scale: 8192.0000 (13875.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3350 (6.7797)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1354 (0.1354)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5310 (2.5310 -- 2.5310)  data: 2.3476 (2.3476 -- 2.3476)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1623 (0.3358)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4963 (0.1759 -- 2.5310)  data: 0.3063 (0.0002 -- 2.3476)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1651 (0.3305)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2609 (0.1703 -- 1.2064)  data: 0.0734 (0.0001 -- 1.0171)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2393 (0.4021)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (99.5851)  time: 0.2785 (0.1362 -- 1.2064)  data: 0.0977 (0.0001 -- 1.0171)  max mem: 16413
Val: Total time: 0:00:09 (0.3418 s / it)
* Acc@1 91.494 Acc@5 99.378 loss 0.393
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 92.74%
Epoch: [127]  [  0/160]  eta: 0:19:47  lr: 0.000014  min_lr: 0.000004  loss: 1.8833 (1.8833)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3622 (8.3622)  time: 7.4198 (7.4198 -- 7.4198)  data: 6.5347 (6.5347 -- 6.5347)  max mem: 16413
Epoch: [127]  [ 20/160]  eta: 0:03:05  lr: 0.000014  min_lr: 0.000004  loss: 1.4286 (1.4679)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4151 (7.5477)  time: 1.0176 (0.5221 -- 4.3911)  data: 0.0779 (0.0002 -- 1.5311)  max mem: 16413
Epoch: [127]  [ 40/160]  eta: 0:02:09  lr: 0.000014  min_lr: 0.000004  loss: 1.1714 (1.3530)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5845 (7.3274)  time: 0.8246 (0.5196 -- 3.0155)  data: 0.0015 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [127]  [ 60/160]  eta: 0:01:46  lr: 0.000014  min_lr: 0.000004  loss: 1.5106 (1.3916)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3383 (7.2973)  time: 1.0394 (0.5184 -- 5.1619)  data: 0.4089 (0.0004 -- 4.6389)  max mem: 16413
[2023-09-23 06:48:55,792] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:48:55,792] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 06:48:55,792] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:48:55,793] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [127]  [ 80/160]  eta: 0:01:19  lr: 0.000014  min_lr: 0.000004  loss: 1.5160 (1.4120)  loss_scale: 8192.0000 (9102.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5218 (7.1614)  time: 0.7967 (0.5207 -- 4.1244)  data: 0.0990 (0.0004 -- 1.4405)  max mem: 16413
Epoch: [127]  [100/160]  eta: 0:00:59  lr: 0.000014  min_lr: 0.000004  loss: 1.5834 (1.4261)  loss_scale: 16384.0000 (10544.1584)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7444 (7.1479)  time: 0.9573 (0.5138 -- 5.2504)  data: 0.4143 (0.0003 -- 4.7399)  max mem: 16413
Epoch: [127]  [120/160]  eta: 0:00:38  lr: 0.000014  min_lr: 0.000004  loss: 1.3445 (1.4206)  loss_scale: 16384.0000 (11509.4215)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4509 (7.1421)  time: 0.8451 (0.5214 -- 4.0368)  data: 0.2956 (0.0002 -- 3.4874)  max mem: 16413
Epoch: [127]  [140/160]  eta: 0:00:19  lr: 0.000014  min_lr: 0.000004  loss: 1.3631 (1.4163)  loss_scale: 16384.0000 (12200.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2037 (7.0784)  time: 1.0267 (0.5258 -- 3.6061)  data: 0.4287 (0.0005 -- 3.0667)  max mem: 16413
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 1.4720 (1.4246)  loss_scale: 16384.0000 (12697.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0870 (7.1359)  time: 0.7130 (0.4933 -- 2.3783)  data: 0.1788 (0.0002 -- 1.8825)  max mem: 16413
Epoch: [127] Total time: 0:02:31 (0.9457 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 1.4720 (1.4250)  loss_scale: 16384.0000 (12697.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0870 (7.1359)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1288 (0.1288)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5028 (2.5028 -- 2.5028)  data: 2.2999 (2.2999 -- 2.2999)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1554 (0.3383)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4805 (0.1796 -- 2.5028)  data: 0.2896 (0.0004 -- 2.2999)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2039 (0.3448)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2542 (0.1678 -- 1.0881)  data: 0.0660 (0.0001 -- 0.8803)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3624 (0.3845)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2907 (0.1317 -- 1.0881)  data: 0.1094 (0.0001 -- 0.8803)  max mem: 16413
Val: Total time: 0:00:09 (0.3496 s / it)
* Acc@1 91.701 Acc@5 99.793 loss 0.369
Accuracy of the network on the 482 val images: 91.70%
Max accuracy: 92.74%
Epoch: [128]  [  0/160]  eta: 0:17:20  lr: 0.000014  min_lr: 0.000004  loss: 2.0081 (2.0081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8634 (6.8634)  time: 6.5042 (6.5042 -- 6.5042)  data: 4.8379 (4.8379 -- 4.8379)  max mem: 16413
Epoch: [128]  [ 20/160]  eta: 0:03:00  lr: 0.000014  min_lr: 0.000004  loss: 1.2800 (1.3203)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0122 (6.9504)  time: 1.0259 (0.5218 -- 4.2922)  data: 0.4725 (0.0008 -- 3.7380)  max mem: 16413
[2023-09-23 06:51:04,735] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:51:04,736] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:51:04,736] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:51:04,736] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [128]  [ 40/160]  eta: 0:02:06  lr: 0.000014  min_lr: 0.000004  loss: 1.5661 (1.4184)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4530 (7.4097)  time: 0.8103 (0.5196 -- 3.8591)  data: 0.2526 (0.0004 -- 3.3224)  max mem: 16413
Epoch: [128]  [ 60/160]  eta: 0:01:43  lr: 0.000014  min_lr: 0.000004  loss: 1.5174 (1.4217)  loss_scale: 32768.0000 (22024.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7056 (7.3726)  time: 1.0070 (0.5248 -- 4.1699)  data: 0.4628 (0.0003 -- 3.6540)  max mem: 16413
[2023-09-23 06:51:32,519] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20551
[2023-09-23 06:51:32,519] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20551
[2023-09-23 06:51:32,520] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:51:32,520] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:51:32,520] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [128]  [ 80/160]  eta: 0:01:18  lr: 0.000014  min_lr: 0.000004  loss: 1.4731 (1.4362)  loss_scale: 16384.0000 (22654.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3672 (7.2618)  time: 0.7818 (0.5318 -- 3.5699)  data: 0.2353 (0.0003 -- 3.0491)  max mem: 16413
Epoch: [128]  [100/160]  eta: 0:00:58  lr: 0.000014  min_lr: 0.000004  loss: 1.3971 (1.4344)  loss_scale: 16384.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1729 (7.2769)  time: 0.9575 (0.5343 -- 3.2719)  data: 0.3901 (0.0005 -- 2.4485)  max mem: 16413
Epoch: [128]  [120/160]  eta: 0:00:38  lr: 0.000014  min_lr: 0.000004  loss: 1.5145 (1.4441)  loss_scale: 16384.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1465 (7.3178)  time: 0.9566 (0.5343 -- 4.3631)  data: 0.4108 (0.0008 -- 3.8066)  max mem: 16413
Epoch: [128]  [140/160]  eta: 0:00:19  lr: 0.000014  min_lr: 0.000004  loss: 1.4781 (1.4451)  loss_scale: 16384.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7664 (7.2422)  time: 0.9265 (0.5091 -- 5.0102)  data: 0.3869 (0.0003 -- 4.4871)  max mem: 16413
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000003  loss: 1.2413 (1.4375)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2386 (7.1854)  time: 0.7595 (0.4941 -- 4.6613)  data: 0.2535 (0.0002 -- 4.1514)  max mem: 16413
Epoch: [128] Total time: 0:02:30 (0.9409 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000003  loss: 1.2413 (1.4274)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2386 (7.1854)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.4133 (0.4133)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5072 (2.5072 -- 2.5072)  data: 2.2793 (2.2793 -- 2.2793)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1933 (0.3006)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4754 (0.1795 -- 2.5072)  data: 0.2861 (0.0003 -- 2.2793)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1933 (0.2957)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2538 (0.1678 -- 1.0569)  data: 0.0682 (0.0001 -- 0.8601)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2692 (0.3678)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (100.0000)  time: 0.2862 (0.1329 -- 1.0569)  data: 0.1065 (0.0001 -- 0.8601)  max mem: 16413
Val: Total time: 0:00:09 (0.3459 s / it)
* Acc@1 91.909 Acc@5 100.000 loss 0.365
Accuracy of the network on the 482 val images: 91.91%
Max accuracy: 92.74%
Epoch: [129]  [  0/160]  eta: 0:21:56  lr: 0.000014  min_lr: 0.000003  loss: 0.8419 (0.8419)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3172 (6.3172)  time: 8.2291 (8.2291 -- 8.2291)  data: 7.0508 (7.0508 -- 7.0508)  max mem: 16413
Epoch: [129]  [ 20/160]  eta: 0:02:48  lr: 0.000014  min_lr: 0.000003  loss: 1.4667 (1.4660)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3232 (7.1882)  time: 0.8501 (0.5267 -- 3.9772)  data: 0.2291 (0.0004 -- 2.5981)  max mem: 16413
[2023-09-23 06:53:44,950] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:53:44,951] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:53:44,952] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:53:44,952] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [129]  [ 40/160]  eta: 0:02:07  lr: 0.000014  min_lr: 0.000003  loss: 1.4227 (1.4519)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7865 (7.4634)  time: 0.9116 (0.5207 -- 4.7791)  data: 0.3314 (0.0005 -- 4.2500)  max mem: 16413
Epoch: [129]  [ 60/160]  eta: 0:01:40  lr: 0.000014  min_lr: 0.000003  loss: 1.4441 (1.4437)  loss_scale: 32768.0000 (22024.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6177 (7.2389)  time: 0.8921 (0.5236 -- 3.1130)  data: 0.3508 (0.0004 -- 2.5784)  max mem: 16413
Epoch: [129]  [ 80/160]  eta: 0:01:18  lr: 0.000014  min_lr: 0.000003  loss: 1.3207 (1.4407)  loss_scale: 32768.0000 (24677.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9670 (7.0699)  time: 0.8918 (0.5334 -- 2.7771)  data: 0.1627 (0.0006 -- 2.2416)  max mem: 16413
Epoch: [129]  [100/160]  eta: 0:00:58  lr: 0.000014  min_lr: 0.000003  loss: 1.3108 (1.4282)  loss_scale: 32768.0000 (26279.2871)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9483 (6.9203)  time: 0.9722 (0.5190 -- 3.4073)  data: 0.0482 (0.0003 -- 0.9334)  max mem: 16413
[2023-09-23 06:54:49,127] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20749
[2023-09-23 06:54:49,127] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:54:49,127] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20749
[2023-09-23 06:54:49,127] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:54:49,127] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [129]  [120/160]  eta: 0:00:38  lr: 0.000014  min_lr: 0.000003  loss: 1.5496 (1.4410)  loss_scale: 16384.0000 (25726.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5064 (6.9627)  time: 0.9124 (0.4983 -- 4.6967)  data: 0.0011 (0.0003 -- 0.0022)  max mem: 16413
Epoch: [129]  [140/160]  eta: 0:00:19  lr: 0.000013  min_lr: 0.000003  loss: 1.4534 (1.4397)  loss_scale: 16384.0000 (24401.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5690 (6.8714)  time: 0.9656 (0.5225 -- 3.7249)  data: 0.0013 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.4178 (1.4296)  loss_scale: 16384.0000 (23449.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7908 (6.9120)  time: 0.7116 (0.4940 -- 3.6629)  data: 0.0008 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [129] Total time: 0:02:29 (0.9368 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.4178 (1.4369)  loss_scale: 16384.0000 (23449.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7908 (6.9120)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1267 (0.1267)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4259 (2.4259 -- 2.4259)  data: 2.2283 (2.2283 -- 2.2283)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1466 (0.3793)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4662 (0.1742 -- 2.4259)  data: 0.2767 (0.0003 -- 2.2283)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1468 (0.3298)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2580 (0.1688 -- 1.0158)  data: 0.0717 (0.0001 -- 0.8093)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2866 (0.4095)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (100.0000)  time: 0.2844 (0.1330 -- 1.0158)  data: 0.1040 (0.0001 -- 0.8093)  max mem: 16413
Val: Total time: 0:00:09 (0.3415 s / it)
* Acc@1 91.079 Acc@5 99.378 loss 0.411
Accuracy of the network on the 482 val images: 91.08%
Max accuracy: 92.74%
Epoch: [130]  [  0/160]  eta: 0:22:37  lr: 0.000013  min_lr: 0.000003  loss: 1.3764 (1.3764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8076 (4.8076)  time: 8.4868 (8.4868 -- 8.4868)  data: 6.1794 (6.1794 -- 6.1794)  max mem: 16413
Epoch: [130]  [ 20/160]  eta: 0:02:56  lr: 0.000013  min_lr: 0.000003  loss: 1.4555 (1.4873)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4440 (6.3615)  time: 0.8957 (0.5122 -- 4.7453)  data: 0.1588 (0.0002 -- 2.0064)  max mem: 16413
Epoch: [130]  [ 40/160]  eta: 0:02:12  lr: 0.000013  min_lr: 0.000003  loss: 1.3535 (1.4831)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0428 (6.5127)  time: 0.9394 (0.5262 -- 3.1503)  data: 0.2537 (0.0002 -- 2.2989)  max mem: 16413
Epoch: [130]  [ 60/160]  eta: 0:01:44  lr: 0.000013  min_lr: 0.000003  loss: 1.3716 (1.4352)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5380 (6.8113)  time: 0.9392 (0.5206 -- 3.0503)  data: 0.3957 (0.0002 -- 2.5379)  max mem: 16413
[2023-09-23 06:57:01,816] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:57:01,817] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:57:01,818] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:57:01,818] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [130]  [ 80/160]  eta: 0:01:21  lr: 0.000013  min_lr: 0.000003  loss: 1.2146 (1.4242)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3941 (6.9753)  time: 0.9351 (0.5212 -- 3.8791)  data: 0.3514 (0.0003 -- 3.3291)  max mem: 16413
Epoch: [130]  [100/160]  eta: 0:00:59  lr: 0.000013  min_lr: 0.000003  loss: 1.3908 (1.4305)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3582 (6.9494)  time: 0.8646 (0.5217 -- 2.5517)  data: 0.2024 (0.0004 -- 2.0423)  max mem: 16413
Epoch: [130]  [120/160]  eta: 0:00:40  lr: 0.000013  min_lr: 0.000003  loss: 1.3903 (1.4327)  loss_scale: 32768.0000 (22206.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0955 (6.8738)  time: 1.0970 (0.5155 -- 6.6548)  data: 0.0019 (0.0002 -- 0.0089)  max mem: 16413
[2023-09-23 06:57:43,472] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20921
[2023-09-23 06:57:43,472] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20921
[2023-09-23 06:57:43,473] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:57:43,473] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 06:57:43,473] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [130]  [140/160]  eta: 0:00:19  lr: 0.000013  min_lr: 0.000003  loss: 1.2119 (1.4157)  loss_scale: 16384.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8902 (6.8063)  time: 0.6934 (0.5078 -- 2.5622)  data: 0.0014 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.3618 (1.4038)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7476 (6.7776)  time: 0.8093 (0.4947 -- 3.6606)  data: 0.0008 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [130] Total time: 0:02:31 (0.9465 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.3618 (1.4076)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7476 (6.7776)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2184 (0.2184)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4550 (2.4550 -- 2.4550)  data: 2.2633 (2.2633 -- 2.2633)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2087 (0.3628)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (98.9899)  time: 0.4821 (0.1747 -- 2.4550)  data: 0.2914 (0.0003 -- 2.2633)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1946 (0.3567)  acc1: 100.0000 (92.0635)  acc5: 100.0000 (99.4709)  time: 0.2596 (0.1681 -- 1.1294)  data: 0.0723 (0.0001 -- 0.9383)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2015 (0.4082)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (99.5851)  time: 0.2804 (0.1328 -- 1.1294)  data: 0.1008 (0.0001 -- 0.9383)  max mem: 16413
Val: Total time: 0:00:09 (0.3423 s / it)
* Acc@1 90.664 Acc@5 99.793 loss 0.399
Accuracy of the network on the 482 val images: 90.66%
Max accuracy: 92.74%
Epoch: [131]  [  0/160]  eta: 0:22:07  lr: 0.000013  min_lr: 0.000003  loss: 1.0034 (1.0034)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2235 (5.2235)  time: 8.2978 (8.2978 -- 8.2978)  data: 7.7408 (7.7408 -- 7.7408)  max mem: 16413
Epoch: [131]  [ 20/160]  eta: 0:03:03  lr: 0.000013  min_lr: 0.000003  loss: 1.2615 (1.2768)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4761 (6.4647)  time: 0.9630 (0.5074 -- 5.5992)  data: 0.4288 (0.0003 -- 5.0803)  max mem: 16413
[2023-09-23 06:59:05,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=121, lr=[3.3048525035975797e-06, 3.3048525035975797e-06, 3.6720583373306447e-06, 3.6720583373306447e-06, 4.080064819256271e-06, 4.080064819256271e-06, 4.5334053547291906e-06, 4.5334053547291906e-06, 5.037117060810212e-06, 5.037117060810212e-06, 5.596796734233568e-06, 5.596796734233568e-06, 6.218663038037298e-06, 6.218663038037298e-06, 6.909625597819219e-06, 6.909625597819219e-06, 7.677361775354689e-06, 7.677361775354689e-06, 8.530401972616319e-06, 8.530401972616319e-06, 9.478224414018133e-06, 9.478224414018133e-06, 1.0531360460020147e-05, 1.0531360460020147e-05, 1.1701511622244608e-05, 1.1701511622244608e-05, 1.3001679580271786e-05, 1.3001679580271786e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 06:59:05,136] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=17.18327405056163, CurrSamplesPerSec=22.16350077611546, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [131]  [ 40/160]  eta: 0:02:15  lr: 0.000013  min_lr: 0.000003  loss: 1.4747 (1.3846)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7810 (6.9780)  time: 0.9391 (0.5234 -- 3.3708)  data: 0.3404 (0.0002 -- 2.8333)  max mem: 16413
Epoch: [131]  [ 60/160]  eta: 0:01:46  lr: 0.000013  min_lr: 0.000003  loss: 1.3901 (1.3754)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9165 (7.0351)  time: 0.9362 (0.5115 -- 4.7383)  data: 0.3949 (0.0003 -- 4.2112)  max mem: 16413
Epoch: [131]  [ 80/160]  eta: 0:01:21  lr: 0.000013  min_lr: 0.000003  loss: 1.4299 (1.3818)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8273 (7.1043)  time: 0.8924 (0.5231 -- 4.5991)  data: 0.3479 (0.0003 -- 4.0450)  max mem: 16413
[2023-09-23 06:59:54,603] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:59:54,603] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 06:59:54,606] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 06:59:54,607] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [131]  [100/160]  eta: 0:00:59  lr: 0.000013  min_lr: 0.000003  loss: 1.3292 (1.3858)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8329 (7.0283)  time: 0.8769 (0.5170 -- 4.6943)  data: 0.3380 (0.0003 -- 4.1418)  max mem: 16413
Epoch: [131]  [120/160]  eta: 0:00:40  lr: 0.000013  min_lr: 0.000003  loss: 1.4743 (1.3888)  loss_scale: 32768.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7762 (7.0551)  time: 1.0289 (0.5099 -- 4.3664)  data: 0.4893 (0.0004 -- 3.8359)  max mem: 16413
Epoch: [131]  [140/160]  eta: 0:00:19  lr: 0.000013  min_lr: 0.000003  loss: 1.3344 (1.3878)  loss_scale: 32768.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4648 (7.0399)  time: 0.8648 (0.5264 -- 2.6450)  data: 0.2521 (0.0002 -- 2.1353)  max mem: 16413
[2023-09-23 07:00:52,635] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21117
[2023-09-23 07:00:52,635] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:00:52,635] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21117
[2023-09-23 07:00:52,635] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:00:52,635] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.3190 (1.3850)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9280 (7.0474)  time: 0.6831 (0.4818 -- 2.7766)  data: 0.1371 (0.0001 -- 2.2545)  max mem: 16413
Epoch: [131] Total time: 0:02:31 (0.9467 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.3190 (1.3957)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9280 (7.0474)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2546 (0.2546)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5740 (2.5740 -- 2.5740)  data: 2.3604 (2.3604 -- 2.3604)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2776 (0.3750)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4762 (0.1755 -- 2.5740)  data: 0.2831 (0.0003 -- 2.3604)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2115 (0.3531)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (100.0000)  time: 0.2495 (0.1683 -- 0.9533)  data: 0.0626 (0.0001 -- 0.7485)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2115 (0.4110)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (100.0000)  time: 0.2742 (0.1317 -- 0.9533)  data: 0.0944 (0.0001 -- 0.7485)  max mem: 16413
Val: Total time: 0:00:09 (0.3400 s / it)
* Acc@1 90.041 Acc@5 99.793 loss 0.384
Accuracy of the network on the 482 val images: 90.04%
Max accuracy: 92.74%
Epoch: [132]  [  0/160]  eta: 0:17:27  lr: 0.000013  min_lr: 0.000003  loss: 1.3574 (1.3574)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5492 (7.5492)  time: 6.5478 (6.5478 -- 6.5478)  data: 5.9900 (5.9900 -- 5.9900)  max mem: 16413
Epoch: [132]  [ 20/160]  eta: 0:03:06  lr: 0.000013  min_lr: 0.000003  loss: 1.4529 (1.3781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6931 (7.1437)  time: 1.0726 (0.5151 -- 4.7770)  data: 0.5318 (0.0002 -- 4.2369)  max mem: 16413
Epoch: [132]  [ 40/160]  eta: 0:02:10  lr: 0.000013  min_lr: 0.000003  loss: 1.4745 (1.4227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0243 (7.0047)  time: 0.8371 (0.5316 -- 2.7961)  data: 0.2903 (0.0002 -- 2.2713)  max mem: 16413
Epoch: [132]  [ 60/160]  eta: 0:01:45  lr: 0.000013  min_lr: 0.000003  loss: 1.4173 (1.4328)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2479 (6.8758)  time: 0.9731 (0.5288 -- 3.5691)  data: 0.4320 (0.0002 -- 3.0508)  max mem: 16413
Epoch: [132]  [ 80/160]  eta: 0:01:20  lr: 0.000013  min_lr: 0.000003  loss: 1.3948 (1.4252)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1949 (7.1061)  time: 0.8691 (0.5178 -- 3.8432)  data: 0.3312 (0.0003 -- 3.3220)  max mem: 16413
Epoch: [132]  [100/160]  eta: 0:01:02  lr: 0.000013  min_lr: 0.000003  loss: 1.2915 (1.4150)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1958 (6.9695)  time: 1.1396 (0.5101 -- 4.6332)  data: 0.5880 (0.0003 -- 4.1298)  max mem: 16413
Epoch: [132]  [120/160]  eta: 0:00:39  lr: 0.000012  min_lr: 0.000003  loss: 1.3433 (1.4031)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4817 (6.8660)  time: 0.7700 (0.5148 -- 3.7506)  data: 0.2358 (0.0003 -- 3.2308)  max mem: 16413
[2023-09-23 07:03:09,884] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:03:09,884] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:03:09,884] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:03:09,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [132]  [140/160]  eta: 0:00:20  lr: 0.000012  min_lr: 0.000003  loss: 1.5495 (1.4247)  loss_scale: 32768.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9406 (6.7712)  time: 1.0753 (0.5068 -- 4.9836)  data: 0.5419 (0.0002 -- 4.4496)  max mem: 16413
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.4518 (1.4334)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0181 (6.7242)  time: 0.6067 (0.4958 -- 2.3544)  data: 0.0925 (0.0001 -- 1.8356)  max mem: 16413
Epoch: [132] Total time: 0:02:32 (0.9559 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.4518 (1.3995)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0181 (6.7242)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1486 (0.1486)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5580 (2.5580 -- 2.5580)  data: 2.3087 (2.3087 -- 2.3087)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1486 (0.3808)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4789 (0.1765 -- 2.5580)  data: 0.2859 (0.0002 -- 2.3087)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1701 (0.3610)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2518 (0.1686 -- 1.0186)  data: 0.0672 (0.0001 -- 0.8290)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3011 (0.4031)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (100.0000)  time: 0.2869 (0.1328 -- 1.0186)  data: 0.1094 (0.0001 -- 0.8480)  max mem: 16413
Val: Total time: 0:00:09 (0.3489 s / it)
* Acc@1 91.494 Acc@5 99.585 loss 0.376
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 92.74%
Epoch: [133]  [  0/160]  eta: 0:19:39  lr: 0.000012  min_lr: 0.000003  loss: 1.7967 (1.7967)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1661 (7.1661)  time: 7.3711 (7.3711 -- 7.3711)  data: 6.0881 (6.0881 -- 6.0881)  max mem: 16413
Epoch: [133]  [ 20/160]  eta: 0:03:00  lr: 0.000012  min_lr: 0.000003  loss: 1.2721 (1.2882)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9939 (6.2284)  time: 0.9823 (0.5211 -- 2.8841)  data: 0.0690 (0.0004 -- 0.6925)  max mem: 16413
Epoch: [133]  [ 40/160]  eta: 0:02:11  lr: 0.000012  min_lr: 0.000003  loss: 1.0366 (1.2258)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4780 (6.3572)  time: 0.8981 (0.5108 -- 3.5189)  data: 0.0020 (0.0004 -- 0.0139)  max mem: 16413
Epoch: [133]  [ 60/160]  eta: 0:01:44  lr: 0.000012  min_lr: 0.000003  loss: 1.5562 (1.3198)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4493 (6.4587)  time: 0.9520 (0.5232 -- 3.5939)  data: 0.0015 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [133]  [ 80/160]  eta: 0:01:21  lr: 0.000012  min_lr: 0.000003  loss: 1.5770 (1.3687)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2068 (6.5627)  time: 0.9502 (0.5068 -- 4.8708)  data: 0.0012 (0.0003 -- 0.0029)  max mem: 16413
[2023-09-23 07:05:20,682] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:05:20,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 07:05:20,683] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:05:20,683] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 07:05:22,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21377
[2023-09-23 07:05:22,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21377
[2023-09-23 07:05:22,318] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 07:05:22,318] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 07:05:22,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [133]  [100/160]  eta: 0:01:00  lr: 0.000012  min_lr: 0.000003  loss: 1.3212 (1.3730)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1025 (6.4163)  time: 0.9672 (0.5235 -- 4.3175)  data: 0.0011 (0.0003 -- 0.0023)  max mem: 16413
[2023-09-23 07:05:42,885] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21396
[2023-09-23 07:05:42,885] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:05:42,885] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21396
[2023-09-23 07:05:42,885] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:05:42,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [133]  [120/160]  eta: 0:00:39  lr: 0.000012  min_lr: 0.000003  loss: 1.4293 (1.3799)  loss_scale: 32768.0000 (32903.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2318 (6.4732)  time: 0.8645 (0.5111 -- 4.8800)  data: 0.0019 (0.0002 -- 0.0169)  max mem: 16413
Epoch: [133]  [140/160]  eta: 0:00:19  lr: 0.000012  min_lr: 0.000003  loss: 1.2685 (1.3721)  loss_scale: 16384.0000 (30560.2270)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9321 (6.4863)  time: 0.9363 (0.5234 -- 4.7473)  data: 0.0012 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.3947 (1.3757)  loss_scale: 16384.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4145 (6.4356)  time: 0.6423 (0.4950 -- 2.3601)  data: 0.0007 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [133] Total time: 0:02:30 (0.9421 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.3947 (1.3854)  loss_scale: 16384.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4145 (6.4356)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2473 (0.2473)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5464 (2.5464 -- 2.5464)  data: 2.3373 (2.3373 -- 2.3373)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1531 (0.3038)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (98.9899)  time: 0.4781 (0.1774 -- 2.5464)  data: 0.2843 (0.0002 -- 2.3373)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1920 (0.2897)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (99.4709)  time: 0.2571 (0.1679 -- 1.0065)  data: 0.0677 (0.0001 -- 0.7845)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2438 (0.3608)  acc1: 100.0000 (91.7012)  acc5: 100.0000 (99.5851)  time: 0.2752 (0.1326 -- 1.0065)  data: 0.0936 (0.0001 -- 0.7845)  max mem: 16413
Val: Total time: 0:00:09 (0.3403 s / it)
* Acc@1 91.701 Acc@5 99.793 loss 0.366
Accuracy of the network on the 482 val images: 91.70%
Max accuracy: 92.74%
Epoch: [134]  [  0/160]  eta: 0:21:51  lr: 0.000012  min_lr: 0.000003  loss: 1.8952 (1.8952)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2876 (11.2876)  time: 8.1944 (8.1944 -- 8.1944)  data: 7.6440 (7.6440 -- 7.6440)  max mem: 16413
Epoch: [134]  [ 20/160]  eta: 0:03:09  lr: 0.000012  min_lr: 0.000003  loss: 1.2724 (1.3692)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0511 (6.2374)  time: 1.0138 (0.5055 -- 3.5478)  data: 0.4702 (0.0002 -- 3.0198)  max mem: 16413
Epoch: [134]  [ 40/160]  eta: 0:02:09  lr: 0.000012  min_lr: 0.000003  loss: 1.3872 (1.3690)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9996 (6.6610)  time: 0.7849 (0.5284 -- 2.1672)  data: 0.2026 (0.0001 -- 1.6458)  max mem: 16413
Epoch: [134]  [ 60/160]  eta: 0:01:44  lr: 0.000012  min_lr: 0.000003  loss: 1.2076 (1.3117)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2516 (6.4937)  time: 0.9753 (0.5237 -- 3.3673)  data: 0.3661 (0.0002 -- 2.8524)  max mem: 16413
Epoch: [134]  [ 80/160]  eta: 0:01:21  lr: 0.000012  min_lr: 0.000003  loss: 1.3535 (1.3259)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6087 (6.5316)  time: 0.9557 (0.5178 -- 3.5628)  data: 0.4147 (0.0004 -- 3.0246)  max mem: 16413
[2023-09-23 07:07:50,803] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:07:50,803] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:07:50,804] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:07:50,805] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:07:57,177] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21530
[2023-09-23 07:07:57,178] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:07:57,177] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21530
[2023-09-23 07:07:57,178] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 07:07:57,178] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [134]  [100/160]  eta: 0:00:59  lr: 0.000012  min_lr: 0.000003  loss: 1.2938 (1.3211)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3380 (6.5977)  time: 0.8827 (0.5234 -- 4.1986)  data: 0.3340 (0.0002 -- 3.6843)  max mem: 16413
Epoch: [134]  [120/160]  eta: 0:00:38  lr: 0.000012  min_lr: 0.000003  loss: 1.3834 (1.3322)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6905 (6.6916)  time: 0.8561 (0.5218 -- 3.2354)  data: 0.1595 (0.0002 -- 2.6942)  max mem: 16413
Epoch: [134]  [140/160]  eta: 0:00:19  lr: 0.000012  min_lr: 0.000003  loss: 1.4933 (1.3528)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2296 (6.7483)  time: 1.0018 (0.5197 -- 4.0371)  data: 0.0013 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.1974 (1.3437)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8194 (6.7725)  time: 0.6788 (0.4952 -- 3.7691)  data: 0.0006 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [134] Total time: 0:02:30 (0.9417 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.1974 (1.3708)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8194 (6.7725)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2666 (0.2666)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3791 (2.3791 -- 2.3791)  data: 2.1893 (2.1893 -- 2.1893)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1519 (0.4044)  acc1: 100.0000 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4770 (0.1772 -- 2.3791)  data: 0.2888 (0.0002 -- 2.1893)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1594 (0.3505)  acc1: 100.0000 (92.0635)  acc5: 100.0000 (100.0000)  time: 0.2630 (0.1710 -- 1.1685)  data: 0.0760 (0.0001 -- 0.9710)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2705 (0.4192)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (100.0000)  time: 0.2870 (0.1319 -- 1.1685)  data: 0.1070 (0.0001 -- 0.9710)  max mem: 16413
Val: Total time: 0:00:09 (0.3430 s / it)
* Acc@1 90.249 Acc@5 100.000 loss 0.402
Accuracy of the network on the 482 val images: 90.25%
Max accuracy: 92.74%
Epoch: [135]  [  0/160]  eta: 0:20:52  lr: 0.000012  min_lr: 0.000003  loss: 1.2174 (1.2174)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4704 (5.4704)  time: 7.8305 (7.8305 -- 7.8305)  data: 7.3135 (7.3135 -- 7.3135)  max mem: 16413
Epoch: [135]  [ 20/160]  eta: 0:02:50  lr: 0.000012  min_lr: 0.000003  loss: 1.2829 (1.3285)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5081 (6.9104)  time: 0.8857 (0.5118 -- 2.5710)  data: 0.2301 (0.0006 -- 2.0639)  max mem: 16413
Epoch: [135]  [ 40/160]  eta: 0:02:08  lr: 0.000012  min_lr: 0.000003  loss: 1.4393 (1.3576)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9396 (7.1544)  time: 0.9207 (0.5186 -- 3.4816)  data: 0.2792 (0.0006 -- 2.9765)  max mem: 16413
[2023-09-23 07:10:09,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:10:09,876] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:10:09,877] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:10:09,877] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [135]  [ 60/160]  eta: 0:01:46  lr: 0.000012  min_lr: 0.000003  loss: 1.2878 (1.3499)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5424 (6.9981)  time: 1.0588 (0.5147 -- 3.7940)  data: 0.4497 (0.0004 -- 3.2755)  max mem: 16413
Epoch: [135]  [ 80/160]  eta: 0:01:19  lr: 0.000012  min_lr: 0.000003  loss: 1.4159 (1.3666)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2843 (6.9364)  time: 0.7526 (0.5138 -- 2.6510)  data: 0.1829 (0.0004 -- 2.1177)  max mem: 16413
Epoch: [135]  [100/160]  eta: 0:00:58  lr: 0.000012  min_lr: 0.000003  loss: 1.4628 (1.3771)  loss_scale: 32768.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8884 (6.9663)  time: 0.9412 (0.5248 -- 2.6899)  data: 0.2120 (0.0004 -- 1.5342)  max mem: 16413
Epoch: [135]  [120/160]  eta: 0:00:39  lr: 0.000012  min_lr: 0.000003  loss: 1.4952 (1.4078)  loss_scale: 32768.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8090 (6.9583)  time: 0.9871 (0.5309 -- 2.7189)  data: 0.0645 (0.0004 -- 1.2632)  max mem: 16413
Epoch: [135]  [140/160]  eta: 0:00:19  lr: 0.000011  min_lr: 0.000003  loss: 1.3921 (1.3962)  loss_scale: 32768.0000 (25912.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8919 (6.9080)  time: 0.9420 (0.5192 -- 2.8943)  data: 0.2833 (0.0004 -- 2.3398)  max mem: 16413
[2023-09-23 07:11:29,562] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21746
[2023-09-23 07:11:29,562] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:11:29,562] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21746
[2023-09-23 07:11:29,563] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:11:29,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.2963 (1.4036)  loss_scale: 16384.0000 (25292.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7877 (6.8070)  time: 0.7549 (0.4927 -- 3.9864)  data: 0.2385 (0.0002 -- 3.4464)  max mem: 16413
Epoch: [135] Total time: 0:02:32 (0.9512 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.2963 (1.3945)  loss_scale: 16384.0000 (25292.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7877 (6.8070)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1300 (0.1300)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3183 (2.3183 -- 2.3183)  data: 2.1255 (2.1255 -- 2.1255)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1537 (0.3548)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4586 (0.1757 -- 2.3183)  data: 0.2720 (0.0003 -- 2.1255)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1594 (0.3244)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2582 (0.1685 -- 1.0702)  data: 0.0735 (0.0001 -- 0.8614)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3440 (0.3964)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (100.0000)  time: 0.2844 (0.1321 -- 1.0702)  data: 0.1058 (0.0001 -- 0.8614)  max mem: 16413
Val: Total time: 0:00:09 (0.3374 s / it)
* Acc@1 90.871 Acc@5 100.000 loss 0.355
Accuracy of the network on the 482 val images: 90.87%
Max accuracy: 92.74%
Epoch: [136]  [  0/160]  eta: 0:20:10  lr: 0.000011  min_lr: 0.000003  loss: 1.6173 (1.6173)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6088 (2.6088)  time: 7.5673 (7.5673 -- 7.5673)  data: 6.0872 (6.0872 -- 6.0872)  max mem: 16413
[2023-09-23 07:12:03,921] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21771
[2023-09-23 07:12:03,921] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21771
[2023-09-23 07:12:03,921] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:12:03,921] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:12:03,921] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [136]  [ 20/160]  eta: 0:02:49  lr: 0.000011  min_lr: 0.000003  loss: 1.4236 (1.4205)  loss_scale: 8192.0000 (12483.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6611 (7.7510)  time: 0.8943 (0.5180 -- 4.0347)  data: 0.2021 (0.0003 -- 2.1864)  max mem: 16413
Epoch: [136]  [ 40/160]  eta: 0:02:15  lr: 0.000011  min_lr: 0.000003  loss: 1.4741 (1.4274)  loss_scale: 8192.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0848 (7.5553)  time: 1.0394 (0.5198 -- 3.6315)  data: 0.1529 (0.0002 -- 1.8000)  max mem: 16413
Epoch: [136]  [ 60/160]  eta: 0:01:47  lr: 0.000011  min_lr: 0.000003  loss: 1.5969 (1.4614)  loss_scale: 8192.0000 (9669.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0640 (7.8048)  time: 0.9539 (0.5226 -- 3.5565)  data: 0.3444 (0.0004 -- 3.0358)  max mem: 16413
Epoch: [136]  [ 80/160]  eta: 0:01:20  lr: 0.000011  min_lr: 0.000003  loss: 1.3399 (1.4213)  loss_scale: 8192.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3683 (7.6910)  time: 0.8328 (0.5066 -- 2.9633)  data: 0.3039 (0.0003 -- 2.4367)  max mem: 16413
Epoch: [136]  [100/160]  eta: 0:01:00  lr: 0.000011  min_lr: 0.000003  loss: 1.3570 (1.4080)  loss_scale: 8192.0000 (9084.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2800 (7.5744)  time: 1.0307 (0.5249 -- 4.2954)  data: 0.4892 (0.0006 -- 3.7712)  max mem: 16413
Epoch: [136]  [120/160]  eta: 0:00:39  lr: 0.000011  min_lr: 0.000003  loss: 1.4803 (1.4159)  loss_scale: 8192.0000 (8936.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1105 (7.5376)  time: 0.8275 (0.5100 -- 3.5514)  data: 0.2881 (0.0002 -- 3.0327)  max mem: 16413
[2023-09-23 07:14:03,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:14:03,706] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 07:14:03,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:14:03,707] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [136]  [140/160]  eta: 0:00:19  lr: 0.000011  min_lr: 0.000003  loss: 1.4366 (1.4169)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2438 (7.4497)  time: 0.8821 (0.5033 -- 4.2681)  data: 0.3470 (0.0003 -- 3.7496)  max mem: 16413
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.4695 (1.4159)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1002 (7.3891)  time: 0.7004 (0.4930 -- 3.2409)  data: 0.1825 (0.0002 -- 2.7319)  max mem: 16413
Epoch: [136] Total time: 0:02:30 (0.9392 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.4695 (1.4075)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1002 (7.3891)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1140 (0.1140)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4743 (2.4743 -- 2.4743)  data: 2.2874 (2.2874 -- 2.2874)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1537 (0.3387)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4754 (0.1779 -- 2.4743)  data: 0.2774 (0.0002 -- 2.2874)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1646 (0.3239)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (99.4709)  time: 0.2521 (0.1698 -- 0.9319)  data: 0.0615 (0.0001 -- 0.7422)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2600 (0.3870)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (99.5851)  time: 0.2732 (0.1329 -- 0.9319)  data: 0.0951 (0.0001 -- 0.7422)  max mem: 16413
Val: Total time: 0:00:09 (0.3418 s / it)
* Acc@1 90.456 Acc@5 99.378 loss 0.397
Accuracy of the network on the 482 val images: 90.46%
Max accuracy: 92.74%
Epoch: [137]  [  0/160]  eta: 0:20:40  lr: 0.000011  min_lr: 0.000003  loss: 1.1361 (1.1361)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4075 (6.4075)  time: 7.7548 (7.7548 -- 7.7548)  data: 7.1920 (7.1920 -- 7.1920)  max mem: 16413
Epoch: [137]  [ 20/160]  eta: 0:02:49  lr: 0.000011  min_lr: 0.000003  loss: 1.4008 (1.4510)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1173 (6.0861)  time: 0.8814 (0.5155 -- 3.2360)  data: 0.1717 (0.0006 -- 2.6977)  max mem: 16413
Epoch: [137]  [ 40/160]  eta: 0:02:15  lr: 0.000011  min_lr: 0.000003  loss: 1.3604 (1.4358)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6036 (5.8718)  time: 1.0419 (0.5065 -- 4.9741)  data: 0.0012 (0.0003 -- 0.0034)  max mem: 16413
[2023-09-23 07:15:31,383] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21980
[2023-09-23 07:15:31,383] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:15:31,383] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21980
[2023-09-23 07:15:31,383] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-23 07:15:31,383] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [137]  [ 60/160]  eta: 0:01:46  lr: 0.000011  min_lr: 0.000003  loss: 1.3869 (1.4276)  loss_scale: 16384.0000 (16249.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7587 (6.1081)  time: 0.9287 (0.5213 -- 3.8557)  data: 0.0011 (0.0003 -- 0.0030)  max mem: 16413
[2023-09-23 07:15:47,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=128, lr=[2.7830670189612398e-06, 2.7830670189612398e-06, 3.092296687734711e-06, 3.092296687734711e-06, 3.4358852085941226e-06, 3.4358852085941226e-06, 3.8176502317712475e-06, 3.8176502317712475e-06, 4.241833590856941e-06, 4.241833590856941e-06, 4.713148434285491e-06, 4.713148434285491e-06, 5.236831593650545e-06, 5.236831593650545e-06, 5.8187017707228276e-06, 5.8187017707228276e-06, 6.46522418969203e-06, 6.46522418969203e-06, 7.1835824329911445e-06, 7.1835824329911445e-06, 7.98175825887905e-06, 7.98175825887905e-06, 8.868620287643388e-06, 8.868620287643388e-06, 9.854022541825986e-06, 9.854022541825986e-06, 1.0948913935362207e-05, 1.0948913935362207e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 07:15:47,229] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=17.22513937359987, CurrSamplesPerSec=22.300815882729818, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [137]  [ 80/160]  eta: 0:01:20  lr: 0.000011  min_lr: 0.000003  loss: 1.4463 (1.4268)  loss_scale: 8192.0000 (14260.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6122 (6.5805)  time: 0.8194 (0.5231 -- 3.8918)  data: 0.0019 (0.0003 -- 0.0067)  max mem: 16413
Epoch: [137]  [100/160]  eta: 0:00:59  lr: 0.000011  min_lr: 0.000003  loss: 1.4362 (1.4359)  loss_scale: 8192.0000 (13058.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7495 (6.7715)  time: 0.9333 (0.5326 -- 3.6205)  data: 0.0015 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [137]  [120/160]  eta: 0:00:39  lr: 0.000011  min_lr: 0.000003  loss: 1.3485 (1.4207)  loss_scale: 8192.0000 (12254.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6332 (6.8520)  time: 1.0477 (0.5151 -- 4.8754)  data: 0.0485 (0.0002 -- 0.7944)  max mem: 16413
Epoch: [137]  [140/160]  eta: 0:00:19  lr: 0.000011  min_lr: 0.000003  loss: 1.3987 (1.4152)  loss_scale: 8192.0000 (11677.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2379 (7.5479)  time: 0.8467 (0.5073 -- 3.1943)  data: 0.0009 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.4207 (1.4215)  loss_scale: 8192.0000 (11264.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9023 (7.4441)  time: 0.8946 (0.4915 -- 7.0493)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [137] Total time: 0:02:35 (0.9696 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.4207 (1.4149)  loss_scale: 8192.0000 (11264.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9023 (7.4441)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1216 (0.1216)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4552 (2.4552 -- 2.4552)  data: 2.2714 (2.2714 -- 2.2714)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1278 (0.3197)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (98.9899)  time: 0.4674 (0.1778 -- 2.4552)  data: 0.2771 (0.0002 -- 2.2714)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1517 (0.3053)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (99.4709)  time: 0.2538 (0.1679 -- 0.9947)  data: 0.0675 (0.0001 -- 0.7712)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2059 (0.3526)  acc1: 88.8889 (92.5311)  acc5: 100.0000 (99.5851)  time: 0.2738 (0.1324 -- 0.9947)  data: 0.0958 (0.0001 -- 0.7712)  max mem: 16413
Val: Total time: 0:00:09 (0.3364 s / it)
* Acc@1 92.531 Acc@5 99.378 loss 0.354
Accuracy of the network on the 482 val images: 92.53%
Max accuracy: 92.74%
Epoch: [138]  [  0/160]  eta: 0:22:39  lr: 0.000011  min_lr: 0.000003  loss: 1.4029 (1.4029)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8276 (6.8276)  time: 8.4978 (8.4978 -- 8.4978)  data: 6.6833 (6.6833 -- 6.6833)  max mem: 16413
Epoch: [138]  [ 20/160]  eta: 0:02:55  lr: 0.000011  min_lr: 0.000003  loss: 1.3728 (1.3796)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1985 (7.2713)  time: 0.8915 (0.5249 -- 3.5721)  data: 0.0663 (0.0007 -- 0.6227)  max mem: 16413
[2023-09-23 07:17:44,620] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:17:44,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 07:17:44,621] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:17:44,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [138]  [ 40/160]  eta: 0:02:04  lr: 0.000011  min_lr: 0.000003  loss: 1.3182 (1.3417)  loss_scale: 16384.0000 (10589.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8169 (6.9757)  time: 0.8188 (0.5230 -- 3.5106)  data: 0.2420 (0.0003 -- 2.9970)  max mem: 16413
Epoch: [138]  [ 60/160]  eta: 0:01:44  lr: 0.000011  min_lr: 0.000003  loss: 1.3888 (1.3610)  loss_scale: 16384.0000 (12489.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6030 (6.8933)  time: 1.0509 (0.5254 -- 3.0836)  data: 0.5137 (0.0004 -- 2.5646)  max mem: 16413
Epoch: [138]  [ 80/160]  eta: 0:01:20  lr: 0.000011  min_lr: 0.000003  loss: 1.2806 (1.3380)  loss_scale: 16384.0000 (13451.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5550 (6.7688)  time: 0.9124 (0.5230 -- 3.9317)  data: 0.2844 (0.0008 -- 2.1984)  max mem: 16413
Epoch: [138]  [100/160]  eta: 0:00:59  lr: 0.000011  min_lr: 0.000003  loss: 1.3668 (1.3503)  loss_scale: 16384.0000 (14031.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8431 (6.9368)  time: 0.8980 (0.5340 -- 2.7319)  data: 0.2422 (0.0004 -- 1.6604)  max mem: 16413
Epoch: [138]  [120/160]  eta: 0:00:38  lr: 0.000011  min_lr: 0.000003  loss: 1.1492 (1.3283)  loss_scale: 16384.0000 (14420.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9842 (6.9451)  time: 0.8937 (0.5130 -- 2.6144)  data: 0.3512 (0.0003 -- 2.0796)  max mem: 16413
Epoch: [138]  [140/160]  eta: 0:00:19  lr: 0.000011  min_lr: 0.000003  loss: 1.3496 (1.3257)  loss_scale: 16384.0000 (14699.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0213 (6.9718)  time: 0.9795 (0.5269 -- 3.4538)  data: 0.4379 (0.0008 -- 2.9083)  max mem: 16413
[2023-09-23 07:19:41,467] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:19:41,467] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:19:41,467] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:19:41,467] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.2913 (1.3219)  loss_scale: 16384.0000 (15206.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8472 (6.9275)  time: 0.7303 (0.4955 -- 3.3007)  data: 0.2085 (0.0001 -- 2.7135)  max mem: 16413
Epoch: [138] Total time: 0:02:31 (0.9469 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.2913 (1.3959)  loss_scale: 16384.0000 (15206.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8472 (6.9275)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1152 (0.1152)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5262 (2.5262 -- 2.5262)  data: 2.3390 (2.3390 -- 2.3390)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1584 (0.3905)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (98.9899)  time: 0.4842 (0.1759 -- 2.5262)  data: 0.2899 (0.0002 -- 2.3390)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1584 (0.3404)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (99.4709)  time: 0.2568 (0.1681 -- 1.0662)  data: 0.0682 (0.0001 -- 0.8441)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1628 (0.3769)  acc1: 100.0000 (92.5311)  acc5: 100.0000 (99.5851)  time: 0.2857 (0.1327 -- 1.0662)  data: 0.1053 (0.0001 -- 0.8441)  max mem: 16413
Val: Total time: 0:00:09 (0.3477 s / it)
* Acc@1 92.739 Acc@5 99.378 loss 0.372
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 92.74%
Epoch: [139]  [  0/160]  eta: 0:25:44  lr: 0.000010  min_lr: 0.000003  loss: 1.2508 (1.2508)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1212 (6.1212)  time: 9.6541 (9.6541 -- 9.6541)  data: 6.3993 (6.3993 -- 6.3993)  max mem: 16413
Epoch: [139]  [ 20/160]  eta: 0:03:04  lr: 0.000010  min_lr: 0.000003  loss: 1.5512 (1.4564)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0764 (6.2304)  time: 0.9012 (0.5247 -- 5.2203)  data: 0.0016 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [139]  [ 40/160]  eta: 0:02:14  lr: 0.000010  min_lr: 0.000003  loss: 1.6111 (1.5027)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7476 (6.7125)  time: 0.9182 (0.5119 -- 3.5635)  data: 0.1728 (0.0005 -- 2.5587)  max mem: 16413
Epoch: [139]  [ 60/160]  eta: 0:01:42  lr: 0.000010  min_lr: 0.000003  loss: 1.3047 (1.4610)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8883 (7.0702)  time: 0.8246 (0.5199 -- 3.4647)  data: 0.0827 (0.0005 -- 0.9086)  max mem: 16413
Epoch: [139]  [ 80/160]  eta: 0:01:20  lr: 0.000010  min_lr: 0.000003  loss: 1.3556 (1.4349)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5549 (7.0743)  time: 0.9284 (0.5236 -- 3.9741)  data: 0.2707 (0.0003 -- 2.6433)  max mem: 16413
Epoch: [139]  [100/160]  eta: 0:00:59  lr: 0.000010  min_lr: 0.000003  loss: 1.4645 (1.4544)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9121 (7.0101)  time: 0.9351 (0.5235 -- 4.5476)  data: 0.1797 (0.0003 -- 2.5338)  max mem: 16413
Epoch: [139]  [120/160]  eta: 0:00:39  lr: 0.000010  min_lr: 0.000003  loss: 1.3499 (1.4523)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2855 (6.9557)  time: 0.9754 (0.5185 -- 4.0186)  data: 0.0427 (0.0002 -- 0.7043)  max mem: 16413
[2023-09-23 07:21:54,010] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:21:54,010] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 07:21:54,011] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:21:54,012] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 07:21:56,433] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22366
[2023-09-23 07:21:56,433] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22366
[2023-09-23 07:21:56,433] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 07:21:56,433] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 07:21:56,433] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-23 07:22:06,124] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22376
[2023-09-23 07:22:06,124] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:22:06,124] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22376
[2023-09-23 07:22:06,125] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:22:06,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [139]  [140/160]  eta: 0:00:19  lr: 0.000010  min_lr: 0.000003  loss: 1.3778 (1.4500)  loss_scale: 32768.0000 (32419.4043)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6955 (6.9394)  time: 0.8582 (0.5155 -- 2.4060)  data: 0.1795 (0.0003 -- 1.8851)  max mem: 16413
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.3831 (1.4437)  loss_scale: 16384.0000 (30515.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5903 (6.9459)  time: 0.6984 (0.4927 -- 2.4728)  data: 0.0136 (0.0002 -- 0.2490)  max mem: 16413
Epoch: [139] Total time: 0:02:29 (0.9370 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.3831 (1.4232)  loss_scale: 16384.0000 (30515.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5903 (6.9459)
[2023-09-23 07:22:21,804] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-09-23 07:22:21,805] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-09-23 07:22:21,805] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-09-23 07:22:21,805] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-09-23 07:22:22,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-09-23 07:22:22,691] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1144 (0.1144)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3485 (2.3485 -- 2.3485)  data: 2.1557 (2.1557 -- 2.1557)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1314 (0.3583)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4626 (0.1808 -- 2.3485)  data: 0.2709 (0.0004 -- 2.1557)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1660 (0.3413)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2568 (0.1696 -- 1.0282)  data: 0.0700 (0.0001 -- 0.8180)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2692 (0.4068)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (100.0000)  time: 0.2898 (0.1322 -- 1.0282)  data: 0.1116 (0.0001 -- 0.8355)  max mem: 16413
Val: Total time: 0:00:09 (0.3443 s / it)
* Acc@1 92.116 Acc@5 99.585 loss 0.368
Accuracy of the network on the 482 val images: 92.12%
Max accuracy: 92.74%
Epoch: [140]  [  0/160]  eta: 0:21:37  lr: 0.000010  min_lr: 0.000003  loss: 1.7029 (1.7029)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7373 (4.7373)  time: 8.1075 (8.1075 -- 8.1075)  data: 7.5509 (7.5509 -- 7.5509)  max mem: 16413
Epoch: [140]  [ 20/160]  eta: 0:03:07  lr: 0.000010  min_lr: 0.000003  loss: 1.6473 (1.5650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7417 (7.1344)  time: 0.9973 (0.5077 -- 4.0717)  data: 0.4395 (0.0003 -- 3.5532)  max mem: 16413
Epoch: [140]  [ 40/160]  eta: 0:02:11  lr: 0.000010  min_lr: 0.000003  loss: 1.5732 (1.5373)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3559 (7.0751)  time: 0.8480 (0.5120 -- 5.0286)  data: 0.1118 (0.0001 -- 1.2983)  max mem: 16413
Epoch: [140]  [ 60/160]  eta: 0:01:46  lr: 0.000010  min_lr: 0.000003  loss: 1.6090 (1.5462)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7596 (6.9231)  time: 0.9981 (0.5241 -- 4.3687)  data: 0.0014 (0.0005 -- 0.0047)  max mem: 16413
Epoch: [140]  [ 80/160]  eta: 0:01:21  lr: 0.000010  min_lr: 0.000003  loss: 1.3486 (1.4998)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9936 (6.8784)  time: 0.8803 (0.5190 -- 4.8845)  data: 0.0264 (0.0003 -- 0.5044)  max mem: 16413
Epoch: [140]  [100/160]  eta: 0:00:59  lr: 0.000010  min_lr: 0.000003  loss: 1.3736 (1.4689)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9866 (6.9125)  time: 0.8912 (0.5143 -- 3.1238)  data: 0.1347 (0.0002 -- 2.4560)  max mem: 16413
[2023-09-23 07:24:19,114] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:24:19,114] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:24:19,115] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:24:19,115] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:24:27,694] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22515
[2023-09-23 07:24:27,694] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22515
[2023-09-23 07:24:27,694] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:24:27,694] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:24:27,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [140]  [120/160]  eta: 0:00:39  lr: 0.000010  min_lr: 0.000003  loss: 1.6019 (1.4943)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9731 (6.9330)  time: 0.8971 (0.5063 -- 4.2818)  data: 0.3575 (0.0003 -- 3.7481)  max mem: 16413
Epoch: [140]  [140/160]  eta: 0:00:19  lr: 0.000010  min_lr: 0.000003  loss: 1.3869 (1.4820)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9662 (6.9861)  time: 0.9470 (0.5209 -- 3.1776)  data: 0.3977 (0.0003 -- 2.4010)  max mem: 16413
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.5463 (1.4827)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5990 (6.9786)  time: 0.7688 (0.4949 -- 2.5177)  data: 0.1011 (0.0002 -- 1.9996)  max mem: 16413
Epoch: [140] Total time: 0:02:32 (0.9508 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.5463 (1.4375)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5990 (6.9786)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1451 (0.1451)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4836 (2.4836 -- 2.4836)  data: 2.2816 (2.2816 -- 2.2816)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1451 (0.3541)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4843 (0.1788 -- 2.4836)  data: 0.2941 (0.0002 -- 2.2816)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1596 (0.3193)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2567 (0.1682 -- 1.1563)  data: 0.0679 (0.0001 -- 0.9480)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2549 (0.3781)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2878 (0.1323 -- 1.1563)  data: 0.1065 (0.0001 -- 0.9480)  max mem: 16413
Val: Total time: 0:00:09 (0.3471 s / it)
* Acc@1 93.361 Acc@5 99.585 loss 0.339
Accuracy of the network on the 482 val images: 93.36%
[2023-09-23 07:25:13,626] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 07:25:13,628] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 07:25:13,628] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 07:25:13,628] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 07:25:14,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 07:25:14,998] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 93.36%
Epoch: [141]  [  0/160]  eta: 0:17:36  lr: 0.000010  min_lr: 0.000003  loss: 1.2374 (1.2374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7376 (4.7376)  time: 6.6059 (6.6059 -- 6.6059)  data: 5.6260 (5.6260 -- 5.6260)  max mem: 16413
Epoch: [141]  [ 20/160]  eta: 0:02:56  lr: 0.000010  min_lr: 0.000002  loss: 1.4498 (1.4129)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9064 (7.1453)  time: 0.9905 (0.5143 -- 4.8658)  data: 0.0162 (0.0003 -- 0.2601)  max mem: 16413
Epoch: [141]  [ 40/160]  eta: 0:02:05  lr: 0.000010  min_lr: 0.000002  loss: 1.4471 (1.3748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1006 (7.0402)  time: 0.8291 (0.5212 -- 3.2952)  data: 0.0163 (0.0003 -- 0.3001)  max mem: 16413
Epoch: [141]  [ 60/160]  eta: 0:01:37  lr: 0.000010  min_lr: 0.000002  loss: 1.3938 (1.3638)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8298 (7.0061)  time: 0.8286 (0.5251 -- 1.8666)  data: 0.1892 (0.0005 -- 1.3282)  max mem: 16413
[2023-09-23 07:26:30,263] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22637
[2023-09-23 07:26:30,264] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:26:30,263] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22637
[2023-09-23 07:26:30,264] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:26:30,264] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [141]  [ 80/160]  eta: 0:01:19  lr: 0.000010  min_lr: 0.000002  loss: 1.4506 (1.4017)  loss_scale: 16384.0000 (15979.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1612 (7.2775)  time: 1.0471 (0.5143 -- 4.1804)  data: 0.0737 (0.0003 -- 0.6677)  max mem: 16413
Epoch: [141]  [100/160]  eta: 0:00:57  lr: 0.000010  min_lr: 0.000002  loss: 1.5487 (1.4185)  loss_scale: 8192.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2594 (7.0574)  time: 0.8509 (0.5242 -- 3.3295)  data: 0.1358 (0.0009 -- 1.5221)  max mem: 16413
Epoch: [141]  [120/160]  eta: 0:00:38  lr: 0.000010  min_lr: 0.000002  loss: 1.3024 (1.4112)  loss_scale: 8192.0000 (13405.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3221 (7.0853)  time: 0.9638 (0.5139 -- 3.6798)  data: 0.2898 (0.0002 -- 3.1629)  max mem: 16413
Epoch: [141]  [140/160]  eta: 0:00:19  lr: 0.000010  min_lr: 0.000002  loss: 1.1668 (1.3796)  loss_scale: 8192.0000 (12665.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2222 (7.1580)  time: 0.9247 (0.5250 -- 3.5348)  data: 0.3442 (0.0005 -- 3.0150)  max mem: 16413
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000002  loss: 1.4698 (1.3850)  loss_scale: 8192.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1687 (7.1155)  time: 0.7121 (0.4946 -- 3.5204)  data: 0.1779 (0.0001 -- 2.9765)  max mem: 16413
Epoch: [141] Total time: 0:02:29 (0.9315 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000002  loss: 1.4698 (1.3702)  loss_scale: 8192.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1687 (7.1155)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1219 (0.1219)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5041 (2.5041 -- 2.5041)  data: 2.3054 (2.3054 -- 2.3054)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1239 (0.2951)  acc1: 100.0000 (94.9495)  acc5: 100.0000 (100.0000)  time: 0.4775 (0.1782 -- 2.5041)  data: 0.2857 (0.0002 -- 2.3054)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1447 (0.2857)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (100.0000)  time: 0.2525 (0.1693 -- 1.0272)  data: 0.0644 (0.0001 -- 0.8301)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1697 (0.3595)  acc1: 100.0000 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2785 (0.1325 -- 1.0272)  data: 0.0987 (0.0001 -- 0.8301)  max mem: 16413
Val: Total time: 0:00:09 (0.3416 s / it)
* Acc@1 92.739 Acc@5 99.585 loss 0.346
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 93.36%
Epoch: [142]  [  0/160]  eta: 0:20:14  lr: 0.000010  min_lr: 0.000002  loss: 1.4664 (1.4664)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6142 (5.6142)  time: 7.5906 (7.5906 -- 7.5906)  data: 7.0458 (7.0458 -- 7.0458)  max mem: 16413
Epoch: [142]  [ 20/160]  eta: 0:02:52  lr: 0.000010  min_lr: 0.000002  loss: 1.3713 (1.4251)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2165 (7.0408)  time: 0.9164 (0.5212 -- 4.2409)  data: 0.3703 (0.0006 -- 3.7262)  max mem: 16413
Epoch: [142]  [ 40/160]  eta: 0:02:13  lr: 0.000009  min_lr: 0.000002  loss: 1.5205 (1.4428)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1704 (7.5155)  time: 0.9931 (0.5281 -- 4.0694)  data: 0.4463 (0.0005 -- 3.4910)  max mem: 16413
[2023-09-23 07:28:43,272] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:28:43,272] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 07:28:43,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:28:43,273] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [142]  [ 60/160]  eta: 0:01:44  lr: 0.000009  min_lr: 0.000002  loss: 1.3575 (1.4166)  loss_scale: 16384.0000 (10206.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2694 (7.2111)  time: 0.8929 (0.5260 -- 4.5937)  data: 0.3420 (0.0002 -- 4.0617)  max mem: 16413
Epoch: [142]  [ 80/160]  eta: 0:01:21  lr: 0.000009  min_lr: 0.000002  loss: 1.3905 (1.4103)  loss_scale: 16384.0000 (11731.7531)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7732 (7.2709)  time: 0.9236 (0.5021 -- 2.7021)  data: 0.2758 (0.0003 -- 1.9154)  max mem: 16413
Epoch: [142]  [100/160]  eta: 0:01:00  lr: 0.000009  min_lr: 0.000002  loss: 1.5406 (1.4336)  loss_scale: 16384.0000 (12652.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6184 (7.2700)  time: 0.9471 (0.5161 -- 3.0774)  data: 0.2867 (0.0004 -- 2.5696)  max mem: 16413
[2023-09-23 07:29:34,909] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22821
[2023-09-23 07:29:34,909] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22821
[2023-09-23 07:29:34,909] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:29:34,909] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:29:34,909] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [142]  [120/160]  eta: 0:00:39  lr: 0.000009  min_lr: 0.000002  loss: 1.2125 (1.3951)  loss_scale: 8192.0000 (11915.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8005 (7.2228)  time: 0.8471 (0.5085 -- 3.7466)  data: 0.0170 (0.0002 -- 0.3213)  max mem: 16413
Epoch: [142]  [140/160]  eta: 0:00:19  lr: 0.000009  min_lr: 0.000002  loss: 1.4357 (1.4031)  loss_scale: 8192.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6180 (7.2988)  time: 0.9843 (0.5199 -- 3.9402)  data: 0.0312 (0.0002 -- 0.5827)  max mem: 16413
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.1486 (1.3840)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2748 (7.2577)  time: 0.7958 (0.4939 -- 3.4261)  data: 0.0006 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [142] Total time: 0:02:33 (0.9566 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.1486 (1.3812)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2748 (7.2577)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3843 (0.3843)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4244 (2.4244 -- 2.4244)  data: 2.2341 (2.2341 -- 2.2341)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.2229 (0.3703)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4763 (0.1763 -- 2.4244)  data: 0.2851 (0.0003 -- 2.2341)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1738 (0.3229)  acc1: 88.8889 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2585 (0.1699 -- 1.0901)  data: 0.0703 (0.0001 -- 0.8941)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2373 (0.3712)  acc1: 100.0000 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2870 (0.1328 -- 1.0901)  data: 0.1075 (0.0001 -- 0.8941)  max mem: 16413
Val: Total time: 0:00:09 (0.3457 s / it)
* Acc@1 92.324 Acc@5 99.585 loss 0.366
Accuracy of the network on the 482 val images: 92.32%
Max accuracy: 93.36%
Epoch: [143]  [  0/160]  eta: 0:18:29  lr: 0.000009  min_lr: 0.000002  loss: 1.7126 (1.7126)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1429 (5.1429)  time: 6.9366 (6.9366 -- 6.9366)  data: 6.3939 (6.3939 -- 6.3939)  max mem: 16413
Epoch: [143]  [ 20/160]  eta: 0:03:04  lr: 0.000009  min_lr: 0.000002  loss: 1.4339 (1.4054)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4592 (6.5414)  time: 1.0341 (0.5196 -- 4.1944)  data: 0.2077 (0.0008 -- 1.1820)  max mem: 16413
Epoch: [143]  [ 40/160]  eta: 0:02:04  lr: 0.000009  min_lr: 0.000002  loss: 1.4802 (1.4265)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3958 (6.6597)  time: 0.7437 (0.5261 -- 2.0293)  data: 0.0025 (0.0001 -- 0.0103)  max mem: 16413
Epoch: [143]  [ 60/160]  eta: 0:01:44  lr: 0.000009  min_lr: 0.000002  loss: 1.3277 (1.3930)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7235 (6.6499)  time: 1.0646 (0.5159 -- 5.8488)  data: 0.0091 (0.0004 -- 0.1583)  max mem: 16413
[2023-09-23 07:31:48,314] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:31:48,314] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 07:31:48,315] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:31:48,316] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [143]  [ 80/160]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000002  loss: 1.3551 (1.3940)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9852 (6.5623)  time: 0.7832 (0.5314 -- 3.5188)  data: 0.0014 (0.0003 -- 0.0052)  max mem: 16413
Epoch: [143]  [100/160]  eta: 0:00:59  lr: 0.000009  min_lr: 0.000002  loss: 1.2699 (1.3742)  loss_scale: 16384.0000 (10706.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3417 (6.8401)  time: 1.0154 (0.5168 -- 4.6943)  data: 0.1701 (0.0003 -- 2.5606)  max mem: 16413
[2023-09-23 07:32:32,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=133, lr=[2.293499919103678e-06, 2.293499919103678e-06, 2.5483332434485317e-06, 2.5483332434485317e-06, 2.8314813816094788e-06, 2.8314813816094788e-06, 3.1460904240105323e-06, 3.1460904240105323e-06, 3.495656026678369e-06, 3.495656026678369e-06, 3.884062251864855e-06, 3.884062251864855e-06, 4.315624724294282e-06, 4.315624724294282e-06, 4.795138582549202e-06, 4.795138582549202e-06, 5.3279317583880026e-06, 5.3279317583880026e-06, 5.91992417598667e-06, 5.91992417598667e-06, 6.577693528874078e-06, 6.577693528874078e-06, 7.308548365415641e-06, 7.308548365415641e-06, 8.120609294906268e-06, 8.120609294906268e-06, 9.02289921656252e-06, 9.02289921656252e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 07:32:32,969] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=17.19470723342146, CurrSamplesPerSec=23.479929819094302, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [143]  [120/160]  eta: 0:00:38  lr: 0.000009  min_lr: 0.000002  loss: 1.4963 (1.3882)  loss_scale: 16384.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8523 (7.0219)  time: 0.8842 (0.5237 -- 4.2995)  data: 0.1640 (0.0004 -- 1.9658)  max mem: 16413
Epoch: [143]  [140/160]  eta: 0:00:19  lr: 0.000009  min_lr: 0.000002  loss: 1.4848 (1.3983)  loss_scale: 16384.0000 (12317.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4995 (6.9624)  time: 1.0102 (0.4953 -- 6.8774)  data: 0.0817 (0.0003 -- 1.6134)  max mem: 16413
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.5037 (1.3984)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9274 (7.0282)  time: 0.7003 (0.4926 -- 3.6714)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [143] Total time: 0:02:31 (0.9450 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.5037 (1.3950)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9274 (7.0282)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1511 (0.1511)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3528 (2.3528 -- 2.3528)  data: 2.1657 (2.1657 -- 2.1657)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1384 (0.3284)  acc1: 100.0000 (94.9495)  acc5: 100.0000 (100.0000)  time: 0.4665 (0.1818 -- 2.3528)  data: 0.2791 (0.0004 -- 2.1657)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2235 (0.3492)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2559 (0.1682 -- 1.0963)  data: 0.0696 (0.0001 -- 0.8948)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2614 (0.4209)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (100.0000)  time: 0.2837 (0.1328 -- 1.0963)  data: 0.1047 (0.0001 -- 0.8948)  max mem: 16413
Val: Total time: 0:00:09 (0.3395 s / it)
* Acc@1 91.494 Acc@5 99.585 loss 0.395
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 93.36%
Epoch: [144]  [  0/160]  eta: 0:22:18  lr: 0.000009  min_lr: 0.000002  loss: 0.6388 (0.6388)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8143 (8.8143)  time: 8.3682 (8.3682 -- 8.3682)  data: 7.8273 (7.8273 -- 7.8273)  max mem: 16413
Epoch: [144]  [ 20/160]  eta: 0:02:53  lr: 0.000009  min_lr: 0.000002  loss: 1.2715 (1.2724)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8872 (7.4495)  time: 0.8805 (0.5120 -- 4.0250)  data: 0.3407 (0.0006 -- 3.4879)  max mem: 16413
[2023-09-23 07:33:57,812] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:33:57,812] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:33:57,812] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:33:57,812] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [144]  [ 40/160]  eta: 0:02:15  lr: 0.000009  min_lr: 0.000002  loss: 1.2407 (1.2914)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9183 (7.8857)  time: 1.0175 (0.5250 -- 4.5980)  data: 0.4720 (0.0005 -- 4.0782)  max mem: 16413
Epoch: [144]  [ 60/160]  eta: 0:01:41  lr: 0.000009  min_lr: 0.000002  loss: 1.4958 (1.3431)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7312 (7.6413)  time: 0.7743 (0.5195 -- 3.1918)  data: 0.2247 (0.0004 -- 2.6485)  max mem: 16413
Epoch: [144]  [ 80/160]  eta: 0:01:20  lr: 0.000009  min_lr: 0.000002  loss: 1.4673 (1.3567)  loss_scale: 32768.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4483 (7.5389)  time: 0.9936 (0.5217 -- 4.5812)  data: 0.1365 (0.0004 -- 1.4575)  max mem: 16413
Epoch: [144]  [100/160]  eta: 0:01:02  lr: 0.000009  min_lr: 0.000002  loss: 1.3775 (1.3669)  loss_scale: 32768.0000 (26603.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6530 (7.3952)  time: 1.1473 (0.5148 -- 5.4648)  data: 0.0013 (0.0003 -- 0.0032)  max mem: 16413
[2023-09-23 07:35:09,118] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23151
[2023-09-23 07:35:09,118] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23151
[2023-09-23 07:35:09,118] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:35:09,118] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:35:09,118] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [144]  [120/160]  eta: 0:00:39  lr: 0.000009  min_lr: 0.000002  loss: 1.4357 (1.3818)  loss_scale: 16384.0000 (26268.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7199 (7.3509)  time: 0.7387 (0.5064 -- 2.5481)  data: 0.0013 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [144]  [140/160]  eta: 0:00:19  lr: 0.000009  min_lr: 0.000002  loss: 1.3435 (1.3819)  loss_scale: 16384.0000 (24866.4965)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5704 (7.2891)  time: 1.0140 (0.5198 -- 5.5407)  data: 0.0011 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.2179 (1.3721)  loss_scale: 16384.0000 (23859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0158 (7.2587)  time: 0.8528 (0.4913 -- 4.6978)  data: 0.0006 (0.0001 -- 0.0019)  max mem: 16413
Epoch: [144] Total time: 0:02:36 (0.9765 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.2179 (1.3762)  loss_scale: 16384.0000 (23859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0158 (7.2587)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1580 (0.1580)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3574 (2.3574 -- 2.3574)  data: 2.1726 (2.1726 -- 2.1726)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1450 (0.3504)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4716 (0.1756 -- 2.3574)  data: 0.2845 (0.0003 -- 2.1726)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1776 (0.3422)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2631 (0.1685 -- 1.1462)  data: 0.0769 (0.0001 -- 0.9512)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2209 (0.3873)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (100.0000)  time: 0.2792 (0.1319 -- 1.1462)  data: 0.1006 (0.0001 -- 0.9512)  max mem: 16413
Val: Total time: 0:00:09 (0.3360 s / it)
* Acc@1 91.909 Acc@5 99.793 loss 0.373
Accuracy of the network on the 482 val images: 91.91%
Max accuracy: 93.36%
Epoch: [145]  [  0/160]  eta: 0:23:25  lr: 0.000009  min_lr: 0.000002  loss: 1.3720 (1.3720)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3511 (6.3511)  time: 8.7874 (8.7874 -- 8.7874)  data: 5.6602 (5.6602 -- 5.6602)  max mem: 16413
Epoch: [145]  [ 20/160]  eta: 0:02:56  lr: 0.000009  min_lr: 0.000002  loss: 1.4429 (1.4105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9895 (6.2306)  time: 0.8826 (0.5180 -- 4.3026)  data: 0.1514 (0.0003 -- 2.0613)  max mem: 16413
Epoch: [145]  [ 40/160]  eta: 0:02:15  lr: 0.000009  min_lr: 0.000002  loss: 1.4506 (1.4205)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4811 (6.4196)  time: 0.9899 (0.5155 -- 4.3805)  data: 0.0140 (0.0003 -- 0.2497)  max mem: 16413
[2023-09-23 07:37:06,803] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23260
[2023-09-23 07:37:06,803] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23260
[2023-09-23 07:37:06,803] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:37:06,803] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:37:06,803] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [145]  [ 60/160]  eta: 0:01:45  lr: 0.000009  min_lr: 0.000002  loss: 1.2597 (1.3955)  loss_scale: 16384.0000 (16249.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3286 (6.5150)  time: 0.9157 (0.5169 -- 3.3353)  data: 0.2796 (0.0003 -- 2.8128)  max mem: 16413
Epoch: [145]  [ 80/160]  eta: 0:01:20  lr: 0.000009  min_lr: 0.000002  loss: 1.5299 (1.4275)  loss_scale: 8192.0000 (14260.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9130 (6.7714)  time: 0.8446 (0.5082 -- 3.3392)  data: 0.2611 (0.0002 -- 2.8192)  max mem: 16413
Epoch: [145]  [100/160]  eta: 0:01:00  lr: 0.000008  min_lr: 0.000002  loss: 1.2562 (1.3959)  loss_scale: 8192.0000 (13058.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4436 (6.7445)  time: 1.0380 (0.5041 -- 4.6677)  data: 0.3877 (0.0003 -- 4.1555)  max mem: 16413
Epoch: [145]  [120/160]  eta: 0:00:39  lr: 0.000008  min_lr: 0.000002  loss: 1.3760 (1.4093)  loss_scale: 8192.0000 (12254.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4109 (6.8977)  time: 0.8935 (0.5327 -- 4.2029)  data: 0.3315 (0.0006 -- 3.6892)  max mem: 16413
Epoch: [145]  [140/160]  eta: 0:00:19  lr: 0.000008  min_lr: 0.000002  loss: 1.4563 (1.4081)  loss_scale: 8192.0000 (11677.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8735 (6.9078)  time: 1.0141 (0.5192 -- 3.6816)  data: 0.1695 (0.0003 -- 1.7579)  max mem: 16413
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.3151 (1.3936)  loss_scale: 8192.0000 (11264.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9238 (6.9637)  time: 0.6344 (0.4925 -- 2.5481)  data: 0.1019 (0.0001 -- 2.0267)  max mem: 16413
Epoch: [145] Total time: 0:02:32 (0.9534 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.3151 (1.4021)  loss_scale: 8192.0000 (11264.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9238 (6.9637)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1241 (0.1241)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4169 (2.4169 -- 2.4169)  data: 2.2081 (2.2081 -- 2.2081)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1468 (0.4248)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4660 (0.1755 -- 2.4169)  data: 0.2757 (0.0004 -- 2.2081)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1499 (0.3420)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (99.4709)  time: 0.2564 (0.1687 -- 1.0161)  data: 0.0682 (0.0001 -- 0.8183)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1798 (0.3842)  acc1: 100.0000 (90.8714)  acc5: 100.0000 (99.5851)  time: 0.2834 (0.1326 -- 1.0161)  data: 0.1027 (0.0001 -- 0.8183)  max mem: 16413
Val: Total time: 0:00:09 (0.3418 s / it)
* Acc@1 92.531 Acc@5 99.170 loss 0.350
Accuracy of the network on the 482 val images: 92.53%
Max accuracy: 93.36%
Epoch: [146]  [  0/160]  eta: 0:21:15  lr: 0.000008  min_lr: 0.000002  loss: 1.6798 (1.6798)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9552 (4.9552)  time: 7.9688 (7.9688 -- 7.9688)  data: 5.3592 (5.3592 -- 5.3592)  max mem: 16413
Epoch: [146]  [ 20/160]  eta: 0:02:41  lr: 0.000008  min_lr: 0.000002  loss: 1.3025 (1.3650)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4888 (6.1656)  time: 0.8122 (0.5234 -- 3.1133)  data: 0.2600 (0.0003 -- 2.5864)  max mem: 16413
[2023-09-23 07:39:18,041] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:39:18,041] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 07:39:18,042] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:39:18,042] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [146]  [ 40/160]  eta: 0:02:12  lr: 0.000008  min_lr: 0.000002  loss: 1.2081 (1.3338)  loss_scale: 16384.0000 (10589.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9147 (6.9627)  time: 1.0568 (0.5127 -- 5.4098)  data: 0.4590 (0.0004 -- 4.8520)  max mem: 16413
Epoch: [146]  [ 60/160]  eta: 0:01:42  lr: 0.000008  min_lr: 0.000002  loss: 1.2494 (1.3275)  loss_scale: 16384.0000 (12489.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7611 (6.8866)  time: 0.8609 (0.5183 -- 2.9020)  data: 0.2673 (0.0003 -- 2.3756)  max mem: 16413
[2023-09-23 07:39:58,368] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23433
[2023-09-23 07:39:58,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:39:58,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23433
[2023-09-23 07:39:58,369] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-23 07:39:58,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [146]  [ 80/160]  eta: 0:01:20  lr: 0.000008  min_lr: 0.000002  loss: 1.3117 (1.3192)  loss_scale: 16384.0000 (12641.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9921 (6.9125)  time: 0.9413 (0.5087 -- 2.7569)  data: 0.4050 (0.0004 -- 2.2239)  max mem: 16413
Epoch: [146]  [100/160]  eta: 0:00:59  lr: 0.000008  min_lr: 0.000002  loss: 1.5304 (1.3414)  loss_scale: 8192.0000 (11760.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2601 (6.9095)  time: 0.9571 (0.5188 -- 2.8496)  data: 0.4117 (0.0004 -- 2.3233)  max mem: 16413
Epoch: [146]  [120/160]  eta: 0:00:39  lr: 0.000008  min_lr: 0.000002  loss: 1.4493 (1.3614)  loss_scale: 8192.0000 (11170.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9897 (6.9030)  time: 0.9815 (0.5073 -- 3.4828)  data: 0.4427 (0.0004 -- 2.9710)  max mem: 16413
Epoch: [146]  [140/160]  eta: 0:00:19  lr: 0.000008  min_lr: 0.000002  loss: 1.0634 (1.3403)  loss_scale: 8192.0000 (10748.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7855 (6.7975)  time: 0.9636 (0.5174 -- 3.7028)  data: 0.4237 (0.0005 -- 3.2020)  max mem: 16413
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.4160 (1.3562)  loss_scale: 8192.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2745 (6.8949)  time: 0.6036 (0.4927 -- 1.3097)  data: 0.0450 (0.0002 -- 0.6938)  max mem: 16413
Epoch: [146] Total time: 0:02:31 (0.9439 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.4160 (1.3736)  loss_scale: 8192.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2745 (6.8949)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1875 (0.1875)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4258 (2.4258 -- 2.4258)  data: 2.2506 (2.2506 -- 2.2506)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1569 (0.3639)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (98.9899)  time: 0.4823 (0.1769 -- 2.4258)  data: 0.2926 (0.0002 -- 2.2506)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1569 (0.3224)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (99.4709)  time: 0.2601 (0.1675 -- 1.1660)  data: 0.0729 (0.0001 -- 0.9477)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2335 (0.3889)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (99.5851)  time: 0.2822 (0.1318 -- 1.1660)  data: 0.1027 (0.0001 -- 0.9477)  max mem: 16413
Val: Total time: 0:00:09 (0.3414 s / it)
* Acc@1 91.909 Acc@5 99.793 loss 0.347
Accuracy of the network on the 482 val images: 91.91%
Max accuracy: 93.36%
Epoch: [147]  [  0/160]  eta: 0:20:20  lr: 0.000008  min_lr: 0.000002  loss: 0.8580 (0.8580)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6119 (7.6119)  time: 7.6269 (7.6269 -- 7.6269)  data: 7.0499 (7.0499 -- 7.0499)  max mem: 16413
Epoch: [147]  [ 20/160]  eta: 0:02:43  lr: 0.000008  min_lr: 0.000002  loss: 1.4234 (1.3584)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9296 (8.1776)  time: 0.8414 (0.5147 -- 3.4679)  data: 0.2942 (0.0006 -- 2.9489)  max mem: 16413
Epoch: [147]  [ 40/160]  eta: 0:02:09  lr: 0.000008  min_lr: 0.000002  loss: 1.4304 (1.4151)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1711 (7.8903)  time: 0.9942 (0.5223 -- 3.0258)  data: 0.4016 (0.0002 -- 2.5146)  max mem: 16413
[2023-09-23 07:42:10,314] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:42:10,314] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 07:42:10,315] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:42:10,315] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [147]  [ 60/160]  eta: 0:01:42  lr: 0.000008  min_lr: 0.000002  loss: 1.4240 (1.4153)  loss_scale: 16384.0000 (10743.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4423 (7.6947)  time: 0.9204 (0.5201 -- 3.9262)  data: 0.3806 (0.0002 -- 3.3887)  max mem: 16413
Epoch: [147]  [ 80/160]  eta: 0:01:19  lr: 0.000008  min_lr: 0.000002  loss: 1.3645 (1.4101)  loss_scale: 16384.0000 (12136.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0810 (7.4691)  time: 0.8893 (0.5080 -- 2.5112)  data: 0.3480 (0.0001 -- 1.9936)  max mem: 16413
Epoch: [147]  [100/160]  eta: 0:00:59  lr: 0.000008  min_lr: 0.000002  loss: 1.5943 (1.4461)  loss_scale: 16384.0000 (12977.4257)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7243 (7.5606)  time: 0.9768 (0.5153 -- 3.6113)  data: 0.4341 (0.0003 -- 3.1024)  max mem: 16413
Epoch: [147]  [120/160]  eta: 0:00:38  lr: 0.000008  min_lr: 0.000002  loss: 1.4539 (1.4380)  loss_scale: 16384.0000 (13540.4959)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8894 (7.5298)  time: 0.8415 (0.5233 -- 3.3764)  data: 0.3046 (0.0004 -- 2.8645)  max mem: 16413
Epoch: [147]  [140/160]  eta: 0:00:19  lr: 0.000008  min_lr: 0.000002  loss: 1.3550 (1.4286)  loss_scale: 16384.0000 (13943.8298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5671 (7.4725)  time: 0.8733 (0.5284 -- 2.1599)  data: 0.3231 (0.0005 -- 1.6253)  max mem: 16413
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.5072 (1.4431)  loss_scale: 16384.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0824 (7.4980)  time: 0.7364 (0.4947 -- 3.4639)  data: 0.2115 (0.0002 -- 2.9462)  max mem: 16413
Epoch: [147] Total time: 0:02:28 (0.9284 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.5072 (1.4196)  loss_scale: 16384.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0824 (7.4980)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1393 (0.1393)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4119 (2.4119 -- 2.4119)  data: 2.1925 (2.1925 -- 2.1925)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1412 (0.3362)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4691 (0.1747 -- 2.4119)  data: 0.2796 (0.0002 -- 2.1925)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1412 (0.2997)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2580 (0.1685 -- 1.0708)  data: 0.0719 (0.0001 -- 0.8787)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1851 (0.3647)  acc1: 100.0000 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2829 (0.1325 -- 1.0708)  data: 0.1034 (0.0001 -- 0.8787)  max mem: 16413
Val: Total time: 0:00:09 (0.3403 s / it)
* Acc@1 92.739 Acc@5 99.793 loss 0.357
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 93.36%
Epoch: [148]  [  0/160]  eta: 0:21:06  lr: 0.000008  min_lr: 0.000002  loss: 1.0117 (1.0117)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.7714 (17.7714)  time: 7.9168 (7.9168 -- 7.9168)  data: 7.3706 (7.3706 -- 7.3706)  max mem: 16413
[2023-09-23 07:44:19,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:44:19,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:44:19,681] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:44:19,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [148]  [ 20/160]  eta: 0:02:54  lr: 0.000008  min_lr: 0.000002  loss: 1.2555 (1.2728)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8133 (6.7301)  time: 0.9128 (0.5164 -- 2.6049)  data: 0.3538 (0.0001 -- 2.0841)  max mem: 16413
[2023-09-23 07:44:45,644] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23720
[2023-09-23 07:44:45,644] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23720
[2023-09-23 07:44:45,644] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:44:45,644] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:44:45,645] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [148]  [ 40/160]  eta: 0:02:06  lr: 0.000008  min_lr: 0.000002  loss: 1.2779 (1.2885)  loss_scale: 32768.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0571 (6.8566)  time: 0.8494 (0.5098 -- 2.5592)  data: 0.2297 (0.0006 -- 2.0473)  max mem: 16413
Epoch: [148]  [ 60/160]  eta: 0:01:41  lr: 0.000008  min_lr: 0.000002  loss: 1.3549 (1.3238)  loss_scale: 16384.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4907 (6.8348)  time: 0.9499 (0.5156 -- 3.5994)  data: 0.3677 (0.0006 -- 3.0502)  max mem: 16413
Epoch: [148]  [ 80/160]  eta: 0:01:19  lr: 0.000008  min_lr: 0.000002  loss: 1.2402 (1.3119)  loss_scale: 16384.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7262 (6.7362)  time: 0.9293 (0.5187 -- 4.1531)  data: 0.3429 (0.0002 -- 3.6391)  max mem: 16413
Epoch: [148]  [100/160]  eta: 0:01:00  lr: 0.000008  min_lr: 0.000002  loss: 1.3267 (1.3181)  loss_scale: 16384.0000 (21250.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8894 (6.8014)  time: 1.0135 (0.5203 -- 4.4784)  data: 0.0014 (0.0003 -- 0.0081)  max mem: 16413
Epoch: [148]  [120/160]  eta: 0:00:38  lr: 0.000008  min_lr: 0.000002  loss: 1.2885 (1.3218)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4703 (6.8672)  time: 0.8001 (0.5241 -- 3.1115)  data: 0.0014 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [148]  [140/160]  eta: 0:00:19  lr: 0.000008  min_lr: 0.000002  loss: 1.3358 (1.3260)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4585 (6.8481)  time: 0.9770 (0.5119 -- 2.1628)  data: 0.1383 (0.0004 -- 1.4282)  max mem: 16413
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.4268 (1.3431)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7493 (6.8590)  time: 0.6779 (0.4940 -- 1.8864)  data: 0.0044 (0.0001 -- 0.0722)  max mem: 16413
Epoch: [148] Total time: 0:02:29 (0.9351 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.4268 (1.3662)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7493 (6.8590)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1213 (0.1213)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4498 (2.4498 -- 2.4498)  data: 2.2340 (2.2340 -- 2.2340)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1444 (0.3778)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4744 (0.1838 -- 2.4498)  data: 0.2790 (0.0002 -- 2.2340)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1603 (0.3638)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2557 (0.1687 -- 1.0364)  data: 0.0651 (0.0001 -- 0.8294)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3171 (0.4101)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (100.0000)  time: 0.2829 (0.1326 -- 1.0364)  data: 0.1000 (0.0001 -- 0.8294)  max mem: 16413
Val: Total time: 0:00:09 (0.3428 s / it)
* Acc@1 91.701 Acc@5 99.793 loss 0.387
Accuracy of the network on the 482 val images: 91.70%
Max accuracy: 93.36%
Epoch: [149]  [  0/160]  eta: 0:23:45  lr: 0.000008  min_lr: 0.000002  loss: 1.3849 (1.3849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7215 (7.7215)  time: 8.9116 (8.9116 -- 8.9116)  data: 8.3968 (8.3968 -- 8.3968)  max mem: 16413
[2023-09-23 07:46:57,810] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:46:57,810] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:46:57,812] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:46:57,812] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [149]  [ 20/160]  eta: 0:02:42  lr: 0.000007  min_lr: 0.000002  loss: 1.2323 (1.2660)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5553 (6.9666)  time: 0.7738 (0.5207 -- 3.1565)  data: 0.2151 (0.0002 -- 2.6029)  max mem: 16413
[2023-09-23 07:47:16,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23868
[2023-09-23 07:47:16,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23868
[2023-09-23 07:47:16,265] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:47:16,265] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:47:16,265] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [149]  [ 40/160]  eta: 0:02:12  lr: 0.000007  min_lr: 0.000002  loss: 1.4600 (1.3650)  loss_scale: 16384.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8925 (7.5629)  time: 1.0466 (0.5082 -- 4.4240)  data: 0.4767 (0.0004 -- 3.9004)  max mem: 16413
Epoch: [149]  [ 60/160]  eta: 0:01:43  lr: 0.000007  min_lr: 0.000002  loss: 1.4400 (1.3769)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6936 (7.8284)  time: 0.9059 (0.5299 -- 3.6101)  data: 0.3562 (0.0005 -- 3.0972)  max mem: 16413
Epoch: [149]  [ 80/160]  eta: 0:01:19  lr: 0.000007  min_lr: 0.000002  loss: 1.4443 (1.3933)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2609 (8.0259)  time: 0.8500 (0.5234 -- 4.0862)  data: 0.3018 (0.0003 -- 3.5546)  max mem: 16413
Epoch: [149]  [100/160]  eta: 0:00:59  lr: 0.000007  min_lr: 0.000002  loss: 1.3895 (1.4006)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4534 (7.7644)  time: 1.0119 (0.5248 -- 4.5595)  data: 0.4566 (0.0009 -- 4.0261)  max mem: 16413
Epoch: [149]  [120/160]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000002  loss: 1.5284 (1.4193)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6840 (7.7280)  time: 0.9346 (0.5028 -- 5.2251)  data: 0.4049 (0.0002 -- 4.7195)  max mem: 16413
Epoch: [149]  [140/160]  eta: 0:00:19  lr: 0.000007  min_lr: 0.000002  loss: 1.3682 (1.4045)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6595 (7.6787)  time: 0.9499 (0.5221 -- 5.0903)  data: 0.4105 (0.0007 -- 4.5692)  max mem: 16413
[2023-09-23 07:49:11,343] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:49:11,343] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:49:11,344] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:49:11,344] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:49:12,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=138, lr=[1.8411106802422644e-06, 1.8411106802422644e-06, 2.0456785336025163e-06, 2.0456785336025163e-06, 2.2729761484472397e-06, 2.2729761484472397e-06, 2.5255290538302664e-06, 2.5255290538302664e-06, 2.8061433931447404e-06, 2.8061433931447404e-06, 3.1179371034941563e-06, 3.1179371034941563e-06, 3.464374559437951e-06, 3.464374559437951e-06, 3.849305066042168e-06, 3.849305066042168e-06, 4.277005628935741e-06, 4.277005628935741e-06, 4.752228476595268e-06, 4.752228476595268e-06, 5.280253862883632e-06, 5.280253862883632e-06, 5.866948736537368e-06, 5.866948736537368e-06, 6.518831929485964e-06, 6.518831929485964e-06, 7.243146588317738e-06, 7.243146588317738e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 07:49:12,366] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=17.25694061691679, CurrSamplesPerSec=24.02317375206968, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.3821 (1.3996)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4474 (7.5271)  time: 0.6577 (0.4940 -- 2.4191)  data: 0.1377 (0.0001 -- 1.9036)  max mem: 16413
Epoch: [149] Total time: 0:02:31 (0.9439 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.3821 (1.4286)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4474 (7.5271)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1412 (0.1412)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4421 (2.4421 -- 2.4421)  data: 2.2208 (2.2208 -- 2.2208)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1485 (0.2580)  acc1: 100.0000 (96.9697)  acc5: 100.0000 (98.9899)  time: 0.4571 (0.1809 -- 2.4421)  data: 0.2624 (0.0002 -- 2.2208)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1834 (0.2562)  acc1: 100.0000 (96.8254)  acc5: 100.0000 (99.4709)  time: 0.2549 (0.1676 -- 0.8586)  data: 0.0662 (0.0001 -- 0.6601)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2352 (0.3256)  acc1: 88.8889 (92.9461)  acc5: 100.0000 (99.5851)  time: 0.2787 (0.1326 -- 0.8586)  data: 0.0988 (0.0001 -- 0.6601)  max mem: 16413
Val: Total time: 0:00:09 (0.3399 s / it)
* Acc@1 93.361 Acc@5 99.793 loss 0.331
Accuracy of the network on the 482 val images: 93.36%
Max accuracy: 93.36%
Epoch: [150]  [  0/160]  eta: 0:19:30  lr: 0.000007  min_lr: 0.000002  loss: 1.6520 (1.6520)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2112 (6.2112)  time: 7.3162 (7.3162 -- 7.3162)  data: 6.4919 (6.4919 -- 6.4919)  max mem: 16413
Epoch: [150]  [ 20/160]  eta: 0:03:00  lr: 0.000007  min_lr: 0.000002  loss: 1.4356 (1.4367)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3088 (6.3028)  time: 0.9893 (0.5123 -- 5.9435)  data: 0.0346 (0.0004 -- 0.6629)  max mem: 16413
[2023-09-23 07:50:00,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24031
[2023-09-23 07:50:00,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24031
[2023-09-23 07:50:00,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:50:00,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 07:50:00,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [150]  [ 40/160]  eta: 0:02:14  lr: 0.000007  min_lr: 0.000002  loss: 1.2408 (1.3539)  loss_scale: 16384.0000 (28771.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0290 (6.6784)  time: 0.9512 (0.5077 -- 6.8017)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [150]  [ 60/160]  eta: 0:01:42  lr: 0.000007  min_lr: 0.000002  loss: 1.4067 (1.3687)  loss_scale: 16384.0000 (24710.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2599 (6.5064)  time: 0.8168 (0.5319 -- 3.8550)  data: 0.0014 (0.0006 -- 0.0027)  max mem: 16413
Epoch: [150]  [ 80/160]  eta: 0:01:21  lr: 0.000007  min_lr: 0.000002  loss: 1.3295 (1.3545)  loss_scale: 16384.0000 (22654.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6256 (6.6116)  time: 0.9953 (0.5183 -- 3.9876)  data: 0.0523 (0.0005 -- 1.0110)  max mem: 16413
Epoch: [150]  [100/160]  eta: 0:00:59  lr: 0.000007  min_lr: 0.000002  loss: 1.4203 (1.3680)  loss_scale: 16384.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3071 (6.7576)  time: 0.8758 (0.5074 -- 4.7039)  data: 0.0013 (0.0004 -- 0.0060)  max mem: 16413
Epoch: [150]  [120/160]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000002  loss: 1.2439 (1.3528)  loss_scale: 16384.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4241 (6.7820)  time: 0.9059 (0.5168 -- 4.2080)  data: 0.0015 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [150]  [140/160]  eta: 0:00:19  lr: 0.000007  min_lr: 0.000002  loss: 1.4433 (1.3721)  loss_scale: 16384.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5059 (6.7766)  time: 0.9268 (0.5191 -- 3.8445)  data: 0.0017 (0.0005 -- 0.0046)  max mem: 16413
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.4160 (1.3744)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0251 (6.8190)  time: 0.7807 (0.4915 -- 3.2700)  data: 0.0010 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [150] Total time: 0:02:31 (0.9478 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.4160 (1.3814)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0251 (6.8190)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1246 (0.1246)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3987 (2.3987 -- 2.3987)  data: 2.1958 (2.1958 -- 2.1958)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1340 (0.3152)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4718 (0.1794 -- 2.3987)  data: 0.2814 (0.0002 -- 2.1958)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2031 (0.2970)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2550 (0.1691 -- 1.0913)  data: 0.0684 (0.0001 -- 0.8951)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2796 (0.3542)  acc1: 88.8889 (92.1162)  acc5: 100.0000 (99.5851)  time: 0.2773 (0.1324 -- 1.0913)  data: 0.0984 (0.0001 -- 0.8951)  max mem: 16413
Val: Total time: 0:00:09 (0.3382 s / it)
* Acc@1 92.116 Acc@5 99.378 loss 0.362
Accuracy of the network on the 482 val images: 92.12%
Max accuracy: 93.36%
[2023-09-23 07:52:11,647] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:52:11,647] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:52:11,649] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:52:11,649] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [151]  [  0/160]  eta: 0:23:44  lr: 0.000007  min_lr: 0.000002  loss: 0.9130 (0.9130)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5046 (7.5046)  time: 8.9050 (8.9050 -- 8.9050)  data: 8.3840 (8.3840 -- 8.3840)  max mem: 16413
Epoch: [151]  [ 20/160]  eta: 0:03:00  lr: 0.000007  min_lr: 0.000002  loss: 1.4040 (1.3749)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6957 (6.9982)  time: 0.9054 (0.5179 -- 3.8733)  data: 0.2734 (0.0002 -- 3.3293)  max mem: 16413
Epoch: [151]  [ 40/160]  eta: 0:02:23  lr: 0.000007  min_lr: 0.000002  loss: 1.4262 (1.3862)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8457 (7.1833)  time: 1.0981 (0.5260 -- 5.2493)  data: 0.1511 (0.0003 -- 2.1719)  max mem: 16413
[2023-09-23 07:52:53,374] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24203
[2023-09-23 07:52:53,374] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24203
[2023-09-23 07:52:53,374] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:52:53,374] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:52:53,374] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [151]  [ 60/160]  eta: 0:01:47  lr: 0.000007  min_lr: 0.000002  loss: 1.4404 (1.3859)  loss_scale: 16384.0000 (27933.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1979 (7.0684)  time: 0.8206 (0.5140 -- 4.2392)  data: 0.0012 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [151]  [ 80/160]  eta: 0:01:23  lr: 0.000007  min_lr: 0.000002  loss: 1.4286 (1.3849)  loss_scale: 16384.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0735 (7.3255)  time: 0.9516 (0.5032 -- 4.4129)  data: 0.0008 (0.0003 -- 0.0019)  max mem: 16413
Epoch: [151]  [100/160]  eta: 0:01:00  lr: 0.000007  min_lr: 0.000002  loss: 1.3176 (1.3797)  loss_scale: 16384.0000 (23359.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4007 (7.3032)  time: 0.9045 (0.5197 -- 3.3742)  data: 0.0013 (0.0006 -- 0.0029)  max mem: 16413
Epoch: [151]  [120/160]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000002  loss: 1.4136 (1.3915)  loss_scale: 16384.0000 (22206.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0160 (7.1913)  time: 0.7744 (0.5128 -- 3.2068)  data: 0.0015 (0.0005 -- 0.0037)  max mem: 16413
Epoch: [151]  [140/160]  eta: 0:00:19  lr: 0.000007  min_lr: 0.000002  loss: 1.2758 (1.3770)  loss_scale: 16384.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8359 (7.1884)  time: 1.0915 (0.5199 -- 4.8918)  data: 0.0025 (0.0002 -- 0.0159)  max mem: 16413
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.4148 (1.3863)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6438 (7.1439)  time: 0.6620 (0.4935 -- 3.1084)  data: 0.0005 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [151] Total time: 0:02:32 (0.9536 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.4148 (1.3874)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6438 (7.1439)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1284 (0.1284)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4599 (2.4599 -- 2.4599)  data: 2.2373 (2.2373 -- 2.2373)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1402 (0.3906)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4762 (0.1824 -- 2.4599)  data: 0.2817 (0.0003 -- 2.2373)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1577 (0.3244)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2633 (0.1707 -- 1.0655)  data: 0.0753 (0.0001 -- 0.8546)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2421 (0.4040)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2819 (0.1339 -- 1.0655)  data: 0.1014 (0.0001 -- 0.8546)  max mem: 16413
Val: Total time: 0:00:09 (0.3425 s / it)
* Acc@1 92.946 Acc@5 100.000 loss 0.361
Accuracy of the network on the 482 val images: 92.95%
Max accuracy: 93.36%
Epoch: [152]  [  0/160]  eta: 0:22:09  lr: 0.000007  min_lr: 0.000002  loss: 1.3706 (1.3706)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9180 (10.9180)  time: 8.3108 (8.3108 -- 8.3108)  data: 7.5408 (7.5408 -- 7.5408)  max mem: 16413
[2023-09-23 07:55:03,039] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:55:03,039] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:55:03,039] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:55:03,039] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [152]  [ 20/160]  eta: 0:03:02  lr: 0.000007  min_lr: 0.000002  loss: 1.2906 (1.3046)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7954 (6.5312)  time: 0.9507 (0.5148 -- 3.3473)  data: 0.2192 (0.0003 -- 2.5313)  max mem: 16413
Epoch: [152]  [ 40/160]  eta: 0:02:13  lr: 0.000007  min_lr: 0.000002  loss: 1.3735 (1.3212)  loss_scale: 32768.0000 (27972.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1259 (6.4916)  time: 0.9104 (0.5058 -- 4.6619)  data: 0.0465 (0.0002 -- 0.7518)  max mem: 16413
[2023-09-23 07:55:35,675] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24364
[2023-09-23 07:55:35,675] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:55:35,675] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24364
[2023-09-23 07:55:35,676] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:55:35,676] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [152]  [ 60/160]  eta: 0:01:44  lr: 0.000007  min_lr: 0.000002  loss: 1.2473 (1.3184)  loss_scale: 16384.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9305 (6.8448)  time: 0.9009 (0.5153 -- 3.8008)  data: 0.0427 (0.0003 -- 0.8275)  max mem: 16413
Epoch: [152]  [ 80/160]  eta: 0:01:21  lr: 0.000007  min_lr: 0.000002  loss: 1.2571 (1.3021)  loss_scale: 16384.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8385 (7.0036)  time: 0.9432 (0.5132 -- 3.2178)  data: 0.1072 (0.0004 -- 1.6741)  max mem: 16413
Epoch: [152]  [100/160]  eta: 0:00:58  lr: 0.000007  min_lr: 0.000002  loss: 1.3411 (1.3179)  loss_scale: 16384.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2644 (6.9529)  time: 0.8295 (0.5193 -- 4.9877)  data: 0.0014 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [152]  [120/160]  eta: 0:00:39  lr: 0.000007  min_lr: 0.000002  loss: 1.2898 (1.3171)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8261 (6.9414)  time: 0.9732 (0.5287 -- 3.6589)  data: 0.0026 (0.0006 -- 0.0137)  max mem: 16413
Epoch: [152]  [140/160]  eta: 0:00:19  lr: 0.000006  min_lr: 0.000002  loss: 1.2339 (1.3196)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4501 (6.9357)  time: 0.8628 (0.5133 -- 2.7157)  data: 0.1290 (0.0005 -- 1.4826)  max mem: 16413
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.4678 (1.3339)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7164 (7.0496)  time: 0.7347 (0.4950 -- 3.6904)  data: 0.0006 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [152] Total time: 0:02:29 (0.9369 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.4678 (1.3698)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7164 (7.0496)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1368 (0.1368)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4027 (2.4027 -- 2.4027)  data: 2.2139 (2.2139 -- 2.2139)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1517 (0.3513)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4751 (0.1739 -- 2.4027)  data: 0.2836 (0.0003 -- 2.2139)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1544 (0.3243)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (100.0000)  time: 0.2583 (0.1686 -- 1.1064)  data: 0.0739 (0.0001 -- 0.8999)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3642 (0.4053)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2842 (0.1328 -- 1.1064)  data: 0.1078 (0.0001 -- 0.8999)  max mem: 16413
Val: Total time: 0:00:09 (0.3434 s / it)
* Acc@1 92.531 Acc@5 99.585 loss 0.380
Accuracy of the network on the 482 val images: 92.53%
Max accuracy: 93.36%
Epoch: [153]  [  0/160]  eta: 0:25:40  lr: 0.000006  min_lr: 0.000002  loss: 1.5254 (1.5254)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6177 (9.6177)  time: 9.6282 (9.6282 -- 9.6282)  data: 6.5024 (6.5024 -- 6.5024)  max mem: 16413
[2023-09-23 07:57:42,601] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:57:42,602] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 07:57:42,602] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 07:57:42,603] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [153]  [ 20/160]  eta: 0:02:45  lr: 0.000006  min_lr: 0.000002  loss: 1.3477 (1.3546)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9282 (6.5925)  time: 0.7574 (0.5336 -- 2.4351)  data: 0.1627 (0.0002 -- 1.6265)  max mem: 16413
Epoch: [153]  [ 40/160]  eta: 0:02:12  lr: 0.000006  min_lr: 0.000002  loss: 1.4515 (1.3724)  loss_scale: 32768.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9853 (6.8056)  time: 1.0171 (0.5107 -- 4.0143)  data: 0.3004 (0.0003 -- 3.4506)  max mem: 16413
Epoch: [153]  [ 60/160]  eta: 0:01:44  lr: 0.000006  min_lr: 0.000002  loss: 1.3595 (1.3944)  loss_scale: 32768.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9416 (6.7737)  time: 0.9223 (0.5147 -- 3.7292)  data: 0.1207 (0.0003 -- 2.1688)  max mem: 16413
[2023-09-23 07:58:34,040] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24545
[2023-09-23 07:58:34,040] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24545
[2023-09-23 07:58:34,040] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:58:34,040] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 07:58:34,040] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [153]  [ 80/160]  eta: 0:01:20  lr: 0.000006  min_lr: 0.000002  loss: 1.3379 (1.3851)  loss_scale: 16384.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3315 (6.9483)  time: 0.8995 (0.5208 -- 4.3109)  data: 0.0017 (0.0004 -- 0.0086)  max mem: 16413
Epoch: [153]  [100/160]  eta: 0:01:00  lr: 0.000006  min_lr: 0.000002  loss: 1.4902 (1.4107)  loss_scale: 16384.0000 (24819.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3922 (6.9880)  time: 0.9830 (0.5258 -- 4.3142)  data: 0.0012 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [153]  [120/160]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000002  loss: 1.3010 (1.4001)  loss_scale: 16384.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1398 (7.0296)  time: 0.8488 (0.5284 -- 3.6374)  data: 0.0800 (0.0003 -- 1.5711)  max mem: 16413
[2023-09-23 07:59:27,861] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24607
[2023-09-23 07:59:27,861] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:59:27,861] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24607
[2023-09-23 07:59:27,862] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 07:59:27,862] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [153]  [140/160]  eta: 0:00:19  lr: 0.000006  min_lr: 0.000002  loss: 1.3390 (1.3921)  loss_scale: 8192.0000 (21612.9362)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3398 (6.9987)  time: 0.9007 (0.5110 -- 4.1555)  data: 0.2714 (0.0003 -- 3.6046)  max mem: 16413
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.3511 (1.3933)  loss_scale: 8192.0000 (20019.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5469 (7.0819)  time: 0.6740 (0.4934 -- 2.5355)  data: 0.0513 (0.0002 -- 1.0099)  max mem: 16413
Epoch: [153] Total time: 0:02:29 (0.9322 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.3511 (1.4050)  loss_scale: 8192.0000 (20019.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5469 (7.0819)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1465 (0.1465)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3932 (2.3932 -- 2.3932)  data: 2.1841 (2.1841 -- 2.1841)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1465 (0.3761)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4719 (0.1839 -- 2.3932)  data: 0.2723 (0.0002 -- 2.1841)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1573 (0.3225)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2584 (0.1680 -- 1.0019)  data: 0.0649 (0.0001 -- 0.7999)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1908 (0.3797)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (100.0000)  time: 0.2808 (0.1325 -- 1.0019)  data: 0.0992 (0.0001 -- 0.7999)  max mem: 16413
Val: Total time: 0:00:09 (0.3423 s / it)
* Acc@1 93.154 Acc@5 99.585 loss 0.337
Accuracy of the network on the 482 val images: 93.15%
Max accuracy: 93.36%
Epoch: [154]  [  0/160]  eta: 0:22:51  lr: 0.000006  min_lr: 0.000002  loss: 1.1704 (1.1704)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5642 (6.5642)  time: 8.5727 (8.5727 -- 8.5727)  data: 5.1838 (5.1838 -- 5.1838)  max mem: 16413
Epoch: [154]  [ 20/160]  eta: 0:02:53  lr: 0.000006  min_lr: 0.000002  loss: 1.4408 (1.4295)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9992 (9.0196)  time: 0.8696 (0.5177 -- 3.6797)  data: 0.2049 (0.0004 -- 2.6749)  max mem: 16413
Epoch: [154]  [ 40/160]  eta: 0:02:06  lr: 0.000006  min_lr: 0.000002  loss: 1.4412 (1.4386)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1097 (8.5449)  time: 0.8609 (0.5242 -- 2.1074)  data: 0.3139 (0.0004 -- 1.5800)  max mem: 16413
Epoch: [154]  [ 60/160]  eta: 0:01:41  lr: 0.000006  min_lr: 0.000002  loss: 1.2741 (1.4032)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0127 (8.0204)  time: 0.9423 (0.5124 -- 5.2637)  data: 0.0023 (0.0004 -- 0.0132)  max mem: 16413
Epoch: [154]  [ 80/160]  eta: 0:01:20  lr: 0.000006  min_lr: 0.000002  loss: 1.4486 (1.4083)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2487 (7.8502)  time: 0.9642 (0.5220 -- 2.5861)  data: 0.2254 (0.0002 -- 1.7841)  max mem: 16413
[2023-09-23 08:01:38,868] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:01:38,868] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:01:38,869] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:01:38,869] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [154]  [100/160]  eta: 0:01:00  lr: 0.000006  min_lr: 0.000002  loss: 1.3682 (1.4115)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7531 (7.7011)  time: 1.0241 (0.5074 -- 3.9496)  data: 0.2938 (0.0003 -- 3.4297)  max mem: 16413
Epoch: [154]  [120/160]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000002  loss: 1.3960 (1.4232)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0858 (7.5790)  time: 0.8157 (0.5145 -- 3.8536)  data: 0.0011 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [154]  [140/160]  eta: 0:00:19  lr: 0.000006  min_lr: 0.000002  loss: 1.4717 (1.4351)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1231 (7.3971)  time: 1.0128 (0.5208 -- 4.2326)  data: 0.0015 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.3292 (1.4188)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9819 (7.4199)  time: 0.7055 (0.4936 -- 3.8699)  data: 0.0007 (0.0001 -- 0.0021)  max mem: 16413
Epoch: [154] Total time: 0:02:31 (0.9497 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.3292 (1.3933)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9819 (7.4199)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1588 (0.1588)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5266 (2.5266 -- 2.5266)  data: 2.3068 (2.3068 -- 2.3068)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1669 (0.3657)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4817 (0.1763 -- 2.5266)  data: 0.2888 (0.0003 -- 2.3068)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1860 (0.3324)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2573 (0.1696 -- 1.0594)  data: 0.0676 (0.0001 -- 0.8626)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2977 (0.3908)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (100.0000)  time: 0.2823 (0.1329 -- 1.0594)  data: 0.0995 (0.0001 -- 0.8626)  max mem: 16413
Val: Total time: 0:00:09 (0.3445 s / it)
* Acc@1 91.494 Acc@5 99.585 loss 0.383
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 93.36%
Epoch: [155]  [  0/160]  eta: 0:24:45  lr: 0.000006  min_lr: 0.000002  loss: 1.4406 (1.4406)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6762 (4.6762)  time: 9.2854 (9.2854 -- 9.2854)  data: 8.7578 (8.7578 -- 8.7578)  max mem: 16413
Epoch: [155]  [ 20/160]  eta: 0:03:12  lr: 0.000006  min_lr: 0.000002  loss: 1.3501 (1.3526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5752 (6.4807)  time: 0.9801 (0.5073 -- 6.2357)  data: 0.4365 (0.0003 -- 5.7223)  max mem: 16413
[2023-09-23 08:03:27,281] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24837
[2023-09-23 08:03:27,281] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24837
[2023-09-23 08:03:27,282] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:03:27,282] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:03:27,282] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [155]  [ 40/160]  eta: 0:02:24  lr: 0.000006  min_lr: 0.000001  loss: 1.4084 (1.3481)  loss_scale: 16384.0000 (15584.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9054 (7.4018)  time: 1.0284 (0.5054 -- 4.9556)  data: 0.4943 (0.0002 -- 4.4262)  max mem: 16413
Epoch: [155]  [ 60/160]  eta: 0:01:48  lr: 0.000006  min_lr: 0.000001  loss: 1.4598 (1.4194)  loss_scale: 8192.0000 (13160.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9851 (7.4787)  time: 0.8237 (0.5149 -- 4.1630)  data: 0.2947 (0.0003 -- 3.6499)  max mem: 16413
Epoch: [155]  [ 80/160]  eta: 0:01:22  lr: 0.000006  min_lr: 0.000001  loss: 1.3755 (1.3932)  loss_scale: 8192.0000 (11934.0247)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9687 (7.2412)  time: 0.8611 (0.5254 -- 3.5353)  data: 0.3190 (0.0002 -- 3.0063)  max mem: 16413
Epoch: [155]  [100/160]  eta: 0:00:59  lr: 0.000006  min_lr: 0.000001  loss: 1.3181 (1.3967)  loss_scale: 8192.0000 (11193.0297)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2848 (7.3664)  time: 0.8703 (0.5174 -- 3.1934)  data: 0.2018 (0.0004 -- 2.6588)  max mem: 16413
Epoch: [155]  [120/160]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000001  loss: 1.3571 (1.3948)  loss_scale: 8192.0000 (10696.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4458 (7.2968)  time: 0.9458 (0.5273 -- 4.2001)  data: 0.3960 (0.0003 -- 3.6717)  max mem: 16413
Epoch: [155]  [140/160]  eta: 0:00:19  lr: 0.000006  min_lr: 0.000001  loss: 1.4943 (1.4063)  loss_scale: 8192.0000 (10341.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6699 (7.2942)  time: 0.9391 (0.5240 -- 3.8926)  data: 0.1710 (0.0003 -- 3.3906)  max mem: 16413
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000001  loss: 1.5700 (1.4233)  loss_scale: 8192.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2548 (7.3309)  time: 0.7389 (0.4943 -- 2.3551)  data: 0.2193 (0.0002 -- 1.8439)  max mem: 16413
Epoch: [155] Total time: 0:02:32 (0.9532 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000001  loss: 1.5700 (1.4108)  loss_scale: 8192.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2548 (7.3309)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1448 (0.1448)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4986 (2.4986 -- 2.4986)  data: 2.2751 (2.2751 -- 2.2751)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1340 (0.3369)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4848 (0.1784 -- 2.4986)  data: 0.2941 (0.0003 -- 2.2751)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1387 (0.3012)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (99.4709)  time: 0.2565 (0.1683 -- 1.1508)  data: 0.0691 (0.0001 -- 0.9535)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1680 (0.3527)  acc1: 100.0000 (92.9461)  acc5: 100.0000 (99.5851)  time: 0.2756 (0.1331 -- 1.1508)  data: 0.0949 (0.0001 -- 0.9535)  max mem: 16413
Val: Total time: 0:00:09 (0.3382 s / it)
* Acc@1 92.946 Acc@5 99.585 loss 0.339
Accuracy of the network on the 482 val images: 92.95%
Max accuracy: 93.36%
Epoch: [156]  [  0/160]  eta: 0:31:12  lr: 0.000006  min_lr: 0.000001  loss: 1.6355 (1.6355)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4238 (7.4238)  time: 11.7014 (11.7014 -- 11.7014)  data: 5.2617 (5.2617 -- 5.2617)  max mem: 16413
[2023-09-23 08:05:40,564] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:05:40,564] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:05:40,564] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:05:40,564] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [156]  [ 20/160]  eta: 0:03:02  lr: 0.000006  min_lr: 0.000001  loss: 1.4426 (1.4182)  loss_scale: 16384.0000 (14043.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8260 (6.9057)  time: 0.7819 (0.5127 -- 3.3947)  data: 0.0019 (0.0002 -- 0.0071)  max mem: 16413
[2023-09-23 08:06:08,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=144, lr=[1.4304821545984304e-06, 1.4304821545984304e-06, 1.5894246162204784e-06, 1.5894246162204784e-06, 1.7660273513560868e-06, 1.7660273513560868e-06, 1.9622526126178744e-06, 1.9622526126178744e-06, 2.180280680686527e-06, 2.180280680686527e-06, 2.4225340896516965e-06, 2.4225340896516965e-06, 2.6917045440574407e-06, 2.6917045440574407e-06, 2.9907828267304892e-06, 2.9907828267304892e-06, 3.323092029700544e-06, 3.323092029700544e-06, 3.6923244774450484e-06, 3.6923244774450484e-06, 4.102582752716721e-06, 4.102582752716721e-06, 4.558425280796356e-06, 4.558425280796356e-06, 5.064916978662618e-06, 5.064916978662618e-06, 5.627685531847353e-06, 5.627685531847353e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 08:06:08,606] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=17.17888284994496, CurrSamplesPerSec=21.911677022094675, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [156]  [ 40/160]  eta: 0:02:13  lr: 0.000006  min_lr: 0.000001  loss: 1.5136 (1.3969)  loss_scale: 16384.0000 (15185.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4319 (6.7899)  time: 0.9205 (0.5156 -- 2.8711)  data: 0.1266 (0.0005 -- 1.4844)  max mem: 16413
Epoch: [156]  [ 60/160]  eta: 0:01:46  lr: 0.000006  min_lr: 0.000001  loss: 1.2757 (1.3725)  loss_scale: 16384.0000 (15578.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4736 (6.8581)  time: 0.9639 (0.5183 -- 3.7743)  data: 0.2672 (0.0004 -- 1.8091)  max mem: 16413
Epoch: [156]  [ 80/160]  eta: 0:01:21  lr: 0.000006  min_lr: 0.000001  loss: 1.4074 (1.3772)  loss_scale: 16384.0000 (15777.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5786 (7.0842)  time: 0.8797 (0.5234 -- 3.5281)  data: 0.3173 (0.0004 -- 2.9895)  max mem: 16413
Epoch: [156]  [100/160]  eta: 0:01:00  lr: 0.000006  min_lr: 0.000001  loss: 1.3295 (1.3686)  loss_scale: 16384.0000 (15897.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0429 (7.0823)  time: 0.9555 (0.5115 -- 4.5089)  data: 0.3507 (0.0004 -- 3.9815)  max mem: 16413
Epoch: [156]  [120/160]  eta: 0:00:39  lr: 0.000006  min_lr: 0.000001  loss: 1.2951 (1.3576)  loss_scale: 16384.0000 (15977.7851)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5039 (6.9538)  time: 0.8947 (0.5144 -- 5.4504)  data: 0.3319 (0.0003 -- 4.9154)  max mem: 16413
[2023-09-23 08:07:34,230] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25089
[2023-09-23 08:07:34,230] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25089
[2023-09-23 08:07:34,231] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:07:34,231] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:07:34,231] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [156]  [140/160]  eta: 0:00:20  lr: 0.000005  min_lr: 0.000001  loss: 1.5063 (1.3803)  loss_scale: 8192.0000 (15338.2128)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1008 (6.9306)  time: 1.0768 (0.5201 -- 4.5751)  data: 0.0885 (0.0002 -- 1.7473)  max mem: 16413
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.0828 (1.3644)  loss_scale: 8192.0000 (14489.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8423 (6.9082)  time: 0.6480 (0.4924 -- 3.0682)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [156] Total time: 0:02:33 (0.9602 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.0828 (1.3705)  loss_scale: 8192.0000 (14489.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8423 (6.9082)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1260 (0.1260)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4347 (2.4347 -- 2.4347)  data: 2.2410 (2.2410 -- 2.2410)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1451 (0.3914)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4704 (0.1788 -- 2.4347)  data: 0.2769 (0.0003 -- 2.2410)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1795 (0.3669)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2542 (0.1680 -- 1.0306)  data: 0.0667 (0.0001 -- 0.7995)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3733 (0.4177)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (100.0000)  time: 0.2810 (0.1328 -- 1.0306)  data: 0.1017 (0.0001 -- 0.7995)  max mem: 16413
Val: Total time: 0:00:09 (0.3409 s / it)
* Acc@1 91.494 Acc@5 99.793 loss 0.371
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 93.36%
Epoch: [157]  [  0/160]  eta: 0:23:00  lr: 0.000005  min_lr: 0.000001  loss: 1.0330 (1.0330)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9719 (7.9719)  time: 8.6297 (8.6297 -- 8.6297)  data: 8.1015 (8.1015 -- 8.1015)  max mem: 16413
Epoch: [157]  [ 20/160]  eta: 0:03:00  lr: 0.000005  min_lr: 0.000001  loss: 1.4549 (1.3475)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6547 (8.5649)  time: 0.9257 (0.5213 -- 3.0118)  data: 0.2376 (0.0004 -- 2.4879)  max mem: 16413
Epoch: [157]  [ 40/160]  eta: 0:02:09  lr: 0.000005  min_lr: 0.000001  loss: 1.2839 (1.3225)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0545 (8.2521)  time: 0.8631 (0.5120 -- 3.2091)  data: 0.0851 (0.0002 -- 1.1043)  max mem: 16413
Epoch: [157]  [ 60/160]  eta: 0:01:48  lr: 0.000005  min_lr: 0.000001  loss: 1.4140 (1.3558)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5507 (7.9574)  time: 1.0951 (0.5029 -- 6.4447)  data: 0.0508 (0.0002 -- 0.9935)  max mem: 16413
Epoch: [157]  [ 80/160]  eta: 0:01:20  lr: 0.000005  min_lr: 0.000001  loss: 1.3746 (1.3669)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7329 (7.7113)  time: 0.7658 (0.5200 -- 2.8610)  data: 0.0019 (0.0002 -- 0.0057)  max mem: 16413
[2023-09-23 08:09:51,615] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:09:51,615] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:09:51,616] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:09:51,616] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [157]  [100/160]  eta: 0:01:01  lr: 0.000005  min_lr: 0.000001  loss: 1.5187 (1.4116)  loss_scale: 8192.0000 (8435.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1219 (7.8879)  time: 1.1076 (0.5119 -- 4.6215)  data: 0.0012 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [157]  [120/160]  eta: 0:00:39  lr: 0.000005  min_lr: 0.000001  loss: 1.3961 (1.4072)  loss_scale: 16384.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7093 (7.7238)  time: 0.7773 (0.5167 -- 3.0355)  data: 0.0013 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [157]  [140/160]  eta: 0:00:19  lr: 0.000005  min_lr: 0.000001  loss: 1.3468 (1.4019)  loss_scale: 16384.0000 (10690.2695)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2154 (7.7413)  time: 0.9926 (0.5082 -- 4.4899)  data: 0.0013 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.2306 (1.3879)  loss_scale: 16384.0000 (11366.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9894 (7.6328)  time: 0.7135 (0.4947 -- 2.2780)  data: 0.1190 (0.0001 -- 1.7835)  max mem: 16413
Epoch: [157] Total time: 0:02:32 (0.9559 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.2306 (1.3996)  loss_scale: 16384.0000 (11366.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9894 (7.6328)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1308 (0.1308)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4049 (2.4049 -- 2.4049)  data: 2.2151 (2.2151 -- 2.2151)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1352 (0.3594)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (97.9798)  time: 0.4752 (0.1813 -- 2.4049)  data: 0.2849 (0.0003 -- 2.2151)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1432 (0.3486)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (98.9418)  time: 0.2564 (0.1686 -- 1.1131)  data: 0.0709 (0.0001 -- 0.9121)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3626 (0.4036)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (99.1701)  time: 0.2880 (0.1321 -- 1.1131)  data: 0.1101 (0.0001 -- 0.9121)  max mem: 16413
Val: Total time: 0:00:09 (0.3447 s / it)
* Acc@1 91.909 Acc@5 99.378 loss 0.389
Accuracy of the network on the 482 val images: 91.91%
Max accuracy: 93.36%
Epoch: [158]  [  0/160]  eta: 0:18:59  lr: 0.000005  min_lr: 0.000001  loss: 1.6699 (1.6699)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9066 (8.9066)  time: 7.1248 (7.1248 -- 7.1248)  data: 6.3856 (6.3856 -- 6.3856)  max mem: 16413
Epoch: [158]  [ 20/160]  eta: 0:02:57  lr: 0.000005  min_lr: 0.000001  loss: 1.3852 (1.3987)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7956 (7.3983)  time: 0.9731 (0.5252 -- 3.5121)  data: 0.2106 (0.0007 -- 2.1068)  max mem: 16413
Epoch: [158]  [ 40/160]  eta: 0:02:13  lr: 0.000005  min_lr: 0.000001  loss: 1.3096 (1.3764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4029 (6.7860)  time: 0.9456 (0.5314 -- 2.7226)  data: 0.2278 (0.0003 -- 1.5206)  max mem: 16413
Epoch: [158]  [ 60/160]  eta: 0:01:48  lr: 0.000005  min_lr: 0.000001  loss: 1.4742 (1.4218)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1684 (6.9643)  time: 1.0344 (0.5122 -- 4.4243)  data: 0.1963 (0.0004 -- 3.8820)  max mem: 16413
[2023-09-23 08:12:03,151] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:12:03,151] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:12:03,152] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:12:03,152] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:12:10,221] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25355
[2023-09-23 08:12:10,221] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25355
[2023-09-23 08:12:10,222] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:12:10,222] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:12:10,222] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [158]  [ 80/160]  eta: 0:01:20  lr: 0.000005  min_lr: 0.000001  loss: 1.4526 (1.4235)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9153 (7.1334)  time: 0.7695 (0.5255 -- 2.9294)  data: 0.0022 (0.0002 -- 0.0151)  max mem: 16413
Epoch: [158]  [100/160]  eta: 0:01:00  lr: 0.000005  min_lr: 0.000001  loss: 1.3714 (1.4082)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8152 (7.0842)  time: 0.9778 (0.5206 -- 4.6419)  data: 0.0014 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [158]  [120/160]  eta: 0:00:39  lr: 0.000005  min_lr: 0.000001  loss: 1.3618 (1.4057)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3122 (7.0355)  time: 0.9064 (0.5106 -- 5.4281)  data: 0.0512 (0.0003 -- 0.9978)  max mem: 16413
Epoch: [158]  [140/160]  eta: 0:00:19  lr: 0.000005  min_lr: 0.000001  loss: 1.3087 (1.3947)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6091 (6.9532)  time: 0.8939 (0.5237 -- 3.3656)  data: 0.2786 (0.0003 -- 2.8557)  max mem: 16413
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.4026 (1.4014)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3366 (7.0299)  time: 0.7110 (0.4951 -- 2.5686)  data: 0.1924 (0.0002 -- 2.0411)  max mem: 16413
Epoch: [158] Total time: 0:02:30 (0.9428 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.4026 (1.3957)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3366 (7.0299)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1395 (0.1395)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4385 (2.4385 -- 2.4385)  data: 2.2561 (2.2561 -- 2.2561)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1395 (0.3337)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4773 (0.1769 -- 2.4385)  data: 0.2853 (0.0003 -- 2.2561)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1649 (0.2949)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (100.0000)  time: 0.2566 (0.1683 -- 1.0911)  data: 0.0676 (0.0001 -- 0.8750)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2477 (0.3654)  acc1: 88.8889 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2860 (0.1327 -- 1.0911)  data: 0.1042 (0.0001 -- 0.8750)  max mem: 16413
Val: Total time: 0:00:09 (0.3442 s / it)
* Acc@1 93.154 Acc@5 99.793 loss 0.348
Accuracy of the network on the 482 val images: 93.15%
Max accuracy: 93.36%
Epoch: [159]  [  0/160]  eta: 0:18:15  lr: 0.000005  min_lr: 0.000001  loss: 1.5482 (1.5482)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7978 (5.7978)  time: 6.8438 (6.8438 -- 6.8438)  data: 6.2746 (6.2746 -- 6.2746)  max mem: 16413
Epoch: [159]  [ 20/160]  eta: 0:02:59  lr: 0.000005  min_lr: 0.000001  loss: 1.3056 (1.3312)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9313 (7.3930)  time: 1.0029 (0.5184 -- 4.3360)  data: 0.1316 (0.0004 -- 1.3439)  max mem: 16413
Epoch: [159]  [ 40/160]  eta: 0:02:10  lr: 0.000005  min_lr: 0.000001  loss: 1.4622 (1.3509)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6110 (7.1520)  time: 0.8883 (0.5192 -- 3.5388)  data: 0.0021 (0.0004 -- 0.0138)  max mem: 16413
[2023-09-23 08:14:21,508] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:14:21,508] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:14:21,509] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:14:21,509] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [159]  [ 60/160]  eta: 0:01:42  lr: 0.000005  min_lr: 0.000001  loss: 1.2881 (1.3522)  loss_scale: 32768.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5245 (7.0673)  time: 0.9032 (0.5105 -- 3.7945)  data: 0.0495 (0.0003 -- 0.8381)  max mem: 16413
[2023-09-23 08:14:44,400] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25505
[2023-09-23 08:14:44,400] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25505
[2023-09-23 08:14:44,400] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:14:44,400] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:14:44,400] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [ 80/160]  eta: 0:01:20  lr: 0.000005  min_lr: 0.000001  loss: 1.2057 (1.3407)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0252 (6.9066)  time: 0.9512 (0.5104 -- 7.9257)  data: 0.1021 (0.0003 -- 2.0179)  max mem: 16413
Epoch: [159]  [100/160]  eta: 0:01:01  lr: 0.000005  min_lr: 0.000001  loss: 1.3162 (1.3352)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3551 (6.8589)  time: 1.0827 (0.5133 -- 5.4662)  data: 0.0010 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [159]  [120/160]  eta: 0:00:39  lr: 0.000005  min_lr: 0.000001  loss: 1.4338 (1.3417)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6176 (6.8735)  time: 0.7510 (0.5245 -- 3.1254)  data: 0.0018 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [159]  [140/160]  eta: 0:00:19  lr: 0.000005  min_lr: 0.000001  loss: 1.2280 (1.3358)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4431 (6.8794)  time: 0.9212 (0.5202 -- 3.9487)  data: 0.0014 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.3402 (1.3395)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8937 (6.8468)  time: 0.7748 (0.4949 -- 2.4856)  data: 0.0010 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [159] Total time: 0:02:31 (0.9489 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.3402 (1.3731)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8937 (6.8468)
[2023-09-23 08:16:03,285] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-09-23 08:16:03,287] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-09-23 08:16:03,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-09-23 08:16:03,287] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-09-23 08:16:04,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-09-23 08:16:04,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1495 (0.1495)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4823 (2.4823 -- 2.4823)  data: 2.2827 (2.2827 -- 2.2827)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1470 (0.3702)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4782 (0.1769 -- 2.4823)  data: 0.2841 (0.0002 -- 2.2827)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1635 (0.3352)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (99.4709)  time: 0.2574 (0.1682 -- 1.0257)  data: 0.0664 (0.0001 -- 0.8298)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2494 (0.3777)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (99.5851)  time: 0.2832 (0.1328 -- 1.0257)  data: 0.1007 (0.0001 -- 0.8298)  max mem: 16413
Val: Total time: 0:00:09 (0.3454 s / it)
* Acc@1 91.079 Acc@5 99.585 loss 0.379
Accuracy of the network on the 482 val images: 91.08%
Max accuracy: 93.36%
Epoch: [160]  [  0/160]  eta: 0:20:30  lr: 0.000005  min_lr: 0.000001  loss: 1.5675 (1.5675)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6119 (11.6119)  time: 7.6937 (7.6937 -- 7.6937)  data: 7.1748 (7.1748 -- 7.1748)  max mem: 16413
Epoch: [160]  [ 20/160]  eta: 0:02:52  lr: 0.000005  min_lr: 0.000001  loss: 1.5129 (1.4495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4381 (6.8674)  time: 0.9062 (0.5249 -- 3.2757)  data: 0.1046 (0.0003 -- 1.1547)  max mem: 16413
[2023-09-23 08:16:54,341] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:16:54,342] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:16:54,342] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:16:54,343] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:16:56,556] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25638
[2023-09-23 08:16:56,556] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:16:56,556] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25638
[2023-09-23 08:16:56,556] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 08:16:56,556] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [160]  [ 40/160]  eta: 0:02:12  lr: 0.000005  min_lr: 0.000001  loss: 1.5065 (1.4002)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3486 (6.7208)  time: 0.9764 (0.5209 -- 3.5222)  data: 0.0029 (0.0007 -- 0.0166)  max mem: 16413
[2023-09-23 08:17:07,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25650
[2023-09-23 08:17:07,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25650
[2023-09-23 08:17:07,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:17:07,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:17:07,505] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [160]  [ 60/160]  eta: 0:01:42  lr: 0.000005  min_lr: 0.000001  loss: 1.4383 (1.3978)  loss_scale: 8192.0000 (15981.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6022 (7.0190)  time: 0.8738 (0.5188 -- 2.5666)  data: 0.0616 (0.0003 -- 1.1151)  max mem: 16413
Epoch: [160]  [ 80/160]  eta: 0:01:20  lr: 0.000005  min_lr: 0.000001  loss: 1.2549 (1.3687)  loss_scale: 8192.0000 (14057.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8512 (6.9373)  time: 0.9430 (0.5169 -- 4.5193)  data: 0.0010 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [160]  [100/160]  eta: 0:01:00  lr: 0.000005  min_lr: 0.000001  loss: 1.2279 (1.3499)  loss_scale: 8192.0000 (12896.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4221 (6.7954)  time: 0.9754 (0.5288 -- 5.2138)  data: 0.0014 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [160]  [120/160]  eta: 0:00:39  lr: 0.000005  min_lr: 0.000001  loss: 1.3978 (1.3642)  loss_scale: 8192.0000 (12118.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7422 (7.0240)  time: 0.9723 (0.5113 -- 4.8158)  data: 0.0010 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [160]  [140/160]  eta: 0:00:19  lr: 0.000005  min_lr: 0.000001  loss: 1.3656 (1.3640)  loss_scale: 8192.0000 (11561.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2132 (7.1540)  time: 0.9017 (0.5251 -- 3.0496)  data: 0.0011 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [160]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.2256 (1.3579)  loss_scale: 8192.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7487 (7.0998)  time: 0.7402 (0.4939 -- 3.7931)  data: 0.0009 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [160] Total time: 0:02:32 (0.9559 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.2256 (1.3598)  loss_scale: 8192.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7487 (7.0998)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1233 (0.1233)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4844 (2.4844 -- 2.4844)  data: 2.3029 (2.3029 -- 2.3029)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1393 (0.3688)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4796 (0.1789 -- 2.4844)  data: 0.2879 (0.0003 -- 2.3029)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1753 (0.3402)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (99.4709)  time: 0.2533 (0.1684 -- 1.0521)  data: 0.0639 (0.0001 -- 0.8581)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2483 (0.3887)  acc1: 88.8889 (92.1162)  acc5: 100.0000 (99.5851)  time: 0.2756 (0.1325 -- 1.0521)  data: 0.0945 (0.0001 -- 0.8581)  max mem: 16413
Val: Total time: 0:00:09 (0.3396 s / it)
* Acc@1 93.361 Acc@5 99.378 loss 0.349
Accuracy of the network on the 482 val images: 93.36%
Max accuracy: 93.36%
Epoch: [161]  [  0/160]  eta: 0:20:33  lr: 0.000005  min_lr: 0.000001  loss: 2.0432 (2.0432)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4237 (7.4237)  time: 7.7114 (7.7114 -- 7.7114)  data: 5.8454 (5.8454 -- 5.8454)  max mem: 16413
[2023-09-23 08:19:22,820] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:19:22,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:19:22,820] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:19:22,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [161]  [ 20/160]  eta: 0:03:00  lr: 0.000004  min_lr: 0.000001  loss: 1.3346 (1.3113)  loss_scale: 8192.0000 (8972.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6138 (7.7028)  time: 0.9675 (0.5242 -- 2.4004)  data: 0.1824 (0.0005 -- 1.6686)  max mem: 16413
Epoch: [161]  [ 40/160]  eta: 0:02:10  lr: 0.000004  min_lr: 0.000001  loss: 1.1603 (1.2971)  loss_scale: 16384.0000 (12587.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7741 (7.4604)  time: 0.8705 (0.5061 -- 3.9939)  data: 0.0023 (0.0002 -- 0.0193)  max mem: 16413
Epoch: [161]  [ 60/160]  eta: 0:01:42  lr: 0.000004  min_lr: 0.000001  loss: 1.4067 (1.3405)  loss_scale: 16384.0000 (13832.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1980 (7.1757)  time: 0.9019 (0.5252 -- 3.1423)  data: 0.1820 (0.0003 -- 2.5506)  max mem: 16413
Epoch: [161]  [ 80/160]  eta: 0:01:20  lr: 0.000004  min_lr: 0.000001  loss: 1.4847 (1.3569)  loss_scale: 16384.0000 (14462.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5372 (7.2055)  time: 0.9677 (0.5176 -- 6.4574)  data: 0.4254 (0.0003 -- 5.9469)  max mem: 16413
Epoch: [161]  [100/160]  eta: 0:00:59  lr: 0.000004  min_lr: 0.000001  loss: 1.4143 (1.3728)  loss_scale: 16384.0000 (14842.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2132 (7.2645)  time: 0.9395 (0.5194 -- 3.4430)  data: 0.2512 (0.0003 -- 2.9166)  max mem: 16413
Epoch: [161]  [120/160]  eta: 0:00:39  lr: 0.000004  min_lr: 0.000001  loss: 1.3168 (1.3622)  loss_scale: 16384.0000 (15097.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8412 (7.1528)  time: 0.8736 (0.5093 -- 3.3006)  data: 0.1996 (0.0002 -- 2.2568)  max mem: 16413
Epoch: [161]  [140/160]  eta: 0:00:19  lr: 0.000004  min_lr: 0.000001  loss: 1.3130 (1.3565)  loss_scale: 16384.0000 (15280.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7321 (7.1631)  time: 1.0482 (0.5012 -- 4.4649)  data: 0.0016 (0.0003 -- 0.0101)  max mem: 16413
[2023-09-23 08:21:20,551] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:21:20,551] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:21:20,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:21:20,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [161]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.4009 (1.3545)  loss_scale: 32768.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3978 (7.0778)  time: 0.6483 (0.4941 -- 1.7850)  data: 0.0098 (0.0002 -- 0.1767)  max mem: 16413
Epoch: [161] Total time: 0:02:31 (0.9471 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.4009 (1.3918)  loss_scale: 32768.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3978 (7.0778)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1450 (0.1450)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3195 (2.3195 -- 2.3195)  data: 2.1300 (2.1300 -- 2.1300)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1428 (0.2816)  acc1: 100.0000 (94.9495)  acc5: 100.0000 (100.0000)  time: 0.4576 (0.1781 -- 2.3195)  data: 0.2634 (0.0003 -- 2.1300)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1428 (0.2681)  acc1: 100.0000 (95.7672)  acc5: 100.0000 (100.0000)  time: 0.2534 (0.1703 -- 0.9564)  data: 0.0627 (0.0001 -- 0.7634)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1608 (0.3355)  acc1: 100.0000 (93.3610)  acc5: 100.0000 (100.0000)  time: 0.2832 (0.1324 -- 0.9564)  data: 0.1004 (0.0001 -- 0.7634)  max mem: 16413
Val: Total time: 0:00:09 (0.3400 s / it)
* Acc@1 92.116 Acc@5 100.000 loss 0.350
Accuracy of the network on the 482 val images: 92.12%
Max accuracy: 93.36%
Epoch: [162]  [  0/160]  eta: 0:26:25  lr: 0.000004  min_lr: 0.000001  loss: 1.4609 (1.4609)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3144 (6.3144)  time: 9.9110 (9.9110 -- 9.9110)  data: 9.3793 (9.3793 -- 9.3793)  max mem: 16413
Epoch: [162]  [ 20/160]  eta: 0:02:56  lr: 0.000004  min_lr: 0.000001  loss: 1.4316 (1.4350)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8451 (7.3222)  time: 0.8279 (0.5213 -- 4.4877)  data: 0.2893 (0.0004 -- 3.9514)  max mem: 16413
[2023-09-23 08:22:10,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25946
[2023-09-23 08:22:10,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25946
[2023-09-23 08:22:10,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:22:10,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:22:10,319] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [162]  [ 40/160]  eta: 0:02:06  lr: 0.000004  min_lr: 0.000001  loss: 1.1828 (1.3634)  loss_scale: 16384.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2512 (6.9955)  time: 0.8407 (0.5194 -- 3.7905)  data: 0.1850 (0.0007 -- 3.2825)  max mem: 16413
Epoch: [162]  [ 60/160]  eta: 0:01:44  lr: 0.000004  min_lr: 0.000001  loss: 1.3225 (1.3406)  loss_scale: 16384.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0001 (7.2102)  time: 1.0155 (0.5077 -- 4.0457)  data: 0.0282 (0.0004 -- 0.3674)  max mem: 16413
[2023-09-23 08:22:58,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=150, lr=[1.0657741445929144e-06, 1.0657741445929144e-06, 1.1841934939921272e-06, 1.1841934939921272e-06, 1.315770548880141e-06, 1.315770548880141e-06, 1.4619672765334902e-06, 1.4619672765334902e-06, 1.6244080850372113e-06, 1.6244080850372113e-06, 1.804897872263568e-06, 1.804897872263568e-06, 2.0054420802928532e-06, 2.0054420802928532e-06, 2.22826897810317e-06, 2.22826897810317e-06, 2.4758544201146333e-06, 2.4758544201146333e-06, 2.750949355682926e-06, 2.750949355682926e-06, 3.0566103952032513e-06, 3.0566103952032513e-06, 3.3962337724480566e-06, 3.3962337724480566e-06, 3.7735930804978407e-06, 3.7735930804978407e-06, 4.192881200553156e-06, 4.192881200553156e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 08:22:58,671] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=17.091361480590184, CurrSamplesPerSec=21.819034956564938, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [162]  [ 80/160]  eta: 0:01:20  lr: 0.000004  min_lr: 0.000001  loss: 1.2363 (1.3360)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1272 (7.2829)  time: 0.9115 (0.5172 -- 4.3836)  data: 0.0017 (0.0004 -- 0.0054)  max mem: 16413
Epoch: [162]  [100/160]  eta: 0:01:00  lr: 0.000004  min_lr: 0.000001  loss: 1.3781 (1.3539)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3117 (7.2562)  time: 0.9684 (0.5154 -- 3.9489)  data: 0.0346 (0.0003 -- 0.6679)  max mem: 16413
Epoch: [162]  [120/160]  eta: 0:00:38  lr: 0.000004  min_lr: 0.000001  loss: 1.2954 (1.3455)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7585 (7.1588)  time: 0.8291 (0.5091 -- 4.3207)  data: 0.2345 (0.0003 -- 2.7394)  max mem: 16413
Epoch: [162]  [140/160]  eta: 0:00:19  lr: 0.000004  min_lr: 0.000001  loss: 1.2594 (1.3436)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0067 (7.1322)  time: 0.9631 (0.5276 -- 4.4695)  data: 0.4200 (0.0004 -- 3.9356)  max mem: 16413
[2023-09-23 08:24:06,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:24:06,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:24:06,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:24:06,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [162]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.2952 (1.3429)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8887 (7.0339)  time: 0.7064 (0.4937 -- 2.9592)  data: 0.1825 (0.0002 -- 2.4349)  max mem: 16413
Epoch: [162] Total time: 0:02:30 (0.9415 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.2952 (1.3783)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8887 (7.0339)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1195 (0.1195)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4793 (2.4793 -- 2.4793)  data: 2.2618 (2.2618 -- 2.2618)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1655 (0.3960)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4681 (0.1787 -- 2.4793)  data: 0.2768 (0.0004 -- 2.2618)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1655 (0.3240)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2536 (0.1682 -- 0.9690)  data: 0.0677 (0.0001 -- 0.7682)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2040 (0.4036)  acc1: 100.0000 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2832 (0.1329 -- 0.9690)  data: 0.1049 (0.0001 -- 0.7682)  max mem: 16413
Val: Total time: 0:00:09 (0.3444 s / it)
* Acc@1 92.531 Acc@5 99.793 loss 0.365
Accuracy of the network on the 482 val images: 92.53%
Max accuracy: 93.36%
Epoch: [163]  [  0/160]  eta: 0:19:58  lr: 0.000004  min_lr: 0.000001  loss: 0.9014 (0.9014)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5645 (7.5645)  time: 7.4915 (7.4915 -- 7.4915)  data: 6.1987 (6.1987 -- 6.1987)  max mem: 16413
Epoch: [163]  [ 20/160]  eta: 0:02:50  lr: 0.000004  min_lr: 0.000001  loss: 1.4051 (1.3756)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7120 (7.2232)  time: 0.9079 (0.5147 -- 3.8606)  data: 0.3655 (0.0004 -- 3.3181)  max mem: 16413
Epoch: [163]  [ 40/160]  eta: 0:02:12  lr: 0.000004  min_lr: 0.000001  loss: 1.4501 (1.3793)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6826 (6.9462)  time: 0.9861 (0.5133 -- 3.3487)  data: 0.1753 (0.0002 -- 1.6585)  max mem: 16413
[2023-09-23 08:25:11,723] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26129
[2023-09-23 08:25:11,723] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:25:11,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26129
[2023-09-23 08:25:11,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:25:11,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [163]  [ 60/160]  eta: 0:01:48  lr: 0.000004  min_lr: 0.000001  loss: 1.2427 (1.3647)  loss_scale: 16384.0000 (29544.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7269 (7.0860)  time: 1.0335 (0.5305 -- 4.3449)  data: 0.4591 (0.0004 -- 3.8212)  max mem: 16413
Epoch: [163]  [ 80/160]  eta: 0:01:22  lr: 0.000004  min_lr: 0.000001  loss: 1.2667 (1.3539)  loss_scale: 16384.0000 (26295.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9560 (7.1461)  time: 0.8826 (0.5191 -- 3.7321)  data: 0.2887 (0.0002 -- 3.2104)  max mem: 16413
[2023-09-23 08:25:52,443] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26175
[2023-09-23 08:25:52,443] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26175
[2023-09-23 08:25:52,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:25:52,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:25:52,443] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [163]  [100/160]  eta: 0:00:58  lr: 0.000004  min_lr: 0.000001  loss: 1.5085 (1.3772)  loss_scale: 16384.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6155 (7.1829)  time: 0.7711 (0.5243 -- 2.1247)  data: 0.1468 (0.0002 -- 1.4638)  max mem: 16413
Epoch: [163]  [120/160]  eta: 0:00:39  lr: 0.000004  min_lr: 0.000001  loss: 1.4265 (1.3888)  loss_scale: 8192.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3836 (7.0274)  time: 0.9761 (0.5326 -- 4.1525)  data: 0.2945 (0.0003 -- 3.6299)  max mem: 16413
Epoch: [163]  [140/160]  eta: 0:00:19  lr: 0.000004  min_lr: 0.000001  loss: 1.4370 (1.3923)  loss_scale: 8192.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1757 (7.1384)  time: 1.0704 (0.5190 -- 4.5543)  data: 0.5347 (0.0003 -- 4.0310)  max mem: 16413
Epoch: [163]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.3925 (1.3901)  loss_scale: 8192.0000 (18073.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9471 (7.0624)  time: 0.6706 (0.4954 -- 3.0540)  data: 0.1537 (0.0001 -- 2.5543)  max mem: 16413
Epoch: [163] Total time: 0:02:32 (0.9560 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.3925 (1.3904)  loss_scale: 8192.0000 (18073.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9471 (7.0624)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1534 (0.1534)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4272 (2.4272 -- 2.4272)  data: 2.2379 (2.2379 -- 2.2379)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1436 (0.3276)  acc1: 100.0000 (94.9495)  acc5: 100.0000 (100.0000)  time: 0.4739 (0.1756 -- 2.4272)  data: 0.2817 (0.0002 -- 2.2379)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1609 (0.3375)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2574 (0.1675 -- 1.0612)  data: 0.0687 (0.0001 -- 0.8552)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3254 (0.4114)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (100.0000)  time: 0.2879 (0.1325 -- 1.0612)  data: 0.1062 (0.0001 -- 0.8552)  max mem: 16413
Val: Total time: 0:00:09 (0.3449 s / it)
* Acc@1 91.909 Acc@5 100.000 loss 0.376
Accuracy of the network on the 482 val images: 91.91%
Max accuracy: 93.36%
Epoch: [164]  [  0/160]  eta: 0:21:02  lr: 0.000004  min_lr: 0.000001  loss: 1.7470 (1.7470)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5417 (6.5417)  time: 7.8910 (7.8910 -- 7.8910)  data: 7.3725 (7.3725 -- 7.3725)  max mem: 16413
Epoch: [164]  [ 20/160]  eta: 0:02:52  lr: 0.000004  min_lr: 0.000001  loss: 1.2764 (1.3661)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8930 (5.9986)  time: 0.9003 (0.5191 -- 3.4562)  data: 0.3514 (0.0009 -- 2.9169)  max mem: 16413
Epoch: [164]  [ 40/160]  eta: 0:02:10  lr: 0.000004  min_lr: 0.000001  loss: 1.3453 (1.4066)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6645 (6.6306)  time: 0.9378 (0.5181 -- 3.0598)  data: 0.4023 (0.0004 -- 2.5587)  max mem: 16413
Epoch: [164]  [ 60/160]  eta: 0:01:42  lr: 0.000004  min_lr: 0.000001  loss: 1.2346 (1.3910)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9847 (6.8703)  time: 0.8941 (0.5293 -- 3.6549)  data: 0.3500 (0.0004 -- 3.1390)  max mem: 16413
[2023-09-23 08:28:06,767] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:28:06,767] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:28:06,768] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:28:06,768] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [164]  [ 80/160]  eta: 0:01:22  lr: 0.000004  min_lr: 0.000001  loss: 1.3431 (1.3889)  loss_scale: 16384.0000 (9911.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3947 (7.1292)  time: 1.0434 (0.5152 -- 4.6798)  data: 0.5056 (0.0005 -- 4.1678)  max mem: 16413
Epoch: [164]  [100/160]  eta: 0:00:59  lr: 0.000004  min_lr: 0.000001  loss: 1.3797 (1.3916)  loss_scale: 16384.0000 (11193.0297)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8902 (7.3298)  time: 0.7973 (0.5220 -- 3.8879)  data: 0.2535 (0.0002 -- 3.3607)  max mem: 16413
Epoch: [164]  [120/160]  eta: 0:00:38  lr: 0.000004  min_lr: 0.000001  loss: 1.3308 (1.3881)  loss_scale: 16384.0000 (12051.0413)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7516 (7.2815)  time: 0.9085 (0.5077 -- 4.7339)  data: 0.3115 (0.0002 -- 4.2252)  max mem: 16413
Epoch: [164]  [140/160]  eta: 0:00:19  lr: 0.000004  min_lr: 0.000001  loss: 1.5423 (1.3966)  loss_scale: 16384.0000 (12665.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2751 (7.1916)  time: 0.8891 (0.5266 -- 2.7610)  data: 0.1835 (0.0002 -- 2.2295)  max mem: 16413
Epoch: [164]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.3416 (1.3915)  loss_scale: 16384.0000 (13107.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2987 (7.1539)  time: 0.7491 (0.4928 -- 3.7530)  data: 0.2278 (0.0001 -- 3.2274)  max mem: 16413
Epoch: [164] Total time: 0:02:29 (0.9359 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.3416 (1.3730)  loss_scale: 16384.0000 (13107.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2987 (7.1539)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1560 (0.1560)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4940 (2.4940 -- 2.4940)  data: 2.3110 (2.3110 -- 2.3110)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1469 (0.3419)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (98.9899)  time: 0.4688 (0.1753 -- 2.4940)  data: 0.2773 (0.0003 -- 2.3110)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1469 (0.2985)  acc1: 100.0000 (95.2381)  acc5: 100.0000 (99.4709)  time: 0.2553 (0.1682 -- 0.9214)  data: 0.0650 (0.0001 -- 0.7238)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1766 (0.3545)  acc1: 100.0000 (92.9461)  acc5: 100.0000 (99.5851)  time: 0.2884 (0.1328 -- 1.0296)  data: 0.1076 (0.0001 -- 0.8568)  max mem: 16413
Val: Total time: 0:00:09 (0.3497 s / it)
* Acc@1 93.361 Acc@5 99.793 loss 0.337
Accuracy of the network on the 482 val images: 93.36%
[2023-09-23 08:29:39,168] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 08:29:39,170] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 08:29:39,170] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 08:29:39,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 08:29:40,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 08:29:40,592] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 93.36%
Epoch: [165]  [  0/160]  eta: 0:22:23  lr: 0.000004  min_lr: 0.000001  loss: 1.7563 (1.7563)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5405 (5.5405)  time: 8.3980 (8.3980 -- 8.3980)  data: 5.6637 (5.6637 -- 5.6637)  max mem: 16413
Epoch: [165]  [ 20/160]  eta: 0:02:57  lr: 0.000004  min_lr: 0.000001  loss: 1.2392 (1.2960)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1729 (6.8524)  time: 0.9090 (0.5275 -- 4.3901)  data: 0.0016 (0.0004 -- 0.0050)  max mem: 16413
[2023-09-23 08:30:19,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:30:19,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:30:19,481] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:30:19,481] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [165]  [ 40/160]  eta: 0:02:10  lr: 0.000004  min_lr: 0.000001  loss: 1.4603 (1.3635)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5669 (6.7742)  time: 0.9049 (0.5300 -- 4.0160)  data: 0.0444 (0.0003 -- 0.5663)  max mem: 16413
[2023-09-23 08:30:32,100] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26446
[2023-09-23 08:30:32,100] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26446
[2023-09-23 08:30:32,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:30:32,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:30:32,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [165]  [ 60/160]  eta: 0:01:46  lr: 0.000004  min_lr: 0.000001  loss: 1.4722 (1.4047)  loss_scale: 16384.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9699 (7.2033)  time: 1.0151 (0.5124 -- 4.1726)  data: 0.0013 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [165]  [ 80/160]  eta: 0:01:21  lr: 0.000004  min_lr: 0.000001  loss: 1.2501 (1.3716)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5383 (7.0300)  time: 0.8552 (0.5205 -- 4.8532)  data: 0.0012 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [165]  [100/160]  eta: 0:01:00  lr: 0.000004  min_lr: 0.000001  loss: 1.3623 (1.3733)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7433 (7.0321)  time: 1.0024 (0.5107 -- 4.1071)  data: 0.0022 (0.0002 -- 0.0172)  max mem: 16413
Epoch: [165]  [120/160]  eta: 0:00:39  lr: 0.000004  min_lr: 0.000001  loss: 1.4219 (1.3839)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9617 (7.0676)  time: 0.8239 (0.5162 -- 3.3591)  data: 0.0019 (0.0003 -- 0.0083)  max mem: 16413
Epoch: [165]  [140/160]  eta: 0:00:19  lr: 0.000003  min_lr: 0.000001  loss: 1.3960 (1.3856)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5455 (7.2022)  time: 0.9416 (0.5140 -- 3.3850)  data: 0.0630 (0.0004 -- 0.8807)  max mem: 16413
Epoch: [165]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.4020 (1.3752)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9000 (7.1146)  time: 0.7498 (0.4916 -- 3.7366)  data: 0.0009 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [165] Total time: 0:02:31 (0.9493 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.4020 (1.3955)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9000 (7.1146)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1494 (0.1494)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4636 (2.4636 -- 2.4636)  data: 2.2726 (2.2726 -- 2.2726)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1469 (0.3516)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4726 (0.1831 -- 2.4636)  data: 0.2794 (0.0003 -- 2.2726)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1469 (0.3100)  acc1: 100.0000 (95.2381)  acc5: 100.0000 (100.0000)  time: 0.2558 (0.1673 -- 1.0003)  data: 0.0687 (0.0001 -- 0.7868)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1856 (0.3780)  acc1: 100.0000 (92.9461)  acc5: 100.0000 (100.0000)  time: 0.2735 (0.1326 -- 1.0003)  data: 0.0947 (0.0001 -- 0.7868)  max mem: 16413
Val: Total time: 0:00:09 (0.3369 s / it)
* Acc@1 93.568 Acc@5 99.585 loss 0.348
Accuracy of the network on the 482 val images: 93.57%
[2023-09-23 08:32:21,980] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 08:32:21,981] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 08:32:21,981] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 08:32:21,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 08:32:23,329] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 08:32:23,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 93.57%
Epoch: [166]  [  0/160]  eta: 0:18:09  lr: 0.000003  min_lr: 0.000001  loss: 1.6761 (1.6761)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0443 (5.0443)  time: 6.8080 (6.8080 -- 6.8080)  data: 5.1346 (5.1346 -- 5.1346)  max mem: 16413
[2023-09-23 08:32:43,379] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:32:43,379] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:32:43,380] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:32:43,380] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [166]  [ 20/160]  eta: 0:03:02  lr: 0.000003  min_lr: 0.000001  loss: 1.5013 (1.4457)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9735 (6.7728)  time: 1.0297 (0.5047 -- 4.4164)  data: 0.0755 (0.0003 -- 0.9021)  max mem: 16413
Epoch: [166]  [ 40/160]  eta: 0:02:12  lr: 0.000003  min_lr: 0.000001  loss: 1.4409 (1.4129)  loss_scale: 32768.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2155 (7.1329)  time: 0.8893 (0.5181 -- 3.7822)  data: 0.1354 (0.0003 -- 1.9923)  max mem: 16413
Epoch: [166]  [ 60/160]  eta: 0:01:44  lr: 0.000003  min_lr: 0.000001  loss: 1.2245 (1.3762)  loss_scale: 32768.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3475 (7.1190)  time: 0.9426 (0.5140 -- 3.9848)  data: 0.3963 (0.0005 -- 3.4391)  max mem: 16413
Epoch: [166]  [ 80/160]  eta: 0:01:21  lr: 0.000003  min_lr: 0.000001  loss: 1.2139 (1.3572)  loss_scale: 32768.0000 (29733.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0100 (7.1312)  time: 0.9492 (0.5174 -- 4.4999)  data: 0.4098 (0.0003 -- 3.9583)  max mem: 16413
Epoch: [166]  [100/160]  eta: 0:00:58  lr: 0.000003  min_lr: 0.000001  loss: 1.4797 (1.3782)  loss_scale: 32768.0000 (30334.7327)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3748 (7.1375)  time: 0.8049 (0.5137 -- 3.6966)  data: 0.2596 (0.0007 -- 3.1689)  max mem: 16413
[2023-09-23 08:34:04,069] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26663
[2023-09-23 08:34:04,070] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26663
[2023-09-23 08:34:04,070] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:34:04,070] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 08:34:04,070] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [166]  [120/160]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000001  loss: 1.3269 (1.3788)  loss_scale: 16384.0000 (28299.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7621 (7.1235)  time: 1.0171 (0.5185 -- 4.2374)  data: 0.4714 (0.0002 -- 3.7181)  max mem: 16413
Epoch: [166]  [140/160]  eta: 0:00:19  lr: 0.000003  min_lr: 0.000001  loss: 1.3272 (1.3793)  loss_scale: 16384.0000 (26609.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2988 (7.1536)  time: 0.8534 (0.5314 -- 3.1624)  data: 0.2436 (0.0002 -- 2.6386)  max mem: 16413
Epoch: [166]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.4429 (1.3872)  loss_scale: 16384.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0192 (7.0903)  time: 0.7312 (0.4947 -- 3.7801)  data: 0.2077 (0.0002 -- 3.2616)  max mem: 16413
Epoch: [166] Total time: 0:02:30 (0.9415 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.4429 (1.3971)  loss_scale: 16384.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0192 (7.0903)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1654 (0.1654)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3793 (2.3793 -- 2.3793)  data: 2.1935 (2.1935 -- 2.1935)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1474 (0.3170)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4604 (0.1742 -- 2.3793)  data: 0.2700 (0.0003 -- 2.1935)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1495 (0.3010)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2526 (0.1678 -- 0.9605)  data: 0.0635 (0.0001 -- 0.7660)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3328 (0.3720)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (100.0000)  time: 0.2872 (0.1327 -- 1.0179)  data: 0.1057 (0.0001 -- 0.8500)  max mem: 16413
Val: Total time: 0:00:09 (0.3444 s / it)
* Acc@1 92.531 Acc@5 99.793 loss 0.358
Accuracy of the network on the 482 val images: 92.53%
Max accuracy: 93.57%
Epoch: [167]  [  0/160]  eta: 0:18:45  lr: 0.000003  min_lr: 0.000001  loss: 1.6092 (1.6092)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1228 (6.1228)  time: 7.0319 (7.0319 -- 7.0319)  data: 6.4795 (6.4795 -- 6.4795)  max mem: 16413
Epoch: [167]  [ 20/160]  eta: 0:02:51  lr: 0.000003  min_lr: 0.000001  loss: 1.4024 (1.4035)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3573 (6.9792)  time: 0.9323 (0.5167 -- 3.3655)  data: 0.3666 (0.0007 -- 2.8546)  max mem: 16413
Epoch: [167]  [ 40/160]  eta: 0:02:07  lr: 0.000003  min_lr: 0.000001  loss: 1.2791 (1.3895)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0117 (7.3248)  time: 0.8908 (0.5264 -- 4.5468)  data: 0.2983 (0.0004 -- 4.0316)  max mem: 16413
Epoch: [167]  [ 60/160]  eta: 0:01:41  lr: 0.000003  min_lr: 0.000001  loss: 1.4690 (1.4217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1379 (7.4330)  time: 0.9145 (0.5204 -- 4.1556)  data: 0.3655 (0.0003 -- 3.6581)  max mem: 16413
[2023-09-23 08:36:18,446] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:36:18,448] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:36:18,450] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:36:18,450] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [167]  [ 80/160]  eta: 0:01:21  lr: 0.000003  min_lr: 0.000001  loss: 1.3827 (1.4129)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2121 (7.3169)  time: 1.0229 (0.5098 -- 4.1817)  data: 0.4822 (0.0004 -- 3.6656)  max mem: 16413
[2023-09-23 08:36:43,711] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26819
[2023-09-23 08:36:43,711] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26819
[2023-09-23 08:36:43,711] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:36:43,711] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:36:43,711] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [167]  [100/160]  eta: 0:00:59  lr: 0.000003  min_lr: 0.000001  loss: 1.4292 (1.4185)  loss_scale: 32768.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2051 (7.2106)  time: 0.9306 (0.5139 -- 3.8414)  data: 0.2119 (0.0004 -- 2.7372)  max mem: 16413
Epoch: [167]  [120/160]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000001  loss: 1.4240 (1.4202)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4830 (7.2760)  time: 1.0046 (0.5125 -- 5.3010)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [167]  [140/160]  eta: 0:00:19  lr: 0.000003  min_lr: 0.000001  loss: 1.4603 (1.4258)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1592 (7.2974)  time: 0.9668 (0.5273 -- 4.4599)  data: 0.0016 (0.0001 -- 0.0034)  max mem: 16413
Epoch: [167]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.3938 (1.4172)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0305 (7.2621)  time: 0.6156 (0.4934 -- 2.5897)  data: 0.0007 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [167] Total time: 0:02:32 (0.9504 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.3938 (1.4033)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0305 (7.2621)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1828 (0.1828)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4869 (2.4869 -- 2.4869)  data: 2.2912 (2.2912 -- 2.2912)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1555 (0.3130)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4728 (0.1785 -- 2.4869)  data: 0.2825 (0.0003 -- 2.2912)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1851 (0.3091)  acc1: 88.8889 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2515 (0.1685 -- 0.9974)  data: 0.0657 (0.0001 -- 0.8103)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2691 (0.3626)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (100.0000)  time: 0.2767 (0.1333 -- 0.9974)  data: 0.0981 (0.0001 -- 0.8103)  max mem: 16413
Val: Total time: 0:00:09 (0.3392 s / it)
* Acc@1 92.116 Acc@5 100.000 loss 0.344
Accuracy of the network on the 482 val images: 92.12%
Max accuracy: 93.57%
Epoch: [168]  [  0/160]  eta: 0:24:39  lr: 0.000003  min_lr: 0.000001  loss: 1.7036 (1.7036)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3499 (6.3499)  time: 9.2496 (9.2496 -- 9.2496)  data: 8.7420 (8.7420 -- 8.7420)  max mem: 16413
Epoch: [168]  [ 20/160]  eta: 0:02:46  lr: 0.000003  min_lr: 0.000001  loss: 1.2971 (1.3543)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1133 (7.1914)  time: 0.7895 (0.5194 -- 3.0424)  data: 0.2273 (0.0001 -- 2.4230)  max mem: 16413
Epoch: [168]  [ 40/160]  eta: 0:02:12  lr: 0.000003  min_lr: 0.000001  loss: 1.3642 (1.3561)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2202 (6.8505)  time: 1.0190 (0.5160 -- 3.9104)  data: 0.0873 (0.0003 -- 0.9863)  max mem: 16413
Epoch: [168]  [ 60/160]  eta: 0:01:47  lr: 0.000003  min_lr: 0.000001  loss: 1.3970 (1.3903)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1213 (7.1784)  time: 1.0187 (0.5088 -- 5.6164)  data: 0.0015 (0.0006 -- 0.0054)  max mem: 16413
[2023-09-23 08:38:56,058] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:38:56,061] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:38:56,062] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:38:56,062] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:39:05,107] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26959
[2023-09-23 08:39:05,107] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26959
[2023-09-23 08:39:05,107] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:39:05,107] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:39:05,107] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [168]  [ 80/160]  eta: 0:01:23  lr: 0.000003  min_lr: 0.000001  loss: 1.4102 (1.4019)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1052 (7.0297)  time: 0.9124 (0.5130 -- 3.7714)  data: 0.0015 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [168]  [100/160]  eta: 0:00:59  lr: 0.000003  min_lr: 0.000001  loss: 1.4204 (1.3950)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8294 (7.0127)  time: 0.8469 (0.5120 -- 4.6669)  data: 0.0011 (0.0003 -- 0.0029)  max mem: 16413
[2023-09-23 08:39:40,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=156, lr=[7.506812626750167e-07, 7.506812626750167e-07, 8.340902918611298e-07, 8.340902918611298e-07, 9.267669909568107e-07, 9.267669909568107e-07, 1.029741101063123e-06, 1.029741101063123e-06, 1.1441567789590256e-06, 1.1441567789590256e-06, 1.271285309954473e-06, 1.271285309954473e-06, 1.4125392332827476e-06, 1.4125392332827476e-06, 1.5694880369808305e-06, 1.5694880369808305e-06, 1.7438755966453672e-06, 1.7438755966453672e-06, 1.9376395518281855e-06, 1.9376395518281855e-06, 2.152932835364651e-06, 2.152932835364651e-06, 2.392147594849612e-06, 2.392147594849612e-06, 2.6579417720551244e-06, 2.6579417720551244e-06, 2.9532686356168047e-06, 2.9532686356168047e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 08:39:40,796] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=17.108666956223832, CurrSamplesPerSec=22.88309770530918, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [168]  [120/160]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000001  loss: 1.3672 (1.3944)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8642 (7.0032)  time: 0.8992 (0.5222 -- 3.0114)  data: 0.1295 (0.0002 -- 1.8810)  max mem: 16413
[2023-09-23 08:40:00,518] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27019
[2023-09-23 08:40:00,518] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27019
[2023-09-23 08:40:00,519] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:40:00,519] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:40:00,519] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [168]  [140/160]  eta: 0:00:19  lr: 0.000003  min_lr: 0.000001  loss: 1.2525 (1.3806)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5290 (6.8755)  time: 0.8638 (0.5335 -- 2.8659)  data: 0.2288 (0.0002 -- 2.3002)  max mem: 16413
Epoch: [168]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.4502 (1.3863)  loss_scale: 8192.0000 (16435.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0927 (6.9818)  time: 0.7192 (0.4954 -- 3.1053)  data: 0.1975 (0.0002 -- 2.5629)  max mem: 16413
Epoch: [168] Total time: 0:02:30 (0.9380 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.4502 (1.3704)  loss_scale: 8192.0000 (16435.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0927 (6.9818)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4325 (2.4325 -- 2.4325)  data: 2.2310 (2.2310 -- 2.2310)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1355 (0.3536)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4685 (0.1795 -- 2.4325)  data: 0.2784 (0.0003 -- 2.2310)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1462 (0.3142)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2576 (0.1682 -- 1.0151)  data: 0.0706 (0.0001 -- 0.8248)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2744 (0.3713)  acc1: 88.8889 (92.5311)  acc5: 100.0000 (99.5851)  time: 0.2804 (0.1326 -- 1.0151)  data: 0.1020 (0.0001 -- 0.8248)  max mem: 16413
Val: Total time: 0:00:09 (0.3406 s / it)
* Acc@1 92.946 Acc@5 99.378 loss 0.347
Accuracy of the network on the 482 val images: 92.95%
Max accuracy: 93.57%
Epoch: [169]  [  0/160]  eta: 0:17:06  lr: 0.000003  min_lr: 0.000001  loss: 1.4315 (1.4315)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7533 (6.7533)  time: 6.4184 (6.4184 -- 6.4184)  data: 5.8953 (5.8953 -- 5.8953)  max mem: 16413
Epoch: [169]  [ 20/160]  eta: 0:02:45  lr: 0.000003  min_lr: 0.000001  loss: 1.4673 (1.4465)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7113 (7.2295)  time: 0.9223 (0.5197 -- 2.0522)  data: 0.3125 (0.0007 -- 1.5274)  max mem: 16413
Epoch: [169]  [ 40/160]  eta: 0:02:08  lr: 0.000003  min_lr: 0.000001  loss: 1.3482 (1.3610)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1162 (7.1171)  time: 0.9531 (0.5265 -- 3.0622)  data: 0.3058 (0.0002 -- 2.5425)  max mem: 16413
Epoch: [169]  [ 60/160]  eta: 0:01:43  lr: 0.000003  min_lr: 0.000001  loss: 1.5715 (1.4105)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4195 (7.4101)  time: 0.9757 (0.5152 -- 4.0879)  data: 0.3208 (0.0007 -- 3.5651)  max mem: 16413
Epoch: [169]  [ 80/160]  eta: 0:01:19  lr: 0.000003  min_lr: 0.000001  loss: 1.3977 (1.4189)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8389 (7.1684)  time: 0.8628 (0.5205 -- 2.4499)  data: 0.1904 (0.0004 -- 1.9229)  max mem: 16413
Epoch: [169]  [100/160]  eta: 0:00:58  lr: 0.000003  min_lr: 0.000001  loss: 1.3604 (1.4054)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5436 (7.4199)  time: 0.9274 (0.5087 -- 4.0496)  data: 0.3554 (0.0002 -- 3.5311)  max mem: 16413
[2023-09-23 08:42:11,318] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:42:11,318] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:42:11,320] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:42:11,320] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [169]  [120/160]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000001  loss: 1.3146 (1.4040)  loss_scale: 16384.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7115 (7.3115)  time: 0.9789 (0.5190 -- 4.1304)  data: 0.4394 (0.0004 -- 3.6044)  max mem: 16413
Epoch: [169]  [140/160]  eta: 0:00:19  lr: 0.000003  min_lr: 0.000001  loss: 1.3528 (1.3969)  loss_scale: 16384.0000 (10109.2766)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4969 (7.2679)  time: 0.9057 (0.5174 -- 2.4228)  data: 0.1723 (0.0003 -- 1.6694)  max mem: 16413
Epoch: [169]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.3251 (1.3878)  loss_scale: 16384.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7171 (7.2416)  time: 0.7616 (0.4940 -- 2.3509)  data: 0.0917 (0.0002 -- 1.7360)  max mem: 16413
Epoch: [169] Total time: 0:02:31 (0.9479 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.3251 (1.3807)  loss_scale: 16384.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7171 (7.2416)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1725 (0.1725)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4051 (2.4051 -- 2.4051)  data: 2.2133 (2.2133 -- 2.2133)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1725 (0.3548)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4705 (0.1771 -- 2.4051)  data: 0.2769 (0.0004 -- 2.2133)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2023 (0.3458)  acc1: 88.8889 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2577 (0.1679 -- 1.0504)  data: 0.0690 (0.0001 -- 0.8071)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3575 (0.3930)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (100.0000)  time: 0.2871 (0.1342 -- 1.0504)  data: 0.1056 (0.0001 -- 0.8071)  max mem: 16413
Val: Total time: 0:00:09 (0.3448 s / it)
* Acc@1 91.701 Acc@5 99.793 loss 0.357
Accuracy of the network on the 482 val images: 91.70%
Max accuracy: 93.57%
Epoch: [170]  [  0/160]  eta: 0:26:10  lr: 0.000003  min_lr: 0.000001  loss: 1.3752 (1.3752)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1995 (7.1995)  time: 9.8136 (9.8136 -- 9.8136)  data: 9.2913 (9.2913 -- 9.2913)  max mem: 16413
Epoch: [170]  [ 20/160]  eta: 0:02:48  lr: 0.000003  min_lr: 0.000001  loss: 1.2505 (1.3139)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7231 (6.4356)  time: 0.7767 (0.5209 -- 3.2294)  data: 0.2343 (0.0007 -- 2.7185)  max mem: 16413
Epoch: [170]  [ 40/160]  eta: 0:02:06  lr: 0.000003  min_lr: 0.000001  loss: 1.2552 (1.3054)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5709 (6.7879)  time: 0.8932 (0.5145 -- 3.3489)  data: 0.2426 (0.0001 -- 2.8134)  max mem: 16413
Epoch: [170]  [ 60/160]  eta: 0:01:48  lr: 0.000003  min_lr: 0.000001  loss: 1.4955 (1.3608)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5983 (7.2505)  time: 1.1482 (0.5029 -- 5.6859)  data: 0.5919 (0.0004 -- 5.1644)  max mem: 16413
[2023-09-23 08:44:25,319] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:44:25,319] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:44:25,323] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:44:25,324] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [170]  [ 80/160]  eta: 0:01:21  lr: 0.000003  min_lr: 0.000001  loss: 1.4183 (1.3779)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5602 (7.1103)  time: 0.8070 (0.5186 -- 4.3364)  data: 0.2669 (0.0002 -- 3.7569)  max mem: 16413
Epoch: [170]  [100/160]  eta: 0:00:59  lr: 0.000003  min_lr: 0.000001  loss: 1.3204 (1.3682)  loss_scale: 32768.0000 (20439.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1814 (7.1613)  time: 0.9149 (0.5248 -- 3.6673)  data: 0.3704 (0.0005 -- 3.1483)  max mem: 16413
[2023-09-23 08:44:52,917] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27306
[2023-09-23 08:44:52,917] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27306
[2023-09-23 08:44:52,917] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:44:52,917] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:44:52,917] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [170]  [120/160]  eta: 0:00:39  lr: 0.000003  min_lr: 0.000001  loss: 1.4747 (1.3775)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4549 (7.1646)  time: 0.8864 (0.5215 -- 4.3879)  data: 0.3405 (0.0003 -- 3.8678)  max mem: 16413
Epoch: [170]  [140/160]  eta: 0:00:19  lr: 0.000003  min_lr: 0.000001  loss: 1.3318 (1.3747)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4069 (7.0545)  time: 1.0014 (0.5168 -- 4.5230)  data: 0.4547 (0.0003 -- 3.9875)  max mem: 16413
Epoch: [170]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.3917 (1.3739)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5129 (7.2698)  time: 0.7143 (0.4951 -- 3.9293)  data: 0.2002 (0.0001 -- 3.4027)  max mem: 16413
Epoch: [170] Total time: 0:02:32 (0.9510 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.3917 (1.3700)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5129 (7.2698)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1422 (0.1422)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3782 (2.3782 -- 2.3782)  data: 2.1793 (2.1793 -- 2.1793)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1289 (0.3295)  acc1: 100.0000 (94.9495)  acc5: 100.0000 (100.0000)  time: 0.4832 (0.1785 -- 2.3782)  data: 0.2916 (0.0003 -- 2.1793)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1289 (0.3059)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (100.0000)  time: 0.2613 (0.1712 -- 1.2065)  data: 0.0736 (0.0001 -- 1.0224)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2812 (0.3602)  acc1: 88.8889 (92.9461)  acc5: 100.0000 (100.0000)  time: 0.2838 (0.1318 -- 1.2065)  data: 0.1054 (0.0001 -- 1.0224)  max mem: 16413
Val: Total time: 0:00:09 (0.3417 s / it)
* Acc@1 93.154 Acc@5 99.793 loss 0.343
Accuracy of the network on the 482 val images: 93.15%
Max accuracy: 93.57%
Epoch: [171]  [  0/160]  eta: 0:19:49  lr: 0.000003  min_lr: 0.000001  loss: 1.5949 (1.5949)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4058 (9.4058)  time: 7.4347 (7.4347 -- 7.4347)  data: 6.1671 (6.1671 -- 6.1671)  max mem: 16413
Epoch: [171]  [ 20/160]  eta: 0:02:48  lr: 0.000003  min_lr: 0.000001  loss: 1.3410 (1.3528)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7918 (7.3509)  time: 0.8885 (0.5131 -- 3.4240)  data: 0.2277 (0.0004 -- 2.9012)  max mem: 16413
Epoch: [171]  [ 40/160]  eta: 0:02:08  lr: 0.000003  min_lr: 0.000001  loss: 1.4400 (1.3572)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8923 (7.6974)  time: 0.9348 (0.5270 -- 3.6279)  data: 0.3328 (0.0002 -- 3.0890)  max mem: 16413
[2023-09-23 08:46:47,763] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27417
[2023-09-23 08:46:47,763] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27417
[2023-09-23 08:46:47,763] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:46:47,763] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:46:47,763] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [171]  [ 60/160]  eta: 0:01:42  lr: 0.000002  min_lr: 0.000001  loss: 1.4095 (1.3338)  loss_scale: 16384.0000 (15846.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0473 (7.3133)  time: 0.9383 (0.5236 -- 3.6150)  data: 0.2820 (0.0004 -- 3.0923)  max mem: 16413
Epoch: [171]  [ 80/160]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000001  loss: 1.3478 (1.3328)  loss_scale: 8192.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2518 (7.1777)  time: 0.8502 (0.5210 -- 3.9419)  data: 0.0021 (0.0003 -- 0.0077)  max mem: 16413
Epoch: [171]  [100/160]  eta: 0:00:59  lr: 0.000002  min_lr: 0.000001  loss: 1.4140 (1.3457)  loss_scale: 8192.0000 (12815.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9994 (7.1860)  time: 1.0496 (0.5172 -- 5.0368)  data: 0.0011 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [171]  [120/160]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000001  loss: 1.5388 (1.3632)  loss_scale: 8192.0000 (12051.0413)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9085 (7.2227)  time: 0.7437 (0.5158 -- 2.1557)  data: 0.0831 (0.0005 -- 1.5258)  max mem: 16413
Epoch: [171]  [140/160]  eta: 0:00:19  lr: 0.000002  min_lr: 0.000001  loss: 1.2834 (1.3564)  loss_scale: 8192.0000 (11503.6596)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7245 (7.0878)  time: 0.9384 (0.5226 -- 2.9942)  data: 0.1682 (0.0003 -- 2.0053)  max mem: 16413
Epoch: [171]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.3532 (1.3612)  loss_scale: 8192.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0676 (7.1107)  time: 0.7866 (0.4920 -- 3.6581)  data: 0.2262 (0.0002 -- 3.1247)  max mem: 16413
Epoch: [171] Total time: 0:02:29 (0.9345 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.3532 (1.3638)  loss_scale: 8192.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0676 (7.1107)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1519 (0.1519)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3733 (2.3733 -- 2.3733)  data: 2.1690 (2.1690 -- 2.1690)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1516 (0.3169)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4679 (0.1754 -- 2.3733)  data: 0.2726 (0.0003 -- 2.1690)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1433 (0.3139)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2592 (0.1682 -- 1.0327)  data: 0.0679 (0.0001 -- 0.8248)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3732 (0.3812)  acc1: 88.8889 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2792 (0.1327 -- 1.0327)  data: 0.0971 (0.0001 -- 0.8248)  max mem: 16413
Val: Total time: 0:00:09 (0.3384 s / it)
* Acc@1 93.361 Acc@5 99.585 loss 0.340
Accuracy of the network on the 482 val images: 93.36%
Max accuracy: 93.57%
Epoch: [172]  [  0/160]  eta: 0:22:33  lr: 0.000002  min_lr: 0.000001  loss: 1.4192 (1.4192)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5109 (8.5109)  time: 8.4616 (8.4616 -- 8.4616)  data: 7.9240 (7.9240 -- 7.9240)  max mem: 16413
Epoch: [172]  [ 20/160]  eta: 0:02:47  lr: 0.000002  min_lr: 0.000001  loss: 1.4088 (1.4265)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2565 (7.6492)  time: 0.8329 (0.5203 -- 3.3310)  data: 0.1042 (0.0003 -- 1.4973)  max mem: 16413
[2023-09-23 08:48:57,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:48:57,142] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:48:57,144] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:48:57,144] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [172]  [ 40/160]  eta: 0:02:05  lr: 0.000002  min_lr: 0.000001  loss: 1.3432 (1.4162)  loss_scale: 16384.0000 (11189.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0609 (7.1647)  time: 0.8838 (0.5237 -- 3.6267)  data: 0.0018 (0.0003 -- 0.0085)  max mem: 16413
Epoch: [172]  [ 60/160]  eta: 0:01:42  lr: 0.000002  min_lr: 0.000001  loss: 1.2597 (1.3711)  loss_scale: 16384.0000 (12892.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3254 (7.3379)  time: 0.9912 (0.5191 -- 4.1044)  data: 0.0455 (0.0005 -- 0.6587)  max mem: 16413
Epoch: [172]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000001  loss: 1.3590 (1.3635)  loss_scale: 16384.0000 (13754.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8094 (7.3473)  time: 0.7798 (0.5172 -- 2.8240)  data: 0.1916 (0.0005 -- 2.2977)  max mem: 16413
[2023-09-23 08:49:53,080] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27607
[2023-09-23 08:49:53,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27607
[2023-09-23 08:49:53,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:49:53,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:49:53,081] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [172]  [100/160]  eta: 0:00:58  lr: 0.000002  min_lr: 0.000001  loss: 1.2007 (1.3562)  loss_scale: 8192.0000 (13139.6436)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9366 (7.4478)  time: 1.0443 (0.5180 -- 3.7168)  data: 0.1497 (0.0005 -- 2.9606)  max mem: 16413
Epoch: [172]  [120/160]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000001  loss: 1.5204 (1.3750)  loss_scale: 8192.0000 (12321.8512)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7973 (7.4666)  time: 0.8073 (0.5147 -- 3.7340)  data: 0.0017 (0.0002 -- 0.0085)  max mem: 16413
Epoch: [172]  [140/160]  eta: 0:00:19  lr: 0.000002  min_lr: 0.000001  loss: 1.2744 (1.3666)  loss_scale: 8192.0000 (11736.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6559 (7.5455)  time: 1.0261 (0.5301 -- 2.9870)  data: 0.0227 (0.0002 -- 0.2967)  max mem: 16413
Epoch: [172]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.2400 (1.3593)  loss_scale: 8192.0000 (11315.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2824 (7.5358)  time: 0.6957 (0.4948 -- 3.2250)  data: 0.1213 (0.0001 -- 2.2646)  max mem: 16413
Epoch: [172] Total time: 0:02:29 (0.9324 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.2400 (1.3742)  loss_scale: 8192.0000 (11315.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2824 (7.5358)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1378 (0.1378)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4655 (2.4655 -- 2.4655)  data: 2.2539 (2.2539 -- 2.2539)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1485 (0.3603)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4809 (0.1802 -- 2.4655)  data: 0.2835 (0.0005 -- 2.2539)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1608 (0.3138)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2580 (0.1682 -- 1.0430)  data: 0.0670 (0.0001 -- 0.8565)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1709 (0.3603)  acc1: 100.0000 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2885 (0.1325 -- 1.0430)  data: 0.1063 (0.0001 -- 0.8565)  max mem: 16413
Val: Total time: 0:00:09 (0.3482 s / it)
* Acc@1 92.531 Acc@5 99.793 loss 0.344
Accuracy of the network on the 482 val images: 92.53%
Max accuracy: 93.57%
Epoch: [173]  [  0/160]  eta: 0:27:23  lr: 0.000002  min_lr: 0.000001  loss: 1.2333 (1.2333)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.0211 (23.0211)  time: 10.2693 (10.2693 -- 10.2693)  data: 7.4009 (7.4009 -- 7.4009)  max mem: 16413
Epoch: [173]  [ 20/160]  eta: 0:03:14  lr: 0.000002  min_lr: 0.000001  loss: 1.4572 (1.4043)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0035 (7.2944)  time: 0.9464 (0.5048 -- 6.0953)  data: 0.0585 (0.0003 -- 1.1482)  max mem: 16413
Epoch: [173]  [ 40/160]  eta: 0:02:25  lr: 0.000002  min_lr: 0.000001  loss: 1.4044 (1.3826)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1656 (7.0046)  time: 1.0337 (0.5099 -- 4.4384)  data: 0.0010 (0.0002 -- 0.0022)  max mem: 16413
[2023-09-23 08:52:08,820] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:52:08,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:52:08,820] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:52:08,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [173]  [ 60/160]  eta: 0:01:49  lr: 0.000002  min_lr: 0.000001  loss: 1.4299 (1.3963)  loss_scale: 8192.0000 (8863.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8712 (7.1060)  time: 0.8442 (0.5115 -- 4.7297)  data: 0.0012 (0.0003 -- 0.0032)  max mem: 16413
[2023-09-23 08:52:16,078] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27748
[2023-09-23 08:52:16,078] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27748
[2023-09-23 08:52:16,078] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:52:16,078] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 08:52:16,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [173]  [ 80/160]  eta: 0:01:20  lr: 0.000002  min_lr: 0.000001  loss: 1.3422 (1.3916)  loss_scale: 8192.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2776 (7.0776)  time: 0.7441 (0.5144 -- 2.8261)  data: 0.0014 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [173]  [100/160]  eta: 0:01:00  lr: 0.000002  min_lr: 0.000001  loss: 1.3445 (1.3830)  loss_scale: 8192.0000 (9165.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0981 (7.0483)  time: 0.9779 (0.5204 -- 5.2506)  data: 0.0011 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [173]  [120/160]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000001  loss: 1.3935 (1.3805)  loss_scale: 8192.0000 (9004.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6781 (7.0492)  time: 0.9052 (0.5238 -- 4.2490)  data: 0.1883 (0.0007 -- 3.7347)  max mem: 16413
Epoch: [173]  [140/160]  eta: 0:00:19  lr: 0.000002  min_lr: 0.000001  loss: 1.5962 (1.3969)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6668 (7.0421)  time: 0.9797 (0.5277 -- 3.7937)  data: 0.4172 (0.0004 -- 3.2733)  max mem: 16413
Epoch: [173]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.3808 (1.3998)  loss_scale: 8192.0000 (8806.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5675 (7.0646)  time: 0.7367 (0.4973 -- 3.3433)  data: 0.2171 (0.0001 -- 2.8456)  max mem: 16413
Epoch: [173] Total time: 0:02:33 (0.9570 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.3808 (1.3926)  loss_scale: 8192.0000 (8806.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5675 (7.0646)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2404 (0.2404)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.6198 (2.6198 -- 2.6198)  data: 2.3982 (2.3982 -- 2.3982)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1992 (0.4036)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4833 (0.1790 -- 2.6198)  data: 0.2911 (0.0005 -- 2.3982)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1975 (0.3355)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2488 (0.1708 -- 0.9980)  data: 0.0606 (0.0002 -- 0.7968)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2228 (0.4144)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (100.0000)  time: 0.2843 (0.1330 -- 1.0223)  data: 0.1028 (0.0001 -- 0.8477)  max mem: 16413
Val: Total time: 0:00:09 (0.3494 s / it)
* Acc@1 91.286 Acc@5 100.000 loss 0.386
Accuracy of the network on the 482 val images: 91.29%
Max accuracy: 93.57%
Epoch: [174]  [  0/160]  eta: 0:17:12  lr: 0.000002  min_lr: 0.000001  loss: 1.4108 (1.4108)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5630 (7.5630)  time: 6.4550 (6.4550 -- 6.4550)  data: 5.9137 (5.9137 -- 5.9137)  max mem: 16413
Epoch: [174]  [ 20/160]  eta: 0:03:00  lr: 0.000002  min_lr: 0.000001  loss: 1.4385 (1.4243)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2847 (6.8017)  time: 1.0305 (0.5190 -- 4.0586)  data: 0.4901 (0.0004 -- 3.5364)  max mem: 16413
[2023-09-23 08:54:26,705] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:54:26,705] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:54:26,745] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 08:54:26,745] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [174]  [ 40/160]  eta: 0:02:06  lr: 0.000002  min_lr: 0.000001  loss: 1.4756 (1.4533)  loss_scale: 8192.0000 (8991.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5535 (7.0586)  time: 0.8126 (0.5201 -- 2.2944)  data: 0.2687 (0.0006 -- 1.7817)  max mem: 16413
Epoch: [174]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000001  loss: 1.1956 (1.3761)  loss_scale: 16384.0000 (11415.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8439 (6.7657)  time: 0.8996 (0.5254 -- 2.9221)  data: 0.3290 (0.0009 -- 2.3735)  max mem: 16413
Epoch: [174]  [ 80/160]  eta: 0:01:20  lr: 0.000002  min_lr: 0.000001  loss: 1.2475 (1.3614)  loss_scale: 16384.0000 (12641.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0524 (6.9097)  time: 0.9851 (0.5195 -- 2.6716)  data: 0.3173 (0.0002 -- 2.0943)  max mem: 16413
Epoch: [174]  [100/160]  eta: 0:00:59  lr: 0.000002  min_lr: 0.000001  loss: 1.2496 (1.3514)  loss_scale: 16384.0000 (13382.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5231 (6.8757)  time: 0.9920 (0.5250 -- 3.4403)  data: 0.2668 (0.0005 -- 2.8812)  max mem: 16413
Epoch: [174]  [120/160]  eta: 0:00:40  lr: 0.000002  min_lr: 0.000000  loss: 1.3226 (1.3408)  loss_scale: 16384.0000 (13879.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4127 (6.8432)  time: 1.0303 (0.5107 -- 4.5365)  data: 0.4812 (0.0004 -- 3.9976)  max mem: 16413
Epoch: [174]  [140/160]  eta: 0:00:19  lr: 0.000002  min_lr: 0.000000  loss: 1.4622 (1.3560)  loss_scale: 16384.0000 (14234.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0307 (6.7875)  time: 0.7274 (0.5142 -- 2.5812)  data: 0.1889 (0.0003 -- 2.0540)  max mem: 16413
[2023-09-23 08:56:18,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=161, lr=[4.883955036797427e-07, 4.883955036797427e-07, 5.426616707552697e-07, 5.426616707552697e-07, 6.029574119502996e-07, 6.029574119502996e-07, 6.699526799447774e-07, 6.699526799447774e-07, 7.443918666053082e-07, 7.443918666053082e-07, 8.27102074005898e-07, 8.27102074005898e-07, 9.190023044509977e-07, 9.190023044509977e-07, 1.0211136716122195e-06, 1.0211136716122195e-06, 1.1345707462357997e-06, 1.1345707462357997e-06, 1.2606341624842215e-06, 1.2606341624842215e-06, 1.4007046249824687e-06, 1.4007046249824687e-06, 1.5563384722027429e-06, 1.5563384722027429e-06, 1.7292649691141586e-06, 1.7292649691141586e-06, 1.921405521237954e-06, 1.921405521237954e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 08:56:18,342] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=17.135782860434077, CurrSamplesPerSec=24.671602459530682, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [174]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.3970 (1.3555)  loss_scale: 16384.0000 (14489.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3122 (6.8672)  time: 0.8037 (0.4955 -- 3.7309)  data: 0.2792 (0.0001 -- 3.1855)  max mem: 16413
Epoch: [174] Total time: 0:02:31 (0.9472 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.3970 (1.3569)  loss_scale: 16384.0000 (14489.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3122 (6.8672)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2094 (0.2094)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4734 (2.4734 -- 2.4734)  data: 2.2685 (2.2685 -- 2.2685)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1579 (0.3299)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4814 (0.1819 -- 2.4734)  data: 0.2920 (0.0004 -- 2.2685)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1654 (0.3267)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2579 (0.1704 -- 1.1322)  data: 0.0684 (0.0001 -- 0.9374)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3099 (0.3856)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2847 (0.1322 -- 1.1322)  data: 0.1026 (0.0001 -- 0.9374)  max mem: 16413
Val: Total time: 0:00:09 (0.3445 s / it)
* Acc@1 92.946 Acc@5 99.793 loss 0.361
Accuracy of the network on the 482 val images: 92.95%
Max accuracy: 93.57%
Epoch: [175]  [  0/160]  eta: 0:22:56  lr: 0.000002  min_lr: 0.000000  loss: 1.3700 (1.3700)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0492 (5.0492)  time: 8.6051 (8.6051 -- 8.6051)  data: 8.0688 (8.0688 -- 8.0688)  max mem: 16413
[2023-09-23 08:56:39,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:56:39,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:56:39,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:56:39,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [175]  [ 20/160]  eta: 0:03:06  lr: 0.000002  min_lr: 0.000000  loss: 1.4160 (1.4069)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6577 (6.1466)  time: 0.9706 (0.5209 -- 4.1990)  data: 0.2463 (0.0005 -- 1.7125)  max mem: 16413
Epoch: [175]  [ 40/160]  eta: 0:02:08  lr: 0.000002  min_lr: 0.000000  loss: 1.2517 (1.3758)  loss_scale: 32768.0000 (30769.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2367 (6.6035)  time: 0.8014 (0.5151 -- 4.4492)  data: 0.0447 (0.0001 -- 0.8737)  max mem: 16413
[2023-09-23 08:57:18,976] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28046
[2023-09-23 08:57:18,976] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:57:18,976] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28046
[2023-09-23 08:57:18,976] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 08:57:18,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [175]  [ 60/160]  eta: 0:01:49  lr: 0.000002  min_lr: 0.000000  loss: 1.3808 (1.3660)  loss_scale: 16384.0000 (27396.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7985 (6.8742)  time: 1.1241 (0.4974 -- 7.0271)  data: 0.0191 (0.0002 -- 0.3505)  max mem: 16413
Epoch: [175]  [ 80/160]  eta: 0:01:23  lr: 0.000002  min_lr: 0.000000  loss: 1.4059 (1.3831)  loss_scale: 16384.0000 (24677.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2404 (7.0940)  time: 0.8788 (0.5105 -- 5.2064)  data: 0.0018 (0.0003 -- 0.0053)  max mem: 16413
Epoch: [175]  [100/160]  eta: 0:01:00  lr: 0.000002  min_lr: 0.000000  loss: 1.4190 (1.3784)  loss_scale: 16384.0000 (23034.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1959 (6.9103)  time: 0.9256 (0.5210 -- 4.5843)  data: 0.0013 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [175]  [120/160]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 1.2508 (1.3769)  loss_scale: 16384.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0780 (6.9279)  time: 0.9004 (0.5276 -- 4.6472)  data: 0.0014 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [175]  [140/160]  eta: 0:00:19  lr: 0.000002  min_lr: 0.000000  loss: 1.0826 (1.3596)  loss_scale: 16384.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6586 (6.8510)  time: 0.9380 (0.5285 -- 4.0006)  data: 0.0010 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [175]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.3792 (1.3574)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9939 (6.7993)  time: 0.6680 (0.4924 -- 3.5854)  data: 0.0008 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [175] Total time: 0:02:32 (0.9515 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.3792 (1.3558)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9939 (6.7993)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4382 (2.4382 -- 2.4382)  data: 2.2554 (2.2554 -- 2.2554)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1391 (0.3789)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4776 (0.1838 -- 2.4382)  data: 0.2860 (0.0002 -- 2.2554)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1524 (0.3362)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2570 (0.1681 -- 1.0707)  data: 0.0695 (0.0001 -- 0.8834)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2224 (0.3893)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (100.0000)  time: 0.2740 (0.1329 -- 1.0707)  data: 0.0965 (0.0001 -- 0.8834)  max mem: 16413
Val: Total time: 0:00:09 (0.3374 s / it)
* Acc@1 92.739 Acc@5 100.000 loss 0.346
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 93.57%
Epoch: [176]  [  0/160]  eta: 0:19:23  lr: 0.000002  min_lr: 0.000000  loss: 1.5050 (1.5050)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2925 (10.2925)  time: 7.2739 (7.2739 -- 7.2739)  data: 6.6777 (6.6777 -- 6.6777)  max mem: 16413
[2023-09-23 08:59:28,838] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:59:28,838] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 08:59:28,839] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 08:59:28,839] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [176]  [ 20/160]  eta: 0:02:54  lr: 0.000002  min_lr: 0.000000  loss: 1.5579 (1.5326)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2002 (6.5759)  time: 0.9447 (0.5252 -- 4.0146)  data: 0.1667 (0.0004 -- 1.7210)  max mem: 16413
Epoch: [176]  [ 40/160]  eta: 0:02:07  lr: 0.000002  min_lr: 0.000000  loss: 1.3154 (1.4177)  loss_scale: 32768.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6815 (6.8524)  time: 0.8723 (0.5182 -- 3.0471)  data: 0.0636 (0.0003 -- 0.8940)  max mem: 16413
Epoch: [176]  [ 60/160]  eta: 0:01:44  lr: 0.000002  min_lr: 0.000000  loss: 1.2294 (1.3613)  loss_scale: 32768.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4418 (6.9064)  time: 1.0045 (0.5233 -- 4.2641)  data: 0.1017 (0.0003 -- 1.7619)  max mem: 16413
Epoch: [176]  [ 80/160]  eta: 0:01:22  lr: 0.000002  min_lr: 0.000000  loss: 1.4534 (1.3735)  loss_scale: 32768.0000 (29733.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7998 (7.0151)  time: 0.9911 (0.5240 -- 4.4708)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
[2023-09-23 09:00:43,457] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28253
[2023-09-23 09:00:43,457] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28253
[2023-09-23 09:00:43,457] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:00:43,457] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:00:43,457] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [176]  [100/160]  eta: 0:01:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5047 (1.3927)  loss_scale: 32768.0000 (29036.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7986 (6.9982)  time: 0.8777 (0.5165 -- 3.9737)  data: 0.0012 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [176]  [120/160]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 1.4910 (1.4059)  loss_scale: 16384.0000 (26945.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6181 (7.0401)  time: 0.9197 (0.5156 -- 3.7178)  data: 0.0013 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [176]  [140/160]  eta: 0:00:19  lr: 0.000002  min_lr: 0.000000  loss: 1.3117 (1.3960)  loss_scale: 16384.0000 (25447.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9600 (6.9904)  time: 0.7943 (0.5288 -- 3.3394)  data: 0.0014 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [176]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.3690 (1.3939)  loss_scale: 16384.0000 (24371.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4492 (6.9097)  time: 0.8122 (0.4936 -- 4.5603)  data: 0.0007 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [176] Total time: 0:02:31 (0.9441 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.3690 (1.3589)  loss_scale: 16384.0000 (24371.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4492 (6.9097)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1406 (0.1406)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4695 (2.4695 -- 2.4695)  data: 2.2715 (2.2715 -- 2.2715)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1406 (0.2841)  acc1: 100.0000 (94.9495)  acc5: 100.0000 (100.0000)  time: 0.4432 (0.1743 -- 2.4695)  data: 0.2511 (0.0003 -- 2.2715)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1501 (0.3031)  acc1: 100.0000 (95.2381)  acc5: 100.0000 (99.4709)  time: 0.2140 (0.1703 -- 0.7138)  data: 0.0295 (0.0001 -- 0.4845)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1898 (0.3518)  acc1: 88.8889 (93.3610)  acc5: 100.0000 (99.5851)  time: 0.2344 (0.1338 -- 0.7472)  data: 0.0579 (0.0001 -- 0.5719)  max mem: 16413
Val: Total time: 0:00:08 (0.3080 s / it)
* Acc@1 92.739 Acc@5 99.378 loss 0.354
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 93.57%
Epoch: [177]  [  0/160]  eta: 0:22:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5218 (1.5218)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0867 (6.0867)  time: 8.3654 (8.3654 -- 8.3654)  data: 6.4408 (6.4408 -- 6.4408)  max mem: 16413
Epoch: [177]  [ 20/160]  eta: 0:02:47  lr: 0.000002  min_lr: 0.000000  loss: 1.2950 (1.2830)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8485 (6.8713)  time: 0.8372 (0.5149 -- 3.4164)  data: 0.0368 (0.0003 -- 0.5505)  max mem: 16413
Epoch: [177]  [ 40/160]  eta: 0:02:08  lr: 0.000002  min_lr: 0.000000  loss: 1.3271 (1.3222)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8737 (6.7472)  time: 0.9446 (0.5181 -- 3.4602)  data: 0.1173 (0.0003 -- 0.9959)  max mem: 16413
Epoch: [177]  [ 60/160]  eta: 0:01:43  lr: 0.000002  min_lr: 0.000000  loss: 1.4229 (1.3436)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2525 (7.0368)  time: 0.9471 (0.5223 -- 2.7653)  data: 0.2092 (0.0004 -- 2.2306)  max mem: 16413
[2023-09-23 09:02:54,512] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:02:54,512] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:02:54,512] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:02:54,512] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:03:10,356] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28400
[2023-09-23 09:03:10,356] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28400
[2023-09-23 09:03:10,356] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:03:10,356] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:03:10,356] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [177]  [ 80/160]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 1.2380 (1.3214)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7817 (7.0169)  time: 0.8469 (0.5099 -- 3.9341)  data: 0.2640 (0.0004 -- 3.4181)  max mem: 16413
Epoch: [177]  [100/160]  eta: 0:01:01  lr: 0.000002  min_lr: 0.000000  loss: 1.4615 (1.3516)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1229 (7.1000)  time: 1.1479 (0.5060 -- 5.2225)  data: 0.6158 (0.0003 -- 4.7149)  max mem: 16413
Epoch: [177]  [120/160]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000000  loss: 1.2341 (1.3478)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7784 (6.9287)  time: 0.8515 (0.5119 -- 4.1756)  data: 0.3128 (0.0004 -- 3.6484)  max mem: 16413
Epoch: [177]  [140/160]  eta: 0:00:19  lr: 0.000002  min_lr: 0.000000  loss: 1.4375 (1.3574)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4743 (6.9468)  time: 0.9658 (0.5206 -- 4.0186)  data: 0.4201 (0.0004 -- 3.4883)  max mem: 16413
Epoch: [177]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.3369 (1.3542)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1987 (6.9799)  time: 0.6392 (0.4936 -- 2.6718)  data: 0.1203 (0.0001 -- 2.1584)  max mem: 16413
Epoch: [177] Total time: 0:02:31 (0.9467 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.3369 (1.3659)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1987 (6.9799)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1342 (0.1342)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4072 (2.4072 -- 2.4072)  data: 2.2196 (2.2196 -- 2.2196)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1342 (0.3055)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4663 (0.1721 -- 2.4072)  data: 0.2803 (0.0002 -- 2.2196)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1346 (0.2878)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (100.0000)  time: 0.2548 (0.1677 -- 1.0569)  data: 0.0702 (0.0001 -- 0.8594)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2627 (0.3408)  acc1: 88.8889 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2891 (0.1326 -- 1.0569)  data: 0.1115 (0.0001 -- 0.8594)  max mem: 16413
Val: Total time: 0:00:09 (0.3450 s / it)
* Acc@1 91.909 Acc@5 99.585 loss 0.358
Accuracy of the network on the 482 val images: 91.91%
Max accuracy: 93.57%
Epoch: [178]  [  0/160]  eta: 0:19:39  lr: 0.000002  min_lr: 0.000000  loss: 1.6822 (1.6822)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9675 (5.9675)  time: 7.3749 (7.3749 -- 7.3749)  data: 6.1871 (6.1871 -- 6.1871)  max mem: 16413
[2023-09-23 09:04:51,067] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28494
[2023-09-23 09:04:51,068] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28494
[2023-09-23 09:04:51,068] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 09:04:51,068] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 09:04:51,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [178]  [ 20/160]  eta: 0:02:47  lr: 0.000001  min_lr: 0.000000  loss: 1.3320 (1.4162)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7971 (7.1867)  time: 0.8865 (0.5263 -- 2.7610)  data: 0.3085 (0.0008 -- 2.2534)  max mem: 16413
Epoch: [178]  [ 40/160]  eta: 0:02:08  lr: 0.000001  min_lr: 0.000000  loss: 1.4507 (1.4187)  loss_scale: 8192.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5304 (7.3377)  time: 0.9477 (0.5132 -- 2.3909)  data: 0.2304 (0.0004 -- 1.6458)  max mem: 16413
Epoch: [178]  [ 60/160]  eta: 0:01:41  lr: 0.000001  min_lr: 0.000000  loss: 1.2607 (1.3796)  loss_scale: 8192.0000 (10072.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7535 (7.1149)  time: 0.9014 (0.5140 -- 2.7877)  data: 0.1984 (0.0006 -- 2.2472)  max mem: 16413
Epoch: [178]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.4738 (1.3930)  loss_scale: 8192.0000 (9607.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7128 (7.2126)  time: 0.8903 (0.5225 -- 2.5397)  data: 0.1785 (0.0003 -- 1.6868)  max mem: 16413
Epoch: [178]  [100/160]  eta: 0:00:59  lr: 0.000001  min_lr: 0.000000  loss: 1.3445 (1.3822)  loss_scale: 8192.0000 (9327.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6716 (7.2913)  time: 1.0223 (0.5179 -- 4.8724)  data: 0.0016 (0.0003 -- 0.0048)  max mem: 16413
Epoch: [178]  [120/160]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.4043 (1.3824)  loss_scale: 8192.0000 (9139.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9100 (7.2988)  time: 0.8698 (0.5179 -- 4.5939)  data: 0.0019 (0.0003 -- 0.0137)  max mem: 16413
Epoch: [178]  [140/160]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 1.3826 (1.3988)  loss_scale: 8192.0000 (9005.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3050 (7.5285)  time: 0.9672 (0.5021 -- 4.4355)  data: 0.0010 (0.0002 -- 0.0019)  max mem: 16413
[2023-09-23 09:06:50,064] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:06:50,064] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 09:06:50,064] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:06:50,064] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [178]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5354 (1.4063)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5141 (7.4502)  time: 0.6891 (0.4921 -- 3.4747)  data: 0.0006 (0.0001 -- 0.0017)  max mem: 16413
Epoch: [178] Total time: 0:02:30 (0.9398 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5354 (1.3916)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5141 (7.4502)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1254 (0.1254)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3981 (2.3981 -- 2.3981)  data: 2.1839 (2.1839 -- 2.1839)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1354 (0.3509)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4677 (0.1736 -- 2.3981)  data: 0.2805 (0.0003 -- 2.1839)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1764 (0.3441)  acc1: 100.0000 (92.5926)  acc5: 100.0000 (100.0000)  time: 0.2590 (0.1687 -- 1.0911)  data: 0.0739 (0.0001 -- 0.8915)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2453 (0.3943)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (100.0000)  time: 0.2871 (0.1323 -- 1.0911)  data: 0.1077 (0.0001 -- 0.8915)  max mem: 16413
Val: Total time: 0:00:09 (0.3422 s / it)
* Acc@1 93.154 Acc@5 99.793 loss 0.350
Accuracy of the network on the 482 val images: 93.15%
Max accuracy: 93.57%
Epoch: [179]  [  0/160]  eta: 0:19:42  lr: 0.000001  min_lr: 0.000000  loss: 1.0987 (1.0987)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6888 (7.6888)  time: 7.3909 (7.3909 -- 7.3909)  data: 6.8709 (6.8709 -- 6.8709)  max mem: 16413
Epoch: [179]  [ 20/160]  eta: 0:02:46  lr: 0.000001  min_lr: 0.000000  loss: 1.4360 (1.4061)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1368 (6.6122)  time: 0.8777 (0.5266 -- 4.6543)  data: 0.3040 (0.0003 -- 4.1352)  max mem: 16413
Epoch: [179]  [ 40/160]  eta: 0:02:10  lr: 0.000001  min_lr: 0.000000  loss: 1.2075 (1.3631)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6312 (6.3952)  time: 0.9873 (0.5231 -- 4.1449)  data: 0.2914 (0.0005 -- 2.3817)  max mem: 16413
Epoch: [179]  [ 60/160]  eta: 0:01:43  lr: 0.000001  min_lr: 0.000000  loss: 1.4717 (1.3754)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1863 (6.7059)  time: 0.9191 (0.5188 -- 4.8280)  data: 0.0790 (0.0003 -- 0.9353)  max mem: 16413
Epoch: [179]  [ 80/160]  eta: 0:01:19  lr: 0.000001  min_lr: 0.000000  loss: 1.4820 (1.3868)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4880 (6.7537)  time: 0.8485 (0.5276 -- 2.8472)  data: 0.1337 (0.0004 -- 2.2772)  max mem: 16413
Epoch: [179]  [100/160]  eta: 0:00:58  lr: 0.000001  min_lr: 0.000000  loss: 1.3360 (1.3857)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4249 (6.9406)  time: 0.8939 (0.5240 -- 3.3940)  data: 0.0060 (0.0002 -- 0.0762)  max mem: 16413
[2023-09-23 09:08:58,326] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:08:58,326] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:08:58,327] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:08:58,327] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [179]  [120/160]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.3889 (1.3852)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0782 (6.8942)  time: 0.8637 (0.5130 -- 3.5954)  data: 0.0438 (0.0002 -- 0.7341)  max mem: 16413
Epoch: [179]  [140/160]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 1.2728 (1.3663)  loss_scale: 32768.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9217 (6.8133)  time: 1.0889 (0.5238 -- 4.3555)  data: 0.4045 (0.0002 -- 3.8184)  max mem: 16413
[2023-09-23 09:09:34,316] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28788
[2023-09-23 09:09:34,316] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:09:34,316] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28788
[2023-09-23 09:09:34,316] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:09:34,316] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [179]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.3676 (1.3617)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7500 (6.7321)  time: 0.7720 (0.4946 -- 2.8272)  data: 0.2625 (0.0001 -- 2.3230)  max mem: 16413
Epoch: [179] Total time: 0:02:29 (0.9352 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.3676 (1.3523)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7500 (6.7321)
[2023-09-23 09:09:40,654] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-09-23 09:09:40,656] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt
[2023-09-23 09:09:40,656] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt...
[2023-09-23 09:09:40,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-09-23 09:09:41,684] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt.
[2023-09-23 09:09:41,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1245 (0.1245)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3992 (2.3992 -- 2.3992)  data: 2.2117 (2.2117 -- 2.2117)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1563 (0.3890)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4792 (0.1795 -- 2.3992)  data: 0.2889 (0.0004 -- 2.2117)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1633 (0.3359)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2601 (0.1678 -- 1.1680)  data: 0.0750 (0.0001 -- 0.9597)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1933 (0.3641)  acc1: 100.0000 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2972 (0.1321 -- 1.1680)  data: 0.1200 (0.0001 -- 0.9597)  max mem: 16413
Val: Total time: 0:00:09 (0.3515 s / it)
* Acc@1 93.154 Acc@5 100.000 loss 0.336
Accuracy of the network on the 482 val images: 93.15%
Max accuracy: 93.57%
Epoch: [180]  [  0/160]  eta: 0:17:08  lr: 0.000001  min_lr: 0.000000  loss: 1.5209 (1.5209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4335 (5.4335)  time: 6.4286 (6.4286 -- 6.4286)  data: 5.8634 (5.8634 -- 5.8634)  max mem: 16413
Epoch: [180]  [ 20/160]  eta: 0:02:48  lr: 0.000001  min_lr: 0.000000  loss: 1.3608 (1.4080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1512 (6.1893)  time: 0.9428 (0.5322 -- 3.4280)  data: 0.3889 (0.0008 -- 2.8662)  max mem: 16413
Epoch: [180]  [ 40/160]  eta: 0:02:06  lr: 0.000001  min_lr: 0.000000  loss: 1.3609 (1.3822)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2484 (6.8245)  time: 0.9040 (0.5322 -- 3.0887)  data: 0.3074 (0.0009 -- 2.1795)  max mem: 16413
Epoch: [180]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.3261 (1.3746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3960 (6.5939)  time: 0.8648 (0.5320 -- 2.9349)  data: 0.2034 (0.0005 -- 2.0642)  max mem: 16413
Epoch: [180]  [ 80/160]  eta: 0:01:19  lr: 0.000001  min_lr: 0.000000  loss: 1.2379 (1.3600)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1846 (6.7125)  time: 0.9786 (0.5175 -- 3.4472)  data: 0.0875 (0.0004 -- 1.7245)  max mem: 16413
Epoch: [180]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.1444 (1.3423)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4015 (6.6627)  time: 0.8256 (0.5268 -- 3.8857)  data: 0.0066 (0.0002 -- 0.0913)  max mem: 16413
[2023-09-23 09:11:45,734] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:11:45,734] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:11:45,734] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:11:45,734] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [180]  [120/160]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.2215 (1.3304)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8193 (6.7058)  time: 1.0335 (0.5119 -- 3.3100)  data: 0.2262 (0.0002 -- 2.7946)  max mem: 16413
[2023-09-23 09:11:56,217] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28927
[2023-09-23 09:11:56,217] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:11:56,217] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28927
[2023-09-23 09:11:56,217] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:11:56,217] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [180]  [140/160]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 1.2014 (1.3261)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0290 (6.5305)  time: 0.9144 (0.5101 -- 4.3494)  data: 0.0463 (0.0008 -- 0.8966)  max mem: 16413
Epoch: [180]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.1952 (1.3238)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6695 (6.4442)  time: 0.7041 (0.4943 -- 2.6574)  data: 0.0010 (0.0000 -- 0.0068)  max mem: 16413
Epoch: [180] Total time: 0:02:29 (0.9329 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.1952 (1.3343)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6695 (6.4442)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1687 (0.1687)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5453 (2.5453 -- 2.5453)  data: 2.3542 (2.3542 -- 2.3542)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1687 (0.3859)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4894 (0.1744 -- 2.5453)  data: 0.2943 (0.0002 -- 2.3542)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1893 (0.3322)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2503 (0.1710 -- 1.0796)  data: 0.0571 (0.0001 -- 0.8770)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2056 (0.3802)  acc1: 100.0000 (92.9461)  acc5: 100.0000 (100.0000)  time: 0.2814 (0.1323 -- 1.0796)  data: 0.0955 (0.0001 -- 0.8770)  max mem: 16413
Val: Total time: 0:00:09 (0.3463 s / it)
* Acc@1 94.191 Acc@5 99.793 loss 0.340
Accuracy of the network on the 482 val images: 94.19%
[2023-09-23 09:12:30,130] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-23 09:12:30,131] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-23 09:12:30,132] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-23 09:12:30,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-23 09:12:31,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-23 09:12:31,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 94.19%
Epoch: [181]  [  0/160]  eta: 0:18:16  lr: 0.000001  min_lr: 0.000000  loss: 1.9167 (1.9167)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1035 (4.1035)  time: 6.8559 (6.8559 -- 6.8559)  data: 5.8108 (5.8108 -- 5.8108)  max mem: 16413
Epoch: [181]  [ 20/160]  eta: 0:03:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4944 (1.4629)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0180 (6.4572)  time: 1.0125 (0.5246 -- 3.2564)  data: 0.3192 (0.0003 -- 2.7198)  max mem: 16413
[2023-09-23 09:13:14,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=167, lr=[2.8157390886715256e-07, 2.8157390886715256e-07, 3.128598987412807e-07, 3.128598987412807e-07, 3.47622109712534e-07, 3.47622109712534e-07, 3.862467885694823e-07, 3.862467885694823e-07, 4.2916309841053583e-07, 4.2916309841053583e-07, 4.768478871228176e-07, 4.768478871228176e-07, 5.298309856920195e-07, 5.298309856920195e-07, 5.887010952133549e-07, 5.887010952133549e-07, 6.541123280148388e-07, 6.541123280148388e-07, 7.267914755720431e-07, 7.267914755720431e-07, 8.075460839689369e-07, 8.075460839689369e-07, 8.97273426632152e-07, 8.97273426632152e-07, 9.969704740357244e-07, 9.969704740357244e-07, 1.1077449711508049e-06, 1.1077449711508049e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 09:13:14,337] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=17.094085647979146, CurrSamplesPerSec=22.80696774684687, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [181]  [ 40/160]  eta: 0:02:06  lr: 0.000001  min_lr: 0.000000  loss: 1.4174 (1.4481)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5805 (6.6990)  time: 0.8131 (0.5252 -- 3.8994)  data: 0.2497 (0.0003 -- 3.3765)  max mem: 16413
Epoch: [181]  [ 60/160]  eta: 0:01:44  lr: 0.000001  min_lr: 0.000000  loss: 1.4830 (1.4225)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4652 (6.6062)  time: 1.0038 (0.5070 -- 3.7234)  data: 0.0990 (0.0003 -- 1.9570)  max mem: 16413
Epoch: [181]  [ 80/160]  eta: 0:01:20  lr: 0.000001  min_lr: 0.000000  loss: 1.4966 (1.4214)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7885 (6.7270)  time: 0.8870 (0.5227 -- 2.8721)  data: 0.0018 (0.0003 -- 0.0070)  max mem: 16413
[2023-09-23 09:14:06,294] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:14:06,294] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:14:06,295] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:14:06,295] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [181]  [100/160]  eta: 0:00:59  lr: 0.000001  min_lr: 0.000000  loss: 1.3843 (1.4114)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4688 (6.7542)  time: 0.9421 (0.5423 -- 3.5644)  data: 0.0015 (0.0003 -- 0.0052)  max mem: 16413
[2023-09-23 09:14:28,439] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29079
[2023-09-23 09:14:28,439] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29079
[2023-09-23 09:14:28,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:14:28,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:14:28,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [181]  [120/160]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.4297 (1.4068)  loss_scale: 32768.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5583 (6.8312)  time: 0.8708 (0.5106 -- 3.3479)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [181]  [140/160]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 1.4009 (1.4056)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1036 (6.8572)  time: 0.9518 (0.5255 -- 4.0033)  data: 0.0014 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [181]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.2971 (1.3980)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5082 (6.9095)  time: 0.6972 (0.4940 -- 2.4769)  data: 0.0011 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [181] Total time: 0:02:29 (0.9370 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.2971 (1.3917)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5082 (6.9095)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1506 (0.1506)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3560 (2.3560 -- 2.3560)  data: 2.1707 (2.1707 -- 2.1707)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1506 (0.2961)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4667 (0.1742 -- 2.3560)  data: 0.2736 (0.0002 -- 2.1707)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1503 (0.2895)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2579 (0.1680 -- 1.0637)  data: 0.0702 (0.0001 -- 0.8318)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1552 (0.3527)  acc1: 100.0000 (92.5311)  acc5: 100.0000 (100.0000)  time: 0.2759 (0.1334 -- 1.0637)  data: 0.0970 (0.0001 -- 0.8318)  max mem: 16413
Val: Total time: 0:00:09 (0.3347 s / it)
* Acc@1 93.361 Acc@5 100.000 loss 0.331
Accuracy of the network on the 482 val images: 93.36%
Max accuracy: 94.19%
Epoch: [182]  [  0/160]  eta: 0:17:37  lr: 0.000001  min_lr: 0.000000  loss: 0.9753 (0.9753)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0265 (8.0265)  time: 6.6122 (6.6122 -- 6.6122)  data: 5.6364 (5.6364 -- 5.6364)  max mem: 16413
Epoch: [182]  [ 20/160]  eta: 0:02:44  lr: 0.000001  min_lr: 0.000000  loss: 1.3321 (1.3217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1502 (7.1207)  time: 0.9034 (0.5362 -- 2.9736)  data: 0.1052 (0.0004 -- 1.6795)  max mem: 16413
Epoch: [182]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.3901 (1.3855)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4376 (7.3399)  time: 0.9507 (0.5300 -- 4.0703)  data: 0.1574 (0.0004 -- 1.7511)  max mem: 16413
Epoch: [182]  [ 60/160]  eta: 0:01:42  lr: 0.000001  min_lr: 0.000000  loss: 1.2416 (1.3285)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4136 (7.1170)  time: 0.9481 (0.5182 -- 3.6344)  data: 0.4126 (0.0003 -- 3.1098)  max mem: 16413
Epoch: [182]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.3918 (1.3234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0980 (6.9475)  time: 0.7820 (0.5352 -- 2.6725)  data: 0.2307 (0.0006 -- 2.1398)  max mem: 16413
[2023-09-23 09:16:36,365] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:16:36,365] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:16:36,365] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:16:36,366] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [182]  [100/160]  eta: 0:00:59  lr: 0.000001  min_lr: 0.000000  loss: 1.3687 (1.3275)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8626 (6.8037)  time: 1.0851 (0.5179 -- 3.2640)  data: 0.2698 (0.0003 -- 1.6406)  max mem: 16413
[2023-09-23 09:17:03,657] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29235
[2023-09-23 09:17:03,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:17:03,658] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29235
[2023-09-23 09:17:03,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:17:03,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 09:17:04,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29237
[2023-09-23 09:17:04,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 09:17:04,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-23 09:17:04,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29237
[2023-09-23 09:17:04,726] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [182]  [120/160]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.3093 (1.3449)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3524 (6.9832)  time: 0.7775 (0.5075 -- 3.0472)  data: 0.1281 (0.0003 -- 2.5408)  max mem: 16413
Epoch: [182]  [140/160]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 1.3992 (1.3442)  loss_scale: 8192.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5358 (6.9867)  time: 1.0855 (0.5216 -- 4.3006)  data: 0.4587 (0.0003 -- 3.7767)  max mem: 16413
Epoch: [182]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4321 (1.3599)  loss_scale: 8192.0000 (16947.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0200 (6.9537)  time: 0.6403 (0.4937 -- 2.8792)  data: 0.1254 (0.0002 -- 2.3587)  max mem: 16413
Epoch: [182] Total time: 0:02:29 (0.9347 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4321 (1.3494)  loss_scale: 8192.0000 (16947.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0200 (6.9537)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1401 (0.1401)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5094 (2.5094 -- 2.5094)  data: 2.3106 (2.3106 -- 2.3106)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1401 (0.3408)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4745 (0.1773 -- 2.5094)  data: 0.2863 (0.0002 -- 2.3106)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1427 (0.3094)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (100.0000)  time: 0.2522 (0.1683 -- 1.0267)  data: 0.0657 (0.0001 -- 0.8290)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2232 (0.3643)  acc1: 100.0000 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2853 (0.1325 -- 1.0267)  data: 0.1057 (0.0001 -- 0.8290)  max mem: 16413
Val: Total time: 0:00:09 (0.3460 s / it)
* Acc@1 90.871 Acc@5 100.000 loss 0.355
Accuracy of the network on the 482 val images: 90.87%
Max accuracy: 94.19%
Epoch: [183]  [  0/160]  eta: 0:22:12  lr: 0.000001  min_lr: 0.000000  loss: 1.4125 (1.4125)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7196 (7.7196)  time: 8.3279 (8.3279 -- 8.3279)  data: 6.9768 (6.9768 -- 6.9768)  max mem: 16413
Epoch: [183]  [ 20/160]  eta: 0:02:41  lr: 0.000001  min_lr: 0.000000  loss: 1.3556 (1.3718)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6656 (6.6010)  time: 0.7916 (0.5223 -- 2.7824)  data: 0.1397 (0.0005 -- 1.8518)  max mem: 16413
Epoch: [183]  [ 40/160]  eta: 0:02:05  lr: 0.000001  min_lr: 0.000000  loss: 1.2505 (1.3280)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0587 (6.0972)  time: 0.9399 (0.5267 -- 3.4169)  data: 0.2275 (0.0004 -- 1.9098)  max mem: 16413
Epoch: [183]  [ 60/160]  eta: 0:01:42  lr: 0.000001  min_lr: 0.000000  loss: 1.3688 (1.3477)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2621 (6.2518)  time: 0.9695 (0.5179 -- 2.3643)  data: 0.1595 (0.0004 -- 1.7815)  max mem: 16413
Epoch: [183]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.2675 (1.3315)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7699 (6.3881)  time: 0.8253 (0.5262 -- 2.5064)  data: 0.2319 (0.0004 -- 1.9639)  max mem: 16413
[2023-09-23 09:19:13,598] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:19:13,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 09:19:13,599] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:19:13,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [183]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.2702 (1.3357)  loss_scale: 16384.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3782 (6.6034)  time: 0.9249 (0.5314 -- 3.0017)  data: 0.2510 (0.0006 -- 2.4918)  max mem: 16413
Epoch: [183]  [120/160]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.2527 (1.3310)  loss_scale: 16384.0000 (10561.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2624 (6.8131)  time: 0.9560 (0.5185 -- 2.3631)  data: 0.1008 (0.0002 -- 1.0609)  max mem: 16413
Epoch: [183]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.2580 (1.3370)  loss_scale: 16384.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4913 (6.9425)  time: 0.8530 (0.5136 -- 4.6873)  data: 0.0384 (0.0003 -- 0.7352)  max mem: 16413
Epoch: [183]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.3931 (1.3481)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1639 (7.0151)  time: 0.8740 (0.4936 -- 3.9643)  data: 0.0269 (0.0002 -- 0.5236)  max mem: 16413
Epoch: [183] Total time: 0:02:30 (0.9407 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.3931 (1.3542)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1639 (7.0151)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1524 (0.1524)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4888 (2.4888 -- 2.4888)  data: 2.3074 (2.3074 -- 2.3074)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1291 (0.3502)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (98.9899)  time: 0.4872 (0.1809 -- 2.4888)  data: 0.2940 (0.0004 -- 2.3074)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1389 (0.3377)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (99.4709)  time: 0.2544 (0.1685 -- 1.1373)  data: 0.0652 (0.0001 -- 0.9195)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1869 (0.3718)  acc1: 100.0000 (92.9461)  acc5: 100.0000 (99.5851)  time: 0.2786 (0.1327 -- 1.1373)  data: 0.0976 (0.0001 -- 0.9195)  max mem: 16413
Val: Total time: 0:00:09 (0.3412 s / it)
* Acc@1 93.568 Acc@5 99.793 loss 0.341
Accuracy of the network on the 482 val images: 93.57%
Max accuracy: 94.19%
Epoch: [184]  [  0/160]  eta: 0:23:22  lr: 0.000001  min_lr: 0.000000  loss: 0.9984 (0.9984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7655 (8.7655)  time: 8.7668 (8.7668 -- 8.7668)  data: 7.4706 (7.4706 -- 7.4706)  max mem: 16413
Epoch: [184]  [ 20/160]  eta: 0:02:42  lr: 0.000001  min_lr: 0.000000  loss: 1.3740 (1.3736)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5464 (6.6783)  time: 0.7806 (0.5205 -- 3.3251)  data: 0.1445 (0.0003 -- 1.6774)  max mem: 16413
Epoch: [184]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.4313 (1.4173)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7879 (6.9142)  time: 0.9530 (0.5273 -- 3.1578)  data: 0.4072 (0.0004 -- 2.6344)  max mem: 16413
[2023-09-23 09:21:28,777] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:21:28,777] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:21:28,777] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:21:28,778] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [184]  [ 60/160]  eta: 0:01:47  lr: 0.000001  min_lr: 0.000000  loss: 1.3749 (1.4106)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9488 (6.8478)  time: 1.1138 (0.5046 -- 3.8910)  data: 0.5013 (0.0004 -- 3.3770)  max mem: 16413
[2023-09-23 09:21:41,428] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29508
[2023-09-23 09:21:41,428] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29508
[2023-09-23 09:21:41,428] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:21:41,429] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:21:41,429] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [184]  [ 80/160]  eta: 0:01:20  lr: 0.000001  min_lr: 0.000000  loss: 1.3211 (1.3860)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6366 (6.8297)  time: 0.7751 (0.5288 -- 3.5055)  data: 0.2282 (0.0001 -- 2.9857)  max mem: 16413
Epoch: [184]  [100/160]  eta: 0:01:01  lr: 0.000001  min_lr: 0.000000  loss: 1.3583 (1.3850)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7024 (7.0169)  time: 1.0789 (0.5140 -- 4.2061)  data: 0.5343 (0.0005 -- 3.6718)  max mem: 16413
Epoch: [184]  [120/160]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 1.3220 (1.3736)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1314 (6.9065)  time: 0.8153 (0.5249 -- 3.5753)  data: 0.2660 (0.0002 -- 3.0310)  max mem: 16413
Epoch: [184]  [140/160]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 1.3644 (1.3813)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6505 (6.8780)  time: 0.9755 (0.5157 -- 4.2665)  data: 0.4422 (0.0004 -- 3.7469)  max mem: 16413
Epoch: [184]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.1443 (1.3731)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7242 (6.7882)  time: 0.7394 (0.4934 -- 2.6931)  data: 0.2174 (0.0002 -- 2.1904)  max mem: 16413
Epoch: [184] Total time: 0:02:32 (0.9555 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.1443 (1.3594)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7242 (6.7882)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1367 (0.1367)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5394 (2.5394 -- 2.5394)  data: 2.3138 (2.3138 -- 2.3138)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1367 (0.3718)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (97.9798)  time: 0.4666 (0.1725 -- 2.5394)  data: 0.2781 (0.0002 -- 2.3138)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1381 (0.3124)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (98.9418)  time: 0.2479 (0.1677 -- 0.9524)  data: 0.0635 (0.0001 -- 0.7355)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1953 (0.3895)  acc1: 100.0000 (92.5311)  acc5: 100.0000 (99.1701)  time: 0.2752 (0.1315 -- 0.9524)  data: 0.0968 (0.0001 -- 0.7355)  max mem: 16413
Val: Total time: 0:00:09 (0.3386 s / it)
* Acc@1 92.739 Acc@5 99.585 loss 0.356
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 94.19%
Epoch: [185]  [  0/160]  eta: 0:21:32  lr: 0.000001  min_lr: 0.000000  loss: 1.4119 (1.4119)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6728 (8.6728)  time: 8.0785 (8.0785 -- 8.0785)  data: 7.5340 (7.5340 -- 7.5340)  max mem: 16413
Epoch: [185]  [ 20/160]  eta: 0:03:07  lr: 0.000001  min_lr: 0.000000  loss: 1.1501 (1.2025)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6168 (6.9994)  time: 1.0006 (0.5100 -- 4.3762)  data: 0.1859 (0.0005 -- 2.0209)  max mem: 16413
[2023-09-23 09:23:56,918] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:23:56,918] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:23:56,919] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:23:56,919] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [185]  [ 40/160]  eta: 0:02:16  lr: 0.000001  min_lr: 0.000000  loss: 1.3225 (1.2980)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4276 (7.1903)  time: 0.9253 (0.5088 -- 5.4509)  data: 0.0010 (0.0002 -- 0.0038)  max mem: 16413
[2023-09-23 09:24:05,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29648
[2023-09-23 09:24:05,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29648
[2023-09-23 09:24:05,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:24:05,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:24:05,955] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [185]  [ 60/160]  eta: 0:01:46  lr: 0.000001  min_lr: 0.000000  loss: 1.2775 (1.3165)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5218 (6.9968)  time: 0.9147 (0.5212 -- 3.4515)  data: 0.0028 (0.0005 -- 0.0097)  max mem: 16413
Epoch: [185]  [ 80/160]  eta: 0:01:21  lr: 0.000001  min_lr: 0.000000  loss: 1.4368 (1.3478)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2729 (6.8159)  time: 0.9038 (0.5247 -- 4.2406)  data: 0.0013 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [185]  [100/160]  eta: 0:01:00  lr: 0.000001  min_lr: 0.000000  loss: 1.3538 (1.3549)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7254 (6.9748)  time: 0.9714 (0.5309 -- 4.2634)  data: 0.0067 (0.0001 -- 0.1116)  max mem: 16413
Epoch: [185]  [120/160]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 1.2485 (1.3427)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7940 (6.7910)  time: 0.9237 (0.5197 -- 3.7182)  data: 0.0016 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [185]  [140/160]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 1.3784 (1.3462)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8473 (6.8148)  time: 0.8234 (0.5288 -- 4.4070)  data: 0.0014 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [185]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.2835 (1.3393)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5966 (6.8003)  time: 0.7731 (0.4936 -- 3.5996)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [185] Total time: 0:02:32 (0.9517 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.2835 (1.3671)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5966 (6.8003)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1417 (0.1417)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4628 (2.4628 -- 2.4628)  data: 2.2556 (2.2556 -- 2.2556)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1417 (0.3302)  acc1: 100.0000 (94.9495)  acc5: 100.0000 (98.9899)  time: 0.4642 (0.1801 -- 2.4628)  data: 0.2752 (0.0003 -- 2.2556)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2060 (0.3095)  acc1: 100.0000 (94.7090)  acc5: 100.0000 (99.4709)  time: 0.2512 (0.1696 -- 0.9519)  data: 0.0657 (0.0001 -- 0.7620)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2073 (0.3433)  acc1: 88.8889 (92.5311)  acc5: 100.0000 (99.5851)  time: 0.2743 (0.1354 -- 0.9519)  data: 0.0949 (0.0001 -- 0.7620)  max mem: 16413
Val: Total time: 0:00:09 (0.3361 s / it)
* Acc@1 93.568 Acc@5 99.378 loss 0.341
Accuracy of the network on the 482 val images: 93.57%
Max accuracy: 94.19%
Epoch: [186]  [  0/160]  eta: 0:29:36  lr: 0.000001  min_lr: 0.000000  loss: 1.4156 (1.4156)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0229 (6.0229)  time: 11.1011 (11.1011 -- 11.1011)  data: 8.8394 (8.8394 -- 8.8394)  max mem: 16413
[2023-09-23 09:26:19,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:26:19,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:26:19,980] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:26:19,980] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [186]  [ 20/160]  eta: 0:03:05  lr: 0.000001  min_lr: 0.000000  loss: 1.2488 (1.2340)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0578 (6.1545)  time: 0.8370 (0.5194 -- 3.8638)  data: 0.0014 (0.0004 -- 0.0040)  max mem: 16413
[2023-09-23 09:26:22,171] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29781
[2023-09-23 09:26:22,171] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:26:22,171] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29781
[2023-09-23 09:26:22,172] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:26:22,172] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [186]  [ 40/160]  eta: 0:02:14  lr: 0.000001  min_lr: 0.000000  loss: 1.4664 (1.3525)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7019 (6.5641)  time: 0.8987 (0.5163 -- 3.8628)  data: 0.0782 (0.0003 -- 1.2661)  max mem: 16413
Epoch: [186]  [ 60/160]  eta: 0:01:44  lr: 0.000001  min_lr: 0.000000  loss: 1.4999 (1.4073)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7970 (7.1716)  time: 0.8819 (0.5184 -- 4.5040)  data: 0.0066 (0.0003 -- 0.1073)  max mem: 16413
Epoch: [186]  [ 80/160]  eta: 0:01:20  lr: 0.000001  min_lr: 0.000000  loss: 1.4496 (1.4197)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0640 (7.2407)  time: 0.9204 (0.5246 -- 3.5698)  data: 0.1705 (0.0003 -- 2.3903)  max mem: 16413
Epoch: [186]  [100/160]  eta: 0:00:58  lr: 0.000001  min_lr: 0.000000  loss: 1.3443 (1.4056)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2241 (7.2204)  time: 0.8409 (0.5190 -- 2.3037)  data: 0.1365 (0.0003 -- 1.2286)  max mem: 16413
Epoch: [186]  [120/160]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.2209 (1.3791)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2303 (7.1590)  time: 0.8959 (0.5272 -- 4.0190)  data: 0.1621 (0.0005 -- 1.7171)  max mem: 16413
Epoch: [186]  [140/160]  eta: 0:00:19  lr: 0.000001  min_lr: 0.000000  loss: 1.2612 (1.3613)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9215 (7.1004)  time: 1.0239 (0.5204 -- 4.2015)  data: 0.0569 (0.0003 -- 1.1136)  max mem: 16413
[2023-09-23 09:28:17,946] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:28:17,946] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:28:17,948] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:28:17,948] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [186]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.3614 (1.3616)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2729 (7.0315)  time: 0.6701 (0.4922 -- 2.0999)  data: 0.0297 (0.0002 -- 0.5808)  max mem: 16413
Epoch: [186] Total time: 0:02:29 (0.9369 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.3614 (1.3884)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2729 (7.0315)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1327 (0.1327)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3903 (2.3903 -- 2.3903)  data: 2.1953 (2.1953 -- 2.1953)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1606 (0.3604)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4593 (0.1740 -- 2.3903)  data: 0.2730 (0.0003 -- 2.1953)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1788 (0.3264)  acc1: 88.8889 (92.0635)  acc5: 100.0000 (100.0000)  time: 0.2513 (0.1685 -- 0.9917)  data: 0.0666 (0.0001 -- 0.8036)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2190 (0.3730)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (100.0000)  time: 0.2738 (0.1327 -- 0.9917)  data: 0.0952 (0.0001 -- 0.8036)  max mem: 16413
Val: Total time: 0:00:08 (0.3323 s / it)
* Acc@1 91.909 Acc@5 99.793 loss 0.352
Accuracy of the network on the 482 val images: 91.91%
Max accuracy: 94.19%
Epoch: [187]  [  0/160]  eta: 0:25:06  lr: 0.000001  min_lr: 0.000000  loss: 1.7755 (1.7755)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1806 (8.1806)  time: 9.4127 (9.4127 -- 9.4127)  data: 8.8779 (8.8779 -- 8.8779)  max mem: 16413
Epoch: [187]  [ 20/160]  eta: 0:03:01  lr: 0.000001  min_lr: 0.000000  loss: 1.3997 (1.4223)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5312 (7.4064)  time: 0.8904 (0.5187 -- 4.6104)  data: 0.3506 (0.0003 -- 4.0974)  max mem: 16413
Epoch: [187]  [ 40/160]  eta: 0:02:13  lr: 0.000001  min_lr: 0.000000  loss: 1.4443 (1.4517)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5735 (7.6046)  time: 0.9191 (0.5156 -- 4.4901)  data: 0.3778 (0.0001 -- 3.9342)  max mem: 16413
Epoch: [187]  [ 60/160]  eta: 0:01:47  lr: 0.000001  min_lr: 0.000000  loss: 1.2724 (1.4255)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0269 (7.5132)  time: 1.0079 (0.5156 -- 2.6468)  data: 0.4770 (0.0004 -- 2.1067)  max mem: 16413
[2023-09-23 09:29:46,420] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29991
[2023-09-23 09:29:46,420] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29991
[2023-09-23 09:29:46,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:29:46,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:29:46,421] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 09:29:54,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=174, lr=[1.3231164921790851e-07, 1.3231164921790851e-07, 1.4701294357545393e-07, 1.4701294357545393e-07, 1.6334771508383767e-07, 1.6334771508383767e-07, 1.814974612042641e-07, 1.814974612042641e-07, 2.0166384578251563e-07, 2.0166384578251563e-07, 2.240709397583507e-07, 2.240709397583507e-07, 2.489677108426119e-07, 2.489677108426119e-07, 2.766307898251243e-07, 2.766307898251243e-07, 3.073675442501381e-07, 3.073675442501381e-07, 3.415194936112645e-07, 3.415194936112645e-07, 3.794661040125162e-07, 3.794661040125162e-07, 4.216290044583513e-07, 4.216290044583513e-07, 4.684766716203903e-07, 4.684766716203903e-07, 5.20529635133767e-07, 5.20529635133767e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 09:29:54,238] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=17.078574756573158, CurrSamplesPerSec=21.76781384003581, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [187]  [ 80/160]  eta: 0:01:20  lr: 0.000001  min_lr: 0.000000  loss: 1.3901 (1.4209)  loss_scale: 16384.0000 (30745.2840)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1309 (7.4872)  time: 0.8057 (0.5175 -- 3.9432)  data: 0.2584 (0.0003 -- 3.4284)  max mem: 16413
Epoch: [187]  [100/160]  eta: 0:01:01  lr: 0.000001  min_lr: 0.000000  loss: 1.2503 (1.3979)  loss_scale: 16384.0000 (27901.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7626 (7.5394)  time: 1.0815 (0.5215 -- 3.8592)  data: 0.5433 (0.0010 -- 3.3366)  max mem: 16413
Epoch: [187]  [120/160]  eta: 0:00:39  lr: 0.000001  min_lr: 0.000000  loss: 1.4467 (1.3968)  loss_scale: 16384.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0807 (7.4432)  time: 0.7918 (0.5175 -- 3.8692)  data: 0.2549 (0.0001 -- 3.3254)  max mem: 16413
Epoch: [187]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.4904 (1.4067)  loss_scale: 16384.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0017 (7.4995)  time: 0.9747 (0.5196 -- 4.0672)  data: 0.4359 (0.0004 -- 3.5472)  max mem: 16413
Epoch: [187]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3439 (1.4006)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0723 (7.4041)  time: 0.7985 (0.4933 -- 3.5976)  data: 0.2845 (0.0001 -- 3.1036)  max mem: 16413
Epoch: [187] Total time: 0:02:32 (0.9518 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3439 (1.3904)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0723 (7.4041)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1476 (0.1476)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5054 (2.5054 -- 2.5054)  data: 2.3149 (2.3149 -- 2.3149)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1330 (0.3700)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4817 (0.1777 -- 2.5054)  data: 0.2922 (0.0004 -- 2.3149)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1613 (0.3382)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (99.4709)  time: 0.2563 (0.1684 -- 1.0914)  data: 0.0698 (0.0001 -- 0.8935)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3335 (0.3907)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (99.5851)  time: 0.2902 (0.1323 -- 1.0914)  data: 0.1111 (0.0001 -- 0.8935)  max mem: 16413
Val: Total time: 0:00:09 (0.3498 s / it)
* Acc@1 92.946 Acc@5 99.585 loss 0.367
Accuracy of the network on the 482 val images: 92.95%
Max accuracy: 94.19%
Epoch: [188]  [  0/160]  eta: 0:22:17  lr: 0.000000  min_lr: 0.000000  loss: 1.0473 (1.0473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7136 (9.7136)  time: 8.3589 (8.3589 -- 8.3589)  data: 6.5750 (6.5750 -- 6.5750)  max mem: 16413
[2023-09-23 09:31:25,778] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30085
[2023-09-23 09:31:25,778] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30085
[2023-09-23 09:31:25,778] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 09:31:25,778] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 09:31:25,778] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [188]  [ 20/160]  eta: 0:02:57  lr: 0.000000  min_lr: 0.000000  loss: 1.4520 (1.4229)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1392 (7.4646)  time: 0.9155 (0.5112 -- 2.7693)  data: 0.2607 (0.0001 -- 2.2496)  max mem: 16413
Epoch: [188]  [ 40/160]  eta: 0:02:14  lr: 0.000000  min_lr: 0.000000  loss: 1.5174 (1.4468)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1570 (7.5451)  time: 0.9706 (0.5093 -- 4.3737)  data: 0.4366 (0.0002 -- 3.8309)  max mem: 16413
Epoch: [188]  [ 60/160]  eta: 0:01:48  lr: 0.000000  min_lr: 0.000000  loss: 1.4702 (1.4628)  loss_scale: 8192.0000 (8863.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6964 (7.5485)  time: 0.9903 (0.5188 -- 3.3258)  data: 0.3828 (0.0004 -- 2.7972)  max mem: 16413
Epoch: [188]  [ 80/160]  eta: 0:01:20  lr: 0.000000  min_lr: 0.000000  loss: 1.2318 (1.4300)  loss_scale: 8192.0000 (8697.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3432 (7.3189)  time: 0.7572 (0.5277 -- 2.6415)  data: 0.1078 (0.0005 -- 2.1212)  max mem: 16413
Epoch: [188]  [100/160]  eta: 0:00:59  lr: 0.000000  min_lr: 0.000000  loss: 1.4300 (1.4265)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0874 (7.2840)  time: 0.9718 (0.5283 -- 2.9646)  data: 0.1921 (0.0002 -- 2.4250)  max mem: 16413
Epoch: [188]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.3885 (1.4143)  loss_scale: 8192.0000 (8530.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5017 (7.3217)  time: 0.8460 (0.5212 -- 2.5861)  data: 0.2751 (0.0004 -- 2.0652)  max mem: 16413
[2023-09-23 09:33:27,573] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:33:27,574] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 09:33:27,575] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:33:27,576] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [188]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.3651 (1.4123)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4892 (7.2836)  time: 0.9448 (0.5282 -- 2.6216)  data: 0.1696 (0.0002 -- 2.0969)  max mem: 16413
Epoch: [188]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.1348 (1.3941)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4892 (7.2563)  time: 0.7881 (0.4958 -- 4.0752)  data: 0.0080 (0.0001 -- 0.1436)  max mem: 16413
Epoch: [188] Total time: 0:02:31 (0.9474 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.1348 (1.4000)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4892 (7.2563)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1311 (0.1311)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4963 (2.4963 -- 2.4963)  data: 2.2849 (2.2849 -- 2.2849)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1381 (0.3497)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4753 (0.1737 -- 2.4963)  data: 0.2865 (0.0003 -- 2.2849)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1421 (0.3132)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (98.9418)  time: 0.2566 (0.1680 -- 1.0578)  data: 0.0709 (0.0001 -- 0.8607)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2418 (0.3717)  acc1: 88.8889 (92.1162)  acc5: 100.0000 (99.1701)  time: 0.2872 (0.1323 -- 1.0578)  data: 0.1080 (0.0001 -- 0.8607)  max mem: 16413
Val: Total time: 0:00:09 (0.3465 s / it)
* Acc@1 92.739 Acc@5 99.378 loss 0.349
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 94.19%
Epoch: [189]  [  0/160]  eta: 0:22:46  lr: 0.000000  min_lr: 0.000000  loss: 0.9641 (0.9641)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9871 (10.9871)  time: 8.5431 (8.5431 -- 8.5431)  data: 6.2290 (6.2290 -- 6.2290)  max mem: 16413
Epoch: [189]  [ 20/160]  eta: 0:02:59  lr: 0.000000  min_lr: 0.000000  loss: 1.4434 (1.4678)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6691 (8.4911)  time: 0.9187 (0.5153 -- 4.5178)  data: 0.1540 (0.0004 -- 0.8607)  max mem: 16413
Epoch: [189]  [ 40/160]  eta: 0:02:17  lr: 0.000000  min_lr: 0.000000  loss: 1.3715 (1.4014)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5668 (7.4867)  time: 0.9990 (0.5247 -- 4.2238)  data: 0.1979 (0.0002 -- 1.6726)  max mem: 16413
Epoch: [189]  [ 60/160]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 1.3487 (1.3894)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2583 (7.4294)  time: 0.7642 (0.5198 -- 4.0749)  data: 0.0025 (0.0002 -- 0.0248)  max mem: 16413
Epoch: [189]  [ 80/160]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 1.3677 (1.3808)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0459 (7.3395)  time: 0.9293 (0.5323 -- 3.5913)  data: 0.0023 (0.0004 -- 0.0085)  max mem: 16413
Epoch: [189]  [100/160]  eta: 0:00:58  lr: 0.000000  min_lr: 0.000000  loss: 1.3904 (1.3779)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1147 (7.3275)  time: 0.9113 (0.5251 -- 4.2063)  data: 0.0668 (0.0004 -- 0.7942)  max mem: 16413
[2023-09-23 09:35:35,787] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:35:35,788] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:35:35,828] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:35:35,828] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [189]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.2757 (1.3719)  loss_scale: 32768.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1300 (7.2891)  time: 0.9176 (0.5325 -- 3.0470)  data: 0.0827 (0.0003 -- 1.4570)  max mem: 16413
[2023-09-23 09:36:05,625] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30371
[2023-09-23 09:36:05,625] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30371
[2023-09-23 09:36:05,625] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:36:05,625] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:36:05,625] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [189]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.4498 (1.3726)  loss_scale: 16384.0000 (19753.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9810 (7.1610)  time: 0.9829 (0.5197 -- 4.5132)  data: 0.1392 (0.0004 -- 1.9329)  max mem: 16413
Epoch: [189]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.2365 (1.3591)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1659 (7.0944)  time: 0.7059 (0.4917 -- 3.3118)  data: 0.0279 (0.0002 -- 0.5447)  max mem: 16413
Epoch: [189] Total time: 0:02:30 (0.9414 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.2365 (1.3746)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1659 (7.0944)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1225 (0.1225)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3777 (2.3777 -- 2.3777)  data: 2.1679 (2.1679 -- 2.1679)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1386 (0.3661)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (98.9899)  time: 0.4740 (0.1747 -- 2.3777)  data: 0.2869 (0.0003 -- 2.1679)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1532 (0.3204)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (99.4709)  time: 0.2568 (0.1696 -- 1.1651)  data: 0.0724 (0.0001 -- 0.9821)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2102 (0.3731)  acc1: 100.0000 (91.7012)  acc5: 100.0000 (99.5851)  time: 0.2788 (0.1327 -- 1.1651)  data: 0.1008 (0.0001 -- 0.9821)  max mem: 16413
Val: Total time: 0:00:09 (0.3359 s / it)
* Acc@1 93.361 Acc@5 99.585 loss 0.337
Accuracy of the network on the 482 val images: 93.36%
Max accuracy: 94.19%
Epoch: [190]  [  0/160]  eta: 0:19:05  lr: 0.000000  min_lr: 0.000000  loss: 0.9253 (0.9253)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9555 (3.9555)  time: 7.1568 (7.1568 -- 7.1568)  data: 6.6258 (6.6258 -- 6.6258)  max mem: 16413
Epoch: [190]  [ 20/160]  eta: 0:02:47  lr: 0.000000  min_lr: 0.000000  loss: 1.2642 (1.2997)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2061 (7.1078)  time: 0.8997 (0.5269 -- 3.7252)  data: 0.3207 (0.0006 -- 3.1932)  max mem: 16413
Epoch: [190]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 1.3748 (1.3141)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3665 (7.0495)  time: 0.9043 (0.5307 -- 2.9618)  data: 0.2408 (0.0002 -- 2.4343)  max mem: 16413
Epoch: [190]  [ 60/160]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 1.3566 (1.3314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2369 (6.8891)  time: 0.9358 (0.5265 -- 2.5263)  data: 0.1560 (0.0004 -- 1.9478)  max mem: 16413
Epoch: [190]  [ 80/160]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 1.3584 (1.3383)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0211 (6.8759)  time: 0.9072 (0.5192 -- 4.3180)  data: 0.3646 (0.0003 -- 3.7880)  max mem: 16413
[2023-09-23 09:38:14,088] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:38:14,089] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:38:14,091] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:38:14,092] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [190]  [100/160]  eta: 0:00:58  lr: 0.000000  min_lr: 0.000000  loss: 1.4665 (1.3602)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8192 (6.8664)  time: 0.9109 (0.5131 -- 2.6951)  data: 0.3672 (0.0004 -- 2.1636)  max mem: 16413
[2023-09-23 09:38:23,235] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30508
[2023-09-23 09:38:23,235] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:38:23,235] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30508
[2023-09-23 09:38:23,235] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:38:23,235] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [190]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.1866 (1.3550)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2233 (6.8336)  time: 0.9081 (0.5095 -- 3.9305)  data: 0.3778 (0.0003 -- 3.3929)  max mem: 16413
Epoch: [190]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.1945 (1.3420)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5168 (6.9196)  time: 0.9455 (0.5222 -- 3.1692)  data: 0.4127 (0.0004 -- 2.6557)  max mem: 16413
Epoch: [190]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3477 (1.3505)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9002 (6.9001)  time: 0.7050 (0.4946 -- 2.5753)  data: 0.1862 (0.0002 -- 2.0269)  max mem: 16413
Epoch: [190] Total time: 0:02:28 (0.9311 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3477 (1.3647)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9002 (6.9001)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1296 (0.1296)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3761 (2.3761 -- 2.3761)  data: 2.1917 (2.1917 -- 2.1917)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1508 (0.3261)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4636 (0.1770 -- 2.3761)  data: 0.2748 (0.0003 -- 2.1917)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1517 (0.3022)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2557 (0.1686 -- 1.0232)  data: 0.0688 (0.0001 -- 0.8258)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1971 (0.3563)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (100.0000)  time: 0.2846 (0.1336 -- 1.0232)  data: 0.1059 (0.0001 -- 0.8258)  max mem: 16413
Val: Total time: 0:00:09 (0.3415 s / it)
* Acc@1 92.946 Acc@5 100.000 loss 0.338
Accuracy of the network on the 482 val images: 92.95%
Max accuracy: 94.19%
Epoch: [191]  [  0/160]  eta: 0:22:58  lr: 0.000000  min_lr: 0.000000  loss: 0.8004 (0.8004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0566 (4.0566)  time: 8.6129 (8.6129 -- 8.6129)  data: 8.0647 (8.0647 -- 8.0647)  max mem: 16413
Epoch: [191]  [ 20/160]  eta: 0:02:56  lr: 0.000000  min_lr: 0.000000  loss: 1.3927 (1.3279)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9984 (6.5208)  time: 0.8965 (0.5179 -- 3.8717)  data: 0.1451 (0.0002 -- 2.0051)  max mem: 16413
[2023-09-23 09:39:42,368] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30583
[2023-09-23 09:39:42,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 09:39:42,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30583
[2023-09-23 09:39:42,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 09:39:42,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [191]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.5520 (1.4154)  loss_scale: 8192.0000 (12787.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3553 (6.6078)  time: 0.8492 (0.5233 -- 2.1651)  data: 0.0796 (0.0004 -- 1.5553)  max mem: 16413
Epoch: [191]  [ 60/160]  eta: 0:01:43  lr: 0.000000  min_lr: 0.000000  loss: 1.4006 (1.4150)  loss_scale: 8192.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4647 (6.5546)  time: 0.9954 (0.5214 -- 3.1578)  data: 0.0845 (0.0006 -- 0.8437)  max mem: 16413
Epoch: [191]  [ 80/160]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 1.3581 (1.3904)  loss_scale: 8192.0000 (10518.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4173 (6.7162)  time: 0.8672 (0.5255 -- 4.4571)  data: 0.0432 (0.0005 -- 0.8419)  max mem: 16413
Epoch: [191]  [100/160]  eta: 0:01:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4409 (1.4054)  loss_scale: 8192.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9871 (6.8654)  time: 1.0705 (0.5142 -- 4.5402)  data: 0.0011 (0.0001 -- 0.0041)  max mem: 16413
Epoch: [191]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.3107 (1.3820)  loss_scale: 8192.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2627 (6.9448)  time: 0.7587 (0.5228 -- 2.8469)  data: 0.0027 (0.0003 -- 0.0103)  max mem: 16413
Epoch: [191]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.1013 (1.3605)  loss_scale: 8192.0000 (9528.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1476 (7.0098)  time: 1.0036 (0.5208 -- 4.5138)  data: 0.0013 (0.0003 -- 0.0034)  max mem: 16413
[2023-09-23 09:41:41,063] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:41:41,063] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:41:41,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 09:41:41,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [191]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4041 (1.3728)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6657 (6.9115)  time: 0.7075 (0.4917 -- 2.4592)  data: 0.0007 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [191] Total time: 0:02:31 (0.9441 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4041 (1.3662)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6657 (6.9115)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1388 (0.1388)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4568 (2.4568 -- 2.4568)  data: 2.2469 (2.2469 -- 2.2469)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1388 (0.3625)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (98.9899)  time: 0.4780 (0.1827 -- 2.4568)  data: 0.2820 (0.0004 -- 2.2469)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1571 (0.3211)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (99.4709)  time: 0.2568 (0.1682 -- 1.0559)  data: 0.0681 (0.0001 -- 0.8413)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1843 (0.3590)  acc1: 100.0000 (92.5311)  acc5: 100.0000 (99.5851)  time: 0.2805 (0.1320 -- 1.0559)  data: 0.1018 (0.0001 -- 0.8413)  max mem: 16413
Val: Total time: 0:00:09 (0.3430 s / it)
* Acc@1 92.739 Acc@5 99.378 loss 0.343
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 94.19%
Epoch: [192]  [  0/160]  eta: 0:21:38  lr: 0.000000  min_lr: 0.000000  loss: 1.0549 (1.0549)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1512 (5.1512)  time: 8.1164 (8.1164 -- 8.1164)  data: 6.9663 (6.9663 -- 6.9663)  max mem: 16413
Epoch: [192]  [ 20/160]  eta: 0:02:49  lr: 0.000000  min_lr: 0.000000  loss: 1.2638 (1.3069)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1672 (6.9935)  time: 0.8646 (0.5086 -- 4.3089)  data: 0.0041 (0.0003 -- 0.0454)  max mem: 16413
Epoch: [192]  [ 40/160]  eta: 0:02:20  lr: 0.000000  min_lr: 0.000000  loss: 1.4338 (1.3627)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9474 (7.2643)  time: 1.1252 (0.5187 -- 4.9756)  data: 0.5905 (0.0004 -- 4.4529)  max mem: 16413
Epoch: [192]  [ 60/160]  eta: 0:01:48  lr: 0.000000  min_lr: 0.000000  loss: 1.4026 (1.3744)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2680 (7.1946)  time: 0.9079 (0.5167 -- 3.8127)  data: 0.2074 (0.0004 -- 2.1872)  max mem: 16413
Epoch: [192]  [ 80/160]  eta: 0:01:21  lr: 0.000000  min_lr: 0.000000  loss: 1.4194 (1.3898)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9751 (7.0400)  time: 0.8460 (0.5164 -- 4.3520)  data: 0.0015 (0.0002 -- 0.0070)  max mem: 16413
Epoch: [192]  [100/160]  eta: 0:01:01  lr: 0.000000  min_lr: 0.000000  loss: 1.5563 (1.4155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6665 (7.1059)  time: 0.9935 (0.5133 -- 4.1557)  data: 0.0015 (0.0003 -- 0.0065)  max mem: 16413
[2023-09-23 09:43:54,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:43:54,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:43:54,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:43:54,239] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [192]  [120/160]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.3872 (1.4137)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9224 (6.9776)  time: 0.8423 (0.5160 -- 3.5924)  data: 0.0217 (0.0002 -- 0.4052)  max mem: 16413
Epoch: [192]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.4201 (1.4209)  loss_scale: 32768.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4096 (6.9398)  time: 0.9540 (0.5278 -- 4.4005)  data: 0.0365 (0.0004 -- 0.6997)  max mem: 16413
Epoch: [192]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4881 (1.4240)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2148 (6.8819)  time: 0.7181 (0.4937 -- 2.8426)  data: 0.0012 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [192] Total time: 0:02:32 (0.9539 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4881 (1.4208)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2148 (6.8819)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1232 (0.1232)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4335 (2.4335 -- 2.4335)  data: 2.2497 (2.2497 -- 2.2497)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1311 (0.3200)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (98.9899)  time: 0.4576 (0.1838 -- 2.4335)  data: 0.2642 (0.0002 -- 2.2497)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.2330 (0.3361)  acc1: 88.8889 (91.0053)  acc5: 100.0000 (99.4709)  time: 0.2516 (0.1685 -- 0.8490)  data: 0.0641 (0.0001 -- 0.6475)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3036 (0.3921)  acc1: 88.8889 (90.0415)  acc5: 100.0000 (99.1701)  time: 0.2786 (0.1322 -- 0.8994)  data: 0.1003 (0.0001 -- 0.7292)  max mem: 16413
Val: Total time: 0:00:09 (0.3399 s / it)
* Acc@1 91.494 Acc@5 99.378 loss 0.351
Accuracy of the network on the 482 val images: 91.49%
Max accuracy: 94.19%
Epoch: [193]  [  0/160]  eta: 0:22:05  lr: 0.000000  min_lr: 0.000000  loss: 1.2847 (1.2847)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6232 (12.6232)  time: 8.2862 (8.2862 -- 8.2862)  data: 7.7503 (7.7503 -- 7.7503)  max mem: 16413
[2023-09-23 09:44:52,507] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30890
[2023-09-23 09:44:52,507] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30890
[2023-09-23 09:44:52,507] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:44:52,507] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:44:52,508] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [193]  [ 20/160]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 1.3079 (1.3422)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7459 (7.6657)  time: 0.8137 (0.5305 -- 2.8953)  data: 0.2351 (0.0003 -- 2.3771)  max mem: 16413
Epoch: [193]  [ 40/160]  eta: 0:02:11  lr: 0.000000  min_lr: 0.000000  loss: 1.3413 (1.3523)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6754 (7.6960)  time: 1.0109 (0.5181 -- 3.3594)  data: 0.3196 (0.0001 -- 2.8339)  max mem: 16413
Epoch: [193]  [ 60/160]  eta: 0:01:45  lr: 0.000000  min_lr: 0.000000  loss: 1.3177 (1.3392)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6887 (7.3765)  time: 0.9706 (0.5168 -- 4.1088)  data: 0.0073 (0.0002 -- 0.1011)  max mem: 16413
Epoch: [193]  [ 80/160]  eta: 0:01:20  lr: 0.000000  min_lr: 0.000000  loss: 1.2298 (1.3305)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8928 (7.4570)  time: 0.8715 (0.5220 -- 3.3739)  data: 0.1335 (0.0003 -- 1.8790)  max mem: 16413
Epoch: [193]  [100/160]  eta: 0:00:59  lr: 0.000000  min_lr: 0.000000  loss: 1.3146 (1.3377)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6367 (7.4977)  time: 0.8941 (0.5210 -- 2.3349)  data: 0.2894 (0.0002 -- 1.7912)  max mem: 16413
[2023-09-23 09:46:33,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=179, lr=[4.212080066010446e-08, 4.212080066010446e-08, 4.68008896223383e-08, 4.68008896223383e-08, 5.200098846926476e-08, 5.200098846926476e-08, 5.777887607696085e-08, 5.777887607696085e-08, 6.419875119662316e-08, 6.419875119662316e-08, 7.133194577402574e-08, 7.133194577402574e-08, 7.925771752669526e-08, 7.925771752669526e-08, 8.806413058521695e-08, 8.806413058521695e-08, 9.784903398357439e-08, 9.784903398357439e-08, 1.087211488706382e-07, 1.087211488706382e-07, 1.2080127652293135e-07, 1.2080127652293135e-07, 1.342236405810348e-07, 1.342236405810348e-07, 1.4913737842337202e-07, 1.4913737842337202e-07, 1.6570819824819113e-07, 1.6570819824819113e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 09:46:33,163] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=17.07286202644666, CurrSamplesPerSec=22.652587387157084, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [193]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.2991 (1.3329)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2268 (7.2971)  time: 0.8863 (0.5097 -- 3.9481)  data: 0.3290 (0.0001 -- 3.2185)  max mem: 16413
[2023-09-23 09:46:52,274] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:46:52,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:46:52,274] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:46:52,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [193]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.2848 (1.3294)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0122 (7.2799)  time: 0.9549 (0.5173 -- 3.4950)  data: 0.2284 (0.0003 -- 1.9279)  max mem: 16413
[2023-09-23 09:47:07,008] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31038
[2023-09-23 09:47:07,008] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:47:07,008] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31038
[2023-09-23 09:47:07,008] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:47:07,009] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [193]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4479 (1.3402)  loss_scale: 32768.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0067 (7.2667)  time: 0.7611 (0.4839 -- 4.9169)  data: 0.1180 (0.0002 -- 2.3439)  max mem: 16413
Epoch: [193] Total time: 0:02:31 (0.9441 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4479 (1.3685)  loss_scale: 32768.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0067 (7.2667)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1501 (0.1501)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3632 (2.3632 -- 2.3632)  data: 2.1840 (2.1840 -- 2.1840)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1638 (0.3649)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4826 (0.1808 -- 2.3632)  data: 0.2914 (0.0003 -- 2.1840)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1657 (0.3383)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2602 (0.1715 -- 1.2079)  data: 0.0682 (0.0001 -- 1.0117)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1789 (0.3857)  acc1: 88.8889 (92.1162)  acc5: 100.0000 (100.0000)  time: 0.2948 (0.1328 -- 1.2079)  data: 0.1087 (0.0001 -- 1.0117)  max mem: 16413
Val: Total time: 0:00:09 (0.3474 s / it)
* Acc@1 93.154 Acc@5 99.378 loss 0.353
Accuracy of the network on the 482 val images: 93.15%
Max accuracy: 94.19%
Epoch: [194]  [  0/160]  eta: 0:23:54  lr: 0.000000  min_lr: 0.000000  loss: 1.4532 (1.4532)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1188 (12.1188)  time: 8.9650 (8.9650 -- 8.9650)  data: 6.6444 (6.6444 -- 6.6444)  max mem: 16413
Epoch: [194]  [ 20/160]  eta: 0:02:58  lr: 0.000000  min_lr: 0.000000  loss: 1.4978 (1.4731)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5224 (7.5996)  time: 0.8910 (0.5116 -- 3.2885)  data: 0.2772 (0.0005 -- 2.3429)  max mem: 16413
Epoch: [194]  [ 40/160]  eta: 0:02:12  lr: 0.000000  min_lr: 0.000000  loss: 1.3091 (1.4270)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8276 (7.2680)  time: 0.9327 (0.5232 -- 4.4953)  data: 0.3532 (0.0002 -- 3.9915)  max mem: 16413
Epoch: [194]  [ 60/160]  eta: 0:01:44  lr: 0.000000  min_lr: 0.000000  loss: 1.2434 (1.3859)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8470 (7.0523)  time: 0.9253 (0.5047 -- 2.6391)  data: 0.3839 (0.0007 -- 2.1163)  max mem: 16413
Epoch: [194]  [ 80/160]  eta: 0:01:24  lr: 0.000000  min_lr: 0.000000  loss: 1.3808 (1.3865)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0882 (6.9178)  time: 1.0745 (0.5269 -- 4.7393)  data: 0.5333 (0.0002 -- 4.2029)  max mem: 16413
Epoch: [194]  [100/160]  eta: 0:01:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4669 (1.4019)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3175 (6.9230)  time: 0.8037 (0.5124 -- 3.9879)  data: 0.2703 (0.0002 -- 3.4533)  max mem: 16413
Epoch: [194]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.3790 (1.3935)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7525 (6.9917)  time: 0.7893 (0.5190 -- 3.7331)  data: 0.2397 (0.0005 -- 3.1884)  max mem: 16413
[2023-09-23 09:49:21,668] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:49:21,668] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:49:21,675] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:49:21,676] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [194]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.3255 (1.3927)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2618 (7.0879)  time: 0.9274 (0.5370 -- 3.9483)  data: 0.3796 (0.0007 -- 3.4237)  max mem: 16413
Epoch: [194]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.2947 (1.3884)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6532 (7.1494)  time: 0.7319 (0.4931 -- 2.5224)  data: 0.2032 (0.0002 -- 1.9926)  max mem: 16413
Epoch: [194] Total time: 0:02:29 (0.9373 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.2947 (1.4019)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6532 (7.1494)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1904 (0.1904)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4467 (2.4467 -- 2.4467)  data: 2.2380 (2.2380 -- 2.2380)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1904 (0.3589)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4810 (0.1802 -- 2.4467)  data: 0.2907 (0.0003 -- 2.2380)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1814 (0.3440)  acc1: 88.8889 (93.1217)  acc5: 100.0000 (100.0000)  time: 0.2575 (0.1684 -- 1.1532)  data: 0.0711 (0.0001 -- 0.9534)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3078 (0.4051)  acc1: 88.8889 (90.8714)  acc5: 100.0000 (100.0000)  time: 0.2822 (0.1326 -- 1.1532)  data: 0.1025 (0.0001 -- 0.9534)  max mem: 16413
Val: Total time: 0:00:09 (0.3413 s / it)
* Acc@1 92.531 Acc@5 100.000 loss 0.362
Accuracy of the network on the 482 val images: 92.53%
Max accuracy: 94.19%
Epoch: [195]  [  0/160]  eta: 0:21:39  lr: 0.000000  min_lr: 0.000000  loss: 0.8749 (0.8749)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4880 (6.4880)  time: 8.1197 (8.1197 -- 8.1197)  data: 7.2254 (7.2254 -- 7.2254)  max mem: 16413
Epoch: [195]  [ 20/160]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.2266 (1.2610)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6742 (6.9779)  time: 0.8408 (0.5205 -- 3.3031)  data: 0.0446 (0.0003 -- 0.5409)  max mem: 16413
Epoch: [195]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.4312 (1.3428)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6335 (7.1599)  time: 0.9293 (0.5186 -- 3.6421)  data: 0.2327 (0.0003 -- 2.6014)  max mem: 16413
Epoch: [195]  [ 60/160]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 1.4843 (1.3686)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3079 (7.1038)  time: 0.9341 (0.5254 -- 4.0481)  data: 0.3919 (0.0002 -- 3.5307)  max mem: 16413
Epoch: [195]  [ 80/160]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 1.4022 (1.3679)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6997 (6.9759)  time: 0.9065 (0.5223 -- 3.9827)  data: 0.3405 (0.0003 -- 3.4703)  max mem: 16413
[2023-09-23 09:51:31,526] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:51:31,527] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-23 09:51:31,533] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:51:31,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [195]  [100/160]  eta: 0:00:59  lr: 0.000000  min_lr: 0.000000  loss: 1.3090 (1.3595)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6334 (6.8757)  time: 1.0144 (0.5127 -- 4.3406)  data: 0.4775 (0.0004 -- 3.8214)  max mem: 16413
[2023-09-23 09:51:39,209] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31304
[2023-09-23 09:51:39,209] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31304
[2023-09-23 09:51:39,209] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 09:51:39,209] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-23 09:51:39,209] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [195]  [120/160]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.3409 (1.3664)  loss_scale: 32768.0000 (35205.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5191 (6.8643)  time: 0.9179 (0.5057 -- 6.1113)  data: 0.3840 (0.0003 -- 5.6029)  max mem: 16413
[2023-09-23 09:52:02,068] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31328
[2023-09-23 09:52:02,068] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:52:02,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 09:52:02,068] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31328
[2023-09-23 09:52:02,068] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [195]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.5275 (1.3872)  loss_scale: 16384.0000 (33348.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4189 (6.8219)  time: 0.8975 (0.5161 -- 3.8476)  data: 0.3536 (0.0004 -- 3.3153)  max mem: 16413
Epoch: [195]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3325 (1.3770)  loss_scale: 16384.0000 (31334.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3955 (6.8600)  time: 0.6987 (0.4938 -- 2.0976)  data: 0.1429 (0.0002 -- 0.9852)  max mem: 16413
Epoch: [195] Total time: 0:02:30 (0.9399 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3325 (1.3825)  loss_scale: 16384.0000 (31334.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3955 (6.8600)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1243 (0.1243)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3710 (2.3710 -- 2.3710)  data: 2.1702 (2.1702 -- 2.1702)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1565 (0.3763)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (100.0000)  time: 0.4682 (0.1783 -- 2.3710)  data: 0.2767 (0.0002 -- 2.1702)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1592 (0.3464)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2593 (0.1679 -- 1.0805)  data: 0.0715 (0.0001 -- 0.8680)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2555 (0.3846)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (100.0000)  time: 0.2748 (0.1327 -- 1.0805)  data: 0.0935 (0.0001 -- 0.8680)  max mem: 16413
Val: Total time: 0:00:08 (0.3325 s / it)
* Acc@1 93.361 Acc@5 99.793 loss 0.337
Accuracy of the network on the 482 val images: 93.36%
Max accuracy: 94.19%
Epoch: [196]  [  0/160]  eta: 0:21:18  lr: 0.000000  min_lr: 0.000000  loss: 1.2295 (1.2295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3023 (5.3023)  time: 7.9923 (7.9923 -- 7.9923)  data: 6.5718 (6.5718 -- 6.5718)  max mem: 16413
Epoch: [196]  [ 20/160]  eta: 0:02:47  lr: 0.000000  min_lr: 0.000000  loss: 1.4322 (1.4015)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2618 (7.5270)  time: 0.8532 (0.5293 -- 2.6585)  data: 0.1990 (0.0004 -- 1.3777)  max mem: 16413
Epoch: [196]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 1.3762 (1.4066)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3763 (7.0585)  time: 0.9394 (0.5197 -- 3.5826)  data: 0.2696 (0.0004 -- 2.1041)  max mem: 16413
Epoch: [196]  [ 60/160]  eta: 0:01:45  lr: 0.000000  min_lr: 0.000000  loss: 1.3852 (1.4105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3020 (7.0279)  time: 1.0200 (0.5307 -- 3.0524)  data: 0.1371 (0.0002 -- 1.1325)  max mem: 16413
Epoch: [196]  [ 80/160]  eta: 0:01:20  lr: 0.000000  min_lr: 0.000000  loss: 1.4676 (1.4094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5621 (7.2450)  time: 0.8554 (0.5242 -- 3.0069)  data: 0.2830 (0.0002 -- 2.4844)  max mem: 16413
[2023-09-23 09:54:15,719] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:54:15,719] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:54:15,719] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:54:15,719] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [196]  [100/160]  eta: 0:01:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3295 (1.4056)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7168 (7.3157)  time: 0.9843 (0.5149 -- 3.3651)  data: 0.2095 (0.0003 -- 2.1165)  max mem: 16413
[2023-09-23 09:54:34,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31480
[2023-09-23 09:54:34,266] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:54:34,266] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31480
[2023-09-23 09:54:34,266] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-23 09:54:34,266] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [196]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.3946 (1.4089)  loss_scale: 32768.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6194 (7.2664)  time: 0.8443 (0.5156 -- 3.3284)  data: 0.0913 (0.0002 -- 0.9036)  max mem: 16413
Epoch: [196]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.4472 (1.4025)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5217 (7.2088)  time: 0.9929 (0.5246 -- 3.9997)  data: 0.2164 (0.0005 -- 1.4159)  max mem: 16413
Epoch: [196]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.2671 (1.3944)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2424 (7.1837)  time: 0.6386 (0.4937 -- 1.8748)  data: 0.0317 (0.0002 -- 0.6165)  max mem: 16413
Epoch: [196] Total time: 0:02:30 (0.9379 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.2671 (1.3827)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2424 (7.1837)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1869 (0.1869)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4456 (2.4456 -- 2.4456)  data: 2.2506 (2.2506 -- 2.2506)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1869 (0.3590)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4725 (0.1740 -- 2.4456)  data: 0.2819 (0.0002 -- 2.2506)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1773 (0.3221)  acc1: 100.0000 (94.1799)  acc5: 100.0000 (99.4709)  time: 0.2590 (0.1715 -- 1.0406)  data: 0.0695 (0.0001 -- 0.8335)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2553 (0.3950)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (99.5851)  time: 0.2851 (0.1322 -- 1.0406)  data: 0.1033 (0.0001 -- 0.8335)  max mem: 16413
Val: Total time: 0:00:09 (0.3444 s / it)
* Acc@1 92.739 Acc@5 99.585 loss 0.360
Accuracy of the network on the 482 val images: 92.74%
Max accuracy: 94.19%
Epoch: [197]  [  0/160]  eta: 0:22:27  lr: 0.000000  min_lr: 0.000000  loss: 1.6430 (1.6430)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9473 (6.9473)  time: 8.4213 (8.4213 -- 8.4213)  data: 7.8730 (7.8730 -- 7.8730)  max mem: 16413
Epoch: [197]  [ 20/160]  eta: 0:02:53  lr: 0.000000  min_lr: 0.000000  loss: 1.4771 (1.3995)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7502 (6.9122)  time: 0.8770 (0.5235 -- 3.1363)  data: 0.3295 (0.0004 -- 2.5885)  max mem: 16413
Epoch: [197]  [ 40/160]  eta: 0:02:10  lr: 0.000000  min_lr: 0.000000  loss: 1.1643 (1.3533)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7814 (6.7975)  time: 0.9246 (0.5196 -- 4.6040)  data: 0.3832 (0.0004 -- 4.0934)  max mem: 16413
Epoch: [197]  [ 60/160]  eta: 0:01:44  lr: 0.000000  min_lr: 0.000000  loss: 1.5445 (1.4017)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5905 (6.8662)  time: 0.9645 (0.5165 -- 4.1547)  data: 0.4302 (0.0004 -- 3.6393)  max mem: 16413
Epoch: [197]  [ 80/160]  eta: 0:01:20  lr: 0.000000  min_lr: 0.000000  loss: 1.3652 (1.3885)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6241 (7.0953)  time: 0.8644 (0.5145 -- 4.9146)  data: 0.3284 (0.0004 -- 4.3843)  max mem: 16413
[2023-09-23 09:56:47,566] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:56:47,567] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:56:47,567] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:56:47,567] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:56:48,095] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31610
[2023-09-23 09:56:48,095] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:56:48,095] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31610
[2023-09-23 09:56:48,095] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:56:48,095] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [197]  [100/160]  eta: 0:00:59  lr: 0.000000  min_lr: 0.000000  loss: 1.1971 (1.3703)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2969 (6.9714)  time: 0.9307 (0.5192 -- 3.9685)  data: 0.3956 (0.0003 -- 3.4422)  max mem: 16413
Epoch: [197]  [120/160]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.3934 (1.3837)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8242 (6.8435)  time: 0.9355 (0.5217 -- 4.5000)  data: 0.3929 (0.0003 -- 3.9952)  max mem: 16413
Epoch: [197]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.3219 (1.3883)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6288 (6.8762)  time: 1.0392 (0.5119 -- 4.3369)  data: 0.5058 (0.0004 -- 3.8320)  max mem: 16413
Epoch: [197]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5089 (1.4052)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8889 (6.9075)  time: 0.6746 (0.4920 -- 2.7160)  data: 0.1629 (0.0002 -- 2.2107)  max mem: 16413
Epoch: [197] Total time: 0:02:32 (0.9508 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5089 (1.3647)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8889 (6.9075)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1379 (0.1379)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4711 (2.4711 -- 2.4711)  data: 2.2793 (2.2793 -- 2.2793)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1379 (0.3312)  acc1: 100.0000 (93.9394)  acc5: 100.0000 (98.9899)  time: 0.4834 (0.1810 -- 2.4711)  data: 0.2848 (0.0003 -- 2.2793)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1610 (0.3320)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (99.4709)  time: 0.2583 (0.1682 -- 1.0600)  data: 0.0670 (0.0001 -- 0.8388)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1842 (0.3826)  acc1: 88.8889 (91.7012)  acc5: 100.0000 (99.5851)  time: 0.2798 (0.1327 -- 1.0600)  data: 0.0987 (0.0001 -- 0.8388)  max mem: 16413
Val: Total time: 0:00:09 (0.3432 s / it)
* Acc@1 93.154 Acc@5 99.378 loss 0.350
Accuracy of the network on the 482 val images: 93.15%
Max accuracy: 94.19%
Epoch: [198]  [  0/160]  eta: 0:20:05  lr: 0.000000  min_lr: 0.000000  loss: 1.2922 (1.2922)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2973 (7.2973)  time: 7.5351 (7.5351 -- 7.5351)  data: 6.5501 (6.5501 -- 6.5501)  max mem: 16413
Epoch: [198]  [ 20/160]  eta: 0:02:50  lr: 0.000000  min_lr: 0.000000  loss: 1.3184 (1.3645)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9973 (7.2593)  time: 0.9017 (0.5212 -- 4.2577)  data: 0.1440 (0.0005 -- 2.2536)  max mem: 16413
Epoch: [198]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 1.3833 (1.4064)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3070 (7.2648)  time: 0.8756 (0.5244 -- 2.4442)  data: 0.2372 (0.0002 -- 1.6911)  max mem: 16413
[2023-09-23 09:59:00,096] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:59:00,096] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-23 09:59:00,097] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 09:59:00,097] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [198]  [ 60/160]  eta: 0:01:43  lr: 0.000000  min_lr: 0.000000  loss: 1.2903 (1.3708)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1058 (7.1366)  time: 1.0014 (0.5140 -- 3.3586)  data: 0.0877 (0.0002 -- 0.9833)  max mem: 16413
[2023-09-23 09:59:07,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31748
[2023-09-23 09:59:07,930] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:59:08,053] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31748
[2023-09-23 09:59:08,054] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-23 09:59:08,054] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [198]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.4266 (1.3668)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2527 (6.9867)  time: 0.8352 (0.5177 -- 1.9664)  data: 0.1253 (0.0002 -- 1.4007)  max mem: 16413
Epoch: [198]  [100/160]  eta: 0:00:58  lr: 0.000000  min_lr: 0.000000  loss: 1.6128 (1.3896)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0462 (6.8421)  time: 0.9045 (0.5170 -- 2.7614)  data: 0.1652 (0.0008 -- 1.9448)  max mem: 16413
Epoch: [198]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.3636 (1.3972)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5778 (6.8817)  time: 0.9103 (0.5191 -- 4.4639)  data: 0.2433 (0.0002 -- 3.7478)  max mem: 16413
Epoch: [198]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.4576 (1.3856)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4849 (6.7512)  time: 1.0495 (0.5146 -- 4.5780)  data: 0.1815 (0.0003 -- 3.3044)  max mem: 16413
Epoch: [198]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3500 (1.3770)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7541 (6.8587)  time: 0.6533 (0.4926 -- 2.6061)  data: 0.0006 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [198] Total time: 0:02:29 (0.9354 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3500 (1.3815)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7541 (6.8587)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1352 (0.1352)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4170 (2.4170 -- 2.4170)  data: 2.2251 (2.2251 -- 2.2251)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1352 (0.3562)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (98.9899)  time: 0.4739 (0.1734 -- 2.4170)  data: 0.2799 (0.0003 -- 2.2251)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1612 (0.3425)  acc1: 100.0000 (93.1217)  acc5: 100.0000 (99.4709)  time: 0.2557 (0.1693 -- 1.0849)  data: 0.0675 (0.0001 -- 0.8495)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.2636 (0.3940)  acc1: 88.8889 (90.4564)  acc5: 100.0000 (99.5851)  time: 0.2846 (0.1324 -- 1.0849)  data: 0.1032 (0.0001 -- 0.8495)  max mem: 16413
Val: Total time: 0:00:09 (0.3417 s / it)
* Acc@1 91.909 Acc@5 99.378 loss 0.361
Accuracy of the network on the 482 val images: 91.91%
Max accuracy: 94.19%
Epoch: [199]  [  0/160]  eta: 0:23:26  lr: 0.000000  min_lr: 0.000000  loss: 1.2057 (1.2057)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9440 (6.9440)  time: 8.7892 (8.7892 -- 8.7892)  data: 8.2376 (8.2376 -- 8.2376)  max mem: 16413
Epoch: [199]  [ 20/160]  eta: 0:02:53  lr: 0.000000  min_lr: 0.000000  loss: 1.3639 (1.3780)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7030 (6.3504)  time: 0.8593 (0.5235 -- 3.5920)  data: 0.2533 (0.0003 -- 3.0682)  max mem: 16413
[2023-09-23 10:01:10,372] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31867
[2023-09-23 10:01:10,372] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31867
[2023-09-23 10:01:10,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 10:01:10,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-23 10:01:10,373] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [199]  [ 40/160]  eta: 0:02:11  lr: 0.000000  min_lr: 0.000000  loss: 1.3787 (1.3974)  loss_scale: 8192.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6185 (7.1345)  time: 0.9436 (0.5119 -- 2.9281)  data: 0.1437 (0.0001 -- 2.3728)  max mem: 16413
Epoch: [199]  [ 60/160]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 1.2453 (1.3683)  loss_scale: 8192.0000 (11817.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0814 (7.1805)  time: 0.8430 (0.5342 -- 2.5897)  data: 0.0103 (0.0003 -- 0.1775)  max mem: 16413
Epoch: [199]  [ 80/160]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 1.4961 (1.3933)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9727 (7.1038)  time: 0.9415 (0.5239 -- 4.3392)  data: 0.0691 (0.0003 -- 1.3500)  max mem: 16413
Epoch: [199]  [100/160]  eta: 0:01:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4791 (1.4034)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6975 (7.4194)  time: 1.0329 (0.5163 -- 4.1683)  data: 0.0592 (0.0004 -- 1.1559)  max mem: 16413
Epoch: [199]  [120/160]  eta: 0:00:39  lr: 0.000000  min_lr: 0.000000  loss: 1.2518 (1.3968)  loss_scale: 8192.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3575 (7.4354)  time: 0.8834 (0.5231 -- 4.0322)  data: 0.0017 (0.0006 -- 0.0043)  max mem: 16413
Epoch: [199]  [140/160]  eta: 0:00:19  lr: 0.000000  min_lr: 0.000000  loss: 1.2529 (1.3772)  loss_scale: 8192.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4642 (7.3040)  time: 0.8799 (0.5278 -- 4.1246)  data: 0.0017 (0.0003 -- 0.0063)  max mem: 16413
[2023-09-23 10:03:05,870] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 10:03:05,870] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 10:03:05,870] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-23 10:03:05,870] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-23 10:03:07,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=186, lr=[1.1915026241310033e-08, 1.1915026241310033e-08, 1.3238918045900038e-08, 1.3238918045900038e-08, 1.4709908939888929e-08, 1.4709908939888929e-08, 1.6344343266543253e-08, 1.6344343266543253e-08, 1.8160381407270283e-08, 1.8160381407270283e-08, 2.0178201563633648e-08, 2.0178201563633648e-08, 2.242022395959294e-08, 2.242022395959294e-08, 2.4911359955103263e-08, 2.4911359955103263e-08, 2.767928883900363e-08, 2.767928883900363e-08, 3.0754765376670695e-08, 3.0754765376670695e-08, 3.4171961529634105e-08, 3.4171961529634105e-08, 3.79688461440379e-08, 3.79688461440379e-08, 4.218760682670877e-08, 4.218760682670877e-08, 4.687511869634308e-08, 4.687511869634308e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-23 10:03:07,375] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=17.111030847193195, CurrSamplesPerSec=24.822834484510278, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [199]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3127 (1.3677)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6471 (7.2073)  time: 0.7458 (0.4935 -- 4.6424)  data: 0.0005 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [199] Total time: 0:02:30 (0.9429 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3127 (1.3592)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6471 (7.2073)
[2023-09-23 10:03:07,378] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-09-23 10:03:07,380] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt
[2023-09-23 10:03:07,380] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt...
[2023-09-23 10:03:07,380] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-09-23 10:03:08,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt.
[2023-09-23 10:03:08,400] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1442 (0.1442)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5202 (2.5202 -- 2.5202)  data: 2.3230 (2.3230 -- 2.3230)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.1423 (0.3399)  acc1: 100.0000 (92.9293)  acc5: 100.0000 (100.0000)  time: 0.4718 (0.1777 -- 2.5202)  data: 0.2818 (0.0004 -- 2.3230)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.1471 (0.3142)  acc1: 100.0000 (93.6508)  acc5: 100.0000 (100.0000)  time: 0.2503 (0.1682 -- 0.9689)  data: 0.0633 (0.0001 -- 0.7709)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.1809 (0.3840)  acc1: 88.8889 (91.2863)  acc5: 100.0000 (100.0000)  time: 0.2839 (0.1328 -- 1.0043)  data: 0.1048 (0.0001 -- 0.8339)  max mem: 16413
Val: Total time: 0:00:09 (0.3462 s / it)
* Acc@1 92.324 Acc@5 99.793 loss 0.351
Accuracy of the network on the 482 val images: 92.32%
Max accuracy: 94.19%
Test:  [  0/603]  eta: 0:28:16  loss: 0.1255 (0.1255)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.8137 (2.8137 -- 2.8137)  data: 2.5325 (2.5325 -- 2.5325)  max mem: 16413
Test:  [ 10/603]  eta: 0:03:44  loss: 0.1356 (0.4078)  acc1: 100.0000 (92.4242)  acc5: 100.0000 (100.0000)  time: 0.3780 (0.1227 -- 2.8137)  data: 0.2316 (0.0002 -- 2.5325)  max mem: 16413
Test:  [ 20/603]  eta: 0:02:53  loss: 0.1265 (0.2748)  acc1: 100.0000 (96.0317)  acc5: 100.0000 (100.0000)  time: 0.1722 (0.1217 -- 0.7153)  data: 0.0415 (0.0002 -- 0.5753)  max mem: 16413
Test:  [ 30/603]  eta: 0:02:22  loss: 0.1250 (0.2756)  acc1: 100.0000 (96.2366)  acc5: 100.0000 (100.0000)  time: 0.1788 (0.1187 -- 0.7153)  data: 0.0488 (0.0002 -- 0.5753)  max mem: 16413
Test:  [ 40/603]  eta: 0:02:17  loss: 0.2093 (0.3334)  acc1: 100.0000 (93.9024)  acc5: 100.0000 (100.0000)  time: 0.1862 (0.1187 -- 0.6024)  data: 0.0521 (0.0002 -- 0.4775)  max mem: 16413
Test:  [ 50/603]  eta: 0:02:04  loss: 0.5395 (0.3538)  acc1: 83.3333 (93.4641)  acc5: 100.0000 (100.0000)  time: 0.1856 (0.1203 -- 0.6024)  data: 0.0517 (0.0002 -- 0.4775)  max mem: 16413
Test:  [ 60/603]  eta: 0:02:07  loss: 0.1279 (0.3169)  acc1: 100.0000 (94.5355)  acc5: 100.0000 (100.0000)  time: 0.2153 (0.1203 -- 1.1029)  data: 0.0838 (0.0002 -- 0.9663)  max mem: 16413
Test:  [ 70/603]  eta: 0:01:56  loss: 0.1290 (0.3111)  acc1: 100.0000 (94.8357)  acc5: 100.0000 (100.0000)  time: 0.2052 (0.1189 -- 1.1029)  data: 0.0763 (0.0002 -- 0.9663)  max mem: 16413
Test:  [ 80/603]  eta: 0:01:52  loss: 0.2076 (0.3362)  acc1: 100.0000 (93.8272)  acc5: 100.0000 (100.0000)  time: 0.1542 (0.1189 -- 0.6215)  data: 0.0245 (0.0002 -- 0.4807)  max mem: 16413
Test:  [ 90/603]  eta: 0:01:45  loss: 0.2271 (0.3473)  acc1: 83.3333 (93.5897)  acc5: 100.0000 (100.0000)  time: 0.1610 (0.1186 -- 0.6215)  data: 0.0319 (0.0002 -- 0.4807)  max mem: 16413
Test:  [100/603]  eta: 0:01:46  loss: 0.1300 (0.3257)  acc1: 100.0000 (94.2244)  acc5: 100.0000 (100.0000)  time: 0.2031 (0.1186 -- 0.8237)  data: 0.0767 (0.0002 -- 0.6952)  max mem: 16413
Test:  [110/603]  eta: 0:01:41  loss: 0.1312 (0.3212)  acc1: 100.0000 (94.4444)  acc5: 100.0000 (100.0000)  time: 0.1988 (0.1188 -- 0.8237)  data: 0.0694 (0.0002 -- 0.6952)  max mem: 16413
Test:  [120/603]  eta: 0:01:38  loss: 0.1712 (0.3371)  acc1: 100.0000 (93.8017)  acc5: 100.0000 (100.0000)  time: 0.1607 (0.1222 -- 0.7078)  data: 0.0279 (0.0002 -- 0.5488)  max mem: 16413
Test:  [130/603]  eta: 0:01:34  loss: 0.1423 (0.3396)  acc1: 100.0000 (93.7659)  acc5: 100.0000 (100.0000)  time: 0.1688 (0.1266 -- 0.7078)  data: 0.0325 (0.0002 -- 0.5488)  max mem: 16413
Test:  [140/603]  eta: 0:01:34  loss: 0.1272 (0.3294)  acc1: 100.0000 (94.0898)  acc5: 100.0000 (100.0000)  time: 0.2039 (0.1194 -- 0.8322)  data: 0.0718 (0.0001 -- 0.7104)  max mem: 16413
Test:  [150/603]  eta: 0:01:30  loss: 0.1346 (0.3259)  acc1: 100.0000 (94.2605)  acc5: 100.0000 (100.0000)  time: 0.1966 (0.1194 -- 0.8322)  data: 0.0673 (0.0001 -- 0.7104)  max mem: 16413
Test:  [160/603]  eta: 0:01:28  loss: 0.1991 (0.3376)  acc1: 100.0000 (93.7888)  acc5: 100.0000 (100.0000)  time: 0.1650 (0.1220 -- 0.7568)  data: 0.0309 (0.0001 -- 0.6021)  max mem: 16413
Test:  [170/603]  eta: 0:01:24  loss: 0.2133 (0.3394)  acc1: 83.3333 (93.7622)  acc5: 100.0000 (100.0000)  time: 0.1648 (0.1175 -- 0.7568)  data: 0.0309 (0.0002 -- 0.6021)  max mem: 16413
Test:  [180/603]  eta: 0:01:24  loss: 0.1307 (0.3315)  acc1: 100.0000 (94.0147)  acc5: 100.0000 (100.0000)  time: 0.1999 (0.1175 -- 1.0439)  data: 0.0706 (0.0001 -- 0.9258)  max mem: 16413
Test:  [190/603]  eta: 0:01:20  loss: 0.1350 (0.3287)  acc1: 100.0000 (94.1536)  acc5: 100.0000 (100.0000)  time: 0.2028 (0.1184 -- 1.0439)  data: 0.0707 (0.0001 -- 0.9258)  max mem: 16413
Test:  [200/603]  eta: 0:01:18  loss: 0.1723 (0.3379)  acc1: 100.0000 (93.7811)  acc5: 100.0000 (100.0000)  time: 0.1623 (0.1202 -- 0.6702)  data: 0.0261 (0.0002 -- 0.5094)  max mem: 16413
Test:  [210/603]  eta: 0:01:15  loss: 0.2149 (0.3394)  acc1: 83.3333 (93.7599)  acc5: 100.0000 (100.0000)  time: 0.1619 (0.1202 -- 0.6702)  data: 0.0291 (0.0002 -- 0.5094)  max mem: 16413
Test:  [220/603]  eta: 0:01:15  loss: 0.1282 (0.3328)  acc1: 100.0000 (93.9668)  acc5: 100.0000 (100.0000)  time: 0.2117 (0.1199 -- 1.3130)  data: 0.0822 (0.0002 -- 1.1797)  max mem: 16413
Test:  [230/603]  eta: 0:01:12  loss: 0.1282 (0.3305)  acc1: 100.0000 (94.0837)  acc5: 100.0000 (100.0000)  time: 0.2082 (0.1160 -- 1.3130)  data: 0.0809 (0.0002 -- 1.1797)  max mem: 16413
Test:  [240/603]  eta: 0:01:10  loss: 0.2044 (0.3381)  acc1: 100.0000 (93.7759)  acc5: 100.0000 (100.0000)  time: 0.1663 (0.1160 -- 0.7992)  data: 0.0356 (0.0002 -- 0.6537)  max mem: 16413
Test:  [250/603]  eta: 0:01:07  loss: 0.1409 (0.3392)  acc1: 83.3333 (93.7583)  acc5: 100.0000 (100.0000)  time: 0.1657 (0.1181 -- 0.7992)  data: 0.0337 (0.0002 -- 0.6537)  max mem: 16413
Test:  [260/603]  eta: 0:01:06  loss: 0.1255 (0.3338)  acc1: 100.0000 (93.9336)  acc5: 100.0000 (100.0000)  time: 0.1837 (0.1181 -- 0.7057)  data: 0.0523 (0.0002 -- 0.5784)  max mem: 16413
Test:  [270/603]  eta: 0:01:04  loss: 0.1265 (0.3317)  acc1: 100.0000 (94.0344)  acc5: 100.0000 (100.0000)  time: 0.2042 (0.1216 -- 0.7057)  data: 0.0730 (0.0002 -- 0.5784)  max mem: 16413
Test:  [280/603]  eta: 0:01:02  loss: 0.2093 (0.3382)  acc1: 100.0000 (93.7722)  acc5: 100.0000 (100.0000)  time: 0.1756 (0.1216 -- 0.6016)  data: 0.0452 (0.0002 -- 0.4785)  max mem: 16413
Test:  [290/603]  eta: 0:00:59  loss: 0.1872 (0.3392)  acc1: 83.3333 (93.7572)  acc5: 100.0000 (100.0000)  time: 0.1670 (0.1224 -- 0.6016)  data: 0.0361 (0.0002 -- 0.4785)  max mem: 16413
Test:  [300/603]  eta: 0:00:57  loss: 0.1279 (0.3345)  acc1: 100.0000 (93.9092)  acc5: 100.0000 (100.0000)  time: 0.1793 (0.1208 -- 0.8800)  data: 0.0501 (0.0002 -- 0.7595)  max mem: 16413
Test:  [310/603]  eta: 0:00:56  loss: 0.1305 (0.3327)  acc1: 100.0000 (93.9979)  acc5: 100.0000 (100.0000)  time: 0.2194 (0.1153 -- 1.1865)  data: 0.0909 (0.0001 -- 1.0497)  max mem: 16413
Test:  [320/603]  eta: 0:00:54  loss: 0.2076 (0.3383)  acc1: 100.0000 (93.7695)  acc5: 100.0000 (100.0000)  time: 0.2079 (0.1153 -- 1.1865)  data: 0.0773 (0.0001 -- 1.0497)  max mem: 16413
Test:  [330/603]  eta: 0:00:52  loss: 0.2076 (0.3391)  acc1: 100.0000 (93.7563)  acc5: 100.0000 (100.0000)  time: 0.1575 (0.1188 -- 0.6355)  data: 0.0252 (0.0002 -- 0.4831)  max mem: 16413
Test:  [340/603]  eta: 0:00:50  loss: 0.1282 (0.3350)  acc1: 100.0000 (93.8905)  acc5: 100.0000 (100.0000)  time: 0.1754 (0.1208 -- 0.9662)  data: 0.0426 (0.0001 -- 0.8168)  max mem: 16413
Test:  [350/603]  eta: 0:00:48  loss: 0.1346 (0.3334)  acc1: 100.0000 (93.9696)  acc5: 100.0000 (100.0000)  time: 0.2102 (0.1208 -- 0.9662)  data: 0.0774 (0.0001 -- 0.8168)  max mem: 16413
Test:  [360/603]  eta: 0:00:46  loss: 0.1712 (0.3384)  acc1: 100.0000 (93.7673)  acc5: 100.0000 (100.0000)  time: 0.1841 (0.1178 -- 0.8251)  data: 0.0547 (0.0002 -- 0.7010)  max mem: 16413
Test:  [370/603]  eta: 0:00:44  loss: 0.1423 (0.3391)  acc1: 100.0000 (93.7556)  acc5: 100.0000 (100.0000)  time: 0.1570 (0.1178 -- 0.5198)  data: 0.0269 (0.0002 -- 0.3834)  max mem: 16413
Test:  [380/603]  eta: 0:00:42  loss: 0.1265 (0.3354)  acc1: 100.0000 (93.8758)  acc5: 100.0000 (100.0000)  time: 0.1836 (0.1186 -- 1.0256)  data: 0.0526 (0.0001 -- 0.8975)  max mem: 16413
Test:  [390/603]  eta: 0:00:40  loss: 0.1346 (0.3322)  acc1: 100.0000 (93.9898)  acc5: 100.0000 (100.0000)  time: 0.2052 (0.1222 -- 1.0256)  data: 0.0764 (0.0001 -- 0.8975)  max mem: 16413
Test:  [400/603]  eta: 0:00:38  loss: 0.1991 (0.3384)  acc1: 100.0000 (93.7656)  acc5: 100.0000 (100.0000)  time: 0.1861 (0.1227 -- 0.7400)  data: 0.0556 (0.0002 -- 0.6220)  max mem: 16413
Test:  [410/603]  eta: 0:00:36  loss: 0.5332 (0.3390)  acc1: 83.3333 (93.7551)  acc5: 100.0000 (100.0000)  time: 0.1548 (0.1188 -- 0.6065)  data: 0.0244 (0.0002 -- 0.4810)  max mem: 16413
Test:  [420/603]  eta: 0:00:34  loss: 0.1305 (0.3357)  acc1: 100.0000 (93.8638)  acc5: 100.0000 (100.0000)  time: 0.1781 (0.1188 -- 1.1304)  data: 0.0507 (0.0002 -- 1.0058)  max mem: 16413
Test:  [430/603]  eta: 0:00:32  loss: 0.1350 (0.3328)  acc1: 100.0000 (93.9675)  acc5: 100.0000 (100.0000)  time: 0.2138 (0.1229 -- 1.1304)  data: 0.0857 (0.0002 -- 1.0058)  max mem: 16413
Test:  [440/603]  eta: 0:00:30  loss: 0.1723 (0.3384)  acc1: 100.0000 (93.7642)  acc5: 100.0000 (100.0000)  time: 0.1935 (0.1158 -- 0.8248)  data: 0.0630 (0.0002 -- 0.6985)  max mem: 16413
Test:  [450/603]  eta: 0:00:28  loss: 0.3584 (0.3390)  acc1: 83.3333 (93.7546)  acc5: 100.0000 (100.0000)  time: 0.1562 (0.1158 -- 0.7048)  data: 0.0280 (0.0002 -- 0.5509)  max mem: 16413
Test:  [460/603]  eta: 0:00:27  loss: 0.1323 (0.3360)  acc1: 100.0000 (93.8539)  acc5: 100.0000 (100.0000)  time: 0.1775 (0.1188 -- 1.1154)  data: 0.0500 (0.0002 -- 0.9919)  max mem: 16413
Test:  [470/603]  eta: 0:00:25  loss: 0.1282 (0.3333)  acc1: 100.0000 (93.9490)  acc5: 100.0000 (100.0000)  time: 0.2039 (0.1188 -- 1.1154)  data: 0.0732 (0.0002 -- 0.9919)  max mem: 16413
Test:  [480/603]  eta: 0:00:23  loss: 0.1654 (0.3373)  acc1: 100.0000 (93.7976)  acc5: 100.0000 (100.0000)  time: 0.1917 (0.1218 -- 0.8357)  data: 0.0593 (0.0002 -- 0.6988)  max mem: 16413
Test:  [490/603]  eta: 0:00:21  loss: 0.3556 (0.3390)  acc1: 83.3333 (93.7542)  acc5: 100.0000 (100.0000)  time: 0.1674 (0.1204 -- 0.8357)  data: 0.0361 (0.0002 -- 0.6988)  max mem: 16413
Test:  [500/603]  eta: 0:00:19  loss: 0.1265 (0.3363)  acc1: 100.0000 (93.8456)  acc5: 100.0000 (100.0000)  time: 0.1679 (0.1204 -- 0.8653)  data: 0.0373 (0.0002 -- 0.7300)  max mem: 16413
Test:  [510/603]  eta: 0:00:17  loss: 0.1265 (0.3337)  acc1: 100.0000 (93.9335)  acc5: 100.0000 (100.0000)  time: 0.2120 (0.1203 -- 0.8653)  data: 0.0809 (0.0001 -- 0.7300)  max mem: 16413
Test:  [520/603]  eta: 0:00:15  loss: 0.1872 (0.3374)  acc1: 100.0000 (93.7940)  acc5: 100.0000 (100.0000)  time: 0.1966 (0.1186 -- 0.5877)  data: 0.0661 (0.0001 -- 0.4603)  max mem: 16413
Test:  [530/603]  eta: 0:00:13  loss: 0.5395 (0.3390)  acc1: 83.3333 (93.7539)  acc5: 100.0000 (100.0000)  time: 0.1609 (0.1186 -- 0.5715)  data: 0.0301 (0.0001 -- 0.4376)  max mem: 16413
Test:  [540/603]  eta: 0:00:11  loss: 0.1290 (0.3365)  acc1: 100.0000 (93.8386)  acc5: 100.0000 (100.0000)  time: 0.1738 (0.1180 -- 0.8492)  data: 0.0440 (0.0001 -- 0.7157)  max mem: 16413
Test:  [550/603]  eta: 0:00:10  loss: 0.1305 (0.3341)  acc1: 100.0000 (93.9201)  acc5: 100.0000 (100.0000)  time: 0.2153 (0.1180 -- 1.1602)  data: 0.0877 (0.0002 -- 1.0300)  max mem: 16413
Test:  [560/603]  eta: 0:00:08  loss: 0.2007 (0.3375)  acc1: 100.0000 (93.7909)  acc5: 100.0000 (100.0000)  time: 0.2051 (0.1185 -- 1.1602)  data: 0.0747 (0.0001 -- 1.0300)  max mem: 16413
Test:  [570/603]  eta: 0:00:06  loss: 0.2271 (0.3390)  acc1: 83.3333 (93.7536)  acc5: 100.0000 (100.0000)  time: 0.1587 (0.1215 -- 0.5826)  data: 0.0256 (0.0001 -- 0.4561)  max mem: 16413
Test:  [580/603]  eta: 0:00:04  loss: 0.1300 (0.3367)  acc1: 100.0000 (93.8325)  acc5: 100.0000 (100.0000)  time: 0.1712 (0.1177 -- 0.8981)  data: 0.0416 (0.0001 -- 0.7766)  max mem: 16413
Test:  [590/603]  eta: 0:00:02  loss: 0.1346 (0.3345)  acc1: 100.0000 (93.9086)  acc5: 100.0000 (100.0000)  time: 0.2017 (0.1177 -- 0.8981)  data: 0.0719 (0.0001 -- 0.7766)  max mem: 16413
Test:  [600/603]  eta: 0:00:00  loss: 0.1705 (0.3376)  acc1: 100.0000 (93.7881)  acc5: 100.0000 (100.0000)  time: 0.1696 (0.1129 -- 0.7717)  data: 0.0443 (0.0001 -- 0.6524)  max mem: 16413
Test:  [602/603]  eta: 0:00:00  loss: 0.1712 (0.3379)  acc1: 100.0000 (93.7759)  acc5: 100.0000 (100.0000)  time: 0.1341 (0.0994 -- 0.3536)  data: 0.0116 (0.0001 -- 0.2285)  max mem: 16413
Test: Total time: 0:01:53 (0.1877 s / it)
* Acc@1 94.191 Acc@5 100.000 loss 0.312
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 7230 test videos: Top-1: 94.19%, Top-5: 100.00%
Training time 8:57:44
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
