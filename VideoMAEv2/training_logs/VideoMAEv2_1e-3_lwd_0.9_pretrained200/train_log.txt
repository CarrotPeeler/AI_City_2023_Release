[2023-09-04 15:05:46,412] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-04 15:05:46,446] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/VideoMAEv2_1e-3_lwd_0.9/checkpoint-199/videomaev2_vit-b_00200.pt', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.9, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f53337d30a0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/VideoMAEv2_1e-3_lwd_0.9/checkpoint-199/videomaev2_vit-b_00200.pt
Load state_dict by model_key = module
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.81
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.81
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.9
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.9
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-09-04 15:05:52,578] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-04 15:05:52,578] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-04 15:05:52,579] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-04 15:05:52,579] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-04 15:05:52,685] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.7040691375732422 seconds
[2023-09-04 15:05:54,137] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-04 15:05:54,146] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-09-04 15:05:54,146] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-09-04 15:05:54,173] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-04 15:05:54,173] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-04 15:05:54,173] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-04 15:05:54,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 15:05:54,174] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f52e2c58250>
[2023-09-04 15:05:54,175] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-09-04 15:05:54,176] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-09-04 15:05:54,177] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-04 15:05:54,178] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-04 15:05:54,178] [INFO] [config.py:964:print]   world_size ................... 2
[2023-09-04 15:05:54,178] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-09-04 15:05:54,178] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-04 15:05:54,178] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-04 15:05:54,178] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-04 15:05:54,178] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-04 15:05:54,178] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:32:24  lr: 0.000000  min_lr: 0.000000  loss: 1.9901 (1.9901)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 12.1507 (12.1507 -- 12.1507)  data: 6.2803 (6.2803 -- 6.2803)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:42  lr: 0.000001  min_lr: 0.000000  loss: 1.6186 (1.6579)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0845 (8.3794)  time: 0.6133 (0.5070 -- 1.8418)  data: 0.0101 (0.0005 -- 0.1507)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:09  lr: 0.000002  min_lr: 0.000001  loss: 1.7087 (1.6922)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0051 (8.0070)  time: 0.9842 (0.5068 -- 3.7144)  data: 0.0026 (0.0005 -- 0.0138)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000001  loss: 1.6582 (1.6737)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2395 (7.8725)  time: 0.8312 (0.5123 -- 3.8008)  data: 0.0133 (0.0006 -- 0.2330)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:19  lr: 0.000005  min_lr: 0.000001  loss: 1.6910 (1.6826)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6002 (7.8335)  time: 0.9839 (0.5201 -- 4.8051)  data: 0.0591 (0.0004 -- 0.7151)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000001  loss: 1.7182 (1.6760)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0648 (7.7397)  time: 0.7319 (0.5209 -- 1.7515)  data: 0.1041 (0.0003 -- 0.7791)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000002  loss: 1.6754 (1.6740)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4146 (7.6097)  time: 0.8627 (0.5220 -- 2.5386)  data: 0.0787 (0.0006 -- 0.7743)  max mem: 16413
[2023-09-04 15:07:52,180] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:07:52,181] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-09-04 15:07:52,181] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:07:52,182] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.4661 (1.6534)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9542 (7.5193)  time: 0.8350 (0.5212 -- 1.7398)  data: 0.2524 (0.0004 -- 1.2111)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.6456 (1.6539)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5083 (7.5403)  time: 0.6923 (0.4902 -- 3.1335)  data: 0.1376 (0.0002 -- 2.6307)  max mem: 16413
Epoch: [0] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.6456 (1.6229)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5083 (7.5403)
Val:  [ 0/27]  eta: 0:01:24  loss: 0.1259 (0.1259)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 3.1111 (3.1111 -- 3.1111)  data: 2.6575 (2.6575 -- 2.6575)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4174 (0.4697)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4838 (0.1986 -- 3.1111)  data: 0.2435 (0.0006 -- 2.6575)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3573 (0.4582)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2098 (0.1701 -- 0.2423)  data: 0.0027 (0.0001 -- 0.0300)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4471 (0.5178)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.1939 (0.1664 -- 0.2423)  data: 0.0022 (0.0001 -- 0.0300)  max mem: 16413
Val: Total time: 0:00:08 (0.3082 s / it)
* Acc@1 86.307 Acc@5 98.340 loss 0.526
Accuracy of the network on the 482 val images: 86.31%
[2023-09-04 15:08:24,995] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-09-04 15:08:24,999] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 15:08:24,999] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 15:08:24,999] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 15:08:26,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 15:08:26,022] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.31%
Epoch: [1]  [  0/160]  eta: 0:20:18  lr: 0.000009  min_lr: 0.000002  loss: 2.2592 (2.2592)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5128 (6.5128)  time: 7.6161 (7.6161 -- 7.6161)  data: 7.1007 (7.1007 -- 7.1007)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:39  lr: 0.000011  min_lr: 0.000003  loss: 1.5284 (1.6357)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2223 (6.4015)  time: 0.8184 (0.5119 -- 2.9423)  data: 0.2675 (0.0002 -- 2.4039)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:03  lr: 0.000012  min_lr: 0.000003  loss: 1.7812 (1.6480)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8875 (7.2037)  time: 0.9027 (0.5170 -- 3.6711)  data: 0.1699 (0.0002 -- 1.8254)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:35  lr: 0.000013  min_lr: 0.000003  loss: 1.6126 (1.6335)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8976 (7.3928)  time: 0.8226 (0.5242 -- 3.0610)  data: 0.2229 (0.0006 -- 2.5334)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:15  lr: 0.000014  min_lr: 0.000004  loss: 1.5673 (1.6314)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4256 (7.2375)  time: 0.8741 (0.5428 -- 2.0665)  data: 0.1685 (0.0004 -- 1.3990)  max mem: 16413
[2023-09-04 15:09:55,871] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:09:55,871] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:09:55,871] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-09-04 15:09:55,871] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000004  loss: 1.5674 (1.6161)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0107 (7.2055)  time: 0.9369 (0.5278 -- 3.2319)  data: 0.2792 (0.0005 -- 2.6885)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000004  loss: 1.7076 (1.6240)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9040 (7.2281)  time: 0.7946 (0.5302 -- 3.0235)  data: 0.2061 (0.0003 -- 2.4962)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000004  loss: 1.3335 (1.5996)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2640 (7.4192)  time: 0.8521 (0.5284 -- 2.7591)  data: 0.2819 (0.0004 -- 2.2291)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.7000 (1.6124)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6202 (7.4897)  time: 0.6926 (0.4964 -- 2.7728)  data: 0.1671 (0.0002 -- 2.2593)  max mem: 16413
Epoch: [1] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.7000 (1.6040)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6202 (7.4897)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1439 (0.1439)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3432 (2.3432 -- 2.3432)  data: 2.0900 (2.0900 -- 2.0900)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5348 (0.5044)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4119 (0.2042 -- 2.3432)  data: 0.1913 (0.0008 -- 2.0900)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4641 (0.4906)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2199 (0.1697 -- 0.5168)  data: 0.0171 (0.0001 -- 0.3237)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4728 (0.5574)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2055 (0.1326 -- 0.5168)  data: 0.0166 (0.0001 -- 0.3237)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 86.100 Acc@5 97.718 loss 0.553
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.31%
Epoch: [2]  [  0/160]  eta: 0:22:07  lr: 0.000019  min_lr: 0.000005  loss: 2.2206 (2.2206)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1486 (7.1486)  time: 8.2948 (8.2948 -- 8.2948)  data: 7.7517 (7.7517 -- 7.7517)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:41  lr: 0.000020  min_lr: 0.000005  loss: 1.5766 (1.6534)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0342 (7.3387)  time: 0.7983 (0.5276 -- 4.6498)  data: 0.2486 (0.0005 -- 4.1210)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:08  lr: 0.000021  min_lr: 0.000005  loss: 1.5783 (1.5895)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8265 (7.2681)  time: 0.9890 (0.5163 -- 4.8391)  data: 0.4426 (0.0004 -- 4.3116)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000006  loss: 1.5343 (1.5781)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7197 (7.2799)  time: 0.7566 (0.5277 -- 3.3526)  data: 0.2064 (0.0004 -- 2.7976)  max mem: 16413
[2023-09-04 15:11:57,843] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:11:57,843] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:11:57,843] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-09-04 15:11:57,843] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000006  loss: 1.6122 (1.5824)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6014 (7.3160)  time: 0.8805 (0.5178 -- 3.3757)  data: 0.3469 (0.0005 -- 2.8546)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000006  loss: 1.5404 (1.5862)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4542 (7.4516)  time: 0.8589 (0.5351 -- 2.6437)  data: 0.2988 (0.0004 -- 2.0760)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000007  loss: 1.7224 (1.6154)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7965 (7.5683)  time: 0.9087 (0.5289 -- 3.3067)  data: 0.3594 (0.0004 -- 2.7699)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.4253 (1.5973)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9247 (7.5288)  time: 0.8029 (0.5295 -- 3.0885)  data: 0.2510 (0.0003 -- 2.5474)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.5156 (1.5849)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3591 (7.3829)  time: 0.6853 (0.4936 -- 2.1776)  data: 0.1614 (0.0002 -- 1.6584)  max mem: 16413
Epoch: [2] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.5156 (1.6072)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3591 (7.3829)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1599 (0.1599)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4372 (2.4372 -- 2.4372)  data: 2.2298 (2.2298 -- 2.2298)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3777 (0.5028)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4170 (0.1942 -- 2.4372)  data: 0.2043 (0.0004 -- 2.2298)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4532 (0.5037)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.8836)  time: 0.2200 (0.1692 -- 0.4617)  data: 0.0143 (0.0001 -- 0.2663)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5675 (0.5627)  acc1: 85.7143 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.2053 (0.1327 -- 0.4617)  data: 0.0138 (0.0001 -- 0.2663)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 86.100 Acc@5 97.718 loss 0.539
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.31%
Epoch: [3]  [  0/160]  eta: 0:17:26  lr: 0.000028  min_lr: 0.000007  loss: 1.5265 (1.5265)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7115 (5.7115)  time: 6.5382 (6.5382 -- 6.5382)  data: 5.9690 (5.9690 -- 5.9690)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:38  lr: 0.000029  min_lr: 0.000007  loss: 1.6328 (1.6659)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4807 (7.5984)  time: 0.8595 (0.5303 -- 2.8543)  data: 0.2893 (0.0006 -- 2.3214)  max mem: 16413
[2023-09-04 15:14:00,439] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:14:00,439] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-09-04 15:14:00,440] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:14:00,440] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:03  lr: 0.000031  min_lr: 0.000008  loss: 1.7037 (1.6574)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7348 (6.9068)  time: 0.9168 (0.5041 -- 2.8043)  data: 0.3709 (0.0004 -- 2.2802)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:38  lr: 0.000032  min_lr: 0.000008  loss: 1.6498 (1.6143)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4061 (6.9415)  time: 0.9156 (0.5078 -- 3.6766)  data: 0.3733 (0.0002 -- 3.1696)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:15  lr: 0.000033  min_lr: 0.000008  loss: 1.6943 (1.6303)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9909 (7.0336)  time: 0.8093 (0.5261 -- 2.6328)  data: 0.2547 (0.0005 -- 2.0930)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:56  lr: 0.000034  min_lr: 0.000009  loss: 1.7209 (1.6476)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5271 (7.0331)  time: 0.9006 (0.5206 -- 2.9801)  data: 0.2943 (0.0001 -- 2.4455)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000009  loss: 1.7780 (1.6601)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9557 (6.9045)  time: 0.8152 (0.5253 -- 3.0936)  data: 0.2613 (0.0002 -- 2.5572)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.8515 (1.6731)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5468 (6.8449)  time: 1.0225 (0.5294 -- 3.6109)  data: 0.4695 (0.0004 -- 3.0805)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 1.6505 (1.6714)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8370 (6.9210)  time: 0.7982 (0.4931 -- 3.4133)  data: 0.2817 (0.0002 -- 2.9001)  max mem: 16413
Epoch: [3] Total time: 0:02:24 (0.9028 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 1.6505 (1.6442)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8370 (6.9210)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1479 (0.1479)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3598 (2.3598 -- 2.3598)  data: 2.1533 (2.1533 -- 2.1533)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3354 (0.5211)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4211 (0.2044 -- 2.3598)  data: 0.2073 (0.0003 -- 2.1533)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4947 (0.5050)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2195 (0.1688 -- 0.3638)  data: 0.0141 (0.0001 -- 0.1516)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5180 (0.5836)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2043 (0.1325 -- 0.3638)  data: 0.0138 (0.0001 -- 0.1516)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 85.270 Acc@5 97.718 loss 0.579
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.31%
[2023-09-04 15:16:01,595] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:16:01,595] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-09-04 15:16:01,597] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:16:01,598] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:14:27  lr: 0.000038  min_lr: 0.000010  loss: 0.7248 (0.7248)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4223 (4.4223)  time: 5.4197 (5.4197 -- 5.4197)  data: 4.4021 (4.4021 -- 4.4021)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:43  lr: 0.000039  min_lr: 0.000010  loss: 1.4217 (1.3828)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2277 (6.9450)  time: 0.9519 (0.5232 -- 3.1744)  data: 0.2986 (0.0005 -- 2.6337)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:01:58  lr: 0.000040  min_lr: 0.000010  loss: 1.6703 (1.5019)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8454 (6.9604)  time: 0.8056 (0.5182 -- 3.4613)  data: 0.2454 (0.0003 -- 2.9288)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:39  lr: 0.000041  min_lr: 0.000010  loss: 1.4095 (1.4915)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5324 (6.9932)  time: 1.0156 (0.5275 -- 4.1503)  data: 0.4648 (0.0006 -- 3.6236)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000011  loss: 1.4280 (1.4917)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0017 (6.8867)  time: 0.7611 (0.5361 -- 3.3635)  data: 0.2038 (0.0006 -- 2.8255)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.6854 (1.5226)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0722 (6.7881)  time: 0.9153 (0.5353 -- 4.0484)  data: 0.3587 (0.0008 -- 3.5419)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.6432 (1.5391)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5117 (6.8753)  time: 0.7781 (0.5274 -- 3.4821)  data: 0.2150 (0.0003 -- 2.9520)  max mem: 16413
[2023-09-04 15:17:54,556] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:17:54,556] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:17:54,556] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-09-04 15:17:54,556] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.6959 (1.5552)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9783 (6.8473)  time: 1.0135 (0.5223 -- 4.4813)  data: 0.4622 (0.0003 -- 3.9338)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.7039 (1.5732)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5525 (6.8238)  time: 0.6181 (0.4953 -- 2.0294)  data: 0.0982 (0.0002 -- 1.5003)  max mem: 16413
Epoch: [4] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.7039 (1.5957)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5525 (6.8238)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1429 (0.1429)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3282 (2.3282 -- 2.3282)  data: 2.1132 (2.1132 -- 2.1132)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4209 (0.5229)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4148 (0.2003 -- 2.3282)  data: 0.1952 (0.0005 -- 2.1132)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4352 (0.5104)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2201 (0.1699 -- 0.4253)  data: 0.0139 (0.0001 -- 0.2404)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5324 (0.5817)  acc1: 77.7778 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2008 (0.1328 -- 0.4253)  data: 0.0128 (0.0001 -- 0.2404)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 85.685 Acc@5 98.133 loss 0.570
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.31%
Epoch: [5]  [  0/160]  eta: 0:17:12  lr: 0.000047  min_lr: 0.000012  loss: 1.1809 (1.1809)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3682 (7.3682)  time: 6.4540 (6.4540 -- 6.4540)  data: 5.7361 (5.7361 -- 5.7361)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000012  loss: 1.4463 (1.5148)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5851 (7.6348)  time: 0.8631 (0.5228 -- 3.1725)  data: 0.1841 (0.0005 -- 2.1589)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:02:00  lr: 0.000047  min_lr: 0.000012  loss: 1.4297 (1.5059)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1762 (7.8169)  time: 0.8658 (0.5218 -- 3.4909)  data: 0.0401 (0.0008 -- 0.7095)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000012  loss: 1.5682 (1.5230)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4877 (7.6762)  time: 0.9467 (0.5240 -- 2.8483)  data: 0.1018 (0.0004 -- 1.1186)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000012  loss: 1.5595 (1.5442)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8782 (7.5369)  time: 0.8195 (0.5331 -- 1.9564)  data: 0.1285 (0.0002 -- 1.4292)  max mem: 16413
[2023-09-04 15:19:55,962] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:19:55,962] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-09-04 15:19:55,964] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:19:55,965] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 1.6077 (1.5609)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8887 (7.3374)  time: 0.8831 (0.5358 -- 2.4136)  data: 0.2543 (0.0003 -- 1.8525)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.7542 (1.5817)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5052 (7.2499)  time: 0.8507 (0.5286 -- 2.7041)  data: 0.0823 (0.0003 -- 1.6126)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.6555 (1.6006)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2247 (7.4018)  time: 0.8066 (0.5248 -- 2.9862)  data: 0.0807 (0.0005 -- 1.1180)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.3553 (1.5850)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9759 (7.3225)  time: 0.7740 (0.4977 -- 2.2892)  data: 0.1024 (0.0002 -- 1.1180)  max mem: 16413
Epoch: [5] Total time: 0:02:20 (0.8812 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.3553 (1.5870)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9759 (7.3225)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1599 (0.1599)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2725 (2.2725 -- 2.2725)  data: 2.0602 (2.0602 -- 2.0602)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3514 (0.5771)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4164 (0.2043 -- 2.2725)  data: 0.1882 (0.0007 -- 2.0602)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5175 (0.5774)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2193 (0.1694 -- 0.2682)  data: 0.0051 (0.0001 -- 0.0869)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5890 (0.6330)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.1999 (0.1328 -- 0.2682)  data: 0.0048 (0.0001 -- 0.0869)  max mem: 16413
Val: Total time: 0:00:07 (0.2834 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.600
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [6]  [  0/160]  eta: 0:16:47  lr: 0.000047  min_lr: 0.000012  loss: 2.0015 (2.0015)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1157 (12.1157)  time: 6.2963 (6.2963 -- 6.2963)  data: 5.7671 (5.7671 -- 5.7671)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:46  lr: 0.000047  min_lr: 0.000012  loss: 1.7157 (1.7609)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4665 (7.2522)  time: 0.9348 (0.5297 -- 3.4302)  data: 0.3124 (0.0007 -- 2.8857)  max mem: 16413
[2023-09-04 15:21:36,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.1913801307802163e-05, 1.1913801307802163e-05, 1.3237557008669071e-05, 1.3237557008669071e-05, 1.4708396676298964e-05, 1.4708396676298964e-05, 1.634266297366552e-05, 1.634266297366552e-05, 1.8158514415183906e-05, 1.8158514415183906e-05, 2.017612712798212e-05, 2.017612712798212e-05, 2.2417919031091242e-05, 2.2417919031091242e-05, 2.490879892343471e-05, 2.490879892343471e-05, 2.7676443248260792e-05, 2.7676443248260792e-05, 3.0751603609178656e-05, 3.0751603609178656e-05, 3.416844845464296e-05, 3.416844845464296e-05, 3.7964942727381054e-05, 3.7964942727381054e-05, 4.218326969709006e-05, 4.218326969709006e-05, 4.68702996634334e-05, 4.68702996634334e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 15:21:36,563] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=19.635471250421524, CurrSamplesPerSec=21.29625235465456, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:02:04  lr: 0.000047  min_lr: 0.000012  loss: 1.5641 (1.6569)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4181 (7.3307)  time: 0.8764 (0.5221 -- 4.5303)  data: 0.3067 (0.0004 -- 4.0268)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000012  loss: 1.6488 (1.6453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2498 (7.3583)  time: 1.0057 (0.5163 -- 4.0760)  data: 0.4602 (0.0003 -- 3.5469)  max mem: 16413
[2023-09-04 15:21:59,485] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:21:59,486] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-09-04 15:21:59,505] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:21:59,506] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 1.5467 (1.6231)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6709 (7.0547)  time: 0.7492 (0.5256 -- 3.6782)  data: 0.1946 (0.0006 -- 3.1454)  max mem: 16413
[2023-09-04 15:22:22,178] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1050
[2023-09-04 15:22:22,178] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1050
[2023-09-04 15:22:22,179] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2023-09-04 15:22:22,179] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2023-09-04 15:22:22,179] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768, reducing to 16384.0
Epoch: [6]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.6236 (1.6336)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0148 (7.1863)  time: 0.8992 (0.5194 -- 3.5800)  data: 0.3474 (0.0006 -- 3.0579)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.6121 (1.6299)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6280 (7.1496)  time: 0.8057 (0.5251 -- 3.2789)  data: 0.2551 (0.0003 -- 2.7286)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.5439 (1.6255)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7196 (7.3032)  time: 0.9040 (0.5406 -- 3.4560)  data: 0.3438 (0.0006 -- 2.9007)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.7015 (1.6315)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8542 (7.2659)  time: 0.7220 (0.4981 -- 4.0378)  data: 0.2047 (0.0002 -- 3.5156)  max mem: 16413
Epoch: [6] Total time: 0:02:23 (0.8983 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.7015 (1.6143)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8542 (7.2659)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1687 (0.1687)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3119 (2.3119 -- 2.3119)  data: 2.0789 (2.0789 -- 2.0789)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3545 (0.5507)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4114 (0.2041 -- 2.3119)  data: 0.1905 (0.0005 -- 2.0789)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4834 (0.5446)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2193 (0.1700 -- 0.3679)  data: 0.0118 (0.0001 -- 0.1855)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5427 (0.5890)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.2021 (0.1325 -- 0.3679)  data: 0.0115 (0.0001 -- 0.1855)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 84.232 Acc@5 97.718 loss 0.601
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.31%
Epoch: [7]  [  0/160]  eta: 0:20:39  lr: 0.000047  min_lr: 0.000012  loss: 1.0548 (1.0548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3981 (5.3981)  time: 7.7461 (7.7461 -- 7.7461)  data: 7.2227 (7.2227 -- 7.2227)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:55  lr: 0.000047  min_lr: 0.000012  loss: 1.7020 (1.6084)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6858 (6.9255)  time: 0.9291 (0.5286 -- 3.3069)  data: 0.2014 (0.0002 -- 2.7633)  max mem: 16413
Epoch: [7]  [ 40/160]  eta: 0:02:00  lr: 0.000047  min_lr: 0.000012  loss: 1.6598 (1.5882)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2884 (7.0562)  time: 0.7379 (0.5262 -- 2.4510)  data: 0.0016 (0.0005 -- 0.0030)  max mem: 16413
[2023-09-04 15:24:23,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:24:23,482] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:24:23,484] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:24:23,485] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [7]  [ 60/160]  eta: 0:01:35  lr: 0.000047  min_lr: 0.000012  loss: 1.8988 (1.6588)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8643 (6.9272)  time: 0.8458 (0.5314 -- 3.5366)  data: 0.0020 (0.0003 -- 0.0065)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000012  loss: 1.7112 (1.6790)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5499 (6.9457)  time: 0.8785 (0.5166 -- 2.4374)  data: 0.0058 (0.0005 -- 0.0666)  max mem: 16413
Epoch: [7]  [100/160]  eta: 0:00:54  lr: 0.000047  min_lr: 0.000012  loss: 1.5384 (1.6608)  loss_scale: 32768.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6623 (6.9665)  time: 0.8035 (0.5209 -- 2.5560)  data: 0.0187 (0.0004 -- 0.3433)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.6693 (1.6511)  loss_scale: 32768.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3890 (6.9632)  time: 0.9185 (0.5351 -- 2.7813)  data: 0.0018 (0.0006 -- 0.0040)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.7354 (1.6512)  loss_scale: 32768.0000 (25912.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6228 (7.0449)  time: 0.8639 (0.5317 -- 3.0386)  data: 0.2287 (0.0004 -- 2.5164)  max mem: 16413
[2023-09-04 15:25:45,414] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1277
[2023-09-04 15:25:45,414] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1277
[2023-09-04 15:25:45,414] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:25:45,414] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:25:45,415] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.7517 (1.6599)  loss_scale: 32768.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9214 (7.0597)  time: 0.6794 (0.4907 -- 1.4812)  data: 0.1466 (0.0002 -- 0.9098)  max mem: 16413
Epoch: [7] Total time: 0:02:20 (0.8772 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.7517 (1.6548)  loss_scale: 32768.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9214 (7.0597)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2363 (0.2363)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3896 (2.3896 -- 2.3896)  data: 2.1422 (2.1422 -- 2.1422)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2962 (0.5687)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (97.9798)  time: 0.4293 (0.2153 -- 2.3896)  data: 0.1976 (0.0006 -- 2.1422)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4967 (0.5592)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (97.3545)  time: 0.2159 (0.1690 -- 0.2539)  data: 0.0040 (0.0001 -- 0.0444)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5999 (0.6100)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.5104)  time: 0.1947 (0.1332 -- 0.2539)  data: 0.0036 (0.0001 -- 0.0444)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 85.062 Acc@5 97.718 loss 0.605
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [8]  [  0/160]  eta: 0:19:40  lr: 0.000047  min_lr: 0.000012  loss: 1.6307 (1.6307)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1839 (8.1839)  time: 7.3789 (7.3789 -- 7.3789)  data: 5.1386 (5.1386 -- 5.1386)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:42  lr: 0.000047  min_lr: 0.000012  loss: 1.4241 (1.5769)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9105 (6.3856)  time: 0.8485 (0.5280 -- 2.3390)  data: 0.0019 (0.0005 -- 0.0054)  max mem: 16413
Epoch: [8]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000012  loss: 1.4847 (1.5288)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6941 (6.1891)  time: 0.9223 (0.5229 -- 4.5429)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [8]  [ 60/160]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000012  loss: 1.5781 (1.5644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4918 (6.6565)  time: 0.9438 (0.5221 -- 3.3702)  data: 0.0014 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [8]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 1.8117 (1.6091)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4053 (6.8625)  time: 0.8065 (0.5258 -- 3.7574)  data: 0.0017 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.7325 (1.6528)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0170 (6.9505)  time: 0.8508 (0.5246 -- 3.1141)  data: 0.0022 (0.0003 -- 0.0132)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.6517 (1.6532)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7757 (7.1047)  time: 0.7762 (0.5326 -- 2.0151)  data: 0.0015 (0.0003 -- 0.0036)  max mem: 16413
[2023-09-04 15:27:51,115] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:27:51,115] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:27:51,117] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:27:51,117] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:27:55,840] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1412
[2023-09-04 15:27:55,840] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1412
[2023-09-04 15:27:55,840] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:27:55,841] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 15:27:55,841] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [8]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.6477 (1.6652)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4719 (7.1709)  time: 0.8861 (0.5246 -- 2.6006)  data: 0.0014 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.7364 (1.6724)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4245 (7.0900)  time: 0.6871 (0.4944 -- 2.7896)  data: 0.0008 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [8] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.7364 (1.6476)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4245 (7.0900)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1516 (0.1516)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3841 (2.3841 -- 2.3841)  data: 2.1252 (2.1252 -- 2.1252)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3042 (0.5592)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4129 (0.2013 -- 2.3841)  data: 0.1978 (0.0007 -- 2.1252)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4593 (0.5447)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2161 (0.1691 -- 0.3729)  data: 0.0134 (0.0001 -- 0.1729)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5587 (0.6166)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (98.3402)  time: 0.2016 (0.1341 -- 0.3729)  data: 0.0128 (0.0001 -- 0.1729)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 85.062 Acc@5 98.548 loss 0.578
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [9]  [  0/160]  eta: 0:16:52  lr: 0.000047  min_lr: 0.000012  loss: 1.3565 (1.3565)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7592 (5.7592)  time: 6.3311 (6.3311 -- 6.3311)  data: 5.7694 (5.7694 -- 5.7694)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:03:06  lr: 0.000047  min_lr: 0.000012  loss: 1.5862 (1.5911)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7431 (6.2456)  time: 1.0836 (0.5153 -- 6.1635)  data: 0.0567 (0.0004 -- 0.7170)  max mem: 16413
Epoch: [9]  [ 40/160]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000012  loss: 1.5730 (1.5903)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1994 (6.5470)  time: 0.7656 (0.5331 -- 3.1532)  data: 0.0015 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [9]  [ 60/160]  eta: 0:01:43  lr: 0.000047  min_lr: 0.000012  loss: 1.5459 (1.5697)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7628 (6.7136)  time: 0.9895 (0.5138 -- 4.4864)  data: 0.0014 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 1.6447 (1.5909)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3115 (6.6310)  time: 0.7314 (0.5173 -- 2.7203)  data: 0.0019 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [9]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 1.6738 (1.6052)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0124 (6.7117)  time: 0.9718 (0.5324 -- 3.9011)  data: 0.0014 (0.0004 -- 0.0037)  max mem: 16413
[2023-09-04 15:30:00,886] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:30:00,886] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:30:00,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:30:00,888] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [9]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.6335 (1.6043)  loss_scale: 32768.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9451 (6.8010)  time: 0.7872 (0.5228 -- 3.6864)  data: 0.0011 (0.0004 -- 0.0021)  max mem: 16413
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.4631 (1.5922)  loss_scale: 32768.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3671 (6.7745)  time: 0.9783 (0.5137 -- 3.6533)  data: 0.0013 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.6226 (1.5954)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5653 (6.7856)  time: 0.6262 (0.4942 -- 1.8862)  data: 0.0007 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [9] Total time: 0:02:24 (0.9030 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.6226 (1.6094)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5653 (6.7856)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1363 (0.1363)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4092 (2.4092 -- 2.4092)  data: 2.1692 (2.1692 -- 2.1692)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4361 (0.5292)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (100.0000)  time: 0.4262 (0.2011 -- 2.4092)  data: 0.2019 (0.0006 -- 2.1692)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4881 (0.5225)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2169 (0.1694 -- 0.2985)  data: 0.0082 (0.0001 -- 0.1082)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5754 (0.5868)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (98.3402)  time: 0.1970 (0.1326 -- 0.2985)  data: 0.0078 (0.0001 -- 0.1082)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 85.062 Acc@5 98.548 loss 0.557
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [10]  [  0/160]  eta: 0:25:06  lr: 0.000047  min_lr: 0.000012  loss: 1.7374 (1.7374)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9675 (6.9675)  time: 9.4157 (9.4157 -- 9.4157)  data: 8.8448 (8.8448 -- 8.8448)  max mem: 16413
[2023-09-04 15:31:15,386] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1612
[2023-09-04 15:31:15,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:31:15,389] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1612
[2023-09-04 15:31:15,390] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:31:15,391] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [10]  [ 20/160]  eta: 0:02:51  lr: 0.000047  min_lr: 0.000012  loss: 1.5404 (1.6221)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4885 (6.3250)  time: 0.8167 (0.5185 -- 4.5473)  data: 0.2654 (0.0005 -- 4.0035)  max mem: 16413
Epoch: [10]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000012  loss: 1.7481 (1.6640)  loss_scale: 16384.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7520 (6.6784)  time: 0.8565 (0.5319 -- 3.0467)  data: 0.2620 (0.0002 -- 1.9817)  max mem: 16413
Epoch: [10]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000012  loss: 1.3980 (1.5864)  loss_scale: 16384.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6075 (6.4371)  time: 0.8721 (0.5287 -- 3.0558)  data: 0.1217 (0.0004 -- 1.2731)  max mem: 16413
Epoch: [10]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 1.4996 (1.5712)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9522 (6.3434)  time: 0.8410 (0.5256 -- 2.7950)  data: 0.1292 (0.0003 -- 2.2357)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.7359 (1.5962)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3723 (6.3778)  time: 0.9052 (0.5324 -- 2.4482)  data: 0.0859 (0.0005 -- 1.1969)  max mem: 16413
Epoch: [10]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.5970 (1.5945)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6115 (6.2699)  time: 0.7351 (0.5272 -- 3.6051)  data: 0.1708 (0.0005 -- 3.0569)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.6680 (1.6093)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9871 (6.3152)  time: 1.0212 (0.5169 -- 4.2934)  data: 0.4751 (0.0004 -- 3.7446)  max mem: 16413
[2023-09-04 15:33:06,359] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:33:06,360] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:33:06,360] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:33:06,360] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.7008 (1.6241)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7851 (6.4026)  time: 0.6650 (0.4951 -- 3.0130)  data: 0.1445 (0.0002 -- 2.4677)  max mem: 16413
Epoch: [10] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.7008 (1.6342)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7851 (6.4026)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1419 (0.1419)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3973 (2.3973 -- 2.3973)  data: 2.1630 (2.1630 -- 2.1630)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2336 (0.4981)  acc1: 100.0000 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4338 (0.1996 -- 2.3973)  data: 0.2094 (0.0007 -- 2.1630)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4258 (0.5046)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2187 (0.1696 -- 0.3724)  data: 0.0105 (0.0001 -- 0.1198)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5793 (0.5945)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2011 (0.1326 -- 0.3724)  data: 0.0096 (0.0001 -- 0.1198)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 84.440 Acc@5 98.340 loss 0.579
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [11]  [  0/160]  eta: 0:18:41  lr: 0.000047  min_lr: 0.000012  loss: 2.1733 (2.1733)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7739 (11.7739)  time: 7.0114 (7.0114 -- 7.0114)  data: 5.5283 (5.5283 -- 5.5283)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:36  lr: 0.000047  min_lr: 0.000012  loss: 1.7647 (1.6890)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5450 (7.0828)  time: 0.8257 (0.5460 -- 1.8844)  data: 0.1582 (0.0008 -- 1.3556)  max mem: 16413
Epoch: [11]  [ 40/160]  eta: 0:02:00  lr: 0.000047  min_lr: 0.000012  loss: 1.5555 (1.6294)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9785 (7.3581)  time: 0.8788 (0.5287 -- 2.7964)  data: 0.1162 (0.0003 -- 1.9197)  max mem: 16413
Epoch: [11]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000012  loss: 1.7201 (1.6478)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6138 (7.2804)  time: 1.0074 (0.5126 -- 3.5236)  data: 0.1554 (0.0004 -- 1.3298)  max mem: 16413
Epoch: [11]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 1.6746 (1.6538)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5941 (7.1714)  time: 0.8068 (0.5314 -- 3.4257)  data: 0.0014 (0.0002 -- 0.0033)  max mem: 16413
[2023-09-04 15:34:59,975] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1859
[2023-09-04 15:34:59,975] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:34:59,975] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 15:34:59,975] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1859
[2023-09-04 15:34:59,976] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [11]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 1.7079 (1.6468)  loss_scale: 32768.0000 (32443.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7715 (7.0823)  time: 0.8395 (0.5318 -- 3.2302)  data: 0.0302 (0.0005 -- 0.4210)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.6723 (1.6480)  loss_scale: 16384.0000 (29789.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3750 (7.0353)  time: 0.8591 (0.5364 -- 3.9570)  data: 0.1481 (0.0003 -- 1.9525)  max mem: 16413
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.4596 (1.6221)  loss_scale: 16384.0000 (27887.6596)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4056 (7.0694)  time: 0.8992 (0.5197 -- 4.2148)  data: 0.0029 (0.0002 -- 0.0166)  max mem: 16413
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.5800 (1.6129)  loss_scale: 16384.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3527 (7.0457)  time: 0.7452 (0.4954 -- 2.3206)  data: 0.0565 (0.0002 -- 1.1011)  max mem: 16413
Epoch: [11] Total time: 0:02:23 (0.8984 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.5800 (1.5991)  loss_scale: 16384.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3527 (7.0457)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1299 (0.1299)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5046 (2.5046 -- 2.5046)  data: 2.2654 (2.2654 -- 2.2654)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2998 (0.4990)  acc1: 100.0000 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4449 (0.1919 -- 2.5046)  data: 0.2274 (0.0004 -- 2.2654)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5105 (0.5149)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2172 (0.1684 -- 0.4815)  data: 0.0146 (0.0001 -- 0.2269)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5151 (0.5713)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2032 (0.1329 -- 0.4815)  data: 0.0143 (0.0001 -- 0.2269)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 83.610 Acc@5 98.133 loss 0.582
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 86.31%
Epoch: [12]  [  0/160]  eta: 0:16:41  lr: 0.000047  min_lr: 0.000012  loss: 1.2122 (1.2122)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7257 (6.7257)  time: 6.2576 (6.2576 -- 6.2576)  data: 5.1768 (5.1768 -- 5.1768)  max mem: 16413
Epoch: [12]  [ 20/160]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000012  loss: 1.5002 (1.4973)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0941 (6.9014)  time: 0.8790 (0.5277 -- 2.2289)  data: 0.3334 (0.0007 -- 1.6919)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:01:55  lr: 0.000047  min_lr: 0.000012  loss: 1.6048 (1.5345)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9051 (6.8683)  time: 0.7895 (0.5195 -- 3.2238)  data: 0.1132 (0.0002 -- 0.8632)  max mem: 16413
Epoch: [12]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000012  loss: 1.5915 (1.5257)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2946 (6.8129)  time: 1.0669 (0.5302 -- 3.6928)  data: 0.3759 (0.0003 -- 3.1382)  max mem: 16413
[2023-09-04 15:37:04,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:37:04,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:37:04,553] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:37:04,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:37:12,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[1.1871674843346339e-05, 1.1871674843346339e-05, 1.3190749825940376e-05, 1.3190749825940376e-05, 1.4656388695489305e-05, 1.4656388695489305e-05, 1.628487632832145e-05, 1.628487632832145e-05, 1.8094307031468277e-05, 1.8094307031468277e-05, 2.010478559052031e-05, 2.010478559052031e-05, 2.2338650656133676e-05, 2.2338650656133676e-05, 2.4820722951259638e-05, 2.4820722951259638e-05, 2.7578581056955155e-05, 2.7578581056955155e-05, 3.064286784106128e-05, 3.064286784106128e-05, 3.4047630934512536e-05, 3.4047630934512536e-05, 3.783070103834726e-05, 3.783070103834726e-05, 4.203411226483028e-05, 4.203411226483028e-05, 4.670456918314476e-05, 4.670456918314476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 15:37:12,586] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=17.952616510010632, CurrSamplesPerSec=23.139338575871605, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000012  loss: 1.6174 (1.5360)  loss_scale: 32768.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5653 (6.7259)  time: 0.7131 (0.5193 -- 2.1057)  data: 0.0297 (0.0001 -- 0.4122)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.9825 (1.5987)  loss_scale: 32768.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3494 (6.6668)  time: 0.9558 (0.5144 -- 3.4815)  data: 0.0016 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [12]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.8611 (1.6227)  loss_scale: 32768.0000 (23560.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1048 (6.7900)  time: 0.8418 (0.5169 -- 2.8018)  data: 0.0680 (0.0002 -- 1.0166)  max mem: 16413
Epoch: [12]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.3961 (1.5987)  loss_scale: 32768.0000 (24866.4965)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7495 (6.6503)  time: 0.8117 (0.5422 -- 1.9472)  data: 0.2005 (0.0005 -- 1.3994)  max mem: 16413
[2023-09-04 15:38:17,795] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2076
[2023-09-04 15:38:17,795] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2076
[2023-09-04 15:38:17,795] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:38:17,795] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:38:17,795] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.7557 (1.6153)  loss_scale: 32768.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8629 (6.7329)  time: 0.7253 (0.4825 -- 2.1467)  data: 0.1991 (0.0002 -- 1.6132)  max mem: 16413
Epoch: [12] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.7557 (1.6027)  loss_scale: 32768.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8629 (6.7329)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.1169 (0.1169)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6847 (2.6847 -- 2.6847)  data: 2.4645 (2.4645 -- 2.4645)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2658 (0.5068)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4465 (0.1946 -- 2.6847)  data: 0.2273 (0.0007 -- 2.4645)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3443 (0.4958)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (99.4709)  time: 0.2096 (0.1696 -- 0.2467)  data: 0.0020 (0.0001 -- 0.0144)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4491 (0.5884)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (99.5851)  time: 0.1926 (0.1328 -- 0.2358)  data: 0.0012 (0.0001 -- 0.0144)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 82.988 Acc@5 98.755 loss 0.591
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 86.31%
Epoch: [13]  [  0/160]  eta: 0:19:38  lr: 0.000047  min_lr: 0.000012  loss: 1.0645 (1.0645)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8121 (5.8121)  time: 7.3666 (7.3666 -- 7.3666)  data: 6.5947 (6.5947 -- 6.5947)  max mem: 16413
Epoch: [13]  [ 20/160]  eta: 0:02:36  lr: 0.000047  min_lr: 0.000012  loss: 1.8095 (1.7125)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3868 (6.5369)  time: 0.8066 (0.5266 -- 2.4243)  data: 0.0174 (0.0005 -- 0.2378)  max mem: 16413
Epoch: [13]  [ 40/160]  eta: 0:01:57  lr: 0.000047  min_lr: 0.000012  loss: 1.7483 (1.7149)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6000 (6.8130)  time: 0.8397 (0.5222 -- 2.7931)  data: 0.1548 (0.0003 -- 2.2479)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000012  loss: 1.7794 (1.7076)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9058 (6.9026)  time: 0.9425 (0.5237 -- 4.5113)  data: 0.3937 (0.0006 -- 3.9729)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000012  loss: 1.7267 (1.6962)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4915 (6.8927)  time: 0.8448 (0.5357 -- 2.2972)  data: 0.2310 (0.0008 -- 1.7767)  max mem: 16413
Epoch: [13]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 1.5541 (1.6757)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8929 (6.9946)  time: 0.9097 (0.5141 -- 2.4242)  data: 0.1949 (0.0005 -- 1.4432)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.5869 (1.6654)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1639 (7.0587)  time: 0.8900 (0.5290 -- 3.0560)  data: 0.0458 (0.0008 -- 0.8526)  max mem: 16413
[2023-09-04 15:40:25,105] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:40:25,105] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:40:25,106] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:40:25,106] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.3862 (1.6469)  loss_scale: 32768.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1065 (6.9208)  time: 0.9691 (0.5255 -- 3.7212)  data: 0.0090 (0.0002 -- 0.1331)  max mem: 16413
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.4487 (1.6299)  loss_scale: 32768.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8087 (6.9684)  time: 0.7671 (0.4954 -- 3.5269)  data: 0.0007 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [13] Total time: 0:02:23 (0.8954 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.4487 (1.6218)  loss_scale: 32768.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8087 (6.9684)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1225 (0.1225)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2448 (2.2448 -- 2.2448)  data: 2.0219 (2.0219 -- 2.0219)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3061 (0.4606)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4168 (0.2001 -- 2.2448)  data: 0.2015 (0.0008 -- 2.0219)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3297 (0.4648)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2240 (0.1698 -- 0.4001)  data: 0.0196 (0.0001 -- 0.1947)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4460 (0.5283)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2086 (0.1330 -- 0.4001)  data: 0.0193 (0.0001 -- 0.1947)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.555
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.31%
Epoch: [14]  [  0/160]  eta: 0:23:09  lr: 0.000047  min_lr: 0.000012  loss: 1.5147 (1.5147)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9571 (9.9571)  time: 8.6859 (8.6859 -- 8.6859)  data: 8.1699 (8.1699 -- 8.1699)  max mem: 16413
Epoch: [14]  [ 20/160]  eta: 0:02:40  lr: 0.000047  min_lr: 0.000012  loss: 1.5069 (1.5877)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2271 (7.6217)  time: 0.7713 (0.5196 -- 3.0513)  data: 0.1764 (0.0002 -- 2.5193)  max mem: 16413
Epoch: [14]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000012  loss: 1.6643 (1.6024)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6786 (7.1926)  time: 0.9090 (0.5184 -- 3.5863)  data: 0.1699 (0.0003 -- 1.6725)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000012  loss: 1.5395 (1.5788)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4446 (7.0570)  time: 0.8892 (0.5209 -- 3.7485)  data: 0.2368 (0.0005 -- 3.2195)  max mem: 16413
[2023-09-04 15:42:13,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2317
[2023-09-04 15:42:13,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:42:13,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2317
[2023-09-04 15:42:13,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:42:13,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000012  loss: 1.6714 (1.5995)  loss_scale: 32768.0000 (31958.9136)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5889 (7.0662)  time: 0.8391 (0.5313 -- 2.7346)  data: 0.2802 (0.0002 -- 2.1857)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.6264 (1.6124)  loss_scale: 16384.0000 (28874.7723)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0061 (7.1398)  time: 0.8707 (0.5342 -- 2.4508)  data: 0.1632 (0.0005 -- 1.6641)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.4515 (1.5902)  loss_scale: 16384.0000 (26810.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2586 (6.9749)  time: 0.9072 (0.5267 -- 3.1802)  data: 0.2119 (0.0005 -- 2.6365)  max mem: 16413
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.5191 (1.5859)  loss_scale: 16384.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0719 (6.9219)  time: 0.7614 (0.5263 -- 2.4039)  data: 0.1861 (0.0001 -- 1.8462)  max mem: 16413
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.4688 (1.5793)  loss_scale: 16384.0000 (24268.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1516 (6.8988)  time: 0.7770 (0.4964 -- 3.0455)  data: 0.2458 (0.0001 -- 2.5239)  max mem: 16413
Epoch: [14] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.4688 (1.6097)  loss_scale: 16384.0000 (24268.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1516 (6.8988)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1481 (0.1481)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3380 (2.3380 -- 2.3380)  data: 2.0883 (2.0883 -- 2.0883)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4137 (0.4754)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (97.9798)  time: 0.4222 (0.2021 -- 2.3380)  data: 0.2073 (0.0007 -- 2.0883)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4465 (0.4970)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2181 (0.1700 -- 0.3917)  data: 0.0127 (0.0001 -- 0.1804)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5733 (0.5663)  acc1: 85.7143 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2037 (0.1368 -- 0.3917)  data: 0.0122 (0.0001 -- 0.1804)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.586
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [15]  [  0/160]  eta: 0:16:56  lr: 0.000047  min_lr: 0.000012  loss: 1.7185 (1.7185)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2669 (5.2669)  time: 6.3503 (6.3503 -- 6.3503)  data: 5.7986 (5.7986 -- 5.7986)  max mem: 16413
Epoch: [15]  [ 20/160]  eta: 0:02:53  lr: 0.000047  min_lr: 0.000012  loss: 1.5652 (1.5710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2384 (6.5340)  time: 0.9811 (0.5213 -- 3.4405)  data: 0.3096 (0.0004 -- 2.9096)  max mem: 16413
Epoch: [15]  [ 40/160]  eta: 0:02:01  lr: 0.000047  min_lr: 0.000012  loss: 1.6170 (1.5897)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7095 (7.1002)  time: 0.7847 (0.5199 -- 3.1718)  data: 0.2410 (0.0002 -- 2.6497)  max mem: 16413
[2023-09-04 15:44:16,727] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:44:16,727] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:44:16,728] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:44:16,729] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000012  loss: 1.5772 (1.6118)  loss_scale: 32768.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4375 (6.9068)  time: 0.8510 (0.5271 -- 3.7552)  data: 0.3005 (0.0009 -- 3.2165)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000012  loss: 1.4785 (1.6083)  loss_scale: 32768.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6651 (6.9819)  time: 0.8490 (0.5146 -- 1.8680)  data: 0.3052 (0.0007 -- 1.3027)  max mem: 16413
Epoch: [15]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.6058 (1.6232)  loss_scale: 32768.0000 (25305.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0456 (6.9111)  time: 0.9455 (0.5231 -- 2.7026)  data: 0.3999 (0.0007 -- 2.1801)  max mem: 16413
[2023-09-04 15:45:15,633] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2514
[2023-09-04 15:45:15,633] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2514
[2023-09-04 15:45:15,633] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:45:15,633] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:45:15,634] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.4226 (1.6139)  loss_scale: 32768.0000 (25591.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3105 (6.9207)  time: 0.9638 (0.5223 -- 3.9669)  data: 0.4142 (0.0009 -- 3.4232)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.7739 (1.6306)  loss_scale: 16384.0000 (24285.5035)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4399 (6.8957)  time: 0.8477 (0.5144 -- 4.9240)  data: 0.3056 (0.0003 -- 4.4063)  max mem: 16413
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.8930 (1.6543)  loss_scale: 16384.0000 (23347.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6275 (6.9145)  time: 0.6616 (0.4938 -- 2.4121)  data: 0.1397 (0.0002 -- 1.8646)  max mem: 16413
Epoch: [15] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.8930 (1.6014)  loss_scale: 16384.0000 (23347.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6275 (6.9145)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1331 (0.1331)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2753 (2.2753 -- 2.2753)  data: 2.0325 (2.0325 -- 2.0325)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3849 (0.5076)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4209 (0.2038 -- 2.2753)  data: 0.1952 (0.0009 -- 2.0325)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3992 (0.5003)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.9418)  time: 0.2262 (0.1687 -- 0.4227)  data: 0.0168 (0.0001 -- 0.2182)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4872 (0.5737)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2055 (0.1322 -- 0.4227)  data: 0.0157 (0.0001 -- 0.2182)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 83.817 Acc@5 97.925 loss 0.591
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 86.31%
Epoch: [16]  [  0/160]  eta: 0:19:03  lr: 0.000047  min_lr: 0.000012  loss: 1.7139 (1.7139)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5503 (8.5503)  time: 7.1492 (7.1492 -- 7.1492)  data: 6.6041 (6.6041 -- 6.6041)  max mem: 16413
Epoch: [16]  [ 20/160]  eta: 0:02:39  lr: 0.000046  min_lr: 0.000012  loss: 1.5523 (1.6359)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1791 (6.5456)  time: 0.8421 (0.5282 -- 3.4566)  data: 0.2930 (0.0003 -- 2.9205)  max mem: 16413
Epoch: [16]  [ 40/160]  eta: 0:02:07  lr: 0.000046  min_lr: 0.000012  loss: 1.6324 (1.6215)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0165 (6.5132)  time: 0.9828 (0.5212 -- 4.0529)  data: 0.4321 (0.0004 -- 3.4946)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000012  loss: 1.5185 (1.6154)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3251 (6.4794)  time: 0.7574 (0.5335 -- 2.5261)  data: 0.1565 (0.0006 -- 1.9704)  max mem: 16413
Epoch: [16]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 1.5666 (1.6269)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3683 (6.4826)  time: 0.9726 (0.5374 -- 3.2961)  data: 0.2369 (0.0006 -- 2.7787)  max mem: 16413
[2023-09-04 15:47:19,797] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:47:19,798] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:47:19,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:47:19,799] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.7171 (1.6212)  loss_scale: 32768.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5050 (6.5728)  time: 0.7920 (0.5349 -- 3.2096)  data: 0.2120 (0.0004 -- 2.6726)  max mem: 16413
[2023-09-04 15:47:46,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2673
[2023-09-04 15:47:46,955] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2673
[2023-09-04 15:47:46,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:47:46,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:47:46,955] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.6387 (1.6334)  loss_scale: 32768.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6961 (6.6489)  time: 0.9248 (0.5392 -- 2.7476)  data: 0.1479 (0.0004 -- 1.9222)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.6597 (1.6381)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3113 (6.7698)  time: 0.8133 (0.5238 -- 1.9765)  data: 0.1303 (0.0005 -- 1.0366)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7210 (1.6508)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7618 (6.8938)  time: 0.7739 (0.4955 -- 2.8398)  data: 0.0007 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [16] Total time: 0:02:23 (0.8989 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7210 (1.6290)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7618 (6.8938)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1409 (0.1409)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3674 (2.3674 -- 2.3674)  data: 2.1248 (2.1248 -- 2.1248)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3742 (0.5042)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4175 (0.2000 -- 2.3674)  data: 0.2026 (0.0007 -- 2.1248)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4113 (0.4852)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (99.4709)  time: 0.2175 (0.1693 -- 0.3777)  data: 0.0146 (0.0001 -- 0.1843)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4959 (0.5534)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (99.1701)  time: 0.2026 (0.1324 -- 0.3777)  data: 0.0136 (0.0001 -- 0.1843)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 86.100 Acc@5 98.548 loss 0.557
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.31%
Epoch: [17]  [  0/160]  eta: 0:18:23  lr: 0.000046  min_lr: 0.000012  loss: 1.9561 (1.9561)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6982 (5.6982)  time: 6.8994 (6.8994 -- 6.8994)  data: 4.4134 (4.4134 -- 4.4134)  max mem: 16413
Epoch: [17]  [ 20/160]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000012  loss: 1.5028 (1.5289)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7896 (7.0930)  time: 0.9296 (0.5289 -- 4.3854)  data: 0.3456 (0.0005 -- 3.8595)  max mem: 16413
Epoch: [17]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000012  loss: 1.6347 (1.5582)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9580 (6.5694)  time: 0.8721 (0.5336 -- 4.0666)  data: 0.3152 (0.0003 -- 3.5175)  max mem: 16413
Epoch: [17]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 1.6970 (1.5908)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5344 (6.7364)  time: 0.8998 (0.5158 -- 4.1133)  data: 0.3533 (0.0005 -- 3.5861)  max mem: 16413
Epoch: [17]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.5513 (1.5772)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9468 (6.6024)  time: 0.7759 (0.5309 -- 2.8815)  data: 0.2240 (0.0001 -- 2.3421)  max mem: 16413
[2023-09-04 15:49:53,196] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:49:53,196] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:49:53,196] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:49:53,196] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.7074 (1.5967)  loss_scale: 32768.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6903 (6.5991)  time: 0.8895 (0.5172 -- 4.8350)  data: 0.3455 (0.0003 -- 4.3262)  max mem: 16413
Epoch: [17]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.4888 (1.5923)  loss_scale: 32768.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2773 (6.5834)  time: 0.9670 (0.5145 -- 4.1027)  data: 0.0989 (0.0004 -- 1.9451)  max mem: 16413
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.3057 (1.5614)  loss_scale: 32768.0000 (23239.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5502 (6.4651)  time: 0.8488 (0.5246 -- 2.7625)  data: 0.0014 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.4189 (1.5601)  loss_scale: 32768.0000 (24371.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9360 (6.5236)  time: 0.6793 (0.4950 -- 2.3682)  data: 0.0009 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [17] Total time: 0:02:21 (0.8863 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.4189 (1.5973)  loss_scale: 32768.0000 (24371.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9360 (6.5236)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1271 (0.1271)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2789 (2.2789 -- 2.2789)  data: 2.0633 (2.0633 -- 2.0633)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4108 (0.5572)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.3976 (0.1815 -- 2.2789)  data: 0.1886 (0.0005 -- 2.0633)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3462 (0.4989)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2158 (0.1697 -- 0.4353)  data: 0.0115 (0.0001 -- 0.2157)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4615 (0.5828)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2036 (0.1328 -- 0.4353)  data: 0.0112 (0.0001 -- 0.2157)  max mem: 16413
Val: Total time: 0:00:07 (0.2808 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.571
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [18]  [  0/160]  eta: 0:20:15  lr: 0.000046  min_lr: 0.000012  loss: 1.7163 (1.7163)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9163 (4.9163)  time: 7.5960 (7.5960 -- 7.5960)  data: 7.0680 (7.0680 -- 7.0680)  max mem: 16413
Epoch: [18]  [ 20/160]  eta: 0:02:41  lr: 0.000046  min_lr: 0.000012  loss: 1.4849 (1.5085)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6881 (7.0450)  time: 0.8297 (0.5193 -- 3.4259)  data: 0.2769 (0.0004 -- 2.9055)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000012  loss: 1.5995 (1.5594)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9336 (7.1700)  time: 0.8827 (0.5157 -- 2.8377)  data: 0.2562 (0.0004 -- 2.2918)  max mem: 16413
[2023-09-04 15:51:52,501] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:51:52,502] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 15:51:52,503] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:51:52,504] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [18]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 1.7115 (1.6312)  loss_scale: 65536.0000 (38676.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3585 (6.9452)  time: 0.9556 (0.5043 -- 4.5149)  data: 0.3540 (0.0003 -- 3.9728)  max mem: 16413
[2023-09-04 15:52:10,386] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2951
[2023-09-04 15:52:10,386] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2951
[2023-09-04 15:52:10,386] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 15:52:10,386] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 15:52:10,386] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [18]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.5161 (1.6078)  loss_scale: 32768.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2612 (7.1946)  time: 0.7698 (0.5223 -- 3.0741)  data: 0.2182 (0.0003 -- 2.5232)  max mem: 16413
Epoch: [18]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.6492 (1.6072)  loss_scale: 32768.0000 (39581.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3168 (7.0354)  time: 0.8982 (0.5366 -- 3.7150)  data: 0.2355 (0.0002 -- 3.1905)  max mem: 16413
[2023-09-04 15:52:50,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=10, lr=[1.1769696168306417e-05, 1.1769696168306417e-05, 1.307744018700713e-05, 1.307744018700713e-05, 1.4530489096674585e-05, 1.4530489096674585e-05, 1.6144987885193987e-05, 1.6144987885193987e-05, 1.7938875427993317e-05, 1.7938875427993317e-05, 1.9932083808881464e-05, 1.9932083808881464e-05, 2.214675978764607e-05, 2.214675978764607e-05, 2.4607510875162297e-05, 2.4607510875162297e-05, 2.734167875018033e-05, 2.734167875018033e-05, 3.037964305575592e-05, 3.037964305575592e-05, 3.3755158950839915e-05, 3.3755158950839915e-05, 3.75057321675999e-05, 3.75057321675999e-05, 4.167303574177767e-05, 4.167303574177767e-05, 4.630337304641963e-05, 4.630337304641963e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 15:52:50,656] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=18.284987083517223, CurrSamplesPerSec=23.46568797484643, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.7243 (1.6135)  loss_scale: 32768.0000 (38455.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8619 (7.1620)  time: 0.7992 (0.5180 -- 3.2069)  data: 0.1673 (0.0003 -- 1.9762)  max mem: 16413
Epoch: [18]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.5854 (1.6223)  loss_scale: 32768.0000 (37648.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1247 (7.1407)  time: 0.8801 (0.5345 -- 2.9380)  data: 0.2355 (0.0009 -- 2.3930)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.6264 (1.6224)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4174 (7.1043)  time: 0.7090 (0.4953 -- 2.3595)  data: 0.1909 (0.0002 -- 1.8325)  max mem: 16413
Epoch: [18] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.6264 (1.6038)  loss_scale: 32768.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4174 (7.1043)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1434 (0.1434)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2374 (2.2374 -- 2.2374)  data: 2.0397 (2.0397 -- 2.0397)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3887 (0.5078)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4236 (0.1908 -- 2.2374)  data: 0.2098 (0.0007 -- 2.0397)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3887 (0.5057)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.8836)  time: 0.2307 (0.1685 -- 0.5011)  data: 0.0247 (0.0001 -- 0.2552)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5288 (0.5867)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.9253)  time: 0.2164 (0.1327 -- 0.5011)  data: 0.0243 (0.0001 -- 0.2552)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.592
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [19]  [  0/160]  eta: 0:16:45  lr: 0.000046  min_lr: 0.000012  loss: 1.0587 (1.0587)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7613 (10.7613)  time: 6.2862 (6.2862 -- 6.2862)  data: 5.7105 (5.7105 -- 5.7105)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:40  lr: 0.000046  min_lr: 0.000012  loss: 1.5276 (1.5609)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2596 (6.4283)  time: 0.8885 (0.5275 -- 3.0983)  data: 0.1912 (0.0005 -- 2.5816)  max mem: 16413
[2023-09-04 15:54:10,717] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:54:10,717] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 15:54:10,719] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:54:10,720] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [19]  [ 40/160]  eta: 0:01:58  lr: 0.000046  min_lr: 0.000012  loss: 1.5391 (1.5845)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6291 (7.0421)  time: 0.8205 (0.5314 -- 4.2743)  data: 0.1800 (0.0002 -- 1.9420)  max mem: 16413
[2023-09-04 15:54:15,681] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3085
[2023-09-04 15:54:15,681] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 15:54:15,681] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3085
[2023-09-04 15:54:15,682] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 15:54:15,683] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000012  loss: 1.7645 (1.6291)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7161 (6.9156)  time: 0.9294 (0.5158 -- 3.7981)  data: 0.3787 (0.0004 -- 3.2556)  max mem: 16413
Epoch: [19]  [ 80/160]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000012  loss: 1.4812 (1.5973)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9953 (6.7690)  time: 0.8147 (0.5404 -- 2.0723)  data: 0.1550 (0.0002 -- 1.1514)  max mem: 16413
Epoch: [19]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.3512 (1.5743)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9209 (6.8661)  time: 0.8791 (0.5229 -- 2.1121)  data: 0.1602 (0.0002 -- 1.5514)  max mem: 16413
[2023-09-04 15:55:19,805] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3156
[2023-09-04 15:55:19,805] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3156
[2023-09-04 15:55:19,806] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:55:19,806] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:55:19,806] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.5898 (1.5733)  loss_scale: 32768.0000 (33445.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5364 (7.0028)  time: 0.9449 (0.5223 -- 3.0724)  data: 0.3097 (0.0003 -- 2.5483)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.6099 (1.5753)  loss_scale: 16384.0000 (31025.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5300 (6.9913)  time: 0.8881 (0.5308 -- 2.6534)  data: 0.3324 (0.0003 -- 2.1117)  max mem: 16413
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7402 (1.5801)  loss_scale: 16384.0000 (29286.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9537 (7.0668)  time: 0.7263 (0.4932 -- 2.2404)  data: 0.2078 (0.0002 -- 1.6939)  max mem: 16413
Epoch: [19] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7402 (1.6036)  loss_scale: 16384.0000 (29286.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9537 (7.0668)
[2023-09-04 15:55:52,169] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-09-04 15:55:52,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-09-04 15:55:52,171] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-09-04 15:55:52,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-09-04 15:55:53,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-09-04 15:55:53,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1497 (0.1497)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4119 (2.4119 -- 2.4119)  data: 2.2039 (2.2039 -- 2.2039)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5762 (0.5584)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4176 (0.2010 -- 2.4119)  data: 0.2015 (0.0004 -- 2.2039)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3625 (0.5087)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (99.4709)  time: 0.2220 (0.1694 -- 0.3151)  data: 0.0093 (0.0001 -- 0.1375)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4752 (0.6156)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (99.1701)  time: 0.2048 (0.1319 -- 0.3151)  data: 0.0089 (0.0001 -- 0.1375)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 84.025 Acc@5 98.548 loss 0.600
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 86.31%
Epoch: [20]  [  0/160]  eta: 0:17:40  lr: 0.000046  min_lr: 0.000012  loss: 1.9333 (1.9333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6391 (5.6391)  time: 6.6273 (6.6273 -- 6.6273)  data: 4.7842 (4.7842 -- 4.7842)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:39  lr: 0.000046  min_lr: 0.000012  loss: 1.4373 (1.5919)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6796 (6.6251)  time: 0.8679 (0.5378 -- 2.9292)  data: 0.0150 (0.0002 -- 0.1853)  max mem: 16413
Epoch: [20]  [ 40/160]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000012  loss: 1.7131 (1.6548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1927 (6.5619)  time: 0.8895 (0.5308 -- 2.9685)  data: 0.1027 (0.0007 -- 1.1197)  max mem: 16413
Epoch: [20]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000012  loss: 1.6098 (1.6485)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1074 (6.4957)  time: 0.9300 (0.5280 -- 3.2617)  data: 0.1548 (0.0003 -- 2.7474)  max mem: 16413
Epoch: [20]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 1.5702 (1.6383)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9882 (6.5012)  time: 0.8349 (0.5421 -- 3.9908)  data: 0.2273 (0.0006 -- 3.4639)  max mem: 16413
[2023-09-04 15:57:23,215] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:57:23,216] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:57:23,217] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:57:23,217] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.5121 (1.6341)  loss_scale: 32768.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3219 (6.7023)  time: 0.8727 (0.5367 -- 2.9618)  data: 0.3000 (0.0003 -- 2.4030)  max mem: 16413
Epoch: [20]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.6194 (1.6331)  loss_scale: 32768.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2862 (6.6300)  time: 0.8718 (0.5183 -- 4.5656)  data: 0.0889 (0.0003 -- 1.2006)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.5360 (1.6083)  loss_scale: 32768.0000 (22891.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1844 (6.5785)  time: 0.8513 (0.5332 -- 3.2679)  data: 0.0575 (0.0003 -- 1.1163)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7073 (1.6122)  loss_scale: 32768.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0561 (6.5465)  time: 0.6852 (0.4948 -- 2.7979)  data: 0.1597 (0.0002 -- 2.2770)  max mem: 16413
Epoch: [20] Total time: 0:02:22 (0.8884 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7073 (1.6153)  loss_scale: 32768.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0561 (6.5465)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1328 (0.1328)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6232 (2.6232 -- 2.6232)  data: 2.3269 (2.3269 -- 2.3269)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2793 (0.4631)  acc1: 100.0000 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4372 (0.1918 -- 2.6232)  data: 0.2125 (0.0007 -- 2.3269)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3385 (0.4731)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2077 (0.1698 -- 0.2540)  data: 0.0007 (0.0001 -- 0.0014)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5197 (0.5468)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (99.1701)  time: 0.1900 (0.1326 -- 0.2447)  data: 0.0004 (0.0001 -- 0.0013)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.544
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.31%
Epoch: [21]  [  0/160]  eta: 0:25:16  lr: 0.000046  min_lr: 0.000012  loss: 1.9591 (1.9591)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4440 (10.4440)  time: 9.4783 (9.4783 -- 9.4783)  data: 8.9469 (8.9469 -- 8.9469)  max mem: 16413
Epoch: [21]  [ 20/160]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000012  loss: 1.8008 (1.6971)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6534 (6.7833)  time: 0.8043 (0.5284 -- 3.1989)  data: 0.2507 (0.0006 -- 2.6573)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:07  lr: 0.000046  min_lr: 0.000012  loss: 1.4553 (1.6034)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8573 (6.5182)  time: 0.9020 (0.5238 -- 3.9453)  data: 0.3594 (0.0003 -- 3.4336)  max mem: 16413
[2023-09-04 15:59:22,144] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3412
[2023-09-04 15:59:22,144] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3412
[2023-09-04 15:59:22,144] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:59:22,144] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 15:59:22,144] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 15:59:24,658] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3414
[2023-09-04 15:59:24,659] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 15:59:24,659] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3414
[2023-09-04 15:59:24,659] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 15:59:24,659] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [21]  [ 60/160]  eta: 0:01:35  lr: 0.000046  min_lr: 0.000012  loss: 1.7750 (1.6203)  loss_scale: 32768.0000 (29410.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7344 (6.5868)  time: 0.7461 (0.5206 -- 2.0727)  data: 0.1278 (0.0002 -- 1.5428)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 1.4543 (1.5776)  loss_scale: 8192.0000 (24171.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2870 (6.5586)  time: 1.0124 (0.5328 -- 4.3754)  data: 0.3977 (0.0002 -- 3.8469)  max mem: 16413
Epoch: [21]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.6604 (1.5838)  loss_scale: 8192.0000 (21007.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0355 (6.7183)  time: 0.7269 (0.5258 -- 2.6960)  data: 0.1724 (0.0003 -- 2.1220)  max mem: 16413
Epoch: [21]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.6047 (1.5885)  loss_scale: 8192.0000 (18888.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4204 (6.7138)  time: 0.9363 (0.5225 -- 3.3262)  data: 0.3861 (0.0006 -- 2.7766)  max mem: 16413
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.3973 (1.5734)  loss_scale: 8192.0000 (17371.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4166 (6.7697)  time: 0.8725 (0.5289 -- 2.5709)  data: 0.1331 (0.0004 -- 1.5749)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7352 (1.5823)  loss_scale: 8192.0000 (16281.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1837 (6.8022)  time: 0.6841 (0.4991 -- 2.4820)  data: 0.0610 (0.0002 -- 0.7945)  max mem: 16413
Epoch: [21] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7352 (1.5972)  loss_scale: 8192.0000 (16281.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1837 (6.8022)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1428 (0.1428)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3818 (2.3818 -- 2.3818)  data: 2.1401 (2.1401 -- 2.1401)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2510 (0.4968)  acc1: 100.0000 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4215 (0.2043 -- 2.3818)  data: 0.1970 (0.0005 -- 2.1401)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3321 (0.4953)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2193 (0.1706 -- 0.3499)  data: 0.0088 (0.0001 -- 0.1471)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4981 (0.5545)  acc1: 77.7778 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.1992 (0.1323 -- 0.3499)  data: 0.0083 (0.0001 -- 0.1471)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 85.477 Acc@5 97.303 loss 0.573
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.31%
Epoch: [22]  [  0/160]  eta: 0:22:39  lr: 0.000046  min_lr: 0.000012  loss: 1.4492 (1.4492)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9899 (4.9899)  time: 8.4945 (8.4945 -- 8.4945)  data: 7.9483 (7.9483 -- 7.9483)  max mem: 16413
Epoch: [22]  [ 20/160]  eta: 0:02:48  lr: 0.000046  min_lr: 0.000012  loss: 1.5891 (1.5554)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4762 (6.4377)  time: 0.8419 (0.5270 -- 3.8245)  data: 0.2886 (0.0005 -- 3.2844)  max mem: 16413
[2023-09-04 16:01:27,493] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:01:27,493] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:01:27,493] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 16:01:27,493] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [22]  [ 40/160]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000012  loss: 1.6004 (1.5971)  loss_scale: 16384.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2621 (6.4779)  time: 0.8308 (0.5338 -- 3.0372)  data: 0.1709 (0.0003 -- 1.3545)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 1.7561 (1.6458)  loss_scale: 16384.0000 (13295.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5173 (6.6067)  time: 0.9283 (0.5159 -- 2.7124)  data: 0.2051 (0.0003 -- 2.1961)  max mem: 16413
[2023-09-04 16:02:16,624] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3598
[2023-09-04 16:02:16,624] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 16:02:16,624] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3598
[2023-09-04 16:02:16,625] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 16:02:16,625] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [22]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 1.5828 (1.6460)  loss_scale: 16384.0000 (13754.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0509 (6.7271)  time: 0.8328 (0.5224 -- 2.4168)  data: 0.1775 (0.0006 -- 1.8681)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.7928 (1.6488)  loss_scale: 8192.0000 (12652.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8948 (6.9018)  time: 0.8740 (0.5210 -- 2.3211)  data: 0.2992 (0.0007 -- 1.7616)  max mem: 16413
Epoch: [22]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.5914 (1.6438)  loss_scale: 8192.0000 (11915.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8023 (6.8579)  time: 0.9610 (0.5287 -- 3.9696)  data: 0.3779 (0.0005 -- 3.4383)  max mem: 16413
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.6510 (1.6521)  loss_scale: 8192.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3571 (6.8644)  time: 0.8701 (0.5225 -- 3.2801)  data: 0.2457 (0.0003 -- 2.7617)  max mem: 16413
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.6360 (1.6460)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3571 (6.8405)  time: 0.7379 (0.4965 -- 1.4534)  data: 0.1293 (0.0002 -- 0.9105)  max mem: 16413
Epoch: [22] Total time: 0:02:24 (0.9047 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.6360 (1.6186)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3571 (6.8405)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2513 (0.2513)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3003 (2.3003 -- 2.3003)  data: 2.0701 (2.0701 -- 2.0701)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4013 (0.5513)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4084 (0.2030 -- 2.3003)  data: 0.1893 (0.0007 -- 2.0701)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4330 (0.5377)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.8836)  time: 0.2217 (0.1686 -- 0.4056)  data: 0.0132 (0.0001 -- 0.2285)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5522 (0.5978)  acc1: 77.7778 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.2060 (0.1333 -- 0.4056)  data: 0.0129 (0.0001 -- 0.2285)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 84.855 Acc@5 97.510 loss 0.603
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [23]  [  0/160]  eta: 0:23:23  lr: 0.000046  min_lr: 0.000012  loss: 1.8274 (1.8274)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7294 (8.7294)  time: 8.7702 (8.7702 -- 8.7702)  data: 8.2515 (8.2515 -- 8.2515)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:51  lr: 0.000046  min_lr: 0.000012  loss: 1.6137 (1.6018)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1398 (6.3205)  time: 0.8448 (0.5216 -- 4.8124)  data: 0.3073 (0.0004 -- 4.2983)  max mem: 16413
Epoch: [23]  [ 40/160]  eta: 0:02:06  lr: 0.000046  min_lr: 0.000012  loss: 1.6670 (1.6017)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7148 (6.7998)  time: 0.8821 (0.5270 -- 3.8819)  data: 0.3354 (0.0004 -- 3.3639)  max mem: 16413
[2023-09-04 16:04:22,276] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:04:22,276] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 16:04:22,277] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:04:22,277] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [23]  [ 60/160]  eta: 0:01:44  lr: 0.000046  min_lr: 0.000012  loss: 1.7309 (1.6615)  loss_scale: 16384.0000 (10072.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1897 (7.0223)  time: 1.0188 (0.5150 -- 4.5992)  data: 0.4791 (0.0003 -- 4.0949)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000012  loss: 1.6311 (1.6523)  loss_scale: 16384.0000 (11630.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9984 (6.8191)  time: 0.7790 (0.5221 -- 2.9466)  data: 0.2324 (0.0002 -- 2.4219)  max mem: 16413
Epoch: [23]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.6562 (1.6551)  loss_scale: 16384.0000 (12571.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1122 (7.0078)  time: 0.9161 (0.5264 -- 3.0367)  data: 0.3601 (0.0002 -- 2.5200)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.7875 (1.6664)  loss_scale: 16384.0000 (13201.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3835 (7.3302)  time: 0.7136 (0.5279 -- 2.2921)  data: 0.1639 (0.0002 -- 1.7445)  max mem: 16413
Epoch: [23]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.5577 (1.6474)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4668 (7.2332)  time: 0.9093 (0.5305 -- 2.3761)  data: 0.2554 (0.0004 -- 1.8488)  max mem: 16413
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.6433 (1.6427)  loss_scale: 16384.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3149 (7.2648)  time: 0.7373 (0.4952 -- 1.8804)  data: 0.1160 (0.0002 -- 0.8358)  max mem: 16413
Epoch: [23] Total time: 0:02:23 (0.8979 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.6433 (1.6323)  loss_scale: 16384.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3149 (7.2648)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1469 (0.1469)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2087 (2.2087 -- 2.2087)  data: 1.9600 (1.9600 -- 1.9600)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3517 (0.4771)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4020 (0.2022 -- 2.2087)  data: 0.1826 (0.0008 -- 1.9600)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4153 (0.4765)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2244 (0.1692 -- 0.3724)  data: 0.0173 (0.0001 -- 0.1487)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5357 (0.5672)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2080 (0.1332 -- 0.3724)  data: 0.0169 (0.0001 -- 0.1487)  max mem: 16413
Val: Total time: 0:00:07 (0.2846 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.591
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [24]  [  0/160]  eta: 0:19:19  lr: 0.000046  min_lr: 0.000012  loss: 1.4827 (1.4827)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1229 (3.1229)  time: 7.2466 (7.2466 -- 7.2466)  data: 6.4366 (6.4366 -- 6.4366)  max mem: 16413
[2023-09-04 16:06:22,541] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:06:22,541] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:06:22,543] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:06:22,543] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 20/160]  eta: 0:02:35  lr: 0.000046  min_lr: 0.000012  loss: 1.5050 (1.5352)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9818 (6.5413)  time: 0.8042 (0.5267 -- 2.9383)  data: 0.1380 (0.0004 -- 1.1349)  max mem: 16413
[2023-09-04 16:06:41,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3874
[2023-09-04 16:06:41,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:06:41,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3874
[2023-09-04 16:06:41,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:06:41,515] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [ 40/160]  eta: 0:02:00  lr: 0.000046  min_lr: 0.000012  loss: 1.4020 (1.4910)  loss_scale: 32768.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2956 (6.6591)  time: 0.8878 (0.5254 -- 2.6842)  data: 0.2016 (0.0005 -- 2.1460)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:43  lr: 0.000046  min_lr: 0.000012  loss: 1.5278 (1.5169)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5342 (6.9167)  time: 1.1053 (0.5194 -- 4.0691)  data: 0.4224 (0.0004 -- 3.5398)  max mem: 16413
Epoch: [24]  [ 80/160]  eta: 0:01:19  lr: 0.000046  min_lr: 0.000012  loss: 1.5002 (1.5330)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1377 (7.3075)  time: 0.8480 (0.5158 -- 3.6932)  data: 0.3158 (0.0002 -- 3.1661)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000012  loss: 1.3875 (1.5025)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7607 (7.4300)  time: 0.8865 (0.5295 -- 4.2012)  data: 0.3398 (0.0004 -- 3.6731)  max mem: 16413
Epoch: [24]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.4636 (1.5035)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5444 (7.4600)  time: 0.7821 (0.5148 -- 3.3466)  data: 0.2392 (0.0002 -- 2.8119)  max mem: 16413
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.6696 (1.5230)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5297 (7.4265)  time: 0.8704 (0.5149 -- 2.7691)  data: 0.3089 (0.0004 -- 2.2261)  max mem: 16413
[2023-09-04 16:08:27,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=16, lr=[1.1608898360300739e-05, 1.1608898360300739e-05, 1.2898775955889711e-05, 1.2898775955889711e-05, 1.4331973284321898e-05, 1.4331973284321898e-05, 1.5924414760357666e-05, 1.5924414760357666e-05, 1.7693794178175183e-05, 1.7693794178175183e-05, 1.9659771309083537e-05, 1.9659771309083537e-05, 2.1844190343426154e-05, 2.1844190343426154e-05, 2.4271322603806833e-05, 2.4271322603806833e-05, 2.6968136226452038e-05, 2.6968136226452038e-05, 2.9964595807168928e-05, 2.9964595807168928e-05, 3.329399534129881e-05, 3.329399534129881e-05, 3.6993328156998675e-05, 3.6993328156998675e-05, 4.1103697952220754e-05, 4.1103697952220754e-05, 4.56707755024675e-05, 4.56707755024675e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 16:08:27,476] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=18.640661840407898, CurrSamplesPerSec=24.75730373113258, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.6514 (1.5329)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5304 (7.4155)  time: 0.7262 (0.4930 -- 2.4207)  data: 0.2009 (0.0002 -- 1.8170)  max mem: 16413
Epoch: [24] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.6514 (1.5457)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5304 (7.4155)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1617 (0.1617)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3205 (2.3205 -- 2.3205)  data: 2.1011 (2.1011 -- 2.1011)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3258 (0.5119)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4148 (0.2005 -- 2.3205)  data: 0.1920 (0.0005 -- 2.1011)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3565 (0.4912)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2244 (0.1720 -- 0.4107)  data: 0.0165 (0.0001 -- 0.2271)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5133 (0.5752)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.7552)  time: 0.2092 (0.1328 -- 0.4107)  data: 0.0161 (0.0001 -- 0.2271)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 84.647 Acc@5 98.133 loss 0.580
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [25]  [  0/160]  eta: 0:20:58  lr: 0.000046  min_lr: 0.000012  loss: 1.4903 (1.4903)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3180 (5.3180)  time: 7.8668 (7.8668 -- 7.8668)  data: 7.3138 (7.3138 -- 7.3138)  max mem: 16413
[2023-09-04 16:08:44,917] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:08:44,917] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:08:44,918] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:08:44,918] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 20/160]  eta: 0:03:06  lr: 0.000046  min_lr: 0.000012  loss: 1.7264 (1.6898)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1859 (7.0184)  time: 1.0036 (0.5167 -- 3.3911)  data: 0.2494 (0.0008 -- 2.7322)  max mem: 16413
Epoch: [25]  [ 40/160]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000012  loss: 1.4848 (1.6320)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7727 (7.1395)  time: 0.7915 (0.5171 -- 3.0296)  data: 0.0013 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000012  loss: 1.7112 (1.6673)  loss_scale: 32768.0000 (31962.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6769 (7.2301)  time: 0.8988 (0.5333 -- 4.0294)  data: 0.0015 (0.0003 -- 0.0041)  max mem: 16413
[2023-09-04 16:09:38,117] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4062
[2023-09-04 16:09:38,117] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4062
[2023-09-04 16:09:38,118] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:09:38,118] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:09:38,118] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 1.4984 (1.6457)  loss_scale: 16384.0000 (28318.0247)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0984 (7.1582)  time: 0.8177 (0.5207 -- 3.4171)  data: 0.0023 (0.0004 -- 0.0156)  max mem: 16413
Epoch: [25]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.6645 (1.6344)  loss_scale: 16384.0000 (25954.8515)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8569 (7.0195)  time: 0.9091 (0.5240 -- 3.5133)  data: 0.0016 (0.0004 -- 0.0054)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.4889 (1.6163)  loss_scale: 16384.0000 (24372.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9350 (6.9222)  time: 0.7257 (0.5244 -- 3.3861)  data: 0.0027 (0.0005 -- 0.0166)  max mem: 16413
Epoch: [25]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.6601 (1.6160)  loss_scale: 16384.0000 (23239.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9310 (6.9845)  time: 0.8711 (0.5190 -- 2.9680)  data: 0.0027 (0.0004 -- 0.0266)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.5396 (1.6074)  loss_scale: 16384.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0927 (7.0712)  time: 0.7348 (0.4944 -- 2.7047)  data: 0.2129 (0.0002 -- 2.1893)  max mem: 16413
Epoch: [25] Total time: 0:02:22 (0.8899 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.5396 (1.6059)  loss_scale: 16384.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0927 (7.0712)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1748 (0.1748)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2706 (2.2706 -- 2.2706)  data: 2.0453 (2.0453 -- 2.0453)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4154 (0.4964)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4200 (0.1837 -- 2.2706)  data: 0.2090 (0.0004 -- 2.0453)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4154 (0.4761)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2275 (0.1701 -- 0.4659)  data: 0.0251 (0.0001 -- 0.2446)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4408 (0.5478)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2157 (0.1321 -- 0.4659)  data: 0.0247 (0.0001 -- 0.2446)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 87.137 Acc@5 98.340 loss 0.553
Accuracy of the network on the 482 val images: 87.14%
[2023-09-04 16:11:05,481] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 16:11:05,482] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 16:11:05,482] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 16:11:05,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 16:11:06,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 16:11:06,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 87.14%
Epoch: [26]  [  0/160]  eta: 0:20:52  lr: 0.000046  min_lr: 0.000012  loss: 0.6511 (0.6511)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1758 (6.1758)  time: 7.8257 (7.8257 -- 7.8257)  data: 5.1799 (5.1799 -- 5.1799)  max mem: 16413
Epoch: [26]  [ 20/160]  eta: 0:02:42  lr: 0.000046  min_lr: 0.000012  loss: 1.6833 (1.6883)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7700 (6.6221)  time: 0.8282 (0.5240 -- 1.8822)  data: 0.0695 (0.0008 -- 1.3584)  max mem: 16413
[2023-09-04 16:11:41,256] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:11:41,256] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:11:41,257] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:11:41,257] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000012  loss: 1.5522 (1.6520)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9876 (7.0227)  time: 0.9202 (0.5254 -- 2.8158)  data: 0.0015 (0.0003 -- 0.0066)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 1.7441 (1.6586)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4028 (6.8183)  time: 0.8945 (0.5268 -- 2.4330)  data: 0.0593 (0.0002 -- 1.1482)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000012  loss: 1.5965 (1.6463)  loss_scale: 32768.0000 (26497.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2111 (7.0056)  time: 0.8347 (0.5287 -- 3.0770)  data: 0.0013 (0.0004 -- 0.0022)  max mem: 16413
Epoch: [26]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000012  loss: 1.6571 (1.6348)  loss_scale: 32768.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3425 (6.9588)  time: 0.9028 (0.5330 -- 2.0064)  data: 0.0748 (0.0002 -- 1.4583)  max mem: 16413
[2023-09-04 16:12:43,331] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4262
[2023-09-04 16:12:43,332] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:12:43,332] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4262
[2023-09-04 16:12:43,332] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:12:43,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000012  loss: 1.5858 (1.6228)  loss_scale: 16384.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9007 (6.9350)  time: 0.8220 (0.5186 -- 2.7238)  data: 0.1858 (0.0004 -- 2.1937)  max mem: 16413
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.5576 (1.6171)  loss_scale: 16384.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6149 (6.9548)  time: 0.8676 (0.5391 -- 3.2208)  data: 0.3013 (0.0001 -- 2.6824)  max mem: 16413
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.7227 (1.6217)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1488 (6.8778)  time: 0.7499 (0.4958 -- 2.8660)  data: 0.2133 (0.0002 -- 2.3286)  max mem: 16413
Epoch: [26] Total time: 0:02:21 (0.8865 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.7227 (1.6142)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1488 (6.8778)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1574 (0.1574)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2992 (2.2992 -- 2.2992)  data: 2.0457 (2.0457 -- 2.0457)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2845 (0.5135)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4200 (0.1961 -- 2.2992)  data: 0.2083 (0.0005 -- 2.0457)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3664 (0.4915)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2264 (0.1698 -- 0.4531)  data: 0.0239 (0.0001 -- 0.2350)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4793 (0.5816)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.7552)  time: 0.2130 (0.1329 -- 0.4531)  data: 0.0235 (0.0001 -- 0.2350)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 85.270 Acc@5 98.340 loss 0.576
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.14%
Epoch: [27]  [  0/160]  eta: 0:19:33  lr: 0.000045  min_lr: 0.000012  loss: 1.6554 (1.6554)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3557 (6.3557)  time: 7.3335 (7.3335 -- 7.3335)  data: 4.8097 (4.8097 -- 4.8097)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:42  lr: 0.000045  min_lr: 0.000012  loss: 1.4448 (1.4186)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0828 (6.6839)  time: 0.8518 (0.5354 -- 3.8620)  data: 0.0864 (0.0002 -- 0.8755)  max mem: 16413
Epoch: [27]  [ 40/160]  eta: 0:02:03  lr: 0.000045  min_lr: 0.000012  loss: 1.5434 (1.4904)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5854 (6.4832)  time: 0.8927 (0.5341 -- 3.4238)  data: 0.2355 (0.0005 -- 2.8955)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000012  loss: 1.6116 (1.5255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5706 (6.7649)  time: 0.8764 (0.5310 -- 2.9426)  data: 0.2959 (0.0004 -- 2.4290)  max mem: 16413
[2023-09-04 16:14:46,647] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:14:46,647] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:14:46,648] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:14:46,648] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000012  loss: 1.6264 (1.5573)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7273 (6.8356)  time: 0.9007 (0.5263 -- 3.1132)  data: 0.0735 (0.0006 -- 1.4298)  max mem: 16413
[2023-09-04 16:14:54,741] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4401
[2023-09-04 16:14:54,741] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:14:54,741] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4401
[2023-09-04 16:14:54,742] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:14:54,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000012  loss: 1.7341 (1.5836)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0329 (6.8680)  time: 0.8041 (0.5247 -- 3.1682)  data: 0.0669 (0.0003 -- 1.2385)  max mem: 16413
Epoch: [27]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000012  loss: 1.4865 (1.5760)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6973 (6.9642)  time: 0.8909 (0.5195 -- 3.9398)  data: 0.3178 (0.0004 -- 3.4021)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.5816 (1.5760)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6231 (6.8749)  time: 0.8917 (0.5218 -- 3.4237)  data: 0.3430 (0.0006 -- 2.8883)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.6444 (1.5872)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2370 (6.9831)  time: 0.6839 (0.4959 -- 3.8217)  data: 0.1670 (0.0002 -- 3.3194)  max mem: 16413
Epoch: [27] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.6444 (1.6018)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2370 (6.9831)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1323 (0.1323)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2614 (2.2614 -- 2.2614)  data: 2.0558 (2.0558 -- 2.0558)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3286 (0.4633)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4113 (0.2025 -- 2.2614)  data: 0.1926 (0.0007 -- 2.0558)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3286 (0.4476)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (99.4709)  time: 0.2271 (0.1694 -- 0.5444)  data: 0.0203 (0.0001 -- 0.3393)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4211 (0.5272)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (99.5851)  time: 0.2080 (0.1325 -- 0.5444)  data: 0.0193 (0.0001 -- 0.3393)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 86.307 Acc@5 97.925 loss 0.551
Accuracy of the network on the 482 val images: 86.31%
Max accuracy: 87.14%
Epoch: [28]  [  0/160]  eta: 0:16:47  lr: 0.000045  min_lr: 0.000012  loss: 1.6564 (1.6564)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8478 (6.8478)  time: 6.2948 (6.2948 -- 6.2948)  data: 4.9617 (4.9617 -- 4.9617)  max mem: 16413
Epoch: [28]  [ 20/160]  eta: 0:02:40  lr: 0.000045  min_lr: 0.000012  loss: 1.4292 (1.4860)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8738 (6.7749)  time: 0.8927 (0.5214 -- 2.9941)  data: 0.3008 (0.0005 -- 2.4690)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:02:01  lr: 0.000045  min_lr: 0.000012  loss: 1.7009 (1.5630)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3022 (6.7105)  time: 0.8670 (0.5239 -- 2.3110)  data: 0.1348 (0.0008 -- 1.1047)  max mem: 16413
[2023-09-04 16:16:57,584] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:16:57,584] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:16:57,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:16:57,585] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 60/160]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000011  loss: 1.5585 (1.5784)  loss_scale: 32768.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5405 (6.6704)  time: 0.8596 (0.5348 -- 2.9892)  data: 0.2067 (0.0002 -- 2.4696)  max mem: 16413
Epoch: [28]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000011  loss: 1.5924 (1.5854)  loss_scale: 32768.0000 (22654.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3971 (6.9602)  time: 0.9225 (0.5172 -- 3.2978)  data: 0.3719 (0.0002 -- 2.7653)  max mem: 16413
Epoch: [28]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000011  loss: 1.6270 (1.6037)  loss_scale: 32768.0000 (24657.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5379 (7.0778)  time: 0.7967 (0.5268 -- 1.6886)  data: 0.1381 (0.0003 -- 1.1440)  max mem: 16413
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.5697 (1.5951)  loss_scale: 32768.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0754 (7.1451)  time: 0.8011 (0.5385 -- 2.7561)  data: 0.1837 (0.0009 -- 2.1932)  max mem: 16413
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.4998 (1.5852)  loss_scale: 32768.0000 (26958.0709)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7469 (7.1955)  time: 0.9661 (0.5341 -- 3.8581)  data: 0.0541 (0.0005 -- 1.0538)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.6324 (1.5878)  loss_scale: 32768.0000 (27648.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2369 (7.1590)  time: 0.6619 (0.4958 -- 2.1508)  data: 0.0009 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [28] Total time: 0:02:21 (0.8821 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.6324 (1.5889)  loss_scale: 32768.0000 (27648.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2369 (7.1590)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1403 (0.1403)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3243 (2.3243 -- 2.3243)  data: 2.0841 (2.0841 -- 2.0841)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4483 (0.5102)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4108 (0.2047 -- 2.3243)  data: 0.1905 (0.0005 -- 2.0841)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3905 (0.4550)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.9418)  time: 0.2169 (0.1695 -- 0.3241)  data: 0.0055 (0.0001 -- 0.0960)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4084 (0.5617)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.7552)  time: 0.2011 (0.1334 -- 0.3241)  data: 0.0052 (0.0001 -- 0.0960)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 85.477 Acc@5 98.340 loss 0.571
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.14%
Epoch: [29]  [  0/160]  eta: 0:22:18  lr: 0.000045  min_lr: 0.000011  loss: 2.3080 (2.3080)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5392 (5.5392)  time: 8.3657 (8.3657 -- 8.3657)  data: 7.8366 (7.8366 -- 7.8366)  max mem: 16413
[2023-09-04 16:18:59,122] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:18:59,123] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:18:59,123] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:18:59,123] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [29]  [ 20/160]  eta: 0:02:43  lr: 0.000045  min_lr: 0.000011  loss: 1.6844 (1.6988)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0426 (7.0225)  time: 0.8061 (0.5361 -- 3.0249)  data: 0.2497 (0.0003 -- 2.4947)  max mem: 16413
[2023-09-04 16:19:02,043] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4661
[2023-09-04 16:19:02,043] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4661
[2023-09-04 16:19:02,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:19:02,043] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:19:02,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 40/160]  eta: 0:02:10  lr: 0.000045  min_lr: 0.000011  loss: 1.5090 (1.6105)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0055 (6.7009)  time: 0.9974 (0.5238 -- 3.2736)  data: 0.4456 (0.0003 -- 2.7096)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:38  lr: 0.000045  min_lr: 0.000011  loss: 1.6815 (1.6370)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6101 (6.3731)  time: 0.7856 (0.5384 -- 3.3919)  data: 0.2346 (0.0003 -- 2.8616)  max mem: 16413
[2023-09-04 16:19:41,011] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4706
[2023-09-04 16:19:41,011] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4706
[2023-09-04 16:19:41,011] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:19:41,011] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:19:41,011] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [ 80/160]  eta: 0:01:17  lr: 0.000045  min_lr: 0.000011  loss: 1.5267 (1.6142)  loss_scale: 16384.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7474 (6.5565)  time: 0.8927 (0.5058 -- 4.0258)  data: 0.3468 (0.0004 -- 3.4862)  max mem: 16413
Epoch: [29]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 1.4083 (1.5917)  loss_scale: 16384.0000 (28063.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5069 (6.5705)  time: 0.8684 (0.5272 -- 4.1085)  data: 0.3136 (0.0005 -- 3.5684)  max mem: 16413
Epoch: [29]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.5439 (1.5922)  loss_scale: 16384.0000 (26133.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3932 (6.5341)  time: 0.9034 (0.5208 -- 3.2279)  data: 0.3547 (0.0004 -- 2.6869)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.5459 (1.5842)  loss_scale: 16384.0000 (24750.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1139 (6.5402)  time: 0.7673 (0.5215 -- 2.9258)  data: 0.2214 (0.0003 -- 2.3832)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.6850 (1.5935)  loss_scale: 16384.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4582 (6.5804)  time: 0.6826 (0.4961 -- 2.5172)  data: 0.1607 (0.0002 -- 1.9749)  max mem: 16413
Epoch: [29] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.6850 (1.5766)  loss_scale: 16384.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4582 (6.5804)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1536 (0.1536)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3264 (2.3264 -- 2.3264)  data: 2.0863 (2.0863 -- 2.0863)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.2956 (0.4807)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4054 (0.1965 -- 2.3264)  data: 0.1919 (0.0004 -- 2.0863)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3426 (0.4531)  acc1: 88.8889 (90.4762)  acc5: 100.0000 (98.4127)  time: 0.2176 (0.1685 -- 0.4002)  data: 0.0126 (0.0001 -- 0.2207)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4731 (0.5530)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2007 (0.1332 -- 0.4002)  data: 0.0116 (0.0001 -- 0.2207)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 85.685 Acc@5 98.340 loss 0.571
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.14%
Epoch: [30]  [  0/160]  eta: 0:16:40  lr: 0.000045  min_lr: 0.000011  loss: 1.5104 (1.5104)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2314 (5.2314)  time: 6.2535 (6.2535 -- 6.2535)  data: 5.7234 (5.7234 -- 5.7234)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:03:04  lr: 0.000045  min_lr: 0.000011  loss: 1.4696 (1.5373)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0657 (6.3787)  time: 1.0687 (0.5056 -- 4.2839)  data: 0.3861 (0.0003 -- 3.7563)  max mem: 16413
[2023-09-04 16:21:46,363] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:21:46,364] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:21:46,364] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:21:46,364] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 40/160]  eta: 0:02:08  lr: 0.000045  min_lr: 0.000011  loss: 1.6057 (1.5382)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4504 (6.4040)  time: 0.8076 (0.5113 -- 4.5411)  data: 0.2552 (0.0004 -- 4.0347)  max mem: 16413
[2023-09-04 16:22:07,966] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4858
[2023-09-04 16:22:07,966] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4858
[2023-09-04 16:22:07,966] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:22:07,966] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:22:07,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [ 60/160]  eta: 0:01:44  lr: 0.000045  min_lr: 0.000011  loss: 1.3754 (1.5128)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5077 (6.5908)  time: 0.9898 (0.5073 -- 4.3884)  data: 0.1828 (0.0004 -- 2.5917)  max mem: 16413
Epoch: [30]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000011  loss: 1.2536 (1.4709)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2705 (6.4885)  time: 0.7745 (0.5162 -- 3.5464)  data: 0.0014 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000011  loss: 1.5383 (1.5001)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7591 (6.6368)  time: 0.8707 (0.5275 -- 4.0903)  data: 0.0389 (0.0006 -- 0.5120)  max mem: 16413
Epoch: [30]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000011  loss: 1.5863 (1.5121)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4756 (6.8000)  time: 0.9445 (0.5132 -- 3.7213)  data: 0.0016 (0.0005 -- 0.0041)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.6027 (1.5247)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6454 (6.8437)  time: 0.7204 (0.5268 -- 2.1867)  data: 0.0016 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.5888 (1.5326)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3468 (7.0207)  time: 0.8037 (0.4974 -- 2.7467)  data: 0.0011 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [30] Total time: 0:02:24 (0.9018 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.5888 (1.5470)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3468 (7.0207)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1199 (0.1199)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4913 (2.4913 -- 2.4913)  data: 2.2693 (2.2693 -- 2.2693)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3460 (0.4872)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4349 (0.2036 -- 2.4913)  data: 0.2102 (0.0007 -- 2.2693)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3543 (0.4626)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2139 (0.1698 -- 0.2587)  data: 0.0060 (0.0001 -- 0.0732)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4677 (0.5863)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.1947 (0.1333 -- 0.2587)  data: 0.0057 (0.0001 -- 0.0732)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.582
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 87.14%
Epoch: [31]  [  0/160]  eta: 0:19:38  lr: 0.000045  min_lr: 0.000011  loss: 0.9245 (0.9245)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4675 (5.4675)  time: 7.3665 (7.3665 -- 7.3665)  data: 6.8444 (6.8444 -- 6.8444)  max mem: 16413
Epoch: [31]  [ 20/160]  eta: 0:02:45  lr: 0.000045  min_lr: 0.000011  loss: 1.6230 (1.5163)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4390 (7.3932)  time: 0.8723 (0.5355 -- 2.8138)  data: 0.1235 (0.0001 -- 1.2535)  max mem: 16413
[2023-09-04 16:24:08,582] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:24:08,582] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:24:08,583] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:24:08,583] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:24:17,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=22, lr=[1.1390910354167532e-05, 1.1390910354167532e-05, 1.2656567060186148e-05, 1.2656567060186148e-05, 1.4062852289095717e-05, 1.4062852289095717e-05, 1.5625391432328577e-05, 1.5625391432328577e-05, 1.736154603592064e-05, 1.736154603592064e-05, 1.9290606706578487e-05, 1.9290606706578487e-05, 2.1434007451753873e-05, 2.1434007451753873e-05, 2.381556383528208e-05, 2.381556383528208e-05, 2.6461737594757868e-05, 2.6461737594757868e-05, 2.9401930660842074e-05, 2.9401930660842074e-05, 3.2668811845380086e-05, 3.2668811845380086e-05, 3.629867982820009e-05, 3.629867982820009e-05, 4.033186647577788e-05, 4.033186647577788e-05, 4.481318497308653e-05, 4.481318497308653e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 16:24:17,779] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=18.410488856542596, CurrSamplesPerSec=21.287857563392006, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:01:59  lr: 0.000045  min_lr: 0.000011  loss: 1.8429 (1.6213)  loss_scale: 32768.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1662 (7.0856)  time: 0.8015 (0.5298 -- 2.8683)  data: 0.1663 (0.0004 -- 2.3294)  max mem: 16413
Epoch: [31]  [ 60/160]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000011  loss: 1.5668 (1.5965)  loss_scale: 32768.0000 (25516.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9073 (7.0528)  time: 0.9220 (0.5229 -- 3.1894)  data: 0.3152 (0.0006 -- 2.6745)  max mem: 16413
Epoch: [31]  [ 80/160]  eta: 0:01:14  lr: 0.000045  min_lr: 0.000011  loss: 1.6826 (1.6006)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6034 (6.9119)  time: 0.8075 (0.5270 -- 2.7100)  data: 0.0460 (0.0005 -- 0.5276)  max mem: 16413
[2023-09-04 16:25:00,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5048
[2023-09-04 16:25:00,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5048
[2023-09-04 16:25:00,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:25:00,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:25:00,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000011  loss: 1.7612 (1.6237)  loss_scale: 16384.0000 (26279.2871)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0273 (6.7381)  time: 0.9103 (0.5377 -- 3.0065)  data: 0.2433 (0.0005 -- 2.4547)  max mem: 16413
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.4970 (1.6138)  loss_scale: 16384.0000 (24643.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7402 (6.6504)  time: 0.8313 (0.5297 -- 2.5363)  data: 0.1862 (0.0004 -- 1.9529)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.4133 (1.5956)  loss_scale: 16384.0000 (23472.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3440 (6.6794)  time: 0.9141 (0.5371 -- 2.6358)  data: 0.0972 (0.0006 -- 0.9589)  max mem: 16413
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.6275 (1.5985)  loss_scale: 16384.0000 (22630.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4356 (6.6591)  time: 0.7728 (0.4952 -- 3.3595)  data: 0.0008 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [31] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.6275 (1.6035)  loss_scale: 16384.0000 (22630.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4356 (6.6591)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1432 (0.1432)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2694 (2.2694 -- 2.2694)  data: 2.0482 (2.0482 -- 2.0482)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3281 (0.4870)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4056 (0.1919 -- 2.2694)  data: 0.1897 (0.0005 -- 2.0482)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3621 (0.4633)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.9418)  time: 0.2182 (0.1695 -- 0.4419)  data: 0.0141 (0.0001 -- 0.2391)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4849 (0.5735)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.7552)  time: 0.2024 (0.1358 -- 0.4419)  data: 0.0138 (0.0001 -- 0.2391)  max mem: 16413
Val: Total time: 0:00:07 (0.2822 s / it)
* Acc@1 85.062 Acc@5 98.340 loss 0.575
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 87.14%
Epoch: [32]  [  0/160]  eta: 0:23:32  lr: 0.000045  min_lr: 0.000011  loss: 2.0258 (2.0258)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7117 (4.7117)  time: 8.8252 (8.8252 -- 8.8252)  data: 7.8532 (7.8532 -- 7.8532)  max mem: 16413
Epoch: [32]  [ 20/160]  eta: 0:03:02  lr: 0.000045  min_lr: 0.000011  loss: 1.5346 (1.6022)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3595 (6.5529)  time: 0.9287 (0.5171 -- 4.8786)  data: 0.2055 (0.0003 -- 2.2105)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:05  lr: 0.000045  min_lr: 0.000011  loss: 1.6768 (1.6336)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9922 (6.5880)  time: 0.7692 (0.5288 -- 2.7437)  data: 0.0016 (0.0003 -- 0.0033)  max mem: 16413
[2023-09-04 16:27:06,627] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:27:06,627] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:27:06,629] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:27:06,630] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 60/160]  eta: 0:01:41  lr: 0.000045  min_lr: 0.000011  loss: 1.6258 (1.6188)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5588 (6.4972)  time: 0.9446 (0.5187 -- 4.5306)  data: 0.0754 (0.0001 -- 0.8835)  max mem: 16413
Epoch: [32]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000011  loss: 1.6348 (1.6173)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9412 (6.4958)  time: 0.7784 (0.5370 -- 2.0672)  data: 0.0927 (0.0002 -- 1.5152)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 1.7999 (1.6320)  loss_scale: 32768.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5237 (6.5295)  time: 0.9006 (0.5372 -- 3.3808)  data: 0.3529 (0.0004 -- 2.8418)  max mem: 16413
Epoch: [32]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.7156 (1.6448)  loss_scale: 32768.0000 (25049.9174)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3613 (6.5049)  time: 0.8299 (0.5266 -- 2.9469)  data: 0.2845 (0.0004 -- 2.3817)  max mem: 16413
[2023-09-04 16:28:16,560] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5258
[2023-09-04 16:28:16,560] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5258
[2023-09-04 16:28:16,560] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:28:16,560] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:28:16,560] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.4705 (1.6328)  loss_scale: 32768.0000 (25796.0851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3132 (6.5760)  time: 0.9605 (0.5353 -- 3.4555)  data: 0.4123 (0.0002 -- 2.9373)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.5564 (1.6242)  loss_scale: 16384.0000 (24678.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2600 (6.5581)  time: 0.6985 (0.4950 -- 2.4925)  data: 0.1780 (0.0002 -- 1.9899)  max mem: 16413
Epoch: [32] Total time: 0:02:24 (0.9035 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.5564 (1.5785)  loss_scale: 16384.0000 (24678.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2600 (6.5581)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1286 (0.1286)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2942 (2.2942 -- 2.2942)  data: 2.0589 (2.0589 -- 2.0589)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3780 (0.5597)  acc1: 77.7778 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4107 (0.2050 -- 2.2942)  data: 0.1898 (0.0008 -- 2.0589)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3729 (0.4926)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2222 (0.1681 -- 0.3288)  data: 0.0137 (0.0001 -- 0.1433)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4556 (0.6086)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (98.3402)  time: 0.2061 (0.1330 -- 0.3288)  data: 0.0134 (0.0001 -- 0.1433)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 82.780 Acc@5 97.718 loss 0.618
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 87.14%
Epoch: [33]  [  0/160]  eta: 0:20:40  lr: 0.000045  min_lr: 0.000011  loss: 2.2191 (2.2191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9994 (9.9994)  time: 7.7544 (7.7544 -- 7.7544)  data: 5.1916 (5.1916 -- 5.1916)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:39  lr: 0.000045  min_lr: 0.000011  loss: 1.7070 (1.6560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0269 (7.3421)  time: 0.8111 (0.5428 -- 2.3742)  data: 0.1003 (0.0007 -- 1.2533)  max mem: 16413
Epoch: [33]  [ 40/160]  eta: 0:01:58  lr: 0.000044  min_lr: 0.000011  loss: 1.5377 (1.6041)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3786 (6.7169)  time: 0.8322 (0.5281 -- 1.9308)  data: 0.1739 (0.0004 -- 1.4057)  max mem: 16413
Epoch: [33]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000011  loss: 1.7027 (1.6113)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1149 (6.6035)  time: 0.9622 (0.5231 -- 3.3312)  data: 0.3772 (0.0004 -- 2.7965)  max mem: 16413
Epoch: [33]  [ 80/160]  eta: 0:01:13  lr: 0.000044  min_lr: 0.000011  loss: 1.6168 (1.6176)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1321 (6.5337)  time: 0.7410 (0.5215 -- 2.1286)  data: 0.1376 (0.0002 -- 1.6016)  max mem: 16413
Epoch: [33]  [100/160]  eta: 0:00:55  lr: 0.000044  min_lr: 0.000011  loss: 1.5459 (1.6014)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5008 (6.5408)  time: 0.8980 (0.5266 -- 4.6958)  data: 0.3205 (0.0005 -- 4.1492)  max mem: 16413
[2023-09-04 16:30:18,372] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:30:18,372] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:30:18,373] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:30:18,373] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000011  loss: 1.6386 (1.5962)  loss_scale: 32768.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7750 (6.5110)  time: 0.8485 (0.5226 -- 3.4641)  data: 0.0015 (0.0006 -- 0.0030)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:17  lr: 0.000044  min_lr: 0.000011  loss: 1.4123 (1.5804)  loss_scale: 32768.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1068 (6.4835)  time: 0.8387 (0.5371 -- 2.6530)  data: 0.2810 (0.0005 -- 2.1174)  max mem: 16413
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.3641 (1.5581)  loss_scale: 32768.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9681 (6.5059)  time: 0.7420 (0.4952 -- 2.6795)  data: 0.2146 (0.0001 -- 2.1589)  max mem: 16413
Epoch: [33] Total time: 0:02:20 (0.8794 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.3641 (1.5407)  loss_scale: 32768.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9681 (6.5059)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1433 (0.1433)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4039 (2.4039 -- 2.4039)  data: 2.1366 (2.1366 -- 2.1366)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4721 (0.5569)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4457 (0.1991 -- 2.4039)  data: 0.2212 (0.0004 -- 2.1366)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4298 (0.4972)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2232 (0.1695 -- 0.5296)  data: 0.0154 (0.0001 -- 0.2848)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4878 (0.5866)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.7552)  time: 0.2060 (0.1325 -- 0.5296)  data: 0.0150 (0.0000 -- 0.2848)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 85.062 Acc@5 98.340 loss 0.586
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 87.14%
Epoch: [34]  [  0/160]  eta: 0:17:46  lr: 0.000044  min_lr: 0.000011  loss: 1.7178 (1.7178)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2946 (7.2946)  time: 6.6649 (6.6649 -- 6.6649)  data: 4.6111 (4.6111 -- 4.6111)  max mem: 16413
Epoch: [34]  [ 20/160]  eta: 0:02:54  lr: 0.000044  min_lr: 0.000011  loss: 1.5247 (1.5943)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8450 (7.5881)  time: 0.9733 (0.5173 -- 4.8980)  data: 0.1169 (0.0002 -- 1.5450)  max mem: 16413
Epoch: [34]  [ 40/160]  eta: 0:02:05  lr: 0.000044  min_lr: 0.000011  loss: 1.5440 (1.5962)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1392 (7.1880)  time: 0.8382 (0.5153 -- 3.7008)  data: 0.0150 (0.0004 -- 0.2769)  max mem: 16413
Epoch: [34]  [ 60/160]  eta: 0:01:35  lr: 0.000044  min_lr: 0.000011  loss: 1.6804 (1.5803)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2141 (7.1259)  time: 0.7587 (0.5268 -- 1.7408)  data: 0.0603 (0.0004 -- 0.9613)  max mem: 16413
[2023-09-04 16:32:22,207] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:32:22,207] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:32:22,209] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:32:22,210] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:32:22,742] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5516
[2023-09-04 16:32:22,742] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5516
[2023-09-04 16:32:22,742] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:32:22,742] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:32:22,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 16:32:23,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5517
[2023-09-04 16:32:23,265] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5517
[2023-09-04 16:32:23,265] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:32:23,265] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:32:23,265] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.5741 (1.5854)  loss_scale: 32768.0000 (32363.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7504 (6.9377)  time: 0.9663 (0.5218 -- 3.5501)  data: 0.1616 (0.0004 -- 2.9906)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 1.4657 (1.5753)  loss_scale: 16384.0000 (29199.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6279 (6.9460)  time: 0.9342 (0.5301 -- 4.1982)  data: 0.3866 (0.0002 -- 3.6823)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.3759 (1.5423)  loss_scale: 16384.0000 (27080.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0938 (6.8307)  time: 0.8431 (0.5254 -- 2.5983)  data: 0.2957 (0.0002 -- 2.0810)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.5524 (1.5335)  loss_scale: 16384.0000 (25563.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7154 (7.1200)  time: 0.9290 (0.5315 -- 3.6354)  data: 0.3759 (0.0002 -- 3.1030)  max mem: 16413
[2023-09-04 16:33:26,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5593
[2023-09-04 16:33:26,033] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 16:33:26,033] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5593
[2023-09-04 16:33:26,033] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 16:33:26,034] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6229 (1.5378)  loss_scale: 16384.0000 (24115.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8039 (7.1306)  time: 0.6899 (0.4920 -- 3.6354)  data: 0.1738 (0.0002 -- 3.1030)  max mem: 16413
Epoch: [34] Total time: 0:02:21 (0.8857 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6229 (1.5581)  loss_scale: 16384.0000 (24115.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8039 (7.1306)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1342 (0.1342)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4179 (2.4179 -- 2.4179)  data: 2.1628 (2.1628 -- 2.1628)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5577 (0.5423)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (97.9798)  time: 0.4237 (0.2118 -- 2.4179)  data: 0.1979 (0.0008 -- 2.1628)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3680 (0.4828)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2188 (0.1700 -- 0.3339)  data: 0.0085 (0.0001 -- 0.1516)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4708 (0.5799)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.1996 (0.1329 -- 0.3339)  data: 0.0080 (0.0001 -- 0.1516)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.602
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.14%
Epoch: [35]  [  0/160]  eta: 0:21:31  lr: 0.000044  min_lr: 0.000011  loss: 1.0654 (1.0654)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3001 (5.3001)  time: 8.0746 (8.0746 -- 8.0746)  data: 7.5317 (7.5317 -- 7.5317)  max mem: 16413
Epoch: [35]  [ 20/160]  eta: 0:02:38  lr: 0.000044  min_lr: 0.000011  loss: 1.4079 (1.4587)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4224 (6.9990)  time: 0.7856 (0.5304 -- 3.6860)  data: 0.2112 (0.0001 -- 2.6918)  max mem: 16413
Epoch: [35]  [ 40/160]  eta: 0:02:07  lr: 0.000044  min_lr: 0.000011  loss: 1.6386 (1.5528)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8574 (7.2739)  time: 0.9919 (0.5300 -- 3.4264)  data: 0.3071 (0.0005 -- 2.9079)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:40  lr: 0.000044  min_lr: 0.000011  loss: 1.6183 (1.5666)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2374 (6.8918)  time: 0.8747 (0.5089 -- 3.2714)  data: 0.1380 (0.0004 -- 2.7104)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.4433 (1.5615)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9182 (6.9615)  time: 0.8219 (0.5243 -- 2.6155)  data: 0.0017 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [35]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 1.6638 (1.5731)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7285 (7.0131)  time: 0.9585 (0.5337 -- 3.1829)  data: 0.0014 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.5156 (1.5709)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2451 (7.0396)  time: 0.8587 (0.5266 -- 4.0943)  data: 0.0020 (0.0006 -- 0.0064)  max mem: 16413
[2023-09-04 16:35:32,123] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:35:32,123] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:35:32,124] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 16:35:32,124] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [35]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.5641 (1.5778)  loss_scale: 16384.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3278 (7.0309)  time: 0.8794 (0.5121 -- 3.4233)  data: 0.0021 (0.0003 -- 0.0161)  max mem: 16413
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6192 (1.5872)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0680 (6.9966)  time: 0.5765 (0.4960 -- 1.4940)  data: 0.0009 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [35] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6192 (1.5687)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0680 (6.9966)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1479 (0.1479)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4082 (2.4082 -- 2.4082)  data: 2.1642 (2.1642 -- 2.1642)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5505 (0.5112)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4180 (0.1999 -- 2.4082)  data: 0.2056 (0.0009 -- 2.1642)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4853 (0.4933)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (99.4709)  time: 0.2164 (0.1705 -- 0.3042)  data: 0.0075 (0.0001 -- 0.0873)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4947 (0.5828)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (99.1701)  time: 0.2025 (0.1322 -- 0.3042)  data: 0.0072 (0.0001 -- 0.0873)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 85.270 Acc@5 98.548 loss 0.600
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.14%
Epoch: [36]  [  0/160]  eta: 0:20:04  lr: 0.000044  min_lr: 0.000011  loss: 1.5494 (1.5494)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0509 (7.0509)  time: 7.5310 (7.5310 -- 7.5310)  data: 7.0071 (7.0071 -- 7.0071)  max mem: 16413
Epoch: [36]  [ 20/160]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000011  loss: 1.4244 (1.4631)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0755 (7.1060)  time: 0.8232 (0.5161 -- 3.1842)  data: 0.2386 (0.0003 -- 2.6478)  max mem: 16413
Epoch: [36]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6495 (1.5241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8100 (6.7637)  time: 0.8581 (0.5258 -- 4.2299)  data: 0.3080 (0.0009 -- 3.7150)  max mem: 16413
Epoch: [36]  [ 60/160]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000011  loss: 1.6152 (1.5391)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1935 (6.8194)  time: 0.9714 (0.5235 -- 4.1404)  data: 0.4216 (0.0005 -- 3.6131)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000011  loss: 1.6774 (1.5710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0963 (6.8425)  time: 0.8038 (0.5227 -- 3.1961)  data: 0.2490 (0.0002 -- 2.6679)  max mem: 16413
[2023-09-04 16:37:32,651] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:37:32,652] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:37:32,654] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:37:32,654] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [100/160]  eta: 0:00:55  lr: 0.000044  min_lr: 0.000011  loss: 1.6694 (1.5901)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7535 (6.9696)  time: 0.8750 (0.5230 -- 3.4646)  data: 0.2875 (0.0005 -- 2.3322)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000011  loss: 1.7503 (1.6237)  loss_scale: 32768.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0526 (7.0635)  time: 0.8746 (0.5117 -- 3.7189)  data: 0.3267 (0.0003 -- 3.1767)  max mem: 16413
[2023-09-04 16:38:11,394] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5893
[2023-09-04 16:38:11,394] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5893
[2023-09-04 16:38:11,395] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:38:11,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 16:38:11,395] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.2138 (1.5998)  loss_scale: 32768.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6428 (7.1093)  time: 0.9291 (0.5291 -- 3.5955)  data: 0.3762 (0.0003 -- 2.9949)  max mem: 16413
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8408 (1.6184)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3504 (7.1319)  time: 0.6265 (0.4959 -- 2.5086)  data: 0.0986 (0.0002 -- 1.9566)  max mem: 16413
Epoch: [36] Total time: 0:02:22 (0.8889 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8408 (1.5816)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3504 (7.1319)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1757 (0.1757)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5142 (2.5142 -- 2.5142)  data: 2.2222 (2.2222 -- 2.2222)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6730 (0.5927)  acc1: 77.7778 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4284 (0.1983 -- 2.5142)  data: 0.2041 (0.0007 -- 2.2222)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3992 (0.5243)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2108 (0.1683 -- 0.2512)  data: 0.0026 (0.0001 -- 0.0260)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4321 (0.6302)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.5104)  time: 0.1955 (0.1328 -- 0.2512)  data: 0.0016 (0.0001 -- 0.0260)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 83.195 Acc@5 97.510 loss 0.642
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 87.14%
Epoch: [37]  [  0/160]  eta: 0:17:58  lr: 0.000044  min_lr: 0.000011  loss: 1.8840 (1.8840)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2218 (9.2218)  time: 6.7375 (6.7375 -- 6.7375)  data: 6.1515 (6.1515 -- 6.1515)  max mem: 16413
Epoch: [37]  [ 20/160]  eta: 0:02:53  lr: 0.000044  min_lr: 0.000011  loss: 1.6317 (1.6522)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1847 (7.1684)  time: 0.9637 (0.5229 -- 3.6641)  data: 0.3470 (0.0004 -- 3.1480)  max mem: 16413
Epoch: [37]  [ 40/160]  eta: 0:02:01  lr: 0.000044  min_lr: 0.000011  loss: 1.5556 (1.6227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8919 (6.9784)  time: 0.7724 (0.5400 -- 2.5388)  data: 0.2192 (0.0003 -- 2.0291)  max mem: 16413
Epoch: [37]  [ 60/160]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000011  loss: 1.6537 (1.6396)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6517 (6.8950)  time: 1.0205 (0.5222 -- 3.8913)  data: 0.4766 (0.0005 -- 3.3617)  max mem: 16413
[2023-09-04 16:39:55,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=28, lr=[1.1117940440317657e-05, 1.1117940440317657e-05, 1.2353267155908508e-05, 1.2353267155908508e-05, 1.3725852395453895e-05, 1.3725852395453895e-05, 1.5250947106059886e-05, 1.5250947106059886e-05, 1.6945496784510984e-05, 1.6945496784510984e-05, 1.882832976056776e-05, 1.882832976056776e-05, 2.092036640063084e-05, 2.092036640063084e-05, 2.324485155625649e-05, 2.324485155625649e-05, 2.5827612840284987e-05, 2.5827612840284987e-05, 2.869734760031665e-05, 2.869734760031665e-05, 3.1885941778129613e-05, 3.1885941778129613e-05, 3.542882419792179e-05, 3.542882419792179e-05, 3.93653602199131e-05, 3.93653602199131e-05, 4.373928913323678e-05, 4.373928913323678e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 16:39:55,917] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=18.452857439858683, CurrSamplesPerSec=22.223793737449025, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:18  lr: 0.000044  min_lr: 0.000011  loss: 1.5205 (1.6169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5706 (6.8260)  time: 0.8686 (0.5166 -- 4.5151)  data: 0.3273 (0.0002 -- 4.0001)  max mem: 16413
Epoch: [37]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 1.6209 (1.6294)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2760 (6.9319)  time: 0.9166 (0.5132 -- 3.7552)  data: 0.3666 (0.0003 -- 3.1969)  max mem: 16413
[2023-09-04 16:40:15,931] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:40:15,931] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:40:15,931] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:40:15,931] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.5646 (1.6045)  loss_scale: 32768.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3706 (6.8393)  time: 0.8125 (0.5331 -- 3.6971)  data: 0.2634 (0.0004 -- 3.1716)  max mem: 16413
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.5366 (1.5849)  loss_scale: 32768.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6343 (6.8172)  time: 0.8760 (0.5258 -- 2.5444)  data: 0.3221 (0.0003 -- 2.0298)  max mem: 16413
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6558 (1.5920)  loss_scale: 32768.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2033 (6.9799)  time: 0.6749 (0.4977 -- 2.3969)  data: 0.1486 (0.0002 -- 1.8598)  max mem: 16413
Epoch: [37] Total time: 0:02:24 (0.9017 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6558 (1.5942)  loss_scale: 32768.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2033 (6.9799)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1970 (0.1970)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3706 (2.3706 -- 2.3706)  data: 2.1281 (2.1281 -- 2.1281)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3548 (0.4609)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (97.9798)  time: 0.4252 (0.2040 -- 2.3706)  data: 0.2079 (0.0005 -- 2.1281)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4679 (0.4938)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (98.4127)  time: 0.2205 (0.1688 -- 0.3562)  data: 0.0168 (0.0001 -- 0.1736)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5532 (0.5995)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2041 (0.1336 -- 0.3562)  data: 0.0165 (0.0001 -- 0.1736)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 85.892 Acc@5 98.340 loss 0.623
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.14%
Epoch: [38]  [  0/160]  eta: 0:15:09  lr: 0.000044  min_lr: 0.000011  loss: 1.7172 (1.7172)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6545 (3.6545)  time: 5.6858 (5.6858 -- 5.6858)  data: 5.1282 (5.1282 -- 5.1282)  max mem: 16413
Epoch: [38]  [ 20/160]  eta: 0:02:37  lr: 0.000044  min_lr: 0.000011  loss: 1.5791 (1.5906)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7168 (6.8567)  time: 0.8956 (0.5351 -- 2.2899)  data: 0.2770 (0.0008 -- 1.5744)  max mem: 16413
Epoch: [38]  [ 40/160]  eta: 0:02:02  lr: 0.000044  min_lr: 0.000011  loss: 1.6115 (1.5802)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3845 (7.2123)  time: 0.9121 (0.5201 -- 2.5836)  data: 0.3598 (0.0004 -- 2.0499)  max mem: 16413
Epoch: [38]  [ 60/160]  eta: 0:01:40  lr: 0.000044  min_lr: 0.000011  loss: 1.5600 (1.5506)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9600 (7.0911)  time: 0.9652 (0.5325 -- 3.7180)  data: 0.2439 (0.0003 -- 3.1886)  max mem: 16413
[2023-09-04 16:42:18,298] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:42:18,298] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:42:18,298] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:42:18,298] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:42:19,777] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6152
[2023-09-04 16:42:19,777] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:42:19,777] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6152
[2023-09-04 16:42:19,777] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:42:19,778] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.4920 (1.5590)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0393 (6.9989)  time: 0.8356 (0.5348 -- 3.4640)  data: 0.0515 (0.0004 -- 0.6074)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 1.7498 (1.5887)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5804 (6.9703)  time: 0.8922 (0.5234 -- 3.2163)  data: 0.0016 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000011  loss: 1.6703 (1.6042)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8924 (7.0371)  time: 0.8656 (0.5116 -- 3.4151)  data: 0.2156 (0.0005 -- 2.0421)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.4940 (1.5921)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1988 (7.1576)  time: 0.8102 (0.5403 -- 3.2469)  data: 0.2542 (0.0003 -- 2.7095)  max mem: 16413
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.6163 (1.5947)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3215 (7.0935)  time: 0.6533 (0.4967 -- 1.6271)  data: 0.1270 (0.0002 -- 1.1149)  max mem: 16413
Epoch: [38] Total time: 0:02:21 (0.8861 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.6163 (1.5790)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3215 (7.0935)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1426 (0.1426)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4191 (2.4191 -- 2.4191)  data: 2.1992 (2.1992 -- 2.1992)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1924 (0.4295)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4134 (0.1882 -- 2.4191)  data: 0.2028 (0.0008 -- 2.1992)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4139 (0.4550)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2156 (0.1699 -- 0.3106)  data: 0.0103 (0.0001 -- 0.1277)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4719 (0.5494)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2025 (0.1329 -- 0.3106)  data: 0.0100 (0.0001 -- 0.1277)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.14%
Epoch: [39]  [  0/160]  eta: 0:17:18  lr: 0.000043  min_lr: 0.000011  loss: 1.6983 (1.6983)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2723 (9.2723)  time: 6.4882 (6.4882 -- 6.4882)  data: 4.2445 (4.2445 -- 4.2445)  max mem: 16413
Epoch: [39]  [ 20/160]  eta: 0:02:37  lr: 0.000043  min_lr: 0.000011  loss: 1.6372 (1.6191)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0785 (6.7839)  time: 0.8549 (0.5299 -- 3.0896)  data: 0.1644 (0.0002 -- 2.5424)  max mem: 16413
Epoch: [39]  [ 40/160]  eta: 0:02:00  lr: 0.000043  min_lr: 0.000011  loss: 1.5542 (1.6179)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2974 (6.7036)  time: 0.8756 (0.5229 -- 3.8270)  data: 0.3255 (0.0005 -- 3.2901)  max mem: 16413
[2023-09-04 16:44:22,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:44:22,539] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:44:22,540] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:44:22,540] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:44:26,260] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6287
[2023-09-04 16:44:26,261] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6287
[2023-09-04 16:44:26,261] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:44:26,261] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:44:26,261] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 60/160]  eta: 0:01:37  lr: 0.000043  min_lr: 0.000011  loss: 1.7035 (1.6377)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8829 (6.7391)  time: 0.9062 (0.5154 -- 2.6350)  data: 0.3609 (0.0004 -- 2.0585)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000011  loss: 1.7177 (1.6499)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1470 (6.8038)  time: 0.8554 (0.5400 -- 4.7987)  data: 0.2973 (0.0002 -- 4.2745)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.6136 (1.6395)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4437 (6.8382)  time: 0.9203 (0.5317 -- 3.0656)  data: 0.3691 (0.0006 -- 2.5059)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.5580 (1.6273)  loss_scale: 32768.0000 (34392.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5941 (6.9394)  time: 0.7858 (0.5317 -- 2.6378)  data: 0.2344 (0.0001 -- 2.1049)  max mem: 16413
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.5227 (1.6284)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2239 (6.9360)  time: 1.0188 (0.5184 -- 5.0031)  data: 0.4708 (0.0003 -- 4.4373)  max mem: 16413
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.4911 (1.6104)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5135 (6.9182)  time: 0.6203 (0.4954 -- 1.7584)  data: 0.0964 (0.0002 -- 1.2317)  max mem: 16413
Epoch: [39] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.4911 (1.5986)  loss_scale: 32768.0000 (33996.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5135 (6.9182)
[2023-09-04 16:46:01,501] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-09-04 16:46:01,503] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-09-04 16:46:01,503] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-09-04 16:46:01,503] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-09-04 16:46:02,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-09-04 16:46:02,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1508 (0.1508)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2163 (2.2163 -- 2.2163)  data: 2.0085 (2.0085 -- 2.0085)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4222 (0.4983)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4142 (0.1908 -- 2.2163)  data: 0.1928 (0.0003 -- 2.0085)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4222 (0.4987)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (97.8836)  time: 0.2288 (0.1690 -- 0.3476)  data: 0.0171 (0.0001 -- 0.1407)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5369 (0.5665)  acc1: 77.7778 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2123 (0.1326 -- 0.3476)  data: 0.0161 (0.0001 -- 0.1407)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.587
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 87.14%
Epoch: [40]  [  0/160]  eta: 0:15:46  lr: 0.000043  min_lr: 0.000011  loss: 0.8165 (0.8165)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8068 (7.8068)  time: 5.9170 (5.9170 -- 5.9170)  data: 4.8298 (4.8298 -- 4.8298)  max mem: 16413
[2023-09-04 16:46:30,871] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:46:30,871] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:46:30,871] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:46:30,871] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:46:33,270] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6420
[2023-09-04 16:46:33,270] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6420
[2023-09-04 16:46:33,270] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:46:33,270] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:46:33,271] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [40]  [ 20/160]  eta: 0:02:32  lr: 0.000043  min_lr: 0.000011  loss: 1.6069 (1.6178)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1907 (7.3432)  time: 0.8485 (0.5118 -- 3.1789)  data: 0.1567 (0.0004 -- 1.5508)  max mem: 16413
[2023-09-04 16:46:33,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6421
[2023-09-04 16:46:33,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:46:33,825] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6421
[2023-09-04 16:46:33,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:46:33,867] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [40]  [ 40/160]  eta: 0:01:57  lr: 0.000043  min_lr: 0.000011  loss: 1.6168 (1.6291)  loss_scale: 16384.0000 (27972.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1907 (6.9111)  time: 0.8711 (0.5311 -- 2.5792)  data: 0.1832 (0.0003 -- 1.1304)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.4296 (1.5700)  loss_scale: 16384.0000 (24173.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0734 (6.7769)  time: 0.9930 (0.5203 -- 3.0157)  data: 0.0567 (0.0003 -- 1.0913)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000011  loss: 1.3925 (1.5396)  loss_scale: 16384.0000 (22249.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8957 (6.6662)  time: 0.7514 (0.5243 -- 1.8070)  data: 0.0372 (0.0001 -- 0.4337)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.5883 (1.5434)  loss_scale: 16384.0000 (21088.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3222 (6.8214)  time: 0.9864 (0.5234 -- 3.4569)  data: 0.0047 (0.0006 -- 0.0631)  max mem: 16413
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.6096 (1.5559)  loss_scale: 16384.0000 (20310.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2133 (6.9013)  time: 0.7963 (0.5199 -- 3.4936)  data: 0.0017 (0.0007 -- 0.0036)  max mem: 16413
[2023-09-04 16:48:15,300] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6535
[2023-09-04 16:48:15,300] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 16:48:15,300] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6535
[2023-09-04 16:48:15,301] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 16:48:15,301] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [40]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.6306 (1.5703)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2323 (6.8780)  time: 0.9076 (0.5183 -- 3.0928)  data: 0.0016 (0.0003 -- 0.0063)  max mem: 16413
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.3176 (1.5533)  loss_scale: 8192.0000 (18073.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5132 (6.8531)  time: 0.7241 (0.4944 -- 2.5366)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [40] Total time: 0:02:21 (0.8854 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.3176 (1.5609)  loss_scale: 8192.0000 (18073.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5132 (6.8531)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1477 (0.1477)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2104 (2.2104 -- 2.2104)  data: 1.9723 (1.9723 -- 1.9723)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5046 (0.5666)  acc1: 77.7778 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4028 (0.1951 -- 2.2104)  data: 0.1820 (0.0004 -- 1.9723)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5046 (0.5387)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2222 (0.1745 -- 0.3736)  data: 0.0110 (0.0001 -- 0.1517)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5370 (0.5945)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2046 (0.1326 -- 0.3736)  data: 0.0107 (0.0001 -- 0.1517)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 84.647 Acc@5 97.510 loss 0.607
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 87.14%
Epoch: [41]  [  0/160]  eta: 0:21:12  lr: 0.000043  min_lr: 0.000011  loss: 1.8048 (1.8048)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4871 (4.4871)  time: 7.9526 (7.9526 -- 7.9526)  data: 7.4210 (7.4210 -- 7.4210)  max mem: 16413
Epoch: [41]  [ 20/160]  eta: 0:02:43  lr: 0.000043  min_lr: 0.000011  loss: 1.5219 (1.6002)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2284 (6.3428)  time: 0.8264 (0.5244 -- 3.2371)  data: 0.1081 (0.0004 -- 1.0786)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:05  lr: 0.000043  min_lr: 0.000011  loss: 1.5981 (1.6153)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6382 (6.5910)  time: 0.9117 (0.5171 -- 2.9512)  data: 0.3680 (0.0002 -- 2.4000)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:36  lr: 0.000043  min_lr: 0.000011  loss: 1.4873 (1.5907)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2364 (6.5531)  time: 0.8175 (0.5337 -- 2.5221)  data: 0.2662 (0.0004 -- 1.9551)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000011  loss: 1.5856 (1.5884)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1771 (6.5982)  time: 0.8272 (0.5381 -- 2.2249)  data: 0.2232 (0.0005 -- 1.6945)  max mem: 16413
Epoch: [41]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.5333 (1.5790)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4669 (6.7157)  time: 0.9560 (0.5217 -- 4.5285)  data: 0.4134 (0.0003 -- 3.9827)  max mem: 16413
[2023-09-04 16:50:16,800] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:50:16,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 16:50:16,801] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:50:16,801] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [41]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.4843 (1.5564)  loss_scale: 16384.0000 (9342.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7555 (6.7312)  time: 0.7945 (0.5157 -- 3.7290)  data: 0.2422 (0.0003 -- 3.1904)  max mem: 16413
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.4565 (1.5510)  loss_scale: 16384.0000 (10341.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2618 (6.7875)  time: 0.8802 (0.5401 -- 2.9770)  data: 0.2916 (0.0002 -- 2.4597)  max mem: 16413
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.6779 (1.5671)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3102 (6.7467)  time: 0.7244 (0.4941 -- 3.9863)  data: 0.1951 (0.0002 -- 3.4643)  max mem: 16413
Epoch: [41] Total time: 0:02:22 (0.8886 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.6779 (1.5679)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3102 (6.7467)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1199 (0.1199)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4612 (2.4612 -- 2.4612)  data: 2.2261 (2.2261 -- 2.2261)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5477 (0.5387)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4158 (0.1961 -- 2.4612)  data: 0.2032 (0.0007 -- 2.2261)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4550 (0.5331)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2166 (0.1687 -- 0.5053)  data: 0.0169 (0.0001 -- 0.3251)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4980 (0.5799)  acc1: 77.7778 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2022 (0.1326 -- 0.5053)  data: 0.0166 (0.0001 -- 0.3251)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 84.025 Acc@5 97.925 loss 0.589
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 87.14%
Epoch: [42]  [  0/160]  eta: 0:21:17  lr: 0.000043  min_lr: 0.000011  loss: 1.8576 (1.8576)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1452 (8.1452)  time: 7.9860 (7.9860 -- 7.9860)  data: 6.2966 (6.2966 -- 6.2966)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:39  lr: 0.000043  min_lr: 0.000011  loss: 1.5427 (1.6805)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7332 (8.1170)  time: 0.7963 (0.5277 -- 2.9050)  data: 0.0583 (0.0003 -- 0.7337)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:03  lr: 0.000043  min_lr: 0.000011  loss: 1.5364 (1.5774)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3126 (7.7973)  time: 0.9062 (0.5264 -- 3.4906)  data: 0.0771 (0.0003 -- 0.8131)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.4873 (1.5759)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6930 (7.4253)  time: 0.9150 (0.5366 -- 3.0730)  data: 0.1653 (0.0006 -- 1.6532)  max mem: 16413
[2023-09-04 16:52:18,580] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:52:18,580] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:52:18,581] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:52:18,581] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [42]  [ 80/160]  eta: 0:01:16  lr: 0.000043  min_lr: 0.000011  loss: 1.6505 (1.5841)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2802 (7.1607)  time: 0.8356 (0.5269 -- 2.6598)  data: 0.2809 (0.0003 -- 2.1394)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.5473 (1.5663)  loss_scale: 32768.0000 (21088.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1919 (6.9762)  time: 0.9215 (0.5276 -- 2.4705)  data: 0.3703 (0.0005 -- 1.9495)  max mem: 16413
Epoch: [42]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.4844 (1.5489)  loss_scale: 32768.0000 (23018.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1679 (6.8564)  time: 0.6762 (0.5399 -- 1.7835)  data: 0.1170 (0.0003 -- 1.2170)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.3806 (1.5342)  loss_scale: 32768.0000 (24401.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3573 (6.8333)  time: 1.0636 (0.5343 -- 3.3993)  data: 0.4738 (0.0004 -- 2.7927)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.4482 (1.5290)  loss_scale: 32768.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3369 (6.7833)  time: 0.7293 (0.4939 -- 3.6584)  data: 0.2124 (0.0002 -- 3.1504)  max mem: 16413
Epoch: [42] Total time: 0:02:24 (0.9020 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.4482 (1.5585)  loss_scale: 32768.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3369 (6.7833)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1365 (0.1365)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3974 (2.3974 -- 2.3974)  data: 2.1988 (2.1988 -- 2.1988)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4862 (0.5500)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4277 (0.1988 -- 2.3974)  data: 0.2139 (0.0005 -- 2.1988)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4200 (0.5276)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2156 (0.1688 -- 0.3814)  data: 0.0081 (0.0001 -- 0.1408)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4200 (0.5592)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.2026 (0.1323 -- 0.3814)  data: 0.0077 (0.0001 -- 0.1408)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 85.892 Acc@5 98.133 loss 0.587
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.14%
Epoch: [43]  [  0/160]  eta: 0:16:22  lr: 0.000043  min_lr: 0.000011  loss: 1.7018 (1.7018)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9853 (7.9853)  time: 6.1412 (6.1412 -- 6.1412)  data: 5.1293 (5.1293 -- 5.1293)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:46  lr: 0.000043  min_lr: 0.000011  loss: 1.6646 (1.6059)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3269 (6.6267)  time: 0.9383 (0.5225 -- 2.5843)  data: 0.2415 (0.0004 -- 2.0525)  max mem: 16413
[2023-09-04 16:54:23,124] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:54:23,125] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:54:23,125] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:54:23,125] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [43]  [ 40/160]  eta: 0:02:00  lr: 0.000043  min_lr: 0.000011  loss: 1.3849 (1.5323)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0324 (6.4169)  time: 0.8165 (0.5105 -- 3.8063)  data: 0.1039 (0.0007 -- 1.0719)  max mem: 16413
[2023-09-04 16:54:30,426] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6926
[2023-09-04 16:54:30,426] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:54:30,426] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6926
[2023-09-04 16:54:30,426] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 16:54:30,426] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [43]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.5266 (1.5457)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6386 (6.7679)  time: 0.9490 (0.5187 -- 4.5210)  data: 0.1144 (0.0005 -- 1.8008)  max mem: 16413
[2023-09-04 16:54:48,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6947
[2023-09-04 16:54:48,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6947
[2023-09-04 16:54:48,506] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:54:48,506] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:54:48,506] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [43]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000011  loss: 1.5780 (1.5340)  loss_scale: 16384.0000 (32363.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9850 (7.1018)  time: 0.8058 (0.5363 -- 2.7034)  data: 0.1013 (0.0002 -- 0.9159)  max mem: 16413
Epoch: [43]  [100/160]  eta: 0:00:55  lr: 0.000042  min_lr: 0.000011  loss: 1.6433 (1.5511)  loss_scale: 16384.0000 (29199.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3972 (7.0802)  time: 0.8414 (0.5285 -- 2.6284)  data: 0.2763 (0.0004 -- 2.1005)  max mem: 16413
[2023-09-04 16:55:31,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=35, lr=[1.079275389402428e-05, 1.079275389402428e-05, 1.199194877113809e-05, 1.199194877113809e-05, 1.3324387523486763e-05, 1.3324387523486763e-05, 1.4804875026096405e-05, 1.4804875026096405e-05, 1.6449861140107117e-05, 1.6449861140107117e-05, 1.8277623489007908e-05, 1.8277623489007908e-05, 2.0308470543342117e-05, 2.0308470543342117e-05, 2.2564967270380128e-05, 2.2564967270380128e-05, 2.5072185855977923e-05, 2.5072185855977923e-05, 2.785798428441991e-05, 2.785798428441991e-05, 3.095331587157768e-05, 3.095331587157768e-05, 3.439257319064187e-05, 3.439257319064187e-05, 3.8213970211824294e-05, 3.8213970211824294e-05, 4.245996690202699e-05, 4.245996690202699e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 16:55:31,856] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=18.518811859779902, CurrSamplesPerSec=23.342624936694538, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.3447 (1.5281)  loss_scale: 16384.0000 (27080.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6890 (7.1296)  time: 0.8678 (0.5294 -- 3.5318)  data: 0.3148 (0.0004 -- 2.9996)  max mem: 16413
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.5993 (1.5263)  loss_scale: 16384.0000 (25563.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6251 (7.1152)  time: 0.9370 (0.5229 -- 3.4107)  data: 0.3660 (0.0004 -- 2.8751)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.5299 (1.5237)  loss_scale: 16384.0000 (24473.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6402 (7.1215)  time: 0.6796 (0.4940 -- 2.2319)  data: 0.1541 (0.0002 -- 1.6979)  max mem: 16413
Epoch: [43] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.5299 (1.5521)  loss_scale: 16384.0000 (24473.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6402 (7.1215)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1366 (0.1366)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5368 (2.5368 -- 2.5368)  data: 2.3150 (2.3150 -- 2.3150)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5129 (0.5987)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4700 (0.2068 -- 2.5368)  data: 0.2507 (0.0006 -- 2.3150)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4316 (0.5407)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.9418)  time: 0.2246 (0.1686 -- 0.6529)  data: 0.0223 (0.0001 -- 0.4197)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5115 (0.6121)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (98.3402)  time: 0.2082 (0.1323 -- 0.6529)  data: 0.0220 (0.0001 -- 0.4197)  max mem: 16413
Val: Total time: 0:00:08 (0.2967 s / it)
* Acc@1 83.610 Acc@5 97.718 loss 0.627
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 87.14%
Epoch: [44]  [  0/160]  eta: 0:21:48  lr: 0.000042  min_lr: 0.000011  loss: 1.4708 (1.4708)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 24.7125 (24.7125)  time: 8.1799 (8.1799 -- 8.1799)  data: 7.6612 (7.6612 -- 7.6612)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:02:47  lr: 0.000042  min_lr: 0.000011  loss: 1.6389 (1.5441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1052 (7.1661)  time: 0.8453 (0.5295 -- 3.2013)  data: 0.2784 (0.0004 -- 2.6769)  max mem: 16413
[2023-09-04 16:56:53,177] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:56:53,178] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 16:56:53,181] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:56:53,181] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [ 40/160]  eta: 0:02:06  lr: 0.000042  min_lr: 0.000011  loss: 1.5000 (1.5176)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5871 (6.7894)  time: 0.9003 (0.5158 -- 3.0053)  data: 0.3557 (0.0004 -- 2.4864)  max mem: 16413
Epoch: [44]  [ 60/160]  eta: 0:01:40  lr: 0.000042  min_lr: 0.000011  loss: 1.5069 (1.5522)  loss_scale: 32768.0000 (23098.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8178 (6.6417)  time: 0.9003 (0.5310 -- 4.2319)  data: 0.3468 (0.0005 -- 3.7275)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000011  loss: 1.4426 (1.5391)  loss_scale: 32768.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5078 (6.6635)  time: 0.8413 (0.5243 -- 4.2401)  data: 0.2902 (0.0006 -- 3.6853)  max mem: 16413
Epoch: [44]  [100/160]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000011  loss: 1.5004 (1.5247)  loss_scale: 32768.0000 (26928.1584)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5635 (6.5760)  time: 0.9476 (0.5259 -- 3.9552)  data: 0.3654 (0.0006 -- 3.4194)  max mem: 16413
Epoch: [44]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000011  loss: 1.6338 (1.5379)  loss_scale: 32768.0000 (27893.4215)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3221 (6.5763)  time: 0.7786 (0.5276 -- 3.1166)  data: 0.0026 (0.0006 -- 0.0174)  max mem: 16413
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.5263 (1.5447)  loss_scale: 32768.0000 (28584.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9189 (6.6757)  time: 1.0454 (0.5162 -- 5.5707)  data: 0.0009 (0.0004 -- 0.0022)  max mem: 16413
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.6047 (1.5518)  loss_scale: 32768.0000 (29081.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9273 (6.7501)  time: 0.5714 (0.4943 -- 1.0630)  data: 0.0006 (0.0001 -- 0.0014)  max mem: 16413
Epoch: [44] Total time: 0:02:24 (0.9017 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.6047 (1.5777)  loss_scale: 32768.0000 (29081.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9273 (6.7501)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1529 (0.1529)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3408 (2.3408 -- 2.3408)  data: 2.1070 (2.1070 -- 2.1070)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3897 (0.5525)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.9798)  time: 0.4109 (0.1971 -- 2.3408)  data: 0.1932 (0.0004 -- 2.1070)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4076 (0.5130)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2214 (0.1713 -- 0.3659)  data: 0.0141 (0.0001 -- 0.1609)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4440 (0.5657)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.3402)  time: 0.2044 (0.1322 -- 0.3659)  data: 0.0138 (0.0001 -- 0.1609)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 86.515 Acc@5 98.133 loss 0.594
Accuracy of the network on the 482 val images: 86.51%
Max accuracy: 87.14%
Epoch: [45]  [  0/160]  eta: 0:19:33  lr: 0.000042  min_lr: 0.000011  loss: 1.6572 (1.6572)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3676 (6.3676)  time: 7.3323 (7.3323 -- 7.3323)  data: 5.3430 (5.3430 -- 5.3430)  max mem: 16413
[2023-09-04 16:58:53,891] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:58:53,891] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 16:58:53,891] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 16:58:53,891] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [45]  [ 20/160]  eta: 0:02:36  lr: 0.000042  min_lr: 0.000011  loss: 1.6005 (1.6186)  loss_scale: 65536.0000 (59294.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3706 (6.8491)  time: 0.8068 (0.5257 -- 2.3295)  data: 0.1150 (0.0008 -- 1.1259)  max mem: 16413
[2023-09-04 16:59:09,974] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7224
[2023-09-04 16:59:09,975] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 16:59:09,975] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 16:59:09,975] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7224
[2023-09-04 16:59:09,975] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [45]  [ 40/160]  eta: 0:02:00  lr: 0.000042  min_lr: 0.000011  loss: 1.6127 (1.6026)  loss_scale: 32768.0000 (48752.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0684 (6.9929)  time: 0.8850 (0.5274 -- 3.2957)  data: 0.2933 (0.0006 -- 2.7573)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:36  lr: 0.000042  min_lr: 0.000011  loss: 1.5602 (1.5768)  loss_scale: 32768.0000 (43511.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7680 (6.9741)  time: 0.8743 (0.5222 -- 2.6690)  data: 0.3224 (0.0007 -- 2.1334)  max mem: 16413
[2023-09-04 16:59:54,652] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7275
[2023-09-04 16:59:54,652] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:59:54,653] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7275
[2023-09-04 16:59:54,694] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 16:59:54,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [45]  [ 80/160]  eta: 0:01:14  lr: 0.000042  min_lr: 0.000011  loss: 1.5277 (1.5624)  loss_scale: 32768.0000 (39645.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3934 (7.1049)  time: 0.8526 (0.5229 -- 3.1884)  data: 0.2846 (0.0005 -- 2.6337)  max mem: 16413
Epoch: [45]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.6060 (1.5760)  loss_scale: 16384.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8383 (7.0893)  time: 0.9848 (0.5431 -- 4.4283)  data: 0.3193 (0.0004 -- 3.8921)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.4983 (1.5764)  loss_scale: 16384.0000 (31955.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3543 (7.1660)  time: 0.8006 (0.5045 -- 2.8070)  data: 0.1080 (0.0003 -- 2.1392)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.6215 (1.5755)  loss_scale: 16384.0000 (29746.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1749 (7.2501)  time: 0.7955 (0.5245 -- 3.6835)  data: 0.0019 (0.0006 -- 0.0074)  max mem: 16413
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.5277 (1.5714)  loss_scale: 16384.0000 (28160.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5655 (7.1536)  time: 0.6656 (0.4962 -- 2.4128)  data: 0.0009 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [45] Total time: 0:02:20 (0.8760 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.5277 (1.5349)  loss_scale: 16384.0000 (28160.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5655 (7.1536)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1311 (0.1311)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3084 (2.3084 -- 2.3084)  data: 2.0944 (2.0944 -- 2.0944)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3437 (0.5249)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.4180 (0.2149 -- 2.3084)  data: 0.1919 (0.0008 -- 2.0944)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3437 (0.5010)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2241 (0.1698 -- 0.3730)  data: 0.0126 (0.0001 -- 0.1850)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4308 (0.5729)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2040 (0.1326 -- 0.3730)  data: 0.0121 (0.0001 -- 0.1850)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.600
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 87.14%
Epoch: [46]  [  0/160]  eta: 0:18:07  lr: 0.000042  min_lr: 0.000011  loss: 1.9249 (1.9249)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3359 (6.3359)  time: 6.7992 (6.7992 -- 6.7992)  data: 6.2740 (6.2740 -- 6.2740)  max mem: 16413
Epoch: [46]  [ 20/160]  eta: 0:02:49  lr: 0.000042  min_lr: 0.000011  loss: 1.5194 (1.5896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1101 (6.5624)  time: 0.9325 (0.5101 -- 3.0856)  data: 0.3788 (0.0003 -- 2.5513)  max mem: 16413
Epoch: [46]  [ 40/160]  eta: 0:02:02  lr: 0.000042  min_lr: 0.000011  loss: 1.4548 (1.5315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8025 (6.6077)  time: 0.8229 (0.5353 -- 3.0066)  data: 0.2632 (0.0002 -- 2.4747)  max mem: 16413
[2023-09-04 17:01:57,350] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:01:57,350] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:01:57,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:01:57,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [46]  [ 60/160]  eta: 0:01:38  lr: 0.000042  min_lr: 0.000011  loss: 1.5241 (1.5548)  loss_scale: 32768.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6505 (6.8704)  time: 0.9133 (0.5399 -- 3.7520)  data: 0.2393 (0.0002 -- 1.3504)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000011  loss: 1.4950 (1.5239)  loss_scale: 32768.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9433 (6.8682)  time: 0.8139 (0.5243 -- 2.4386)  data: 0.1302 (0.0001 -- 1.5736)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000011  loss: 1.4829 (1.5097)  loss_scale: 32768.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4900 (6.8687)  time: 0.9911 (0.5162 -- 4.9247)  data: 0.0785 (0.0003 -- 1.5498)  max mem: 16413
Epoch: [46]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000011  loss: 1.6669 (1.5406)  loss_scale: 32768.0000 (26810.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6400 (6.8610)  time: 0.8388 (0.5304 -- 3.3105)  data: 0.0023 (0.0004 -- 0.0065)  max mem: 16413
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.5459 (1.5485)  loss_scale: 32768.0000 (27655.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8317 (6.9603)  time: 0.8899 (0.5255 -- 3.9843)  data: 0.0013 (0.0005 -- 0.0028)  max mem: 16413
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.6058 (1.5527)  loss_scale: 32768.0000 (28262.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3138 (6.9245)  time: 0.7218 (0.4945 -- 3.8223)  data: 0.0006 (0.0002 -- 0.0012)  max mem: 16413
Epoch: [46] Total time: 0:02:24 (0.9049 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.6058 (1.5695)  loss_scale: 32768.0000 (28262.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3138 (6.9245)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1541 (0.1541)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4330 (2.4330 -- 2.4330)  data: 2.1819 (2.1819 -- 2.1819)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4302 (0.5118)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (97.9798)  time: 0.4179 (0.1916 -- 2.4330)  data: 0.1998 (0.0007 -- 2.1819)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4605 (0.5146)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2166 (0.1743 -- 0.3098)  data: 0.0107 (0.0001 -- 0.0987)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5635 (0.5668)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.2021 (0.1326 -- 0.3098)  data: 0.0104 (0.0001 -- 0.0987)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 84.855 Acc@5 97.718 loss 0.609
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 87.14%
Epoch: [47]  [  0/160]  eta: 0:18:46  lr: 0.000042  min_lr: 0.000011  loss: 1.3881 (1.3881)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8155 (8.8155)  time: 7.0437 (7.0437 -- 7.0437)  data: 6.2480 (6.2480 -- 6.2480)  max mem: 16413
[2023-09-04 17:04:02,549] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:04:02,549] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:04:02,549] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:04:02,549] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [47]  [ 20/160]  eta: 0:02:41  lr: 0.000042  min_lr: 0.000011  loss: 1.5563 (1.5529)  loss_scale: 32768.0000 (46811.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8486 (6.9661)  time: 0.8602 (0.5315 -- 4.3920)  data: 0.3069 (0.0004 -- 3.8498)  max mem: 16413
[2023-09-04 17:04:22,340] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7554
[2023-09-04 17:04:22,340] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:04:22,340] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 17:04:22,340] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7554
[2023-09-04 17:04:22,340] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [47]  [ 40/160]  eta: 0:02:03  lr: 0.000042  min_lr: 0.000011  loss: 1.5077 (1.5359)  loss_scale: 65536.0000 (50350.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3961 (7.2407)  time: 0.8992 (0.5325 -- 3.0090)  data: 0.3456 (0.0004 -- 2.4774)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:41  lr: 0.000042  min_lr: 0.000011  loss: 1.5092 (1.5088)  loss_scale: 32768.0000 (44585.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3355 (7.0903)  time: 0.9743 (0.5132 -- 4.6528)  data: 0.0124 (0.0004 -- 0.2268)  max mem: 16413
Epoch: [47]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000011  loss: 1.4441 (1.5105)  loss_scale: 32768.0000 (41667.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7772 (6.8940)  time: 0.7302 (0.5256 -- 1.8998)  data: 0.0409 (0.0002 -- 0.7841)  max mem: 16413
[2023-09-04 17:05:02,039] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7601
[2023-09-04 17:05:02,039] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7601
[2023-09-04 17:05:02,039] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:05:02,039] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:05:02,039] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.4886 (1.5073)  loss_scale: 16384.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9870 (6.8047)  time: 0.9637 (0.5265 -- 3.5238)  data: 0.0614 (0.0005 -- 1.1884)  max mem: 16413
Epoch: [47]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.5877 (1.5232)  loss_scale: 16384.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8642 (6.8815)  time: 0.8147 (0.5171 -- 1.9737)  data: 0.1778 (0.0004 -- 1.4280)  max mem: 16413
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.6270 (1.5311)  loss_scale: 16384.0000 (30908.8227)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7722 (6.9002)  time: 0.9353 (0.5278 -- 3.7571)  data: 0.0325 (0.0002 -- 0.6212)  max mem: 16413
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000011  loss: 1.5944 (1.5215)  loss_scale: 16384.0000 (29184.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5715 (6.9433)  time: 0.6451 (0.4961 -- 2.2101)  data: 0.0010 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [47] Total time: 0:02:22 (0.8937 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000011  loss: 1.5944 (1.5371)  loss_scale: 16384.0000 (29184.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5715 (6.9433)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1426 (0.1426)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3069 (2.3069 -- 2.3069)  data: 2.1086 (2.1086 -- 2.1086)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5587 (0.5805)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4102 (0.1904 -- 2.3069)  data: 0.1988 (0.0008 -- 2.1086)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4943 (0.5441)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2202 (0.1697 -- 0.3207)  data: 0.0148 (0.0001 -- 0.1147)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4812 (0.5835)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.5104)  time: 0.2056 (0.1321 -- 0.3207)  data: 0.0145 (0.0001 -- 0.1147)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.605
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 87.14%
Epoch: [48]  [  0/160]  eta: 0:17:51  lr: 0.000041  min_lr: 0.000011  loss: 1.3639 (1.3639)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4882 (5.4882)  time: 6.6968 (6.6968 -- 6.6968)  data: 5.6282 (5.6282 -- 5.6282)  max mem: 16413
Epoch: [48]  [ 20/160]  eta: 0:02:40  lr: 0.000041  min_lr: 0.000011  loss: 1.6452 (1.6275)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1162 (7.4229)  time: 0.8717 (0.5198 -- 2.3858)  data: 0.3124 (0.0003 -- 1.8509)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:03  lr: 0.000041  min_lr: 0.000011  loss: 1.5103 (1.5334)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2367 (7.0213)  time: 0.9106 (0.5217 -- 2.6479)  data: 0.2707 (0.0003 -- 2.0991)  max mem: 16413
[2023-09-04 17:07:08,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:07:08,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:07:08,110] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:07:08,110] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [48]  [ 60/160]  eta: 0:01:41  lr: 0.000041  min_lr: 0.000011  loss: 1.5584 (1.5365)  loss_scale: 32768.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7177 (6.9136)  time: 0.9716 (0.5146 -- 4.7631)  data: 0.0262 (0.0004 -- 0.4989)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:16  lr: 0.000041  min_lr: 0.000011  loss: 1.3753 (1.5310)  loss_scale: 32768.0000 (22654.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3776 (6.8402)  time: 0.7912 (0.5294 -- 3.8349)  data: 0.0017 (0.0004 -- 0.0056)  max mem: 16413
Epoch: [48]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000011  loss: 1.4744 (1.5254)  loss_scale: 32768.0000 (24657.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9760 (6.8144)  time: 0.8300 (0.5111 -- 3.0576)  data: 0.2005 (0.0006 -- 2.5320)  max mem: 16413
Epoch: [48]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.5084 (1.5371)  loss_scale: 32768.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8295 (6.8578)  time: 0.8327 (0.5423 -- 2.0699)  data: 0.0238 (0.0002 -- 0.4417)  max mem: 16413
Epoch: [48]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.5869 (1.5497)  loss_scale: 32768.0000 (26958.0709)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0501 (6.8227)  time: 0.8745 (0.5385 -- 2.6971)  data: 0.0420 (0.0003 -- 0.7898)  max mem: 16413
[2023-09-04 17:08:36,666] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7836
[2023-09-04 17:08:36,666] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7836
[2023-09-04 17:08:36,666] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:08:36,666] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:08:36,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.5056 (1.5450)  loss_scale: 32768.0000 (27238.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8994 (6.8332)  time: 0.8436 (0.4946 -- 2.6971)  data: 0.0422 (0.0002 -- 0.7898)  max mem: 16413
Epoch: [48] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.5056 (1.5367)  loss_scale: 32768.0000 (27238.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8994 (6.8332)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1500 (0.1500)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1887 (2.1887 -- 2.1887)  data: 1.9843 (1.9843 -- 1.9843)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6476 (0.6779)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (96.9697)  time: 0.4113 (0.1983 -- 2.1887)  data: 0.2029 (0.0007 -- 1.9843)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4587 (0.6025)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2276 (0.1703 -- 0.4407)  data: 0.0248 (0.0001 -- 0.2386)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4587 (0.6615)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.6805)  time: 0.2138 (0.1334 -- 0.4407)  data: 0.0245 (0.0001 -- 0.2386)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 84.440 Acc@5 97.303 loss 0.652
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 87.14%
Epoch: [49]  [  0/160]  eta: 0:18:12  lr: 0.000041  min_lr: 0.000010  loss: 1.9281 (1.9281)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8870 (9.8870)  time: 6.8259 (6.8259 -- 6.8259)  data: 6.2972 (6.2972 -- 6.2972)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:48  lr: 0.000041  min_lr: 0.000010  loss: 1.5753 (1.5923)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4139 (6.5701)  time: 0.9238 (0.5376 -- 3.1483)  data: 0.3736 (0.0003 -- 2.6039)  max mem: 16413
Epoch: [49]  [ 40/160]  eta: 0:02:04  lr: 0.000041  min_lr: 0.000010  loss: 1.6201 (1.6100)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2998 (6.5044)  time: 0.8660 (0.5242 -- 2.4277)  data: 0.3167 (0.0002 -- 1.8994)  max mem: 16413
Epoch: [49]  [ 60/160]  eta: 0:01:37  lr: 0.000041  min_lr: 0.000010  loss: 1.6282 (1.5991)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6779 (6.2861)  time: 0.8576 (0.5421 -- 3.4525)  data: 0.1333 (0.0003 -- 0.9816)  max mem: 16413
Epoch: [49]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000010  loss: 1.6266 (1.6016)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4271 (6.5226)  time: 0.9448 (0.5103 -- 3.6672)  data: 0.0622 (0.0001 -- 0.7860)  max mem: 16413
Epoch: [49]  [100/160]  eta: 0:00:57  lr: 0.000041  min_lr: 0.000010  loss: 1.5129 (1.5860)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7623 (6.6438)  time: 0.8932 (0.5208 -- 3.4481)  data: 0.2673 (0.0004 -- 2.9109)  max mem: 16413
Epoch: [49]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000010  loss: 1.6729 (1.5857)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1642 (6.6143)  time: 0.8525 (0.5123 -- 4.0324)  data: 0.2994 (0.0003 -- 3.5063)  max mem: 16413
[2023-09-04 17:10:42,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:10:42,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:10:42,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:10:42,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.4860 (1.5697)  loss_scale: 32768.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5820 (6.6660)  time: 0.7420 (0.5366 -- 3.1608)  data: 0.1715 (0.0004 -- 2.6009)  max mem: 16413
[2023-09-04 17:11:09,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=40, lr=[1.0418644962272174e-05, 1.0418644962272174e-05, 1.1576272180302416e-05, 1.1576272180302416e-05, 1.286252464478046e-05, 1.286252464478046e-05, 1.4291694049756068e-05, 1.4291694049756068e-05, 1.587966005528452e-05, 1.587966005528452e-05, 1.764406672809391e-05, 1.764406672809391e-05, 1.960451858677101e-05, 1.960451858677101e-05, 2.1782798429745566e-05, 2.1782798429745566e-05, 2.4203109366383962e-05, 2.4203109366383962e-05, 2.689234374042662e-05, 2.689234374042662e-05, 2.988038193380736e-05, 2.988038193380736e-05, 3.3200424370897066e-05, 3.3200424370897066e-05, 3.688936041210785e-05, 3.688936041210785e-05, 4.0988178235675386e-05, 4.0988178235675386e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 17:11:09,563] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=18.421896222370354, CurrSamplesPerSec=24.762577801786804, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.4666 (1.5542)  loss_scale: 32768.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8764 (6.6182)  time: 0.7888 (0.4951 -- 2.9506)  data: 0.0747 (0.0002 -- 1.4799)  max mem: 16413
Epoch: [49] Total time: 0:02:23 (0.8979 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.4666 (1.5662)  loss_scale: 32768.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8764 (6.6182)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1624 (0.1624)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3113 (2.3113 -- 2.3113)  data: 2.0867 (2.0867 -- 2.0867)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4777 (0.5922)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4363 (0.2093 -- 2.3113)  data: 0.2143 (0.0007 -- 2.0867)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4581 (0.5354)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2239 (0.1736 -- 0.4709)  data: 0.0146 (0.0001 -- 0.2463)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4581 (0.5804)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2064 (0.1328 -- 0.4709)  data: 0.0141 (0.0001 -- 0.2463)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.652
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 87.14%
Epoch: [50]  [  0/160]  eta: 0:20:00  lr: 0.000041  min_lr: 0.000010  loss: 1.4815 (1.4815)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6795 (6.6795)  time: 7.5049 (7.5049 -- 7.5049)  data: 5.1680 (5.1680 -- 5.1680)  max mem: 16413
Epoch: [50]  [ 20/160]  eta: 0:02:48  lr: 0.000041  min_lr: 0.000010  loss: 1.4431 (1.4369)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3905 (6.1220)  time: 0.8881 (0.5237 -- 2.7936)  data: 0.2456 (0.0005 -- 2.2643)  max mem: 16413
Epoch: [50]  [ 40/160]  eta: 0:02:06  lr: 0.000041  min_lr: 0.000010  loss: 1.4023 (1.4570)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0548 (6.3601)  time: 0.8964 (0.5255 -- 4.1247)  data: 0.3412 (0.0002 -- 3.5879)  max mem: 16413
Epoch: [50]  [ 60/160]  eta: 0:01:41  lr: 0.000041  min_lr: 0.000010  loss: 1.5735 (1.4685)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2195 (6.4770)  time: 0.9280 (0.5343 -- 3.1548)  data: 0.3774 (0.0004 -- 2.6153)  max mem: 16413
Epoch: [50]  [ 80/160]  eta: 0:01:15  lr: 0.000041  min_lr: 0.000010  loss: 1.6556 (1.4955)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6272 (6.7069)  time: 0.7408 (0.5342 -- 2.2095)  data: 0.1824 (0.0006 -- 1.6558)  max mem: 16413
[2023-09-04 17:12:50,070] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:12:50,070] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:12:50,070] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:12:50,070] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:12:52,169] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8097
[2023-09-04 17:12:52,170] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:12:52,170] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8097
[2023-09-04 17:12:52,170] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:12:52,170] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [50]  [100/160]  eta: 0:00:57  lr: 0.000041  min_lr: 0.000010  loss: 1.5784 (1.4976)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8825 (6.7301)  time: 0.9950 (0.5207 -- 5.5960)  data: 0.4548 (0.0003 -- 5.0804)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000010  loss: 1.5155 (1.5068)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2185 (6.7177)  time: 0.8199 (0.5234 -- 4.0083)  data: 0.2760 (0.0004 -- 3.4970)  max mem: 16413
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.5063 (1.5151)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5214 (6.6450)  time: 0.8670 (0.5112 -- 3.9955)  data: 0.3206 (0.0004 -- 3.4751)  max mem: 16413
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.5631 (1.5144)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1649 (6.6897)  time: 0.6936 (0.4961 -- 3.9531)  data: 0.1770 (0.0001 -- 3.4354)  max mem: 16413
Epoch: [50] Total time: 0:02:23 (0.8970 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.5631 (1.5285)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1649 (6.6897)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1312 (0.1312)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4240 (2.4240 -- 2.4240)  data: 2.2082 (2.2082 -- 2.2082)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5512 (0.5864)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4275 (0.2064 -- 2.4240)  data: 0.2121 (0.0007 -- 2.2082)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5087 (0.5351)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2167 (0.1687 -- 0.3412)  data: 0.0142 (0.0001 -- 0.1566)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5087 (0.5655)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.2016 (0.1324 -- 0.3412)  data: 0.0140 (0.0001 -- 0.1566)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.607
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.14%
Epoch: [51]  [  0/160]  eta: 0:17:57  lr: 0.000041  min_lr: 0.000010  loss: 1.5952 (1.5952)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5882 (7.5882)  time: 6.7374 (6.7374 -- 6.7374)  data: 5.9615 (5.9615 -- 5.9615)  max mem: 16413
Epoch: [51]  [ 20/160]  eta: 0:02:41  lr: 0.000041  min_lr: 0.000010  loss: 1.5886 (1.5488)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2898 (7.0560)  time: 0.8755 (0.5193 -- 3.0869)  data: 0.0466 (0.0002 -- 0.9013)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:01:57  lr: 0.000041  min_lr: 0.000010  loss: 1.5161 (1.5318)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8899 (6.5702)  time: 0.7992 (0.5274 -- 2.9034)  data: 0.1468 (0.0004 -- 1.2279)  max mem: 16413
Epoch: [51]  [ 60/160]  eta: 0:01:34  lr: 0.000041  min_lr: 0.000010  loss: 1.4186 (1.5312)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3554 (6.7417)  time: 0.8666 (0.5206 -- 2.9955)  data: 0.2018 (0.0005 -- 1.7664)  max mem: 16413
[2023-09-04 17:14:53,058] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:14:53,059] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:14:53,060] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:14:53,060] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:15:01,006] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8234
[2023-09-04 17:15:01,006] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8234
[2023-09-04 17:15:01,007] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:15:01,007] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:15:01,007] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [51]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000010  loss: 1.5275 (1.5192)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0096 (6.6831)  time: 1.0465 (0.5141 -- 4.1319)  data: 0.5000 (0.0003 -- 3.6220)  max mem: 16413
Epoch: [51]  [100/160]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000010  loss: 1.5443 (1.5223)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6346 (6.6373)  time: 0.8121 (0.5133 -- 4.2341)  data: 0.2611 (0.0004 -- 3.6926)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.6543 (1.5455)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4389 (6.7397)  time: 0.8358 (0.5291 -- 3.6085)  data: 0.2814 (0.0002 -- 3.0595)  max mem: 16413
Epoch: [51]  [140/160]  eta: 0:00:17  lr: 0.000041  min_lr: 0.000010  loss: 1.5498 (1.5570)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0094 (6.7823)  time: 0.7550 (0.5338 -- 3.1783)  data: 0.2055 (0.0005 -- 2.6555)  max mem: 16413
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.6482 (1.5609)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2004 (6.7911)  time: 0.8357 (0.4967 -- 3.7170)  data: 0.3141 (0.0002 -- 3.1989)  max mem: 16413
Epoch: [51] Total time: 0:02:22 (0.8922 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.6482 (1.5618)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2004 (6.7911)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1235 (0.1235)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3880 (2.3880 -- 2.3880)  data: 2.1347 (2.1347 -- 2.1347)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5696 (0.6063)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.4198 (0.1964 -- 2.3880)  data: 0.2059 (0.0005 -- 2.1347)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3547 (0.5305)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2226 (0.1684 -- 0.4643)  data: 0.0205 (0.0001 -- 0.2781)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4421 (0.5896)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.9253)  time: 0.2079 (0.1327 -- 0.4643)  data: 0.0203 (0.0001 -- 0.2781)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.619
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 87.14%
Epoch: [52]  [  0/160]  eta: 0:18:02  lr: 0.000040  min_lr: 0.000010  loss: 1.3987 (1.3987)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3632 (5.3632)  time: 6.7645 (6.7645 -- 6.7645)  data: 6.2340 (6.2340 -- 6.2340)  max mem: 16413
Epoch: [52]  [ 20/160]  eta: 0:02:38  lr: 0.000040  min_lr: 0.000010  loss: 1.2219 (1.3867)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8520 (6.4745)  time: 0.8499 (0.5291 -- 2.4089)  data: 0.1977 (0.0007 -- 1.7997)  max mem: 16413
Epoch: [52]  [ 40/160]  eta: 0:02:01  lr: 0.000040  min_lr: 0.000010  loss: 1.5315 (1.4447)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1653 (6.3840)  time: 0.8885 (0.5271 -- 2.0116)  data: 0.0956 (0.0002 -- 1.2068)  max mem: 16413
[2023-09-04 17:17:03,589] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:17:03,589] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:17:03,589] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:17:03,589] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:17:12,149] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8374
[2023-09-04 17:17:12,149] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:17:12,149] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 17:17:12,149] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8374
[2023-09-04 17:17:12,150] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [52]  [ 60/160]  eta: 0:01:35  lr: 0.000040  min_lr: 0.000010  loss: 1.6739 (1.5203)  loss_scale: 65536.0000 (38676.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5757 (6.2801)  time: 0.8335 (0.5213 -- 2.4915)  data: 0.0886 (0.0008 -- 1.0387)  max mem: 16413
Epoch: [52]  [ 80/160]  eta: 0:01:16  lr: 0.000040  min_lr: 0.000010  loss: 1.6131 (1.5240)  loss_scale: 32768.0000 (37217.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3568 (6.6245)  time: 0.9426 (0.5203 -- 3.2886)  data: 0.1918 (0.0003 -- 2.7448)  max mem: 16413
Epoch: [52]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 1.5782 (1.5339)  loss_scale: 32768.0000 (36336.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2298 (6.6626)  time: 0.8599 (0.5338 -- 3.0824)  data: 0.0921 (0.0002 -- 1.6894)  max mem: 16413
Epoch: [52]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000010  loss: 1.5207 (1.5391)  loss_scale: 32768.0000 (35746.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5476 (6.7227)  time: 0.8987 (0.5349 -- 3.6319)  data: 0.0487 (0.0004 -- 0.5946)  max mem: 16413
[2023-09-04 17:18:27,381] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8460
[2023-09-04 17:18:27,381] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8460
[2023-09-04 17:18:27,382] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:18:27,382] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 17:18:27,382] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [52]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.6015 (1.5476)  loss_scale: 32768.0000 (35208.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9193 (6.7800)  time: 0.7960 (0.5401 -- 1.8016)  data: 0.1070 (0.0007 -- 1.2336)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.4999 (1.5468)  loss_scale: 16384.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7250 (6.7408)  time: 0.7550 (0.4953 -- 2.3721)  data: 0.0583 (0.0001 -- 0.8058)  max mem: 16413
Epoch: [52] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.4999 (1.5670)  loss_scale: 16384.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7250 (6.7408)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1332 (0.1332)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3255 (2.3255 -- 2.3255)  data: 2.1176 (2.1176 -- 2.1176)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4483 (0.5543)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (97.9798)  time: 0.4197 (0.2026 -- 2.3255)  data: 0.2056 (0.0008 -- 2.1176)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4013 (0.5423)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2268 (0.1737 -- 0.4535)  data: 0.0207 (0.0001 -- 0.2666)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4013 (0.5776)  acc1: 77.7778 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2137 (0.1331 -- 0.4535)  data: 0.0205 (0.0001 -- 0.2666)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 85.477 Acc@5 98.340 loss 0.598
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.14%
Epoch: [53]  [  0/160]  eta: 0:22:12  lr: 0.000040  min_lr: 0.000010  loss: 1.5825 (1.5825)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5595 (4.5595)  time: 8.3274 (8.3274 -- 8.3274)  data: 7.7938 (7.7938 -- 7.7938)  max mem: 16413
Epoch: [53]  [ 20/160]  eta: 0:02:42  lr: 0.000040  min_lr: 0.000010  loss: 1.4699 (1.5392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9901 (7.6039)  time: 0.7989 (0.5306 -- 3.1569)  data: 0.2401 (0.0003 -- 2.6291)  max mem: 16413
Epoch: [53]  [ 40/160]  eta: 0:02:10  lr: 0.000040  min_lr: 0.000010  loss: 1.3920 (1.5117)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9947 (7.2913)  time: 1.0118 (0.5291 -- 4.1502)  data: 0.4598 (0.0005 -- 3.6250)  max mem: 16413
Epoch: [53]  [ 60/160]  eta: 0:01:36  lr: 0.000040  min_lr: 0.000010  loss: 1.7130 (1.5943)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3382 (7.1516)  time: 0.7312 (0.5218 -- 2.4382)  data: 0.1747 (0.0002 -- 1.9033)  max mem: 16413
Epoch: [53]  [ 80/160]  eta: 0:01:12  lr: 0.000040  min_lr: 0.000010  loss: 1.4531 (1.5696)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7376 (6.9374)  time: 0.7362 (0.5138 -- 2.7957)  data: 0.1377 (0.0003 -- 2.2575)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000010  loss: 1.6294 (1.5838)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2580 (6.8432)  time: 1.0753 (0.5341 -- 4.5941)  data: 0.1708 (0.0006 -- 1.4738)  max mem: 16413
[2023-09-04 17:20:33,545] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:20:33,546] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:20:33,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:20:33,547] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [53]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000010  loss: 1.6351 (1.5805)  loss_scale: 32768.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3651 (6.6010)  time: 0.8225 (0.5243 -- 4.0319)  data: 0.0010 (0.0003 -- 0.0025)  max mem: 16413
[2023-09-04 17:20:58,418] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8620
[2023-09-04 17:20:58,418] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8620
[2023-09-04 17:20:58,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:20:58,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:20:58,419] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.5162 (1.5784)  loss_scale: 32768.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9451 (6.6581)  time: 0.8377 (0.5239 -- 3.9829)  data: 0.0081 (0.0008 -- 0.1295)  max mem: 16413
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.6606 (1.5725)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4718 (6.7366)  time: 0.7242 (0.4958 -- 2.3791)  data: 0.0705 (0.0002 -- 1.3958)  max mem: 16413
Epoch: [53] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.6606 (1.5467)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4718 (6.7366)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.1390 (0.1390)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6828 (2.6828 -- 2.6828)  data: 2.4573 (2.4573 -- 2.4573)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5430 (0.6000)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4426 (0.2020 -- 2.6828)  data: 0.2244 (0.0007 -- 2.4573)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4659 (0.5566)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2102 (0.1688 -- 0.2936)  data: 0.0057 (0.0001 -- 0.0993)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4762 (0.5800)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.7552)  time: 0.1945 (0.1333 -- 0.2936)  data: 0.0054 (0.0001 -- 0.0993)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.607
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 87.14%
Epoch: [54]  [  0/160]  eta: 0:17:55  lr: 0.000040  min_lr: 0.000010  loss: 2.0532 (2.0532)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9850 (6.9850)  time: 6.7226 (6.7226 -- 6.7226)  data: 5.6508 (5.6508 -- 5.6508)  max mem: 16413
Epoch: [54]  [ 20/160]  eta: 0:02:37  lr: 0.000040  min_lr: 0.000010  loss: 1.4163 (1.4699)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3750 (6.6754)  time: 0.8432 (0.5374 -- 2.3009)  data: 0.2387 (0.0005 -- 1.2861)  max mem: 16413
Epoch: [54]  [ 40/160]  eta: 0:02:02  lr: 0.000040  min_lr: 0.000010  loss: 1.4058 (1.4834)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2656 (6.7750)  time: 0.9106 (0.5274 -- 3.7692)  data: 0.3054 (0.0004 -- 3.2384)  max mem: 16413
Epoch: [54]  [ 60/160]  eta: 0:01:39  lr: 0.000040  min_lr: 0.000010  loss: 1.4093 (1.4927)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0330 (6.8490)  time: 0.9513 (0.5323 -- 4.4708)  data: 0.4028 (0.0004 -- 3.9474)  max mem: 16413
Epoch: [54]  [ 80/160]  eta: 0:01:16  lr: 0.000040  min_lr: 0.000010  loss: 1.6918 (1.5332)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2137 (6.7144)  time: 0.8119 (0.5212 -- 2.8978)  data: 0.2597 (0.0001 -- 2.3679)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000010  loss: 1.5783 (1.5326)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6750 (6.6763)  time: 0.9413 (0.5111 -- 4.7260)  data: 0.3921 (0.0003 -- 4.2060)  max mem: 16413
[2023-09-04 17:23:02,116] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:23:02,116] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:23:02,116] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:23:02,116] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [54]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000010  loss: 1.5315 (1.5385)  loss_scale: 32768.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3339 (6.6405)  time: 0.8385 (0.5112 -- 3.2092)  data: 0.2818 (0.0003 -- 2.6483)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.5676 (1.5429)  loss_scale: 32768.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7575 (6.6964)  time: 0.8001 (0.5237 -- 3.0799)  data: 0.2471 (0.0005 -- 2.5661)  max mem: 16413
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.4617 (1.5425)  loss_scale: 32768.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6602 (6.6919)  time: 0.7026 (0.4949 -- 3.6303)  data: 0.1838 (0.0002 -- 3.1149)  max mem: 16413
Epoch: [54] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.4617 (1.5466)  loss_scale: 32768.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6602 (6.6919)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1281 (0.1281)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3506 (2.3506 -- 2.3506)  data: 2.1329 (2.1329 -- 2.1329)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4503 (0.5407)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4139 (0.2038 -- 2.3506)  data: 0.1950 (0.0008 -- 2.1329)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4503 (0.5215)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2213 (0.1715 -- 0.3389)  data: 0.0136 (0.0001 -- 0.1298)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4857 (0.5850)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.3402)  time: 0.2059 (0.1328 -- 0.3389)  data: 0.0133 (0.0001 -- 0.1298)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.599
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 87.14%
Epoch: [55]  [  0/160]  eta: 0:21:18  lr: 0.000040  min_lr: 0.000010  loss: 1.3625 (1.3625)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3624 (4.3624)  time: 7.9914 (7.9914 -- 7.9914)  data: 6.4987 (6.4987 -- 6.4987)  max mem: 16413
Epoch: [55]  [ 20/160]  eta: 0:02:43  lr: 0.000040  min_lr: 0.000010  loss: 1.5446 (1.6390)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1774 (6.9756)  time: 0.8301 (0.5216 -- 2.6642)  data: 0.2609 (0.0004 -- 2.1297)  max mem: 16413
Epoch: [55]  [ 40/160]  eta: 0:02:02  lr: 0.000040  min_lr: 0.000010  loss: 1.6981 (1.6590)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2114 (7.0747)  time: 0.8554 (0.5235 -- 3.0090)  data: 0.3113 (0.0005 -- 2.4487)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:42  lr: 0.000040  min_lr: 0.000010  loss: 1.5603 (1.6298)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0977 (6.8860)  time: 1.0261 (0.5423 -- 3.3295)  data: 0.4764 (0.0004 -- 2.8073)  max mem: 16413
[2023-09-04 17:25:06,938] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:25:06,939] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:25:06,940] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:25:06,940] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [55]  [ 80/160]  eta: 0:01:17  lr: 0.000040  min_lr: 0.000010  loss: 1.4527 (1.5817)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5962 (6.7364)  time: 0.8085 (0.5160 -- 2.9522)  data: 0.2617 (0.0001 -- 2.4116)  max mem: 16413
[2023-09-04 17:25:23,936] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8895
[2023-09-04 17:25:23,936] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:25:23,936] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8895
[2023-09-04 17:25:23,937] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:25:23,937] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [55]  [100/160]  eta: 0:00:57  lr: 0.000040  min_lr: 0.000010  loss: 1.6367 (1.5708)  loss_scale: 65536.0000 (38607.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1937 (6.5819)  time: 0.9235 (0.5145 -- 4.4355)  data: 0.3802 (0.0003 -- 3.8888)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.5832 (1.5671)  loss_scale: 32768.0000 (37642.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2494 (6.5496)  time: 0.8270 (0.5244 -- 4.0043)  data: 0.2762 (0.0007 -- 3.4735)  max mem: 16413
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.5630 (1.5807)  loss_scale: 32768.0000 (36951.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2094 (6.5843)  time: 0.8175 (0.5348 -- 2.7687)  data: 0.2711 (0.0006 -- 2.2395)  max mem: 16413
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.5799 (1.5833)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4825 (6.6453)  time: 0.7955 (0.4982 -- 2.7781)  data: 0.2124 (0.0002 -- 2.2706)  max mem: 16413
Epoch: [55] Total time: 0:02:23 (0.8979 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.5799 (1.5603)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4825 (6.6453)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1422 (0.1422)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4366 (2.4366 -- 2.4366)  data: 2.2054 (2.2054 -- 2.2054)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4585 (0.5133)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4220 (0.1998 -- 2.4366)  data: 0.2014 (0.0006 -- 2.2054)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4014 (0.5016)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2139 (0.1688 -- 0.2504)  data: 0.0008 (0.0001 -- 0.0014)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4014 (0.5545)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (99.1701)  time: 0.1962 (0.1326 -- 0.2479)  data: 0.0005 (0.0001 -- 0.0013)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 85.892 Acc@5 98.548 loss 0.600
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.14%
Epoch: [56]  [  0/160]  eta: 0:17:41  lr: 0.000039  min_lr: 0.000010  loss: 1.7508 (1.7508)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.2613 (15.2613)  time: 6.6315 (6.6315 -- 6.6315)  data: 6.0778 (6.0778 -- 6.0778)  max mem: 16413
Epoch: [56]  [ 20/160]  eta: 0:02:46  lr: 0.000039  min_lr: 0.000010  loss: 1.5891 (1.5871)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1706 (7.5775)  time: 0.9145 (0.5404 -- 2.6429)  data: 0.2592 (0.0003 -- 2.0931)  max mem: 16413
[2023-09-04 17:27:03,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=46, lr=[9.999403491949061e-06, 9.999403491949061e-06, 1.1110448324387846e-05, 1.1110448324387846e-05, 1.234494258265316e-05, 1.234494258265316e-05, 1.3716602869614624e-05, 1.3716602869614624e-05, 1.524066985512736e-05, 1.524066985512736e-05, 1.6934077616808177e-05, 1.6934077616808177e-05, 1.881564179645353e-05, 1.881564179645353e-05, 2.0906268662726143e-05, 2.0906268662726143e-05, 2.3229187403029046e-05, 2.3229187403029046e-05, 2.5810208225587826e-05, 2.5810208225587826e-05, 2.8678009139542034e-05, 2.8678009139542034e-05, 3.186445459949115e-05, 3.186445459949115e-05, 3.540494955499016e-05, 3.540494955499016e-05, 3.9338832838877956e-05, 3.9338832838877956e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 17:27:03,252] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=18.55126310369959, CurrSamplesPerSec=22.115207980723095, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:03  lr: 0.000039  min_lr: 0.000010  loss: 1.4676 (1.4960)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3261 (7.1911)  time: 0.8640 (0.5235 -- 2.8867)  data: 0.1913 (0.0005 -- 1.4184)  max mem: 16413
Epoch: [56]  [ 60/160]  eta: 0:01:39  lr: 0.000039  min_lr: 0.000010  loss: 1.5191 (1.4973)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3556 (7.2950)  time: 0.9158 (0.5170 -- 2.9906)  data: 0.1978 (0.0008 -- 2.3360)  max mem: 16413
[2023-09-04 17:27:24,354] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:27:24,354] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:27:24,355] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:27:24,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:27:30,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9030
[2023-09-04 17:27:30,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9030
[2023-09-04 17:27:30,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:27:30,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:27:30,603] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [56]  [ 80/160]  eta: 0:01:16  lr: 0.000039  min_lr: 0.000010  loss: 1.5113 (1.5090)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1253 (7.2585)  time: 0.8657 (0.5099 -- 2.6894)  data: 0.1099 (0.0004 -- 1.2590)  max mem: 16413
[2023-09-04 17:27:42,014] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9043
[2023-09-04 17:27:42,014] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9043
[2023-09-04 17:27:42,015] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:27:42,015] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:27:42,015] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [56]  [100/160]  eta: 0:00:55  lr: 0.000039  min_lr: 0.000010  loss: 1.4224 (1.4747)  loss_scale: 16384.0000 (31794.6931)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1865 (7.1446)  time: 0.8056 (0.5240 -- 2.2595)  data: 0.0529 (0.0008 -- 0.7423)  max mem: 16413
Epoch: [56]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000010  loss: 1.6548 (1.4861)  loss_scale: 16384.0000 (29247.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3512 (7.3207)  time: 0.8154 (0.5183 -- 2.3739)  data: 0.0934 (0.0003 -- 1.8329)  max mem: 16413
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.5347 (1.4882)  loss_scale: 16384.0000 (27422.8652)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2603 (7.3320)  time: 0.9705 (0.5296 -- 4.3058)  data: 0.4189 (0.0003 -- 3.7485)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.6707 (1.5061)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2705 (7.3034)  time: 0.7152 (0.4961 -- 2.2573)  data: 0.1259 (0.0002 -- 1.7106)  max mem: 16413
Epoch: [56] Total time: 0:02:21 (0.8859 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.6707 (1.5156)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2705 (7.3034)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1475 (0.1475)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3263 (2.3263 -- 2.3263)  data: 2.0903 (2.0903 -- 2.0903)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4982 (0.5221)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4121 (0.2064 -- 2.3263)  data: 0.1939 (0.0004 -- 2.0903)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5064 (0.5126)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2226 (0.1703 -- 0.3735)  data: 0.0160 (0.0001 -- 0.1915)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5241 (0.5934)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.7552)  time: 0.2055 (0.1326 -- 0.3735)  data: 0.0158 (0.0001 -- 0.1915)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 83.610 Acc@5 98.133 loss 0.641
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 87.14%
Epoch: [57]  [  0/160]  eta: 0:18:48  lr: 0.000039  min_lr: 0.000010  loss: 1.6869 (1.6869)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0509 (6.0509)  time: 7.0536 (7.0536 -- 7.0536)  data: 6.4849 (6.4849 -- 6.4849)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:41  lr: 0.000039  min_lr: 0.000010  loss: 1.6733 (1.5944)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4506 (7.1390)  time: 0.8569 (0.5288 -- 2.2014)  data: 0.3104 (0.0005 -- 1.6568)  max mem: 16413
Epoch: [57]  [ 40/160]  eta: 0:02:13  lr: 0.000039  min_lr: 0.000010  loss: 1.3795 (1.5160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4778 (7.2833)  time: 1.0686 (0.5248 -- 4.6970)  data: 0.5246 (0.0005 -- 4.1625)  max mem: 16413
[2023-09-04 17:29:44,627] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:29:44,627] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:29:44,627] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:29:44,627] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [ 60/160]  eta: 0:01:36  lr: 0.000039  min_lr: 0.000010  loss: 1.5215 (1.5247)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8607 (7.1941)  time: 0.6575 (0.5235 -- 1.9038)  data: 0.1094 (0.0003 -- 1.3537)  max mem: 16413
Epoch: [57]  [ 80/160]  eta: 0:01:16  lr: 0.000039  min_lr: 0.000010  loss: 1.4864 (1.5187)  loss_scale: 32768.0000 (22249.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6212 (7.0491)  time: 0.9507 (0.5234 -- 2.7829)  data: 0.3944 (0.0004 -- 2.2579)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000010  loss: 1.5861 (1.5324)  loss_scale: 32768.0000 (24332.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5958 (7.0508)  time: 0.8724 (0.5268 -- 2.3748)  data: 0.3276 (0.0004 -- 1.7980)  max mem: 16413
Epoch: [57]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.4911 (1.5354)  loss_scale: 32768.0000 (25726.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8051 (6.9330)  time: 0.9640 (0.5158 -- 3.5276)  data: 0.4246 (0.0003 -- 3.0022)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.6339 (1.5509)  loss_scale: 32768.0000 (26725.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5302 (7.0850)  time: 0.7720 (0.5337 -- 2.0461)  data: 0.0927 (0.0007 -- 0.7744)  max mem: 16413
[2023-09-04 17:31:09,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9271
[2023-09-04 17:31:09,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:31:09,870] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9271
[2023-09-04 17:31:09,870] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:31:09,870] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.5659 (1.5527)  loss_scale: 32768.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7425 (7.0746)  time: 0.6770 (0.4903 -- 2.7047)  data: 0.0158 (0.0002 -- 0.3012)  max mem: 16413
Epoch: [57] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.5659 (1.5677)  loss_scale: 32768.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7425 (7.0746)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1521 (0.1521)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2294 (2.2294 -- 2.2294)  data: 2.0223 (2.0223 -- 2.0223)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5858 (0.5438)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4104 (0.1870 -- 2.2294)  data: 0.1922 (0.0003 -- 2.0223)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4758 (0.5353)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2228 (0.1696 -- 0.3806)  data: 0.0125 (0.0001 -- 0.1543)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4986 (0.5871)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (99.1701)  time: 0.2063 (0.1329 -- 0.3806)  data: 0.0118 (0.0001 -- 0.1543)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 84.232 Acc@5 98.340 loss 0.621
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 87.14%
Epoch: [58]  [  0/160]  eta: 0:24:41  lr: 0.000039  min_lr: 0.000010  loss: 1.5092 (1.5092)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3757 (7.3757)  time: 9.2590 (9.2590 -- 9.2590)  data: 6.5215 (6.5215 -- 6.5215)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:44  lr: 0.000039  min_lr: 0.000010  loss: 1.2923 (1.3663)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5676 (6.5904)  time: 0.7729 (0.5261 -- 3.4862)  data: 0.0017 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [58]  [ 40/160]  eta: 0:02:06  lr: 0.000039  min_lr: 0.000010  loss: 1.5085 (1.4325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5710 (6.7962)  time: 0.9292 (0.5255 -- 3.9714)  data: 0.1904 (0.0004 -- 1.4939)  max mem: 16413
Epoch: [58]  [ 60/160]  eta: 0:01:40  lr: 0.000039  min_lr: 0.000010  loss: 1.5878 (1.4495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2556 (6.7444)  time: 0.8996 (0.5321 -- 4.8677)  data: 0.1150 (0.0003 -- 1.0744)  max mem: 16413
[2023-09-04 17:32:37,913] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9359
[2023-09-04 17:32:37,913] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9359
[2023-09-04 17:32:37,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 17:32:37,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 17:32:37,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [58]  [ 80/160]  eta: 0:01:19  lr: 0.000039  min_lr: 0.000010  loss: 1.4059 (1.4408)  loss_scale: 16384.0000 (16181.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5583 (6.8497)  time: 0.9450 (0.5360 -- 4.0859)  data: 0.0022 (0.0004 -- 0.0157)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:57  lr: 0.000039  min_lr: 0.000010  loss: 1.6303 (1.4849)  loss_scale: 8192.0000 (14599.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4447 (6.9306)  time: 0.8460 (0.5229 -- 4.0484)  data: 0.0012 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.7502 (1.5240)  loss_scale: 8192.0000 (13540.4959)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1872 (7.0001)  time: 0.7961 (0.5369 -- 2.9736)  data: 0.0013 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [58]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.6567 (1.5419)  loss_scale: 8192.0000 (12781.8440)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1529 (7.0900)  time: 0.7694 (0.5255 -- 2.8605)  data: 0.0019 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.6186 (1.5474)  loss_scale: 8192.0000 (12236.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3709 (7.0196)  time: 0.6910 (0.4964 -- 2.6518)  data: 0.0039 (0.0002 -- 0.0472)  max mem: 16413
Epoch: [58] Total time: 0:02:21 (0.8859 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.6186 (1.5629)  loss_scale: 8192.0000 (12236.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3709 (7.0196)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1344 (0.1344)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3571 (2.3571 -- 2.3571)  data: 2.1463 (2.1463 -- 2.1463)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4175 (0.5492)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4138 (0.1992 -- 2.3571)  data: 0.1967 (0.0008 -- 2.1463)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4175 (0.5191)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2177 (0.1722 -- 0.3027)  data: 0.0085 (0.0001 -- 0.0999)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4532 (0.5530)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (99.1701)  time: 0.2020 (0.1359 -- 0.3027)  data: 0.0079 (0.0001 -- 0.0999)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 86.100 Acc@5 98.548 loss 0.589
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.14%
Epoch: [59]  [  0/160]  eta: 0:19:17  lr: 0.000039  min_lr: 0.000010  loss: 1.3247 (1.3247)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3990 (7.3990)  time: 7.2334 (7.2334 -- 7.2334)  data: 6.6552 (6.6552 -- 6.6552)  max mem: 16413
Epoch: [59]  [ 20/160]  eta: 0:02:37  lr: 0.000039  min_lr: 0.000010  loss: 1.4100 (1.5225)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6343 (6.5769)  time: 0.8204 (0.5370 -- 3.4338)  data: 0.2432 (0.0003 -- 2.5044)  max mem: 16413
Epoch: [59]  [ 40/160]  eta: 0:02:05  lr: 0.000038  min_lr: 0.000010  loss: 1.6681 (1.6005)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7884 (6.7882)  time: 0.9653 (0.5291 -- 3.4441)  data: 0.4156 (0.0003 -- 2.9069)  max mem: 16413
[2023-09-04 17:34:39,684] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:34:39,685] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:34:39,725] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 17:34:39,725] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [59]  [ 60/160]  eta: 0:01:34  lr: 0.000038  min_lr: 0.000010  loss: 1.6314 (1.6002)  loss_scale: 16384.0000 (9937.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4609 (7.0218)  time: 0.7449 (0.5323 -- 3.0689)  data: 0.1830 (0.0003 -- 2.4857)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000010  loss: 1.3083 (1.5346)  loss_scale: 16384.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4385 (6.9286)  time: 0.9179 (0.5275 -- 3.3715)  data: 0.3210 (0.0003 -- 2.1577)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.7335 (1.5838)  loss_scale: 16384.0000 (12490.7723)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3326 (7.0037)  time: 0.9398 (0.5334 -- 4.2658)  data: 0.3599 (0.0004 -- 3.7165)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.5421 (1.5644)  loss_scale: 16384.0000 (13134.2810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3061 (6.9457)  time: 0.8175 (0.5253 -- 3.4339)  data: 0.2688 (0.0003 -- 2.8882)  max mem: 16413
Epoch: [59]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.4604 (1.5527)  loss_scale: 16384.0000 (13595.2340)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3028 (6.8597)  time: 0.8867 (0.5298 -- 3.8689)  data: 0.3325 (0.0009 -- 3.3456)  max mem: 16413
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.3445 (1.5373)  loss_scale: 16384.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8565 (6.8512)  time: 0.7393 (0.4956 -- 3.4691)  data: 0.2210 (0.0003 -- 2.9218)  max mem: 16413
Epoch: [59] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.3445 (1.5173)  loss_scale: 16384.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8565 (6.8512)
[2023-09-04 17:36:14,647] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-09-04 17:36:14,649] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-09-04 17:36:14,649] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-09-04 17:36:14,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-09-04 17:36:15,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-09-04 17:36:15,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1458 (0.1458)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5286 (2.5286 -- 2.5286)  data: 2.2795 (2.2795 -- 2.2795)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2210 (0.4990)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4261 (0.2032 -- 2.5286)  data: 0.2094 (0.0007 -- 2.2795)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3779 (0.4983)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.9418)  time: 0.2104 (0.1682 -- 0.2441)  data: 0.0062 (0.0001 -- 0.0682)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4446 (0.5714)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.7552)  time: 0.1944 (0.1324 -- 0.2441)  data: 0.0053 (0.0001 -- 0.0682)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 85.892 Acc@5 98.133 loss 0.598
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.14%
Epoch: [60]  [  0/160]  eta: 0:23:13  lr: 0.000038  min_lr: 0.000010  loss: 1.7640 (1.7640)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9693 (6.9693)  time: 8.7099 (8.7099 -- 8.7099)  data: 8.1704 (8.1704 -- 8.1704)  max mem: 16413
[2023-09-04 17:36:44,657] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:36:44,657] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:36:44,661] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:36:44,661] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [60]  [ 20/160]  eta: 0:02:48  lr: 0.000038  min_lr: 0.000010  loss: 1.4795 (1.4663)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2722 (6.1775)  time: 0.8283 (0.5337 -- 2.5316)  data: 0.2802 (0.0004 -- 2.0015)  max mem: 16413
Epoch: [60]  [ 40/160]  eta: 0:02:04  lr: 0.000038  min_lr: 0.000010  loss: 1.4336 (1.4801)  loss_scale: 32768.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4774 (6.5426)  time: 0.8700 (0.5303 -- 4.4468)  data: 0.3236 (0.0003 -- 3.9215)  max mem: 16413
Epoch: [60]  [ 60/160]  eta: 0:01:36  lr: 0.000038  min_lr: 0.000010  loss: 1.5285 (1.4987)  loss_scale: 32768.0000 (28470.5574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8121 (6.7696)  time: 0.8241 (0.5311 -- 2.3138)  data: 0.2725 (0.0004 -- 1.7961)  max mem: 16413
Epoch: [60]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000010  loss: 1.6875 (1.5517)  loss_scale: 32768.0000 (29531.6543)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7866 (6.7070)  time: 0.8783 (0.5296 -- 4.4512)  data: 0.2961 (0.0004 -- 3.9010)  max mem: 16413
[2023-09-04 17:37:44,649] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9682
[2023-09-04 17:37:44,649] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9682
[2023-09-04 17:37:44,649] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:37:44,649] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:37:44,649] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [60]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.5734 (1.5550)  loss_scale: 16384.0000 (27090.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6091 (6.7640)  time: 0.8843 (0.5165 -- 3.7987)  data: 0.3292 (0.0004 -- 3.2648)  max mem: 16413
Epoch: [60]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.6329 (1.5619)  loss_scale: 16384.0000 (25320.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7920 (6.8885)  time: 0.8681 (0.5138 -- 2.8779)  data: 0.3193 (0.0002 -- 2.3461)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.5850 (1.5677)  loss_scale: 16384.0000 (24053.1064)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2146 (7.0386)  time: 0.8759 (0.5303 -- 2.1876)  data: 0.2097 (0.0003 -- 1.3597)  max mem: 16413
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.5732 (1.5648)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4179 (6.9914)  time: 0.7370 (0.4966 -- 2.7302)  data: 0.2142 (0.0002 -- 2.2238)  max mem: 16413
Epoch: [60] Total time: 0:02:23 (0.8969 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.5732 (1.5488)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4179 (6.9914)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1317 (0.1317)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3256 (2.3256 -- 2.3256)  data: 2.0777 (2.0777 -- 2.0777)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.2814 (0.5297)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4091 (0.2041 -- 2.3256)  data: 0.1910 (0.0006 -- 2.0777)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4009 (0.5179)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2183 (0.1684 -- 0.3761)  data: 0.0144 (0.0001 -- 0.2020)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4668 (0.5807)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (99.1701)  time: 0.2032 (0.1324 -- 0.3761)  data: 0.0136 (0.0001 -- 0.2020)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 85.477 Acc@5 98.755 loss 0.574
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.14%
Epoch: [61]  [  0/160]  eta: 0:19:20  lr: 0.000038  min_lr: 0.000010  loss: 1.8349 (1.8349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0095 (6.0095)  time: 7.2553 (7.2553 -- 7.2553)  data: 5.6936 (5.6936 -- 5.6936)  max mem: 16413
Epoch: [61]  [ 20/160]  eta: 0:02:53  lr: 0.000038  min_lr: 0.000010  loss: 1.4522 (1.4742)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8976 (6.6855)  time: 0.9417 (0.5256 -- 4.7762)  data: 0.0435 (0.0003 -- 0.8051)  max mem: 16413
Epoch: [61]  [ 40/160]  eta: 0:02:08  lr: 0.000038  min_lr: 0.000010  loss: 1.4796 (1.5095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1077 (6.5474)  time: 0.8870 (0.5304 -- 3.3538)  data: 0.0015 (0.0003 -- 0.0028)  max mem: 16413
[2023-09-04 17:39:46,239] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:39:46,239] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:39:46,240] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:39:46,240] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [61]  [ 60/160]  eta: 0:01:38  lr: 0.000038  min_lr: 0.000010  loss: 1.4818 (1.5207)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2845 (6.5769)  time: 0.7980 (0.5155 -- 3.6289)  data: 0.0167 (0.0003 -- 0.3024)  max mem: 16413
Epoch: [61]  [ 80/160]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000010  loss: 1.5321 (1.5031)  loss_scale: 32768.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6719 (6.4866)  time: 0.9837 (0.5217 -- 4.2712)  data: 0.0012 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.5503 (1.5128)  loss_scale: 32768.0000 (24494.8911)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9802 (6.5152)  time: 0.7681 (0.5345 -- 2.7258)  data: 0.0467 (0.0002 -- 0.4707)  max mem: 16413
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000038  min_lr: 0.000010  loss: 1.4378 (1.5199)  loss_scale: 32768.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6786 (6.6571)  time: 0.8822 (0.5205 -- 3.5021)  data: 0.0324 (0.0002 -- 0.5169)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.5303 (1.5223)  loss_scale: 32768.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7558 (6.6973)  time: 0.8565 (0.5319 -- 3.7105)  data: 0.0199 (0.0003 -- 0.3571)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.7351 (1.5373)  loss_scale: 32768.0000 (27545.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0793 (6.8198)  time: 0.6956 (0.4951 -- 2.2507)  data: 0.0719 (0.0002 -- 0.9976)  max mem: 16413
Epoch: [61] Total time: 0:02:22 (0.8937 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.7351 (1.5527)  loss_scale: 32768.0000 (27545.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0793 (6.8198)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1521 (0.1521)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3051 (2.3051 -- 2.3051)  data: 2.0950 (2.0950 -- 2.0950)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4888 (0.5218)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4072 (0.1989 -- 2.3051)  data: 0.1943 (0.0007 -- 2.0950)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4242 (0.5179)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.9418)  time: 0.2233 (0.1685 -- 0.3517)  data: 0.0156 (0.0001 -- 0.1443)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4565 (0.5984)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.7552)  time: 0.2073 (0.1329 -- 0.3517)  data: 0.0149 (0.0001 -- 0.1443)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 85.892 Acc@5 98.340 loss 0.614
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.14%
Epoch: [62]  [  0/160]  eta: 0:19:53  lr: 0.000038  min_lr: 0.000010  loss: 1.4682 (1.4682)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0099 (6.0099)  time: 7.4621 (7.4621 -- 7.4621)  data: 6.9023 (6.9023 -- 6.9023)  max mem: 16413
[2023-09-04 17:41:34,682] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9923
[2023-09-04 17:41:34,683] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:41:34,683] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9923
[2023-09-04 17:41:34,683] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:41:34,684] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [62]  [ 20/160]  eta: 0:02:36  lr: 0.000038  min_lr: 0.000010  loss: 1.6240 (1.5757)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0107 (6.2621)  time: 0.7978 (0.5161 -- 2.9302)  data: 0.2483 (0.0005 -- 2.3748)  max mem: 16413
Epoch: [62]  [ 40/160]  eta: 0:02:05  lr: 0.000038  min_lr: 0.000010  loss: 1.4306 (1.5257)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1646 (6.4587)  time: 0.9781 (0.5282 -- 3.9499)  data: 0.4201 (0.0003 -- 3.4280)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:39  lr: 0.000038  min_lr: 0.000010  loss: 1.6760 (1.5463)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5605 (6.6418)  time: 0.8717 (0.5226 -- 3.8863)  data: 0.3229 (0.0004 -- 3.3067)  max mem: 16413
[2023-09-04 17:42:39,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=52, lr=[9.539276537446474e-06, 9.539276537446474e-06, 1.0599196152718305e-05, 1.0599196152718305e-05, 1.1776884614131447e-05, 1.1776884614131447e-05, 1.3085427349034943e-05, 1.3085427349034943e-05, 1.4539363721149935e-05, 1.4539363721149935e-05, 1.6154848579055485e-05, 1.6154848579055485e-05, 1.794983175450609e-05, 1.794983175450609e-05, 1.9944257505006766e-05, 1.9944257505006766e-05, 2.2160286116674188e-05, 2.2160286116674188e-05, 2.4622540129637982e-05, 2.4622540129637982e-05, 2.7358377921819982e-05, 2.7358377921819982e-05, 3.0398197690911092e-05, 3.0398197690911092e-05, 3.377577521212343e-05, 3.377577521212343e-05, 3.752863912458159e-05, 3.752863912458159e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 17:42:39,815] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=18.549865350232615, CurrSamplesPerSec=22.46077213795184, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:17  lr: 0.000038  min_lr: 0.000010  loss: 1.4367 (1.5356)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5821 (6.6683)  time: 0.8842 (0.5208 -- 3.8992)  data: 0.2605 (0.0005 -- 1.9393)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:55  lr: 0.000037  min_lr: 0.000010  loss: 1.5344 (1.5408)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5458 (6.7118)  time: 0.8077 (0.5208 -- 3.9852)  data: 0.0509 (0.0003 -- 0.7468)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000010  loss: 1.7016 (1.5579)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6896 (6.8547)  time: 0.8227 (0.5271 -- 3.3386)  data: 0.0019 (0.0005 -- 0.0058)  max mem: 16413
[2023-09-04 17:43:26,466] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:43:26,466] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:43:26,469] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:43:26,470] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000010  loss: 1.5488 (1.5575)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8326 (6.7636)  time: 0.8667 (0.5247 -- 2.9405)  data: 0.1539 (0.0008 -- 1.2095)  max mem: 16413
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 1.5632 (1.5613)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9270 (6.7410)  time: 0.6882 (0.4946 -- 2.2149)  data: 0.1613 (0.0002 -- 1.6757)  max mem: 16413
Epoch: [62] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 1.5632 (1.5273)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9270 (6.7410)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1569 (0.1569)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3856 (2.3856 -- 2.3856)  data: 2.1648 (2.1648 -- 2.1648)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4254 (0.5258)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4260 (0.2009 -- 2.3856)  data: 0.2082 (0.0006 -- 2.1648)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4391 (0.5288)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.9418)  time: 0.2223 (0.1686 -- 0.3347)  data: 0.0149 (0.0001 -- 0.1154)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4823 (0.5958)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (98.7552)  time: 0.2079 (0.1331 -- 0.3347)  data: 0.0146 (0.0001 -- 0.1154)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 84.440 Acc@5 98.548 loss 0.605
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 87.14%
Epoch: [63]  [  0/160]  eta: 0:20:11  lr: 0.000037  min_lr: 0.000010  loss: 1.7525 (1.7525)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9715 (8.9715)  time: 7.5718 (7.5718 -- 7.5718)  data: 6.0276 (6.0276 -- 6.0276)  max mem: 16413
Epoch: [63]  [ 20/160]  eta: 0:03:00  lr: 0.000037  min_lr: 0.000009  loss: 1.6054 (1.5397)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5420 (6.9542)  time: 0.9780 (0.5285 -- 4.5035)  data: 0.0520 (0.0006 -- 0.5004)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:02:01  lr: 0.000037  min_lr: 0.000009  loss: 1.5637 (1.5378)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6892 (6.4751)  time: 0.7273 (0.5242 -- 3.1178)  data: 0.0025 (0.0003 -- 0.0178)  max mem: 16413
Epoch: [63]  [ 60/160]  eta: 0:01:37  lr: 0.000037  min_lr: 0.000009  loss: 1.5315 (1.5302)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1349 (6.5944)  time: 0.8848 (0.5258 -- 3.8730)  data: 0.0716 (0.0008 -- 1.2656)  max mem: 16413
Epoch: [63]  [ 80/160]  eta: 0:01:15  lr: 0.000037  min_lr: 0.000009  loss: 1.3882 (1.5118)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3043 (6.6657)  time: 0.8312 (0.5232 -- 2.6185)  data: 0.2693 (0.0005 -- 2.1056)  max mem: 16413
[2023-09-04 17:45:29,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:45:29,268] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 17:45:29,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:45:29,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [63]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.5808 (1.5297)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7108 (6.7698)  time: 0.9273 (0.5167 -- 3.6608)  data: 0.3387 (0.0003 -- 3.1235)  max mem: 16413
[2023-09-04 17:45:32,899] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10185
[2023-09-04 17:45:32,899] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10185
[2023-09-04 17:45:32,899] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:45:32,899] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 17:45:32,899] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [63]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000009  loss: 1.6559 (1.5414)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2603 (6.7768)  time: 0.8342 (0.5323 -- 2.7865)  data: 0.1407 (0.0008 -- 2.2660)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.6116 (1.5355)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3274 (6.7997)  time: 0.9102 (0.5273 -- 2.4262)  data: 0.1475 (0.0002 -- 1.1824)  max mem: 16413
[2023-09-04 17:46:15,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10236
[2023-09-04 17:46:15,318] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:46:15,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10236
[2023-09-04 17:46:15,318] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:46:15,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.4774 (1.5294)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0562 (6.8244)  time: 0.6575 (0.4858 -- 2.2590)  data: 0.1167 (0.0002 -- 1.7406)  max mem: 16413
Epoch: [63] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.4774 (1.5390)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0562 (6.8244)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1367 (0.1367)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2882 (2.2882 -- 2.2882)  data: 2.0846 (2.0846 -- 2.0846)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4118 (0.5592)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4057 (0.2053 -- 2.2882)  data: 0.1904 (0.0004 -- 2.0846)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4659 (0.5341)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (98.4127)  time: 0.2344 (0.1681 -- 0.7550)  data: 0.0298 (0.0001 -- 0.5837)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4827 (0.6124)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (98.3402)  time: 0.2175 (0.1322 -- 0.7550)  data: 0.0295 (0.0001 -- 0.5837)  max mem: 16413
Val: Total time: 0:00:07 (0.2948 s / it)
* Acc@1 82.988 Acc@5 97.718 loss 0.613
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 87.14%
Epoch: [64]  [  0/160]  eta: 0:17:51  lr: 0.000037  min_lr: 0.000009  loss: 1.1963 (1.1963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1006 (4.1006)  time: 6.6956 (6.6956 -- 6.6956)  data: 6.1524 (6.1524 -- 6.1524)  max mem: 16413
Epoch: [64]  [ 20/160]  eta: 0:02:40  lr: 0.000037  min_lr: 0.000009  loss: 1.6070 (1.6267)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8595 (7.3378)  time: 0.8718 (0.5324 -- 3.3671)  data: 0.3012 (0.0011 -- 2.8141)  max mem: 16413
Epoch: [64]  [ 40/160]  eta: 0:02:02  lr: 0.000037  min_lr: 0.000009  loss: 1.6078 (1.6098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1290 (7.5279)  time: 0.8794 (0.5191 -- 2.3725)  data: 0.2021 (0.0004 -- 1.1179)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000009  loss: 1.3969 (1.5566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0883 (7.2369)  time: 0.9383 (0.5323 -- 2.7450)  data: 0.2570 (0.0006 -- 2.2050)  max mem: 16413
Epoch: [64]  [ 80/160]  eta: 0:01:15  lr: 0.000037  min_lr: 0.000009  loss: 1.4014 (1.5401)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8861 (7.1721)  time: 0.7800 (0.5308 -- 2.7930)  data: 0.0883 (0.0002 -- 0.8543)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.6723 (1.5615)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4981 (7.2978)  time: 0.9279 (0.5239 -- 3.8437)  data: 0.3273 (0.0003 -- 3.3335)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000009  loss: 1.6422 (1.5832)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2058 (7.3091)  time: 0.8281 (0.5175 -- 3.4943)  data: 0.2459 (0.0005 -- 2.9585)  max mem: 16413
[2023-09-04 17:48:21,842] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:48:21,842] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:48:21,842] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:48:21,842] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.3812 (1.5619)  loss_scale: 32768.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5939 (7.2021)  time: 0.8614 (0.5220 -- 3.4663)  data: 0.1840 (0.0004 -- 2.9301)  max mem: 16413
[2023-09-04 17:48:43,773] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10394
[2023-09-04 17:48:43,773] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:48:43,773] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10394
[2023-09-04 17:48:43,773] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:48:43,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.5641 (1.5553)  loss_scale: 32768.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1131 (7.2601)  time: 0.6780 (0.4964 -- 3.4556)  data: 0.0010 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [64] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.5641 (1.5517)  loss_scale: 32768.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1131 (7.2601)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1296 (0.1296)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3614 (2.3614 -- 2.3614)  data: 2.1162 (2.1162 -- 2.1162)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5380 (0.5075)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4257 (0.2063 -- 2.3614)  data: 0.2025 (0.0006 -- 2.1162)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3444 (0.4740)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2210 (0.1712 -- 0.3147)  data: 0.0092 (0.0001 -- 0.0897)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4858 (0.5427)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2037 (0.1336 -- 0.3147)  data: 0.0083 (0.0001 -- 0.0897)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 86.100 Acc@5 98.133 loss 0.563
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.14%
Epoch: [65]  [  0/160]  eta: 0:15:35  lr: 0.000037  min_lr: 0.000009  loss: 1.1074 (1.1074)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3610 (7.3610)  time: 5.8478 (5.8478 -- 5.8478)  data: 4.9276 (4.9276 -- 4.9276)  max mem: 16413
Epoch: [65]  [ 20/160]  eta: 0:02:44  lr: 0.000037  min_lr: 0.000009  loss: 1.5629 (1.5073)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6827 (7.0414)  time: 0.9434 (0.5267 -- 2.9874)  data: 0.1334 (0.0003 -- 1.4169)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:03  lr: 0.000037  min_lr: 0.000009  loss: 1.2889 (1.4100)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6006 (6.4355)  time: 0.8770 (0.5256 -- 4.5050)  data: 0.2629 (0.0002 -- 3.9724)  max mem: 16413
Epoch: [65]  [ 60/160]  eta: 0:01:40  lr: 0.000037  min_lr: 0.000009  loss: 1.4740 (1.4277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6016 (6.6259)  time: 0.9487 (0.5193 -- 2.4783)  data: 0.2035 (0.0005 -- 1.3856)  max mem: 16413
Epoch: [65]  [ 80/160]  eta: 0:01:14  lr: 0.000037  min_lr: 0.000009  loss: 1.7079 (1.4640)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7020 (6.6210)  time: 0.7231 (0.5271 -- 2.7744)  data: 0.1750 (0.0003 -- 2.2390)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.5381 (1.4686)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1688 (6.6368)  time: 0.9923 (0.5304 -- 4.3752)  data: 0.2505 (0.0004 -- 3.8662)  max mem: 16413
Epoch: [65]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000009  loss: 1.5283 (1.4883)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1946 (6.6152)  time: 0.8436 (0.5256 -- 2.5619)  data: 0.1327 (0.0004 -- 1.5481)  max mem: 16413
[2023-09-04 17:50:51,172] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:50:51,172] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:50:51,172] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:50:51,172] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.3170 (1.4799)  loss_scale: 32768.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5246 (6.7465)  time: 0.9004 (0.5274 -- 3.7306)  data: 0.3549 (0.0003 -- 3.2148)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.4959 (1.4886)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8049 (6.7417)  time: 0.7357 (0.4945 -- 2.5611)  data: 0.0894 (0.0002 -- 1.7740)  max mem: 16413
Epoch: [65] Total time: 0:02:24 (0.9037 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.4959 (1.5297)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8049 (6.7417)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1376 (0.1376)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4079 (2.4079 -- 2.4079)  data: 2.1850 (2.1850 -- 2.1850)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2229 (0.4473)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (100.0000)  time: 0.4155 (0.1975 -- 2.4079)  data: 0.1998 (0.0009 -- 2.1850)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3801 (0.4559)  acc1: 88.8889 (91.0053)  acc5: 100.0000 (98.4127)  time: 0.2152 (0.1727 -- 0.3325)  data: 0.0086 (0.0001 -- 0.1568)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4591 (0.4984)  acc1: 88.8889 (89.2116)  acc5: 100.0000 (97.9253)  time: 0.2010 (0.1326 -- 0.3325)  data: 0.0083 (0.0001 -- 0.1568)  max mem: 16413
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 87.552 Acc@5 97.925 loss 0.539
Accuracy of the network on the 482 val images: 87.55%
[2023-09-04 17:51:26,380] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 17:51:26,382] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 17:51:26,382] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 17:51:26,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 17:51:27,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 17:51:27,753] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 87.55%
Epoch: [66]  [  0/160]  eta: 0:16:10  lr: 0.000036  min_lr: 0.000009  loss: 0.9842 (0.9842)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6543 (8.6543)  time: 6.0674 (6.0674 -- 6.0674)  data: 4.4040 (4.4040 -- 4.4040)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:45  lr: 0.000036  min_lr: 0.000009  loss: 1.6576 (1.5251)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4003 (6.9643)  time: 0.9382 (0.5179 -- 3.1896)  data: 0.3914 (0.0004 -- 2.6656)  max mem: 16413
Epoch: [66]  [ 40/160]  eta: 0:02:03  lr: 0.000036  min_lr: 0.000009  loss: 1.4729 (1.5259)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4027 (6.7905)  time: 0.8660 (0.5215 -- 3.7277)  data: 0.3226 (0.0003 -- 3.2213)  max mem: 16413
[2023-09-04 17:52:13,420] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10601
[2023-09-04 17:52:13,420] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10601
[2023-09-04 17:52:13,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:52:13,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:52:13,421] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [ 60/160]  eta: 0:01:42  lr: 0.000036  min_lr: 0.000009  loss: 1.4765 (1.5197)  loss_scale: 16384.0000 (27396.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4374 (6.5951)  time: 1.0110 (0.5255 -- 4.2149)  data: 0.4616 (0.0006 -- 3.6894)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000009  loss: 1.6701 (1.5642)  loss_scale: 16384.0000 (24677.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6478 (6.7052)  time: 0.7433 (0.5287 -- 2.6452)  data: 0.1859 (0.0003 -- 2.0849)  max mem: 16413
Epoch: [66]  [100/160]  eta: 0:00:58  lr: 0.000036  min_lr: 0.000009  loss: 1.4178 (1.5469)  loss_scale: 16384.0000 (23034.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9729 (6.7263)  time: 1.0310 (0.5233 -- 4.9675)  data: 0.4736 (0.0004 -- 4.4388)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.7216 (1.5722)  loss_scale: 16384.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6788 (6.7964)  time: 0.8527 (0.5178 -- 3.6640)  data: 0.3109 (0.0002 -- 3.1230)  max mem: 16413
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.5218 (1.5665)  loss_scale: 16384.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4193 (6.9276)  time: 0.9455 (0.5194 -- 3.7761)  data: 0.4041 (0.0003 -- 3.2535)  max mem: 16413
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.4463 (1.5507)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4187 (6.8792)  time: 0.6024 (0.4937 -- 2.2889)  data: 0.0891 (0.0002 -- 1.7708)  max mem: 16413
Epoch: [66] Total time: 0:02:25 (0.9084 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.4463 (1.5743)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4187 (6.8792)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1292 (0.1292)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2882 (2.2882 -- 2.2882)  data: 2.0689 (2.0689 -- 2.0689)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3887 (0.5024)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4142 (0.1991 -- 2.2882)  data: 0.2034 (0.0007 -- 2.0689)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3471 (0.4778)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2147 (0.1704 -- 0.3569)  data: 0.0101 (0.0001 -- 0.1582)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3595 (0.5255)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (97.9253)  time: 0.1993 (0.1327 -- 0.3569)  data: 0.0098 (0.0001 -- 0.1582)  max mem: 16413
Val: Total time: 0:00:07 (0.2809 s / it)
* Acc@1 86.100 Acc@5 97.925 loss 0.574
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.55%
Epoch: [67]  [  0/160]  eta: 0:17:17  lr: 0.000036  min_lr: 0.000009  loss: 1.6096 (1.6096)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3892 (3.3892)  time: 6.4862 (6.4862 -- 6.4862)  data: 5.9508 (5.9508 -- 5.9508)  max mem: 16413
[2023-09-04 17:54:17,535] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:54:17,536] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:54:17,536] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:54:17,536] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [67]  [ 20/160]  eta: 0:02:46  lr: 0.000036  min_lr: 0.000009  loss: 1.4447 (1.5243)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9440 (6.3811)  time: 0.9251 (0.5281 -- 3.8328)  data: 0.3682 (0.0004 -- 3.2711)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:02:10  lr: 0.000036  min_lr: 0.000009  loss: 1.4226 (1.5043)  loss_scale: 32768.0000 (28771.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4296 (6.4526)  time: 0.9801 (0.5163 -- 4.1131)  data: 0.4390 (0.0003 -- 3.5677)  max mem: 16413
Epoch: [67]  [ 60/160]  eta: 0:01:37  lr: 0.000036  min_lr: 0.000009  loss: 1.5097 (1.4934)  loss_scale: 32768.0000 (30082.0984)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5549 (6.5246)  time: 0.7305 (0.5197 -- 3.3088)  data: 0.1881 (0.0004 -- 2.8014)  max mem: 16413
[2023-09-04 17:55:06,826] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10785
[2023-09-04 17:55:06,826] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10785
[2023-09-04 17:55:06,826] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:55:06,826] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:55:06,827] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [67]  [ 80/160]  eta: 0:01:18  lr: 0.000036  min_lr: 0.000009  loss: 1.4862 (1.5032)  loss_scale: 16384.0000 (27508.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9688 (6.5345)  time: 1.0337 (0.5141 -- 4.5676)  data: 0.4929 (0.0003 -- 4.0398)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000009  loss: 1.4892 (1.5220)  loss_scale: 16384.0000 (25305.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3550 (6.6957)  time: 0.8142 (0.5234 -- 3.4494)  data: 0.2712 (0.0004 -- 2.9329)  max mem: 16413
Epoch: [67]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.3885 (1.5083)  loss_scale: 16384.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4231 (6.7249)  time: 0.9344 (0.5222 -- 3.6623)  data: 0.3838 (0.0004 -- 3.1405)  max mem: 16413
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.5821 (1.5131)  loss_scale: 16384.0000 (22774.9220)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7353 (6.6890)  time: 0.7946 (0.5115 -- 3.8133)  data: 0.2542 (0.0004 -- 3.3031)  max mem: 16413
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.6058 (1.5170)  loss_scale: 16384.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8707 (6.6916)  time: 0.6508 (0.4985 -- 1.8148)  data: 0.1210 (0.0002 -- 1.2703)  max mem: 16413
Epoch: [67] Total time: 0:02:23 (0.8951 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.6058 (1.5100)  loss_scale: 16384.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8707 (6.6916)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1264 (0.1264)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3338 (2.3338 -- 2.3338)  data: 2.0956 (2.0956 -- 2.0956)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4235 (0.5023)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4173 (0.2058 -- 2.3338)  data: 0.1935 (0.0005 -- 2.0956)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4017 (0.4835)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2214 (0.1691 -- 0.4604)  data: 0.0153 (0.0001 -- 0.2702)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4017 (0.5151)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (97.9253)  time: 0.2044 (0.1333 -- 0.4604)  data: 0.0149 (0.0001 -- 0.2702)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 86.100 Acc@5 97.925 loss 0.563
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.55%
Epoch: [68]  [  0/160]  eta: 0:22:51  lr: 0.000036  min_lr: 0.000009  loss: 1.9366 (1.9366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9928 (3.9928)  time: 8.5693 (8.5693 -- 8.5693)  data: 8.0069 (8.0069 -- 8.0069)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:45  lr: 0.000036  min_lr: 0.000009  loss: 1.4639 (1.4377)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9413 (6.6819)  time: 0.8092 (0.5332 -- 3.2915)  data: 0.2583 (0.0004 -- 2.7555)  max mem: 16413
[2023-09-04 17:57:08,666] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:57:08,666] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 17:57:08,667] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 17:57:08,667] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [68]  [ 40/160]  eta: 0:02:10  lr: 0.000036  min_lr: 0.000009  loss: 1.3806 (1.4344)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5129 (6.5903)  time: 0.9867 (0.5158 -- 3.4810)  data: 0.0736 (0.0001 -- 1.4278)  max mem: 16413
Epoch: [68]  [ 60/160]  eta: 0:01:38  lr: 0.000036  min_lr: 0.000009  loss: 1.5360 (1.4398)  loss_scale: 32768.0000 (23635.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8564 (6.5431)  time: 0.7853 (0.5257 -- 2.9127)  data: 0.0014 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000009  loss: 1.5708 (1.4907)  loss_scale: 32768.0000 (25890.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3922 (6.8597)  time: 0.8480 (0.5233 -- 2.9431)  data: 0.0580 (0.0004 -- 1.0910)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:56  lr: 0.000036  min_lr: 0.000009  loss: 1.5981 (1.5137)  loss_scale: 32768.0000 (27252.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3461 (6.9073)  time: 0.9079 (0.5075 -- 3.8830)  data: 0.3204 (0.0003 -- 3.3450)  max mem: 16413
[2023-09-04 17:58:26,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=57, lr=[9.042925336597765e-06, 9.042925336597765e-06, 1.0047694818441962e-05, 1.0047694818441962e-05, 1.1164105353824401e-05, 1.1164105353824401e-05, 1.2404561504249335e-05, 1.2404561504249335e-05, 1.3782846115832595e-05, 1.3782846115832595e-05, 1.5314273462036215e-05, 1.5314273462036215e-05, 1.701585940226246e-05, 1.701585940226246e-05, 1.890651044695829e-05, 1.890651044695829e-05, 2.1007233829953654e-05, 2.1007233829953654e-05, 2.3341370922170724e-05, 2.3341370922170724e-05, 2.59348565801897e-05, 2.59348565801897e-05, 2.8816507311321884e-05, 2.8816507311321884e-05, 3.2018341457024315e-05, 3.2018341457024315e-05, 3.557593495224924e-05, 3.557593495224924e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 17:58:26,403] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=18.577411306357476, CurrSamplesPerSec=22.89464124639568, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:38  lr: 0.000036  min_lr: 0.000009  loss: 1.5211 (1.5181)  loss_scale: 32768.0000 (28164.2314)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4819 (6.9922)  time: 0.9916 (0.5344 -- 5.0267)  data: 0.0024 (0.0003 -- 0.0163)  max mem: 16413
[2023-09-04 17:58:38,131] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11014
[2023-09-04 17:58:38,131] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:58:38,131] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11014
[2023-09-04 17:58:38,132] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 17:58:38,132] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.7862 (1.5466)  loss_scale: 32768.0000 (28003.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2197 (7.0694)  time: 0.7985 (0.5233 -- 2.6380)  data: 0.0687 (0.0004 -- 1.3402)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.5783 (1.5459)  loss_scale: 16384.0000 (26624.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1963 (7.0064)  time: 0.6888 (0.4944 -- 1.9838)  data: 0.0743 (0.0002 -- 1.4719)  max mem: 16413
Epoch: [68] Total time: 0:02:24 (0.9021 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.5783 (1.5379)  loss_scale: 16384.0000 (26624.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1963 (7.0064)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1354 (0.1354)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2288 (2.2288 -- 2.2288)  data: 1.9810 (1.9810 -- 1.9810)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5019 (0.5505)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4099 (0.2066 -- 2.2288)  data: 0.1834 (0.0010 -- 1.9810)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3222 (0.5056)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2248 (0.1696 -- 0.3725)  data: 0.0116 (0.0001 -- 0.1916)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3447 (0.5447)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (99.1701)  time: 0.2049 (0.1333 -- 0.3725)  data: 0.0108 (0.0001 -- 0.1916)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 84.647 Acc@5 98.340 loss 0.616
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 87.55%
Epoch: [69]  [  0/160]  eta: 0:18:59  lr: 0.000035  min_lr: 0.000009  loss: 2.1586 (2.1586)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4148 (6.4148)  time: 7.1242 (7.1242 -- 7.1242)  data: 5.7204 (5.7204 -- 5.7204)  max mem: 16413
Epoch: [69]  [ 20/160]  eta: 0:02:49  lr: 0.000035  min_lr: 0.000009  loss: 1.6017 (1.5994)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0368 (7.2733)  time: 0.9185 (0.5337 -- 3.5430)  data: 0.3623 (0.0008 -- 3.0055)  max mem: 16413
Epoch: [69]  [ 40/160]  eta: 0:02:06  lr: 0.000035  min_lr: 0.000009  loss: 1.5441 (1.5849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6846 (7.3735)  time: 0.8923 (0.5195 -- 2.6388)  data: 0.2870 (0.0006 -- 2.0843)  max mem: 16413
Epoch: [69]  [ 60/160]  eta: 0:01:37  lr: 0.000035  min_lr: 0.000009  loss: 1.6719 (1.6293)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6047 (7.4119)  time: 0.8035 (0.5417 -- 3.1276)  data: 0.2480 (0.0003 -- 2.6010)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:16  lr: 0.000035  min_lr: 0.000009  loss: 1.6855 (1.6150)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7427 (7.4018)  time: 0.9276 (0.5365 -- 3.2626)  data: 0.2309 (0.0008 -- 2.7032)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:57  lr: 0.000035  min_lr: 0.000009  loss: 1.6266 (1.6039)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3883 (7.3146)  time: 0.9363 (0.5210 -- 4.3549)  data: 0.0216 (0.0003 -- 0.4061)  max mem: 16413
[2023-09-04 18:00:45,697] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:00:45,698] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:00:45,697] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:00:45,698] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [69]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 1.5669 (1.6112)  loss_scale: 32768.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9106 (7.2836)  time: 0.8798 (0.5176 -- 4.0759)  data: 0.0011 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.5855 (1.6024)  loss_scale: 32768.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4616 (7.2794)  time: 0.8829 (0.5218 -- 4.1560)  data: 0.0014 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.4172 (1.5817)  loss_scale: 32768.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4616 (7.2139)  time: 0.6602 (0.4966 -- 1.9963)  data: 0.0014 (0.0002 -- 0.0142)  max mem: 16413
Epoch: [69] Total time: 0:02:24 (0.9039 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.4172 (1.5553)  loss_scale: 32768.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4616 (7.2139)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1505 (0.1505)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3943 (2.3943 -- 2.3943)  data: 2.1556 (2.1556 -- 2.1556)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3892 (0.5025)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4266 (0.2120 -- 2.3943)  data: 0.1995 (0.0009 -- 2.1556)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3997 (0.5046)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.9418)  time: 0.2235 (0.1701 -- 0.4305)  data: 0.0145 (0.0001 -- 0.2487)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5986 (0.5541)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.7552)  time: 0.2046 (0.1327 -- 0.4305)  data: 0.0135 (0.0001 -- 0.2487)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 85.685 Acc@5 98.133 loss 0.602
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.55%
Epoch: [70]  [  0/160]  eta: 0:18:54  lr: 0.000035  min_lr: 0.000009  loss: 1.5590 (1.5590)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9028 (7.9028)  time: 7.0885 (7.0885 -- 7.0885)  data: 6.5275 (6.5275 -- 6.5275)  max mem: 16413
Epoch: [70]  [ 20/160]  eta: 0:02:54  lr: 0.000035  min_lr: 0.000009  loss: 1.4469 (1.5164)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5003 (7.5804)  time: 0.9510 (0.5305 -- 4.2271)  data: 0.3745 (0.0007 -- 3.6749)  max mem: 16413
[2023-09-04 18:02:09,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11230
[2023-09-04 18:02:09,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:02:09,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11230
[2023-09-04 18:02:09,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:02:09,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [70]  [ 40/160]  eta: 0:02:01  lr: 0.000035  min_lr: 0.000009  loss: 1.3744 (1.4980)  loss_scale: 16384.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7240 (7.3752)  time: 0.7770 (0.5208 -- 2.9740)  data: 0.2235 (0.0002 -- 2.4054)  max mem: 16413
Epoch: [70]  [ 60/160]  eta: 0:01:41  lr: 0.000035  min_lr: 0.000009  loss: 1.4061 (1.4955)  loss_scale: 16384.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7328 (7.2555)  time: 1.0131 (0.5154 -- 3.5834)  data: 0.4090 (0.0005 -- 3.0521)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:18  lr: 0.000035  min_lr: 0.000009  loss: 1.4990 (1.5161)  loss_scale: 16384.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8250 (7.4338)  time: 0.8677 (0.5207 -- 3.4964)  data: 0.0340 (0.0002 -- 0.6576)  max mem: 16413
Epoch: [70]  [100/160]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000009  loss: 1.7333 (1.5461)  loss_scale: 16384.0000 (21250.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6386 (7.3414)  time: 0.7960 (0.5239 -- 2.8282)  data: 0.0176 (0.0006 -- 0.3267)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000009  loss: 1.5049 (1.5465)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7419 (7.3852)  time: 0.7700 (0.5433 -- 1.5911)  data: 0.0698 (0.0005 -- 0.7417)  max mem: 16413
Epoch: [70]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.5719 (1.5517)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9260 (7.2904)  time: 0.9007 (0.5302 -- 3.6268)  data: 0.1927 (0.0005 -- 3.0537)  max mem: 16413
[2023-09-04 18:03:59,166] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:03:59,166] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:03:59,166] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:03:59,166] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.6872 (1.5604)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7397 (7.3578)  time: 0.7396 (0.4938 -- 2.8504)  data: 0.0967 (0.0002 -- 1.1634)  max mem: 16413
Epoch: [70] Total time: 0:02:22 (0.8929 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.6872 (1.5383)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7397 (7.3578)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.1497 (0.1497)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1822 (2.1822 -- 2.1822)  data: 1.9711 (1.9711 -- 1.9711)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4376 (0.5083)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4214 (0.1955 -- 2.1822)  data: 0.2038 (0.0007 -- 1.9711)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4575 (0.5059)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (99.4709)  time: 0.2267 (0.1693 -- 0.4400)  data: 0.0191 (0.0001 -- 0.2271)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4575 (0.5488)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (99.1701)  time: 0.2104 (0.1381 -- 0.4400)  data: 0.0186 (0.0001 -- 0.2271)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 86.100 Acc@5 98.755 loss 0.589
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.55%
Epoch: [71]  [  0/160]  eta: 0:19:29  lr: 0.000035  min_lr: 0.000009  loss: 1.0844 (1.0844)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7399 (4.7399)  time: 7.3094 (7.3094 -- 7.3094)  data: 6.5580 (6.5580 -- 6.5580)  max mem: 16413
Epoch: [71]  [ 20/160]  eta: 0:02:48  lr: 0.000035  min_lr: 0.000009  loss: 1.4912 (1.4601)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1684 (7.3647)  time: 0.9011 (0.5291 -- 3.4802)  data: 0.3527 (0.0003 -- 2.9675)  max mem: 16413
Epoch: [71]  [ 40/160]  eta: 0:02:04  lr: 0.000035  min_lr: 0.000009  loss: 1.2928 (1.4357)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0886 (6.5229)  time: 0.8563 (0.5176 -- 3.3516)  data: 0.0541 (0.0001 -- 1.0521)  max mem: 16413
[2023-09-04 18:05:04,990] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11417
[2023-09-04 18:05:04,990] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11417
[2023-09-04 18:05:04,990] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:05:04,990] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:05:04,990] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [71]  [ 60/160]  eta: 0:01:37  lr: 0.000035  min_lr: 0.000009  loss: 1.6366 (1.4797)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0235 (6.9168)  time: 0.8629 (0.5107 -- 3.7769)  data: 0.2813 (0.0003 -- 3.2543)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:16  lr: 0.000035  min_lr: 0.000009  loss: 1.3322 (1.4678)  loss_scale: 16384.0000 (27913.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1215 (6.9810)  time: 0.9045 (0.5297 -- 3.5921)  data: 0.3568 (0.0007 -- 3.0702)  max mem: 16413
Epoch: [71]  [100/160]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000009  loss: 1.6435 (1.4819)  loss_scale: 16384.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3486 (7.0403)  time: 0.8319 (0.5320 -- 2.9602)  data: 0.2806 (0.0003 -- 2.3692)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 1.5510 (1.5062)  loss_scale: 16384.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4940 (7.0978)  time: 1.0253 (0.5193 -- 4.3246)  data: 0.4769 (0.0002 -- 3.7809)  max mem: 16413
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.7451 (1.5299)  loss_scale: 16384.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4487 (7.0679)  time: 0.7856 (0.5225 -- 4.0858)  data: 0.2400 (0.0002 -- 3.5489)  max mem: 16413
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.6141 (1.5438)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1875 (7.0532)  time: 0.6499 (0.4940 -- 2.8956)  data: 0.1288 (0.0002 -- 2.3576)  max mem: 16413
Epoch: [71] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.6141 (1.5088)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1875 (7.0532)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1422 (0.1422)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3595 (2.3595 -- 2.3595)  data: 2.1601 (2.1601 -- 2.1601)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2841 (0.4970)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4159 (0.1990 -- 2.3595)  data: 0.2045 (0.0007 -- 2.1601)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3633 (0.4707)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2201 (0.1686 -- 0.3113)  data: 0.0146 (0.0001 -- 0.1090)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.3633 (0.5186)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (99.1701)  time: 0.2042 (0.1331 -- 0.3113)  data: 0.0143 (0.0001 -- 0.1090)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 84.647 Acc@5 98.133 loss 0.574
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 87.55%
Epoch: [72]  [  0/160]  eta: 0:17:46  lr: 0.000035  min_lr: 0.000009  loss: 1.2615 (1.2615)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3823 (6.3823)  time: 6.6648 (6.6648 -- 6.6648)  data: 5.6815 (5.6815 -- 5.6815)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:02:46  lr: 0.000034  min_lr: 0.000009  loss: 1.4077 (1.4753)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5612 (8.1654)  time: 0.9189 (0.5243 -- 2.9049)  data: 0.3637 (0.0009 -- 2.3752)  max mem: 16413
[2023-09-04 18:07:07,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:07:07,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:07:07,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:07:07,753] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [72]  [ 40/160]  eta: 0:02:00  lr: 0.000034  min_lr: 0.000009  loss: 1.4633 (1.4944)  loss_scale: 32768.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9684 (8.0496)  time: 0.8081 (0.5322 -- 2.5031)  data: 0.2567 (0.0002 -- 1.9713)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:39  lr: 0.000034  min_lr: 0.000009  loss: 1.4136 (1.4662)  loss_scale: 32768.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5405 (7.7087)  time: 0.9695 (0.5380 -- 3.7624)  data: 0.4192 (0.0008 -- 3.2149)  max mem: 16413
Epoch: [72]  [ 80/160]  eta: 0:01:16  lr: 0.000034  min_lr: 0.000009  loss: 1.5260 (1.4804)  loss_scale: 32768.0000 (27508.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0212 (7.5646)  time: 0.8197 (0.5356 -- 3.2675)  data: 0.2641 (0.0002 -- 2.7406)  max mem: 16413
Epoch: [72]  [100/160]  eta: 0:00:56  lr: 0.000034  min_lr: 0.000009  loss: 1.5651 (1.4903)  loss_scale: 32768.0000 (28550.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5280 (7.4132)  time: 0.9276 (0.5159 -- 4.5985)  data: 0.3851 (0.0004 -- 4.0787)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000009  loss: 1.5805 (1.4910)  loss_scale: 32768.0000 (29247.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7295 (7.3642)  time: 0.8408 (0.5309 -- 4.1097)  data: 0.2943 (0.0004 -- 3.5648)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.6688 (1.5036)  loss_scale: 32768.0000 (29746.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8719 (7.2379)  time: 0.9432 (0.5301 -- 3.8496)  data: 0.3885 (0.0004 -- 3.2841)  max mem: 16413
[2023-09-04 18:08:58,817] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:08:58,817] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 18:08:58,818] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:08:58,818] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 18:09:00,306] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11677
[2023-09-04 18:09:00,306] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 18:09:00,306] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11677
[2023-09-04 18:09:00,306] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 18:09:00,306] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.5020 (1.5018)  loss_scale: 32768.0000 (30720.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3911 (7.1699)  time: 0.6422 (0.4839 -- 2.0096)  data: 0.1275 (0.0002 -- 1.4703)  max mem: 16413
Epoch: [72] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.5020 (1.5128)  loss_scale: 32768.0000 (30720.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3911 (7.1699)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1422 (0.1422)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2753 (2.2753 -- 2.2753)  data: 2.0393 (2.0393 -- 2.0393)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4713 (0.5544)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4139 (0.1985 -- 2.2753)  data: 0.1871 (0.0006 -- 2.0393)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4338 (0.5102)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (99.4709)  time: 0.2231 (0.1703 -- 0.3977)  data: 0.0121 (0.0001 -- 0.2184)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4338 (0.5589)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (98.7552)  time: 0.2032 (0.1329 -- 0.3977)  data: 0.0118 (0.0001 -- 0.2184)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 84.647 Acc@5 97.718 loss 0.588
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 87.55%
Epoch: [73]  [  0/160]  eta: 0:19:57  lr: 0.000034  min_lr: 0.000009  loss: 2.0425 (2.0425)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7904 (6.7904)  time: 7.4865 (7.4865 -- 7.4865)  data: 5.9820 (5.9820 -- 5.9820)  max mem: 16413
Epoch: [73]  [ 20/160]  eta: 0:02:48  lr: 0.000034  min_lr: 0.000009  loss: 1.4732 (1.5268)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2036 (6.9572)  time: 0.8917 (0.5314 -- 2.8734)  data: 0.0206 (0.0003 -- 0.3828)  max mem: 16413
Epoch: [73]  [ 40/160]  eta: 0:02:03  lr: 0.000034  min_lr: 0.000009  loss: 1.4796 (1.4642)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6731 (7.0505)  time: 0.8517 (0.5179 -- 2.6266)  data: 0.0014 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:37  lr: 0.000034  min_lr: 0.000009  loss: 1.6154 (1.5012)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4835 (7.0075)  time: 0.8424 (0.5427 -- 3.0460)  data: 0.0021 (0.0006 -- 0.0084)  max mem: 16413
Epoch: [73]  [ 80/160]  eta: 0:01:16  lr: 0.000034  min_lr: 0.000009  loss: 1.4381 (1.4921)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6579 (7.0327)  time: 0.9273 (0.5213 -- 5.1170)  data: 0.0436 (0.0005 -- 0.8390)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000009  loss: 1.4746 (1.4998)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5114 (6.9615)  time: 0.7750 (0.5285 -- 2.6966)  data: 0.0437 (0.0003 -- 0.8400)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000009  loss: 1.4424 (1.4996)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5738 (6.9357)  time: 0.9848 (0.5255 -- 2.9242)  data: 0.1409 (0.0004 -- 2.4021)  max mem: 16413
[2023-09-04 18:11:06,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:11:06,980] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 18:11:06,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:11:06,980] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 18:11:14,260] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11814
[2023-09-04 18:11:14,260] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 18:11:14,260] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11814
[2023-09-04 18:11:14,261] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 18:11:14,261] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.4799 (1.5079)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6679 (7.0028)  time: 0.8087 (0.5234 -- 2.8871)  data: 0.1926 (0.0002 -- 2.3823)  max mem: 16413
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.4635 (1.5056)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3049 (6.9549)  time: 0.6608 (0.4964 -- 2.1470)  data: 0.0487 (0.0002 -- 0.9580)  max mem: 16413
Epoch: [73] Total time: 0:02:21 (0.8863 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.4635 (1.5301)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3049 (6.9549)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1540 (0.1540)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5461 (2.5461 -- 2.5461)  data: 2.2948 (2.2948 -- 2.2948)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5977 (0.5389)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4293 (0.1992 -- 2.5461)  data: 0.2131 (0.0008 -- 2.2948)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4865 (0.5207)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2097 (0.1696 -- 0.2601)  data: 0.0052 (0.0001 -- 0.0421)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4865 (0.5606)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.7552)  time: 0.1954 (0.1328 -- 0.2601)  data: 0.0049 (0.0001 -- 0.0421)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 86.100 Acc@5 98.133 loss 0.570
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.55%
Epoch: [74]  [  0/160]  eta: 0:20:44  lr: 0.000034  min_lr: 0.000009  loss: 1.4361 (1.4361)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6994 (10.6994)  time: 7.7751 (7.7751 -- 7.7751)  data: 7.2124 (7.2124 -- 7.2124)  max mem: 16413
[2023-09-04 18:11:56,422] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11850
[2023-09-04 18:11:56,423] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:11:56,423] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11850
[2023-09-04 18:11:56,423] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:11:56,424] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [74]  [ 20/160]  eta: 0:02:44  lr: 0.000034  min_lr: 0.000009  loss: 1.4821 (1.4792)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4670 (7.5973)  time: 0.8465 (0.5353 -- 2.2398)  data: 0.2959 (0.0004 -- 1.7013)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:02:01  lr: 0.000034  min_lr: 0.000009  loss: 1.4160 (1.4501)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5477 (7.4736)  time: 0.8356 (0.5277 -- 3.8777)  data: 0.2759 (0.0004 -- 3.3251)  max mem: 16413
Epoch: [74]  [ 60/160]  eta: 0:01:39  lr: 0.000034  min_lr: 0.000009  loss: 1.5483 (1.4809)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5736 (7.6291)  time: 0.9597 (0.5179 -- 4.2659)  data: 0.4161 (0.0004 -- 3.7445)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:15  lr: 0.000034  min_lr: 0.000009  loss: 1.5399 (1.5035)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5585 (7.5922)  time: 0.7733 (0.5306 -- 2.5039)  data: 0.1861 (0.0004 -- 1.9238)  max mem: 16413
Epoch: [74]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000009  loss: 1.4895 (1.5160)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2062 (7.6236)  time: 0.8511 (0.5354 -- 2.9240)  data: 0.2700 (0.0001 -- 2.3786)  max mem: 16413
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000009  loss: 1.6539 (1.5310)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4120 (7.4387)  time: 0.8955 (0.5321 -- 4.5011)  data: 0.2915 (0.0003 -- 3.9878)  max mem: 16413
[2023-09-04 18:13:46,298] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:13:46,298] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:13:46,299] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:13:46,299] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [74]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.5109 (1.5425)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1116 (7.4351)  time: 0.8457 (0.5308 -- 3.1298)  data: 0.1347 (0.0004 -- 1.4629)  max mem: 16413
[2023-09-04 18:14:00,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=63, lr=[8.515378090801247e-06, 8.515378090801247e-06, 9.461531212001385e-06, 9.461531212001385e-06, 1.0512812457779315e-05, 1.0512812457779315e-05, 1.1680902730865906e-05, 1.1680902730865906e-05, 1.297878081207323e-05, 1.297878081207323e-05, 1.4420867568970255e-05, 1.4420867568970255e-05, 1.6023186187744725e-05, 1.6023186187744725e-05, 1.780354020860525e-05, 1.780354020860525e-05, 1.9781711342894723e-05, 1.9781711342894723e-05, 2.1979679269883025e-05, 2.1979679269883025e-05, 2.4421865855425585e-05, 2.4421865855425585e-05, 2.7135406506028427e-05, 2.7135406506028427e-05, 3.0150451673364916e-05, 3.0150451673364916e-05, 3.350050185929435e-05, 3.350050185929435e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 18:14:00,347] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=18.587805365490688, CurrSamplesPerSec=24.604304561301966, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.4029 (1.5381)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2475 (7.4591)  time: 0.7015 (0.4971 -- 2.1360)  data: 0.0010 (0.0001 -- 0.0036)  max mem: 16413
Epoch: [74] Total time: 0:02:21 (0.8839 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.4029 (1.5459)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2475 (7.4591)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1411 (0.1411)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3746 (2.3746 -- 2.3746)  data: 2.1543 (2.1543 -- 2.1543)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5210 (0.5271)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4180 (0.2037 -- 2.3746)  data: 0.2073 (0.0006 -- 2.1543)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4558 (0.5233)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (97.8836)  time: 0.2271 (0.1686 -- 0.6130)  data: 0.0279 (0.0001 -- 0.4283)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4558 (0.5591)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.9253)  time: 0.2125 (0.1346 -- 0.6130)  data: 0.0275 (0.0001 -- 0.4283)  max mem: 16413
Val: Total time: 0:00:07 (0.2926 s / it)
* Acc@1 86.722 Acc@5 97.925 loss 0.565
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.55%
Epoch: [75]  [  0/160]  eta: 0:20:53  lr: 0.000033  min_lr: 0.000009  loss: 1.5960 (1.5960)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9477 (5.9477)  time: 7.8373 (7.8373 -- 7.8373)  data: 5.5287 (5.5287 -- 5.5287)  max mem: 16413
[2023-09-04 18:14:34,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12019
[2023-09-04 18:14:34,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:14:34,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12019
[2023-09-04 18:14:34,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:14:34,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [75]  [ 20/160]  eta: 0:02:59  lr: 0.000033  min_lr: 0.000009  loss: 1.3158 (1.4171)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6075 (6.1332)  time: 0.9531 (0.5143 -- 5.5103)  data: 0.1554 (0.0003 -- 2.6949)  max mem: 16413
Epoch: [75]  [ 40/160]  eta: 0:02:06  lr: 0.000033  min_lr: 0.000008  loss: 1.6462 (1.5165)  loss_scale: 16384.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9860 (6.4241)  time: 0.8225 (0.5236 -- 3.5794)  data: 0.1826 (0.0003 -- 2.0723)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:41  lr: 0.000033  min_lr: 0.000008  loss: 1.4960 (1.5041)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8138 (6.8680)  time: 0.9233 (0.5176 -- 2.8538)  data: 0.3171 (0.0003 -- 2.3068)  max mem: 16413
Epoch: [75]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000008  loss: 1.4781 (1.5026)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5960 (6.9316)  time: 0.7935 (0.5157 -- 2.7786)  data: 0.1961 (0.0003 -- 2.2679)  max mem: 16413
[2023-09-04 18:15:37,147] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12093
[2023-09-04 18:15:37,147] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12093
[2023-09-04 18:15:37,188] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 18:15:37,188] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 18:15:37,188] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [75]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000008  loss: 1.2801 (1.4756)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7563 (7.0785)  time: 0.8593 (0.5203 -- 4.0433)  data: 0.1922 (0.0003 -- 3.4977)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000008  loss: 1.4801 (1.4707)  loss_scale: 8192.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1108 (7.0782)  time: 0.8143 (0.5178 -- 2.3144)  data: 0.0023 (0.0002 -- 0.0053)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.5876 (1.4746)  loss_scale: 8192.0000 (15803.0071)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5213 (7.0993)  time: 0.8965 (0.5172 -- 3.1197)  data: 0.0135 (0.0004 -- 0.2356)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.4199 (1.4765)  loss_scale: 8192.0000 (14899.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9686 (7.0949)  time: 0.7368 (0.4931 -- 1.8715)  data: 0.0475 (0.0002 -- 0.6957)  max mem: 16413
Epoch: [75] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.4199 (1.4867)  loss_scale: 8192.0000 (14899.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9686 (7.0949)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1399 (0.1399)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1974 (2.1974 -- 2.1974)  data: 1.9889 (1.9889 -- 1.9889)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3928 (0.4875)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4162 (0.2023 -- 2.1974)  data: 0.2081 (0.0009 -- 1.9889)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4489 (0.5066)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2264 (0.1709 -- 0.4848)  data: 0.0207 (0.0001 -- 0.2818)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4504 (0.5666)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (99.1701)  time: 0.2123 (0.1326 -- 0.4848)  data: 0.0203 (0.0001 -- 0.2818)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 86.307 Acc@5 98.133 loss 0.566
Accuracy of the network on the 482 val images: 86.31%
Max accuracy: 87.55%
Epoch: [76]  [  0/160]  eta: 0:16:16  lr: 0.000033  min_lr: 0.000008  loss: 1.5684 (1.5684)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0904 (9.0904)  time: 6.1020 (6.1020 -- 6.1020)  data: 4.3863 (4.3863 -- 4.3863)  max mem: 16413
Epoch: [76]  [ 20/160]  eta: 0:02:55  lr: 0.000033  min_lr: 0.000008  loss: 1.5833 (1.5828)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7310 (7.5616)  time: 1.0109 (0.5267 -- 4.7799)  data: 0.4543 (0.0005 -- 4.2496)  max mem: 16413
Epoch: [76]  [ 40/160]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000008  loss: 1.0665 (1.4154)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6107 (7.0423)  time: 0.8239 (0.5150 -- 3.4680)  data: 0.2551 (0.0004 -- 2.9518)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:39  lr: 0.000033  min_lr: 0.000008  loss: 1.4590 (1.4520)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1728 (6.9600)  time: 0.8987 (0.5289 -- 3.7425)  data: 0.3520 (0.0004 -- 3.2234)  max mem: 16413
[2023-09-04 18:17:40,949] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:17:40,949] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:17:40,949] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 18:17:40,949] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [76]  [ 80/160]  eta: 0:01:14  lr: 0.000033  min_lr: 0.000008  loss: 1.6587 (1.5068)  loss_scale: 16384.0000 (10113.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7464 (7.1366)  time: 0.7399 (0.5332 -- 2.3546)  data: 0.1824 (0.0002 -- 1.8149)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000008  loss: 1.5700 (1.5280)  loss_scale: 16384.0000 (11355.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4925 (7.2633)  time: 0.8958 (0.5337 -- 2.9203)  data: 0.3361 (0.0005 -- 2.3786)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000008  loss: 1.5130 (1.5355)  loss_scale: 16384.0000 (12186.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3118 (7.3169)  time: 0.8256 (0.5277 -- 2.3962)  data: 0.2024 (0.0005 -- 1.8593)  max mem: 16413
Epoch: [76]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.5026 (1.5388)  loss_scale: 16384.0000 (12781.8440)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6535 (7.3681)  time: 0.9512 (0.5341 -- 2.9221)  data: 0.4011 (0.0008 -- 2.3939)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.3456 (1.5244)  loss_scale: 16384.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4375 (7.2343)  time: 0.7006 (0.4940 -- 3.0262)  data: 0.1844 (0.0001 -- 2.4787)  max mem: 16413
Epoch: [76] Total time: 0:02:22 (0.8905 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.3456 (1.5328)  loss_scale: 16384.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4375 (7.2343)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1393 (0.1393)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3293 (2.3293 -- 2.3293)  data: 2.1053 (2.1053 -- 2.1053)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4961 (0.5149)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4210 (0.2051 -- 2.3293)  data: 0.2045 (0.0004 -- 2.1053)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4221 (0.5099)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (99.4709)  time: 0.2221 (0.1693 -- 0.3876)  data: 0.0161 (0.0001 -- 0.1404)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4221 (0.5489)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (99.1701)  time: 0.2081 (0.1328 -- 0.3876)  data: 0.0158 (0.0001 -- 0.1404)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.595
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.55%
Epoch: [77]  [  0/160]  eta: 0:19:41  lr: 0.000033  min_lr: 0.000008  loss: 1.6384 (1.6384)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6970 (7.6970)  time: 7.3843 (7.3843 -- 7.3843)  data: 6.6520 (6.6520 -- 6.6520)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:46  lr: 0.000033  min_lr: 0.000008  loss: 1.6194 (1.5667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0031 (6.9766)  time: 0.8801 (0.5228 -- 4.4746)  data: 0.3308 (0.0005 -- 3.9425)  max mem: 16413
[2023-09-04 18:19:42,105] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:19:42,106] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:19:42,108] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:19:42,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:19:49,119] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12357
[2023-09-04 18:19:49,119] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12357
[2023-09-04 18:19:49,120] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:19:49,120] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:19:49,120] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [77]  [ 40/160]  eta: 0:02:06  lr: 0.000033  min_lr: 0.000008  loss: 1.5260 (1.5479)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5369 (7.1177)  time: 0.9151 (0.5277 -- 3.0819)  data: 0.3677 (0.0006 -- 2.5425)  max mem: 16413
Epoch: [77]  [ 60/160]  eta: 0:01:37  lr: 0.000033  min_lr: 0.000008  loss: 1.4616 (1.4937)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9574 (7.1527)  time: 0.8234 (0.5371 -- 2.7660)  data: 0.2656 (0.0002 -- 2.2501)  max mem: 16413
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000008  loss: 1.3052 (1.4779)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9144 (7.1123)  time: 0.8743 (0.5436 -- 2.7690)  data: 0.3216 (0.0005 -- 2.2363)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000008  loss: 1.3537 (1.4658)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6641 (7.1203)  time: 0.8437 (0.5240 -- 3.9628)  data: 0.2839 (0.0004 -- 3.4227)  max mem: 16413
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000008  loss: 1.3623 (1.4601)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1798 (7.2643)  time: 0.8095 (0.5378 -- 3.0250)  data: 0.2508 (0.0006 -- 2.4947)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.3619 (1.4543)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6254 (7.1997)  time: 0.8987 (0.5233 -- 2.9717)  data: 0.3262 (0.0007 -- 2.4387)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.5799 (1.4788)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0281 (7.1689)  time: 0.7515 (0.4943 -- 2.4468)  data: 0.1940 (0.0002 -- 1.9295)  max mem: 16413
Epoch: [77] Total time: 0:02:22 (0.8925 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.5799 (1.5193)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0281 (7.1689)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1406 (0.1406)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3220 (2.3220 -- 2.3220)  data: 2.1141 (2.1141 -- 2.1141)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4967 (0.5276)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4295 (0.2088 -- 2.3220)  data: 0.2151 (0.0004 -- 2.1141)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4586 (0.5175)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (99.4709)  time: 0.2254 (0.1689 -- 0.4707)  data: 0.0177 (0.0001 -- 0.2411)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4620 (0.5707)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (98.7552)  time: 0.2107 (0.1324 -- 0.4707)  data: 0.0173 (0.0001 -- 0.2411)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 85.270 Acc@5 98.755 loss 0.580
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.55%
Epoch: [78]  [  0/160]  eta: 0:21:19  lr: 0.000032  min_lr: 0.000008  loss: 1.2666 (1.2666)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1428 (9.1428)  time: 7.9990 (7.9990 -- 7.9990)  data: 7.4497 (7.4497 -- 7.4497)  max mem: 16413
[2023-09-04 18:21:51,283] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:21:51,284] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:21:51,285] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:21:51,285] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:22:02,044] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12497
[2023-09-04 18:22:02,044] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:22:02,044] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12497
[2023-09-04 18:22:02,044] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:22:02,044] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [78]  [ 20/160]  eta: 0:02:38  lr: 0.000032  min_lr: 0.000008  loss: 1.3579 (1.4158)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8940 (6.7799)  time: 0.7897 (0.5440 -- 2.4681)  data: 0.2215 (0.0008 -- 1.9201)  max mem: 16413
Epoch: [78]  [ 40/160]  eta: 0:01:59  lr: 0.000032  min_lr: 0.000008  loss: 1.6653 (1.5006)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0145 (7.1128)  time: 0.8597 (0.5425 -- 1.9943)  data: 0.3047 (0.0011 -- 1.4600)  max mem: 16413
Epoch: [78]  [ 60/160]  eta: 0:01:36  lr: 0.000032  min_lr: 0.000008  loss: 1.5656 (1.5305)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7310 (7.0557)  time: 0.8797 (0.5188 -- 2.9547)  data: 0.2880 (0.0007 -- 2.4124)  max mem: 16413
Epoch: [78]  [ 80/160]  eta: 0:01:15  lr: 0.000032  min_lr: 0.000008  loss: 1.6254 (1.5450)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5775 (7.0596)  time: 0.8697 (0.5349 -- 3.9551)  data: 0.3154 (0.0006 -- 3.4204)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:56  lr: 0.000032  min_lr: 0.000008  loss: 1.5517 (1.5475)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4963 (7.1757)  time: 0.9393 (0.5121 -- 4.1451)  data: 0.3958 (0.0003 -- 3.6198)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.5329 (1.5469)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0876 (7.1401)  time: 0.9060 (0.5095 -- 4.1247)  data: 0.3591 (0.0004 -- 3.5817)  max mem: 16413
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.5237 (1.5286)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2723 (7.2287)  time: 0.9004 (0.5100 -- 4.8769)  data: 0.3669 (0.0003 -- 4.3820)  max mem: 16413
[2023-09-04 18:23:57,397] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:23:57,397] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:23:57,398] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:23:57,398] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:23:59,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12631
[2023-09-04 18:23:59,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12631
[2023-09-04 18:23:59,965] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:23:59,965] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:23:59,965] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.3881 (1.5247)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8008 (7.1544)  time: 0.7030 (0.4908 -- 3.7099)  data: 0.1814 (0.0002 -- 3.1863)  max mem: 16413
Epoch: [78] Total time: 0:02:24 (0.9028 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.3881 (1.5259)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8008 (7.1544)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1347 (0.1347)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2800 (2.2800 -- 2.2800)  data: 2.0453 (2.0453 -- 2.0453)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4491 (0.4695)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4065 (0.2068 -- 2.2800)  data: 0.1872 (0.0008 -- 2.0453)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4491 (0.4946)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.9418)  time: 0.2230 (0.1692 -- 0.3755)  data: 0.0144 (0.0001 -- 0.1922)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5093 (0.5619)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.7552)  time: 0.2071 (0.1321 -- 0.3755)  data: 0.0140 (0.0001 -- 0.1922)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 85.685 Acc@5 97.718 loss 0.569
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.55%
Epoch: [79]  [  0/160]  eta: 0:25:08  lr: 0.000032  min_lr: 0.000008  loss: 1.6976 (1.6976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4493 (6.4493)  time: 9.4295 (9.4295 -- 9.4295)  data: 8.8847 (8.8847 -- 8.8847)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:46  lr: 0.000032  min_lr: 0.000008  loss: 1.5287 (1.5727)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6686 (6.7115)  time: 0.7806 (0.5221 -- 3.3742)  data: 0.2285 (0.0002 -- 2.8620)  max mem: 16413
Epoch: [79]  [ 40/160]  eta: 0:02:02  lr: 0.000032  min_lr: 0.000008  loss: 1.6454 (1.6363)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6533 (7.0880)  time: 0.8400 (0.5241 -- 2.4551)  data: 0.2316 (0.0003 -- 1.8862)  max mem: 16413
Epoch: [79]  [ 60/160]  eta: 0:01:38  lr: 0.000032  min_lr: 0.000008  loss: 1.6115 (1.6403)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9485 (7.0649)  time: 0.9232 (0.5386 -- 4.0916)  data: 0.0249 (0.0003 -- 0.4661)  max mem: 16413
Epoch: [79]  [ 80/160]  eta: 0:01:15  lr: 0.000032  min_lr: 0.000008  loss: 1.5872 (1.6168)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6442 (7.0767)  time: 0.7921 (0.5327 -- 3.1506)  data: 0.0018 (0.0002 -- 0.0146)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:57  lr: 0.000032  min_lr: 0.000008  loss: 1.5300 (1.6103)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7826 (7.0915)  time: 1.0128 (0.5283 -- 4.1461)  data: 0.0013 (0.0002 -- 0.0027)  max mem: 16413
[2023-09-04 18:26:04,196] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:26:04,196] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:26:04,198] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:26:04,199] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [79]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.3710 (1.5714)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2036 (7.1555)  time: 0.7834 (0.5241 -- 3.5997)  data: 0.0022 (0.0004 -- 0.0159)  max mem: 16413
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.4379 (1.5575)  loss_scale: 32768.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8286 (7.1619)  time: 0.8599 (0.5210 -- 4.2459)  data: 0.0010 (0.0003 -- 0.0020)  max mem: 16413
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.3084 (1.5441)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8396 (7.1791)  time: 0.6870 (0.4945 -- 3.8255)  data: 0.0014 (0.0002 -- 0.0156)  max mem: 16413
Epoch: [79] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.3084 (1.5451)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8396 (7.1791)
[2023-09-04 18:26:34,589] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-09-04 18:26:34,590] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-09-04 18:26:34,590] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-09-04 18:26:34,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-09-04 18:26:35,671] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-09-04 18:26:35,671] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1347 (0.1347)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5047 (2.5047 -- 2.5047)  data: 2.2280 (2.2280 -- 2.2280)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2701 (0.4894)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4372 (0.2069 -- 2.5047)  data: 0.2036 (0.0006 -- 2.2280)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4483 (0.4897)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2163 (0.1690 -- 0.2516)  data: 0.0031 (0.0001 -- 0.0458)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4483 (0.5074)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (99.1701)  time: 0.1969 (0.1328 -- 0.2508)  data: 0.0028 (0.0001 -- 0.0458)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 87.552 Acc@5 98.133 loss 0.529
Accuracy of the network on the 482 val images: 87.55%
[2023-09-04 18:26:43,542] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 18:26:43,543] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 18:26:43,543] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 18:26:43,543] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 18:26:44,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 18:26:44,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 87.55%
Epoch: [80]  [  0/160]  eta: 0:21:25  lr: 0.000032  min_lr: 0.000008  loss: 1.8868 (1.8868)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9840 (6.9840)  time: 8.0370 (8.0370 -- 8.0370)  data: 6.5270 (6.5270 -- 6.5270)  max mem: 16413
Epoch: [80]  [ 20/160]  eta: 0:02:41  lr: 0.000032  min_lr: 0.000008  loss: 1.5102 (1.5625)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3415 (6.4663)  time: 0.8117 (0.5253 -- 3.8810)  data: 0.1062 (0.0003 -- 1.5337)  max mem: 16413
[2023-09-04 18:27:20,499] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12833
[2023-09-04 18:27:20,499] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12833
[2023-09-04 18:27:20,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:27:20,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:27:20,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [80]  [ 40/160]  eta: 0:02:01  lr: 0.000032  min_lr: 0.000008  loss: 1.4920 (1.5251)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2007 (6.8784)  time: 0.8643 (0.5291 -- 2.2797)  data: 0.1443 (0.0002 -- 1.6534)  max mem: 16413
Epoch: [80]  [ 60/160]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000008  loss: 1.5480 (1.5260)  loss_scale: 16384.0000 (25247.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3946 (6.8615)  time: 0.8953 (0.5256 -- 3.7082)  data: 0.0414 (0.0002 -- 0.5162)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:18  lr: 0.000032  min_lr: 0.000008  loss: 1.5287 (1.5405)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4065 (7.0464)  time: 0.9913 (0.5308 -- 3.2834)  data: 0.1776 (0.0002 -- 2.7561)  max mem: 16413
Epoch: [80]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000008  loss: 1.5839 (1.5520)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2873 (7.2119)  time: 0.7443 (0.5135 -- 2.3001)  data: 0.1919 (0.0001 -- 1.7405)  max mem: 16413
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.5676 (1.5646)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9420 (7.4373)  time: 0.8194 (0.5222 -- 3.4837)  data: 0.2483 (0.0002 -- 2.9507)  max mem: 16413
Epoch: [80]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.3393 (1.5395)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6467 (7.4340)  time: 0.8276 (0.5371 -- 1.8174)  data: 0.2193 (0.0003 -- 1.2958)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.5430 (1.5418)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7289 (7.5317)  time: 0.7691 (0.4947 -- 1.8177)  data: 0.0949 (0.0002 -- 1.1731)  max mem: 16413
Epoch: [80] Total time: 0:02:22 (0.8877 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.5430 (1.5346)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7289 (7.5317)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1238 (0.1238)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2612 (2.2612 -- 2.2612)  data: 2.0206 (2.0206 -- 2.0206)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3961 (0.5429)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4129 (0.2053 -- 2.2612)  data: 0.1853 (0.0007 -- 2.0206)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4275 (0.5193)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2199 (0.1722 -- 0.2860)  data: 0.0070 (0.0001 -- 0.0601)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4682 (0.5554)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.1978 (0.1332 -- 0.2860)  data: 0.0064 (0.0001 -- 0.0601)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 84.647 Acc@5 98.340 loss 0.557
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 87.55%
Epoch: [81]  [  0/160]  eta: 0:21:41  lr: 0.000031  min_lr: 0.000008  loss: 1.6921 (1.6921)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4642 (5.4642)  time: 8.1340 (8.1340 -- 8.1340)  data: 7.6009 (7.6009 -- 7.6009)  max mem: 16413
[2023-09-04 18:29:23,828] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:29:23,828] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:29:23,829] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:29:23,829] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:29:32,576] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12973
[2023-09-04 18:29:32,576] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12973
[2023-09-04 18:29:32,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:29:32,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:29:32,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [81]  [ 20/160]  eta: 0:02:44  lr: 0.000031  min_lr: 0.000008  loss: 1.6521 (1.5971)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3040 (6.4083)  time: 0.8237 (0.5231 -- 2.3746)  data: 0.2350 (0.0007 -- 1.8268)  max mem: 16413
[2023-09-04 18:29:55,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=70, lr=[7.961979027681322e-06, 7.961979027681322e-06, 8.846643364090358e-06, 8.846643364090358e-06, 9.829603737878174e-06, 9.829603737878174e-06, 1.0921781930975751e-05, 1.0921781930975751e-05, 1.2135313256639722e-05, 1.2135313256639722e-05, 1.3483681396266359e-05, 1.3483681396266359e-05, 1.498186821807373e-05, 1.498186821807373e-05, 1.6646520242304144e-05, 1.6646520242304144e-05, 1.849613360256016e-05, 1.849613360256016e-05, 2.0551259558400174e-05, 2.0551259558400174e-05, 2.2834732842666863e-05, 2.2834732842666863e-05, 2.537192538074096e-05, 2.537192538074096e-05, 2.8191028200823287e-05, 2.8191028200823287e-05, 3.132336466758143e-05, 3.132336466758143e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 18:29:55,670] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=18.61796341513621, CurrSamplesPerSec=20.302886723444043, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:02  lr: 0.000031  min_lr: 0.000008  loss: 1.3078 (1.5011)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7140 (7.1608)  time: 0.8585 (0.5208 -- 3.0062)  data: 0.1438 (0.0003 -- 1.4429)  max mem: 16413
Epoch: [81]  [ 60/160]  eta: 0:01:40  lr: 0.000031  min_lr: 0.000008  loss: 1.6631 (1.5483)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3798 (7.3056)  time: 0.9667 (0.5164 -- 4.7498)  data: 0.3437 (0.0005 -- 4.2099)  max mem: 16413
Epoch: [81]  [ 80/160]  eta: 0:01:17  lr: 0.000031  min_lr: 0.000008  loss: 1.5949 (1.5317)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9015 (7.2620)  time: 0.8906 (0.5050 -- 4.0251)  data: 0.3586 (0.0002 -- 3.5084)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000008  loss: 1.6442 (1.5312)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1187 (7.0410)  time: 0.7476 (0.5301 -- 2.0487)  data: 0.2036 (0.0002 -- 1.5225)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.3531 (1.5080)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5071 (7.0168)  time: 0.8093 (0.5231 -- 2.3387)  data: 0.2093 (0.0003 -- 1.8096)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.5362 (1.5061)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9515 (6.9620)  time: 1.0143 (0.5285 -- 4.0788)  data: 0.4677 (0.0007 -- 3.5474)  max mem: 16413
[2023-09-04 18:31:25,860] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:31:25,860] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:31:25,861] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:31:25,861] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:31:32,900] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13112
[2023-09-04 18:31:32,900] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13112
[2023-09-04 18:31:32,900] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:31:32,900] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:31:32,900] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.3722 (1.4949)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5545 (6.8694)  time: 0.6385 (0.4927 -- 2.1859)  data: 0.1208 (0.0002 -- 1.6527)  max mem: 16413
Epoch: [81] Total time: 0:02:22 (0.8913 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.3722 (1.4964)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5545 (6.8694)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1307 (0.1307)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3435 (2.3435 -- 2.3435)  data: 2.0997 (2.0997 -- 2.0997)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3787 (0.5034)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4048 (0.1992 -- 2.3435)  data: 0.1919 (0.0003 -- 2.0997)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4228 (0.5063)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.8836)  time: 0.2147 (0.1702 -- 0.3089)  data: 0.0089 (0.0002 -- 0.1071)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5128 (0.5663)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (98.3402)  time: 0.2000 (0.1330 -- 0.3089)  data: 0.0086 (0.0001 -- 0.1071)  max mem: 16413
Val: Total time: 0:00:07 (0.2824 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.574
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 87.55%
Epoch: [82]  [  0/160]  eta: 0:20:23  lr: 0.000031  min_lr: 0.000008  loss: 1.8487 (1.8487)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5545 (4.5545)  time: 7.6479 (7.6479 -- 7.6479)  data: 5.5287 (5.5287 -- 5.5287)  max mem: 16413
Epoch: [82]  [ 20/160]  eta: 0:02:36  lr: 0.000031  min_lr: 0.000008  loss: 1.4593 (1.5896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1286 (7.3364)  time: 0.7946 (0.5285 -- 1.8098)  data: 0.1150 (0.0008 -- 1.0157)  max mem: 16413
Epoch: [82]  [ 40/160]  eta: 0:02:10  lr: 0.000031  min_lr: 0.000008  loss: 1.5605 (1.5633)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2105 (7.9740)  time: 1.0501 (0.5295 -- 4.3130)  data: 0.4942 (0.0004 -- 3.7216)  max mem: 16413
Epoch: [82]  [ 60/160]  eta: 0:01:37  lr: 0.000031  min_lr: 0.000008  loss: 1.4354 (1.5319)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2554 (7.7849)  time: 0.7483 (0.5199 -- 3.1385)  data: 0.2009 (0.0003 -- 2.5873)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:14  lr: 0.000031  min_lr: 0.000008  loss: 1.5106 (1.5471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5298 (7.6108)  time: 0.8004 (0.5420 -- 2.6491)  data: 0.2430 (0.0003 -- 2.1026)  max mem: 16413
Epoch: [82]  [100/160]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000008  loss: 1.6932 (1.5632)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1306 (7.4031)  time: 0.8997 (0.5189 -- 2.4395)  data: 0.2835 (0.0002 -- 1.9268)  max mem: 16413
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.4271 (1.5420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0595 (7.3578)  time: 0.8159 (0.5331 -- 2.8454)  data: 0.2503 (0.0008 -- 2.3138)  max mem: 16413
[2023-09-04 18:33:35,145] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:33:35,145] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:33:35,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:33:35,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.2988 (1.5198)  loss_scale: 32768.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8202 (7.3109)  time: 0.9855 (0.5443 -- 4.1778)  data: 0.4277 (0.0004 -- 3.6532)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.4220 (1.5088)  loss_scale: 32768.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1727 (7.2476)  time: 0.6929 (0.4967 -- 2.6846)  data: 0.1682 (0.0001 -- 2.1428)  max mem: 16413
Epoch: [82] Total time: 0:02:20 (0.8796 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.4220 (1.5236)  loss_scale: 32768.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1727 (7.2476)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1352 (0.1352)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3038 (2.3038 -- 2.3038)  data: 2.0563 (2.0563 -- 2.0563)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3599 (0.4984)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4159 (0.2072 -- 2.3038)  data: 0.1944 (0.0009 -- 2.0563)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4014 (0.5008)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2234 (0.1687 -- 0.3261)  data: 0.0137 (0.0001 -- 0.1263)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5183 (0.5392)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.7552)  time: 0.2051 (0.1334 -- 0.3261)  data: 0.0133 (0.0001 -- 0.1263)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 86.100 Acc@5 98.133 loss 0.570
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.55%
Epoch: [83]  [  0/160]  eta: 0:21:17  lr: 0.000031  min_lr: 0.000008  loss: 1.6521 (1.6521)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5070 (4.5070)  time: 7.9857 (7.9857 -- 7.9857)  data: 7.4412 (7.4412 -- 7.4412)  max mem: 16413
Epoch: [83]  [ 20/160]  eta: 0:02:45  lr: 0.000031  min_lr: 0.000008  loss: 1.3437 (1.5086)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3259 (7.5828)  time: 0.8427 (0.5270 -- 3.9023)  data: 0.2884 (0.0004 -- 3.3470)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:01:59  lr: 0.000031  min_lr: 0.000008  loss: 1.2981 (1.4294)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2852 (7.1625)  time: 0.8072 (0.5288 -- 2.9724)  data: 0.2040 (0.0008 -- 2.4228)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:41  lr: 0.000031  min_lr: 0.000008  loss: 1.5997 (1.4752)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9157 (7.1441)  time: 1.0314 (0.5309 -- 3.1111)  data: 0.2190 (0.0005 -- 1.9458)  max mem: 16413
Epoch: [83]  [ 80/160]  eta: 0:01:15  lr: 0.000031  min_lr: 0.000008  loss: 1.6970 (1.4999)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5844 (7.0996)  time: 0.7527 (0.5180 -- 3.9464)  data: 0.0320 (0.0003 -- 0.5929)  max mem: 16413
[2023-09-04 18:35:37,507] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:35:37,507] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 18:35:37,509] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:35:37,510] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 18:35:41,218] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13373
[2023-09-04 18:35:41,218] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13373
[2023-09-04 18:35:41,219] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 18:35:41,219] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 18:35:41,219] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [83]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000008  loss: 1.8075 (1.5484)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1968 (7.0676)  time: 0.8854 (0.5223 -- 2.4255)  data: 0.1153 (0.0002 -- 0.7034)  max mem: 16413
Epoch: [83]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000008  loss: 1.4003 (1.5226)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6169 (7.2366)  time: 0.8346 (0.5298 -- 3.4414)  data: 0.2848 (0.0006 -- 2.9181)  max mem: 16413
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.2522 (1.5017)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2230 (7.2596)  time: 1.0104 (0.5265 -- 4.8019)  data: 0.4661 (0.0003 -- 4.2766)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.6400 (1.5144)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7705 (7.2483)  time: 0.6675 (0.4941 -- 3.3231)  data: 0.1510 (0.0002 -- 2.8056)  max mem: 16413
Epoch: [83] Total time: 0:02:24 (0.9007 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.6400 (1.5389)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7705 (7.2483)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.1341 (0.1341)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0934 (2.0934 -- 2.0934)  data: 1.8683 (1.8683 -- 1.8683)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4286 (0.4815)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (98.9899)  time: 0.3948 (0.2006 -- 2.0934)  data: 0.1797 (0.0007 -- 1.8683)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4520 (0.5071)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2271 (0.1703 -- 0.4931)  data: 0.0199 (0.0001 -- 0.2843)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4520 (0.5373)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.9253)  time: 0.2073 (0.1330 -- 0.4931)  data: 0.0149 (0.0001 -- 0.2843)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 86.722 Acc@5 97.925 loss 0.565
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.55%
Epoch: [84]  [  0/160]  eta: 0:21:15  lr: 0.000030  min_lr: 0.000008  loss: 1.5751 (1.5751)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0630 (7.0630)  time: 7.9730 (7.9730 -- 7.9730)  data: 7.4540 (7.4540 -- 7.4540)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:52  lr: 0.000030  min_lr: 0.000008  loss: 1.3870 (1.4155)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0760 (6.2549)  time: 0.8974 (0.5290 -- 4.1253)  data: 0.3424 (0.0005 -- 3.5958)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:02  lr: 0.000030  min_lr: 0.000008  loss: 1.5367 (1.4751)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7228 (6.9192)  time: 0.7951 (0.5132 -- 2.8442)  data: 0.2367 (0.0005 -- 2.2896)  max mem: 16413
Epoch: [84]  [ 60/160]  eta: 0:01:36  lr: 0.000030  min_lr: 0.000008  loss: 1.5238 (1.5062)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2789 (7.0777)  time: 0.8553 (0.5334 -- 3.9454)  data: 0.3093 (0.0009 -- 3.4048)  max mem: 16413
[2023-09-04 18:37:45,206] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13502
[2023-09-04 18:37:45,207] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:37:45,207] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13502
[2023-09-04 18:37:45,207] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:37:45,207] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [84]  [ 80/160]  eta: 0:01:15  lr: 0.000030  min_lr: 0.000008  loss: 1.4192 (1.4965)  loss_scale: 16384.0000 (28924.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5359 (7.0174)  time: 0.8779 (0.5349 -- 2.7440)  data: 0.3278 (0.0004 -- 2.2126)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000008  loss: 1.5561 (1.4944)  loss_scale: 16384.0000 (26441.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9219 (6.8535)  time: 0.8143 (0.5422 -- 2.1817)  data: 0.1973 (0.0008 -- 1.6500)  max mem: 16413
Epoch: [84]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000008  loss: 1.6049 (1.5100)  loss_scale: 16384.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4832 (6.8157)  time: 0.8799 (0.5186 -- 2.0842)  data: 0.3064 (0.0009 -- 1.5506)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.4112 (1.5086)  loss_scale: 16384.0000 (23588.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0170 (6.8882)  time: 0.9321 (0.5280 -- 2.4470)  data: 0.3022 (0.0004 -- 1.3228)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.5035 (1.4965)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2119 (6.8310)  time: 0.7249 (0.4975 -- 1.8982)  data: 0.1822 (0.0002 -- 1.3228)  max mem: 16413
Epoch: [84] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.5035 (1.5381)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2119 (6.8310)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1399 (0.1399)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2564 (2.2564 -- 2.2564)  data: 2.0459 (2.0459 -- 2.0459)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5536 (0.4894)  acc1: 88.8889 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4140 (0.1992 -- 2.2564)  data: 0.1988 (0.0008 -- 2.0459)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5630 (0.5103)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (99.4709)  time: 0.2222 (0.1710 -- 0.3783)  data: 0.0177 (0.0001 -- 0.1736)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5630 (0.5383)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (98.7552)  time: 0.2075 (0.1365 -- 0.3783)  data: 0.0171 (0.0001 -- 0.1736)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 87.137 Acc@5 98.133 loss 0.572
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.55%
Epoch: [85]  [  0/160]  eta: 0:22:54  lr: 0.000030  min_lr: 0.000008  loss: 1.1349 (1.1349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3066 (8.3066)  time: 8.5908 (8.5908 -- 8.5908)  data: 6.7633 (6.7633 -- 6.7633)  max mem: 16413
Epoch: [85]  [ 20/160]  eta: 0:02:38  lr: 0.000030  min_lr: 0.000008  loss: 1.5177 (1.5217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5577 (7.3494)  time: 0.7558 (0.5311 -- 1.8418)  data: 0.0769 (0.0002 -- 1.2758)  max mem: 16413
[2023-09-04 18:39:46,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:39:46,980] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:39:46,982] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:39:46,982] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [85]  [ 40/160]  eta: 0:01:56  lr: 0.000030  min_lr: 0.000008  loss: 1.4925 (1.5025)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9027 (7.2490)  time: 0.8107 (0.5310 -- 2.2583)  data: 0.1904 (0.0002 -- 1.7450)  max mem: 16413
Epoch: [85]  [ 60/160]  eta: 0:01:33  lr: 0.000030  min_lr: 0.000008  loss: 1.1514 (1.4634)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5935 (7.1364)  time: 0.8490 (0.5387 -- 2.5320)  data: 0.2530 (0.0005 -- 1.9904)  max mem: 16413
Epoch: [85]  [ 80/160]  eta: 0:01:13  lr: 0.000030  min_lr: 0.000008  loss: 1.5854 (1.4974)  loss_scale: 32768.0000 (26497.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7129 (6.8895)  time: 0.8973 (0.5302 -- 4.3210)  data: 0.3082 (0.0003 -- 3.7729)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000008  loss: 1.3458 (1.4640)  loss_scale: 32768.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4524 (6.7957)  time: 0.9725 (0.5242 -- 3.3235)  data: 0.4192 (0.0005 -- 2.7754)  max mem: 16413
[2023-09-04 18:40:56,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13709
[2023-09-04 18:40:56,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:40:56,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13709
[2023-09-04 18:40:56,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:40:56,725] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000008  loss: 1.5012 (1.4749)  loss_scale: 16384.0000 (26945.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9648 (6.8772)  time: 0.8808 (0.5243 -- 4.2915)  data: 0.3336 (0.0003 -- 3.7489)  max mem: 16413
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.3341 (1.4732)  loss_scale: 16384.0000 (25447.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5846 (6.7978)  time: 0.7759 (0.5279 -- 2.1321)  data: 0.2175 (0.0006 -- 1.6007)  max mem: 16413
[2023-09-04 18:41:30,246] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13748
[2023-09-04 18:41:30,246] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13748
[2023-09-04 18:41:30,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 18:41:30,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 18:41:30,247] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.5662 (1.4890)  loss_scale: 8192.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7116 (6.9666)  time: 0.7738 (0.4960 -- 2.4796)  data: 0.0919 (0.0001 -- 1.8190)  max mem: 16413
Epoch: [85] Total time: 0:02:22 (0.8899 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.5662 (1.5268)  loss_scale: 8192.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7116 (6.9666)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1324 (0.1324)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3858 (2.3858 -- 2.3858)  data: 2.1608 (2.1608 -- 2.1608)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3621 (0.5058)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4426 (0.1984 -- 2.3858)  data: 0.2171 (0.0008 -- 2.1608)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4948 (0.5064)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2282 (0.1722 -- 0.4611)  data: 0.0151 (0.0001 -- 0.2166)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4948 (0.5257)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.7552)  time: 0.2127 (0.1366 -- 0.4611)  data: 0.0148 (0.0001 -- 0.2166)  max mem: 16413
Val: Total time: 0:00:07 (0.2942 s / it)
* Acc@1 87.137 Acc@5 98.548 loss 0.545
Accuracy of the network on the 482 val images: 87.14%
Max accuracy: 87.55%
Epoch: [86]  [  0/160]  eta: 0:17:38  lr: 0.000030  min_lr: 0.000008  loss: 1.9474 (1.9474)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5230 (6.5230)  time: 6.6139 (6.6139 -- 6.6139)  data: 6.0587 (6.0587 -- 6.0587)  max mem: 16413
Epoch: [86]  [ 20/160]  eta: 0:02:45  lr: 0.000030  min_lr: 0.000008  loss: 1.5099 (1.5682)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4859 (8.1115)  time: 0.9120 (0.5239 -- 2.9769)  data: 0.3388 (0.0005 -- 2.4154)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:02  lr: 0.000030  min_lr: 0.000008  loss: 1.3907 (1.5059)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9114 (7.6127)  time: 0.8576 (0.5312 -- 2.7678)  data: 0.1418 (0.0004 -- 1.6337)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:40  lr: 0.000029  min_lr: 0.000007  loss: 1.3921 (1.4965)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4231 (7.6658)  time: 0.9509 (0.5217 -- 3.5784)  data: 0.0412 (0.0002 -- 0.5395)  max mem: 16413
Epoch: [86]  [ 80/160]  eta: 0:01:16  lr: 0.000029  min_lr: 0.000007  loss: 1.7007 (1.5252)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9349 (7.7452)  time: 0.8009 (0.5307 -- 1.7783)  data: 0.1015 (0.0002 -- 0.8735)  max mem: 16413
Epoch: [86]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000007  loss: 1.3924 (1.5130)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5600 (7.6032)  time: 0.8401 (0.5295 -- 2.3244)  data: 0.1603 (0.0002 -- 1.7889)  max mem: 16413
[2023-09-04 18:43:35,778] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:43:35,778] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 18:43:35,779] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:43:35,779] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [86]  [120/160]  eta: 0:00:38  lr: 0.000029  min_lr: 0.000007  loss: 1.6557 (1.5321)  loss_scale: 8192.0000 (8462.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5009 (7.5991)  time: 1.0802 (0.5277 -- 4.3009)  data: 0.3793 (0.0003 -- 3.7799)  max mem: 16413
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.4840 (1.5195)  loss_scale: 16384.0000 (9586.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8700 (7.6587)  time: 0.7905 (0.5216 -- 3.0516)  data: 0.2510 (0.0001 -- 2.5401)  max mem: 16413
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.5148 (1.5159)  loss_scale: 16384.0000 (10393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7641 (7.7654)  time: 0.6665 (0.4951 -- 3.5137)  data: 0.1497 (0.0002 -- 2.9831)  max mem: 16413
Epoch: [86] Total time: 0:02:24 (0.9005 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.5148 (1.4963)  loss_scale: 16384.0000 (10393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7641 (7.7654)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1382 (0.1382)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2871 (2.2871 -- 2.2871)  data: 2.0386 (2.0386 -- 2.0386)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4594 (0.5198)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4136 (0.2044 -- 2.2871)  data: 0.1876 (0.0007 -- 2.0386)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4594 (0.5166)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (99.4709)  time: 0.2246 (0.1740 -- 0.3963)  data: 0.0112 (0.0001 -- 0.1941)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4880 (0.5419)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.7552)  time: 0.2052 (0.1378 -- 0.3963)  data: 0.0106 (0.0001 -- 0.1941)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 85.270 Acc@5 98.548 loss 0.567
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.55%
Epoch: [87]  [  0/160]  eta: 0:22:13  lr: 0.000029  min_lr: 0.000007  loss: 1.3933 (1.3933)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7977 (10.7977)  time: 8.3340 (8.3340 -- 8.3340)  data: 7.7778 (7.7778 -- 7.7778)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:49  lr: 0.000029  min_lr: 0.000007  loss: 1.4613 (1.4350)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1623 (6.7960)  time: 0.8537 (0.5326 -- 4.0914)  data: 0.2855 (0.0005 -- 3.5323)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:03  lr: 0.000029  min_lr: 0.000007  loss: 1.5125 (1.4525)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2095 (6.7303)  time: 0.8336 (0.5164 -- 2.8842)  data: 0.2876 (0.0002 -- 2.3446)  max mem: 16413
Epoch: [87]  [ 60/160]  eta: 0:01:36  lr: 0.000029  min_lr: 0.000007  loss: 1.4478 (1.4548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5393 (6.9032)  time: 0.8429 (0.5326 -- 3.3688)  data: 0.2863 (0.0003 -- 2.8290)  max mem: 16413
[2023-09-04 18:45:31,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=75, lr=[7.38833426229976e-06, 7.38833426229976e-06, 8.20926029144418e-06, 8.20926029144418e-06, 9.121400323826864e-06, 9.121400323826864e-06, 1.0134889248696515e-05, 1.0134889248696515e-05, 1.126098805410724e-05, 1.126098805410724e-05, 1.2512208949008044e-05, 1.2512208949008044e-05, 1.3902454387786715e-05, 1.3902454387786715e-05, 1.5447171541985237e-05, 1.5447171541985237e-05, 1.7163523935539154e-05, 1.7163523935539154e-05, 1.9070582150599057e-05, 1.9070582150599057e-05, 2.1189535722887845e-05, 2.1189535722887845e-05, 2.3543928580986492e-05, 2.3543928580986492e-05, 2.6159920645540545e-05, 2.6159920645540545e-05, 2.906657849504505e-05, 2.906657849504505e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 18:45:31,815] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=18.73104005008237, CurrSamplesPerSec=21.648421024560886, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:14  lr: 0.000029  min_lr: 0.000007  loss: 1.7486 (1.5156)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9423 (6.8737)  time: 0.8311 (0.5341 -- 3.8713)  data: 0.2804 (0.0009 -- 3.3310)  max mem: 16413
[2023-09-04 18:45:37,303] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:45:37,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:45:37,304] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:45:37,304] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [87]  [100/160]  eta: 0:00:56  lr: 0.000029  min_lr: 0.000007  loss: 1.4945 (1.5049)  loss_scale: 32768.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6964 (6.8280)  time: 0.9448 (0.5352 -- 2.1810)  data: 0.3039 (0.0002 -- 1.6489)  max mem: 16413
Epoch: [87]  [120/160]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000007  loss: 1.5878 (1.5145)  loss_scale: 32768.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6698 (6.8361)  time: 0.8695 (0.5303 -- 3.8011)  data: 0.3210 (0.0004 -- 3.2729)  max mem: 16413
[2023-09-04 18:46:13,113] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14044
[2023-09-04 18:46:13,114] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14044
[2023-09-04 18:46:13,114] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:46:13,114] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:46:13,114] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [87]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.6346 (1.5248)  loss_scale: 16384.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3453 (6.8742)  time: 0.9152 (0.5271 -- 3.4055)  data: 0.3684 (0.0007 -- 2.8608)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.5682 (1.5294)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2564 (6.9360)  time: 0.8327 (0.4958 -- 3.4055)  data: 0.3130 (0.0002 -- 2.8608)  max mem: 16413
Epoch: [87] Total time: 0:02:23 (0.8965 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.5682 (1.5039)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2564 (6.9360)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1605 (0.1605)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2791 (2.2791 -- 2.2791)  data: 2.0635 (2.0635 -- 2.0635)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3704 (0.4797)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4054 (0.2023 -- 2.2791)  data: 0.1903 (0.0007 -- 2.0635)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4338 (0.4991)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (99.4709)  time: 0.2211 (0.1702 -- 0.3385)  data: 0.0138 (0.0001 -- 0.1336)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5675 (0.5286)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (99.1701)  time: 0.2025 (0.1326 -- 0.3385)  data: 0.0128 (0.0001 -- 0.1336)  max mem: 16413
Val: Total time: 0:00:07 (0.2846 s / it)
* Acc@1 86.722 Acc@5 98.548 loss 0.558
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.55%
Epoch: [88]  [  0/160]  eta: 0:16:52  lr: 0.000029  min_lr: 0.000007  loss: 1.5855 (1.5855)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3880 (5.3880)  time: 6.3268 (6.3268 -- 6.3268)  data: 5.6882 (5.6882 -- 5.6882)  max mem: 16413
Epoch: [88]  [ 20/160]  eta: 0:02:45  lr: 0.000029  min_lr: 0.000007  loss: 1.6673 (1.5962)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2816 (7.5156)  time: 0.9254 (0.5220 -- 3.3518)  data: 0.3763 (0.0009 -- 2.8295)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:02:00  lr: 0.000029  min_lr: 0.000007  loss: 1.3825 (1.5002)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0921 (7.1232)  time: 0.8117 (0.5277 -- 2.2879)  data: 0.2174 (0.0003 -- 1.7443)  max mem: 16413
Epoch: [88]  [ 60/160]  eta: 0:01:39  lr: 0.000029  min_lr: 0.000007  loss: 1.5971 (1.5173)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2700 (6.9715)  time: 0.9689 (0.5321 -- 3.3691)  data: 0.4151 (0.0004 -- 2.8467)  max mem: 16413
Epoch: [88]  [ 80/160]  eta: 0:01:14  lr: 0.000029  min_lr: 0.000007  loss: 1.5221 (1.5148)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7042 (6.9265)  time: 0.7614 (0.5144 -- 2.8781)  data: 0.2147 (0.0002 -- 2.3527)  max mem: 16413
[2023-09-04 18:48:18,033] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:48:18,033] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:48:18,034] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:48:18,034] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [88]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000007  loss: 1.3821 (1.5112)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9346 (7.0170)  time: 0.9158 (0.5322 -- 3.9868)  data: 0.3450 (0.0003 -- 3.4456)  max mem: 16413
Epoch: [88]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000007  loss: 1.5202 (1.5264)  loss_scale: 32768.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3887 (7.0013)  time: 0.9578 (0.5251 -- 3.4056)  data: 0.4069 (0.0002 -- 2.8667)  max mem: 16413
Epoch: [88]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.4465 (1.5107)  loss_scale: 32768.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0098 (7.0801)  time: 0.8279 (0.5126 -- 3.4022)  data: 0.2774 (0.0002 -- 2.8600)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.5238 (1.5047)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3504 (7.0776)  time: 0.7079 (0.4958 -- 2.9583)  data: 0.1833 (0.0002 -- 2.3617)  max mem: 16413
Epoch: [88] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.5238 (1.5040)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3504 (7.0776)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1542 (0.1542)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2673 (2.2673 -- 2.2673)  data: 2.0112 (2.0112 -- 2.0112)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2825 (0.4389)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4201 (0.2082 -- 2.2673)  data: 0.1959 (0.0007 -- 2.0112)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3266 (0.4703)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.9418)  time: 0.2230 (0.1699 -- 0.3420)  data: 0.0146 (0.0001 -- 0.1442)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5203 (0.5040)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (98.7552)  time: 0.2049 (0.1329 -- 0.3420)  data: 0.0140 (0.0001 -- 0.1442)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 86.929 Acc@5 97.718 loss 0.546
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.55%
Epoch: [89]  [  0/160]  eta: 0:21:18  lr: 0.000029  min_lr: 0.000007  loss: 1.6683 (1.6683)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4818 (6.4818)  time: 7.9896 (7.9896 -- 7.9896)  data: 7.4441 (7.4441 -- 7.4441)  max mem: 16413
Epoch: [89]  [ 20/160]  eta: 0:02:52  lr: 0.000028  min_lr: 0.000007  loss: 1.4772 (1.5056)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0076 (7.9144)  time: 0.8932 (0.5109 -- 4.7403)  data: 0.3421 (0.0005 -- 4.2025)  max mem: 16413
[2023-09-04 18:49:50,954] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14268
[2023-09-04 18:49:50,954] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14268
[2023-09-04 18:49:50,954] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:49:50,955] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 18:49:50,955] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [89]  [ 40/160]  eta: 0:02:07  lr: 0.000028  min_lr: 0.000007  loss: 1.3584 (1.5024)  loss_scale: 16384.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9297 (8.0243)  time: 0.8819 (0.5150 -- 3.2401)  data: 0.3332 (0.0002 -- 2.6972)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:36  lr: 0.000028  min_lr: 0.000007  loss: 1.4449 (1.5197)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1231 (7.8415)  time: 0.7814 (0.5289 -- 1.9964)  data: 0.2311 (0.0005 -- 1.4775)  max mem: 16413
Epoch: [89]  [ 80/160]  eta: 0:01:16  lr: 0.000028  min_lr: 0.000007  loss: 1.5153 (1.5247)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7787 (7.6320)  time: 0.9411 (0.5345 -- 2.4584)  data: 0.2732 (0.0004 -- 1.9376)  max mem: 16413
Epoch: [89]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000007  loss: 1.4253 (1.5170)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8826 (7.5981)  time: 0.8089 (0.5275 -- 2.3898)  data: 0.2433 (0.0004 -- 1.8725)  max mem: 16413
Epoch: [89]  [120/160]  eta: 0:00:37  lr: 0.000028  min_lr: 0.000007  loss: 1.3810 (1.4923)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3527 (7.4834)  time: 0.9093 (0.5303 -- 2.6366)  data: 0.1302 (0.0002 -- 2.0969)  max mem: 16413
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 1.6516 (1.5225)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6546 (7.4702)  time: 0.9862 (0.5299 -- 4.7697)  data: 0.3946 (0.0002 -- 4.2443)  max mem: 16413
[2023-09-04 18:51:40,150] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:51:40,150] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:51:40,150] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:51:40,150] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.5763 (1.5225)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6887 (7.3121)  time: 0.5389 (0.5004 -- 0.7138)  data: 0.0008 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [89] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.5763 (1.5480)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6887 (7.3121)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1409 (0.1409)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2915 (2.2915 -- 2.2915)  data: 2.0580 (2.0580 -- 2.0580)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.2246 (0.4482)  acc1: 100.0000 (91.9192)  acc5: 100.0000 (98.9899)  time: 0.4011 (0.1974 -- 2.2915)  data: 0.1881 (0.0006 -- 2.0580)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4498 (0.4981)  acc1: 88.8889 (89.4180)  acc5: 100.0000 (97.8836)  time: 0.2233 (0.1680 -- 0.3694)  data: 0.0146 (0.0001 -- 0.1831)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5721 (0.5315)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (97.9253)  time: 0.2078 (0.1324 -- 0.3694)  data: 0.0142 (0.0001 -- 0.1831)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 86.100 Acc@5 97.095 loss 0.568
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.55%
Epoch: [90]  [  0/160]  eta: 0:20:21  lr: 0.000028  min_lr: 0.000007  loss: 1.8734 (1.8734)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0432 (12.0432)  time: 7.6339 (7.6339 -- 7.6339)  data: 7.0968 (7.0968 -- 7.0968)  max mem: 16413
Epoch: [90]  [ 20/160]  eta: 0:02:43  lr: 0.000028  min_lr: 0.000007  loss: 1.5275 (1.5658)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8260 (7.9557)  time: 0.8474 (0.5255 -- 3.2307)  data: 0.2388 (0.0004 -- 2.7093)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:04  lr: 0.000028  min_lr: 0.000007  loss: 1.6730 (1.6168)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8323 (7.6254)  time: 0.8971 (0.5301 -- 3.2647)  data: 0.2979 (0.0006 -- 2.7364)  max mem: 16413
Epoch: [90]  [ 60/160]  eta: 0:01:39  lr: 0.000028  min_lr: 0.000007  loss: 1.6758 (1.6306)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0161 (7.4477)  time: 0.9030 (0.5267 -- 3.5120)  data: 0.1928 (0.0004 -- 2.4049)  max mem: 16413
Epoch: [90]  [ 80/160]  eta: 0:01:15  lr: 0.000028  min_lr: 0.000007  loss: 1.6553 (1.6200)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6047 (7.5656)  time: 0.7718 (0.5198 -- 2.4330)  data: 0.1574 (0.0002 -- 1.5975)  max mem: 16413
[2023-09-04 18:53:22,748] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14498
[2023-09-04 18:53:22,748] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14498
[2023-09-04 18:53:22,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:53:22,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:53:22,749] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [90]  [100/160]  eta: 0:00:56  lr: 0.000028  min_lr: 0.000007  loss: 1.5137 (1.6013)  loss_scale: 32768.0000 (32281.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9975 (7.4125)  time: 0.9313 (0.5177 -- 3.3064)  data: 0.0810 (0.0005 -- 0.9953)  max mem: 16413
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000007  loss: 1.4413 (1.5807)  loss_scale: 16384.0000 (29653.6860)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6369 (7.3802)  time: 0.8230 (0.5275 -- 2.4612)  data: 0.1053 (0.0004 -- 1.0720)  max mem: 16413
Epoch: [90]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 1.5698 (1.5738)  loss_scale: 16384.0000 (27771.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1481 (7.3634)  time: 0.9187 (0.5219 -- 3.1506)  data: 0.2695 (0.0003 -- 2.6375)  max mem: 16413
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.7005 (1.5747)  loss_scale: 16384.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1161 (7.4111)  time: 0.6008 (0.4955 -- 1.7486)  data: 0.0627 (0.0002 -- 1.2366)  max mem: 16413
Epoch: [90] Total time: 0:02:20 (0.8811 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.7005 (1.5561)  loss_scale: 16384.0000 (26419.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1161 (7.4111)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1478 (0.1478)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1931 (2.1931 -- 2.1931)  data: 1.9505 (1.9505 -- 1.9505)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.2962 (0.4861)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4101 (0.1959 -- 2.1931)  data: 0.1971 (0.0008 -- 1.9505)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4162 (0.5042)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9418)  time: 0.2243 (0.1705 -- 0.3694)  data: 0.0172 (0.0001 -- 0.1664)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5294 (0.5406)  acc1: 88.8889 (88.3817)  acc5: 100.0000 (98.3402)  time: 0.2118 (0.1327 -- 0.3694)  data: 0.0169 (0.0001 -- 0.1664)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 86.722 Acc@5 97.718 loss 0.573
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.55%
Epoch: [91]  [  0/160]  eta: 0:21:23  lr: 0.000028  min_lr: 0.000007  loss: 1.7461 (1.7461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3284 (8.3284)  time: 8.0238 (8.0238 -- 8.0238)  data: 6.3870 (6.3870 -- 6.3870)  max mem: 16413
Epoch: [91]  [ 20/160]  eta: 0:02:39  lr: 0.000028  min_lr: 0.000007  loss: 1.3388 (1.4446)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1318 (7.4354)  time: 0.7935 (0.5243 -- 5.2779)  data: 0.1110 (0.0004 -- 2.1837)  max mem: 16413
Epoch: [91]  [ 40/160]  eta: 0:02:08  lr: 0.000028  min_lr: 0.000007  loss: 1.5108 (1.4773)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7424 (7.3397)  time: 1.0037 (0.5297 -- 4.5361)  data: 0.0983 (0.0001 -- 1.3976)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:38  lr: 0.000028  min_lr: 0.000007  loss: 1.6879 (1.5443)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8342 (7.3504)  time: 0.8025 (0.5192 -- 3.5613)  data: 0.0012 (0.0003 -- 0.0021)  max mem: 16413
[2023-09-04 18:55:21,900] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14621
[2023-09-04 18:55:21,900] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 18:55:21,900] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-04 18:55:21,900] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14621
[2023-09-04 18:55:21,900] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [91]  [ 80/160]  eta: 0:01:18  lr: 0.000028  min_lr: 0.000007  loss: 1.5321 (1.5134)  loss_scale: 8192.0000 (14361.2840)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6500 (7.3756)  time: 0.9613 (0.5227 -- 4.1182)  data: 0.0010 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [91]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000007  loss: 1.4471 (1.5118)  loss_scale: 8192.0000 (13139.6436)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0379 (7.3740)  time: 0.7279 (0.5268 -- 4.0477)  data: 0.0015 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [91]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000007  loss: 1.2035 (1.4813)  loss_scale: 8192.0000 (12321.8512)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0212 (7.2123)  time: 1.0292 (0.5125 -- 4.8679)  data: 0.0014 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.2246 (1.4696)  loss_scale: 8192.0000 (11736.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0667 (7.0021)  time: 0.7667 (0.5208 -- 2.7297)  data: 0.0019 (0.0001 -- 0.0137)  max mem: 16413
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.2730 (1.4579)  loss_scale: 8192.0000 (11315.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4518 (7.0541)  time: 0.6629 (0.4971 -- 1.8166)  data: 0.0009 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [91] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.2730 (1.4839)  loss_scale: 8192.0000 (11315.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4518 (7.0541)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1288 (0.1288)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3175 (2.3175 -- 2.3175)  data: 2.0405 (2.0405 -- 2.0405)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1593 (0.4422)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (98.9899)  time: 0.4150 (0.2048 -- 2.3175)  data: 0.1866 (0.0008 -- 2.0405)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3741 (0.4711)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (97.8836)  time: 0.2163 (0.1693 -- 0.2841)  data: 0.0039 (0.0001 -- 0.0417)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5311 (0.5050)  acc1: 88.8889 (88.3817)  acc5: 100.0000 (97.9253)  time: 0.1980 (0.1328 -- 0.2533)  data: 0.0035 (0.0001 -- 0.0417)  max mem: 16413
Val: Total time: 0:00:07 (0.2826 s / it)
* Acc@1 87.759 Acc@5 97.718 loss 0.524
Accuracy of the network on the 482 val images: 87.76%
[2023-09-04 18:56:47,956] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 18:56:47,958] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 18:56:47,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 18:56:47,958] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 18:56:49,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 18:56:49,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 87.76%
Epoch: [92]  [  0/160]  eta: 0:19:45  lr: 0.000027  min_lr: 0.000007  loss: 1.3979 (1.3979)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9928 (6.9928)  time: 7.4090 (7.4090 -- 7.4090)  data: 6.8681 (6.8681 -- 6.8681)  max mem: 16413
Epoch: [92]  [ 20/160]  eta: 0:02:53  lr: 0.000027  min_lr: 0.000007  loss: 1.6121 (1.5250)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5369 (6.3205)  time: 0.9291 (0.5242 -- 3.0855)  data: 0.0989 (0.0003 -- 1.1488)  max mem: 16413
[2023-09-04 18:57:22,948] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:57:22,948] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 18:57:22,948] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:57:22,948] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [92]  [ 40/160]  eta: 0:02:09  lr: 0.000027  min_lr: 0.000007  loss: 1.5113 (1.4989)  loss_scale: 16384.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4652 (6.4294)  time: 0.9086 (0.5146 -- 3.7835)  data: 0.0759 (0.0001 -- 1.4920)  max mem: 16413
Epoch: [92]  [ 60/160]  eta: 0:01:43  lr: 0.000027  min_lr: 0.000007  loss: 1.3839 (1.4961)  loss_scale: 16384.0000 (12355.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1586 (6.3697)  time: 0.9563 (0.5268 -- 3.7156)  data: 0.0032 (0.0005 -- 0.0365)  max mem: 16413
Epoch: [92]  [ 80/160]  eta: 0:01:17  lr: 0.000027  min_lr: 0.000007  loss: 1.6125 (1.5340)  loss_scale: 16384.0000 (13349.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5126 (6.4488)  time: 0.7536 (0.5112 -- 2.6050)  data: 0.0016 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [92]  [100/160]  eta: 0:00:57  lr: 0.000027  min_lr: 0.000007  loss: 1.4121 (1.5176)  loss_scale: 16384.0000 (13950.7327)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3753 (6.6710)  time: 0.8925 (0.5099 -- 4.2382)  data: 0.0011 (0.0004 -- 0.0022)  max mem: 16413
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000007  loss: 1.4698 (1.4940)  loss_scale: 16384.0000 (14352.9256)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0175 (6.6638)  time: 0.7767 (0.5269 -- 3.1944)  data: 0.0023 (0.0005 -- 0.0128)  max mem: 16413
Epoch: [92]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.4931 (1.4962)  loss_scale: 16384.0000 (14641.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5791 (6.6714)  time: 0.9020 (0.5259 -- 2.2199)  data: 0.0099 (0.0003 -- 0.1611)  max mem: 16413
[2023-09-04 18:59:11,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:59:11,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 18:59:11,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 18:59:11,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.4703 (1.4998)  loss_scale: 16384.0000 (15052.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8926 (6.6355)  time: 0.7741 (0.4959 -- 2.6654)  data: 0.0168 (0.0002 -- 0.3135)  max mem: 16413
Epoch: [92] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.4703 (1.5046)  loss_scale: 16384.0000 (15052.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8926 (6.6355)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.1338 (0.1338)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1625 (2.1625 -- 2.1625)  data: 1.9239 (1.9239 -- 1.9239)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.2246 (0.4646)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4056 (0.2055 -- 2.1625)  data: 0.1854 (0.0007 -- 1.9239)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5090 (0.5012)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2279 (0.1696 -- 0.4535)  data: 0.0203 (0.0001 -- 0.2434)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5294 (0.5380)  acc1: 88.8889 (87.1369)  acc5: 100.0000 (97.9253)  time: 0.2102 (0.1331 -- 0.4535)  data: 0.0200 (0.0001 -- 0.2434)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 86.722 Acc@5 97.510 loss 0.561
Accuracy of the network on the 482 val images: 86.72%
Max accuracy: 87.76%
Epoch: [93]  [  0/160]  eta: 0:17:32  lr: 0.000027  min_lr: 0.000007  loss: 0.9244 (0.9244)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3700 (7.3700)  time: 6.5763 (6.5763 -- 6.5763)  data: 5.6988 (5.6988 -- 5.6988)  max mem: 16413
[2023-09-04 18:59:41,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14896
[2023-09-04 18:59:41,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14896
[2023-09-04 18:59:41,335] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:59:41,335] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 18:59:41,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [93]  [ 20/160]  eta: 0:02:45  lr: 0.000027  min_lr: 0.000007  loss: 1.4709 (1.4515)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2394 (6.5944)  time: 0.9106 (0.5389 -- 2.6478)  data: 0.1931 (0.0004 -- 1.4146)  max mem: 16413
Epoch: [93]  [ 40/160]  eta: 0:02:05  lr: 0.000027  min_lr: 0.000007  loss: 1.3602 (1.4649)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1350 (6.8469)  time: 0.9097 (0.5232 -- 3.1513)  data: 0.1805 (0.0002 -- 2.6187)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:37  lr: 0.000027  min_lr: 0.000007  loss: 1.4570 (1.4722)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7744 (6.9868)  time: 0.8275 (0.5335 -- 2.7146)  data: 0.2358 (0.0004 -- 1.7664)  max mem: 16413
Epoch: [93]  [ 80/160]  eta: 0:01:16  lr: 0.000027  min_lr: 0.000007  loss: 1.5347 (1.4773)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6896 (7.1024)  time: 0.9007 (0.5134 -- 4.1822)  data: 0.1347 (0.0002 -- 1.6463)  max mem: 16413
Epoch: [93]  [100/160]  eta: 0:00:56  lr: 0.000027  min_lr: 0.000007  loss: 1.3866 (1.4809)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1081 (7.3026)  time: 0.8986 (0.5295 -- 4.5993)  data: 0.0775 (0.0004 -- 1.5241)  max mem: 16413
[2023-09-04 19:01:09,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=80, lr=[6.800255005360772e-06, 6.800255005360772e-06, 7.555838894845303e-06, 7.555838894845303e-06, 8.395376549828112e-06, 8.395376549828112e-06, 9.328196166475682e-06, 9.328196166475682e-06, 1.0364662407195201e-05, 1.0364662407195201e-05, 1.1516291563550223e-05, 1.1516291563550223e-05, 1.2795879515055802e-05, 1.2795879515055802e-05, 1.4217643905617558e-05, 1.4217643905617558e-05, 1.5797382117352843e-05, 1.5797382117352843e-05, 1.7552646797058713e-05, 1.7552646797058713e-05, 1.9502940885620792e-05, 1.9502940885620792e-05, 2.1669934317356435e-05, 2.1669934317356435e-05, 2.4077704797062705e-05, 2.4077704797062705e-05, 2.675300533006967e-05, 2.675300533006967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 19:01:09,860] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=18.66588673900924, CurrSamplesPerSec=21.1927636218944, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [93]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000007  loss: 1.4835 (1.5039)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2032 (7.2983)  time: 0.7344 (0.5269 -- 2.8002)  data: 0.0020 (0.0001 -- 0.0084)  max mem: 16413
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.5892 (1.5038)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8600 (7.2996)  time: 0.9046 (0.5400 -- 4.1259)  data: 0.0025 (0.0004 -- 0.0150)  max mem: 16413
[2023-09-04 19:01:33,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:01:33,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:01:33,474] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:01:33,474] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.2748 (1.4918)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7843 (7.2926)  time: 0.7060 (0.5020 -- 2.8057)  data: 0.0066 (0.0002 -- 0.0965)  max mem: 16413
Epoch: [93] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.2748 (1.5158)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7843 (7.2926)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.1291 (0.1291)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1572 (2.1572 -- 2.1572)  data: 1.9489 (1.9489 -- 1.9489)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4501 (0.5274)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4132 (0.2071 -- 2.1572)  data: 0.1964 (0.0007 -- 1.9489)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4501 (0.5194)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2308 (0.1708 -- 0.3741)  data: 0.0200 (0.0001 -- 0.1582)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4618 (0.5433)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2121 (0.1329 -- 0.3741)  data: 0.0167 (0.0001 -- 0.1582)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 85.685 Acc@5 97.510 loss 0.581
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.76%
Epoch: [94]  [  0/160]  eta: 0:19:27  lr: 0.000027  min_lr: 0.000007  loss: 1.6511 (1.6511)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8713 (4.8713)  time: 7.2988 (7.2988 -- 7.2988)  data: 6.7601 (6.7601 -- 6.7601)  max mem: 16413
[2023-09-04 19:02:05,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15049
[2023-09-04 19:02:05,837] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:02:05,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15049
[2023-09-04 19:02:05,838] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:02:05,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [94]  [ 20/160]  eta: 0:02:48  lr: 0.000027  min_lr: 0.000007  loss: 1.2963 (1.4132)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9127 (7.1129)  time: 0.8955 (0.5241 -- 3.2714)  data: 0.2298 (0.0003 -- 2.0223)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:01:56  lr: 0.000027  min_lr: 0.000007  loss: 1.5043 (1.4428)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1874 (7.2462)  time: 0.7291 (0.5270 -- 2.1673)  data: 0.1388 (0.0002 -- 1.6082)  max mem: 16413
Epoch: [94]  [ 60/160]  eta: 0:01:35  lr: 0.000027  min_lr: 0.000007  loss: 1.6044 (1.4863)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2949 (7.4533)  time: 0.9171 (0.5364 -- 2.4439)  data: 0.3320 (0.0008 -- 1.9289)  max mem: 16413
Epoch: [94]  [ 80/160]  eta: 0:01:13  lr: 0.000026  min_lr: 0.000007  loss: 1.2978 (1.4736)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0670 (7.4975)  time: 0.8296 (0.5265 -- 2.4786)  data: 0.1584 (0.0003 -- 1.4603)  max mem: 16413
Epoch: [94]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000007  loss: 1.5599 (1.4873)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2409 (7.4746)  time: 0.9734 (0.5233 -- 3.7118)  data: 0.4111 (0.0003 -- 3.1977)  max mem: 16413
Epoch: [94]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000007  loss: 1.5733 (1.5004)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8508 (7.4022)  time: 0.8212 (0.5251 -- 2.5287)  data: 0.2738 (0.0006 -- 1.9981)  max mem: 16413
[2023-09-04 19:03:55,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:03:55,521] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:03:55,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:03:55,522] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [94]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.6646 (1.5269)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7434 (7.5052)  time: 0.8186 (0.5376 -- 1.7818)  data: 0.2639 (0.0002 -- 1.2389)  max mem: 16413
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.5793 (1.5271)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0579 (7.5575)  time: 0.7092 (0.4957 -- 3.7250)  data: 0.1735 (0.0002 -- 3.1848)  max mem: 16413
Epoch: [94] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.5793 (1.5203)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0579 (7.5575)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1925 (0.1925)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4270 (2.4270 -- 2.4270)  data: 2.1819 (2.1819 -- 2.1819)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1925 (0.5128)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4623 (0.2059 -- 2.4270)  data: 0.2442 (0.0006 -- 2.1819)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5081 (0.5319)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2297 (0.1704 -- 0.7108)  data: 0.0254 (0.0001 -- 0.4951)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5729 (0.5593)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (99.1701)  time: 0.2158 (0.1329 -- 0.7108)  data: 0.0251 (0.0001 -- 0.4951)  max mem: 16413
Val: Total time: 0:00:08 (0.2983 s / it)
* Acc@1 85.685 Acc@5 98.340 loss 0.604
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.76%
Epoch: [95]  [  0/160]  eta: 0:20:48  lr: 0.000026  min_lr: 0.000007  loss: 1.4899 (1.4899)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2818 (9.2818)  time: 7.8009 (7.8009 -- 7.8009)  data: 7.2459 (7.2459 -- 7.2459)  max mem: 16413
[2023-09-04 19:04:34,342] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15208
[2023-09-04 19:04:34,342] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15208
[2023-09-04 19:04:34,343] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:04:34,343] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:04:34,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [95]  [ 20/160]  eta: 0:02:50  lr: 0.000026  min_lr: 0.000007  loss: 1.6086 (1.6090)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4051 (7.4045)  time: 0.8924 (0.5259 -- 3.9695)  data: 0.1580 (0.0002 -- 2.5754)  max mem: 16413
Epoch: [95]  [ 40/160]  eta: 0:02:02  lr: 0.000026  min_lr: 0.000007  loss: 1.4672 (1.5623)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4279 (7.0948)  time: 0.8161 (0.5270 -- 4.6473)  data: 0.0013 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [95]  [ 60/160]  eta: 0:01:38  lr: 0.000026  min_lr: 0.000007  loss: 1.5490 (1.5544)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6689 (7.1435)  time: 0.9039 (0.5268 -- 3.6092)  data: 0.3008 (0.0007 -- 3.0476)  max mem: 16413
Epoch: [95]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000007  loss: 1.5946 (1.5545)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5321 (7.0747)  time: 0.8134 (0.5322 -- 3.3763)  data: 0.2608 (0.0004 -- 2.8423)  max mem: 16413
Epoch: [95]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000007  loss: 1.4814 (1.5493)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5208 (7.0173)  time: 0.9447 (0.5205 -- 3.7803)  data: 0.3960 (0.0004 -- 3.2589)  max mem: 16413
Epoch: [95]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000007  loss: 1.4696 (1.5334)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3200 (7.1711)  time: 0.8534 (0.5283 -- 4.1648)  data: 0.2984 (0.0001 -- 3.6350)  max mem: 16413
[2023-09-04 19:06:27,457] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:06:27,457] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:06:27,458] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:06:27,458] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.4119 (1.5232)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9269 (7.1457)  time: 0.9085 (0.5256 -- 4.1806)  data: 0.3655 (0.0006 -- 3.6495)  max mem: 16413
[2023-09-04 19:06:35,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15348
[2023-09-04 19:06:35,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15348
[2023-09-04 19:06:35,255] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:06:35,255] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:06:35,256] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.5371 (1.5219)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8826 (7.1468)  time: 0.6530 (0.4960 -- 2.2244)  data: 0.1251 (0.0002 -- 1.6844)  max mem: 16413
Epoch: [95] Total time: 0:02:23 (0.8938 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.5371 (1.5327)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8826 (7.1468)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1681 (0.1681)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2475 (2.2475 -- 2.2475)  data: 2.0563 (2.0563 -- 2.0563)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4336 (0.5117)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4088 (0.2114 -- 2.2475)  data: 0.1938 (0.0006 -- 2.0563)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5004 (0.5175)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (99.4709)  time: 0.2253 (0.1692 -- 0.4341)  data: 0.0195 (0.0001 -- 0.2477)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5390 (0.5652)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2077 (0.1323 -- 0.4341)  data: 0.0191 (0.0001 -- 0.2477)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 86.100 Acc@5 97.925 loss 0.589
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.76%
Epoch: [96]  [  0/160]  eta: 0:16:54  lr: 0.000026  min_lr: 0.000007  loss: 1.4438 (1.4438)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1135 (4.1135)  time: 6.3398 (6.3398 -- 6.3398)  data: 4.8737 (4.8737 -- 4.8737)  max mem: 16413
Epoch: [96]  [ 20/160]  eta: 0:02:32  lr: 0.000026  min_lr: 0.000007  loss: 1.4415 (1.5828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2973 (7.4215)  time: 0.8238 (0.5327 -- 3.5039)  data: 0.1421 (0.0008 -- 1.4405)  max mem: 16413
Epoch: [96]  [ 40/160]  eta: 0:02:00  lr: 0.000026  min_lr: 0.000007  loss: 1.3937 (1.4945)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1214 (7.3259)  time: 0.9131 (0.5259 -- 2.9083)  data: 0.2210 (0.0008 -- 1.6791)  max mem: 16413
Epoch: [96]  [ 60/160]  eta: 0:01:36  lr: 0.000026  min_lr: 0.000007  loss: 1.4372 (1.4995)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1132 (7.2497)  time: 0.9009 (0.5183 -- 3.4167)  data: 0.0659 (0.0005 -- 0.7025)  max mem: 16413
Epoch: [96]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000007  loss: 1.4389 (1.4917)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6241 (7.3347)  time: 0.8637 (0.5301 -- 3.3128)  data: 0.0396 (0.0004 -- 0.7296)  max mem: 16413
Epoch: [96]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000007  loss: 1.4593 (1.4934)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1067 (7.2647)  time: 0.9713 (0.5278 -- 4.7876)  data: 0.4261 (0.0003 -- 4.2256)  max mem: 16413
[2023-09-04 19:08:38,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:08:38,775] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:08:38,777] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:08:38,778] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000007  loss: 1.4937 (1.4781)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4290 (7.2970)  time: 0.7863 (0.5312 -- 2.6156)  data: 0.2085 (0.0009 -- 2.0764)  max mem: 16413
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.5843 (1.4924)  loss_scale: 32768.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4255 (7.2656)  time: 0.9459 (0.5224 -- 3.3601)  data: 0.2544 (0.0007 -- 2.8227)  max mem: 16413
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000006  loss: 1.6881 (1.5134)  loss_scale: 32768.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3106 (7.2659)  time: 0.6985 (0.4934 -- 2.4061)  data: 0.1747 (0.0001 -- 1.8727)  max mem: 16413
Epoch: [96] Total time: 0:02:23 (0.8994 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000006  loss: 1.6881 (1.5091)  loss_scale: 32768.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3106 (7.2659)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1538 (0.1538)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4272 (2.4272 -- 2.4272)  data: 2.1702 (2.1702 -- 2.1702)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2851 (0.4831)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4207 (0.1991 -- 2.4272)  data: 0.1986 (0.0004 -- 2.1702)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4458 (0.4955)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (99.4709)  time: 0.2194 (0.1708 -- 0.4146)  data: 0.0121 (0.0001 -- 0.2257)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4480 (0.5371)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (99.1701)  time: 0.2028 (0.1332 -- 0.4146)  data: 0.0117 (0.0001 -- 0.2257)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.565
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.76%
Epoch: [97]  [  0/160]  eta: 0:19:41  lr: 0.000026  min_lr: 0.000006  loss: 2.0091 (2.0091)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9287 (8.9287)  time: 7.3873 (7.3873 -- 7.3873)  data: 6.8357 (6.8357 -- 6.8357)  max mem: 16413
Epoch: [97]  [ 20/160]  eta: 0:02:36  lr: 0.000025  min_lr: 0.000006  loss: 1.4262 (1.5431)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3752 (6.7753)  time: 0.8081 (0.5213 -- 4.8390)  data: 0.2626 (0.0004 -- 4.3245)  max mem: 16413
Epoch: [97]  [ 40/160]  eta: 0:02:00  lr: 0.000025  min_lr: 0.000006  loss: 1.6411 (1.5874)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9858 (7.2476)  time: 0.8817 (0.5263 -- 2.0183)  data: 0.1432 (0.0002 -- 0.9676)  max mem: 16413
Epoch: [97]  [ 60/160]  eta: 0:01:39  lr: 0.000025  min_lr: 0.000006  loss: 1.6563 (1.5746)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0932 (7.6736)  time: 0.9862 (0.5350 -- 3.0472)  data: 0.3183 (0.0006 -- 2.5370)  max mem: 16413
[2023-09-04 19:10:24,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15584
[2023-09-04 19:10:24,191] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:10:24,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15584
[2023-09-04 19:10:24,193] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:10:24,193] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [97]  [ 80/160]  eta: 0:01:17  lr: 0.000025  min_lr: 0.000006  loss: 1.4688 (1.5444)  loss_scale: 16384.0000 (29329.3827)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6342 (7.2567)  time: 0.8532 (0.5232 -- 4.1663)  data: 0.3087 (0.0003 -- 3.6334)  max mem: 16413
Epoch: [97]  [100/160]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000006  loss: 1.4576 (1.5271)  loss_scale: 16384.0000 (26765.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5476 (7.2620)  time: 0.9228 (0.5223 -- 3.3127)  data: 0.3758 (0.0004 -- 2.7982)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000006  loss: 1.6202 (1.5324)  loss_scale: 16384.0000 (25049.9174)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2880 (7.3339)  time: 0.8644 (0.5082 -- 3.7879)  data: 0.3181 (0.0003 -- 3.2674)  max mem: 16413
Epoch: [97]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 1.4038 (1.5322)  loss_scale: 16384.0000 (23820.7092)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3978 (7.3099)  time: 0.7930 (0.5393 -- 2.8576)  data: 0.2391 (0.0007 -- 2.3305)  max mem: 16413
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.4444 (1.5235)  loss_scale: 16384.0000 (22937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5151 (7.3574)  time: 0.6638 (0.4952 -- 3.2946)  data: 0.1396 (0.0003 -- 2.7773)  max mem: 16413
Epoch: [97] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.4444 (1.5154)  loss_scale: 16384.0000 (22937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5151 (7.3574)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1389 (0.1389)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3087 (2.3087 -- 2.3087)  data: 2.1028 (2.1028 -- 2.1028)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4264 (0.5166)  acc1: 77.7778 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4204 (0.2093 -- 2.3087)  data: 0.2015 (0.0003 -- 2.1028)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4511 (0.5160)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (99.4709)  time: 0.2215 (0.1724 -- 0.3259)  data: 0.0104 (0.0002 -- 0.0999)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4511 (0.5661)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.3402)  time: 0.2047 (0.1332 -- 0.3259)  data: 0.0101 (0.0001 -- 0.0999)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.580
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.76%
Epoch: [98]  [  0/160]  eta: 0:18:18  lr: 0.000025  min_lr: 0.000006  loss: 0.9650 (0.9650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4404 (10.4404)  time: 6.8670 (6.8670 -- 6.8670)  data: 6.3472 (6.3472 -- 6.3472)  max mem: 16413
Epoch: [98]  [ 20/160]  eta: 0:02:44  lr: 0.000025  min_lr: 0.000006  loss: 1.5433 (1.4961)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9226 (7.6424)  time: 0.8884 (0.5388 -- 4.2144)  data: 0.3364 (0.0008 -- 3.6860)  max mem: 16413
[2023-09-04 19:12:29,118] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:12:29,118] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:12:29,118] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:12:29,119] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [98]  [ 40/160]  eta: 0:02:09  lr: 0.000025  min_lr: 0.000006  loss: 1.5370 (1.5236)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6413 (7.2936)  time: 0.9874 (0.5293 -- 3.7835)  data: 0.4344 (0.0002 -- 3.2267)  max mem: 16413
Epoch: [98]  [ 60/160]  eta: 0:01:39  lr: 0.000025  min_lr: 0.000006  loss: 1.4734 (1.4945)  loss_scale: 32768.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4856 (7.4069)  time: 0.8197 (0.5078 -- 5.5168)  data: 0.2756 (0.0001 -- 5.0065)  max mem: 16413
[2023-09-04 19:12:52,619] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15741
[2023-09-04 19:12:52,619] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15741
[2023-09-04 19:12:52,620] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:12:52,620] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:12:52,620] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [98]  [ 80/160]  eta: 0:01:18  lr: 0.000025  min_lr: 0.000006  loss: 1.5769 (1.5125)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3674 (7.3215)  time: 0.9582 (0.5298 -- 4.0884)  data: 0.4096 (0.0007 -- 3.5523)  max mem: 16413
Epoch: [98]  [100/160]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000006  loss: 1.4580 (1.4919)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7971 (7.3477)  time: 0.8103 (0.5139 -- 3.8552)  data: 0.2610 (0.0004 -- 3.3428)  max mem: 16413
Epoch: [98]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000006  loss: 1.6736 (1.5025)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1830 (7.3803)  time: 0.9222 (0.5253 -- 3.5852)  data: 0.3545 (0.0006 -- 3.0377)  max mem: 16413
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 1.6062 (1.5097)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3304 (7.4448)  time: 0.7266 (0.5309 -- 2.6636)  data: 0.1785 (0.0004 -- 2.1186)  max mem: 16413
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.5074 (1.5107)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5967 (7.4294)  time: 0.7531 (0.4946 -- 3.1221)  data: 0.1698 (0.0001 -- 2.1658)  max mem: 16413
Epoch: [98] Total time: 0:02:23 (0.8979 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.5074 (1.5067)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5967 (7.4294)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1408 (0.1408)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2853 (2.2853 -- 2.2853)  data: 2.0313 (2.0313 -- 2.0313)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3727 (0.5285)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4115 (0.2051 -- 2.2853)  data: 0.1871 (0.0005 -- 2.0313)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4445 (0.5222)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2222 (0.1703 -- 0.3687)  data: 0.0104 (0.0001 -- 0.1771)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5012 (0.5750)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2012 (0.1328 -- 0.3687)  data: 0.0099 (0.0001 -- 0.1771)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 85.270 Acc@5 97.718 loss 0.590
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.76%
Epoch: [99]  [  0/160]  eta: 0:17:03  lr: 0.000025  min_lr: 0.000006  loss: 1.8102 (1.8102)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6316 (6.6316)  time: 6.3991 (6.3991 -- 6.3991)  data: 5.3864 (5.3864 -- 5.3864)  max mem: 16413
Epoch: [99]  [ 20/160]  eta: 0:02:38  lr: 0.000025  min_lr: 0.000006  loss: 1.4862 (1.4854)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7921 (8.2037)  time: 0.8662 (0.5228 -- 2.4178)  data: 0.1433 (0.0008 -- 0.8738)  max mem: 16413
[2023-09-04 19:14:56,363] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:14:56,363] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:14:56,365] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:14:56,365] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [99]  [ 40/160]  eta: 0:02:04  lr: 0.000025  min_lr: 0.000006  loss: 1.5517 (1.5498)  loss_scale: 32768.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7585 (7.7097)  time: 0.9358 (0.5226 -- 3.4429)  data: 0.2023 (0.0002 -- 2.8897)  max mem: 16413
Epoch: [99]  [ 60/160]  eta: 0:01:35  lr: 0.000025  min_lr: 0.000006  loss: 1.6019 (1.5660)  loss_scale: 32768.0000 (24710.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3096 (7.4817)  time: 0.8005 (0.5271 -- 2.4499)  data: 0.0702 (0.0004 -- 0.4998)  max mem: 16413
Epoch: [99]  [ 80/160]  eta: 0:01:14  lr: 0.000025  min_lr: 0.000006  loss: 1.5260 (1.5234)  loss_scale: 32768.0000 (26699.8519)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0597 (7.5430)  time: 0.8712 (0.5168 -- 2.6058)  data: 0.3210 (0.0005 -- 2.0702)  max mem: 16413
Epoch: [99]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000006  loss: 1.5108 (1.5068)  loss_scale: 32768.0000 (27901.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2812 (7.5001)  time: 0.9275 (0.5192 -- 2.7338)  data: 0.3755 (0.0006 -- 2.2163)  max mem: 16413
Epoch: [99]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000006  loss: 1.3980 (1.5015)  loss_scale: 32768.0000 (28705.8512)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5893 (7.5044)  time: 0.8204 (0.5338 -- 3.3984)  data: 0.2615 (0.0007 -- 2.8139)  max mem: 16413
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000006  loss: 1.4909 (1.4978)  loss_scale: 32768.0000 (29282.0426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6676 (7.4190)  time: 0.9856 (0.4987 -- 4.2058)  data: 0.1330 (0.0003 -- 2.6438)  max mem: 16413
[2023-09-04 19:16:46,173] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:16:46,174] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 19:16:46,174] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:16:46,174] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 19:16:46,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=85, lr=[6.20369869372962e-06, 6.20369869372962e-06, 6.892998548588467e-06, 6.892998548588467e-06, 7.658887276209406e-06, 7.658887276209406e-06, 8.509874751343784e-06, 8.509874751343784e-06, 9.455416390381983e-06, 9.455416390381983e-06, 1.0506018211535537e-05, 1.0506018211535537e-05, 1.1673353568372818e-05, 1.1673353568372818e-05, 1.2970392853747574e-05, 1.2970392853747574e-05, 1.4411547615275083e-05, 1.4411547615275083e-05, 1.601283068363898e-05, 1.601283068363898e-05, 1.77920340929322e-05, 1.77920340929322e-05, 1.9768926769924666e-05, 1.9768926769924666e-05, 2.1965474188805186e-05, 2.1965474188805186e-05, 2.440608243200576e-05, 2.440608243200576e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 19:16:46,681] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=18.731906672661935, CurrSamplesPerSec=24.695946572537615, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.5503 (1.4960)  loss_scale: 32768.0000 (30105.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8804 (7.4390)  time: 0.6984 (0.4951 -- 4.1521)  data: 0.1831 (0.0002 -- 3.6478)  max mem: 16413
Epoch: [99] Total time: 0:02:24 (0.9000 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.5503 (1.5033)  loss_scale: 32768.0000 (30105.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8804 (7.4390)
[2023-09-04 19:16:46,685] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-09-04 19:16:46,686] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-09-04 19:16:46,686] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-09-04 19:16:46,687] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-09-04 19:16:47,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-09-04 19:16:47,790] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1479 (0.1479)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2241 (2.2241 -- 2.2241)  data: 2.0198 (2.0198 -- 2.0198)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5878 (0.6026)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (100.0000)  time: 0.4110 (0.2059 -- 2.2241)  data: 0.1947 (0.0007 -- 2.0198)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4412 (0.5359)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (99.4709)  time: 0.2268 (0.1688 -- 0.3772)  data: 0.0200 (0.0001 -- 0.1749)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4412 (0.5836)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (99.1701)  time: 0.2110 (0.1322 -- 0.3772)  data: 0.0197 (0.0001 -- 0.1749)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 83.817 Acc@5 98.133 loss 0.635
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 87.76%
Epoch: [100]  [  0/160]  eta: 0:19:15  lr: 0.000024  min_lr: 0.000006  loss: 1.7749 (1.7749)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0076 (7.0076)  time: 7.2217 (7.2217 -- 7.2217)  data: 5.9140 (5.9140 -- 5.9140)  max mem: 16413
[2023-09-04 19:17:04,442] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16003
[2023-09-04 19:17:04,442] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16003
[2023-09-04 19:17:04,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 19:17:04,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 19:17:04,443] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [100]  [ 20/160]  eta: 0:02:37  lr: 0.000024  min_lr: 0.000006  loss: 1.3578 (1.4837)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6150 (6.8802)  time: 0.8239 (0.5264 -- 3.7392)  data: 0.0171 (0.0006 -- 0.3144)  max mem: 16413
Epoch: [100]  [ 40/160]  eta: 0:02:09  lr: 0.000024  min_lr: 0.000006  loss: 1.2425 (1.4500)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0809 (6.8809)  time: 1.0266 (0.5300 -- 4.3267)  data: 0.0543 (0.0004 -- 0.7557)  max mem: 16413
Epoch: [100]  [ 60/160]  eta: 0:01:38  lr: 0.000024  min_lr: 0.000006  loss: 1.4875 (1.4696)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6397 (6.8671)  time: 0.7929 (0.5267 -- 3.2574)  data: 0.0019 (0.0004 -- 0.0050)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:18  lr: 0.000024  min_lr: 0.000006  loss: 1.6820 (1.4857)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0789 (7.0783)  time: 0.9707 (0.5243 -- 2.7346)  data: 0.2011 (0.0004 -- 2.2043)  max mem: 16413
Epoch: [100]  [100/160]  eta: 0:00:57  lr: 0.000024  min_lr: 0.000006  loss: 1.4202 (1.4920)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5647 (7.1521)  time: 0.8591 (0.5213 -- 3.1727)  data: 0.3015 (0.0004 -- 2.6432)  max mem: 16413
[2023-09-04 19:18:32,804] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16101
[2023-09-04 19:18:32,804] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16101
[2023-09-04 19:18:32,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:18:32,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:18:32,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000006  loss: 1.5480 (1.4964)  loss_scale: 16384.0000 (30872.3306)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4101 (7.2561)  time: 0.8007 (0.5261 -- 3.0416)  data: 0.2163 (0.0005 -- 2.5028)  max mem: 16413
Epoch: [100]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000006  loss: 1.6708 (1.5148)  loss_scale: 16384.0000 (28817.2482)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0799 (7.2542)  time: 1.0520 (0.5364 -- 4.3604)  data: 0.4827 (0.0002 -- 3.8434)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.6832 (1.5302)  loss_scale: 16384.0000 (27340.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2712 (7.3076)  time: 0.7532 (0.4944 -- 4.3604)  data: 0.2416 (0.0002 -- 3.8434)  max mem: 16413
Epoch: [100] Total time: 0:02:24 (0.9030 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.6832 (1.5444)  loss_scale: 16384.0000 (27340.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2712 (7.3076)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1298 (0.1298)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2254 (2.2254 -- 2.2254)  data: 2.0177 (2.0177 -- 2.0177)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3489 (0.5059)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4050 (0.2055 -- 2.2254)  data: 0.1846 (0.0006 -- 2.0177)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4397 (0.5098)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2260 (0.1686 -- 0.5013)  data: 0.0166 (0.0001 -- 0.3166)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5131 (0.5636)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (99.1701)  time: 0.2063 (0.1319 -- 0.5013)  data: 0.0162 (0.0001 -- 0.3166)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.607
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 87.76%
Epoch: [101]  [  0/160]  eta: 0:17:58  lr: 0.000024  min_lr: 0.000006  loss: 1.4614 (1.4614)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8854 (7.8854)  time: 6.7396 (6.7396 -- 6.7396)  data: 6.2075 (6.2075 -- 6.2075)  max mem: 16413
Epoch: [101]  [ 20/160]  eta: 0:02:45  lr: 0.000024  min_lr: 0.000006  loss: 1.5189 (1.5092)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5377 (7.2699)  time: 0.9072 (0.5217 -- 2.5577)  data: 0.1862 (0.0008 -- 2.0277)  max mem: 16413
Epoch: [101]  [ 40/160]  eta: 0:02:03  lr: 0.000024  min_lr: 0.000006  loss: 1.4106 (1.4889)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9037 (7.1983)  time: 0.8647 (0.5195 -- 3.3972)  data: 0.0500 (0.0003 -- 0.9785)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:41  lr: 0.000024  min_lr: 0.000006  loss: 1.3986 (1.4733)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7715 (7.3093)  time: 0.9741 (0.5134 -- 4.0117)  data: 0.0019 (0.0002 -- 0.0071)  max mem: 16413
[2023-09-04 19:20:39,684] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:20:39,685] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:20:39,685] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:20:39,685] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [101]  [ 80/160]  eta: 0:01:19  lr: 0.000024  min_lr: 0.000006  loss: 1.3880 (1.4626)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4353 (7.3009)  time: 0.9239 (0.5172 -- 5.3646)  data: 0.0009 (0.0004 -- 0.0021)  max mem: 16413
Epoch: [101]  [100/160]  eta: 0:00:57  lr: 0.000024  min_lr: 0.000006  loss: 1.4336 (1.4756)  loss_scale: 32768.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2966 (7.3183)  time: 0.8405 (0.5340 -- 2.9331)  data: 0.0017 (0.0005 -- 0.0045)  max mem: 16413
Epoch: [101]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000006  loss: 1.2162 (1.4673)  loss_scale: 32768.0000 (23289.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9498 (7.3478)  time: 0.8654 (0.5145 -- 4.4004)  data: 0.0014 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000006  loss: 1.4497 (1.4718)  loss_scale: 32768.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9087 (7.4071)  time: 0.9013 (0.5128 -- 3.9236)  data: 0.0008 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.4371 (1.4702)  loss_scale: 32768.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0311 (7.3941)  time: 0.6122 (0.4939 -- 2.2504)  data: 0.0007 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [101] Total time: 0:02:24 (0.9003 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.4371 (1.5099)  loss_scale: 32768.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0311 (7.3941)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1526 (0.1526)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4167 (2.4167 -- 2.4167)  data: 2.1673 (2.1673 -- 2.1673)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4043 (0.5376)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4113 (0.1850 -- 2.4167)  data: 0.1979 (0.0006 -- 2.1673)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5796 (0.5521)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2162 (0.1720 -- 0.5078)  data: 0.0167 (0.0001 -- 0.3214)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5796 (0.5903)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2032 (0.1330 -- 0.5078)  data: 0.0165 (0.0001 -- 0.3214)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 85.270 Acc@5 97.718 loss 0.601
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 87.76%
Epoch: [102]  [  0/160]  eta: 0:21:52  lr: 0.000024  min_lr: 0.000006  loss: 1.9791 (1.9791)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3568 (4.3568)  time: 8.2006 (8.2006 -- 8.2006)  data: 7.6416 (7.6416 -- 7.6416)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:42  lr: 0.000024  min_lr: 0.000006  loss: 1.4709 (1.5531)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9528 (6.6285)  time: 0.8073 (0.5339 -- 4.3401)  data: 0.2491 (0.0006 -- 3.8103)  max mem: 16413
[2023-09-04 19:22:39,248] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:22:39,248] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 19:22:39,249] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:22:39,249] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 19:22:40,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16360
[2023-09-04 19:22:40,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16360
[2023-09-04 19:22:40,370] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 19:22:40,370] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 19:22:40,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [102]  [ 40/160]  eta: 0:01:59  lr: 0.000024  min_lr: 0.000006  loss: 1.6150 (1.5402)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6803 (7.0245)  time: 0.8213 (0.5282 -- 2.1055)  data: 0.2762 (0.0006 -- 1.5532)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:37  lr: 0.000024  min_lr: 0.000006  loss: 1.5548 (1.5094)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1302 (7.1333)  time: 0.9215 (0.5182 -- 2.5510)  data: 0.3822 (0.0006 -- 2.0085)  max mem: 16413
Epoch: [102]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000006  loss: 1.4816 (1.4958)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2992 (7.1719)  time: 0.8945 (0.5283 -- 2.6723)  data: 0.3475 (0.0003 -- 2.1558)  max mem: 16413
Epoch: [102]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000006  loss: 1.6016 (1.5052)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4590 (7.0750)  time: 0.8856 (0.5284 -- 2.6557)  data: 0.3326 (0.0005 -- 2.1379)  max mem: 16413
[2023-09-04 19:23:41,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16429
[2023-09-04 19:23:41,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16429
[2023-09-04 19:23:41,359] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:23:41,359] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:23:41,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [102]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000006  loss: 1.5249 (1.5101)  loss_scale: 16384.0000 (31684.7603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0024 (7.0831)  time: 0.8141 (0.5293 -- 2.5198)  data: 0.1635 (0.0005 -- 1.9944)  max mem: 16413
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.5689 (1.5027)  loss_scale: 16384.0000 (29514.4397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9193 (7.0795)  time: 0.8915 (0.5382 -- 3.0120)  data: 0.2872 (0.0003 -- 2.4569)  max mem: 16413
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.4907 (1.4993)  loss_scale: 16384.0000 (27955.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2804 (6.9939)  time: 0.6873 (0.4978 -- 2.2290)  data: 0.0845 (0.0002 -- 1.6768)  max mem: 16413
Epoch: [102] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.4907 (1.5043)  loss_scale: 16384.0000 (27955.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2804 (6.9939)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1410 (0.1410)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2062 (2.2062 -- 2.2062)  data: 1.9953 (1.9953 -- 1.9953)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5746 (0.5700)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.4122 (0.1974 -- 2.2062)  data: 0.2014 (0.0004 -- 1.9953)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4861 (0.5594)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2304 (0.1687 -- 0.5239)  data: 0.0261 (0.0001 -- 0.2975)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4558 (0.5896)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2174 (0.1327 -- 0.5239)  data: 0.0257 (0.0001 -- 0.2975)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 84.855 Acc@5 97.303 loss 0.604
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 87.76%
Epoch: [103]  [  0/160]  eta: 0:20:41  lr: 0.000023  min_lr: 0.000006  loss: 1.3389 (1.3389)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4479 (7.4479)  time: 7.7624 (7.7624 -- 7.7624)  data: 5.4978 (5.4978 -- 5.4978)  max mem: 16413
Epoch: [103]  [ 20/160]  eta: 0:02:40  lr: 0.000023  min_lr: 0.000006  loss: 1.5857 (1.5586)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5882 (6.7677)  time: 0.8125 (0.5233 -- 2.7647)  data: 0.1774 (0.0002 -- 2.0418)  max mem: 16413
Epoch: [103]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000006  loss: 1.6836 (1.5717)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1141 (7.0034)  time: 0.8670 (0.5276 -- 2.8549)  data: 0.1379 (0.0002 -- 2.3247)  max mem: 16413
Epoch: [103]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000006  loss: 1.5194 (1.5442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5451 (6.9670)  time: 0.8916 (0.5248 -- 3.1168)  data: 0.3003 (0.0005 -- 2.5610)  max mem: 16413
[2023-09-04 19:25:44,260] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:25:44,260] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:25:44,261] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:25:44,261] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [103]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000006  loss: 1.4641 (1.5127)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8644 (7.0786)  time: 0.8324 (0.5389 -- 2.6052)  data: 0.1745 (0.0003 -- 2.0819)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000006  loss: 1.5414 (1.5286)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1981 (6.9859)  time: 0.9098 (0.5264 -- 2.5431)  data: 0.1730 (0.0004 -- 1.9963)  max mem: 16413
[2023-09-04 19:26:12,335] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16590
[2023-09-04 19:26:12,335] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16590
[2023-09-04 19:26:12,335] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:26:12,335] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:26:12,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [103]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000006  loss: 1.5573 (1.5202)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0704 (7.1319)  time: 0.9070 (0.5001 -- 3.7700)  data: 0.2403 (0.0006 -- 3.2183)  max mem: 16413
Epoch: [103]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.5444 (1.5207)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8286 (7.0628)  time: 0.8380 (0.5333 -- 2.5212)  data: 0.0994 (0.0003 -- 1.9589)  max mem: 16413
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.6185 (1.5222)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0938 (7.0276)  time: 0.7063 (0.4992 -- 1.8372)  data: 0.1133 (0.0003 -- 1.3068)  max mem: 16413
Epoch: [103] Total time: 0:02:22 (0.8907 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.6185 (1.5098)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0938 (7.0276)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1302 (0.1302)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3422 (2.3422 -- 2.3422)  data: 2.0801 (2.0801 -- 2.0801)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3718 (0.5508)  acc1: 77.7778 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4211 (0.1937 -- 2.3422)  data: 0.1981 (0.0008 -- 2.0801)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4057 (0.5340)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (98.4127)  time: 0.2236 (0.1688 -- 0.4594)  data: 0.0187 (0.0001 -- 0.2715)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4728 (0.5758)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.9253)  time: 0.2070 (0.1327 -- 0.4594)  data: 0.0183 (0.0001 -- 0.2715)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 84.025 Acc@5 97.925 loss 0.586
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 87.76%
Epoch: [104]  [  0/160]  eta: 0:18:00  lr: 0.000023  min_lr: 0.000006  loss: 1.8352 (1.8352)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4201 (11.4201)  time: 6.7557 (6.7557 -- 6.7557)  data: 6.1150 (6.1150 -- 6.1150)  max mem: 16413
Epoch: [104]  [ 20/160]  eta: 0:02:44  lr: 0.000023  min_lr: 0.000006  loss: 1.3863 (1.4749)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7780 (7.3709)  time: 0.8967 (0.5268 -- 2.8935)  data: 0.1184 (0.0003 -- 2.0400)  max mem: 16413
Epoch: [104]  [ 40/160]  eta: 0:02:08  lr: 0.000023  min_lr: 0.000006  loss: 1.4737 (1.4726)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2957 (7.4561)  time: 0.9573 (0.5171 -- 3.3292)  data: 0.0019 (0.0004 -- 0.0138)  max mem: 16413
Epoch: [104]  [ 60/160]  eta: 0:01:43  lr: 0.000023  min_lr: 0.000006  loss: 1.5787 (1.5075)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8498 (7.3565)  time: 0.9694 (0.5156 -- 5.2445)  data: 0.0017 (0.0006 -- 0.0037)  max mem: 16413
[2023-09-04 19:28:17,488] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:28:17,489] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:28:17,491] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:28:17,492] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [104]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000006  loss: 1.4850 (1.4939)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5726 (7.0198)  time: 0.7487 (0.5300 -- 3.4276)  data: 0.0015 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000006  loss: 1.4851 (1.4893)  loss_scale: 32768.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7773 (7.0244)  time: 0.8819 (0.5200 -- 4.3651)  data: 0.0024 (0.0005 -- 0.0157)  max mem: 16413
Epoch: [104]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000006  loss: 1.7918 (1.5249)  loss_scale: 32768.0000 (22071.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5473 (7.0140)  time: 0.8444 (0.5342 -- 3.6349)  data: 0.1638 (0.0006 -- 3.0998)  max mem: 16413
[2023-09-04 19:29:05,331] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16777
[2023-09-04 19:29:05,332] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16777
[2023-09-04 19:29:05,332] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:29:05,332] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:29:05,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [104]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.4513 (1.5302)  loss_scale: 32768.0000 (23123.5177)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0608 (6.9419)  time: 0.7741 (0.5338 -- 2.0476)  data: 0.1366 (0.0006 -- 1.5129)  max mem: 16413
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.3760 (1.5189)  loss_scale: 16384.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7011 (7.0176)  time: 0.7895 (0.4933 -- 1.8560)  data: 0.0453 (0.0002 -- 0.8549)  max mem: 16413
Epoch: [104] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.3760 (1.5048)  loss_scale: 16384.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7011 (7.0176)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1465 (0.1465)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2724 (2.2724 -- 2.2724)  data: 2.0644 (2.0644 -- 2.0644)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4678 (0.5537)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4143 (0.2129 -- 2.2724)  data: 0.1890 (0.0009 -- 2.0644)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4678 (0.5191)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2186 (0.1690 -- 0.2984)  data: 0.0054 (0.0001 -- 0.0628)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4871 (0.5638)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.1996 (0.1322 -- 0.2984)  data: 0.0049 (0.0001 -- 0.0628)  max mem: 16413
Val: Total time: 0:00:07 (0.2828 s / it)
* Acc@1 85.892 Acc@5 98.340 loss 0.576
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.76%
Epoch: [105]  [  0/160]  eta: 0:24:18  lr: 0.000023  min_lr: 0.000006  loss: 1.2026 (1.2026)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8270 (7.8270)  time: 9.1129 (9.1129 -- 9.1129)  data: 8.5689 (8.5689 -- 8.5689)  max mem: 16413
Epoch: [105]  [ 20/160]  eta: 0:02:51  lr: 0.000022  min_lr: 0.000006  loss: 1.3622 (1.3311)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0649 (6.7044)  time: 0.8319 (0.5187 -- 3.5922)  data: 0.2822 (0.0007 -- 3.0505)  max mem: 16413
Epoch: [105]  [ 40/160]  eta: 0:02:04  lr: 0.000022  min_lr: 0.000006  loss: 1.6097 (1.4501)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5239 (6.8264)  time: 0.8335 (0.5302 -- 2.6458)  data: 0.1691 (0.0003 -- 1.3920)  max mem: 16413
[2023-09-04 19:30:23,774] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16855
[2023-09-04 19:30:23,774] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16855
[2023-09-04 19:30:23,774] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 19:30:23,774] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 19:30:23,774] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [105]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000006  loss: 1.5810 (1.4769)  loss_scale: 16384.0000 (15578.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5076 (7.1212)  time: 0.8586 (0.5303 -- 2.4384)  data: 0.1676 (0.0005 -- 1.3315)  max mem: 16413
Epoch: [105]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000006  loss: 1.6889 (1.5067)  loss_scale: 8192.0000 (13754.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3455 (7.3057)  time: 0.7776 (0.5421 -- 1.9032)  data: 0.1830 (0.0004 -- 1.3869)  max mem: 16413
Epoch: [105]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000006  loss: 1.4074 (1.4848)  loss_scale: 8192.0000 (12652.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5193 (7.5282)  time: 0.9734 (0.5323 -- 2.3272)  data: 0.1656 (0.0009 -- 1.0115)  max mem: 16413
Epoch: [105]  [120/160]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000006  loss: 1.4409 (1.4875)  loss_scale: 8192.0000 (11915.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8610 (7.5955)  time: 0.8730 (0.5190 -- 3.1857)  data: 0.1930 (0.0005 -- 1.5010)  max mem: 16413
Epoch: [105]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000006  loss: 1.3759 (1.4892)  loss_scale: 8192.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7950 (7.5600)  time: 0.8601 (0.5316 -- 2.8821)  data: 0.2621 (0.0004 -- 2.3409)  max mem: 16413
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.5721 (1.4957)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8694 (7.4535)  time: 0.6556 (0.4957 -- 2.4520)  data: 0.0356 (0.0002 -- 0.6953)  max mem: 16413
Epoch: [105] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.5721 (1.4998)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8694 (7.4535)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1448 (0.1448)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2790 (2.2790 -- 2.2790)  data: 2.0351 (2.0351 -- 2.0351)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3904 (0.5882)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4084 (0.1983 -- 2.2790)  data: 0.1955 (0.0007 -- 2.0351)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4262 (0.5474)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2257 (0.1709 -- 0.4334)  data: 0.0218 (0.0001 -- 0.2422)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4555 (0.5846)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2111 (0.1329 -- 0.4334)  data: 0.0215 (0.0001 -- 0.2422)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 87.76%
Epoch: [106]  [  0/160]  eta: 0:16:44  lr: 0.000022  min_lr: 0.000006  loss: 1.0468 (1.0468)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6046 (8.6046)  time: 6.2789 (6.2789 -- 6.2789)  data: 4.7857 (4.7857 -- 4.7857)  max mem: 16413
Epoch: [106]  [ 20/160]  eta: 0:02:42  lr: 0.000022  min_lr: 0.000006  loss: 1.4469 (1.4882)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6825 (7.8795)  time: 0.9059 (0.5331 -- 3.1070)  data: 0.0177 (0.0005 -- 0.3123)  max mem: 16413
[2023-09-04 19:32:26,265] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:32:26,265] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 19:32:26,269] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:32:26,270] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 19:32:39,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=92, lr=[5.604708639631972e-06, 5.604708639631972e-06, 6.227454044035524e-06, 6.227454044035524e-06, 6.919393382261692e-06, 6.919393382261692e-06, 7.688214869179659e-06, 7.688214869179659e-06, 8.542460965755176e-06, 8.542460965755176e-06, 9.49162329528353e-06, 9.49162329528353e-06, 1.0546248105870587e-05, 1.0546248105870587e-05, 1.1718053450967318e-05, 1.1718053450967318e-05, 1.3020059389963686e-05, 1.3020059389963686e-05, 1.4466732655515206e-05, 1.4466732655515206e-05, 1.6074147395016896e-05, 1.6074147395016896e-05, 1.7860163772240996e-05, 1.7860163772240996e-05, 1.9844626413601107e-05, 1.9844626413601107e-05, 2.2049584904001228e-05, 2.2049584904001228e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 19:32:39,676] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=18.633077975526717, CurrSamplesPerSec=22.52802841670251, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:01:58  lr: 0.000022  min_lr: 0.000006  loss: 1.6430 (1.5541)  loss_scale: 16384.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4401 (7.3712)  time: 0.8092 (0.5213 -- 4.8137)  data: 0.0166 (0.0003 -- 0.2899)  max mem: 16413
Epoch: [106]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000006  loss: 1.5233 (1.5373)  loss_scale: 16384.0000 (13160.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1803 (7.5098)  time: 0.9514 (0.5235 -- 3.5465)  data: 0.1512 (0.0005 -- 1.7593)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000006  loss: 1.2996 (1.4994)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1268 (7.4615)  time: 0.8172 (0.5409 -- 3.2900)  data: 0.1415 (0.0004 -- 2.2420)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000006  loss: 1.4994 (1.4835)  loss_scale: 16384.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3599 (7.3207)  time: 0.9203 (0.5182 -- 2.6136)  data: 0.2309 (0.0004 -- 1.7503)  max mem: 16413
Epoch: [106]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000006  loss: 1.4340 (1.4778)  loss_scale: 16384.0000 (14759.1405)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6170 (7.2438)  time: 0.8741 (0.5300 -- 3.8595)  data: 0.3251 (0.0007 -- 3.3292)  max mem: 16413
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000006  loss: 1.6327 (1.4994)  loss_scale: 16384.0000 (14989.6170)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1910 (7.2348)  time: 1.0270 (0.5183 -- 4.7189)  data: 0.4447 (0.0001 -- 4.1982)  max mem: 16413
[2023-09-04 19:34:19,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:34:19,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:34:19,481] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:34:19,481] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.5757 (1.5060)  loss_scale: 16384.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4887 (7.1873)  time: 0.6056 (0.4950 -- 1.5961)  data: 0.0893 (0.0002 -- 1.0823)  max mem: 16413
Epoch: [106] Total time: 0:02:24 (0.9000 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.5757 (1.5198)  loss_scale: 16384.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4887 (7.1873)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1532 (0.1532)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3276 (2.3276 -- 2.3276)  data: 2.1028 (2.1028 -- 2.1028)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3696 (0.5378)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4254 (0.1997 -- 2.3276)  data: 0.1977 (0.0010 -- 2.1028)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4558 (0.5205)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2219 (0.1692 -- 0.3679)  data: 0.0131 (0.0001 -- 0.1866)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4735 (0.5664)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.5104)  time: 0.2031 (0.1324 -- 0.3679)  data: 0.0123 (0.0001 -- 0.1866)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 85.062 Acc@5 97.303 loss 0.590
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 87.76%
Epoch: [107]  [  0/160]  eta: 0:19:53  lr: 0.000022  min_lr: 0.000006  loss: 1.3802 (1.3802)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1832 (5.1832)  time: 7.4596 (7.4596 -- 7.4596)  data: 6.9395 (6.9395 -- 6.9395)  max mem: 16413
Epoch: [107]  [ 20/160]  eta: 0:02:45  lr: 0.000022  min_lr: 0.000006  loss: 1.4476 (1.5307)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3138 (6.6398)  time: 0.8685 (0.5286 -- 4.9583)  data: 0.2979 (0.0003 -- 4.4423)  max mem: 16413
Epoch: [107]  [ 40/160]  eta: 0:02:08  lr: 0.000022  min_lr: 0.000006  loss: 1.5120 (1.5557)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5130 (6.8106)  time: 0.9602 (0.5141 -- 4.6862)  data: 0.4140 (0.0003 -- 4.1422)  max mem: 16413
[2023-09-04 19:35:31,293] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17180
[2023-09-04 19:35:31,293] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:35:31,293] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17180
[2023-09-04 19:35:31,293] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:35:31,293] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [107]  [ 60/160]  eta: 0:01:38  lr: 0.000022  min_lr: 0.000005  loss: 1.5219 (1.5603)  loss_scale: 32768.0000 (32499.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8201 (7.0808)  time: 0.7914 (0.5205 -- 2.3683)  data: 0.1739 (0.0005 -- 1.8502)  max mem: 16413
Epoch: [107]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000005  loss: 1.4048 (1.5274)  loss_scale: 16384.0000 (28520.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5082 (7.2489)  time: 0.8033 (0.5275 -- 2.1921)  data: 0.1936 (0.0003 -- 1.6565)  max mem: 16413
Epoch: [107]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000005  loss: 1.4949 (1.5517)  loss_scale: 16384.0000 (26117.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9586 (7.3100)  time: 0.8499 (0.5382 -- 2.0022)  data: 0.1496 (0.0001 -- 1.4635)  max mem: 16413
Epoch: [107]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000005  loss: 1.4667 (1.5422)  loss_scale: 16384.0000 (24508.2975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3968 (7.2332)  time: 0.9653 (0.5358 -- 2.7138)  data: 0.1891 (0.0004 -- 2.1552)  max mem: 16413
Epoch: [107]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.6116 (1.5507)  loss_scale: 16384.0000 (23355.9149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4983 (7.2709)  time: 0.8361 (0.5290 -- 3.5140)  data: 0.0398 (0.0003 -- 0.7720)  max mem: 16413
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.5143 (1.5368)  loss_scale: 16384.0000 (22528.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7511 (7.2082)  time: 0.6555 (0.4980 -- 1.7828)  data: 0.0142 (0.0003 -- 0.2680)  max mem: 16413
Epoch: [107] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.5143 (1.5125)  loss_scale: 16384.0000 (22528.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7511 (7.2082)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1372 (0.1372)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4474 (2.4474 -- 2.4474)  data: 2.2343 (2.2343 -- 2.2343)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2791 (0.4930)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4207 (0.2070 -- 2.4474)  data: 0.2049 (0.0009 -- 2.2343)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4972 (0.5015)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.9418)  time: 0.2188 (0.1698 -- 0.4763)  data: 0.0153 (0.0001 -- 0.2847)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5097 (0.5440)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.7552)  time: 0.2024 (0.1328 -- 0.4763)  data: 0.0146 (0.0001 -- 0.2847)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 85.892 Acc@5 98.340 loss 0.577
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.76%
Epoch: [108]  [  0/160]  eta: 0:19:16  lr: 0.000021  min_lr: 0.000005  loss: 1.6552 (1.6552)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6049 (6.6049)  time: 7.2310 (7.2310 -- 7.2310)  data: 5.2520 (5.2520 -- 5.2520)  max mem: 16413
Epoch: [108]  [ 20/160]  eta: 0:02:48  lr: 0.000021  min_lr: 0.000005  loss: 1.5864 (1.5257)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9215 (7.5653)  time: 0.9008 (0.5226 -- 3.8357)  data: 0.2505 (0.0006 -- 3.2992)  max mem: 16413
[2023-09-04 19:37:35,432] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:37:35,432] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:37:35,432] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:37:35,432] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [108]  [ 40/160]  eta: 0:02:03  lr: 0.000021  min_lr: 0.000005  loss: 1.5683 (1.5596)  loss_scale: 32768.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3849 (6.8263)  time: 0.8516 (0.5249 -- 4.0797)  data: 0.1798 (0.0002 -- 3.5664)  max mem: 16413
Epoch: [108]  [ 60/160]  eta: 0:01:40  lr: 0.000021  min_lr: 0.000005  loss: 1.5316 (1.5459)  loss_scale: 32768.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7856 (6.9109)  time: 0.9374 (0.5249 -- 3.9949)  data: 0.0471 (0.0003 -- 0.7105)  max mem: 16413
Epoch: [108]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000005  loss: 1.5012 (1.5421)  loss_scale: 32768.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5699 (7.0797)  time: 0.8164 (0.5275 -- 3.1556)  data: 0.0028 (0.0002 -- 0.0262)  max mem: 16413
Epoch: [108]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000005  loss: 1.4080 (1.5258)  loss_scale: 32768.0000 (28063.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5123 (7.0100)  time: 0.8958 (0.5422 -- 3.6601)  data: 0.0018 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000005  loss: 1.4972 (1.5274)  loss_scale: 32768.0000 (28841.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1391 (7.1873)  time: 0.8278 (0.5287 -- 3.5266)  data: 0.0710 (0.0004 -- 0.9767)  max mem: 16413
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.5993 (1.5482)  loss_scale: 32768.0000 (29398.2411)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0569 (7.3257)  time: 0.8022 (0.5334 -- 2.0549)  data: 0.1295 (0.0004 -- 1.5034)  max mem: 16413
[2023-09-04 19:39:20,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:39:20,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 19:39:20,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:39:20,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.5014 (1.5431)  loss_scale: 32768.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2892 (7.2762)  time: 0.6970 (0.4975 -- 2.4071)  data: 0.0994 (0.0002 -- 1.8458)  max mem: 16413
Epoch: [108] Total time: 0:02:21 (0.8828 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.5014 (1.5176)  loss_scale: 32768.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2892 (7.2762)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1400 (0.1400)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3936 (2.3936 -- 2.3936)  data: 2.1906 (2.1906 -- 2.1906)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.1933 (0.4533)  acc1: 100.0000 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4135 (0.1947 -- 2.3936)  data: 0.2001 (0.0008 -- 2.1906)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3875 (0.4728)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (99.4709)  time: 0.2143 (0.1713 -- 0.3539)  data: 0.0093 (0.0001 -- 0.1679)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4791 (0.5081)  acc1: 88.8889 (87.9668)  acc5: 100.0000 (99.1701)  time: 0.2008 (0.1335 -- 0.3539)  data: 0.0091 (0.0001 -- 0.1679)  max mem: 16413
Val: Total time: 0:00:07 (0.2844 s / it)
* Acc@1 86.929 Acc@5 98.755 loss 0.554
Accuracy of the network on the 482 val images: 86.93%
Max accuracy: 87.76%
Epoch: [109]  [  0/160]  eta: 0:16:55  lr: 0.000021  min_lr: 0.000005  loss: 1.7633 (1.7633)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8084 (8.8084)  time: 6.3482 (6.3482 -- 6.3482)  data: 5.8275 (5.8275 -- 5.8275)  max mem: 16413
[2023-09-04 19:39:38,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17442
[2023-09-04 19:39:38,498] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17442
[2023-09-04 19:39:38,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 19:39:38,499] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 19:39:38,499] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 19:39:40,756] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17446
[2023-09-04 19:39:40,756] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17446
[2023-09-04 19:39:40,756] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:39:40,756] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:39:40,756] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [109]  [ 20/160]  eta: 0:02:37  lr: 0.000021  min_lr: 0.000005  loss: 1.4366 (1.4582)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3786 (7.8119)  time: 0.8661 (0.5246 -- 2.7366)  data: 0.0973 (0.0008 -- 0.7429)  max mem: 16413
Epoch: [109]  [ 40/160]  eta: 0:01:58  lr: 0.000021  min_lr: 0.000005  loss: 1.7203 (1.5337)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1369 (7.7552)  time: 0.8426 (0.5322 -- 2.3449)  data: 0.0702 (0.0004 -- 0.7688)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:38  lr: 0.000021  min_lr: 0.000005  loss: 1.6414 (1.5477)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6827 (7.7256)  time: 0.9860 (0.5260 -- 3.0814)  data: 0.0652 (0.0005 -- 0.5652)  max mem: 16413
Epoch: [109]  [ 80/160]  eta: 0:01:14  lr: 0.000021  min_lr: 0.000005  loss: 1.7058 (1.5689)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2035 (7.7368)  time: 0.7795 (0.5227 -- 3.8534)  data: 0.0023 (0.0004 -- 0.0119)  max mem: 16413
Epoch: [109]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000005  loss: 1.6847 (1.5639)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2644 (7.5078)  time: 0.8935 (0.5291 -- 3.3636)  data: 0.0436 (0.0003 -- 0.5651)  max mem: 16413
Epoch: [109]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000005  loss: 1.4984 (1.5557)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2898 (7.4261)  time: 0.9344 (0.5379 -- 2.5387)  data: 0.2262 (0.0002 -- 2.0159)  max mem: 16413
[2023-09-04 19:41:36,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:41:36,077] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:41:36,081] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:41:36,081] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.5778 (1.5604)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5800 (7.4570)  time: 0.8368 (0.5325 -- 2.7971)  data: 0.2174 (0.0003 -- 2.2844)  max mem: 16413
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.4912 (1.5597)  loss_scale: 32768.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1188 (7.3552)  time: 0.7027 (0.4972 -- 2.6838)  data: 0.1098 (0.0003 -- 2.1802)  max mem: 16413
Epoch: [109] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.4912 (1.5209)  loss_scale: 32768.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1188 (7.3552)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1257 (0.1257)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4888 (2.4888 -- 2.4888)  data: 2.2710 (2.2710 -- 2.2710)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3484 (0.5191)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4295 (0.2041 -- 2.4888)  data: 0.2080 (0.0004 -- 2.2710)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4311 (0.5112)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (99.4709)  time: 0.2140 (0.1686 -- 0.2843)  data: 0.0056 (0.0001 -- 0.0924)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4311 (0.5354)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (99.1701)  time: 0.1944 (0.1337 -- 0.2843)  data: 0.0050 (0.0001 -- 0.0924)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 85.477 Acc@5 98.548 loss 0.581
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.76%
Epoch: [110]  [  0/160]  eta: 0:21:32  lr: 0.000021  min_lr: 0.000005  loss: 1.7966 (1.7966)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6920 (7.6920)  time: 8.0782 (8.0782 -- 8.0782)  data: 3.3611 (3.3611 -- 3.3611)  max mem: 16413
Epoch: [110]  [ 20/160]  eta: 0:02:45  lr: 0.000021  min_lr: 0.000005  loss: 1.5628 (1.5747)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9127 (7.4627)  time: 0.8353 (0.5270 -- 3.1536)  data: 0.0368 (0.0002 -- 0.7083)  max mem: 16413
[2023-09-04 19:42:39,445] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17635
[2023-09-04 19:42:39,445] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17635
[2023-09-04 19:42:39,445] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:42:39,445] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:42:39,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [110]  [ 40/160]  eta: 0:02:05  lr: 0.000021  min_lr: 0.000005  loss: 1.4969 (1.5713)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8025 (7.4796)  time: 0.9007 (0.5280 -- 3.1986)  data: 0.0020 (0.0005 -- 0.0043)  max mem: 16413
Epoch: [110]  [ 60/160]  eta: 0:01:40  lr: 0.000020  min_lr: 0.000005  loss: 1.4656 (1.5769)  loss_scale: 16384.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4770 (7.3703)  time: 0.9115 (0.5125 -- 2.8617)  data: 0.2036 (0.0003 -- 2.3274)  max mem: 16413
Epoch: [110]  [ 80/160]  eta: 0:01:16  lr: 0.000020  min_lr: 0.000005  loss: 1.6128 (1.5736)  loss_scale: 16384.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5699 (7.2597)  time: 0.8202 (0.5344 -- 3.5519)  data: 0.2384 (0.0005 -- 3.0000)  max mem: 16413
Epoch: [110]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000005  loss: 1.3199 (1.5394)  loss_scale: 16384.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6872 (7.2032)  time: 0.8296 (0.5175 -- 3.1938)  data: 0.2863 (0.0003 -- 2.6605)  max mem: 16413
Epoch: [110]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000005  loss: 1.4655 (1.5284)  loss_scale: 16384.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7717 (7.1728)  time: 0.9237 (0.5231 -- 3.3965)  data: 0.2072 (0.0005 -- 2.8803)  max mem: 16413
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.4751 (1.5189)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6969 (7.1440)  time: 0.9119 (0.5332 -- 2.7350)  data: 0.2306 (0.0004 -- 2.1946)  max mem: 16413
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.6843 (1.5369)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2422 (7.1715)  time: 0.7103 (0.4949 -- 3.8172)  data: 0.1878 (0.0001 -- 3.3063)  max mem: 16413
Epoch: [110] Total time: 0:02:24 (0.9027 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.6843 (1.5108)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2422 (7.1715)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1301 (0.1301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3716 (2.3716 -- 2.3716)  data: 2.1534 (2.1534 -- 2.1534)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3215 (0.5108)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4226 (0.2035 -- 2.3716)  data: 0.2092 (0.0010 -- 2.1534)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4442 (0.5200)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (99.4709)  time: 0.2169 (0.1693 -- 0.3220)  data: 0.0104 (0.0001 -- 0.1039)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4988 (0.5333)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2009 (0.1329 -- 0.3220)  data: 0.0084 (0.0001 -- 0.1039)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 85.685 Acc@5 98.340 loss 0.576
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.76%
Epoch: [111]  [  0/160]  eta: 0:20:54  lr: 0.000020  min_lr: 0.000005  loss: 1.5248 (1.5248)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9469 (6.9469)  time: 7.8381 (7.8381 -- 7.8381)  data: 7.3125 (7.3125 -- 7.3125)  max mem: 16413
[2023-09-04 19:44:42,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:44:42,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:44:42,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:44:42,349] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [111]  [ 20/160]  eta: 0:02:56  lr: 0.000020  min_lr: 0.000005  loss: 1.4617 (1.4656)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4890 (7.0050)  time: 0.9321 (0.5176 -- 4.9077)  data: 0.3830 (0.0003 -- 4.3820)  max mem: 16413
Epoch: [111]  [ 40/160]  eta: 0:02:07  lr: 0.000020  min_lr: 0.000005  loss: 1.5653 (1.4542)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8681 (7.1436)  time: 0.8467 (0.5192 -- 3.7080)  data: 0.2997 (0.0003 -- 3.1619)  max mem: 16413
Epoch: [111]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000005  loss: 1.5091 (1.4515)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0554 (7.2867)  time: 0.7536 (0.5239 -- 2.6854)  data: 0.2017 (0.0004 -- 2.1350)  max mem: 16413
Epoch: [111]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000005  loss: 1.7082 (1.4833)  loss_scale: 32768.0000 (31958.9136)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6506 (7.1197)  time: 0.8470 (0.5470 -- 1.8847)  data: 0.2159 (0.0007 -- 0.9271)  max mem: 16413
[2023-09-04 19:45:53,293] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17845
[2023-09-04 19:45:53,293] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17845
[2023-09-04 19:45:53,293] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:45:53,293] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:45:53,294] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [111]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000005  loss: 1.5305 (1.4858)  loss_scale: 16384.0000 (29523.6436)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3574 (7.1515)  time: 0.9067 (0.5159 -- 3.4479)  data: 0.3603 (0.0004 -- 2.9194)  max mem: 16413
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000005  loss: 1.4419 (1.4777)  loss_scale: 16384.0000 (27351.8017)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9083 (7.2300)  time: 0.8830 (0.5301 -- 3.4576)  data: 0.3280 (0.0005 -- 2.9133)  max mem: 16413
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.5277 (1.4805)  loss_scale: 16384.0000 (25796.0851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2766 (7.2672)  time: 0.8735 (0.5293 -- 2.4899)  data: 0.1683 (0.0004 -- 1.9212)  max mem: 16413
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.4640 (1.4864)  loss_scale: 16384.0000 (24678.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5205 (7.3723)  time: 0.6977 (0.4966 -- 3.9808)  data: 0.1725 (0.0002 -- 3.4337)  max mem: 16413
Epoch: [111] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.4640 (1.5129)  loss_scale: 16384.0000 (24678.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5205 (7.3723)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1323 (0.1323)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4035 (2.4035 -- 2.4035)  data: 2.1590 (2.1590 -- 2.1590)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3112 (0.5072)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4229 (0.1921 -- 2.4035)  data: 0.2037 (0.0008 -- 2.1590)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4783 (0.5197)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (99.4709)  time: 0.2152 (0.1711 -- 0.3082)  data: 0.0098 (0.0001 -- 0.1109)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5314 (0.5470)  acc1: 88.8889 (87.5519)  acc5: 100.0000 (98.3402)  time: 0.2010 (0.1329 -- 0.3082)  data: 0.0095 (0.0001 -- 0.1109)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 85.892 Acc@5 97.510 loss 0.571
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 87.76%
Epoch: [112]  [  0/160]  eta: 0:21:53  lr: 0.000020  min_lr: 0.000005  loss: 1.5088 (1.5088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7955 (8.7955)  time: 8.2075 (8.2075 -- 8.2075)  data: 7.6796 (7.6796 -- 7.6796)  max mem: 16413
Epoch: [112]  [ 20/160]  eta: 0:02:49  lr: 0.000020  min_lr: 0.000005  loss: 1.4060 (1.4443)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1084 (6.9293)  time: 0.8633 (0.5359 -- 2.9327)  data: 0.0393 (0.0004 -- 0.7577)  max mem: 16413
Epoch: [112]  [ 40/160]  eta: 0:01:58  lr: 0.000020  min_lr: 0.000005  loss: 1.4918 (1.4708)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8636 (6.9693)  time: 0.7508 (0.5294 -- 2.9028)  data: 0.0018 (0.0003 -- 0.0064)  max mem: 16413
[2023-09-04 19:47:55,213] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:47:55,213] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:47:55,215] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:47:55,215] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [112]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000005  loss: 1.7398 (1.5105)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7462 (7.0257)  time: 0.9132 (0.5342 -- 2.8637)  data: 0.0326 (0.0009 -- 0.6151)  max mem: 16413
[2023-09-04 19:48:16,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=97, lr=[5.0093528099075036e-06, 5.0093528099075036e-06, 5.565947566563894e-06, 5.565947566563894e-06, 6.184386185070991e-06, 6.184386185070991e-06, 6.8715402056344355e-06, 6.8715402056344355e-06, 7.63504467292715e-06, 7.63504467292715e-06, 8.483382969919056e-06, 8.483382969919056e-06, 9.42598107768784e-06, 9.42598107768784e-06, 1.0473312308542044e-05, 1.0473312308542044e-05, 1.1637013676157826e-05, 1.1637013676157826e-05, 1.2930015195730916e-05, 1.2930015195730916e-05, 1.436668355081213e-05, 1.436668355081213e-05, 1.5962981723124588e-05, 1.5962981723124588e-05, 1.773664635902732e-05, 1.773664635902732e-05, 1.970738484336369e-05, 1.970738484336369e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 19:48:16,887] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=18.57509826719646, CurrSamplesPerSec=23.24837906411825, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [112]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000005  loss: 1.5077 (1.5068)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8491 (6.9641)  time: 0.8299 (0.5306 -- 3.4741)  data: 0.0062 (0.0005 -- 0.0939)  max mem: 16413
Epoch: [112]  [100/160]  eta: 0:00:54  lr: 0.000020  min_lr: 0.000005  loss: 1.6221 (1.5199)  loss_scale: 32768.0000 (24008.2376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5191 (7.0505)  time: 0.8570 (0.5302 -- 2.3747)  data: 0.0087 (0.0004 -- 0.1451)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000005  loss: 1.3146 (1.4948)  loss_scale: 32768.0000 (25456.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9702 (7.1256)  time: 0.9637 (0.5305 -- 3.0059)  data: 0.1950 (0.0004 -- 2.4896)  max mem: 16413
Epoch: [112]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.3724 (1.4820)  loss_scale: 32768.0000 (26493.2766)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2894 (7.1937)  time: 0.8150 (0.5295 -- 3.4930)  data: 0.2630 (0.0004 -- 2.9364)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.6134 (1.4956)  loss_scale: 32768.0000 (27238.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1903 (7.3056)  time: 0.6503 (0.4966 -- 2.5728)  data: 0.1242 (0.0002 -- 2.0330)  max mem: 16413
Epoch: [112] Total time: 0:02:20 (0.8785 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.6134 (1.4914)  loss_scale: 32768.0000 (27238.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1903 (7.3056)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1353 (0.1353)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2478 (2.2478 -- 2.2478)  data: 2.0385 (2.0385 -- 2.0385)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4000 (0.4938)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4186 (0.2004 -- 2.2478)  data: 0.1863 (0.0004 -- 2.0385)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4389 (0.5142)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (99.4709)  time: 0.2248 (0.1723 -- 0.3144)  data: 0.0064 (0.0001 -- 0.1138)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5346 (0.5547)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (99.1701)  time: 0.2036 (0.1330 -- 0.3144)  data: 0.0061 (0.0001 -- 0.1138)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 85.685 Acc@5 98.548 loss 0.577
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.76%
Epoch: [113]  [  0/160]  eta: 0:18:52  lr: 0.000020  min_lr: 0.000005  loss: 0.6983 (0.6983)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2322 (7.2322)  time: 7.0811 (7.0811 -- 7.0811)  data: 6.5658 (6.5658 -- 6.5658)  max mem: 16413
Epoch: [113]  [ 20/160]  eta: 0:02:40  lr: 0.000019  min_lr: 0.000005  loss: 1.6781 (1.5955)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9974 (7.6251)  time: 0.8497 (0.5274 -- 3.7026)  data: 0.2988 (0.0004 -- 3.1712)  max mem: 16413
[2023-09-04 19:49:55,602] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:49:55,602] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 19:49:55,604] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:49:55,604] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 19:50:00,629] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18106
[2023-09-04 19:50:00,629] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18106
[2023-09-04 19:50:00,629] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 19:50:00,629] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 19:50:00,630] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [113]  [ 40/160]  eta: 0:02:03  lr: 0.000019  min_lr: 0.000005  loss: 1.4665 (1.5462)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2199 (6.8933)  time: 0.9052 (0.5133 -- 3.6430)  data: 0.3535 (0.0004 -- 3.0661)  max mem: 16413
Epoch: [113]  [ 60/160]  eta: 0:01:34  lr: 0.000019  min_lr: 0.000005  loss: 1.4544 (1.5419)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5204 (7.1595)  time: 0.7851 (0.5334 -- 3.5186)  data: 0.1873 (0.0005 -- 2.9778)  max mem: 16413
[2023-09-04 19:50:36,701] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18151
[2023-09-04 19:50:36,701] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18151
[2023-09-04 19:50:36,702] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:50:36,702] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:50:36,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [113]  [ 80/160]  eta: 0:01:13  lr: 0.000019  min_lr: 0.000005  loss: 1.6490 (1.5559)  loss_scale: 16384.0000 (32363.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3563 (7.4548)  time: 0.8258 (0.5191 -- 2.3168)  data: 0.1135 (0.0006 -- 1.2040)  max mem: 16413
Epoch: [113]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000005  loss: 1.5172 (1.5548)  loss_scale: 16384.0000 (29199.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8883 (7.8088)  time: 0.9371 (0.5202 -- 3.1332)  data: 0.0766 (0.0002 -- 0.9526)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000005  loss: 1.3031 (1.5285)  loss_scale: 16384.0000 (27080.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2795 (7.8215)  time: 0.8966 (0.5281 -- 2.8460)  data: 0.0115 (0.0002 -- 0.1924)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.5617 (1.5386)  loss_scale: 16384.0000 (25563.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9873 (7.8826)  time: 0.8700 (0.5267 -- 2.1060)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.7654 (1.5565)  loss_scale: 16384.0000 (24473.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3832 (7.8200)  time: 0.7372 (0.4943 -- 2.9301)  data: 0.0078 (0.0002 -- 0.1369)  max mem: 16413
Epoch: [113] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.7654 (1.5192)  loss_scale: 16384.0000 (24473.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3832 (7.8200)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1469 (0.1469)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3606 (2.3606 -- 2.3606)  data: 2.1338 (2.1338 -- 2.1338)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3961 (0.5094)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4224 (0.2101 -- 2.3606)  data: 0.2039 (0.0007 -- 2.1338)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4108 (0.5075)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (99.4709)  time: 0.2198 (0.1709 -- 0.3170)  data: 0.0129 (0.0001 -- 0.1451)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4577 (0.5481)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2033 (0.1329 -- 0.3170)  data: 0.0124 (0.0001 -- 0.1451)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 86.100 Acc@5 98.340 loss 0.587
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.76%
Epoch: [114]  [  0/160]  eta: 0:17:40  lr: 0.000019  min_lr: 0.000005  loss: 1.5704 (1.5704)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2341 (9.2341)  time: 6.6252 (6.6252 -- 6.6252)  data: 4.2883 (4.2883 -- 4.2883)  max mem: 16413
Epoch: [114]  [ 20/160]  eta: 0:02:42  lr: 0.000019  min_lr: 0.000005  loss: 1.4468 (1.5180)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0313 (7.4864)  time: 0.8838 (0.5166 -- 3.2582)  data: 0.3222 (0.0006 -- 2.7306)  max mem: 16413
[2023-09-04 19:52:41,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:52:41,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:52:41,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:52:41,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [114]  [ 40/160]  eta: 0:01:58  lr: 0.000019  min_lr: 0.000005  loss: 1.5120 (1.5515)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6196 (7.0388)  time: 0.8087 (0.5418 -- 2.8191)  data: 0.2281 (0.0005 -- 2.2623)  max mem: 16413
[2023-09-04 19:52:46,590] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18284
[2023-09-04 19:52:46,590] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18284
[2023-09-04 19:52:46,591] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:52:46,591] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:52:46,591] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [114]  [ 60/160]  eta: 0:01:37  lr: 0.000019  min_lr: 0.000005  loss: 1.4875 (1.5013)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8305 (7.0224)  time: 0.9611 (0.5175 -- 3.3143)  data: 0.3929 (0.0004 -- 2.7888)  max mem: 16413
Epoch: [114]  [ 80/160]  eta: 0:01:14  lr: 0.000019  min_lr: 0.000005  loss: 1.5122 (1.4958)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4173 (7.2525)  time: 0.7662 (0.5385 -- 3.1327)  data: 0.2104 (0.0002 -- 2.5762)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000005  loss: 1.4552 (1.4950)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6207 (7.3852)  time: 0.9238 (0.5351 -- 2.8338)  data: 0.3043 (0.0008 -- 2.3000)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000005  loss: 1.4240 (1.4932)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0130 (7.3290)  time: 0.8145 (0.5400 -- 2.5032)  data: 0.2586 (0.0004 -- 1.9826)  max mem: 16413
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.5821 (1.5097)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8821 (7.2553)  time: 0.9543 (0.5305 -- 2.9295)  data: 0.4076 (0.0004 -- 2.3836)  max mem: 16413
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.6063 (1.5156)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1396 (7.2931)  time: 0.6654 (0.4944 -- 2.0335)  data: 0.1402 (0.0002 -- 1.4997)  max mem: 16413
Epoch: [114] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.6063 (1.4970)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1396 (7.2931)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1381 (0.1381)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2812 (2.2812 -- 2.2812)  data: 2.0396 (2.0396 -- 2.0396)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3650 (0.5035)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4427 (0.1951 -- 2.2812)  data: 0.2270 (0.0006 -- 2.0396)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3868 (0.4944)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.9418)  time: 0.2272 (0.1708 -- 0.6472)  data: 0.0230 (0.0001 -- 0.4445)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4082 (0.5497)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.7552)  time: 0.2122 (0.1326 -- 0.6472)  data: 0.0228 (0.0001 -- 0.4445)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 85.685 Acc@5 98.340 loss 0.585
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 87.76%
Epoch: [115]  [  0/160]  eta: 0:22:19  lr: 0.000019  min_lr: 0.000005  loss: 1.2867 (1.2867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5027 (5.5027)  time: 8.3714 (8.3714 -- 8.3714)  data: 5.9467 (5.9467 -- 5.9467)  max mem: 16413
[2023-09-04 19:54:48,952] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:54:48,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:54:48,953] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:54:48,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:54:53,553] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18417
[2023-09-04 19:54:53,553] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18417
[2023-09-04 19:54:53,554] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:54:53,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 19:54:53,554] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [115]  [ 20/160]  eta: 0:02:45  lr: 0.000019  min_lr: 0.000005  loss: 1.4354 (1.4506)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8950 (7.1541)  time: 0.8256 (0.5213 -- 2.3826)  data: 0.0930 (0.0003 -- 1.2961)  max mem: 16413
Epoch: [115]  [ 40/160]  eta: 0:02:05  lr: 0.000019  min_lr: 0.000005  loss: 1.3267 (1.4236)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6967 (7.1061)  time: 0.8971 (0.5335 -- 3.7600)  data: 0.1036 (0.0001 -- 2.0410)  max mem: 16413
Epoch: [115]  [ 60/160]  eta: 0:01:36  lr: 0.000019  min_lr: 0.000005  loss: 1.4060 (1.4317)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0439 (7.3229)  time: 0.8014 (0.5293 -- 2.7207)  data: 0.0017 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:14  lr: 0.000019  min_lr: 0.000005  loss: 1.5413 (1.4680)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2606 (7.4179)  time: 0.8454 (0.5266 -- 3.8500)  data: 0.0058 (0.0002 -- 0.0831)  max mem: 16413
Epoch: [115]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000005  loss: 1.5430 (1.4787)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5025 (7.4505)  time: 0.8450 (0.5326 -- 3.1379)  data: 0.1098 (0.0003 -- 1.0393)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000005  loss: 1.4649 (1.4914)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8784 (7.4254)  time: 0.8607 (0.5303 -- 2.8466)  data: 0.0422 (0.0002 -- 0.3488)  max mem: 16413
Epoch: [115]  [140/160]  eta: 0:00:17  lr: 0.000018  min_lr: 0.000005  loss: 1.3979 (1.4882)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4411 (7.5821)  time: 0.8467 (0.5409 -- 2.0134)  data: 0.1360 (0.0009 -- 1.1076)  max mem: 16413
[2023-09-04 19:56:43,713] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:56:43,713] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 19:56:43,714] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 19:56:43,714] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.5513 (1.4907)  loss_scale: 32768.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8674 (7.5307)  time: 0.7292 (0.4980 -- 3.7830)  data: 0.1640 (0.0002 -- 2.5452)  max mem: 16413
Epoch: [115] Total time: 0:02:20 (0.8803 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.5513 (1.4989)  loss_scale: 32768.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8674 (7.5307)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1464 (0.1464)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3791 (2.3791 -- 2.3791)  data: 2.1338 (2.1338 -- 2.1338)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3846 (0.5078)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4308 (0.1982 -- 2.3791)  data: 0.2075 (0.0004 -- 2.1338)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3999 (0.4941)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.9418)  time: 0.2280 (0.1690 -- 0.4780)  data: 0.0225 (0.0001 -- 0.2995)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4454 (0.5444)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2109 (0.1328 -- 0.4780)  data: 0.0222 (0.0001 -- 0.2995)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 86.100 Acc@5 98.340 loss 0.590
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 87.76%
[2023-09-04 19:57:07,021] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18560
[2023-09-04 19:57:07,021] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18560
[2023-09-04 19:57:07,022] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:57:07,022] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 19:57:07,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [  0/160]  eta: 0:21:08  lr: 0.000018  min_lr: 0.000005  loss: 1.7983 (1.7983)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7794 (8.7794)  time: 7.9264 (7.9264 -- 7.9264)  data: 7.4149 (7.4149 -- 7.4149)  max mem: 16413
Epoch: [116]  [ 20/160]  eta: 0:02:43  lr: 0.000018  min_lr: 0.000005  loss: 1.4724 (1.4924)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3218 (7.4301)  time: 0.8294 (0.5145 -- 3.6063)  data: 0.2828 (0.0002 -- 3.0865)  max mem: 16413
Epoch: [116]  [ 40/160]  eta: 0:02:05  lr: 0.000018  min_lr: 0.000005  loss: 1.5260 (1.5387)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9722 (7.2824)  time: 0.9202 (0.5373 -- 4.4339)  data: 0.3613 (0.0004 -- 3.8753)  max mem: 16413
Epoch: [116]  [ 60/160]  eta: 0:01:40  lr: 0.000018  min_lr: 0.000005  loss: 1.5635 (1.5191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0880 (7.3389)  time: 0.9216 (0.5274 -- 3.3704)  data: 0.3763 (0.0005 -- 2.8389)  max mem: 16413
[2023-09-04 19:58:02,692] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18624
[2023-09-04 19:58:02,692] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18624
[2023-09-04 19:58:02,693] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 19:58:02,693] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-04 19:58:02,693] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [116]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000005  loss: 1.5069 (1.5021)  loss_scale: 8192.0000 (14664.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4308 (7.3532)  time: 0.7983 (0.5067 -- 3.5255)  data: 0.2471 (0.0003 -- 3.0119)  max mem: 16413
Epoch: [116]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000005  loss: 1.5173 (1.5079)  loss_scale: 8192.0000 (13382.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2966 (7.2337)  time: 0.8871 (0.5165 -- 3.1394)  data: 0.3367 (0.0005 -- 2.6008)  max mem: 16413
Epoch: [116]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000005  loss: 1.3107 (1.4800)  loss_scale: 8192.0000 (12524.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3234 (7.1007)  time: 0.8154 (0.5406 -- 2.9439)  data: 0.2465 (0.0004 -- 2.3331)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000005  loss: 1.5677 (1.4888)  loss_scale: 8192.0000 (11910.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7622 (7.0662)  time: 0.9142 (0.5316 -- 3.1445)  data: 0.3612 (0.0003 -- 2.6021)  max mem: 16413
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.4217 (1.4833)  loss_scale: 8192.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1661 (6.9732)  time: 0.8594 (0.4955 -- 3.1931)  data: 0.2053 (0.0001 -- 2.5090)  max mem: 16413
Epoch: [116] Total time: 0:02:23 (0.8989 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.4217 (1.4933)  loss_scale: 8192.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1661 (6.9732)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1504 (0.1504)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2536 (2.2536 -- 2.2536)  data: 2.0502 (2.0502 -- 2.0502)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4499 (0.5331)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4071 (0.1887 -- 2.2536)  data: 0.1968 (0.0005 -- 2.0502)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4499 (0.5051)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2222 (0.1688 -- 0.3715)  data: 0.0189 (0.0001 -- 0.1754)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4633 (0.5595)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2084 (0.1334 -- 0.3715)  data: 0.0181 (0.0001 -- 0.1754)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.586
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 87.76%
Epoch: [117]  [  0/160]  eta: 0:17:11  lr: 0.000018  min_lr: 0.000005  loss: 1.4399 (1.4399)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3360 (6.3360)  time: 6.4478 (6.4478 -- 6.4478)  data: 5.8898 (5.8898 -- 5.8898)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:42  lr: 0.000018  min_lr: 0.000005  loss: 1.5712 (1.4515)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6645 (7.3345)  time: 0.8930 (0.5405 -- 2.1479)  data: 0.1868 (0.0003 -- 1.4598)  max mem: 16413
[2023-09-04 20:00:08,086] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 20:00:08,086] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 20:00:08,086] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 20:00:08,087] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [117]  [ 40/160]  eta: 0:02:01  lr: 0.000018  min_lr: 0.000005  loss: 1.4426 (1.4803)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3678 (7.3494)  time: 0.8534 (0.5311 -- 4.4707)  data: 0.2963 (0.0004 -- 3.9279)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:37  lr: 0.000018  min_lr: 0.000005  loss: 1.7411 (1.5509)  loss_scale: 16384.0000 (11952.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3697 (7.4139)  time: 0.9146 (0.5285 -- 3.1988)  data: 0.3216 (0.0006 -- 2.5014)  max mem: 16413
Epoch: [117]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000005  loss: 1.6025 (1.5589)  loss_scale: 16384.0000 (13046.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3206 (7.4344)  time: 0.8202 (0.5265 -- 3.4334)  data: 0.1426 (0.0004 -- 1.5957)  max mem: 16413
Epoch: [117]  [100/160]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000005  loss: 1.4324 (1.5444)  loss_scale: 16384.0000 (13707.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4160 (7.3386)  time: 0.9950 (0.5213 -- 4.3828)  data: 0.3004 (0.0004 -- 2.6240)  max mem: 16413
Epoch: [117]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000005  loss: 1.4683 (1.5270)  loss_scale: 16384.0000 (14149.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2895 (7.3812)  time: 0.7384 (0.5296 -- 3.0264)  data: 0.1093 (0.0003 -- 1.2019)  max mem: 16413
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000005  loss: 1.4204 (1.5270)  loss_scale: 16384.0000 (14466.7234)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1513 (7.4111)  time: 0.8740 (0.5277 -- 2.7385)  data: 0.3195 (0.0007 -- 2.0538)  max mem: 16413
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000004  loss: 1.6045 (1.5235)  loss_scale: 16384.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4361 (7.3522)  time: 0.7190 (0.4946 -- 3.2175)  data: 0.1758 (0.0002 -- 2.4452)  max mem: 16413
Epoch: [117] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000004  loss: 1.6045 (1.5100)  loss_scale: 16384.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4361 (7.3522)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1408 (0.1408)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4210 (2.4210 -- 2.4210)  data: 2.2008 (2.2008 -- 2.2008)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5585 (0.5575)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4247 (0.2016 -- 2.4210)  data: 0.2128 (0.0008 -- 2.2008)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5585 (0.5501)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.9418)  time: 0.2232 (0.1697 -- 0.5222)  data: 0.0241 (0.0001 -- 0.3397)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5675 (0.5942)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.7552)  time: 0.2097 (0.1330 -- 0.5222)  data: 0.0237 (0.0001 -- 0.3397)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.601
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.76%
Epoch: [118]  [  0/160]  eta: 0:23:52  lr: 0.000018  min_lr: 0.000004  loss: 1.2116 (1.2116)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8310 (7.8310)  time: 8.9526 (8.9526 -- 8.9526)  data: 8.4355 (8.4355 -- 8.4355)  max mem: 16413
[2023-09-04 20:02:10,121] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 20:02:10,121] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 20:02:10,122] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 20:02:10,122] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [118]  [ 20/160]  eta: 0:02:44  lr: 0.000018  min_lr: 0.000004  loss: 1.5178 (1.5674)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2947 (7.4451)  time: 0.7867 (0.5352 -- 2.9968)  data: 0.2328 (0.0004 -- 2.4562)  max mem: 16413
[2023-09-04 20:02:40,688] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18919
[2023-09-04 20:02:40,688] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18919
[2023-09-04 20:02:40,688] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 20:02:40,688] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 20:02:40,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [118]  [ 40/160]  eta: 0:02:02  lr: 0.000018  min_lr: 0.000004  loss: 1.4162 (1.5199)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3779 (7.5685)  time: 0.8504 (0.5122 -- 2.5957)  data: 0.2997 (0.0005 -- 2.0705)  max mem: 16413
Epoch: [118]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000004  loss: 1.4999 (1.5047)  loss_scale: 16384.0000 (26590.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2742 (7.6113)  time: 0.9198 (0.5305 -- 2.4595)  data: 0.3748 (0.0006 -- 1.9195)  max mem: 16413
Epoch: [118]  [ 80/160]  eta: 0:01:16  lr: 0.000017  min_lr: 0.000004  loss: 1.5963 (1.5193)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9035 (7.4630)  time: 0.8722 (0.5184 -- 3.1047)  data: 0.3261 (0.0003 -- 2.5773)  max mem: 16413
Epoch: [118]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000004  loss: 1.4052 (1.4993)  loss_scale: 16384.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7352 (7.4550)  time: 0.8505 (0.5441 -- 2.7043)  data: 0.2933 (0.0004 -- 2.1436)  max mem: 16413
[2023-09-04 20:03:51,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=104, lr=[4.423662355504158e-06, 4.423662355504158e-06, 4.91518039500462e-06, 4.91518039500462e-06, 5.4613115500051325e-06, 5.4613115500051325e-06, 6.068123944450148e-06, 6.068123944450148e-06, 6.742359938277941e-06, 6.742359938277941e-06, 7.491511042531047e-06, 7.491511042531047e-06, 8.323901158367829e-06, 8.323901158367829e-06, 9.248779064853141e-06, 9.248779064853141e-06, 1.0276421183170158e-05, 1.0276421183170158e-05, 1.1418245759077952e-05, 1.1418245759077952e-05, 1.2686939732308837e-05, 1.2686939732308837e-05, 1.4096599702565375e-05, 1.4096599702565375e-05, 1.566288855840597e-05, 1.566288855840597e-05, 1.7403209509339967e-05, 1.7403209509339967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 20:03:51,832] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=18.60588573806995, CurrSamplesPerSec=22.095887656648262, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000004  loss: 1.5475 (1.4821)  loss_scale: 16384.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5857 (7.6095)  time: 0.8597 (0.5325 -- 2.7640)  data: 0.3109 (0.0006 -- 2.2436)  max mem: 16413
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.4775 (1.4838)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6921 (7.5283)  time: 0.8999 (0.5255 -- 3.5181)  data: 0.3531 (0.0003 -- 2.9688)  max mem: 16413
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.4695 (1.4718)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1271 (7.4704)  time: 0.7147 (0.4984 -- 1.7302)  data: 0.1615 (0.0002 -- 1.2031)  max mem: 16413
Epoch: [118] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.4695 (1.4890)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1271 (7.4704)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1610 (0.1610)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3971 (2.3971 -- 2.3971)  data: 2.1244 (2.1244 -- 2.1244)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5204 (0.5312)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (98.9899)  time: 0.4209 (0.2104 -- 2.3971)  data: 0.1951 (0.0010 -- 2.1244)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5013 (0.5421)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2183 (0.1687 -- 0.4091)  data: 0.0125 (0.0001 -- 0.2247)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5013 (0.5862)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2004 (0.1328 -- 0.4091)  data: 0.0118 (0.0001 -- 0.2247)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 85.477 Acc@5 97.718 loss 0.602
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 87.76%
Epoch: [119]  [  0/160]  eta: 0:20:23  lr: 0.000017  min_lr: 0.000004  loss: 1.3915 (1.3915)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9107 (6.9107)  time: 7.6449 (7.6449 -- 7.6449)  data: 6.6444 (6.6444 -- 6.6444)  max mem: 16413
[2023-09-04 20:04:45,220] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 20:04:45,220] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 20:04:45,221] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 20:04:45,222] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [119]  [ 20/160]  eta: 0:02:35  lr: 0.000017  min_lr: 0.000004  loss: 1.5936 (1.5400)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2934 (7.7010)  time: 0.7821 (0.5358 -- 2.3976)  data: 0.2108 (0.0009 -- 1.5990)  max mem: 16413
[2023-09-04 20:05:01,487] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19067
[2023-09-04 20:05:01,487] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 20:05:01,487] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19067
[2023-09-04 20:05:01,488] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 20:05:01,488] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [119]  [ 40/160]  eta: 0:02:01  lr: 0.000017  min_lr: 0.000004  loss: 1.5814 (1.5690)  loss_scale: 16384.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0795 (7.8059)  time: 0.9190 (0.5204 -- 4.3510)  data: 0.3160 (0.0004 -- 2.8241)  max mem: 16413
Epoch: [119]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000004  loss: 1.5800 (1.5594)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3237 (7.9094)  time: 0.9130 (0.5288 -- 2.7629)  data: 0.2233 (0.0003 -- 1.6661)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000004  loss: 1.6437 (1.5812)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4318 (7.9347)  time: 0.8356 (0.5145 -- 4.1169)  data: 0.1817 (0.0003 -- 1.9758)  max mem: 16413
Epoch: [119]  [100/160]  eta: 0:00:57  lr: 0.000017  min_lr: 0.000004  loss: 1.3545 (1.5431)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5390 (7.7119)  time: 1.0066 (0.5122 -- 3.7804)  data: 0.4611 (0.0004 -- 3.0953)  max mem: 16413
Epoch: [119]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000004  loss: 1.3962 (1.5302)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5273 (7.6817)  time: 0.7676 (0.5293 -- 2.7081)  data: 0.1946 (0.0004 -- 2.1846)  max mem: 16413
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.6016 (1.5457)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8921 (7.9988)  time: 0.9598 (0.5269 -- 2.6979)  data: 0.4033 (0.0007 -- 2.1631)  max mem: 16413
[2023-09-04 20:06:52,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 20:06:52,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 20:06:52,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 20:06:52,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.5413 (1.5412)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5732 (7.8992)  time: 0.6127 (0.4950 -- 2.3490)  data: 0.0919 (0.0002 -- 1.8253)  max mem: 16413
Epoch: [119] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.5413 (1.5339)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5732 (7.8992)
[2023-09-04 20:06:54,169] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-09-04 20:06:54,170] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-09-04 20:06:54,170] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-09-04 20:06:54,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-09-04 20:06:55,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-09-04 20:06:55,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1623 (0.1623)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3276 (2.3276 -- 2.3276)  data: 2.0850 (2.0850 -- 2.0850)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5192 (0.5117)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4460 (0.2013 -- 2.3276)  data: 0.2299 (0.0008 -- 2.0850)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4932 (0.5186)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (99.4709)  time: 0.2314 (0.1693 -- 0.5262)  data: 0.0284 (0.0001 -- 0.3020)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4932 (0.5845)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (99.1701)  time: 0.2166 (0.1324 -- 0.5262)  data: 0.0282 (0.0001 -- 0.3020)  max mem: 16413
Val: Total time: 0:00:07 (0.2947 s / it)
* Acc@1 84.440 Acc@5 98.548 loss 0.629
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 87.76%
Epoch: [120]  [  0/160]  eta: 0:19:26  lr: 0.000017  min_lr: 0.000004  loss: 1.5140 (1.5140)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7235 (8.7235)  time: 7.2884 (7.2884 -- 7.2884)  data: 6.6981 (6.6981 -- 6.6981)  max mem: 16413
Epoch: [120]  [ 20/160]  eta: 0:02:39  lr: 0.000017  min_lr: 0.000004  loss: 1.6311 (1.6270)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3122 (8.7069)  time: 0.8325 (0.5333 -- 3.1631)  data: 0.0348 (0.0004 -- 0.6108)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:12  lr: 0.000017  min_lr: 0.000004  loss: 1.6118 (1.6139)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9443 (7.8463)  time: 1.0653 (0.5275 -- 4.6928)  data: 0.5118 (0.0004 -- 4.1501)  max mem: 16413
[2023-09-04 20:07:57,134] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19249
[2023-09-04 20:07:57,135] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 20:07:57,135] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19249
[2023-09-04 20:07:57,135] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 20:07:57,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 20:08:03,888] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19259
[2023-09-04 20:08:03,889] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 20:08:03,889] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19259
[2023-09-04 20:08:03,889] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-04 20:08:03,889] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [120]  [ 60/160]  eta: 0:01:40  lr: 0.000017  min_lr: 0.000004  loss: 1.4322 (1.5678)  loss_scale: 16384.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9685 (7.6061)  time: 0.8050 (0.5127 -- 4.4194)  data: 0.2620 (0.0002 -- 3.8861)  max mem: 16413
Epoch: [120]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000004  loss: 1.3210 (1.5216)  loss_scale: 8192.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9258 (7.6490)  time: 0.7619 (0.5306 -- 2.4545)  data: 0.1813 (0.0004 -- 1.9335)  max mem: 16413
Epoch: [120]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000004  loss: 1.5544 (1.5480)  loss_scale: 8192.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3036 (7.7016)  time: 0.8511 (0.5355 -- 4.1247)  data: 0.2664 (0.0004 -- 3.5950)  max mem: 16413
configs/vit_b_k710_pretrained200.sh: line 43: 2849197 Killed                  OMP_NUM_THREADS=1 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node=${GPUS_PER_NODE} --master_port 12320 --nnodes=1 --node_rank=0 --master_addr=localhost run_class_finetuning.py --model vit_base_patch16_224 --data_set AI-City-Track-3 --nb_classes 16 --data_path ${DATA_PATH} --finetune ${MODEL_PATH} --log_dir ${OUTPUT_DIR} --output_dir ${OUTPUT_DIR} --batch_size 6 --input_size 224 --short_side_size 224 --save_ckpt_freq 20 --num_frames 16 --sampling_rate 4 --num_sample 2 --num_workers 8 --opt adamw --lr 1e-3 --drop_path 0.1 --head_drop_rate 0.0 --layer_decay 0.9 --opt_betas 0.9 0.999 --warmup_epochs 5 --epochs 200 --test_num_segment 5 --test_num_crop 3 --dist_eval --enable_deepspeed
