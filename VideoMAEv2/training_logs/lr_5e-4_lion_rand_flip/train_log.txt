[2023-08-30 21:32:26,447] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 21:32:26,496] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.0005, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='lion', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f57374b9940>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00002344
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-08-30 21:32:31,897] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-30 21:32:31,897] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-30 21:32:32,034] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-30 21:32:32,034] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-30 21:32:32,137] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.503664493560791 seconds
[2023-08-30 21:32:33,230] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-08-30 21:32:33,239] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-08-30 21:32:33,239] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-08-30 21:32:33,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-08-30 21:32:33,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-30 21:32:33,265] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-30 21:32:33,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 21:32:33,265] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 21:32:33,266] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 21:32:33,266] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 21:32:33,266] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 21:32:33,266] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 21:32:33,266] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 21:32:33,266] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 21:32:33,266] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f56e9f12400>
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 21:32:33,267] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0005, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 21:32:33,268] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 21:32:33,269] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 21:32:33,269] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.0005, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:29:55  lr: 0.000000  min_lr: 0.000000  loss: 2.7734 (2.7734)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 11.2225 (11.2225 -- 11.2225)  data: 7.3012 (7.3012 -- 7.3012)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:47  lr: 0.000001  min_lr: 0.000000  loss: 2.7730 (2.7731)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5932 (1.6020)  time: 0.6944 (0.5089 -- 2.7622)  data: 0.0639 (0.0004 -- 0.7786)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:13  lr: 0.000001  min_lr: 0.000000  loss: 2.7725 (2.7728)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4588 (1.5416)  time: 1.0191 (0.5083 -- 4.1875)  data: 0.0820 (0.0003 -- 1.0280)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 2.7725 (2.7727)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4180 (1.5246)  time: 0.7559 (0.5071 -- 3.4859)  data: 0.0020 (0.0003 -- 0.0160)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:19  lr: 0.000002  min_lr: 0.000000  loss: 2.7724 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4507 (1.5166)  time: 1.0154 (0.5131 -- 5.7759)  data: 0.0015 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4778 (1.5100)  time: 0.6433 (0.5141 -- 2.4252)  data: 0.0015 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 2.7720 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4889 (1.5102)  time: 0.9707 (0.5131 -- 4.0684)  data: 0.0305 (0.0004 -- 0.5725)  max mem: 16413
[2023-08-30 21:34:32,648] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:34:32,648] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-08-30 21:34:32,649] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:34:32,650] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 2.7718 (2.7724)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4382 (1.5000)  time: 0.7749 (0.5111 -- 3.4717)  data: 0.0012 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 2.7715 (2.7723)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3509 (1.4870)  time: 0.7237 (0.4870 -- 3.3459)  data: 0.0007 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [0] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 2.7715 (2.7723)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3509 (1.4870)
Val:  [ 0/27]  eta: 0:01:21  loss: 2.7695 (2.7695)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 3.0073 (3.0073 -- 3.0073)  data: 2.6196 (2.6196 -- 2.6196)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 2.7695 (2.7695)  acc1: 22.2222 (29.2929)  acc5: 77.7778 (73.7374)  time: 0.4753 (0.2064 -- 3.0073)  data: 0.2398 (0.0004 -- 2.6196)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7693 (2.7694)  acc1: 33.3333 (30.6878)  acc5: 77.7778 (77.2487)  time: 0.2147 (0.1689 -- 0.3117)  data: 0.0079 (0.0001 -- 0.1360)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7693 (2.7696)  acc1: 28.5714 (30.2905)  acc5: 77.7778 (75.5187)  time: 0.2000 (0.1683 -- 0.3117)  data: 0.0073 (0.0001 -- 0.1360)  max mem: 16413
Val: Total time: 0:00:08 (0.3083 s / it)
* Acc@1 30.705 Acc@5 76.141 loss 2.770
Accuracy of the network on the 482 val images: 30.71%
[2023-08-30 21:35:04,405] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-30 21:35:04,408] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 21:35:04,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 21:35:04,408] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 21:35:05,451] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 21:35:05,451] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.71%
Epoch: [1]  [  0/160]  eta: 0:19:13  lr: 0.000005  min_lr: 0.000000  loss: 2.7710 (2.7710)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7463 (1.7463)  time: 7.2124 (7.2124 -- 7.2124)  data: 6.6967 (6.6967 -- 6.6967)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:33  lr: 0.000005  min_lr: 0.000000  loss: 2.7710 (2.7709)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4784 (1.5156)  time: 0.7925 (0.5255 -- 2.7439)  data: 0.2300 (0.0004 -- 2.1722)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:01  lr: 0.000006  min_lr: 0.000000  loss: 2.7706 (2.7708)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5322 (1.5263)  time: 0.9186 (0.5064 -- 3.1259)  data: 0.1815 (0.0004 -- 1.8427)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:35  lr: 0.000006  min_lr: 0.000000  loss: 2.7690 (2.7702)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5083 (1.5371)  time: 0.8345 (0.5168 -- 3.1001)  data: 0.1742 (0.0005 -- 1.9740)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000000  loss: 2.7680 (2.7696)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5333 (1.5264)  time: 0.9338 (0.5210 -- 3.7164)  data: 0.1760 (0.0004 -- 1.7502)  max mem: 16413
[2023-08-30 21:36:36,738] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:36:36,738] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:36:36,738] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-08-30 21:36:36,751] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 2.7658 (2.7689)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5376 (1.5301)  time: 0.8404 (0.5214 -- 3.1672)  data: 0.0497 (0.0004 -- 0.7836)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 2.7638 (2.7681)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5096 (1.5221)  time: 0.9476 (0.5304 -- 4.0766)  data: 0.1057 (0.0004 -- 1.2073)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 2.7596 (2.7669)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5479 (1.5255)  time: 0.7573 (0.5281 -- 2.8411)  data: 0.0016 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.7594 (2.7659)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5030 (1.5268)  time: 0.6900 (0.4946 -- 1.8758)  data: 0.0342 (0.0002 -- 0.6629)  max mem: 16413
Epoch: [1] Total time: 0:02:20 (0.8811 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.7594 (2.7660)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5030 (1.5268)
Val:  [ 0/27]  eta: 0:01:07  loss: 2.7381 (2.7381)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4962 (2.4962 -- 2.4962)  data: 2.2498 (2.2498 -- 2.2498)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7431 (2.7426)  acc1: 33.3333 (37.3737)  acc5: 77.7778 (85.8586)  time: 0.4308 (0.2040 -- 2.4962)  data: 0.2192 (0.0007 -- 2.2498)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7409 (2.7411)  acc1: 44.4444 (41.2698)  acc5: 88.8889 (88.3598)  time: 0.2190 (0.1718 -- 0.3653)  data: 0.0177 (0.0001 -- 0.1895)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7435 (2.7426)  acc1: 44.4444 (41.9087)  acc5: 88.8889 (87.1369)  time: 0.2048 (0.1329 -- 0.3653)  data: 0.0174 (0.0001 -- 0.1895)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 42.116 Acc@5 86.100 loss 2.742
Accuracy of the network on the 482 val images: 42.12%
[2023-08-30 21:37:34,298] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 21:37:34,300] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 21:37:34,300] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 21:37:34,300] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 21:37:35,580] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 21:37:35,580] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.12%
Epoch: [2]  [  0/160]  eta: 0:23:03  lr: 0.000009  min_lr: 0.000000  loss: 2.7672 (2.7672)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4090 (1.4090)  time: 8.6448 (8.6448 -- 8.6448)  data: 8.1128 (8.1128 -- 8.1128)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:44  lr: 0.000010  min_lr: 0.000000  loss: 2.7547 (2.7549)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5348 (1.5218)  time: 0.8002 (0.5153 -- 5.1163)  data: 0.2613 (0.0003 -- 4.5930)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:08  lr: 0.000011  min_lr: 0.000000  loss: 2.7471 (2.7525)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5253 (1.5369)  time: 0.9574 (0.5123 -- 4.7400)  data: 0.4158 (0.0005 -- 4.2230)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:36  lr: 0.000011  min_lr: 0.000000  loss: 2.7453 (2.7500)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5030 (1.5354)  time: 0.7646 (0.5361 -- 3.5035)  data: 0.2153 (0.0005 -- 2.9658)  max mem: 16413
[2023-08-30 21:38:38,321] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:38:38,322] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:38:38,322] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-08-30 21:38:38,322] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 2.7388 (2.7479)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5147 (1.5315)  time: 0.8618 (0.5215 -- 4.0422)  data: 0.3099 (0.0003 -- 3.5180)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 2.7361 (2.7460)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5558 (1.5413)  time: 0.8979 (0.5290 -- 2.4200)  data: 0.1507 (0.0004 -- 1.8745)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 2.7320 (2.7435)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5593 (1.5522)  time: 0.8552 (0.5361 -- 2.6584)  data: 0.2119 (0.0004 -- 2.1238)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:17  lr: 0.000013  min_lr: 0.000000  loss: 2.7170 (2.7394)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4963 (1.5558)  time: 0.7740 (0.5225 -- 2.8080)  data: 0.1992 (0.0004 -- 2.2589)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 2.7113 (2.7360)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5729 (1.5602)  time: 0.6946 (0.4948 -- 1.8223)  data: 0.1049 (0.0002 -- 1.2369)  max mem: 16413
Epoch: [2] Total time: 0:02:20 (0.8764 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 2.7113 (2.7357)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5729 (1.5602)
Val:  [ 0/27]  eta: 0:01:07  loss: 2.6352 (2.6352)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4978 (2.4978 -- 2.4978)  data: 2.1777 (2.1777 -- 2.1777)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.6593 (2.6553)  acc1: 44.4444 (40.4040)  acc5: 100.0000 (88.8889)  time: 0.4353 (0.2007 -- 2.4978)  data: 0.2072 (0.0006 -- 2.1777)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6421 (2.6491)  acc1: 44.4444 (39.1534)  acc5: 88.8889 (89.9471)  time: 0.2117 (0.1709 -- 0.3326)  data: 0.0053 (0.0001 -- 0.0932)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6469 (2.6535)  acc1: 44.4444 (39.8340)  acc5: 88.8889 (88.3817)  time: 0.1950 (0.1328 -- 0.3326)  data: 0.0050 (0.0001 -- 0.0932)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 41.494 Acc@5 87.759 loss 2.652
Accuracy of the network on the 482 val images: 41.49%
Max accuracy: 42.12%
Epoch: [3]  [  0/160]  eta: 0:16:52  lr: 0.000014  min_lr: 0.000000  loss: 2.6975 (2.6975)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8170 (1.8170)  time: 6.3283 (6.3283 -- 6.3283)  data: 5.7859 (5.7859 -- 5.7859)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:32  lr: 0.000015  min_lr: 0.000000  loss: 2.7112 (2.7121)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6816 (1.6910)  time: 0.8299 (0.5240 -- 3.0233)  data: 0.2767 (0.0004 -- 2.5027)  max mem: 16413
[2023-08-30 21:40:39,714] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:40:39,715] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-08-30 21:40:39,717] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:40:39,718] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:01  lr: 0.000015  min_lr: 0.000000  loss: 2.6923 (2.7031)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5166 (1.6149)  time: 0.9328 (0.5371 -- 4.0321)  data: 0.3663 (0.0003 -- 3.4483)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000000  loss: 2.6744 (2.6941)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6707 (1.6315)  time: 0.8525 (0.5220 -- 2.7302)  data: 0.2747 (0.0004 -- 2.2118)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:17  lr: 0.000016  min_lr: 0.000000  loss: 2.6739 (2.6894)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7300 (1.6627)  time: 0.9944 (0.5260 -- 4.2913)  data: 0.2531 (0.0006 -- 2.3793)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000000  loss: 2.6836 (2.6864)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6225 (1.6646)  time: 0.8272 (0.5286 -- 3.0851)  data: 0.0011 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 2.6485 (2.6798)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7590 (1.6848)  time: 0.7648 (0.5295 -- 2.2187)  data: 0.0015 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 2.6548 (2.6753)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7301 (1.6918)  time: 1.0091 (0.5248 -- 3.2835)  data: 0.0495 (0.0002 -- 0.9588)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.6525 (2.6712)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7349 (1.6992)  time: 0.6902 (0.4940 -- 2.3372)  data: 0.0086 (0.0002 -- 0.1613)  max mem: 16413
Epoch: [3] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.6525 (2.6706)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7349 (1.6992)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.4688 (2.4688)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3888 (2.3888 -- 2.3888)  data: 2.1611 (2.1611 -- 2.1611)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.5317 (2.5169)  acc1: 33.3333 (40.4040)  acc5: 100.0000 (90.9091)  time: 0.4177 (0.1909 -- 2.3888)  data: 0.2061 (0.0008 -- 2.1611)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.4915 (2.5038)  acc1: 44.4444 (41.2698)  acc5: 88.8889 (91.5344)  time: 0.2179 (0.1706 -- 0.4119)  data: 0.0168 (0.0001 -- 0.2263)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.4954 (2.5127)  acc1: 44.4444 (41.4938)  acc5: 88.8889 (87.9668)  time: 0.2042 (0.1330 -- 0.4119)  data: 0.0162 (0.0001 -- 0.2263)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 42.116 Acc@5 87.552 loss 2.512
Accuracy of the network on the 482 val images: 42.12%
Max accuracy: 42.12%
[2023-08-30 21:42:38,825] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:42:38,826] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-08-30 21:42:38,826] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:42:38,826] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:14:41  lr: 0.000019  min_lr: 0.000000  loss: 2.5885 (2.5885)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9866 (1.9866)  time: 5.5111 (5.5111 -- 5.5111)  data: 4.6764 (4.6764 -- 4.6764)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:45  lr: 0.000019  min_lr: 0.000000  loss: 2.6112 (2.6027)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8496 (1.8509)  time: 0.9648 (0.5339 -- 3.1516)  data: 0.3155 (0.0005 -- 2.5897)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:01:58  lr: 0.000020  min_lr: 0.000000  loss: 2.6109 (2.6041)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8621 (1.8817)  time: 0.7896 (0.5256 -- 3.5835)  data: 0.2419 (0.0002 -- 3.0684)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:39  lr: 0.000021  min_lr: 0.000000  loss: 2.5765 (2.5920)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8486 (1.8879)  time: 0.9965 (0.5235 -- 3.9711)  data: 0.4486 (0.0003 -- 3.4285)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000001  loss: 2.5813 (2.5874)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0296 (1.9303)  time: 0.7732 (0.5282 -- 1.7005)  data: 0.1463 (0.0003 -- 0.9642)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.6081 (2.5835)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0659 (1.9542)  time: 0.8577 (0.5159 -- 3.0354)  data: 0.2623 (0.0006 -- 2.5250)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.5517 (2.5754)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0573 (1.9775)  time: 0.9315 (0.5249 -- 4.9139)  data: 0.0049 (0.0004 -- 0.0706)  max mem: 16413
[2023-08-30 21:44:31,052] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:44:31,052] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:44:31,052] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-08-30 21:44:31,052] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.5899 (2.5757)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0594 (1.9911)  time: 0.9659 (0.5249 -- 2.9534)  data: 0.2521 (0.0003 -- 2.3007)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.5684 (2.5752)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0600 (2.0029)  time: 0.6820 (0.4940 -- 2.9534)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [4] Total time: 0:02:21 (0.8864 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.5684 (2.5845)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0600 (2.0029)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.2973 (2.2973)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3380 (2.3380 -- 2.3380)  data: 2.1068 (2.1068 -- 2.1068)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.3486 (2.3492)  acc1: 44.4444 (43.4343)  acc5: 100.0000 (92.9293)  time: 0.4215 (0.2052 -- 2.3380)  data: 0.1999 (0.0008 -- 2.1068)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.3203 (2.3330)  acc1: 44.4444 (42.3280)  acc5: 100.0000 (92.5926)  time: 0.2183 (0.1698 -- 0.2932)  data: 0.0082 (0.0001 -- 0.0678)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.3203 (2.3490)  acc1: 44.4444 (42.7386)  acc5: 88.8889 (89.2116)  time: 0.2013 (0.1322 -- 0.2932)  data: 0.0071 (0.0001 -- 0.0678)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 44.606 Acc@5 89.419 loss 2.346
Accuracy of the network on the 482 val images: 44.61%
[2023-08-30 21:45:02,847] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 21:45:02,849] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 21:45:02,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 21:45:02,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 21:45:04,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 21:45:04,040] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.61%
Epoch: [5]  [  0/160]  eta: 0:16:59  lr: 0.000023  min_lr: 0.000001  loss: 2.4740 (2.4740)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2130 (2.2130)  time: 6.3715 (6.3715 -- 6.3715)  data: 5.8497 (5.8497 -- 5.8497)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:41  lr: 0.000023  min_lr: 0.000001  loss: 2.5181 (2.5317)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2899 (2.4945)  time: 0.8908 (0.5344 -- 4.0648)  data: 0.2489 (0.0007 -- 3.3385)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 2.4960 (2.5143)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2965 (2.4316)  time: 0.8727 (0.5191 -- 3.7807)  data: 0.0251 (0.0004 -- 0.4568)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000001  loss: 2.5373 (2.5170)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2579 (2.4292)  time: 0.9599 (0.5092 -- 2.6660)  data: 0.0027 (0.0004 -- 0.0150)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.4792 (2.5098)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3379 (2.4686)  time: 0.7944 (0.5231 -- 2.4799)  data: 0.0013 (0.0002 -- 0.0022)  max mem: 16413
[2023-08-30 21:46:33,073] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:46:33,073] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-08-30 21:46:33,073] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:46:33,073] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.5067 (2.5083)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6045 (2.5075)  time: 0.8164 (0.5363 -- 2.3561)  data: 0.0019 (0.0003 -- 0.0076)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.4722 (2.5039)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4061 (2.5137)  time: 0.8318 (0.5317 -- 3.2884)  data: 0.0018 (0.0004 -- 0.0070)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.5033 (2.5038)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4767 (2.5238)  time: 0.9601 (0.5378 -- 5.3270)  data: 0.0020 (0.0005 -- 0.0093)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.3968 (2.4968)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4767 (2.5418)  time: 0.6262 (0.4951 -- 2.1891)  data: 0.0006 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [5] Total time: 0:02:20 (0.8806 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.3968 (2.5000)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4767 (2.5418)
Val:  [ 0/27]  eta: 0:01:09  loss: 2.1385 (2.1385)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.5679 (2.5679 -- 2.5679)  data: 2.3364 (2.3364 -- 2.3364)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.1979 (2.1953)  acc1: 44.4444 (43.4343)  acc5: 100.0000 (94.9495)  time: 0.4570 (0.1919 -- 2.5679)  data: 0.2390 (0.0007 -- 2.3364)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.1532 (2.1746)  acc1: 44.4444 (42.3280)  acc5: 100.0000 (94.1799)  time: 0.2163 (0.1685 -- 0.5259)  data: 0.0148 (0.0001 -- 0.2843)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.1860 (2.1958)  acc1: 44.4444 (41.9087)  acc5: 88.8889 (92.1162)  time: 0.2034 (0.1325 -- 0.5259)  data: 0.0146 (0.0001 -- 0.2843)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 44.398 Acc@5 91.494 loss 2.191
Accuracy of the network on the 482 val images: 44.40%
Max accuracy: 44.61%
Epoch: [6]  [  0/160]  eta: 0:17:11  lr: 0.000023  min_lr: 0.000001  loss: 2.5952 (2.5952)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7533 (3.7533)  time: 6.4479 (6.4479 -- 6.4479)  data: 5.8873 (5.8873 -- 5.8873)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:42  lr: 0.000023  min_lr: 0.000001  loss: 2.5396 (2.5327)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8527 (2.9108)  time: 0.8962 (0.5327 -- 2.2013)  data: 0.2123 (0.0003 -- 1.6488)  max mem: 16413
[2023-08-30 21:48:13,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[5.567550977443171e-07, 5.567550977443171e-07, 7.423401303257562e-07, 7.423401303257562e-07, 9.897868404343417e-07, 9.897868404343417e-07, 1.3197157872457889e-06, 1.3197157872457889e-06, 1.759621049661052e-06, 1.759621049661052e-06, 2.346161399548069e-06, 2.346161399548069e-06, 3.1282151993974255e-06, 3.1282151993974255e-06, 4.170953599196567e-06, 4.170953599196567e-06, 5.561271465595423e-06, 5.561271465595423e-06, 7.415028620793898e-06, 7.415028620793898e-06, 9.886704827725196e-06, 9.886704827725196e-06, 1.3182273103633595e-05, 1.3182273103633595e-05, 1.757636413817813e-05, 1.757636413817813e-05, 2.3435152184237504e-05, 2.3435152184237504e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 21:48:13,952] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=17.268033345984794, CurrSamplesPerSec=21.805668595016616, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 2.4703 (2.4864)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8932 (2.9513)  time: 0.8637 (0.5177 -- 4.3045)  data: 0.3215 (0.0005 -- 3.7763)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.4208 (2.4676)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7084 (2.8803)  time: 0.9256 (0.5251 -- 3.1332)  data: 0.3532 (0.0006 -- 2.6128)  max mem: 16413
[2023-08-30 21:48:35,195] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:48:35,195] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-08-30 21:48:35,198] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:48:35,199] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:13  lr: 0.000023  min_lr: 0.000001  loss: 2.4294 (2.4616)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7622 (2.9055)  time: 0.7135 (0.5252 -- 1.7742)  data: 0.1076 (0.0004 -- 1.2281)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.4443 (2.4597)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1450 (2.9800)  time: 1.0288 (0.5232 -- 4.6415)  data: 0.1888 (0.0003 -- 2.5777)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.3818 (2.4513)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8809 (2.9751)  time: 0.7784 (0.5269 -- 2.5228)  data: 0.0016 (0.0004 -- 0.0050)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.3638 (2.4412)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1231 (3.0359)  time: 0.9712 (0.5273 -- 4.2675)  data: 0.0021 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.4177 (2.4357)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9326 (3.0447)  time: 0.6302 (0.4935 -- 2.2790)  data: 0.0008 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [6] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.4177 (2.4341)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9326 (3.0447)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.9914 (1.9914)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.5568 (2.5568 -- 2.5568)  data: 2.3003 (2.3003 -- 2.3003)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.0610 (2.0668)  acc1: 44.4444 (41.4141)  acc5: 100.0000 (93.9394)  time: 0.4273 (0.1965 -- 2.5568)  data: 0.2101 (0.0006 -- 2.3003)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.0281 (2.0446)  acc1: 44.4444 (39.6825)  acc5: 100.0000 (93.6508)  time: 0.2044 (0.1693 -- 0.2333)  data: 0.0023 (0.0001 -- 0.0265)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0592 (2.0708)  acc1: 44.4444 (39.4191)  acc5: 88.8889 (91.7012)  time: 0.1890 (0.1328 -- 0.2211)  data: 0.0020 (0.0001 -- 0.0265)  max mem: 16413
Val: Total time: 0:00:07 (0.2826 s / it)
* Acc@1 43.154 Acc@5 90.664 loss 2.059
Accuracy of the network on the 482 val images: 43.15%
Max accuracy: 44.61%
Epoch: [7]  [  0/160]  eta: 0:18:41  lr: 0.000023  min_lr: 0.000001  loss: 2.1770 (2.1770)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6473 (2.6473)  time: 7.0101 (7.0101 -- 7.0101)  data: 6.4629 (6.4629 -- 6.4629)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:57  lr: 0.000023  min_lr: 0.000001  loss: 2.4316 (2.4128)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0403 (3.3433)  time: 0.9821 (0.5144 -- 4.3146)  data: 0.1269 (0.0003 -- 2.4946)  max mem: 16413
[2023-08-30 21:50:38,189] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1152
[2023-08-30 21:50:38,189] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1152
[2023-08-30 21:50:38,189] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2023-08-30 21:50:38,189] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0
[2023-08-30 21:50:38,189] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768, reducing to 16384.0
Epoch: [7]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 2.3286 (2.3929)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9611 (3.2970)  time: 0.7456 (0.5249 -- 2.9666)  data: 0.0013 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [7]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 2.5037 (2.4082)  loss_scale: 16384.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3191 (3.4512)  time: 0.8519 (0.5278 -- 3.3325)  data: 0.0017 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.4251 (2.4141)  loss_scale: 16384.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0560 (3.4013)  time: 0.8671 (0.5300 -- 2.5746)  data: 0.2292 (0.0003 -- 1.7964)  max mem: 16413
Epoch: [7]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.3556 (2.3997)  loss_scale: 16384.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0941 (3.4228)  time: 0.8402 (0.5303 -- 2.8685)  data: 0.1862 (0.0002 -- 2.3214)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.3462 (2.3943)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5695 (3.4639)  time: 0.8311 (0.5273 -- 3.5077)  data: 0.0839 (0.0004 -- 0.5762)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.3800 (2.3886)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4082 (3.5202)  time: 0.8947 (0.5185 -- 2.6870)  data: 0.3374 (0.0004 -- 2.1466)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.3696 (2.3898)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1953 (3.5702)  time: 0.6974 (0.4955 -- 2.7809)  data: 0.1840 (0.0002 -- 2.2532)  max mem: 16413
Epoch: [7] Total time: 0:02:20 (0.8795 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.3696 (2.3912)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1953 (3.5702)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.8941 (1.8941)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4396 (2.4396 -- 2.4396)  data: 2.1875 (2.1875 -- 2.1875)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.9326 (1.9641)  acc1: 44.4444 (43.4343)  acc5: 100.0000 (92.9293)  time: 0.4186 (0.1935 -- 2.4396)  data: 0.2000 (0.0007 -- 2.1875)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.9090 (1.9421)  acc1: 44.4444 (41.7989)  acc5: 100.0000 (93.1217)  time: 0.2188 (0.1693 -- 0.4356)  data: 0.0130 (0.0001 -- 0.2453)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.9529 (1.9739)  acc1: 33.3333 (40.2490)  acc5: 88.8889 (92.9461)  time: 0.2002 (0.1320 -- 0.4356)  data: 0.0127 (0.0001 -- 0.2453)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 45.228 Acc@5 92.324 loss 1.958
Accuracy of the network on the 482 val images: 45.23%
[2023-08-30 21:52:31,053] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 21:52:31,055] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 21:52:31,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 21:52:31,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 21:52:32,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 21:52:32,416] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 45.23%
Epoch: [8]  [  0/160]  eta: 0:21:05  lr: 0.000023  min_lr: 0.000001  loss: 2.2241 (2.2241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5924 (5.5924)  time: 7.9092 (7.9092 -- 7.9092)  data: 6.0412 (6.0412 -- 6.0412)  max mem: 16413
[2023-08-30 21:52:40,999] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:52:40,999] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:52:40,999] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 21:52:40,999] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [8]  [ 20/160]  eta: 0:02:49  lr: 0.000023  min_lr: 0.000001  loss: 2.3571 (2.3459)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5028 (3.7582)  time: 0.8738 (0.5186 -- 2.8061)  data: 0.0830 (0.0004 -- 0.8795)  max mem: 16413
Epoch: [8]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 2.2971 (2.3186)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9327 (4.0376)  time: 0.9044 (0.5139 -- 4.4318)  data: 0.0017 (0.0002 -- 0.0108)  max mem: 16413
Epoch: [8]  [ 60/160]  eta: 0:01:40  lr: 0.000023  min_lr: 0.000001  loss: 2.2959 (2.3249)  loss_scale: 32768.0000 (32499.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8509 (3.9799)  time: 0.8980 (0.5281 -- 3.8605)  data: 0.0015 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [8]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.3320 (2.3235)  loss_scale: 32768.0000 (32565.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4784 (3.9087)  time: 0.8039 (0.5356 -- 3.9036)  data: 0.0019 (0.0003 -- 0.0131)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.3641 (2.3354)  loss_scale: 32768.0000 (32605.7822)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5697 (3.8975)  time: 0.8455 (0.5327 -- 2.6903)  data: 0.0021 (0.0004 -- 0.0134)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.3076 (2.3415)  loss_scale: 32768.0000 (32632.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0571 (4.0067)  time: 0.8025 (0.5376 -- 2.2693)  data: 0.0019 (0.0009 -- 0.0044)  max mem: 16413
[2023-08-30 21:54:30,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:54:30,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 21:54:30,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:54:30,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 21:54:31,985] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1411
[2023-08-30 21:54:31,985] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1411
[2023-08-30 21:54:31,985] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 21:54:31,985] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 21:54:31,986] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.4578 (2.3535)  loss_scale: 32768.0000 (33116.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4133 (4.1004)  time: 0.8846 (0.5361 -- 2.8794)  data: 0.0097 (0.0006 -- 0.1620)  max mem: 16413
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.4143 (2.3574)  loss_scale: 32768.0000 (33075.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5255 (4.0599)  time: 0.6716 (0.4951 -- 2.4571)  data: 0.1215 (0.0002 -- 1.3486)  max mem: 16413
Epoch: [8] Total time: 0:02:21 (0.8815 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.4143 (2.3520)  loss_scale: 32768.0000 (33075.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5255 (4.0599)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.8060 (1.8060)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4431 (2.4431 -- 2.4431)  data: 2.1692 (2.1692 -- 2.1692)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.8513 (1.8732)  acc1: 55.5556 (48.4848)  acc5: 100.0000 (93.9394)  time: 0.4226 (0.2064 -- 2.4431)  data: 0.2010 (0.0009 -- 2.1692)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8321 (1.8477)  acc1: 55.5556 (47.0899)  acc5: 100.0000 (93.6508)  time: 0.2197 (0.1692 -- 0.4465)  data: 0.0157 (0.0001 -- 0.2696)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.8627 (1.8825)  acc1: 44.4444 (46.8880)  acc5: 88.8889 (93.3610)  time: 0.2033 (0.1329 -- 0.4465)  data: 0.0153 (0.0001 -- 0.2696)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 51.660 Acc@5 92.324 loss 1.864
Accuracy of the network on the 482 val images: 51.66%
[2023-08-30 21:55:01,297] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 21:55:01,299] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 21:55:01,299] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 21:55:01,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 21:55:02,755] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 21:55:02,756] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 51.66%
Epoch: [9]  [  0/160]  eta: 0:17:09  lr: 0.000023  min_lr: 0.000001  loss: 2.2984 (2.2984)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6065 (3.6065)  time: 6.4344 (6.4344 -- 6.4344)  data: 5.9061 (5.9061 -- 5.9061)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:03:00  lr: 0.000023  min_lr: 0.000001  loss: 2.3059 (2.3058)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0761 (4.4435)  time: 1.0326 (0.5244 -- 4.4336)  data: 0.0016 (0.0003 -- 0.0051)  max mem: 16413
Epoch: [9]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 2.2971 (2.3178)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6696 (4.2752)  time: 0.8264 (0.5244 -- 3.9080)  data: 0.0013 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [9]  [ 60/160]  eta: 0:01:43  lr: 0.000023  min_lr: 0.000001  loss: 2.2527 (2.3091)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3552 (4.3255)  time: 0.9803 (0.5159 -- 5.2699)  data: 0.0014 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.3697 (2.3147)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9714 (4.2517)  time: 0.7139 (0.5148 -- 2.6642)  data: 0.0023 (0.0004 -- 0.0170)  max mem: 16413
[2023-08-30 21:56:38,123] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:56:38,124] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 21:56:38,124] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:56:38,124] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.3299 (2.3137)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5047 (4.3442)  time: 0.8926 (0.5242 -- 3.8680)  data: 0.0017 (0.0003 -- 0.0036)  max mem: 16413
[2023-08-30 21:56:44,779] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1547
[2023-08-30 21:56:44,779] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1547
[2023-08-30 21:56:44,779] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 21:56:44,779] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 21:56:44,779] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.2413 (2.3073)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4998 (4.2592)  time: 0.7759 (0.5214 -- 3.3875)  data: 0.0017 (0.0003 -- 0.0074)  max mem: 16413
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.2115 (2.2933)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8972 (4.2306)  time: 0.9599 (0.5308 -- 3.4464)  data: 0.0343 (0.0004 -- 0.6555)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1718 (2.2923)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1868 (4.2938)  time: 0.6455 (0.4944 -- 2.1807)  data: 0.0962 (0.0002 -- 1.6466)  max mem: 16413
Epoch: [9] Total time: 0:02:22 (0.8905 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1718 (2.2961)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1868 (4.2938)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.7415 (1.7415)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4777 (2.4777 -- 2.4777)  data: 2.2088 (2.2088 -- 2.2088)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.7521 (1.7886)  acc1: 55.5556 (51.5152)  acc5: 100.0000 (93.9394)  time: 0.4405 (0.1971 -- 2.4777)  data: 0.2237 (0.0005 -- 2.2088)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.7521 (1.7581)  acc1: 55.5556 (49.2063)  acc5: 100.0000 (94.7090)  time: 0.2196 (0.1694 -- 0.4617)  data: 0.0173 (0.0001 -- 0.2430)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7600 (1.7958)  acc1: 44.4444 (49.7925)  acc5: 88.8889 (94.1909)  time: 0.2067 (0.1327 -- 0.4617)  data: 0.0171 (0.0001 -- 0.2430)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 53.527 Acc@5 93.154 loss 1.776
Accuracy of the network on the 482 val images: 53.53%
[2023-08-30 21:57:33,102] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 21:57:33,103] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 21:57:33,103] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 21:57:33,104] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 21:57:34,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 21:57:34,475] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 53.53%
Epoch: [10]  [  0/160]  eta: 0:24:45  lr: 0.000023  min_lr: 0.000001  loss: 2.5251 (2.5251)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1062 (3.1062)  time: 9.2833 (9.2833 -- 9.2833)  data: 8.7362 (8.7362 -- 8.7362)  max mem: 16413
Epoch: [10]  [ 20/160]  eta: 0:02:55  lr: 0.000023  min_lr: 0.000001  loss: 2.2103 (2.2596)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9903 (4.5015)  time: 0.8494 (0.5202 -- 4.3962)  data: 0.2565 (0.0002 -- 3.8591)  max mem: 16413
Epoch: [10]  [ 40/160]  eta: 0:02:11  lr: 0.000023  min_lr: 0.000001  loss: 2.2940 (2.2953)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3473 (4.6447)  time: 0.9404 (0.5202 -- 3.6790)  data: 0.0739 (0.0004 -- 1.0298)  max mem: 16413
Epoch: [10]  [ 60/160]  eta: 0:01:40  lr: 0.000023  min_lr: 0.000001  loss: 2.1143 (2.2506)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1607 (4.7737)  time: 0.8171 (0.5061 -- 3.4185)  data: 0.0008 (0.0003 -- 0.0023)  max mem: 16413
[2023-08-30 21:58:43,056] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1670
[2023-08-30 21:58:43,056] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:58:43,056] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1670
[2023-08-30 21:58:43,056] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:58:43,056] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [10]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1816 (2.2310)  loss_scale: 16384.0000 (30543.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7839 (4.6106)  time: 0.9211 (0.5058 -- 4.3453)  data: 0.0009 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:57  lr: 0.000023  min_lr: 0.000001  loss: 2.3977 (2.2524)  loss_scale: 16384.0000 (27739.2475)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5474 (4.7272)  time: 0.8137 (0.5232 -- 3.5663)  data: 0.0012 (0.0001 -- 0.0031)  max mem: 16413
Epoch: [10]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 2.2936 (2.2531)  loss_scale: 16384.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4896 (4.7158)  time: 0.8067 (0.5158 -- 3.5851)  data: 0.0018 (0.0004 -- 0.0058)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.3321 (2.2682)  loss_scale: 16384.0000 (24517.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0121 (4.7943)  time: 0.7951 (0.5248 -- 3.3872)  data: 0.0014 (0.0005 -- 0.0035)  max mem: 16413
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.2888 (2.2782)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3685 (4.8551)  time: 0.6506 (0.5003 -- 1.5823)  data: 0.0751 (0.0002 -- 0.8276)  max mem: 16413
Epoch: [10] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.2888 (2.2754)  loss_scale: 16384.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3685 (4.8551)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.6853 (1.6853)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3191 (2.3191 -- 2.3191)  data: 2.0399 (2.0399 -- 2.0399)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.6910 (1.7159)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (95.9596)  time: 0.4166 (0.2095 -- 2.3191)  data: 0.1873 (0.0005 -- 2.0399)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6910 (1.6959)  acc1: 55.5556 (51.3228)  acc5: 100.0000 (95.7672)  time: 0.2226 (0.1720 -- 0.4266)  data: 0.0130 (0.0001 -- 0.2355)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7312 (1.7371)  acc1: 55.5556 (51.8672)  acc5: 88.8889 (94.1909)  time: 0.2040 (0.1325 -- 0.4266)  data: 0.0122 (0.0001 -- 0.2355)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 54.979 Acc@5 93.361 loss 1.715
Accuracy of the network on the 482 val images: 54.98%
[2023-08-30 22:00:02,882] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:00:02,884] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:00:02,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:00:02,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:00:04,265] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:00:04,265] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 54.98%
Epoch: [11]  [  0/160]  eta: 0:19:16  lr: 0.000023  min_lr: 0.000001  loss: 2.4035 (2.4035)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2701 (4.2701)  time: 7.2250 (7.2250 -- 7.2250)  data: 5.8677 (5.8677 -- 5.8677)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:31  lr: 0.000023  min_lr: 0.000001  loss: 2.3269 (2.3478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8705 (5.0759)  time: 0.7718 (0.5285 -- 2.8380)  data: 0.1392 (0.0004 -- 1.4017)  max mem: 16413
[2023-08-30 22:00:43,422] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:00:43,422] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:00:43,425] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:00:43,425] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 2.2114 (2.2890)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7667 (5.2007)  time: 0.9447 (0.5285 -- 3.4622)  data: 0.0144 (0.0005 -- 0.2557)  max mem: 16413
Epoch: [11]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 2.3152 (2.2952)  loss_scale: 32768.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1499 (5.2711)  time: 0.8806 (0.5225 -- 2.4012)  data: 0.1871 (0.0005 -- 1.8863)  max mem: 16413
Epoch: [11]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.2477 (2.2897)  loss_scale: 32768.0000 (24879.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3396 (5.2873)  time: 0.8181 (0.5246 -- 1.8612)  data: 0.0017 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.2722 (2.2774)  loss_scale: 32768.0000 (26441.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2614 (5.2905)  time: 0.8237 (0.5287 -- 2.7923)  data: 0.2236 (0.0003 -- 2.2665)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.2139 (2.2721)  loss_scale: 32768.0000 (27487.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9354 (5.2860)  time: 0.8782 (0.5227 -- 4.2069)  data: 0.3220 (0.0004 -- 3.6611)  max mem: 16413
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1705 (2.2544)  loss_scale: 32768.0000 (28236.2553)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0867 (5.2847)  time: 0.8957 (0.5207 -- 2.9517)  data: 0.0293 (0.0002 -- 0.5499)  max mem: 16413
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1549 (2.2443)  loss_scale: 32768.0000 (28774.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4061 (5.3312)  time: 0.7151 (0.4945 -- 4.1707)  data: 0.0006 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [11] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1549 (2.2343)  loss_scale: 32768.0000 (28774.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4061 (5.3312)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.6117 (1.6117)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4559 (2.4559 -- 2.4559)  data: 2.2003 (2.2003 -- 2.2003)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.6064 (1.6401)  acc1: 55.5556 (52.5253)  acc5: 100.0000 (94.9495)  time: 0.4374 (0.1941 -- 2.4559)  data: 0.2180 (0.0006 -- 2.2003)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5743 (1.6136)  acc1: 55.5556 (49.7354)  acc5: 100.0000 (95.7672)  time: 0.2153 (0.1698 -- 0.4501)  data: 0.0101 (0.0001 -- 0.1876)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6380 (1.6647)  acc1: 55.5556 (50.2075)  acc5: 100.0000 (94.6058)  time: 0.2019 (0.1325 -- 0.4501)  data: 0.0098 (0.0001 -- 0.1876)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 53.527 Acc@5 93.776 loss 1.638
Accuracy of the network on the 482 val images: 53.53%
Max accuracy: 54.98%
Epoch: [12]  [  0/160]  eta: 0:19:02  lr: 0.000023  min_lr: 0.000001  loss: 1.6044 (1.6044)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2770 (6.2770)  time: 7.1419 (7.1419 -- 7.1419)  data: 4.6777 (4.6777 -- 4.6777)  max mem: 16413
[2023-08-30 22:02:44,324] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:02:44,324] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:02:44,324] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:02:44,324] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:02:47,689] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1930
[2023-08-30 22:02:47,689] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:02:47,689] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1930
[2023-08-30 22:02:47,689] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:02:47,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [ 20/160]  eta: 0:02:38  lr: 0.000023  min_lr: 0.000001  loss: 2.1101 (2.1136)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3222 (5.3206)  time: 0.8285 (0.5271 -- 2.2207)  data: 0.1809 (0.0004 -- 1.2045)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:01:56  lr: 0.000023  min_lr: 0.000001  loss: 2.1807 (2.1628)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7820 (5.6583)  time: 0.8089 (0.5295 -- 2.6515)  data: 0.0673 (0.0003 -- 0.5692)  max mem: 16413
Epoch: [12]  [ 60/160]  eta: 0:01:40  lr: 0.000023  min_lr: 0.000001  loss: 2.1860 (2.1683)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1452 (5.4206)  time: 1.0747 (0.5246 -- 3.7020)  data: 0.4040 (0.0005 -- 3.1481)  max mem: 16413
[2023-08-30 22:03:48,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[5.547884169782988e-07, 5.547884169782988e-07, 7.397178893043984e-07, 7.397178893043984e-07, 9.862905190725312e-07, 9.862905190725312e-07, 1.3150540254300417e-06, 1.3150540254300417e-06, 1.7534053672400554e-06, 1.7534053672400554e-06, 2.3378738229867407e-06, 2.3378738229867407e-06, 3.1171650973156544e-06, 3.1171650973156544e-06, 4.1562201297542055e-06, 4.1562201297542055e-06, 5.541626839672274e-06, 5.541626839672274e-06, 7.388835786229699e-06, 7.388835786229699e-06, 9.851781048306264e-06, 9.851781048306264e-06, 1.3135708064408354e-05, 1.3135708064408354e-05, 1.7514277419211137e-05, 1.7514277419211137e-05, 2.3352369892281517e-05, 2.3352369892281517e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 22:03:48,049] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=16.662448893778805, CurrSamplesPerSec=21.217036473728495, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.1398 (2.1631)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7787 (5.4296)  time: 0.6948 (0.5184 -- 2.7313)  data: 0.1391 (0.0002 -- 2.2113)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.3491 (2.2021)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2483 (5.5048)  time: 0.9136 (0.5234 -- 2.8838)  data: 0.2888 (0.0007 -- 2.3138)  max mem: 16413
Epoch: [12]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.2585 (2.2161)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4795 (5.5692)  time: 0.8223 (0.5321 -- 2.5594)  data: 0.2083 (0.0003 -- 2.0459)  max mem: 16413
[2023-08-30 22:04:40,071] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:04:40,071] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:04:40,071] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:04:40,071] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [12]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0829 (2.1965)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6183 (5.4677)  time: 0.8641 (0.5331 -- 2.6322)  data: 0.2404 (0.0002 -- 2.0940)  max mem: 16413
[2023-08-30 22:04:43,698] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2063
[2023-08-30 22:04:43,698] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:04:43,698] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2063
[2023-08-30 22:04:43,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:04:43,699] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.2737 (2.2046)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9857 (5.4529)  time: 0.7111 (0.4955 -- 2.3184)  data: 0.0697 (0.0002 -- 0.9530)  max mem: 16413
Epoch: [12] Total time: 0:02:21 (0.8813 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.2737 (2.2062)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9857 (5.4529)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.5521 (1.5521)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4180 (2.4180 -- 2.4180)  data: 2.1794 (2.1794 -- 2.1794)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5521 (1.5831)  acc1: 66.6667 (55.5556)  acc5: 100.0000 (93.9394)  time: 0.4205 (0.2033 -- 2.4180)  data: 0.2048 (0.0009 -- 2.1794)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5151 (1.5639)  acc1: 55.5556 (52.9101)  acc5: 100.0000 (94.7090)  time: 0.2208 (0.1690 -- 0.4717)  data: 0.0180 (0.0001 -- 0.2845)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5839 (1.6151)  acc1: 55.5556 (52.6971)  acc5: 100.0000 (94.1909)  time: 0.2043 (0.1325 -- 0.4717)  data: 0.0177 (0.0001 -- 0.2845)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 56.224 Acc@5 93.361 loss 1.587
Accuracy of the network on the 482 val images: 56.22%
[2023-08-30 22:05:02,134] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:05:02,136] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:05:02,136] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:05:02,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:05:03,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:05:03,537] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 56.22%
Epoch: [13]  [  0/160]  eta: 0:20:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0367 (2.0367)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1518 (5.1518)  time: 7.8472 (7.8472 -- 7.8472)  data: 6.3229 (6.3229 -- 6.3229)  max mem: 16413
Epoch: [13]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 2.2532 (2.2625)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6021 (5.4107)  time: 0.8483 (0.5183 -- 4.2506)  data: 0.0370 (0.0005 -- 0.7105)  max mem: 16413
Epoch: [13]  [ 40/160]  eta: 0:02:04  lr: 0.000023  min_lr: 0.000001  loss: 2.2456 (2.2641)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4156 (5.8473)  time: 0.8823 (0.5236 -- 3.1978)  data: 0.0151 (0.0004 -- 0.2328)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.1710 (2.2448)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1986 (5.7801)  time: 0.8700 (0.5224 -- 4.1087)  data: 0.1097 (0.0004 -- 1.1093)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000001  loss: 2.2743 (2.2473)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9308 (5.9648)  time: 0.9889 (0.5202 -- 3.9992)  data: 0.0015 (0.0004 -- 0.0062)  max mem: 16413
[2023-08-30 22:06:30,398] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2170
[2023-08-30 22:06:30,398] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2170
[2023-08-30 22:06:30,399] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:06:30,399] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:06:30,399] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1592 (2.2288)  loss_scale: 16384.0000 (30983.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2678 (6.0483)  time: 0.6859 (0.5124 -- 2.3862)  data: 0.0015 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1548 (2.2140)  loss_scale: 16384.0000 (28570.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5960 (6.1677)  time: 0.9012 (0.5173 -- 3.2099)  data: 0.0019 (0.0009 -- 0.0036)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1708 (2.1966)  loss_scale: 16384.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2846 (6.0598)  time: 0.8918 (0.5175 -- 3.7011)  data: 0.0023 (0.0004 -- 0.0132)  max mem: 16413
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1543 (2.1834)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3815 (6.1259)  time: 0.7054 (0.4945 -- 3.3301)  data: 0.0006 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [13] Total time: 0:02:22 (0.8907 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1543 (2.1921)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3815 (6.1259)
Val:  [ 0/27]  eta: 0:00:59  loss: 1.4975 (1.4975)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2001 (2.2001 -- 2.2001)  data: 1.9647 (1.9647 -- 1.9647)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.4527 (1.5380)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (96.9697)  time: 0.4138 (0.1971 -- 2.2001)  data: 0.1952 (0.0008 -- 1.9647)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4429 (1.5123)  acc1: 55.5556 (52.9101)  acc5: 100.0000 (96.2963)  time: 0.2269 (0.1703 -- 0.3982)  data: 0.0192 (0.0001 -- 0.1981)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5094 (1.5668)  acc1: 55.5556 (52.6971)  acc5: 100.0000 (95.4357)  time: 0.2078 (0.1331 -- 0.3982)  data: 0.0180 (0.0001 -- 0.1981)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 57.054 Acc@5 94.398 loss 1.534
Accuracy of the network on the 482 val images: 57.05%
[2023-08-30 22:07:33,812] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:07:33,813] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:07:33,813] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:07:33,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:07:35,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:07:35,181] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 57.05%
Epoch: [14]  [  0/160]  eta: 0:23:21  lr: 0.000023  min_lr: 0.000001  loss: 2.0625 (2.0625)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9090 (4.9090)  time: 8.7611 (8.7611 -- 8.7611)  data: 8.2207 (8.2207 -- 8.2207)  max mem: 16413
Epoch: [14]  [ 20/160]  eta: 0:02:46  lr: 0.000023  min_lr: 0.000001  loss: 2.1779 (2.1687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2531 (7.1973)  time: 0.8078 (0.5254 -- 3.4375)  data: 0.1519 (0.0002 -- 2.2339)  max mem: 16413
Epoch: [14]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 2.1833 (2.1489)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7909 (7.0107)  time: 0.9334 (0.5148 -- 4.5107)  data: 0.0074 (0.0001 -- 0.1214)  max mem: 16413
[2023-08-30 22:08:34,118] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:08:34,118] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:08:34,118] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:08:34,118] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.1936 (2.1636)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3381 (6.7215)  time: 0.7940 (0.5374 -- 3.5665)  data: 0.1902 (0.0004 -- 2.3866)  max mem: 16413
Epoch: [14]  [ 80/160]  eta: 0:01:19  lr: 0.000023  min_lr: 0.000001  loss: 2.2370 (2.1717)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2141 (6.6460)  time: 1.0395 (0.5233 -- 4.2133)  data: 0.1409 (0.0003 -- 1.8576)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1999 (2.1723)  loss_scale: 32768.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0331 (6.6738)  time: 0.6659 (0.5159 -- 2.5035)  data: 0.0013 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.9973 (2.1506)  loss_scale: 32768.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1013 (6.5501)  time: 1.0001 (0.5314 -- 3.6539)  data: 0.0023 (0.0003 -- 0.0142)  max mem: 16413
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1382 (2.1542)  loss_scale: 32768.0000 (25912.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7678 (6.5634)  time: 0.8834 (0.5261 -- 4.5088)  data: 0.0014 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0401 (2.1499)  loss_scale: 32768.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3108 (6.5199)  time: 0.5812 (0.4956 -- 1.5431)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [14] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0401 (2.1816)  loss_scale: 32768.0000 (26726.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3108 (6.5199)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.4514 (1.4514)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4728 (2.4728 -- 2.4728)  data: 2.2631 (2.2631 -- 2.2631)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.4299 (1.5047)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (94.9495)  time: 0.4532 (0.2045 -- 2.4728)  data: 0.2414 (0.0006 -- 2.2631)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4117 (1.4761)  acc1: 55.5556 (51.3228)  acc5: 100.0000 (95.7672)  time: 0.2248 (0.1698 -- 0.5780)  data: 0.0220 (0.0001 -- 0.3756)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4593 (1.5358)  acc1: 55.5556 (51.4523)  acc5: 100.0000 (94.6058)  time: 0.2104 (0.1329 -- 0.5780)  data: 0.0214 (0.0001 -- 0.3756)  max mem: 16413
Val: Total time: 0:00:07 (0.2944 s / it)
* Acc@1 55.602 Acc@5 93.568 loss 1.505
Accuracy of the network on the 482 val images: 55.60%
Max accuracy: 57.05%
Epoch: [15]  [  0/160]  eta: 0:17:03  lr: 0.000023  min_lr: 0.000001  loss: 2.0643 (2.0643)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5967 (6.5967)  time: 6.3978 (6.3978 -- 6.3978)  data: 5.8083 (5.8083 -- 5.8083)  max mem: 16413
Epoch: [15]  [ 20/160]  eta: 0:02:43  lr: 0.000023  min_lr: 0.000001  loss: 2.2146 (2.1665)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1975 (6.1874)  time: 0.9065 (0.5221 -- 3.0278)  data: 0.2430 (0.0004 -- 2.4764)  max mem: 16413
[2023-08-30 22:10:35,832] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:10:35,832] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:10:35,832] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:10:35,833] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:10:44,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2437
[2023-08-30 22:10:44,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:10:44,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2437
[2023-08-30 22:10:44,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:10:44,157] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [ 40/160]  eta: 0:01:58  lr: 0.000023  min_lr: 0.000001  loss: 2.1679 (2.1578)  loss_scale: 32768.0000 (40760.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9123 (6.2844)  time: 0.8033 (0.5123 -- 3.4708)  data: 0.2648 (0.0004 -- 2.9366)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1385 (2.1623)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6097 (6.3608)  time: 0.9243 (0.5189 -- 4.0641)  data: 0.3651 (0.0003 -- 3.5216)  max mem: 16413
[2023-08-30 22:11:10,038] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2464
[2023-08-30 22:11:10,038] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2464
[2023-08-30 22:11:10,038] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:11:10,038] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:11:10,038] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.0601 (2.1566)  loss_scale: 16384.0000 (33374.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5698 (6.3704)  time: 0.9418 (0.5197 -- 3.7992)  data: 0.3973 (0.0004 -- 3.2639)  max mem: 16413
Epoch: [15]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.1282 (2.1666)  loss_scale: 16384.0000 (30010.2970)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9681 (6.3430)  time: 0.8311 (0.5288 -- 4.2065)  data: 0.2882 (0.0001 -- 3.6870)  max mem: 16413
Epoch: [15]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1164 (2.1608)  loss_scale: 16384.0000 (27758.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2821 (6.4759)  time: 0.8211 (0.5161 -- 2.3954)  data: 0.2519 (0.0005 -- 1.8539)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.2673 (2.1720)  loss_scale: 16384.0000 (26144.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3933 (6.4672)  time: 0.8286 (0.5289 -- 3.1128)  data: 0.2401 (0.0008 -- 2.5922)  max mem: 16413
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.2505 (2.1869)  loss_scale: 16384.0000 (24985.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3565 (6.5069)  time: 0.7473 (0.4949 -- 2.4242)  data: 0.0889 (0.0002 -- 0.8927)  max mem: 16413
Epoch: [15] Total time: 0:02:21 (0.8874 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.2505 (2.1659)  loss_scale: 16384.0000 (24985.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3565 (6.5069)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.4582 (1.4582)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3578 (2.3578 -- 2.3578)  data: 2.1258 (2.1258 -- 2.1258)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3706 (1.4562)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (95.9596)  time: 0.4510 (0.2064 -- 2.3578)  data: 0.2330 (0.0008 -- 2.1258)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3478 (1.4330)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (95.7672)  time: 0.2254 (0.1691 -- 0.6448)  data: 0.0220 (0.0001 -- 0.4265)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4395 (1.4861)  acc1: 55.5556 (55.1867)  acc5: 100.0000 (95.0207)  time: 0.2095 (0.1371 -- 0.6448)  data: 0.0216 (0.0001 -- 0.4265)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 58.506 Acc@5 94.191 loss 1.457
Accuracy of the network on the 482 val images: 58.51%
[2023-08-30 22:12:35,348] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:12:35,350] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:12:35,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:12:35,350] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:12:36,863] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:12:36,863] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 58.51%
Epoch: [16]  [  0/160]  eta: 0:21:45  lr: 0.000023  min_lr: 0.000001  loss: 2.0773 (2.0773)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4416 (10.4416)  time: 8.1588 (8.1588 -- 8.1588)  data: 7.6035 (7.6035 -- 7.6035)  max mem: 16413
Epoch: [16]  [ 20/160]  eta: 0:02:47  lr: 0.000023  min_lr: 0.000001  loss: 2.0942 (2.1743)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1429 (7.1333)  time: 0.8490 (0.5171 -- 4.0421)  data: 0.2994 (0.0002 -- 3.5002)  max mem: 16413
[2023-08-30 22:13:14,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:13:14,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:13:14,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:13:14,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [ 40/160]  eta: 0:02:10  lr: 0.000023  min_lr: 0.000001  loss: 2.1822 (2.1501)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9135 (6.8298)  time: 0.9658 (0.5229 -- 3.9915)  data: 0.4147 (0.0003 -- 3.4509)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.1577 (2.1500)  loss_scale: 32768.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7633 (6.5862)  time: 0.7487 (0.5177 -- 2.1840)  data: 0.1405 (0.0003 -- 1.6702)  max mem: 16413
[2023-08-30 22:13:44,517] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2632
[2023-08-30 22:13:44,517] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2632
[2023-08-30 22:13:44,517] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:13:44,517] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:13:44,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.1123 (2.1528)  loss_scale: 32768.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1681 (6.7562)  time: 0.8083 (0.5270 -- 2.9953)  data: 0.0095 (0.0006 -- 0.1605)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.1295 (2.1499)  loss_scale: 16384.0000 (22710.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6712 (6.7893)  time: 0.8261 (0.5320 -- 2.1350)  data: 0.0018 (0.0004 -- 0.0049)  max mem: 16413
Epoch: [16]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.2315 (2.1626)  loss_scale: 16384.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3722 (6.7340)  time: 0.8495 (0.5351 -- 3.1800)  data: 0.0360 (0.0003 -- 0.5992)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.2443 (2.1647)  loss_scale: 16384.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0133 (6.6596)  time: 0.9581 (0.5336 -- 3.3062)  data: 0.1284 (0.0008 -- 1.5048)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.2443 (2.1727)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9604 (6.5839)  time: 0.6704 (0.4950 -- 2.9688)  data: 0.0008 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [16] Total time: 0:02:21 (0.8822 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.2443 (2.1699)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9604 (6.5839)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.4102 (1.4102)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4367 (2.4367 -- 2.4367)  data: 2.1717 (2.1717 -- 2.1717)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3616 (1.4371)  acc1: 66.6667 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4373 (0.2021 -- 2.4367)  data: 0.2159 (0.0008 -- 2.1717)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3394 (1.4146)  acc1: 55.5556 (57.6720)  acc5: 100.0000 (95.2381)  time: 0.2179 (0.1700 -- 0.4431)  data: 0.0107 (0.0001 -- 0.1914)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4468 (1.4665)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (94.1909)  time: 0.2026 (0.1327 -- 0.4431)  data: 0.0104 (0.0001 -- 0.1914)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 60.373 Acc@5 93.983 loss 1.431
Accuracy of the network on the 482 val images: 60.37%
[2023-08-30 22:15:05,816] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:15:05,817] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:15:05,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:15:05,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:15:07,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:15:07,212] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 60.37%
Epoch: [17]  [  0/160]  eta: 0:20:18  lr: 0.000023  min_lr: 0.000001  loss: 2.4569 (2.4569)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5281 (5.5281)  time: 7.6137 (7.6137 -- 7.6137)  data: 4.3914 (4.3914 -- 4.3914)  max mem: 16413
[2023-08-30 22:15:27,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2733
[2023-08-30 22:15:27,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2733
[2023-08-30 22:15:27,303] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 22:15:27,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 22:15:27,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [17]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 2.0259 (2.0839)  loss_scale: 16384.0000 (13263.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5769 (6.7887)  time: 0.8614 (0.5219 -- 4.9845)  data: 0.0433 (0.0009 -- 0.8316)  max mem: 16413
Epoch: [17]  [ 40/160]  eta: 0:02:03  lr: 0.000023  min_lr: 0.000001  loss: 2.0503 (2.0854)  loss_scale: 8192.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6125 (6.9171)  time: 0.8652 (0.5322 -- 2.2706)  data: 0.0018 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [17]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.0117 (2.0859)  loss_scale: 8192.0000 (9937.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0637 (6.7617)  time: 0.8944 (0.5282 -- 2.8799)  data: 0.2078 (0.0005 -- 1.3846)  max mem: 16413
Epoch: [17]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.1622 (2.0913)  loss_scale: 8192.0000 (9506.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8015 (6.8264)  time: 0.8953 (0.5249 -- 4.0240)  data: 0.1744 (0.0001 -- 2.5159)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1389 (2.1043)  loss_scale: 8192.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9093 (6.9030)  time: 0.7392 (0.5358 -- 3.3003)  data: 0.0121 (0.0004 -- 0.2015)  max mem: 16413
Epoch: [17]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 2.0390 (2.1013)  loss_scale: 8192.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6601 (6.8249)  time: 1.0163 (0.5186 -- 4.6233)  data: 0.0013 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0187 (2.0851)  loss_scale: 8192.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0317 (6.7409)  time: 0.8148 (0.5295 -- 2.4649)  data: 0.0012 (0.0003 -- 0.0023)  max mem: 16413
[2023-08-30 22:17:17,664] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:17:17,664] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 22:17:17,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:17:17,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0060 (2.0802)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6565 (6.6445)  time: 0.6637 (0.4955 -- 2.3361)  data: 0.0010 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [17] Total time: 0:02:20 (0.8770 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0060 (2.1061)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6565 (6.6445)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.3601 (1.3601)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4521 (2.4521 -- 2.4521)  data: 2.2191 (2.2191 -- 2.2191)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3295 (1.4122)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (94.9495)  time: 0.4513 (0.2060 -- 2.4521)  data: 0.2346 (0.0008 -- 2.2191)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3253 (1.3852)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (95.2381)  time: 0.2219 (0.1694 -- 0.5618)  data: 0.0202 (0.0001 -- 0.3369)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4131 (1.4424)  acc1: 55.5556 (53.1120)  acc5: 100.0000 (94.1909)  time: 0.2069 (0.1335 -- 0.5618)  data: 0.0198 (0.0001 -- 0.3369)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 58.921 Acc@5 93.983 loss 1.410
Accuracy of the network on the 482 val images: 58.92%
Max accuracy: 60.37%
Epoch: [18]  [  0/160]  eta: 0:17:27  lr: 0.000023  min_lr: 0.000001  loss: 2.1878 (2.1878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3112 (6.3112)  time: 6.5468 (6.5468 -- 6.5468)  data: 5.9631 (5.9631 -- 5.9631)  max mem: 16413
Epoch: [18]  [ 20/160]  eta: 0:02:40  lr: 0.000023  min_lr: 0.000001  loss: 2.1171 (2.1374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7285 (7.1843)  time: 0.8746 (0.5331 -- 3.8258)  data: 0.2957 (0.0005 -- 3.3060)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 2.2350 (2.1452)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6347 (6.9973)  time: 0.8758 (0.5302 -- 2.7803)  data: 0.1269 (0.0004 -- 1.8412)  max mem: 16413
Epoch: [18]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.2357 (2.1817)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1815 (7.0375)  time: 0.9090 (0.5147 -- 3.7752)  data: 0.1203 (0.0003 -- 2.2170)  max mem: 16413
Epoch: [18]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.0490 (2.1502)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8379 (7.0838)  time: 0.7799 (0.5195 -- 3.7994)  data: 0.0649 (0.0004 -- 1.1758)  max mem: 16413
Epoch: [18]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1553 (2.1539)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1606 (6.9725)  time: 0.9171 (0.5264 -- 4.1148)  data: 0.1295 (0.0004 -- 2.0996)  max mem: 16413
[2023-08-30 22:19:17,572] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:19:17,572] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:19:17,573] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:19:17,573] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:19:22,936] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2996
[2023-08-30 22:19:22,936] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2996
[2023-08-30 22:19:22,936] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:19:22,936] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:19:22,936] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 22:19:24,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[5.500275261384e-07, 5.500275261384e-07, 7.333700348512001e-07, 7.333700348512001e-07, 9.778267131349334e-07, 9.778267131349334e-07, 1.303768950846578e-06, 1.303768950846578e-06, 1.7383586011287706e-06, 1.7383586011287706e-06, 2.317811468171694e-06, 2.317811468171694e-06, 3.0904152908955924e-06, 3.0904152908955924e-06, 4.120553721194123e-06, 4.120553721194123e-06, 5.494071628258831e-06, 5.494071628258831e-06, 7.325428837678441e-06, 7.325428837678441e-06, 9.767238450237921e-06, 9.767238450237921e-06, 1.3022984600317229e-05, 1.3022984600317229e-05, 1.736397946708964e-05, 1.736397946708964e-05, 2.3151972622786183e-05, 2.3151972622786183e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 22:19:24,547] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=16.695318481406318, CurrSamplesPerSec=23.266476799695276, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.2508 (2.1679)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3448 (6.9807)  time: 0.7964 (0.5295 -- 3.3856)  data: 0.0014 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [18]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.0163 (2.1637)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6798 (6.8925)  time: 0.8396 (0.5414 -- 2.8686)  data: 0.0305 (0.0003 -- 0.5765)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0274 (2.1505)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1562 (6.7924)  time: 0.7100 (0.4938 -- 1.6578)  data: 0.1323 (0.0002 -- 1.0551)  max mem: 16413
Epoch: [18] Total time: 0:02:20 (0.8755 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0274 (2.1300)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1562 (6.7924)
Val:  [ 0/27]  eta: 0:01:10  loss: 1.3159 (1.3159)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.6102 (2.6102 -- 2.6102)  data: 2.3727 (2.3727 -- 2.3727)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3095 (1.3776)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (94.9495)  time: 0.4504 (0.2039 -- 2.6102)  data: 0.2350 (0.0006 -- 2.3727)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2945 (1.3594)  acc1: 55.5556 (55.0265)  acc5: 100.0000 (95.2381)  time: 0.2163 (0.1708 -- 0.3893)  data: 0.0134 (0.0001 -- 0.1887)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3736 (1.4145)  acc1: 55.5556 (54.7718)  acc5: 100.0000 (94.6058)  time: 0.2001 (0.1326 -- 0.3893)  data: 0.0129 (0.0001 -- 0.1887)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 59.544 Acc@5 93.776 loss 1.377
Accuracy of the network on the 482 val images: 59.54%
Max accuracy: 60.37%
Epoch: [19]  [  0/160]  eta: 0:17:13  lr: 0.000023  min_lr: 0.000001  loss: 1.8712 (1.8712)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8559 (4.8559)  time: 6.4574 (6.4574 -- 6.4574)  data: 5.9217 (5.9217 -- 5.9217)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:41  lr: 0.000023  min_lr: 0.000001  loss: 1.9704 (2.0018)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0514 (6.4728)  time: 0.8867 (0.5387 -- 3.3363)  data: 0.1724 (0.0009 -- 1.8638)  max mem: 16413
Epoch: [19]  [ 40/160]  eta: 0:01:57  lr: 0.000023  min_lr: 0.000001  loss: 2.0843 (2.0608)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9809 (7.0880)  time: 0.8008 (0.5243 -- 4.0666)  data: 0.0624 (0.0004 -- 0.7705)  max mem: 16413
Epoch: [19]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.1829 (2.1137)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8760 (6.8219)  time: 1.0017 (0.5313 -- 4.9818)  data: 0.3247 (0.0005 -- 4.4176)  max mem: 16413
Epoch: [19]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 1.9844 (2.0877)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3262 (7.0035)  time: 0.8675 (0.5325 -- 2.8147)  data: 0.3181 (0.0003 -- 2.2596)  max mem: 16413
[2023-08-30 22:21:23,860] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:21:23,860] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:21:23,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:21:23,902] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 1.9472 (2.0807)  loss_scale: 32768.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1462 (7.0199)  time: 0.8536 (0.5312 -- 3.8643)  data: 0.2959 (0.0005 -- 3.3026)  max mem: 16413
Epoch: [19]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1638 (2.0902)  loss_scale: 32768.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7435 (6.9613)  time: 0.8123 (0.5239 -- 2.9315)  data: 0.2606 (0.0003 -- 2.3916)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1584 (2.1001)  loss_scale: 32768.0000 (22891.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4660 (6.9718)  time: 0.8915 (0.5311 -- 3.8633)  data: 0.1061 (0.0004 -- 1.7161)  max mem: 16413
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1424 (2.0992)  loss_scale: 32768.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5821 (6.9517)  time: 0.7677 (0.4969 -- 3.8633)  data: 0.0010 (0.0002 -- 0.0067)  max mem: 16413
Epoch: [19] Total time: 0:02:20 (0.8766 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1424 (2.1132)  loss_scale: 32768.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5821 (6.9517)
[2023-08-30 22:22:23,739] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-08-30 22:22:23,741] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-08-30 22:22:23,741] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-08-30 22:22:23,741] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-08-30 22:22:24,710] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-08-30 22:22:24,710] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:06  loss: 1.3095 (1.3095)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4578 (2.4578 -- 2.4578)  data: 2.2234 (2.2234 -- 2.2234)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3095 (1.3663)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (94.9495)  time: 0.4403 (0.1951 -- 2.4578)  data: 0.2240 (0.0009 -- 2.2234)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2568 (1.3365)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (95.7672)  time: 0.2170 (0.1697 -- 0.4296)  data: 0.0122 (0.0001 -- 0.2225)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3934 (1.4021)  acc1: 55.5556 (52.6971)  acc5: 100.0000 (94.6058)  time: 0.2003 (0.1326 -- 0.4296)  data: 0.0116 (0.0001 -- 0.2225)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 58.921 Acc@5 93.776 loss 1.366
Accuracy of the network on the 482 val images: 58.92%
Max accuracy: 60.37%
Epoch: [20]  [  0/160]  eta: 0:17:28  lr: 0.000023  min_lr: 0.000001  loss: 2.3651 (2.3651)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0938 (4.0938)  time: 6.5524 (6.5524 -- 6.5524)  data: 4.6720 (4.6720 -- 4.6720)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0271 (2.0772)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4913 (6.7605)  time: 0.8456 (0.5187 -- 3.1576)  data: 0.0148 (0.0006 -- 0.2047)  max mem: 16413
Epoch: [20]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 2.1671 (2.1255)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2035 (6.8636)  time: 0.9061 (0.5228 -- 4.2000)  data: 0.0128 (0.0003 -- 0.1670)  max mem: 16413
[2023-08-30 22:23:26,080] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:23:26,080] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:23:26,080] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:23:26,080] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:23:27,709] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3256
[2023-08-30 22:23:27,709] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3256
[2023-08-30 22:23:27,709] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:23:27,709] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:23:27,710] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.2082 (2.1275)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4339 (6.7857)  time: 0.9284 (0.5139 -- 2.7365)  data: 0.1462 (0.0002 -- 2.2125)  max mem: 16413
Epoch: [20]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.1114 (2.1155)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6836 (6.6639)  time: 0.8046 (0.5275 -- 2.3400)  data: 0.0151 (0.0002 -- 0.2686)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.0807 (2.1081)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9617 (6.6445)  time: 0.9433 (0.5323 -- 3.4255)  data: 0.0016 (0.0004 -- 0.0037)  max mem: 16413
[2023-08-30 22:24:15,076] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3309
[2023-08-30 22:24:15,076] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3309
[2023-08-30 22:24:15,076] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:24:15,076] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:24:15,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1435 (2.1051)  loss_scale: 16384.0000 (31955.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0333 (6.6439)  time: 0.7918 (0.5307 -- 2.8315)  data: 0.0012 (0.0001 -- 0.0028)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0612 (2.1022)  loss_scale: 16384.0000 (29746.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4372 (6.7012)  time: 0.8445 (0.5359 -- 5.0299)  data: 0.0015 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1953 (2.1120)  loss_scale: 16384.0000 (28160.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0738 (6.7118)  time: 0.6593 (0.4976 -- 1.4652)  data: 0.0010 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [20] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1953 (2.1095)  loss_scale: 16384.0000 (28160.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0738 (6.7118)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.3011 (1.3011)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2925 (2.2925 -- 2.2925)  data: 2.0674 (2.0674 -- 2.0674)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2425 (1.3272)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (95.9596)  time: 0.4144 (0.1967 -- 2.2925)  data: 0.2037 (0.0007 -- 2.0674)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2193 (1.3054)  acc1: 55.5556 (56.6138)  acc5: 100.0000 (96.2963)  time: 0.2222 (0.1702 -- 0.4257)  data: 0.0218 (0.0001 -- 0.2429)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3263 (1.3564)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (95.0207)  time: 0.2069 (0.1325 -- 0.4257)  data: 0.0215 (0.0001 -- 0.2429)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 61.203 Acc@5 93.983 loss 1.323
Accuracy of the network on the 482 val images: 61.20%
[2023-08-30 22:25:00,813] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:25:00,815] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:25:00,815] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:25:00,815] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:25:02,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:25:02,125] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 61.20%
Epoch: [21]  [  0/160]  eta: 0:23:11  lr: 0.000023  min_lr: 0.000001  loss: 2.1639 (2.1639)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2090 (8.2090)  time: 8.6985 (8.6985 -- 8.6985)  data: 8.0565 (8.0565 -- 8.0565)  max mem: 16413
Epoch: [21]  [ 20/160]  eta: 0:02:48  lr: 0.000023  min_lr: 0.000001  loss: 2.1266 (2.1355)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5526 (7.0102)  time: 0.8283 (0.5160 -- 3.4813)  data: 0.1970 (0.0003 -- 2.2023)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 2.0928 (2.1032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9306 (6.6668)  time: 0.9068 (0.5247 -- 3.7539)  data: 0.3353 (0.0003 -- 3.2279)  max mem: 16413
Epoch: [21]  [ 60/160]  eta: 0:01:34  lr: 0.000023  min_lr: 0.000001  loss: 2.2485 (2.1194)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0189 (6.7975)  time: 0.7227 (0.5160 -- 2.5443)  data: 0.1225 (0.0002 -- 2.0139)  max mem: 16413
[2023-08-30 22:26:15,662] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:26:15,662] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:26:15,663] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:26:15,664] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 1.9928 (2.0950)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6140 (6.8086)  time: 0.9710 (0.5085 -- 5.1647)  data: 0.1805 (0.0003 -- 2.6525)  max mem: 16413
Epoch: [21]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.1212 (2.0940)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3262 (6.8581)  time: 0.7499 (0.5333 -- 2.8302)  data: 0.1899 (0.0002 -- 2.2799)  max mem: 16413
[2023-08-30 22:26:39,020] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3464
[2023-08-30 22:26:39,020] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3464
[2023-08-30 22:26:39,020] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:26:39,020] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:26:39,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0340 (2.0896)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6357 (6.8690)  time: 0.9526 (0.5294 -- 3.9874)  data: 0.3937 (0.0004 -- 3.4292)  max mem: 16413
Epoch: [21]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 1.9945 (2.0817)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8929 (7.0777)  time: 0.7369 (0.5264 -- 2.2290)  data: 0.0996 (0.0004 -- 0.9743)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0311 (2.0837)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7847 (7.1037)  time: 0.7091 (0.4953 -- 3.0106)  data: 0.1245 (0.0002 -- 2.4712)  max mem: 16413
Epoch: [21] Total time: 0:02:19 (0.8731 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0311 (2.0829)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7847 (7.1037)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.2617 (1.2617)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5091 (2.5091 -- 2.5091)  data: 2.2763 (2.2763 -- 2.2763)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2243 (1.2909)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (95.9596)  time: 0.4358 (0.1924 -- 2.5091)  data: 0.2269 (0.0006 -- 2.2763)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1702 (1.2722)  acc1: 55.5556 (56.6138)  acc5: 100.0000 (96.2963)  time: 0.2153 (0.1693 -- 0.4316)  data: 0.0152 (0.0001 -- 0.2096)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2762 (1.3272)  acc1: 55.5556 (56.4315)  acc5: 100.0000 (95.4357)  time: 0.2023 (0.1324 -- 0.4316)  data: 0.0149 (0.0001 -- 0.2096)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 60.581 Acc@5 94.398 loss 1.296
Accuracy of the network on the 482 val images: 60.58%
Max accuracy: 61.20%
Epoch: [22]  [  0/160]  eta: 0:21:37  lr: 0.000023  min_lr: 0.000001  loss: 1.9684 (1.9684)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8739 (4.8739)  time: 8.1102 (8.1102 -- 8.1102)  data: 7.5921 (7.5921 -- 7.5921)  max mem: 16413
Epoch: [22]  [ 20/160]  eta: 0:02:54  lr: 0.000023  min_lr: 0.000001  loss: 2.1157 (2.0551)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1804 (7.8836)  time: 0.9023 (0.5208 -- 3.5298)  data: 0.2611 (0.0002 -- 3.0106)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:04  lr: 0.000023  min_lr: 0.000001  loss: 2.1770 (2.1184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5390 (8.0160)  time: 0.8150 (0.5229 -- 3.1290)  data: 0.0016 (0.0003 -- 0.0052)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.2842 (2.1625)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9754 (7.7588)  time: 0.8938 (0.5217 -- 4.3774)  data: 0.0153 (0.0002 -- 0.2863)  max mem: 16413
[2023-08-30 22:28:41,829] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:28:41,829] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:28:41,831] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:28:41,832] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.1052 (2.1451)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6393 (7.7073)  time: 0.8116 (0.5199 -- 2.1641)  data: 0.0653 (0.0003 -- 0.6518)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1667 (2.1338)  loss_scale: 32768.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5642 (7.5044)  time: 0.8032 (0.5330 -- 2.4388)  data: 0.0749 (0.0002 -- 0.8548)  max mem: 16413
Epoch: [22]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 2.1260 (2.1256)  loss_scale: 32768.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8166 (7.3344)  time: 1.0488 (0.5322 -- 3.7671)  data: 0.0205 (0.0004 -- 0.3833)  max mem: 16413
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0856 (2.1254)  loss_scale: 32768.0000 (24285.5035)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9580 (7.2479)  time: 0.8045 (0.5199 -- 3.4446)  data: 0.0021 (0.0002 -- 0.0161)  max mem: 16413
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0729 (2.1216)  loss_scale: 32768.0000 (25292.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2628 (7.1849)  time: 0.6419 (0.4942 -- 2.0491)  data: 0.0011 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [22] Total time: 0:02:22 (0.8875 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0729 (2.0971)  loss_scale: 32768.0000 (25292.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2628 (7.1849)
Val:  [ 0/27]  eta: 0:00:59  loss: 1.2835 (1.2835)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2019 (2.2019 -- 2.2019)  data: 1.9773 (1.9773 -- 1.9773)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.1923 (1.2854)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (96.9697)  time: 0.4092 (0.2031 -- 2.2019)  data: 0.1955 (0.0006 -- 1.9773)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1809 (1.2583)  acc1: 55.5556 (57.1429)  acc5: 100.0000 (96.8254)  time: 0.2254 (0.1734 -- 0.3808)  data: 0.0183 (0.0001 -- 0.1585)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2420 (1.3097)  acc1: 55.5556 (57.2614)  acc5: 100.0000 (95.8506)  time: 0.2104 (0.1337 -- 0.3808)  data: 0.0179 (0.0001 -- 0.1585)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 61.203 Acc@5 94.606 loss 1.277
Accuracy of the network on the 482 val images: 61.20%
[2023-08-30 22:29:59,417] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:29:59,419] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:29:59,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:29:59,419] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:30:00,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:30:00,953] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 61.20%
Epoch: [23]  [  0/160]  eta: 0:24:13  lr: 0.000023  min_lr: 0.000001  loss: 2.3056 (2.3056)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4184 (7.4184)  time: 9.0856 (9.0856 -- 9.0856)  data: 8.5144 (8.5144 -- 8.5144)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:46  lr: 0.000023  min_lr: 0.000001  loss: 2.1260 (2.0924)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4368 (6.9218)  time: 0.7971 (0.5175 -- 4.1990)  data: 0.2566 (0.0003 -- 3.6702)  max mem: 16413
Epoch: [23]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 2.0276 (2.0675)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7441 (7.0758)  time: 0.8258 (0.5399 -- 2.7046)  data: 0.2737 (0.0004 -- 2.1544)  max mem: 16413
[2023-08-30 22:30:43,090] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:30:43,090] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:30:43,091] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:30:43,091] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 22:30:47,850] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3724
[2023-08-30 22:30:47,850] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3724
[2023-08-30 22:30:47,850] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:30:47,850] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 22:30:47,850] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [ 60/160]  eta: 0:01:42  lr: 0.000023  min_lr: 0.000001  loss: 2.2739 (2.1290)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6390 (7.1162)  time: 1.0423 (0.5205 -- 4.5996)  data: 0.4915 (0.0004 -- 4.0782)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.2045 (2.1305)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0577 (6.8470)  time: 0.7500 (0.5189 -- 3.2543)  data: 0.2117 (0.0002 -- 2.7266)  max mem: 16413
[2023-08-30 22:31:29,445] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3771
[2023-08-30 22:31:29,445] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:31:29,445] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3771
[2023-08-30 22:31:29,445] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 22:31:29,445] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [23]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.1328 (2.1215)  loss_scale: 16384.0000 (32119.1287)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7573 (6.9448)  time: 0.9213 (0.5102 -- 3.4103)  data: 0.3725 (0.0003 -- 2.8673)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0851 (2.1211)  loss_scale: 16384.0000 (29518.2810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5901 (7.0054)  time: 0.7046 (0.5278 -- 2.4047)  data: 0.1466 (0.0001 -- 1.8780)  max mem: 16413
Epoch: [23]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.0636 (2.1169)  loss_scale: 16384.0000 (27655.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5469 (7.2464)  time: 0.8494 (0.5339 -- 2.2058)  data: 0.2480 (0.0003 -- 1.6561)  max mem: 16413
[2023-08-30 22:32:11,305] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3825
[2023-08-30 22:32:11,305] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3825
[2023-08-30 22:32:11,306] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 22:32:11,306] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 22:32:11,306] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0299 (2.1142)  loss_scale: 8192.0000 (25548.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0204 (7.3843)  time: 0.7456 (0.4964 -- 2.9919)  data: 0.1767 (0.0001 -- 2.4529)  max mem: 16413
Epoch: [23] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0299 (2.1153)  loss_scale: 8192.0000 (25548.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0204 (7.3843)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.2482 (1.2482)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5092 (2.5092 -- 2.5092)  data: 2.2977 (2.2977 -- 2.2977)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1976 (1.2836)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (96.9697)  time: 0.4319 (0.1935 -- 2.5092)  data: 0.2185 (0.0006 -- 2.2977)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1825 (1.2434)  acc1: 55.5556 (57.6720)  acc5: 100.0000 (96.8254)  time: 0.2161 (0.1689 -- 0.3268)  data: 0.0101 (0.0001 -- 0.0971)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2523 (1.2974)  acc1: 55.5556 (57.2614)  acc5: 100.0000 (95.8506)  time: 0.2020 (0.1329 -- 0.3268)  data: 0.0098 (0.0001 -- 0.0971)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 62.241 Acc@5 94.813 loss 1.271
Accuracy of the network on the 482 val images: 62.24%
[2023-08-30 22:32:29,939] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:32:29,941] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:32:29,941] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:32:29,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:32:31,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:32:31,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.24%
Epoch: [24]  [  0/160]  eta: 0:18:50  lr: 0.000023  min_lr: 0.000001  loss: 1.7211 (1.7211)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0725 (10.0725)  time: 7.0630 (7.0630 -- 7.0630)  data: 5.8549 (5.8549 -- 5.8549)  max mem: 16413
Epoch: [24]  [ 20/160]  eta: 0:02:35  lr: 0.000023  min_lr: 0.000001  loss: 2.1210 (2.0332)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8304 (8.4522)  time: 0.8118 (0.5199 -- 2.9940)  data: 0.1385 (0.0003 -- 1.4256)  max mem: 16413
Epoch: [24]  [ 40/160]  eta: 0:01:58  lr: 0.000023  min_lr: 0.000001  loss: 1.9773 (2.0145)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4644 (7.8003)  time: 0.8542 (0.5251 -- 2.8943)  data: 0.1638 (0.0007 -- 2.1964)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:41  lr: 0.000023  min_lr: 0.000001  loss: 1.9364 (2.0109)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5666 (7.5320)  time: 1.0751 (0.5212 -- 3.7406)  data: 0.2729 (0.0002 -- 3.2203)  max mem: 16413
Epoch: [24]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1637 (2.0387)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7047 (7.6937)  time: 0.8598 (0.5076 -- 3.8990)  data: 0.3236 (0.0003 -- 3.3733)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:57  lr: 0.000023  min_lr: 0.000001  loss: 1.8562 (2.0038)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8951 (7.7003)  time: 0.8655 (0.5386 -- 4.1687)  data: 0.3176 (0.0005 -- 3.6457)  max mem: 16413
[2023-08-30 22:34:17,062] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:34:17,062] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 22:34:17,062] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:34:17,062] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [24]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9461 (1.9996)  loss_scale: 8192.0000 (8665.9174)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1887 (7.6920)  time: 0.7440 (0.5265 -- 2.7371)  data: 0.1986 (0.0004 -- 2.2117)  max mem: 16413
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1159 (2.0174)  loss_scale: 16384.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7995 (7.7233)  time: 0.8612 (0.5366 -- 2.6689)  data: 0.3001 (0.0004 -- 2.1483)  max mem: 16413
[2023-08-30 22:34:52,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[5.425206546193266e-07, 5.425206546193266e-07, 7.233608728257689e-07, 7.233608728257689e-07, 9.644811637676918e-07, 9.644811637676918e-07, 1.2859748850235891e-06, 1.2859748850235891e-06, 1.7146331800314522e-06, 1.7146331800314522e-06, 2.2861775733752696e-06, 2.2861775733752696e-06, 3.0482367645003596e-06, 3.0482367645003596e-06, 4.064315686000479e-06, 4.064315686000479e-06, 5.4190875813339725e-06, 5.4190875813339725e-06, 7.225450108445297e-06, 7.225450108445297e-06, 9.633933477927063e-06, 9.633933477927063e-06, 1.2845244637236082e-05, 1.2845244637236082e-05, 1.712699284964811e-05, 1.712699284964811e-05, 2.283599046619748e-05, 2.283599046619748e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 22:34:52,387] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=17.029937263116956, CurrSamplesPerSec=24.701085428210927, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0671 (2.0229)  loss_scale: 16384.0000 (10547.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6950 (7.6048)  time: 0.7409 (0.4949 -- 2.2665)  data: 0.2100 (0.0003 -- 1.6942)  max mem: 16413
Epoch: [24] Total time: 0:02:21 (0.8819 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0671 (2.0368)  loss_scale: 16384.0000 (10547.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6950 (7.6048)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.2341 (1.2341)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4750 (2.4750 -- 2.4750)  data: 2.2542 (2.2542 -- 2.2542)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1812 (1.2271)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (95.9596)  time: 0.4420 (0.2144 -- 2.4750)  data: 0.2202 (0.0004 -- 2.2542)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1231 (1.2059)  acc1: 55.5556 (57.6720)  acc5: 100.0000 (96.2963)  time: 0.2247 (0.1692 -- 0.3811)  data: 0.0144 (0.0001 -- 0.1583)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2017 (1.2598)  acc1: 55.5556 (58.0913)  acc5: 100.0000 (95.0207)  time: 0.2078 (0.1331 -- 0.3811)  data: 0.0141 (0.0001 -- 0.1583)  max mem: 16413
Val: Total time: 0:00:07 (0.2949 s / it)
* Acc@1 61.618 Acc@5 94.606 loss 1.231
Accuracy of the network on the 482 val images: 61.62%
Max accuracy: 62.24%
Epoch: [25]  [  0/160]  eta: 0:19:32  lr: 0.000023  min_lr: 0.000001  loss: 2.2155 (2.2155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0490 (8.0490)  time: 7.3252 (7.3252 -- 7.3252)  data: 6.7934 (6.7934 -- 6.7934)  max mem: 16413
Epoch: [25]  [ 20/160]  eta: 0:03:01  lr: 0.000023  min_lr: 0.000001  loss: 2.0462 (2.1516)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6331 (7.6990)  time: 0.9951 (0.5075 -- 3.5645)  data: 0.2440 (0.0003 -- 3.0316)  max mem: 16413
Epoch: [25]  [ 40/160]  eta: 0:02:05  lr: 0.000023  min_lr: 0.000001  loss: 2.0169 (2.0922)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4206 (7.4292)  time: 0.7809 (0.5227 -- 2.8587)  data: 0.0017 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.1816 (2.1253)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9554 (7.4560)  time: 0.8658 (0.5355 -- 2.9118)  data: 0.0018 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [25]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 1.9969 (2.1117)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4178 (7.3695)  time: 0.8064 (0.5166 -- 3.6572)  data: 0.0018 (0.0002 -- 0.0045)  max mem: 16413
[2023-08-30 22:36:17,806] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:36:17,806] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:36:17,806] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:36:17,807] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.9905 (2.0959)  loss_scale: 32768.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4580 (7.2793)  time: 0.8687 (0.5354 -- 3.5239)  data: 0.0018 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9826 (2.0777)  loss_scale: 32768.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8119 (7.1633)  time: 0.8464 (0.5260 -- 2.2839)  data: 0.0254 (0.0003 -- 0.4799)  max mem: 16413
[2023-08-30 22:36:56,984] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4125
[2023-08-30 22:36:56,984] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:36:56,984] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4125
[2023-08-30 22:36:56,984] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:36:56,985] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0767 (2.0802)  loss_scale: 16384.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4149 (7.2342)  time: 0.8360 (0.5271 -- 3.8184)  data: 0.0017 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1535 (2.0775)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7260 (7.3106)  time: 0.7182 (0.4952 -- 3.3974)  data: 0.0007 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [25] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1535 (2.0812)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7260 (7.3106)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.2134 (1.2134)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5581 (2.5581 -- 2.5581)  data: 2.2946 (2.2946 -- 2.2946)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1420 (1.2300)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.4335 (0.2053 -- 2.5581)  data: 0.2137 (0.0005 -- 2.2946)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1397 (1.2064)  acc1: 55.5556 (58.7302)  acc5: 100.0000 (96.2963)  time: 0.2186 (0.1696 -- 0.3607)  data: 0.0112 (0.0001 -- 0.1651)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2030 (1.2616)  acc1: 55.5556 (59.3361)  acc5: 100.0000 (95.0207)  time: 0.2027 (0.1336 -- 0.3607)  data: 0.0109 (0.0001 -- 0.1651)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 62.448 Acc@5 94.606 loss 1.230
Accuracy of the network on the 482 val images: 62.45%
[2023-08-30 22:37:29,487] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:37:29,488] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:37:29,489] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:37:29,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:37:30,934] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:37:30,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.45%
Epoch: [26]  [  0/160]  eta: 0:22:00  lr: 0.000023  min_lr: 0.000001  loss: 1.4884 (1.4884)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5550 (4.5550)  time: 8.2536 (8.2536 -- 8.2536)  data: 5.8397 (5.8397 -- 5.8397)  max mem: 16413
Epoch: [26]  [ 20/160]  eta: 0:02:39  lr: 0.000023  min_lr: 0.000001  loss: 2.2360 (2.1769)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9066 (6.9900)  time: 0.7839 (0.5226 -- 2.5563)  data: 0.0520 (0.0003 -- 0.5485)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:09  lr: 0.000023  min_lr: 0.000001  loss: 2.0936 (2.1339)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4003 (6.8847)  time: 1.0175 (0.5301 -- 3.1417)  data: 0.0568 (0.0006 -- 1.1036)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.1765 (2.1325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8961 (6.7317)  time: 0.7996 (0.5199 -- 3.0531)  data: 0.0023 (0.0005 -- 0.0159)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0955 (2.1152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0466 (6.8690)  time: 0.9460 (0.5138 -- 3.7684)  data: 0.0016 (0.0003 -- 0.0078)  max mem: 16413
[2023-08-30 22:39:00,635] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:39:00,635] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:39:00,635] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:39:00,635] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [26]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.1832 (2.1116)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1672 (6.8298)  time: 0.7890 (0.5259 -- 3.2903)  data: 0.0018 (0.0003 -- 0.0052)  max mem: 16413
Epoch: [26]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 2.1782 (2.1117)  loss_scale: 32768.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6625 (6.8771)  time: 0.9809 (0.5307 -- 3.6525)  data: 0.0013 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0523 (2.1025)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5182 (6.9257)  time: 0.6776 (0.5268 -- 2.3063)  data: 0.0019 (0.0002 -- 0.0054)  max mem: 16413
[2023-08-30 22:39:40,768] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4303
[2023-08-30 22:39:40,768] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4303
[2023-08-30 22:39:40,768] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:39:40,768] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:39:40,768] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 22:39:48,003] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4311
[2023-08-30 22:39:48,003] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 22:39:48,003] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4311
[2023-08-30 22:39:48,003] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 22:39:48,004] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1697 (2.1034)  loss_scale: 16384.0000 (20940.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7612 (6.9164)  time: 0.7251 (0.4952 -- 3.4432)  data: 0.0009 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [26] Total time: 0:02:22 (0.8884 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1697 (2.0827)  loss_scale: 16384.0000 (20940.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7612 (6.9164)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.2180 (1.2180)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3273 (2.3273 -- 2.3273)  data: 2.0794 (2.0794 -- 2.0794)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1664 (1.2156)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (95.9596)  time: 0.4494 (0.2010 -- 2.3273)  data: 0.2294 (0.0010 -- 2.0794)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0674 (1.1863)  acc1: 55.5556 (59.2593)  acc5: 100.0000 (95.7672)  time: 0.2278 (0.1687 -- 0.6629)  data: 0.0226 (0.0001 -- 0.4325)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1664 (1.2354)  acc1: 55.5556 (59.3361)  acc5: 100.0000 (94.1909)  time: 0.2115 (0.1333 -- 0.6629)  data: 0.0223 (0.0001 -- 0.4325)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 61.411 Acc@5 93.983 loss 1.207
Accuracy of the network on the 482 val images: 61.41%
Max accuracy: 62.45%
Epoch: [27]  [  0/160]  eta: 0:21:50  lr: 0.000023  min_lr: 0.000001  loss: 2.1007 (2.1007)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5889 (3.5889)  time: 8.1918 (8.1918 -- 8.1918)  data: 4.8478 (4.8478 -- 4.8478)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:49  lr: 0.000023  min_lr: 0.000001  loss: 2.0047 (2.0316)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2284 (7.3287)  time: 0.8635 (0.5280 -- 3.7541)  data: 0.0410 (0.0004 -- 0.4524)  max mem: 16413
Epoch: [27]  [ 40/160]  eta: 0:02:05  lr: 0.000023  min_lr: 0.000001  loss: 1.9920 (2.0247)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4376 (6.9196)  time: 0.8794 (0.5293 -- 2.8781)  data: 0.1393 (0.0004 -- 2.3364)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.1024 (2.0291)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4615 (7.1924)  time: 0.8657 (0.5281 -- 4.0504)  data: 0.0304 (0.0003 -- 0.5726)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000001  loss: 2.0329 (2.0354)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0419 (7.0733)  time: 0.9053 (0.5228 -- 3.6569)  data: 0.0015 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [27]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1813 (2.0572)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2867 (7.2207)  time: 0.7297 (0.5307 -- 2.2412)  data: 0.0778 (0.0003 -- 1.3777)  max mem: 16413
[2023-08-30 22:41:50,439] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:41:50,439] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 22:41:50,441] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:41:50,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [27]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0109 (2.0591)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1573 (7.4317)  time: 0.8190 (0.5409 -- 2.0417)  data: 0.1976 (0.0005 -- 1.5239)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1079 (2.0524)  loss_scale: 16384.0000 (9412.0851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5070 (7.4341)  time: 0.9185 (0.5344 -- 2.6173)  data: 0.2517 (0.0004 -- 2.0857)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1492 (2.0590)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9862 (7.5270)  time: 0.6913 (0.4949 -- 3.0690)  data: 0.1284 (0.0002 -- 2.5524)  max mem: 16413
Epoch: [27] Total time: 0:02:21 (0.8822 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1492 (2.0702)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9862 (7.5270)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.1609 (1.1609)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5057 (2.5057 -- 2.5057)  data: 2.2521 (2.2521 -- 2.2521)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1090 (1.2224)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.4403 (0.1897 -- 2.5057)  data: 0.2257 (0.0003 -- 2.2521)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0883 (1.1916)  acc1: 55.5556 (59.2593)  acc5: 100.0000 (95.7672)  time: 0.2247 (0.1688 -- 0.4629)  data: 0.0232 (0.0001 -- 0.2291)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1987 (1.2422)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (94.6058)  time: 0.2119 (0.1329 -- 0.4629)  data: 0.0230 (0.0001 -- 0.2291)  max mem: 16413
Val: Total time: 0:00:07 (0.2958 s / it)
* Acc@1 62.033 Acc@5 94.398 loss 1.206
Accuracy of the network on the 482 val images: 62.03%
Max accuracy: 62.45%
Epoch: [28]  [  0/160]  eta: 0:17:26  lr: 0.000023  min_lr: 0.000001  loss: 2.0969 (2.0969)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3851 (11.3851)  time: 6.5387 (6.5387 -- 6.5387)  data: 5.3360 (5.3360 -- 5.3360)  max mem: 16413
Epoch: [28]  [ 20/160]  eta: 0:02:38  lr: 0.000023  min_lr: 0.000001  loss: 1.8690 (1.9349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1491 (7.8242)  time: 0.8621 (0.5329 -- 2.8019)  data: 0.1459 (0.0003 -- 0.9762)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:02:06  lr: 0.000023  min_lr: 0.000001  loss: 2.1742 (2.0094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6004 (7.8607)  time: 0.9787 (0.5376 -- 3.5973)  data: 0.0868 (0.0003 -- 1.2989)  max mem: 16413
Epoch: [28]  [ 60/160]  eta: 0:01:34  lr: 0.000023  min_lr: 0.000001  loss: 1.9207 (1.9866)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3779 (7.6343)  time: 0.7194 (0.5274 -- 3.9467)  data: 0.0015 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [28]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.0789 (2.0040)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5018 (7.6995)  time: 0.9645 (0.5277 -- 3.5704)  data: 0.3041 (0.0005 -- 2.5077)  max mem: 16413
[2023-08-30 22:43:55,691] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:43:55,691] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:43:55,692] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:43:55,692] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:44:02,506] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4578
[2023-08-30 22:44:02,506] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4578
[2023-08-30 22:44:02,507] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:44:02,507] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:44:02,507] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1044 (2.0287)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2110 (7.6172)  time: 0.8225 (0.5075 -- 4.6326)  data: 0.0015 (0.0005 -- 0.0036)  max mem: 16413
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.8923 (2.0226)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1268 (7.6413)  time: 0.8564 (0.5210 -- 2.9748)  data: 0.0655 (0.0004 -- 1.2770)  max mem: 16413
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0712 (2.0233)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4315 (7.7647)  time: 0.8172 (0.5290 -- 3.4637)  data: 0.0171 (0.0004 -- 0.3161)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0095 (2.0206)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5185 (7.7451)  time: 0.6862 (0.4952 -- 2.9424)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [28] Total time: 0:02:20 (0.8760 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0095 (2.0515)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5185 (7.7451)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.1984 (1.1984)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4368 (2.4368 -- 2.4368)  data: 2.2006 (2.2006 -- 2.2006)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1493 (1.2049)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (96.9697)  time: 0.4187 (0.1966 -- 2.4368)  data: 0.2010 (0.0007 -- 2.2006)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1307 (1.1690)  acc1: 55.5556 (60.8466)  acc5: 100.0000 (96.8254)  time: 0.2156 (0.1696 -- 0.3290)  data: 0.0070 (0.0001 -- 0.1254)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1307 (1.2183)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.8506)  time: 0.1997 (0.1330 -- 0.3290)  data: 0.0066 (0.0001 -- 0.1254)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 62.448 Acc@5 95.228 loss 1.178
Accuracy of the network on the 482 val images: 62.45%
[2023-08-30 22:44:58,012] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:44:58,014] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:44:58,014] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:44:58,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:44:59,425] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:44:59,426] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.45%
Epoch: [29]  [  0/160]  eta: 0:19:46  lr: 0.000023  min_lr: 0.000001  loss: 2.5048 (2.5048)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6038 (5.6038)  time: 7.4153 (7.4153 -- 7.4153)  data: 6.8854 (6.8854 -- 6.8854)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:38  lr: 0.000023  min_lr: 0.000001  loss: 2.1904 (2.1762)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7280 (7.5021)  time: 0.8170 (0.5200 -- 2.3020)  data: 0.2622 (0.0005 -- 1.7712)  max mem: 16413
Epoch: [29]  [ 40/160]  eta: 0:02:06  lr: 0.000023  min_lr: 0.000001  loss: 1.9872 (2.0899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4257 (7.5380)  time: 0.9734 (0.5329 -- 2.8988)  data: 0.4008 (0.0006 -- 2.3674)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.0321 (2.1217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7588 (7.7771)  time: 0.8151 (0.5277 -- 3.8229)  data: 0.2653 (0.0003 -- 3.3161)  max mem: 16413
[2023-08-30 22:46:04,562] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:46:04,563] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:46:04,604] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:46:04,604] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:46:06,740] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4711
[2023-08-30 22:46:06,741] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:46:06,740] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4711
[2023-08-30 22:46:06,741] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:46:06,741] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.0678 (2.0994)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0604 (7.7542)  time: 0.8664 (0.5240 -- 3.4624)  data: 0.3145 (0.0004 -- 2.9322)  max mem: 16413
Epoch: [29]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.9186 (2.0748)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8435 (7.6744)  time: 0.8429 (0.5253 -- 4.0261)  data: 0.2876 (0.0004 -- 3.5029)  max mem: 16413
Epoch: [29]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0149 (2.0618)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2634 (7.6942)  time: 0.8578 (0.5169 -- 2.6833)  data: 0.3138 (0.0004 -- 2.1636)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9671 (2.0467)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4106 (7.7922)  time: 0.8782 (0.5368 -- 4.7899)  data: 0.0114 (0.0005 -- 0.1934)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0472 (2.0529)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8915 (7.6864)  time: 0.7347 (0.4941 -- 2.6095)  data: 0.0013 (0.0002 -- 0.0127)  max mem: 16413
Epoch: [29] Total time: 0:02:22 (0.8913 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0472 (2.0347)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8915 (7.6864)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.2030 (1.2030)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3840 (2.3840 -- 2.3840)  data: 2.1254 (2.1254 -- 2.1254)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1004 (1.1757)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.4142 (0.1958 -- 2.3840)  data: 0.1942 (0.0006 -- 2.1254)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0584 (1.1423)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (96.2963)  time: 0.2189 (0.1709 -- 0.3969)  data: 0.0137 (0.0001 -- 0.2208)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0934 (1.1861)  acc1: 66.6667 (62.2407)  acc5: 100.0000 (95.0207)  time: 0.2041 (0.1329 -- 0.3969)  data: 0.0135 (0.0001 -- 0.2208)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 63.278 Acc@5 94.606 loss 1.153
Accuracy of the network on the 482 val images: 63.28%
[2023-08-30 22:47:29,788] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:47:29,789] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:47:29,789] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:47:29,790] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:47:31,018] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:47:31,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.28%
Epoch: [30]  [  0/160]  eta: 0:19:11  lr: 0.000023  min_lr: 0.000001  loss: 1.8342 (1.8342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8576 (7.8576)  time: 7.1942 (7.1942 -- 7.1942)  data: 6.6574 (6.6574 -- 6.6574)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:03:07  lr: 0.000022  min_lr: 0.000001  loss: 1.9351 (2.0200)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1231 (7.7293)  time: 1.0487 (0.5192 -- 5.2335)  data: 0.4356 (0.0003 -- 4.7307)  max mem: 16413
[2023-08-30 22:48:14,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:48:14,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:48:14,547] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:48:14,547] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 40/160]  eta: 0:02:07  lr: 0.000022  min_lr: 0.000001  loss: 2.1588 (2.0215)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6780 (7.5093)  time: 0.7681 (0.5122 -- 3.8608)  data: 0.2280 (0.0003 -- 3.3122)  max mem: 16413
[2023-08-30 22:48:19,257] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4843
[2023-08-30 22:48:19,257] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4843
[2023-08-30 22:48:19,257] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:48:19,257] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:48:19,257] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [ 60/160]  eta: 0:01:44  lr: 0.000022  min_lr: 0.000001  loss: 1.9751 (2.0035)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2978 (7.4034)  time: 1.0042 (0.5118 -- 4.6500)  data: 0.1613 (0.0003 -- 1.9252)  max mem: 16413
Epoch: [30]  [ 80/160]  eta: 0:01:16  lr: 0.000022  min_lr: 0.000001  loss: 1.8033 (1.9673)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5249 (7.6292)  time: 0.7058 (0.5214 -- 2.9375)  data: 0.0011 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000001  loss: 2.0606 (1.9845)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5396 (7.5753)  time: 0.8474 (0.5061 -- 3.5498)  data: 0.1611 (0.0003 -- 1.3478)  max mem: 16413
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 2.0959 (1.9938)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9255 (7.7134)  time: 0.9792 (0.5347 -- 4.5018)  data: 0.0487 (0.0002 -- 0.4877)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 2.0943 (2.0089)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7413 (7.6449)  time: 0.7648 (0.5245 -- 1.7527)  data: 0.0750 (0.0002 -- 0.7514)  max mem: 16413
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0164 (2.0183)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4453 (7.7204)  time: 0.7703 (0.4955 -- 2.1665)  data: 0.0359 (0.0002 -- 0.6998)  max mem: 16413
Epoch: [30] Total time: 0:02:23 (0.8956 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0164 (2.0051)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4453 (7.7204)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.1682 (1.1682)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4395 (2.4395 -- 2.4395)  data: 2.2130 (2.2130 -- 2.2130)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0557 (1.1655)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4305 (0.2051 -- 2.4395)  data: 0.2163 (0.0007 -- 2.2130)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0303 (1.1369)  acc1: 55.5556 (60.8466)  acc5: 100.0000 (96.2963)  time: 0.2231 (0.1683 -- 0.4348)  data: 0.0208 (0.0001 -- 0.2453)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1045 (1.1839)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (94.1909)  time: 0.2088 (0.1329 -- 0.4348)  data: 0.0204 (0.0001 -- 0.2453)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 63.278 Acc@5 94.191 loss 1.142
Accuracy of the network on the 482 val images: 63.28%
Max accuracy: 63.28%
Epoch: [31]  [  0/160]  eta: 0:22:37  lr: 0.000022  min_lr: 0.000001  loss: 1.2859 (1.2859)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8496 (8.8496)  time: 8.4850 (8.4850 -- 8.4850)  data: 7.9704 (7.9704 -- 7.9704)  max mem: 16413
[2023-08-30 22:50:20,231] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:50:20,232] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:50:20,239] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:50:20,239] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 20/160]  eta: 0:02:39  lr: 0.000022  min_lr: 0.000001  loss: 2.0966 (2.0105)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5996 (8.4468)  time: 0.7744 (0.5185 -- 3.0893)  data: 0.2011 (0.0002 -- 2.5586)  max mem: 16413
[2023-08-30 22:50:42,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[5.323438495058938e-07, 5.323438495058938e-07, 7.097917993411918e-07, 7.097917993411918e-07, 9.463890657882557e-07, 9.463890657882557e-07, 1.2618520877176744e-06, 1.2618520877176744e-06, 1.6824694502902324e-06, 1.6824694502902324e-06, 2.2432926003869768e-06, 2.2432926003869768e-06, 2.991056800515969e-06, 2.991056800515969e-06, 3.988075734021291e-06, 3.988075734021291e-06, 5.317434312028389e-06, 5.317434312028389e-06, 7.089912416037852e-06, 7.089912416037852e-06, 9.453216554717135e-06, 9.453216554717135e-06, 1.2604288739622848e-05, 1.2604288739622848e-05, 1.680571831949713e-05, 1.680571831949713e-05, 2.2407624425996174e-05, 2.2407624425996174e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 22:50:42,483] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=16.9053549192303, CurrSamplesPerSec=22.459539346860677, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:01:59  lr: 0.000022  min_lr: 0.000001  loss: 2.1316 (2.0637)  loss_scale: 32768.0000 (27972.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5636 (8.0736)  time: 0.8396 (0.5325 -- 2.1766)  data: 0.0467 (0.0003 -- 0.5477)  max mem: 16413
[2023-08-30 22:50:47,927] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5005
[2023-08-30 22:50:47,927] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5005
[2023-08-30 22:50:47,927] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:50:47,927] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:50:47,927] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9129 (2.0400)  loss_scale: 16384.0000 (25247.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3657 (7.6240)  time: 0.9152 (0.5360 -- 2.3095)  data: 0.1844 (0.0005 -- 1.2684)  max mem: 16413
Epoch: [31]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 2.0759 (2.0366)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1689 (7.6200)  time: 0.8325 (0.5236 -- 2.1371)  data: 0.1207 (0.0002 -- 1.5354)  max mem: 16413
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.1851 (2.0589)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9435 (7.6278)  time: 0.8755 (0.5258 -- 3.0816)  data: 0.3177 (0.0004 -- 2.5471)  max mem: 16413
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.0133 (2.0599)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3675 (7.6622)  time: 0.8425 (0.5281 -- 2.5767)  data: 0.1506 (0.0005 -- 1.8054)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 1.8931 (2.0435)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9887 (7.6831)  time: 0.7975 (0.5343 -- 3.1366)  data: 0.0206 (0.0004 -- 0.3626)  max mem: 16413
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.1268 (2.0383)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5187 (7.5935)  time: 0.7539 (0.4951 -- 3.5233)  data: 0.0006 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [31] Total time: 0:02:20 (0.8787 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.1268 (2.0357)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5187 (7.5935)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.1772 (1.1772)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5163 (2.5163 -- 2.5163)  data: 2.2714 (2.2714 -- 2.2714)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1492 (1.1931)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (95.9596)  time: 0.4595 (0.2005 -- 2.5163)  data: 0.2404 (0.0006 -- 2.2714)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0798 (1.1501)  acc1: 55.5556 (59.2593)  acc5: 100.0000 (96.2963)  time: 0.2244 (0.1689 -- 0.5807)  data: 0.0188 (0.0001 -- 0.3504)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1178 (1.1947)  acc1: 55.5556 (58.0913)  acc5: 100.0000 (95.4357)  time: 0.2094 (0.1328 -- 0.5807)  data: 0.0179 (0.0001 -- 0.3504)  max mem: 16413
Val: Total time: 0:00:07 (0.2962 s / it)
* Acc@1 62.448 Acc@5 94.813 loss 1.150
Accuracy of the network on the 482 val images: 62.45%
Max accuracy: 63.28%
Epoch: [32]  [  0/160]  eta: 0:23:12  lr: 0.000022  min_lr: 0.000001  loss: 2.6439 (2.6439)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7809 (6.7809)  time: 8.7032 (8.7032 -- 8.7032)  data: 7.0680 (7.0680 -- 7.0680)  max mem: 16413
[2023-08-30 22:52:50,695] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:52:50,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:52:50,696] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:52:50,697] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 20/160]  eta: 0:02:59  lr: 0.000022  min_lr: 0.000001  loss: 2.0515 (2.0473)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2383 (7.3366)  time: 0.9121 (0.5198 -- 4.4083)  data: 0.1159 (0.0004 -- 1.7456)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:05  lr: 0.000022  min_lr: 0.000001  loss: 2.0880 (2.0616)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9065 (7.5121)  time: 0.8038 (0.5382 -- 3.5343)  data: 0.0014 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [32]  [ 60/160]  eta: 0:01:41  lr: 0.000022  min_lr: 0.000001  loss: 1.9094 (2.0380)  loss_scale: 32768.0000 (29007.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9080 (7.6455)  time: 0.9497 (0.5262 -- 5.0849)  data: 0.0096 (0.0003 -- 0.1545)  max mem: 16413
Epoch: [32]  [ 80/160]  eta: 0:01:17  lr: 0.000022  min_lr: 0.000001  loss: 2.0338 (2.0294)  loss_scale: 32768.0000 (29936.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4694 (7.6922)  time: 0.8428 (0.5347 -- 3.6140)  data: 0.0012 (0.0003 -- 0.0020)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000001  loss: 2.1718 (2.0412)  loss_scale: 32768.0000 (30496.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4793 (7.6784)  time: 0.8188 (0.5242 -- 2.7965)  data: 0.0676 (0.0004 -- 0.9310)  max mem: 16413
[2023-08-30 22:54:13,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5230
[2023-08-30 22:54:13,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5230
[2023-08-30 22:54:13,690] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:54:13,690] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:54:13,690] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.1177 (2.0637)  loss_scale: 16384.0000 (29382.8760)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1225 (7.8769)  time: 0.8046 (0.5266 -- 2.5490)  data: 0.2519 (0.0002 -- 2.0027)  max mem: 16413
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9875 (2.0621)  loss_scale: 16384.0000 (27539.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8286 (7.9672)  time: 0.9298 (0.5354 -- 3.3234)  data: 0.3489 (0.0004 -- 2.7932)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.1241 (2.0573)  loss_scale: 16384.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1460 (7.8953)  time: 0.6285 (0.4973 -- 1.6357)  data: 0.0948 (0.0002 -- 1.1098)  max mem: 16413
Epoch: [32] Total time: 0:02:21 (0.8871 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.1241 (2.0280)  loss_scale: 16384.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1460 (7.8953)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.1066 (1.1066)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5153 (2.5153 -- 2.5153)  data: 2.1857 (2.1857 -- 2.1857)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0390 (1.1592)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (94.9495)  time: 0.4209 (0.1995 -- 2.5153)  data: 0.1999 (0.0008 -- 2.1857)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0275 (1.1196)  acc1: 66.6667 (62.4339)  acc5: 100.0000 (95.2381)  time: 0.2107 (0.1712 -- 0.3455)  data: 0.0084 (0.0001 -- 0.1506)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1080 (1.1706)  acc1: 55.5556 (60.9959)  acc5: 100.0000 (94.1909)  time: 0.1966 (0.1331 -- 0.3455)  data: 0.0080 (0.0001 -- 0.1506)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 64.523 Acc@5 94.191 loss 1.128
Accuracy of the network on the 482 val images: 64.52%
[2023-08-30 22:55:00,488] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:55:00,490] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:55:00,490] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:55:00,490] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:55:01,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:55:01,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.52%
Epoch: [33]  [  0/160]  eta: 0:21:31  lr: 0.000022  min_lr: 0.000001  loss: 2.2299 (2.2299)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0904 (8.0904)  time: 8.0742 (8.0742 -- 8.0742)  data: 5.4534 (5.4534 -- 5.4534)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:49  lr: 0.000022  min_lr: 0.000001  loss: 2.1239 (2.0687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7001 (8.4417)  time: 0.8685 (0.5150 -- 2.9440)  data: 0.0214 (0.0001 -- 0.2280)  max mem: 16413
Epoch: [33]  [ 40/160]  eta: 0:02:06  lr: 0.000022  min_lr: 0.000001  loss: 1.9428 (2.0160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1030 (8.3419)  time: 0.8931 (0.5240 -- 3.7977)  data: 0.0448 (0.0003 -- 0.8742)  max mem: 16413
Epoch: [33]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 2.1766 (2.0352)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6814 (8.4135)  time: 0.7990 (0.5317 -- 2.2439)  data: 0.1622 (0.0004 -- 1.7025)  max mem: 16413
[2023-08-30 22:56:16,983] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:56:16,983] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:56:16,985] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:56:16,985] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 2.0353 (2.0392)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7043 (8.3854)  time: 0.8209 (0.5269 -- 3.0728)  data: 0.0645 (0.0006 -- 1.2562)  max mem: 16413
[2023-08-30 22:56:18,626] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5362
[2023-08-30 22:56:18,626] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5362
[2023-08-30 22:56:18,626] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:56:18,626] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 22:56:18,627] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9869 (2.0306)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7801 (8.3646)  time: 0.8670 (0.5265 -- 3.3482)  data: 0.0015 (0.0005 -- 0.0038)  max mem: 16413
Epoch: [33]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.0007 (2.0248)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1142 (8.4102)  time: 0.8875 (0.5249 -- 4.1364)  data: 0.0021 (0.0005 -- 0.0047)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 1.9563 (2.0063)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3678 (8.2686)  time: 0.7422 (0.5206 -- 2.5313)  data: 0.0019 (0.0007 -- 0.0073)  max mem: 16413
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0056 (1.9979)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0669 (8.4850)  time: 0.7325 (0.4961 -- 4.6999)  data: 0.0011 (0.0003 -- 0.0064)  max mem: 16413
Epoch: [33] Total time: 0:02:19 (0.8738 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0056 (1.9955)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0669 (8.4850)
Val:  [ 0/27]  eta: 0:01:08  loss: 1.1595 (1.1595)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5541 (2.5541 -- 2.5541)  data: 2.3189 (2.3189 -- 2.3189)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1051 (1.1542)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4372 (0.2077 -- 2.5541)  data: 0.2186 (0.0005 -- 2.3189)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9823 (1.0997)  acc1: 55.5556 (62.9630)  acc5: 100.0000 (95.7672)  time: 0.2141 (0.1710 -- 0.2958)  data: 0.0066 (0.0001 -- 0.0750)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0466 (1.1442)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (94.6058)  time: 0.1985 (0.1331 -- 0.2958)  data: 0.0063 (0.0001 -- 0.0750)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 64.730 Acc@5 94.191 loss 1.110
Accuracy of the network on the 482 val images: 64.73%
[2023-08-30 22:57:29,470] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:57:29,472] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:57:29,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:57:29,472] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 22:57:30,888] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 22:57:30,888] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.73%
Epoch: [34]  [  0/160]  eta: 0:17:25  lr: 0.000022  min_lr: 0.000001  loss: 1.8206 (1.8206)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7169 (9.7169)  time: 6.5358 (6.5358 -- 6.5358)  data: 5.3128 (5.3128 -- 5.3128)  max mem: 16413
Epoch: [34]  [ 20/160]  eta: 0:02:53  lr: 0.000022  min_lr: 0.000001  loss: 2.0758 (2.0456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8615 (9.0115)  time: 0.9777 (0.5224 -- 4.1774)  data: 0.0626 (0.0003 -- 1.0265)  max mem: 16413
Epoch: [34]  [ 40/160]  eta: 0:02:08  lr: 0.000022  min_lr: 0.000001  loss: 2.0423 (2.0162)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9472 (8.5517)  time: 0.8891 (0.5302 -- 3.1530)  data: 0.0014 (0.0001 -- 0.0032)  max mem: 16413
[2023-08-30 22:58:23,652] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:58:23,652] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 22:58:23,652] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 22:58:23,652] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [ 60/160]  eta: 0:01:38  lr: 0.000022  min_lr: 0.000001  loss: 2.0239 (1.9981)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0477 (8.0766)  time: 0.8097 (0.5301 -- 2.2808)  data: 0.0018 (0.0005 -- 0.0063)  max mem: 16413
Epoch: [34]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 2.0562 (2.0188)  loss_scale: 32768.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2142 (8.1932)  time: 0.8359 (0.5213 -- 2.2586)  data: 0.0015 (0.0008 -- 0.0030)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000001  loss: 2.0425 (2.0176)  loss_scale: 32768.0000 (24494.8911)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7079 (8.1198)  time: 0.9165 (0.5279 -- 2.6989)  data: 0.1323 (0.0003 -- 1.3321)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.8411 (1.9871)  loss_scale: 32768.0000 (25862.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9486 (8.0729)  time: 0.8207 (0.5193 -- 3.1586)  data: 0.1127 (0.0003 -- 1.1993)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9524 (1.9799)  loss_scale: 32768.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5044 (8.0086)  time: 0.9026 (0.5212 -- 3.3073)  data: 0.0110 (0.0004 -- 0.1678)  max mem: 16413
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.1058 (1.9874)  loss_scale: 32768.0000 (27545.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3473 (8.0909)  time: 0.6425 (0.4965 -- 1.9156)  data: 0.0006 (0.0002 -- 0.0014)  max mem: 16413
Epoch: [34] Total time: 0:02:20 (0.8785 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.1058 (2.0180)  loss_scale: 32768.0000 (27545.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3473 (8.0909)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.0976 (1.0976)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4481 (2.4481 -- 2.4481)  data: 2.2089 (2.2089 -- 2.2089)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0275 (1.1285)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4364 (0.2044 -- 2.4481)  data: 0.2185 (0.0006 -- 2.2089)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9361 (1.0845)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2180 (0.1699 -- 0.4143)  data: 0.0099 (0.0001 -- 0.1819)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0362 (1.1266)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (94.6058)  time: 0.2021 (0.1326 -- 0.4143)  data: 0.0097 (0.0001 -- 0.1819)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 65.768 Acc@5 94.191 loss 1.090
Accuracy of the network on the 482 val images: 65.77%
[2023-08-30 22:59:59,284] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 22:59:59,286] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 22:59:59,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 22:59:59,286] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:00:00,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:00:00,665] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.77%
[2023-08-30 23:00:08,901] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5600
[2023-08-30 23:00:08,902] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5600
[2023-08-30 23:00:08,902] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:00:08,902] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:00:08,902] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [  0/160]  eta: 0:21:57  lr: 0.000022  min_lr: 0.000001  loss: 1.8267 (1.8267)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3473 (9.3473)  time: 8.2350 (8.2350 -- 8.2350)  data: 7.7360 (7.7360 -- 7.7360)  max mem: 16413
Epoch: [35]  [ 20/160]  eta: 0:02:40  lr: 0.000022  min_lr: 0.000001  loss: 1.9387 (1.8827)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9418 (7.5926)  time: 0.7901 (0.5280 -- 4.1753)  data: 0.1639 (0.0004 -- 2.4957)  max mem: 16413
Epoch: [35]  [ 40/160]  eta: 0:02:06  lr: 0.000022  min_lr: 0.000001  loss: 2.0753 (2.0000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0202 (8.0648)  time: 0.9572 (0.5189 -- 3.3278)  data: 0.1726 (0.0004 -- 2.3844)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:41  lr: 0.000022  min_lr: 0.000001  loss: 2.0280 (1.9963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4353 (8.0430)  time: 0.9286 (0.5356 -- 2.9299)  data: 0.1854 (0.0004 -- 2.4004)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:16  lr: 0.000022  min_lr: 0.000001  loss: 1.7878 (1.9786)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3888 (8.1519)  time: 0.7947 (0.5283 -- 4.3798)  data: 0.0018 (0.0005 -- 0.0049)  max mem: 16413
Epoch: [35]  [100/160]  eta: 0:00:57  lr: 0.000022  min_lr: 0.000001  loss: 2.1374 (1.9994)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3607 (8.2197)  time: 0.9521 (0.5249 -- 3.5164)  data: 0.0017 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 2.0401 (1.9914)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8900 (8.2334)  time: 0.8002 (0.5253 -- 3.6362)  data: 0.0016 (0.0002 -- 0.0036)  max mem: 16413
[2023-08-30 23:02:02,030] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:02:02,031] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:02:02,034] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:02:02,035] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [35]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 2.0205 (1.9974)  loss_scale: 32768.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5706 (8.1094)  time: 0.8976 (0.5265 -- 4.2968)  data: 0.0015 (0.0004 -- 0.0032)  max mem: 16413
[2023-08-30 23:02:14,034] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5745
[2023-08-30 23:02:14,034] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5745
[2023-08-30 23:02:14,035] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:02:14,035] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:02:14,035] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9645 (2.0010)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2303 (8.1247)  time: 0.5587 (0.4951 -- 1.3245)  data: 0.0006 (0.0001 -- 0.0014)  max mem: 16413
Epoch: [35] Total time: 0:02:21 (0.8832 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9645 (1.9892)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2303 (8.1247)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.0691 (1.0691)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5680 (2.5680 -- 2.5680)  data: 2.2960 (2.2960 -- 2.2960)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0256 (1.1235)  acc1: 55.5556 (62.6263)  acc5: 100.0000 (96.9697)  time: 0.4439 (0.2046 -- 2.5680)  data: 0.2204 (0.0003 -- 2.2960)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0214 (1.0780)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2177 (0.1702 -- 0.3553)  data: 0.0109 (0.0001 -- 0.1216)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0402 (1.1220)  acc1: 55.5556 (62.2407)  acc5: 100.0000 (94.6058)  time: 0.2019 (0.1335 -- 0.3553)  data: 0.0107 (0.0001 -- 0.1216)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 65.353 Acc@5 94.606 loss 1.080
Accuracy of the network on the 482 val images: 65.35%
Max accuracy: 65.77%
Epoch: [36]  [  0/160]  eta: 0:21:52  lr: 0.000022  min_lr: 0.000001  loss: 2.0332 (2.0332)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3599 (6.3599)  time: 8.2020 (8.2020 -- 8.2020)  data: 7.6853 (7.6853 -- 7.6853)  max mem: 16413
Epoch: [36]  [ 20/160]  eta: 0:02:40  lr: 0.000022  min_lr: 0.000001  loss: 1.7918 (1.8015)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7637 (7.8314)  time: 0.7922 (0.5280 -- 3.3075)  data: 0.2221 (0.0007 -- 2.7854)  max mem: 16413
[2023-08-30 23:03:07,585] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5794
[2023-08-30 23:03:07,585] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5794
[2023-08-30 23:03:07,585] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 23:03:07,586] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 23:03:07,586] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [36]  [ 40/160]  eta: 0:02:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0308 (1.9102)  loss_scale: 16384.0000 (14985.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2800 (8.1779)  time: 0.8530 (0.5240 -- 4.1170)  data: 0.2852 (0.0004 -- 3.5871)  max mem: 16413
Epoch: [36]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 1.8855 (1.9069)  loss_scale: 8192.0000 (12758.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7536 (8.2321)  time: 0.8958 (0.5282 -- 4.0426)  data: 0.0321 (0.0005 -- 0.4571)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 2.0500 (1.9535)  loss_scale: 8192.0000 (11630.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4911 (8.1762)  time: 0.8644 (0.5288 -- 3.0815)  data: 0.0693 (0.0003 -- 0.7750)  max mem: 16413
Epoch: [36]  [100/160]  eta: 0:00:57  lr: 0.000022  min_lr: 0.000001  loss: 2.1522 (1.9861)  loss_scale: 8192.0000 (10949.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1286 (8.2339)  time: 1.0311 (0.5075 -- 4.6955)  data: 0.0009 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.1334 (2.0197)  loss_scale: 8192.0000 (10493.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8084 (8.2075)  time: 0.6434 (0.5199 -- 2.2904)  data: 0.0017 (0.0004 -- 0.0092)  max mem: 16413
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.8494 (2.0139)  loss_scale: 8192.0000 (10167.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3770 (8.0239)  time: 0.9574 (0.5165 -- 3.6808)  data: 0.0016 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0881 (2.0220)  loss_scale: 8192.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4723 (8.0013)  time: 0.6538 (0.4956 -- 3.2182)  data: 0.0010 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [36] Total time: 0:02:21 (0.8845 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0881 (2.0045)  loss_scale: 8192.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4723 (8.0013)
Val:  [ 0/27]  eta: 0:01:10  loss: 1.0675 (1.0675)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.6273 (2.6273 -- 2.6273)  data: 2.3388 (2.3388 -- 2.3388)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0675 (1.1236)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (94.9495)  time: 0.4389 (0.1934 -- 2.6273)  data: 0.2178 (0.0007 -- 2.3388)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9604 (1.0788)  acc1: 66.6667 (61.9048)  acc5: 100.0000 (95.2381)  time: 0.2158 (0.1696 -- 0.3973)  data: 0.0139 (0.0001 -- 0.2183)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0550 (1.1230)  acc1: 66.6667 (60.9959)  acc5: 100.0000 (93.7759)  time: 0.2003 (0.1324 -- 0.3973)  data: 0.0134 (0.0001 -- 0.2183)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 65.145 Acc@5 93.983 loss 1.084
Accuracy of the network on the 482 val images: 65.15%
Max accuracy: 65.77%
Epoch: [37]  [  0/160]  eta: 0:20:31  lr: 0.000022  min_lr: 0.000001  loss: 2.3789 (2.3789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1298 (8.1298)  time: 7.6986 (7.6986 -- 7.6986)  data: 6.0454 (6.0454 -- 6.0454)  max mem: 16413
[2023-08-30 23:05:09,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:05:09,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 23:05:09,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:05:09,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [37]  [ 20/160]  eta: 0:02:47  lr: 0.000022  min_lr: 0.000001  loss: 1.9438 (2.0294)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7370 (6.8038)  time: 0.8704 (0.5194 -- 3.3154)  data: 0.1011 (0.0003 -- 0.9060)  max mem: 16413
Epoch: [37]  [ 40/160]  eta: 0:02:01  lr: 0.000022  min_lr: 0.000001  loss: 2.0419 (2.0186)  loss_scale: 16384.0000 (15784.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7017 (7.2766)  time: 0.8286 (0.5198 -- 3.0328)  data: 0.1235 (0.0003 -- 1.2384)  max mem: 16413
Epoch: [37]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 1.9819 (2.0281)  loss_scale: 16384.0000 (15981.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1945 (7.5329)  time: 0.9017 (0.5222 -- 3.3371)  data: 0.3592 (0.0005 -- 2.8179)  max mem: 16413
[2023-08-30 23:06:15,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=30, lr=[5.196002051909714e-07, 5.196002051909714e-07, 6.928002735879619e-07, 6.928002735879619e-07, 9.237336981172825e-07, 9.237336981172825e-07, 1.2316449308230434e-06, 1.2316449308230434e-06, 1.642193241097391e-06, 1.642193241097391e-06, 2.189590988129855e-06, 2.189590988129855e-06, 2.9194546508398063e-06, 2.9194546508398063e-06, 3.892606201119742e-06, 3.892606201119742e-06, 5.190141601492989e-06, 5.190141601492989e-06, 6.920188801990652e-06, 6.920188801990652e-06, 9.226918402654204e-06, 9.226918402654204e-06, 1.2302557870205604e-05, 1.2302557870205604e-05, 1.640341049360747e-05, 1.640341049360747e-05, 2.187121399147663e-05, 2.187121399147663e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 23:06:15,281] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=16.77634328611941, CurrSamplesPerSec=22.029929609829587, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 1.9856 (2.0103)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5002 (7.5372)  time: 0.8392 (0.5191 -- 4.1484)  data: 0.2945 (0.0005 -- 3.6179)  max mem: 16413
Epoch: [37]  [100/160]  eta: 0:00:57  lr: 0.000022  min_lr: 0.000001  loss: 2.0388 (2.0173)  loss_scale: 16384.0000 (16140.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8001 (7.5548)  time: 0.9809 (0.5249 -- 5.2574)  data: 0.4362 (0.0003 -- 4.7088)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.0702 (2.0090)  loss_scale: 16384.0000 (16180.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2801 (7.7531)  time: 0.7126 (0.5225 -- 2.1538)  data: 0.1616 (0.0002 -- 1.5938)  max mem: 16413
[2023-08-30 23:07:00,266] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:07:00,266] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:07:00,266] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:07:00,266] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9695 (1.9964)  loss_scale: 16384.0000 (17371.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7088 (7.7023)  time: 0.8822 (0.5298 -- 4.8076)  data: 0.3134 (0.0004 -- 4.2715)  max mem: 16413
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9465 (1.9943)  loss_scale: 32768.0000 (19200.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9731 (7.6732)  time: 0.7345 (0.4959 -- 3.2259)  data: 0.1997 (0.0001 -- 2.5540)  max mem: 16413
Epoch: [37] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9465 (2.0069)  loss_scale: 32768.0000 (19200.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9731 (7.6732)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.9918 (0.9918)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5402 (2.5402 -- 2.5402)  data: 2.2904 (2.2904 -- 2.2904)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9384 (1.1046)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4457 (0.2015 -- 2.5402)  data: 0.2278 (0.0008 -- 2.2904)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9212 (1.0638)  acc1: 55.5556 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2145 (0.1694 -- 0.4162)  data: 0.0115 (0.0001 -- 0.2057)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0751 (1.1154)  acc1: 55.5556 (60.9959)  acc5: 100.0000 (94.6058)  time: 0.1975 (0.1327 -- 0.4162)  data: 0.0112 (0.0001 -- 0.2057)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 64.730 Acc@5 94.813 loss 1.073
Accuracy of the network on the 482 val images: 64.73%
Max accuracy: 65.77%
Epoch: [38]  [  0/160]  eta: 0:16:26  lr: 0.000022  min_lr: 0.000001  loss: 2.0700 (2.0700)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0813 (10.0813)  time: 6.1665 (6.1665 -- 6.1665)  data: 4.8343 (4.8343 -- 4.8343)  max mem: 16413
Epoch: [38]  [ 20/160]  eta: 0:02:38  lr: 0.000022  min_lr: 0.000001  loss: 2.0709 (2.0357)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4568 (7.3787)  time: 0.8808 (0.5347 -- 2.3293)  data: 0.2706 (0.0008 -- 1.7682)  max mem: 16413
Epoch: [38]  [ 40/160]  eta: 0:01:59  lr: 0.000022  min_lr: 0.000001  loss: 1.9109 (2.0191)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0794 (8.1061)  time: 0.8489 (0.5194 -- 3.0915)  data: 0.3014 (0.0002 -- 2.5710)  max mem: 16413
[2023-08-30 23:08:27,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6140
[2023-08-30 23:08:27,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6140
[2023-08-30 23:08:27,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:08:27,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 23:08:27,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [38]  [ 60/160]  eta: 0:01:35  lr: 0.000022  min_lr: 0.000001  loss: 1.8863 (1.9819)  loss_scale: 32768.0000 (32499.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9264 (8.1540)  time: 0.8656 (0.5260 -- 5.5786)  data: 0.3119 (0.0003 -- 5.0313)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 2.0034 (1.9900)  loss_scale: 16384.0000 (28520.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0044 (8.0637)  time: 0.8743 (0.5312 -- 3.6906)  data: 0.2323 (0.0004 -- 2.2926)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.1041 (2.0210)  loss_scale: 16384.0000 (26117.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6771 (8.0425)  time: 0.8961 (0.5294 -- 3.2155)  data: 0.2853 (0.0004 -- 2.3645)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.0820 (2.0319)  loss_scale: 16384.0000 (24508.2975)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5299 (8.1677)  time: 0.8968 (0.5079 -- 4.3195)  data: 0.3539 (0.0003 -- 3.8135)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.8963 (2.0246)  loss_scale: 16384.0000 (23355.9149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2869 (8.2251)  time: 0.8373 (0.5300 -- 3.7087)  data: 0.2907 (0.0003 -- 3.1913)  max mem: 16413
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0264 (2.0221)  loss_scale: 16384.0000 (22528.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7406 (8.0900)  time: 0.6390 (0.4948 -- 2.8161)  data: 0.1215 (0.0002 -- 2.2981)  max mem: 16413
Epoch: [38] Total time: 0:02:20 (0.8777 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0264 (2.0022)  loss_scale: 16384.0000 (22528.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7406 (8.0900)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.0461 (1.0461)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4562 (2.4562 -- 2.4562)  data: 2.2312 (2.2312 -- 2.2312)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9790 (1.0971)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4300 (0.1980 -- 2.4562)  data: 0.2094 (0.0006 -- 2.2312)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8951 (1.0529)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2223 (0.1683 -- 0.4029)  data: 0.0129 (0.0001 -- 0.1828)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0162 (1.1000)  acc1: 66.6667 (62.6556)  acc5: 100.0000 (95.0207)  time: 0.2026 (0.1323 -- 0.4029)  data: 0.0119 (0.0001 -- 0.1828)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 66.598 Acc@5 94.813 loss 1.060
Accuracy of the network on the 482 val images: 66.60%
[2023-08-30 23:09:57,688] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:09:57,690] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:09:57,690] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:09:57,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:09:58,976] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:09:58,976] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.60%
Epoch: [39]  [  0/160]  eta: 0:18:04  lr: 0.000022  min_lr: 0.000001  loss: 2.0171 (2.0171)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1987 (5.1987)  time: 6.7812 (6.7812 -- 6.7812)  data: 4.2260 (4.2260 -- 4.2260)  max mem: 16413
Epoch: [39]  [ 20/160]  eta: 0:02:31  lr: 0.000022  min_lr: 0.000001  loss: 1.9840 (2.0202)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4973 (7.3788)  time: 0.7984 (0.5317 -- 2.6185)  data: 0.0682 (0.0007 -- 1.1567)  max mem: 16413
[2023-08-30 23:10:28,681] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:10:28,681] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:10:28,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:10:28,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [ 40/160]  eta: 0:01:56  lr: 0.000022  min_lr: 0.000001  loss: 1.9635 (2.0109)  loss_scale: 32768.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4656 (7.1798)  time: 0.8556 (0.5250 -- 2.9041)  data: 0.0822 (0.0002 -- 1.0234)  max mem: 16413
[2023-08-30 23:10:49,195] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6293
[2023-08-30 23:10:49,195] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6293
[2023-08-30 23:10:49,195] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:10:49,195] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:10:49,195] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [ 60/160]  eta: 0:01:33  lr: 0.000022  min_lr: 0.000001  loss: 2.0761 (2.0200)  loss_scale: 32768.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9663 (7.4960)  time: 0.8558 (0.5214 -- 2.5243)  data: 0.0830 (0.0003 -- 1.2323)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:13  lr: 0.000022  min_lr: 0.000001  loss: 2.1600 (2.0493)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3764 (7.4704)  time: 0.8817 (0.5211 -- 3.5037)  data: 0.0021 (0.0003 -- 0.0118)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9585 (2.0418)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6635 (7.6759)  time: 0.9593 (0.5253 -- 4.6120)  data: 0.0014 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9479 (2.0276)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6395 (7.6601)  time: 0.7930 (0.5255 -- 2.2673)  data: 0.0326 (0.0003 -- 0.6138)  max mem: 16413
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 2.1353 (2.0298)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3040 (7.6807)  time: 0.9757 (0.5291 -- 4.0812)  data: 0.0309 (0.0003 -- 0.5923)  max mem: 16413
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0317 (2.0148)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0279 (7.7303)  time: 0.5980 (0.4958 -- 1.9860)  data: 0.0007 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [39] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0317 (2.0060)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0279 (7.7303)
[2023-08-30 23:12:19,623] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-08-30 23:12:19,625] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-08-30 23:12:19,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-08-30 23:12:19,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-08-30 23:12:20,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-08-30 23:12:20,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:05  loss: 0.9465 (0.9465)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4100 (2.4100 -- 2.4100)  data: 2.1282 (2.1282 -- 2.1282)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9244 (1.0824)  acc1: 55.5556 (62.6263)  acc5: 100.0000 (96.9697)  time: 0.4223 (0.1980 -- 2.4100)  data: 0.1946 (0.0006 -- 2.1282)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9244 (1.0491)  acc1: 66.6667 (62.4339)  acc5: 100.0000 (96.2963)  time: 0.2219 (0.1710 -- 0.3232)  data: 0.0115 (0.0001 -- 0.1367)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0376 (1.0980)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.0207)  time: 0.2033 (0.1328 -- 0.3232)  data: 0.0113 (0.0001 -- 0.1367)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 66.390 Acc@5 94.813 loss 1.050
Accuracy of the network on the 482 val images: 66.39%
Max accuracy: 66.60%
Epoch: [40]  [  0/160]  eta: 0:18:06  lr: 0.000022  min_lr: 0.000001  loss: 1.5577 (1.5577)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8612 (6.8612)  time: 6.7905 (6.7905 -- 6.7905)  data: 4.9149 (4.9149 -- 4.9149)  max mem: 16413
Epoch: [40]  [ 20/160]  eta: 0:02:40  lr: 0.000022  min_lr: 0.000001  loss: 2.0376 (1.9848)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3447 (7.0297)  time: 0.8636 (0.5329 -- 4.6266)  data: 0.0355 (0.0003 -- 0.4437)  max mem: 16413
[2023-08-30 23:12:53,619] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:12:53,619] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:12:53,619] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:12:53,620] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:12:59,775] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6428
[2023-08-30 23:12:59,775] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:12:59,775] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6428
[2023-08-30 23:12:59,775] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 23:12:59,775] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [40]  [ 40/160]  eta: 0:01:59  lr: 0.000022  min_lr: 0.000001  loss: 2.0233 (1.9945)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4092 (7.5472)  time: 0.8315 (0.5140 -- 2.1485)  data: 0.0015 (0.0003 -- 0.0070)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:35  lr: 0.000022  min_lr: 0.000001  loss: 1.9126 (1.9712)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3094 (7.6562)  time: 0.8875 (0.5108 -- 4.0694)  data: 0.0017 (0.0004 -- 0.0050)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 1.9668 (1.9626)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2304 (7.7282)  time: 0.9113 (0.5396 -- 2.6002)  data: 0.0032 (0.0007 -- 0.0172)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:54  lr: 0.000022  min_lr: 0.000001  loss: 2.0546 (1.9692)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2056 (7.7866)  time: 0.7576 (0.5297 -- 2.3018)  data: 0.0018 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.0818 (1.9851)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2218 (8.0366)  time: 0.9295 (0.5281 -- 3.6105)  data: 0.0017 (0.0004 -- 0.0075)  max mem: 16413
Epoch: [40]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 2.0998 (2.0031)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4832 (7.9803)  time: 0.7662 (0.5255 -- 2.5075)  data: 0.0015 (0.0003 -- 0.0045)  max mem: 16413
[2023-08-30 23:14:47,850] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:14:47,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:14:47,851] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:14:47,851] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9906 (1.9894)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4140 (7.9942)  time: 0.7571 (0.4960 -- 3.5842)  data: 0.0006 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [40] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9906 (1.9876)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4140 (7.9942)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.9619 (0.9619)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3890 (2.3890 -- 2.3890)  data: 2.1477 (2.1477 -- 2.1477)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9391 (1.0806)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (94.9495)  time: 0.4342 (0.1938 -- 2.3890)  data: 0.2173 (0.0006 -- 2.1477)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8838 (1.0372)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (95.7672)  time: 0.2180 (0.1704 -- 0.4632)  data: 0.0136 (0.0001 -- 0.2334)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0484 (1.0850)  acc1: 55.5556 (61.4108)  acc5: 100.0000 (95.0207)  time: 0.2018 (0.1326 -- 0.4632)  data: 0.0133 (0.0001 -- 0.2334)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 66.390 Acc@5 94.813 loss 1.042
Accuracy of the network on the 482 val images: 66.39%
Max accuracy: 66.60%
Epoch: [41]  [  0/160]  eta: 0:21:55  lr: 0.000022  min_lr: 0.000001  loss: 2.3564 (2.3564)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7825 (7.7825)  time: 8.2213 (8.2213 -- 8.2213)  data: 7.6754 (7.6754 -- 7.6754)  max mem: 16413
[2023-08-30 23:15:07,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6565
[2023-08-30 23:15:07,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6565
[2023-08-30 23:15:07,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:15:07,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:15:07,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [ 20/160]  eta: 0:02:40  lr: 0.000022  min_lr: 0.000001  loss: 1.9920 (2.0193)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8824 (7.4343)  time: 0.7938 (0.5274 -- 3.0255)  data: 0.1024 (0.0003 -- 1.4553)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:03  lr: 0.000021  min_lr: 0.000001  loss: 2.0101 (2.0276)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5901 (7.7458)  time: 0.9132 (0.5195 -- 2.9560)  data: 0.3693 (0.0003 -- 2.4342)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:36  lr: 0.000021  min_lr: 0.000001  loss: 1.9647 (2.0123)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1638 (7.4004)  time: 0.8360 (0.5287 -- 3.5346)  data: 0.0746 (0.0003 -- 0.6895)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000001  loss: 2.0489 (2.0078)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1578 (7.3632)  time: 0.8934 (0.5271 -- 4.2460)  data: 0.0012 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [41]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000001  loss: 2.0068 (1.9889)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7512 (7.3652)  time: 0.8174 (0.5171 -- 4.1163)  data: 0.0492 (0.0001 -- 0.5013)  max mem: 16413
Epoch: [41]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000001  loss: 1.8769 (1.9756)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9446 (7.4077)  time: 0.9315 (0.5296 -- 3.6639)  data: 0.0014 (0.0006 -- 0.0033)  max mem: 16413
[2023-08-30 23:16:58,154] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:16:58,154] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:16:58,154] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:16:58,155] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:17:00,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6696
[2023-08-30 23:17:00,690] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:17:00,690] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6696
[2023-08-30 23:17:00,690] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:17:00,690] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.9503 (1.9722)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8267 (7.6164)  time: 0.7641 (0.5291 -- 2.2125)  data: 0.0160 (0.0002 -- 0.2917)  max mem: 16413
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 2.0140 (1.9818)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9108 (7.7593)  time: 0.7566 (0.4939 -- 3.3061)  data: 0.0955 (0.0002 -- 1.7800)  max mem: 16413
Epoch: [41] Total time: 0:02:21 (0.8863 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 2.0140 (1.9837)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9108 (7.7593)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.9435 (0.9435)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4312 (2.4312 -- 2.4312)  data: 2.1985 (2.1985 -- 2.1985)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9340 (1.0616)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4446 (0.2059 -- 2.4312)  data: 0.2262 (0.0006 -- 2.1985)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8998 (1.0236)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (95.7672)  time: 0.2224 (0.1692 -- 0.4916)  data: 0.0147 (0.0001 -- 0.2814)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0135 (1.0704)  acc1: 55.5556 (63.0705)  acc5: 100.0000 (95.4357)  time: 0.2060 (0.1327 -- 0.4916)  data: 0.0144 (0.0001 -- 0.2814)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 67.427 Acc@5 94.813 loss 1.025
Accuracy of the network on the 482 val images: 67.43%
[2023-08-30 23:17:26,327] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:17:26,329] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:17:26,329] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:17:26,329] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:17:27,713] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:17:27,714] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.43%
Epoch: [42]  [  0/160]  eta: 0:21:08  lr: 0.000021  min_lr: 0.000001  loss: 2.2605 (2.2605)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8143 (9.8143)  time: 7.9301 (7.9301 -- 7.9301)  data: 6.1003 (6.1003 -- 6.1003)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:37  lr: 0.000021  min_lr: 0.000001  loss: 1.9536 (2.0105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2700 (8.1355)  time: 0.7853 (0.5271 -- 3.4872)  data: 0.0250 (0.0002 -- 0.4694)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:04  lr: 0.000021  min_lr: 0.000001  loss: 2.0007 (1.9737)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8387 (8.3522)  time: 0.9402 (0.5241 -- 4.9223)  data: 0.0012 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:35  lr: 0.000021  min_lr: 0.000001  loss: 2.0040 (1.9804)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1381 (8.1159)  time: 0.7897 (0.5323 -- 3.3848)  data: 0.0015 (0.0005 -- 0.0029)  max mem: 16413
Epoch: [42]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000001  loss: 2.0888 (1.9967)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9667 (7.9438)  time: 0.9279 (0.5250 -- 1.9691)  data: 0.1779 (0.0001 -- 1.2853)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000001  loss: 1.8452 (1.9861)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8219 (7.9790)  time: 0.8413 (0.5258 -- 2.5776)  data: 0.1226 (0.0005 -- 1.0509)  max mem: 16413
[2023-08-30 23:19:06,085] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:19:06,085] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:19:06,086] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:19:06,086] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:19:11,997] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6832
[2023-08-30 23:19:11,997] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6832
[2023-08-30 23:19:11,997] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:19:11,997] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:19:11,997] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [42]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000001  loss: 1.8713 (1.9715)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0095 (7.7265)  time: 0.9469 (0.5285 -- 4.4125)  data: 0.3144 (0.0002 -- 3.8786)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.8062 (1.9578)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4725 (7.7947)  time: 0.8255 (0.5161 -- 3.3655)  data: 0.2843 (0.0002 -- 2.8521)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.9591 (1.9517)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1906 (7.7423)  time: 0.6989 (0.4959 -- 4.2389)  data: 0.1863 (0.0002 -- 3.7140)  max mem: 16413
Epoch: [42] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.9591 (1.9606)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1906 (7.7423)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.9276 (0.9276)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5178 (2.5178 -- 2.5178)  data: 2.2791 (2.2791 -- 2.2791)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9276 (1.0466)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (97.9798)  time: 0.4288 (0.1978 -- 2.5178)  data: 0.2130 (0.0005 -- 2.2791)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8809 (0.9999)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (97.3545)  time: 0.2119 (0.1689 -- 0.2662)  data: 0.0080 (0.0001 -- 0.0579)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9474 (1.0460)  acc1: 66.6667 (63.4855)  acc5: 100.0000 (95.8506)  time: 0.1971 (0.1323 -- 0.2662)  data: 0.0077 (0.0001 -- 0.0579)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 68.050 Acc@5 95.643 loss 1.008
Accuracy of the network on the 482 val images: 68.05%
[2023-08-30 23:19:57,998] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:19:57,999] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:19:57,999] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:19:58,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:19:59,371] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:19:59,372] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.05%
Epoch: [43]  [  0/160]  eta: 0:16:48  lr: 0.000021  min_lr: 0.000001  loss: 2.1698 (2.1698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0316 (10.0316)  time: 6.3026 (6.3026 -- 6.3026)  data: 5.5816 (5.5816 -- 5.5816)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:46  lr: 0.000021  min_lr: 0.000001  loss: 2.0438 (2.0178)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9893 (7.9210)  time: 0.9346 (0.5287 -- 2.3891)  data: 0.2141 (0.0003 -- 1.6627)  max mem: 16413
Epoch: [43]  [ 40/160]  eta: 0:02:00  lr: 0.000021  min_lr: 0.000001  loss: 1.9649 (1.9487)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4945 (8.1236)  time: 0.8102 (0.5142 -- 3.8807)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [43]  [ 60/160]  eta: 0:01:38  lr: 0.000021  min_lr: 0.000001  loss: 1.9494 (1.9573)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0676 (8.0504)  time: 0.9456 (0.5267 -- 4.7085)  data: 0.0013 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [43]  [ 80/160]  eta: 0:01:14  lr: 0.000021  min_lr: 0.000001  loss: 1.9687 (1.9436)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3591 (7.9900)  time: 0.7911 (0.5227 -- 3.0590)  data: 0.0020 (0.0002 -- 0.0061)  max mem: 16413
[2023-08-30 23:21:17,168] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:21:17,168] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:21:17,169] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:21:17,169] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:21:23,923] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6970
[2023-08-30 23:21:23,923] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6970
[2023-08-30 23:21:23,923] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:21:23,923] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:21:23,923] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [43]  [100/160]  eta: 0:00:54  lr: 0.000021  min_lr: 0.000001  loss: 1.9074 (1.9542)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2436 (7.8400)  time: 0.8086 (0.5248 -- 1.8534)  data: 0.0335 (0.0004 -- 0.4499)  max mem: 16413
[2023-08-30 23:21:48,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=37, lr=[5.044188189952753e-07, 5.044188189952753e-07, 6.725584253270338e-07, 6.725584253270338e-07, 8.967445671027117e-07, 8.967445671027117e-07, 1.1956594228036155e-06, 1.1956594228036155e-06, 1.5942125637381541e-06, 1.5942125637381541e-06, 2.1256167516508723e-06, 2.1256167516508723e-06, 2.8341556688678293e-06, 2.8341556688678293e-06, 3.778874225157106e-06, 3.778874225157106e-06, 5.038498966876141e-06, 5.038498966876141e-06, 6.717998622501522e-06, 6.717998622501522e-06, 8.957331496668695e-06, 8.957331496668695e-06, 1.1943108662224928e-05, 1.1943108662224928e-05, 1.592414488296657e-05, 1.592414488296657e-05, 2.123219317728876e-05, 2.123219317728876e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 23:21:48,295] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=16.76519111277085, CurrSamplesPerSec=22.372286121191323, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.8563 (1.9298)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4323 (7.9038)  time: 0.8668 (0.5330 -- 2.3069)  data: 0.1015 (0.0003 -- 1.7244)  max mem: 16413
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.9380 (1.9391)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7691 (7.9815)  time: 0.9204 (0.5275 -- 3.5716)  data: 0.2279 (0.0002 -- 3.0565)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 2.0333 (1.9389)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4898 (7.9537)  time: 0.6746 (0.4989 -- 2.6102)  data: 0.0014 (0.0002 -- 0.0137)  max mem: 16413
Epoch: [43] Total time: 0:02:20 (0.8780 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 2.0333 (1.9610)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4898 (7.9537)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.9599 (0.9599)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4178 (2.4178 -- 2.4178)  data: 2.1562 (2.1562 -- 2.1562)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9599 (1.0574)  acc1: 55.5556 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4421 (0.1994 -- 2.4178)  data: 0.2202 (0.0007 -- 2.1562)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8781 (1.0056)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2232 (0.1706 -- 0.4661)  data: 0.0167 (0.0001 -- 0.2555)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9866 (1.0506)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (95.4357)  time: 0.2057 (0.1324 -- 0.4661)  data: 0.0164 (0.0001 -- 0.2555)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 67.220 Acc@5 95.228 loss 1.009
Accuracy of the network on the 482 val images: 67.22%
Max accuracy: 68.05%
Epoch: [44]  [  0/160]  eta: 0:19:24  lr: 0.000021  min_lr: 0.000001  loss: 1.8992 (1.8992)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8243 (9.8243)  time: 7.2755 (7.2755 -- 7.2755)  data: 6.7283 (6.7283 -- 6.7283)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:02:51  lr: 0.000021  min_lr: 0.000001  loss: 1.9325 (1.9212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0895 (7.8929)  time: 0.9216 (0.5210 -- 3.0051)  data: 0.1727 (0.0007 -- 1.5679)  max mem: 16413
Epoch: [44]  [ 40/160]  eta: 0:02:08  lr: 0.000021  min_lr: 0.000001  loss: 1.8720 (1.8941)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4863 (7.7298)  time: 0.9046 (0.5183 -- 3.1093)  data: 0.3063 (0.0003 -- 2.5954)  max mem: 16413
[2023-08-30 23:23:27,692] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:23:27,693] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:23:27,693] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:23:27,693] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [ 60/160]  eta: 0:01:39  lr: 0.000021  min_lr: 0.000001  loss: 2.0467 (1.9394)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8796 (7.7530)  time: 0.8361 (0.5210 -- 3.9714)  data: 0.2966 (0.0003 -- 3.4682)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000001  loss: 1.9932 (1.9533)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2411 (7.8533)  time: 0.8548 (0.5196 -- 4.6441)  data: 0.3147 (0.0003 -- 4.1038)  max mem: 16413
Epoch: [44]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000001  loss: 1.8991 (1.9309)  loss_scale: 32768.0000 (23197.1485)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5650 (7.7457)  time: 0.8837 (0.5396 -- 3.7327)  data: 0.2948 (0.0005 -- 3.1961)  max mem: 16413
[2023-08-30 23:24:15,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7155
[2023-08-30 23:24:15,622] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:24:15,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7155
[2023-08-30 23:24:15,622] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:24:15,623] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [44]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.9611 (1.9418)  loss_scale: 32768.0000 (23966.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8072 (7.8553)  time: 0.7664 (0.5210 -- 2.8080)  data: 0.0021 (0.0004 -- 0.0096)  max mem: 16413
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 2.0001 (1.9497)  loss_scale: 16384.0000 (22891.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3595 (7.9420)  time: 1.0714 (0.5169 -- 5.7101)  data: 0.0010 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 2.0919 (1.9551)  loss_scale: 16384.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2912 (8.0041)  time: 0.5569 (0.4951 -- 0.8105)  data: 0.0008 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [44] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 2.0919 (1.9714)  loss_scale: 16384.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2912 (8.0041)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.8917 (0.8917)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5886 (2.5886 -- 2.5886)  data: 2.3196 (2.3196 -- 2.3196)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8634 (1.0482)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4533 (0.1988 -- 2.5886)  data: 0.2273 (0.0007 -- 2.3196)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8397 (1.0018)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.2963)  time: 0.2177 (0.1698 -- 0.4196)  data: 0.0126 (0.0001 -- 0.1698)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0159 (1.0487)  acc1: 55.5556 (61.8257)  acc5: 100.0000 (95.4357)  time: 0.1995 (0.1335 -- 0.4196)  data: 0.0122 (0.0001 -- 0.1698)  max mem: 16413
Val: Total time: 0:00:07 (0.2939 s / it)
* Acc@1 67.012 Acc@5 95.228 loss 1.005
Accuracy of the network on the 482 val images: 67.01%
Max accuracy: 68.05%
Epoch: [45]  [  0/160]  eta: 0:19:36  lr: 0.000021  min_lr: 0.000001  loss: 2.0929 (2.0929)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9646 (7.9646)  time: 7.3554 (7.3554 -- 7.3554)  data: 5.2532 (5.2532 -- 5.2532)  max mem: 16413
Epoch: [45]  [ 20/160]  eta: 0:02:33  lr: 0.000021  min_lr: 0.000001  loss: 2.0018 (1.9829)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7553 (8.3447)  time: 0.7842 (0.5228 -- 2.5134)  data: 0.0037 (0.0005 -- 0.0448)  max mem: 16413
Epoch: [45]  [ 40/160]  eta: 0:01:57  lr: 0.000021  min_lr: 0.000001  loss: 2.0272 (2.0059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1505 (8.4500)  time: 0.8605 (0.5258 -- 2.6302)  data: 0.1221 (0.0006 -- 1.3730)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:34  lr: 0.000021  min_lr: 0.000001  loss: 1.9787 (1.9858)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4775 (8.4185)  time: 0.8609 (0.5299 -- 2.4541)  data: 0.0826 (0.0003 -- 1.6163)  max mem: 16413
Epoch: [45]  [ 80/160]  eta: 0:01:13  lr: 0.000021  min_lr: 0.000000  loss: 2.0342 (1.9675)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4948 (8.2255)  time: 0.8434 (0.5210 -- 2.4970)  data: 0.2658 (0.0004 -- 1.9739)  max mem: 16413
[2023-08-30 23:26:18,642] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:26:18,642] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:26:18,646] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:26:18,646] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [45]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 2.1341 (1.9914)  loss_scale: 32768.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4591 (8.2806)  time: 1.0486 (0.5202 -- 4.6599)  data: 0.0447 (0.0003 -- 0.8608)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 1.8898 (1.9883)  loss_scale: 32768.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7004 (8.2314)  time: 0.8462 (0.5104 -- 3.8796)  data: 0.0010 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 2.0184 (1.9763)  loss_scale: 32768.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5438 (8.0644)  time: 0.7354 (0.5109 -- 3.0162)  data: 0.0016 (0.0003 -- 0.0048)  max mem: 16413
[2023-08-30 23:27:14,010] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7352
[2023-08-30 23:27:14,010] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:27:14,010] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7352
[2023-08-30 23:27:14,010] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:27:14,010] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9656 (1.9767)  loss_scale: 32768.0000 (23347.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4801 (8.0430)  time: 0.7107 (0.5058 -- 2.3254)  data: 0.0016 (0.0002 -- 0.0168)  max mem: 16413
Epoch: [45] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9656 (1.9438)  loss_scale: 32768.0000 (23347.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4801 (8.0430)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.9318 (0.9318)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4902 (2.4902 -- 2.4902)  data: 2.2816 (2.2816 -- 2.2816)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8685 (1.0256)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (96.9697)  time: 0.4230 (0.2034 -- 2.4902)  data: 0.2087 (0.0009 -- 2.2816)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8503 (0.9898)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2182 (0.1689 -- 0.4140)  data: 0.0113 (0.0001 -- 0.2088)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9465 (1.0323)  acc1: 55.5556 (63.0705)  acc5: 100.0000 (95.4357)  time: 0.2043 (0.1360 -- 0.4140)  data: 0.0109 (0.0001 -- 0.2088)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 67.427 Acc@5 95.436 loss 0.990
Accuracy of the network on the 482 val images: 67.43%
Max accuracy: 68.05%
Epoch: [46]  [  0/160]  eta: 0:16:57  lr: 0.000021  min_lr: 0.000000  loss: 2.4646 (2.4646)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1499 (7.1499)  time: 6.3573 (6.3573 -- 6.3573)  data: 5.8209 (5.8209 -- 5.8209)  max mem: 16413
Epoch: [46]  [ 20/160]  eta: 0:02:51  lr: 0.000021  min_lr: 0.000000  loss: 1.9621 (1.9670)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2128 (7.6504)  time: 0.9693 (0.5342 -- 3.2535)  data: 0.3984 (0.0005 -- 2.7231)  max mem: 16413
Epoch: [46]  [ 40/160]  eta: 0:02:02  lr: 0.000021  min_lr: 0.000000  loss: 1.8969 (1.9632)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3302 (7.9193)  time: 0.8094 (0.5274 -- 3.4294)  data: 0.2594 (0.0003 -- 2.8957)  max mem: 16413
Epoch: [46]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000000  loss: 2.0741 (1.9949)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5418 (8.0859)  time: 0.8865 (0.5217 -- 3.0142)  data: 0.2828 (0.0002 -- 2.4617)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:14  lr: 0.000021  min_lr: 0.000000  loss: 1.8943 (1.9634)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6374 (8.0442)  time: 0.7896 (0.5259 -- 3.1122)  data: 0.1931 (0.0003 -- 2.5902)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000000  loss: 1.8103 (1.9323)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4697 (8.0476)  time: 0.9160 (0.5329 -- 3.6489)  data: 0.3657 (0.0006 -- 3.1187)  max mem: 16413
Epoch: [46]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000000  loss: 2.1401 (1.9596)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8182 (8.0015)  time: 0.8590 (0.5351 -- 3.7610)  data: 0.3112 (0.0001 -- 3.2376)  max mem: 16413
[2023-08-30 23:29:18,343] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:29:18,344] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:29:18,344] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:29:18,344] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.9235 (1.9613)  loss_scale: 32768.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5394 (7.9480)  time: 0.9314 (0.5231 -- 3.2610)  data: 0.3798 (0.0002 -- 2.7259)  max mem: 16413
[2023-08-30 23:29:46,112] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7515
[2023-08-30 23:29:46,112] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7515
[2023-08-30 23:29:46,113] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:29:46,113] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:29:46,113] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 2.0223 (1.9656)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9017 (7.9401)  time: 0.6098 (0.4821 -- 2.3096)  data: 0.0885 (0.0002 -- 1.7602)  max mem: 16413
Epoch: [46] Total time: 0:02:21 (0.8829 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 2.0223 (1.9702)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9017 (7.9401)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.9113 (0.9113)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.5656 (2.5656 -- 2.5656)  data: 2.3578 (2.3578 -- 2.3578)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8446 (1.0176)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4457 (0.1958 -- 2.5656)  data: 0.2317 (0.0003 -- 2.3578)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8168 (0.9798)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (95.7672)  time: 0.2166 (0.1696 -- 0.4317)  data: 0.0147 (0.0001 -- 0.1832)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9894 (1.0277)  acc1: 55.5556 (63.4855)  acc5: 100.0000 (95.0207)  time: 0.2020 (0.1329 -- 0.4317)  data: 0.0144 (0.0001 -- 0.1832)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 67.427 Acc@5 95.021 loss 0.984
Accuracy of the network on the 482 val images: 67.43%
Max accuracy: 68.05%
Epoch: [47]  [  0/160]  eta: 0:20:10  lr: 0.000021  min_lr: 0.000000  loss: 1.8835 (1.8835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4040 (7.4040)  time: 7.5676 (7.5676 -- 7.5676)  data: 6.1990 (6.1990 -- 6.1990)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:45  lr: 0.000021  min_lr: 0.000000  loss: 2.0007 (1.9671)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1361 (8.2773)  time: 0.8597 (0.5240 -- 3.9566)  data: 0.3103 (0.0002 -- 3.4143)  max mem: 16413
Epoch: [47]  [ 40/160]  eta: 0:02:04  lr: 0.000021  min_lr: 0.000000  loss: 1.8892 (1.9059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8222 (8.2327)  time: 0.8883 (0.5223 -- 3.8089)  data: 0.3481 (0.0002 -- 3.2735)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:39  lr: 0.000021  min_lr: 0.000000  loss: 1.9006 (1.9080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2051 (7.9213)  time: 0.9144 (0.5215 -- 2.7883)  data: 0.1912 (0.0006 -- 2.2018)  max mem: 16413
Epoch: [47]  [ 80/160]  eta: 0:01:17  lr: 0.000021  min_lr: 0.000000  loss: 1.8670 (1.9131)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1547 (7.8787)  time: 0.9051 (0.5262 -- 3.6869)  data: 0.2849 (0.0004 -- 3.1710)  max mem: 16413
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 1.8840 (1.9112)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3982 (7.9333)  time: 0.8180 (0.5284 -- 3.4899)  data: 0.2698 (0.0003 -- 2.9634)  max mem: 16413
Epoch: [47]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 2.0643 (1.9302)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7916 (8.0855)  time: 0.8678 (0.5064 -- 4.3925)  data: 0.3277 (0.0002 -- 3.8545)  max mem: 16413
[2023-08-30 23:31:50,796] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:31:50,797] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:31:50,798] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:31:50,798] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:31:55,085] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7649
[2023-08-30 23:31:55,085] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:31:55,085] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7649
[2023-08-30 23:31:55,085] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:31:55,086] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 2.0130 (1.9388)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8265 (8.0667)  time: 0.7472 (0.5302 -- 2.6836)  data: 0.1825 (0.0004 -- 2.1403)  max mem: 16413
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9447 (1.9268)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6699 (8.1397)  time: 0.7054 (0.4972 -- 3.1675)  data: 0.1816 (0.0002 -- 2.6477)  max mem: 16413
Epoch: [47] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9447 (1.9387)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6699 (8.1397)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.8438 (0.8438)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.5856 (2.5856 -- 2.5856)  data: 2.3394 (2.3394 -- 2.3394)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8438 (1.0434)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.9596)  time: 0.4397 (0.1939 -- 2.5856)  data: 0.2186 (0.0005 -- 2.3394)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8019 (0.9962)  acc1: 66.6667 (65.6085)  acc5: 100.0000 (96.2963)  time: 0.2093 (0.1680 -- 0.2837)  data: 0.0034 (0.0001 -- 0.0519)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9956 (1.0417)  acc1: 66.6667 (63.4855)  acc5: 100.0000 (95.4357)  time: 0.1905 (0.1327 -- 0.2837)  data: 0.0029 (0.0001 -- 0.0519)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 68.465 Acc@5 95.228 loss 0.986
Accuracy of the network on the 482 val images: 68.46%
[2023-08-30 23:32:24,965] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:32:24,966] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:32:24,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:32:24,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:32:26,345] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:32:26,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.46%
Epoch: [48]  [  0/160]  eta: 0:18:16  lr: 0.000021  min_lr: 0.000000  loss: 1.7149 (1.7149)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1275 (5.1275)  time: 6.8504 (6.8504 -- 6.8504)  data: 5.8517 (5.8517 -- 5.8517)  max mem: 16413
Epoch: [48]  [ 20/160]  eta: 0:02:42  lr: 0.000021  min_lr: 0.000000  loss: 2.0514 (2.0305)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7346 (7.7048)  time: 0.8743 (0.5263 -- 2.8151)  data: 0.2152 (0.0007 -- 2.2660)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:04  lr: 0.000021  min_lr: 0.000000  loss: 1.7914 (1.9392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7481 (7.8907)  time: 0.9055 (0.5247 -- 3.2177)  data: 0.1070 (0.0003 -- 2.1125)  max mem: 16413
Epoch: [48]  [ 60/160]  eta: 0:01:40  lr: 0.000021  min_lr: 0.000000  loss: 1.9808 (1.9591)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4942 (8.0119)  time: 0.9566 (0.5292 -- 5.1882)  data: 0.0015 (0.0003 -- 0.0053)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000000  loss: 1.9473 (1.9643)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2641 (7.9068)  time: 0.7851 (0.5337 -- 3.6492)  data: 0.0013 (0.0003 -- 0.0024)  max mem: 16413
[2023-08-30 23:33:56,228] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:33:56,230] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:33:56,270] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:33:56,270] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [48]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000000  loss: 1.9321 (1.9481)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4137 (7.9374)  time: 0.8317 (0.5226 -- 2.7769)  data: 0.0354 (0.0003 -- 0.6811)  max mem: 16413
[2023-08-30 23:34:03,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7786
[2023-08-30 23:34:03,992] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7786
[2023-08-30 23:34:03,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:34:03,992] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:34:03,993] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [48]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000000  loss: 1.9647 (1.9491)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2147 (7.9228)  time: 0.8140 (0.5244 -- 2.8379)  data: 0.0233 (0.0002 -- 0.4439)  max mem: 16413
Epoch: [48]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 2.0173 (1.9587)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3362 (7.9380)  time: 0.8614 (0.5305 -- 2.6627)  data: 0.1484 (0.0001 -- 2.1422)  max mem: 16413
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9588 (1.9469)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4360 (7.9420)  time: 0.6934 (0.4962 -- 1.8890)  data: 0.1050 (0.0002 -- 1.3651)  max mem: 16413
Epoch: [48] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9588 (1.9401)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4360 (7.9420)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.9258 (0.9258)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4236 (2.4236 -- 2.4236)  data: 2.2295 (2.2295 -- 2.2295)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8436 (1.0251)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (96.9697)  time: 0.4306 (0.2062 -- 2.4236)  data: 0.2129 (0.0005 -- 2.2295)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8092 (0.9865)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (96.8254)  time: 0.2182 (0.1752 -- 0.3449)  data: 0.0115 (0.0001 -- 0.1093)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9932 (1.0363)  acc1: 55.5556 (62.2407)  acc5: 100.0000 (95.8506)  time: 0.2024 (0.1329 -- 0.3449)  data: 0.0112 (0.0001 -- 0.1093)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 67.220 Acc@5 95.228 loss 0.984
Accuracy of the network on the 482 val images: 67.22%
Max accuracy: 68.46%
Epoch: [49]  [  0/160]  eta: 0:19:36  lr: 0.000021  min_lr: 0.000000  loss: 2.4903 (2.4903)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7472 (9.7472)  time: 7.3560 (7.3560 -- 7.3560)  data: 6.8168 (6.8168 -- 6.8168)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000000  loss: 1.9743 (2.0074)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8929 (8.0814)  time: 0.9014 (0.5345 -- 3.7524)  data: 0.3547 (0.0005 -- 3.1993)  max mem: 16413
Epoch: [49]  [ 40/160]  eta: 0:02:10  lr: 0.000021  min_lr: 0.000000  loss: 2.0497 (2.0089)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6545 (7.5783)  time: 0.9585 (0.5282 -- 3.3809)  data: 0.4085 (0.0003 -- 2.8076)  max mem: 16413
Epoch: [49]  [ 60/160]  eta: 0:01:39  lr: 0.000021  min_lr: 0.000000  loss: 1.8660 (1.9905)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1047 (7.5605)  time: 0.8151 (0.5141 -- 4.4739)  data: 0.2726 (0.0003 -- 3.9372)  max mem: 16413
[2023-08-30 23:36:08,314] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:36:08,314] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:36:08,314] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:36:08,314] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [49]  [ 80/160]  eta: 0:01:18  lr: 0.000021  min_lr: 0.000000  loss: 2.0378 (1.9851)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2117 (7.6514)  time: 0.9173 (0.5312 -- 3.1265)  data: 0.3356 (0.0002 -- 2.6121)  max mem: 16413
[2023-08-30 23:36:24,829] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7934
[2023-08-30 23:36:24,829] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7934
[2023-08-30 23:36:24,829] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:36:24,830] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:36:24,830] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [49]  [100/160]  eta: 0:00:57  lr: 0.000021  min_lr: 0.000000  loss: 2.0531 (1.9847)  loss_scale: 32768.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0760 (7.5607)  time: 0.8648 (0.5103 -- 3.8864)  data: 0.3204 (0.0004 -- 3.3575)  max mem: 16413
Epoch: [49]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 2.0470 (1.9839)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2945 (7.5254)  time: 0.8055 (0.5167 -- 3.6710)  data: 0.2642 (0.0002 -- 3.1313)  max mem: 16413
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.9552 (1.9689)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1411 (7.6481)  time: 0.7832 (0.5215 -- 3.7981)  data: 0.2373 (0.0003 -- 3.2726)  max mem: 16413
[2023-08-30 23:37:17,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[4.869534833689233e-07, 4.869534833689233e-07, 6.492713111585644e-07, 6.492713111585644e-07, 8.656950815447526e-07, 8.656950815447526e-07, 1.1542601087263367e-06, 1.1542601087263367e-06, 1.5390134783017824e-06, 1.5390134783017824e-06, 2.052017971069043e-06, 2.052017971069043e-06, 2.7360239614253907e-06, 2.7360239614253907e-06, 3.6480319485671876e-06, 3.6480319485671876e-06, 4.864042598089584e-06, 4.864042598089584e-06, 6.485390130786111e-06, 6.485390130786111e-06, 8.647186841048149e-06, 8.647186841048149e-06, 1.1529582454730866e-05, 1.1529582454730866e-05, 1.537277660630782e-05, 1.537277660630782e-05, 2.0497035475077093e-05, 2.0497035475077093e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 23:37:17,772] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=16.99107193092147, CurrSamplesPerSec=24.650022283713298, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.8992 (1.9549)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5781 (7.6971)  time: 0.7572 (0.4953 -- 4.9497)  data: 0.2378 (0.0002 -- 4.4351)  max mem: 16413
Epoch: [49] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.8992 (1.9665)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5781 (7.6971)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.8336 (0.8336)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4262 (2.4262 -- 2.4262)  data: 2.1850 (2.1850 -- 2.1850)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8336 (1.0169)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4391 (0.1967 -- 2.4262)  data: 0.2220 (0.0008 -- 2.1850)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7846 (0.9720)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (96.2963)  time: 0.2184 (0.1712 -- 0.4680)  data: 0.0130 (0.0001 -- 0.2470)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9815 (1.0246)  acc1: 55.5556 (62.2407)  acc5: 100.0000 (95.4357)  time: 0.2030 (0.1362 -- 0.4680)  data: 0.0127 (0.0001 -- 0.2470)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 68.257 Acc@5 95.021 loss 0.974
Accuracy of the network on the 482 val images: 68.26%
Max accuracy: 68.46%
Epoch: [50]  [  0/160]  eta: 0:20:35  lr: 0.000020  min_lr: 0.000000  loss: 2.2282 (2.2282)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7347 (7.7347)  time: 7.7233 (7.7233 -- 7.7233)  data: 4.9655 (4.9655 -- 4.9655)  max mem: 16413
Epoch: [50]  [ 20/160]  eta: 0:02:39  lr: 0.000020  min_lr: 0.000000  loss: 1.6875 (1.7888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8058 (7.0987)  time: 0.8068 (0.5308 -- 3.5602)  data: 0.0532 (0.0004 -- 0.7534)  max mem: 16413
Epoch: [50]  [ 40/160]  eta: 0:02:00  lr: 0.000020  min_lr: 0.000000  loss: 1.6900 (1.8135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0859 (7.2065)  time: 0.8725 (0.5213 -- 3.3810)  data: 0.3128 (0.0004 -- 2.8323)  max mem: 16413
Epoch: [50]  [ 60/160]  eta: 0:01:37  lr: 0.000020  min_lr: 0.000000  loss: 1.9018 (1.8267)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8608 (7.3135)  time: 0.8955 (0.5197 -- 3.0543)  data: 0.3391 (0.0004 -- 2.5445)  max mem: 16413
[2023-08-30 23:38:27,800] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:38:27,801] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:38:27,802] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:38:27,802] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [50]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 2.0392 (1.8591)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7693 (7.4845)  time: 0.8747 (0.5278 -- 2.0096)  data: 0.2859 (0.0004 -- 1.4700)  max mem: 16413
[2023-08-30 23:38:47,915] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8086
[2023-08-30 23:38:47,915] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8086
[2023-08-30 23:38:47,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:38:47,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:38:47,916] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [50]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 1.9283 (1.8802)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9539 (7.6557)  time: 0.9010 (0.5233 -- 5.3068)  data: 0.3579 (0.0004 -- 4.7906)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8165 (1.8853)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1820 (7.9230)  time: 0.8360 (0.5158 -- 3.9036)  data: 0.2918 (0.0003 -- 3.3958)  max mem: 16413
[2023-08-30 23:39:17,596] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8121
[2023-08-30 23:39:17,596] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 23:39:17,596] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8121
[2023-08-30 23:39:17,597] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 23:39:17,597] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.9871 (1.9059)  loss_scale: 8192.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6188 (8.1230)  time: 0.8251 (0.5106 -- 4.0844)  data: 0.2809 (0.0002 -- 3.5214)  max mem: 16413
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9105 (1.9084)  loss_scale: 8192.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3685 (8.2064)  time: 0.6670 (0.4967 -- 3.4941)  data: 0.1481 (0.0002 -- 2.9461)  max mem: 16413
Epoch: [50] Total time: 0:02:20 (0.8801 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9105 (1.9363)  loss_scale: 8192.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3685 (8.2064)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.8079 (0.8079)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.5378 (2.5378 -- 2.5378)  data: 2.3001 (2.3001 -- 2.3001)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7868 (1.0085)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4568 (0.2001 -- 2.5378)  data: 0.2414 (0.0005 -- 2.3001)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7868 (0.9700)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.2963)  time: 0.2198 (0.1700 -- 0.5821)  data: 0.0180 (0.0001 -- 0.3474)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9662 (1.0162)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (95.8506)  time: 0.2065 (0.1325 -- 0.5821)  data: 0.0177 (0.0001 -- 0.3474)  max mem: 16413
Val: Total time: 0:00:07 (0.2947 s / it)
* Acc@1 68.257 Acc@5 95.436 loss 0.965
Accuracy of the network on the 482 val images: 68.26%
Max accuracy: 68.46%
Epoch: [51]  [  0/160]  eta: 0:17:30  lr: 0.000020  min_lr: 0.000000  loss: 2.2207 (2.2207)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9287 (8.9287)  time: 6.5644 (6.5644 -- 6.5644)  data: 5.7267 (5.7267 -- 5.7267)  max mem: 16413
Epoch: [51]  [ 20/160]  eta: 0:02:47  lr: 0.000020  min_lr: 0.000000  loss: 1.9265 (1.9489)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7898 (8.3163)  time: 0.9288 (0.5326 -- 2.7202)  data: 0.0773 (0.0004 -- 1.5081)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:01:59  lr: 0.000020  min_lr: 0.000000  loss: 1.8248 (1.9249)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6348 (8.1514)  time: 0.7804 (0.5298 -- 3.3556)  data: 0.0305 (0.0002 -- 0.5625)  max mem: 16413
Epoch: [51]  [ 60/160]  eta: 0:01:32  lr: 0.000020  min_lr: 0.000000  loss: 1.8828 (1.9328)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3328 (8.3979)  time: 0.7869 (0.5311 -- 1.7667)  data: 0.1273 (0.0002 -- 1.2447)  max mem: 16413
Epoch: [51]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 1.8762 (1.9336)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5957 (8.2483)  time: 1.0018 (0.5253 -- 3.9236)  data: 0.4527 (0.0004 -- 3.3902)  max mem: 16413
[2023-08-30 23:41:17,914] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:41:17,915] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 23:41:17,919] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:41:17,920] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [51]  [100/160]  eta: 0:00:54  lr: 0.000020  min_lr: 0.000000  loss: 1.9273 (1.9414)  loss_scale: 16384.0000 (9084.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3859 (8.1210)  time: 0.7762 (0.5279 -- 1.9241)  data: 0.2063 (0.0008 -- 1.3804)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.9873 (1.9496)  loss_scale: 16384.0000 (10290.7769)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4871 (8.0999)  time: 0.9031 (0.5287 -- 4.1206)  data: 0.3474 (0.0004 -- 3.5998)  max mem: 16413
Epoch: [51]  [140/160]  eta: 0:00:17  lr: 0.000020  min_lr: 0.000000  loss: 1.9116 (1.9510)  loss_scale: 16384.0000 (11155.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5987 (8.1486)  time: 0.7551 (0.5300 -- 2.9797)  data: 0.1771 (0.0007 -- 2.4566)  max mem: 16413
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9450 (1.9496)  loss_scale: 16384.0000 (11776.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1810 (8.2761)  time: 0.7573 (0.4956 -- 1.5986)  data: 0.1009 (0.0002 -- 1.0658)  max mem: 16413
Epoch: [51] Total time: 0:02:19 (0.8739 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9450 (1.9505)  loss_scale: 16384.0000 (11776.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1810 (8.2761)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.7990 (0.7990)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.6074 (2.6074 -- 2.6074)  data: 2.3822 (2.3822 -- 2.3822)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8059 (1.0143)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4375 (0.1935 -- 2.6074)  data: 0.2257 (0.0009 -- 2.3822)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8059 (0.9696)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (96.2963)  time: 0.2128 (0.1687 -- 0.3167)  data: 0.0100 (0.0001 -- 0.0960)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9843 (1.0184)  acc1: 66.6667 (62.2407)  acc5: 100.0000 (95.8506)  time: 0.1991 (0.1329 -- 0.3167)  data: 0.0097 (0.0001 -- 0.0960)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 68.257 Acc@5 95.643 loss 0.969
Accuracy of the network on the 482 val images: 68.26%
Max accuracy: 68.46%
Epoch: [52]  [  0/160]  eta: 0:19:07  lr: 0.000020  min_lr: 0.000000  loss: 1.9315 (1.9315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8438 (11.8438)  time: 7.1723 (7.1723 -- 7.1723)  data: 6.5885 (6.5885 -- 6.5885)  max mem: 16413
Epoch: [52]  [ 20/160]  eta: 0:02:27  lr: 0.000020  min_lr: 0.000000  loss: 1.7225 (1.8027)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1888 (7.6432)  time: 0.7486 (0.5405 -- 2.8733)  data: 0.0656 (0.0001 -- 0.6992)  max mem: 16413
Epoch: [52]  [ 40/160]  eta: 0:01:58  lr: 0.000020  min_lr: 0.000000  loss: 1.9549 (1.8553)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1992 (7.7379)  time: 0.9118 (0.5174 -- 3.2923)  data: 0.0348 (0.0004 -- 0.6646)  max mem: 16413
[2023-08-30 23:43:18,328] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:43:18,328] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:43:18,329] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:43:18,329] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [52]  [ 60/160]  eta: 0:01:34  lr: 0.000020  min_lr: 0.000000  loss: 2.0663 (1.9171)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9765 (7.7743)  time: 0.8679 (0.5271 -- 2.5249)  data: 0.0142 (0.0004 -- 0.2494)  max mem: 16413
[2023-08-30 23:43:29,027] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8390
[2023-08-30 23:43:29,027] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8390
[2023-08-30 23:43:29,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:43:29,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:43:29,027] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [52]  [ 80/160]  eta: 0:01:16  lr: 0.000020  min_lr: 0.000000  loss: 1.9126 (1.9298)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8827 (8.0714)  time: 0.9626 (0.5212 -- 3.3398)  data: 0.0456 (0.0003 -- 0.8857)  max mem: 16413
Epoch: [52]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 2.0099 (1.9396)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7856 (7.9242)  time: 0.8841 (0.5247 -- 4.2533)  data: 0.0027 (0.0007 -- 0.0173)  max mem: 16413
Epoch: [52]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.9096 (1.9470)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5418 (7.8863)  time: 0.8512 (0.5198 -- 3.8248)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.9719 (1.9464)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6580 (7.9023)  time: 0.9567 (0.5290 -- 4.0886)  data: 0.0018 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9647 (1.9483)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0021 (7.8836)  time: 0.6240 (0.4957 -- 2.3164)  data: 0.0007 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [52] Total time: 0:02:22 (0.8929 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9647 (1.9652)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0021 (7.8836)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.8615 (0.8615)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.5089 (2.5089 -- 2.5089)  data: 2.2454 (2.2454 -- 2.2454)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8599 (1.0120)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4306 (0.2026 -- 2.5089)  data: 0.2063 (0.0006 -- 2.2454)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7959 (0.9670)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (96.2963)  time: 0.2189 (0.1699 -- 0.3789)  data: 0.0107 (0.0001 -- 0.1870)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9362 (1.0126)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (95.8506)  time: 0.2051 (0.1336 -- 0.3789)  data: 0.0098 (0.0001 -- 0.1870)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 69.087 Acc@5 96.058 loss 0.963
Accuracy of the network on the 482 val images: 69.09%
[2023-08-30 23:44:52,765] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:44:52,767] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:44:52,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:44:52,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:44:54,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:44:54,286] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.09%
Epoch: [53]  [  0/160]  eta: 0:20:06  lr: 0.000020  min_lr: 0.000000  loss: 2.1070 (2.1070)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1673 (5.1673)  time: 7.5384 (7.5384 -- 7.5384)  data: 6.9372 (6.9372 -- 6.9372)  max mem: 16413
Epoch: [53]  [ 20/160]  eta: 0:02:35  lr: 0.000020  min_lr: 0.000000  loss: 1.9528 (1.9295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4707 (7.9260)  time: 0.7868 (0.5325 -- 3.0105)  data: 0.2283 (0.0005 -- 2.4704)  max mem: 16413
[2023-08-30 23:45:34,634] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:45:34,634] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:45:34,634] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:45:34,635] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [53]  [ 40/160]  eta: 0:02:09  lr: 0.000020  min_lr: 0.000000  loss: 1.9189 (1.9145)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2235 (7.9244)  time: 1.0515 (0.5214 -- 4.8123)  data: 0.5105 (0.0003 -- 4.2506)  max mem: 16413
[2023-08-30 23:45:39,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8522
[2023-08-30 23:45:39,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8522
[2023-08-30 23:45:39,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:45:39,727] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:45:39,727] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [53]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000000  loss: 2.0563 (1.9886)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2061 (7.7778)  time: 0.7393 (0.5152 -- 2.7511)  data: 0.1886 (0.0004 -- 2.2244)  max mem: 16413
Epoch: [53]  [ 80/160]  eta: 0:01:13  lr: 0.000020  min_lr: 0.000000  loss: 1.8687 (1.9596)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5204 (7.7715)  time: 0.7529 (0.5350 -- 2.3492)  data: 0.1752 (0.0002 -- 1.7944)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 2.0983 (1.9797)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4854 (7.7861)  time: 0.9609 (0.5450 -- 3.3970)  data: 0.1592 (0.0004 -- 1.6503)  max mem: 16413
Epoch: [53]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 2.0036 (1.9686)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9692 (7.6197)  time: 0.8641 (0.5251 -- 3.7404)  data: 0.0259 (0.0003 -- 0.2926)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:17  lr: 0.000020  min_lr: 0.000000  loss: 1.8264 (1.9578)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7725 (7.7018)  time: 0.7995 (0.5316 -- 4.0363)  data: 0.0013 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.0353 (1.9553)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0125 (7.8640)  time: 0.7542 (0.4956 -- 3.8414)  data: 0.0226 (0.0002 -- 0.4365)  max mem: 16413
Epoch: [53] Total time: 0:02:21 (0.8825 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.0353 (1.9317)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0125 (7.8640)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.7786 (0.7786)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4579 (2.4579 -- 2.4579)  data: 2.2250 (2.2250 -- 2.2250)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7786 (0.9975)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (94.9495)  time: 0.4385 (0.1958 -- 2.4579)  data: 0.2222 (0.0005 -- 2.2250)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7407 (0.9523)  acc1: 66.6667 (68.2540)  acc5: 100.0000 (95.7672)  time: 0.2234 (0.1692 -- 0.4208)  data: 0.0200 (0.0001 -- 0.2114)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9275 (1.0022)  acc1: 55.5556 (65.1452)  acc5: 100.0000 (95.4357)  time: 0.2067 (0.1324 -- 0.4208)  data: 0.0197 (0.0001 -- 0.2114)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 69.502 Acc@5 95.436 loss 0.956
Accuracy of the network on the 482 val images: 69.50%
[2023-08-30 23:47:23,406] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:47:23,408] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:47:23,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:47:23,408] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:47:24,744] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:47:24,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.50%
Epoch: [54]  [  0/160]  eta: 0:20:09  lr: 0.000020  min_lr: 0.000000  loss: 2.4073 (2.4073)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9955 (8.9955)  time: 7.5603 (7.5603 -- 7.5603)  data: 5.8457 (5.8457 -- 5.8457)  max mem: 16413
[2023-08-30 23:47:40,886] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:47:40,886] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:47:40,886] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:47:40,886] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:47:48,700] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8658
[2023-08-30 23:47:48,700] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8658
[2023-08-30 23:47:48,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:47:48,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:47:48,701] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [54]  [ 20/160]  eta: 0:02:46  lr: 0.000020  min_lr: 0.000000  loss: 1.9208 (1.8965)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6284 (8.4264)  time: 0.8737 (0.5281 -- 4.4226)  data: 0.1213 (0.0003 -- 1.5581)  max mem: 16413
Epoch: [54]  [ 40/160]  eta: 0:02:04  lr: 0.000020  min_lr: 0.000000  loss: 1.7977 (1.9073)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6313 (7.8309)  time: 0.8731 (0.5285 -- 3.2247)  data: 0.0116 (0.0006 -- 0.1913)  max mem: 16413
Epoch: [54]  [ 60/160]  eta: 0:01:38  lr: 0.000020  min_lr: 0.000000  loss: 1.9009 (1.9055)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6676 (8.0694)  time: 0.8770 (0.5177 -- 5.0473)  data: 0.3017 (0.0003 -- 4.5212)  max mem: 16413
Epoch: [54]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 2.0897 (1.9403)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3556 (8.1270)  time: 0.8134 (0.5290 -- 3.7949)  data: 0.2671 (0.0003 -- 3.2618)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 1.9797 (1.9366)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7173 (8.1941)  time: 0.9303 (0.5221 -- 4.3838)  data: 0.3868 (0.0005 -- 3.8372)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.9561 (1.9361)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6331 (8.3018)  time: 0.8451 (0.5352 -- 3.1583)  data: 0.2911 (0.0007 -- 2.6055)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:17  lr: 0.000020  min_lr: 0.000000  loss: 1.9584 (1.9338)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9879 (8.2034)  time: 0.7450 (0.5295 -- 2.6712)  data: 0.1917 (0.0004 -- 2.1395)  max mem: 16413
[2023-08-30 23:49:38,448] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:49:38,448] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:49:38,449] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:49:38,449] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.8231 (1.9290)  loss_scale: 32768.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7256 (8.1839)  time: 0.7258 (0.4941 -- 3.6866)  data: 0.2079 (0.0001 -- 3.1398)  max mem: 16413
Epoch: [54] Total time: 0:02:20 (0.8797 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.8231 (1.9317)  loss_scale: 32768.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7256 (8.1839)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.8253 (0.8253)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3852 (2.3852 -- 2.3852)  data: 2.1647 (2.1647 -- 2.1647)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8253 (1.0091)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (94.9495)  time: 0.4327 (0.2020 -- 2.3852)  data: 0.2202 (0.0004 -- 2.1647)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7345 (0.9551)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (95.7672)  time: 0.2227 (0.1691 -- 0.4648)  data: 0.0168 (0.0001 -- 0.2498)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9547 (1.0030)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (95.0207)  time: 0.2074 (0.1330 -- 0.4648)  data: 0.0165 (0.0001 -- 0.2498)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 69.295 Acc@5 95.228 loss 0.954
Accuracy of the network on the 482 val images: 69.29%
Max accuracy: 69.50%
Epoch: [55]  [  0/160]  eta: 0:21:15  lr: 0.000020  min_lr: 0.000000  loss: 1.6072 (1.6072)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0881 (10.0881)  time: 7.9704 (7.9704 -- 7.9704)  data: 5.5312 (5.5312 -- 5.5312)  max mem: 16413
[2023-08-30 23:50:16,428] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8818
[2023-08-30 23:50:16,429] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8818
[2023-08-30 23:50:16,429] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:50:16,429] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:50:16,429] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [55]  [ 20/160]  eta: 0:02:41  lr: 0.000020  min_lr: 0.000000  loss: 1.9662 (1.9994)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7575 (8.5471)  time: 0.8097 (0.5234 -- 3.2606)  data: 0.1234 (0.0004 -- 1.3050)  max mem: 16413
Epoch: [55]  [ 40/160]  eta: 0:01:59  lr: 0.000020  min_lr: 0.000000  loss: 2.0398 (2.0324)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3607 (8.1183)  time: 0.8344 (0.5157 -- 2.1981)  data: 0.2419 (0.0005 -- 1.6573)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:37  lr: 0.000020  min_lr: 0.000000  loss: 1.9660 (2.0030)  loss_scale: 16384.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5689 (8.1767)  time: 0.9405 (0.5279 -- 3.6965)  data: 0.3883 (0.0004 -- 3.1587)  max mem: 16413
Epoch: [55]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000000  loss: 1.8986 (1.9777)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2791 (8.1986)  time: 0.8009 (0.5190 -- 3.0439)  data: 0.2070 (0.0003 -- 2.5050)  max mem: 16413
Epoch: [55]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.9756 (1.9649)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0019 (8.3207)  time: 0.9029 (0.5352 -- 3.2155)  data: 0.3554 (0.0001 -- 2.6887)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.9805 (1.9635)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0947 (8.2292)  time: 0.8070 (0.5285 -- 2.2528)  data: 0.1225 (0.0006 -- 0.7454)  max mem: 16413
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 2.0744 (1.9728)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2823 (8.1868)  time: 0.9114 (0.5212 -- 3.3754)  data: 0.0014 (0.0002 -- 0.0045)  max mem: 16413
[2023-08-30 23:52:08,398] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:52:08,398] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:52:08,409] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:52:08,410] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 23:52:11,274] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8950
[2023-08-30 23:52:11,274] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:52:11,274] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8950
[2023-08-30 23:52:11,274] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 23:52:11,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 23:52:15,456] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8958
[2023-08-30 23:52:15,456] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 23:52:15,456] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8958
[2023-08-30 23:52:15,456] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 23:52:15,456] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9922 (1.9823)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7901 (8.1807)  time: 0.7509 (0.4933 -- 3.6476)  data: 0.0009 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [55] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9922 (1.9562)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7901 (8.1807)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.7897 (0.7897)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4661 (2.4661 -- 2.4661)  data: 2.2178 (2.2178 -- 2.2178)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8025 (1.0175)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4383 (0.1982 -- 2.4661)  data: 0.2203 (0.0005 -- 2.2178)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8025 (0.9575)  acc1: 66.6667 (67.1958)  acc5: 100.0000 (96.2963)  time: 0.2251 (0.1710 -- 0.4170)  data: 0.0217 (0.0001 -- 0.2247)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9257 (1.0097)  acc1: 66.6667 (65.1452)  acc5: 100.0000 (95.4357)  time: 0.2092 (0.1321 -- 0.4170)  data: 0.0214 (0.0000 -- 0.2247)  max mem: 16413
Val: Total time: 0:00:07 (0.2945 s / it)
* Acc@1 69.917 Acc@5 95.436 loss 0.957
Accuracy of the network on the 482 val images: 69.92%
[2023-08-30 23:52:23,907] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:52:23,909] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:52:23,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:52:23,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:52:25,184] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:52:25,184] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.92%
Epoch: [56]  [  0/160]  eta: 0:17:27  lr: 0.000020  min_lr: 0.000000  loss: 2.2108 (2.2108)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2148 (6.2148)  time: 6.5454 (6.5454 -- 6.5454)  data: 5.5742 (5.5742 -- 5.5742)  max mem: 16413
Epoch: [56]  [ 20/160]  eta: 0:02:40  lr: 0.000020  min_lr: 0.000000  loss: 1.9922 (2.0229)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3736 (10.5286)  time: 0.8760 (0.5230 -- 2.9952)  data: 0.1632 (0.0006 -- 2.3913)  max mem: 16413
[2023-08-30 23:53:04,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=51, lr=[4.6738112792318307e-07, 4.6738112792318307e-07, 6.231748372309107e-07, 6.231748372309107e-07, 8.308997829745477e-07, 8.308997829745477e-07, 1.1078663772993968e-06, 1.1078663772993968e-06, 1.4771551697325293e-06, 1.4771551697325293e-06, 1.969540226310039e-06, 1.969540226310039e-06, 2.626053635080052e-06, 2.626053635080052e-06, 3.5014048467734025e-06, 3.5014048467734025e-06, 4.66853979569787e-06, 4.66853979569787e-06, 6.22471972759716e-06, 6.22471972759716e-06, 8.299626303462881e-06, 8.299626303462881e-06, 1.1066168404617173e-05, 1.1066168404617173e-05, 1.475489120615623e-05, 1.475489120615623e-05, 1.9673188274874975e-05, 1.9673188274874975e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 23:53:04,610] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=17.11561885565679, CurrSamplesPerSec=22.4080835638772, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:01:58  lr: 0.000020  min_lr: 0.000000  loss: 1.6987 (1.9290)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5531 (9.3180)  time: 0.8244 (0.5254 -- 2.3661)  data: 0.2419 (0.0004 -- 1.8388)  max mem: 16413
Epoch: [56]  [ 60/160]  eta: 0:01:38  lr: 0.000020  min_lr: 0.000000  loss: 1.9174 (1.9102)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8658 (8.8592)  time: 0.9643 (0.5277 -- 3.7216)  data: 0.0053 (0.0006 -- 0.0756)  max mem: 16413
Epoch: [56]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 2.0017 (1.9357)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9393 (8.9148)  time: 0.8292 (0.5180 -- 3.4064)  data: 0.2300 (0.0003 -- 2.8708)  max mem: 16413
Epoch: [56]  [100/160]  eta: 0:00:54  lr: 0.000020  min_lr: 0.000000  loss: 1.7983 (1.9025)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4375 (8.9501)  time: 0.7874 (0.5246 -- 3.2372)  data: 0.1621 (0.0001 -- 2.7160)  max mem: 16413
Epoch: [56]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 2.0256 (1.9162)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0857 (8.8440)  time: 0.8846 (0.5214 -- 2.8585)  data: 0.0016 (0.0003 -- 0.0044)  max mem: 16413
[2023-08-30 23:54:22,735] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:54:22,735] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 23:54:22,736] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:54:22,736] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.8952 (1.9131)  loss_scale: 16384.0000 (9005.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5170 (8.6606)  time: 0.9128 (0.5176 -- 4.3957)  data: 0.0013 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9250 (1.9197)  loss_scale: 16384.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1714 (8.5516)  time: 0.6703 (0.4970 -- 2.0422)  data: 0.0010 (0.0002 -- 0.0042)  max mem: 16413
Epoch: [56] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9250 (1.9136)  loss_scale: 16384.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1714 (8.5516)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.7652 (0.7652)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4113 (2.4113 -- 2.4113)  data: 2.1734 (2.1734 -- 2.1734)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7652 (1.0009)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (96.9697)  time: 0.4445 (0.2010 -- 2.4113)  data: 0.2220 (0.0004 -- 2.1734)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7328 (0.9395)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (96.8254)  time: 0.2195 (0.1693 -- 0.4488)  data: 0.0142 (0.0001 -- 0.2449)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9083 (0.9926)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (95.8506)  time: 0.2013 (0.1325 -- 0.4488)  data: 0.0132 (0.0001 -- 0.2449)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 69.295 Acc@5 95.851 loss 0.943
Accuracy of the network on the 482 val images: 69.29%
Max accuracy: 69.92%
Epoch: [57]  [  0/160]  eta: 0:19:53  lr: 0.000020  min_lr: 0.000000  loss: 2.2015 (2.2015)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1149 (8.1149)  time: 7.4624 (7.4624 -- 7.4624)  data: 6.9331 (6.9331 -- 6.9331)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:46  lr: 0.000020  min_lr: 0.000000  loss: 2.0125 (1.9988)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7343 (8.6460)  time: 0.8743 (0.5227 -- 2.6665)  data: 0.3110 (0.0006 -- 2.1098)  max mem: 16413
Epoch: [57]  [ 40/160]  eta: 0:02:13  lr: 0.000020  min_lr: 0.000000  loss: 1.7747 (1.8794)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8582 (8.6199)  time: 1.0387 (0.5280 -- 4.9976)  data: 0.4862 (0.0002 -- 4.4694)  max mem: 16413
Epoch: [57]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 1.9233 (1.8784)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5292 (8.3892)  time: 0.6358 (0.5194 -- 1.9195)  data: 0.0904 (0.0002 -- 1.3954)  max mem: 16413
[2023-08-30 23:55:54,072] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9183
[2023-08-30 23:55:54,072] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 23:55:54,072] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9183
[2023-08-30 23:55:54,072] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 23:55:54,072] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [57]  [ 80/160]  eta: 0:01:17  lr: 0.000019  min_lr: 0.000000  loss: 1.8406 (1.8903)  loss_scale: 8192.0000 (14563.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9376 (8.7096)  time: 1.0031 (0.5243 -- 4.0662)  data: 0.3969 (0.0002 -- 3.5030)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 2.0368 (1.9026)  loss_scale: 8192.0000 (13301.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3850 (8.5604)  time: 0.8153 (0.5116 -- 3.6212)  data: 0.2567 (0.0004 -- 3.1129)  max mem: 16413
Epoch: [57]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 1.9522 (1.8990)  loss_scale: 8192.0000 (12457.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1044 (8.5132)  time: 1.0062 (0.5184 -- 4.0772)  data: 0.4608 (0.0004 -- 3.5641)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 2.0120 (1.9122)  loss_scale: 8192.0000 (11852.2553)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5305 (8.4658)  time: 0.7577 (0.5203 -- 2.1778)  data: 0.0922 (0.0002 -- 1.6475)  max mem: 16413
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.0013 (1.9209)  loss_scale: 8192.0000 (11417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5238 (8.4415)  time: 0.6589 (0.4956 -- 1.8439)  data: 0.0990 (0.0002 -- 1.2724)  max mem: 16413
Epoch: [57] Total time: 0:02:22 (0.8919 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.0013 (1.9550)  loss_scale: 8192.0000 (11417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5238 (8.4415)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.7224 (0.7224)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5544 (2.5544 -- 2.5544)  data: 2.2907 (2.2907 -- 2.2907)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7224 (0.9887)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (96.9697)  time: 0.4455 (0.1996 -- 2.5544)  data: 0.2328 (0.0007 -- 2.2907)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7224 (0.9280)  acc1: 66.6667 (71.4286)  acc5: 100.0000 (96.8254)  time: 0.2208 (0.1701 -- 0.4797)  data: 0.0197 (0.0001 -- 0.2578)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9003 (0.9847)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (95.8506)  time: 0.2070 (0.1327 -- 0.4797)  data: 0.0193 (0.0001 -- 0.2578)  max mem: 16413
Val: Total time: 0:00:07 (0.2946 s / it)
* Acc@1 71.162 Acc@5 96.058 loss 0.936
Accuracy of the network on the 482 val images: 71.16%
[2023-08-30 23:57:24,662] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:57:24,664] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:57:24,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:57:24,664] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:57:26,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:57:26,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.16%
Epoch: [58]  [  0/160]  eta: 0:23:45  lr: 0.000019  min_lr: 0.000000  loss: 1.9054 (1.9054)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8916 (7.8916)  time: 8.9094 (8.9094 -- 8.9094)  data: 5.7138 (5.7138 -- 5.7138)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:47  lr: 0.000019  min_lr: 0.000000  loss: 1.6994 (1.8003)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7318 (7.4667)  time: 0.8118 (0.5341 -- 3.6739)  data: 0.0016 (0.0003 -- 0.0031)  max mem: 16413
[2023-08-30 23:58:03,384] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:58:03,384] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 23:58:03,385] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 23:58:03,385] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [58]  [ 40/160]  eta: 0:02:09  lr: 0.000019  min_lr: 0.000000  loss: 1.9994 (1.8706)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6324 (7.5148)  time: 0.9617 (0.5289 -- 4.7324)  data: 0.0067 (0.0005 -- 0.0970)  max mem: 16413
Epoch: [58]  [ 60/160]  eta: 0:01:42  lr: 0.000019  min_lr: 0.000000  loss: 1.9701 (1.8718)  loss_scale: 16384.0000 (12086.5574)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3046 (7.5847)  time: 0.9098 (0.5157 -- 6.0843)  data: 0.0013 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [58]  [ 80/160]  eta: 0:01:20  lr: 0.000019  min_lr: 0.000000  loss: 1.8813 (1.8800)  loss_scale: 16384.0000 (13147.6543)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2220 (7.9234)  time: 0.9333 (0.5272 -- 4.1955)  data: 0.0020 (0.0003 -- 0.0099)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:57  lr: 0.000019  min_lr: 0.000000  loss: 2.1750 (1.9217)  loss_scale: 16384.0000 (13788.5149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2332 (8.0023)  time: 0.8106 (0.5017 -- 3.5526)  data: 0.0008 (0.0001 -- 0.0021)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 2.0432 (1.9540)  loss_scale: 16384.0000 (14217.5207)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0328 (8.0035)  time: 0.8256 (0.5277 -- 2.8416)  data: 0.0028 (0.0004 -- 0.0283)  max mem: 16413
Epoch: [58]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 2.0440 (1.9578)  loss_scale: 16384.0000 (14524.8227)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6223 (7.9480)  time: 0.8307 (0.5228 -- 3.5353)  data: 0.0015 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9453 (1.9557)  loss_scale: 16384.0000 (14745.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3268 (7.9229)  time: 0.6643 (0.4961 -- 2.8289)  data: 0.0007 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [58] Total time: 0:02:23 (0.8957 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.9453 (1.9530)  loss_scale: 16384.0000 (14745.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3268 (7.9229)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.7789 (0.7789)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.5754 (2.5754 -- 2.5754)  data: 2.3403 (2.3403 -- 2.3403)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7766 (0.9809)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (96.9697)  time: 0.4364 (0.2076 -- 2.5754)  data: 0.2146 (0.0007 -- 2.3403)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7057 (0.9271)  acc1: 66.6667 (69.8413)  acc5: 100.0000 (96.8254)  time: 0.2146 (0.1689 -- 0.2926)  data: 0.0064 (0.0001 -- 0.1058)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8771 (0.9774)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (96.2656)  time: 0.1948 (0.1328 -- 0.2926)  data: 0.0059 (0.0001 -- 0.1058)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 71.369 Acc@5 96.058 loss 0.926
Accuracy of the network on the 482 val images: 71.37%
[2023-08-30 23:59:57,208] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 23:59:57,210] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 23:59:57,210] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 23:59:57,210] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 23:59:58,601] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 23:59:58,602] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.37%
[2023-08-31 00:00:06,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:00:06,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:00:06,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:00:06,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [59]  [  0/160]  eta: 0:20:55  lr: 0.000019  min_lr: 0.000000  loss: 1.8058 (1.8058)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6738 (7.6738)  time: 7.8466 (7.8466 -- 7.8466)  data: 6.2341 (6.2341 -- 6.2341)  max mem: 16413
Epoch: [59]  [ 20/160]  eta: 0:02:39  lr: 0.000019  min_lr: 0.000000  loss: 1.8259 (1.9238)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2413 (7.4373)  time: 0.8015 (0.5327 -- 3.7567)  data: 0.0739 (0.0004 -- 1.4484)  max mem: 16413
[2023-08-31 00:00:27,633] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9464
[2023-08-31 00:00:27,633] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9464
[2023-08-31 00:00:27,633] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:00:27,633] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:00:27,633] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [59]  [ 40/160]  eta: 0:02:06  lr: 0.000019  min_lr: 0.000000  loss: 2.0261 (1.9856)  loss_scale: 16384.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0063 (7.7591)  time: 0.9607 (0.5225 -- 4.0562)  data: 0.3411 (0.0003 -- 3.5205)  max mem: 16413
Epoch: [59]  [ 60/160]  eta: 0:01:34  lr: 0.000019  min_lr: 0.000000  loss: 1.9523 (1.9782)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5549 (7.7354)  time: 0.7313 (0.5215 -- 2.2406)  data: 0.1196 (0.0006 -- 1.7100)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:14  lr: 0.000019  min_lr: 0.000000  loss: 1.7156 (1.9150)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1056 (7.8493)  time: 0.8709 (0.5322 -- 3.6685)  data: 0.0270 (0.0003 -- 0.5134)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 2.0134 (1.9430)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3529 (7.8422)  time: 0.9623 (0.5344 -- 3.7842)  data: 0.0130 (0.0004 -- 0.2333)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.9354 (1.9262)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1997 (7.9075)  time: 0.7733 (0.5230 -- 2.8820)  data: 0.0015 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [59]  [140/160]  eta: 0:00:17  lr: 0.000019  min_lr: 0.000000  loss: 1.8888 (1.9238)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8544 (7.9018)  time: 0.8242 (0.5262 -- 3.0466)  data: 0.0016 (0.0003 -- 0.0031)  max mem: 16413
[2023-08-31 00:02:16,352] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:02:16,352] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:02:16,352] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:02:16,352] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:02:18,936] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9597
[2023-08-31 00:02:18,936] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:02:18,936] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 00:02:18,936] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9597
[2023-08-31 00:02:18,936] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.8367 (1.9177)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3843 (7.9374)  time: 0.7767 (0.4947 -- 2.7804)  data: 0.0035 (0.0002 -- 0.0550)  max mem: 16413
Epoch: [59] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.8367 (1.9080)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3843 (7.9374)
[2023-08-31 00:02:19,984] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-08-31 00:02:19,987] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-08-31 00:02:19,989] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-08-31 00:02:19,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-08-31 00:02:21,089] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-08-31 00:02:21,089] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.7784 (0.7784)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3544 (2.3544 -- 2.3544)  data: 2.1259 (2.1259 -- 2.1259)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7784 (0.9702)  acc1: 77.7778 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4311 (0.2069 -- 2.3544)  data: 0.2099 (0.0005 -- 2.1259)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7410 (0.9139)  acc1: 66.6667 (69.8413)  acc5: 100.0000 (96.2963)  time: 0.2243 (0.1721 -- 0.4008)  data: 0.0177 (0.0001 -- 0.1752)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8587 (0.9682)  acc1: 66.6667 (68.0498)  acc5: 100.0000 (95.8506)  time: 0.2064 (0.1324 -- 0.4008)  data: 0.0174 (0.0001 -- 0.1752)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 71.992 Acc@5 95.851 loss 0.918
Accuracy of the network on the 482 val images: 71.99%
[2023-08-31 00:02:28,928] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:02:28,930] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:02:28,930] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:02:28,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:02:30,288] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:02:30,288] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.99%
Epoch: [60]  [  0/160]  eta: 0:23:12  lr: 0.000019  min_lr: 0.000000  loss: 2.1941 (2.1941)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0390 (11.0390)  time: 8.7031 (8.7031 -- 8.7031)  data: 8.1777 (8.1777 -- 8.1777)  max mem: 16413
Epoch: [60]  [ 20/160]  eta: 0:02:42  lr: 0.000019  min_lr: 0.000000  loss: 1.7324 (1.8773)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5317 (8.2456)  time: 0.7822 (0.5268 -- 2.7899)  data: 0.2344 (0.0004 -- 2.2563)  max mem: 16413
Epoch: [60]  [ 40/160]  eta: 0:02:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9150 (1.9113)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3876 (7.8904)  time: 0.8444 (0.5311 -- 2.1746)  data: 0.2289 (0.0003 -- 1.6386)  max mem: 16413
Epoch: [60]  [ 60/160]  eta: 0:01:35  lr: 0.000019  min_lr: 0.000000  loss: 1.9295 (1.9169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6875 (8.1559)  time: 0.8389 (0.5326 -- 2.4423)  data: 0.2633 (0.0008 -- 1.8963)  max mem: 16413
Epoch: [60]  [ 80/160]  eta: 0:01:13  lr: 0.000019  min_lr: 0.000000  loss: 1.9331 (1.9342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7162 (8.1294)  time: 0.8444 (0.5198 -- 2.8741)  data: 0.0854 (0.0005 -- 1.0881)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.9547 (1.9396)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6013 (8.0817)  time: 0.9504 (0.5266 -- 3.6031)  data: 0.3155 (0.0004 -- 3.0731)  max mem: 16413
Epoch: [60]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 2.0108 (1.9352)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6839 (8.2002)  time: 0.9042 (0.5167 -- 4.9807)  data: 0.3602 (0.0002 -- 4.4612)  max mem: 16413
[2023-08-31 00:04:26,683] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:04:26,683] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:04:26,684] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:04:26,685] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.9932 (1.9512)  loss_scale: 32768.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0717 (8.2237)  time: 0.8640 (0.5267 -- 3.5135)  data: 0.3137 (0.0004 -- 2.9576)  max mem: 16413
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.8836 (1.9448)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7632 (8.1093)  time: 0.6428 (0.4968 -- 1.8735)  data: 0.0934 (0.0002 -- 1.3523)  max mem: 16413
Epoch: [60] Total time: 0:02:21 (0.8850 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.8836 (1.9314)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7632 (8.1093)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.7197 (0.7197)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4376 (2.4376 -- 2.4376)  data: 2.2302 (2.2302 -- 2.2302)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7197 (0.9655)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4514 (0.2051 -- 2.4376)  data: 0.2383 (0.0004 -- 2.2302)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6949 (0.9118)  acc1: 66.6667 (70.3704)  acc5: 100.0000 (96.2963)  time: 0.2229 (0.1719 -- 0.6016)  data: 0.0197 (0.0001 -- 0.3822)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9003 (0.9642)  acc1: 66.6667 (68.0498)  acc5: 100.0000 (95.8506)  time: 0.2079 (0.1325 -- 0.6016)  data: 0.0195 (0.0001 -- 0.3822)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 71.784 Acc@5 96.058 loss 0.912
Accuracy of the network on the 482 val images: 71.78%
Max accuracy: 71.99%
Epoch: [61]  [  0/160]  eta: 0:19:18  lr: 0.000019  min_lr: 0.000000  loss: 2.2084 (2.2084)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6787 (7.6787)  time: 7.2395 (7.2395 -- 7.2395)  data: 5.5541 (5.5541 -- 5.5541)  max mem: 16413
[2023-08-31 00:05:25,835] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9780
[2023-08-31 00:05:25,835] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9780
[2023-08-31 00:05:25,835] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:05:25,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 00:05:25,835] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [61]  [ 20/160]  eta: 0:02:53  lr: 0.000019  min_lr: 0.000000  loss: 1.9263 (1.8896)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5176 (8.6629)  time: 0.9405 (0.5049 -- 4.9636)  data: 0.0376 (0.0005 -- 0.7257)  max mem: 16413
Epoch: [61]  [ 40/160]  eta: 0:02:10  lr: 0.000019  min_lr: 0.000000  loss: 1.8816 (1.8871)  loss_scale: 16384.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6268 (8.1538)  time: 0.9187 (0.5181 -- 2.7688)  data: 0.0014 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [61]  [ 60/160]  eta: 0:01:39  lr: 0.000019  min_lr: 0.000000  loss: 1.8561 (1.8976)  loss_scale: 16384.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5688 (8.0961)  time: 0.7989 (0.5285 -- 2.9552)  data: 0.0016 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [61]  [ 80/160]  eta: 0:01:18  lr: 0.000019  min_lr: 0.000000  loss: 1.7788 (1.8763)  loss_scale: 16384.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7630 (7.9213)  time: 0.9710 (0.5209 -- 4.0696)  data: 0.0011 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 1.8798 (1.8918)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7108 (7.9527)  time: 0.7559 (0.5138 -- 2.8999)  data: 0.0020 (0.0004 -- 0.0103)  max mem: 16413
[2023-08-31 00:06:35,241] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9861
[2023-08-31 00:06:35,241] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:06:35,244] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9861
[2023-08-31 00:06:35,244] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:06:35,245] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 1.8578 (1.8977)  loss_scale: 8192.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1348 (7.9130)  time: 0.9492 (0.5016 -- 4.1988)  data: 0.0020 (0.0002 -- 0.0131)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.9800 (1.9116)  loss_scale: 8192.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4937 (7.9780)  time: 0.8068 (0.5246 -- 3.5911)  data: 0.0016 (0.0001 -- 0.0029)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9779 (1.9185)  loss_scale: 8192.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4647 (7.9237)  time: 0.6555 (0.4947 -- 2.6712)  data: 0.0006 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [61] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.9779 (1.9375)  loss_scale: 8192.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4647 (7.9237)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.7196 (0.7196)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.4954 (2.4954 -- 2.4954)  data: 2.2384 (2.2384 -- 2.2384)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7196 (0.9650)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4244 (0.1998 -- 2.4954)  data: 0.2044 (0.0004 -- 2.2384)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7049 (0.9103)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (96.2963)  time: 0.2112 (0.1692 -- 0.2520)  data: 0.0058 (0.0001 -- 0.0523)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8908 (0.9647)  acc1: 66.6667 (66.8050)  acc5: 100.0000 (95.8506)  time: 0.1946 (0.1377 -- 0.2520)  data: 0.0055 (0.0001 -- 0.0523)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 71.784 Acc@5 95.851 loss 0.908
Accuracy of the network on the 482 val images: 71.78%
Max accuracy: 71.99%
Epoch: [62]  [  0/160]  eta: 0:20:11  lr: 0.000019  min_lr: 0.000000  loss: 2.0660 (2.0660)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8626 (5.8626)  time: 7.5733 (7.5733 -- 7.5733)  data: 7.0267 (7.0267 -- 7.0267)  max mem: 16413
Epoch: [62]  [ 20/160]  eta: 0:02:38  lr: 0.000019  min_lr: 0.000000  loss: 2.0280 (1.9992)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5515 (7.7132)  time: 0.8088 (0.5264 -- 3.0301)  data: 0.2430 (0.0004 -- 2.2768)  max mem: 16413
Epoch: [62]  [ 40/160]  eta: 0:02:08  lr: 0.000019  min_lr: 0.000000  loss: 1.8189 (1.9274)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3879 (8.0885)  time: 1.0084 (0.5338 -- 3.9786)  data: 0.4591 (0.0005 -- 3.4550)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:39  lr: 0.000019  min_lr: 0.000000  loss: 1.9526 (1.9559)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5822 (8.1304)  time: 0.8282 (0.5092 -- 3.9811)  data: 0.2876 (0.0004 -- 3.4615)  max mem: 16413
[2023-08-31 00:08:38,034] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:08:38,034] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 00:08:38,034] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:08:38,034] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 00:08:45,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=56, lr=[4.4590002707514483e-07, 4.4590002707514483e-07, 5.945333694335264e-07, 5.945333694335264e-07, 7.927111592447019e-07, 7.927111592447019e-07, 1.0569482123262692e-06, 1.0569482123262692e-06, 1.4092642831016922e-06, 1.4092642831016922e-06, 1.8790190441355896e-06, 1.8790190441355896e-06, 2.5053587255141196e-06, 2.5053587255141196e-06, 3.340478300685493e-06, 3.340478300685493e-06, 4.4539710675806566e-06, 4.4539710675806566e-06, 5.938628090107543e-06, 5.938628090107543e-06, 7.918170786810056e-06, 7.918170786810056e-06, 1.0557561049080076e-05, 1.0557561049080076e-05, 1.40767480654401e-05, 1.40767480654401e-05, 1.87689974205868e-05, 1.87689974205868e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 00:08:45,644] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=17.094942526565962, CurrSamplesPerSec=22.576461859902235, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:19  lr: 0.000019  min_lr: 0.000000  loss: 1.8689 (1.9389)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4984 (8.1757)  time: 1.0085 (0.5296 -- 5.1810)  data: 0.2098 (0.0002 -- 1.8558)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:57  lr: 0.000019  min_lr: 0.000000  loss: 1.8634 (1.9282)  loss_scale: 16384.0000 (10706.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6238 (8.3100)  time: 0.7960 (0.5172 -- 4.2680)  data: 0.0012 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 2.0470 (1.9388)  loss_scale: 16384.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2393 (8.3641)  time: 0.8988 (0.5329 -- 4.5092)  data: 0.0019 (0.0002 -- 0.0084)  max mem: 16413
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8832 (1.9352)  loss_scale: 16384.0000 (12317.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5875 (8.3008)  time: 0.7667 (0.5207 -- 4.1043)  data: 0.0017 (0.0003 -- 0.0060)  max mem: 16413
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9549 (1.9304)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5811 (8.2736)  time: 0.6262 (0.4956 -- 1.7875)  data: 0.0010 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [62] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.9549 (1.9072)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5811 (8.2736)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6582 (0.6582)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3955 (2.3955 -- 2.3955)  data: 2.1566 (2.1566 -- 2.1566)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6780 (0.9542)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (95.9596)  time: 0.4347 (0.1915 -- 2.3955)  data: 0.2215 (0.0005 -- 2.1566)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6941 (0.9059)  acc1: 66.6667 (69.8413)  acc5: 100.0000 (96.2963)  time: 0.2267 (0.1704 -- 0.4767)  data: 0.0237 (0.0001 -- 0.2713)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8674 (0.9572)  acc1: 66.6667 (66.8050)  acc5: 100.0000 (95.8506)  time: 0.2125 (0.1331 -- 0.4767)  data: 0.0235 (0.0001 -- 0.2713)  max mem: 16413
Val: Total time: 0:00:07 (0.2932 s / it)
* Acc@1 72.199 Acc@5 96.058 loss 0.903
Accuracy of the network on the 482 val images: 72.20%
[2023-08-31 00:09:59,956] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:09:59,958] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:09:59,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:09:59,958] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:10:01,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:10:01,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 72.20%
Epoch: [63]  [  0/160]  eta: 0:19:21  lr: 0.000019  min_lr: 0.000000  loss: 2.0591 (2.0591)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4999 (5.4999)  time: 7.2565 (7.2565 -- 7.2565)  data: 5.8217 (5.8217 -- 5.8217)  max mem: 16413
Epoch: [63]  [ 20/160]  eta: 0:03:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9228 (1.8524)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6530 (7.9301)  time: 0.9878 (0.5297 -- 4.5787)  data: 0.0200 (0.0003 -- 0.2372)  max mem: 16413
[2023-08-31 00:10:42,899] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:10:42,899] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:10:42,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:10:42,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [63]  [ 40/160]  eta: 0:02:04  lr: 0.000019  min_lr: 0.000000  loss: 1.8962 (1.9123)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2644 (7.8446)  time: 0.7815 (0.5195 -- 3.1924)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [63]  [ 60/160]  eta: 0:01:39  lr: 0.000019  min_lr: 0.000000  loss: 1.9478 (1.9123)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5881 (7.7072)  time: 0.8892 (0.5248 -- 3.7903)  data: 0.0014 (0.0003 -- 0.0038)  max mem: 16413
[2023-08-31 00:11:04,052] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10144
[2023-08-31 00:11:04,052] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:11:04,052] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10144
[2023-08-31 00:11:04,053] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:11:04,053] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [63]  [ 80/160]  eta: 0:01:14  lr: 0.000019  min_lr: 0.000000  loss: 1.8255 (1.9076)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0840 (7.9910)  time: 0.7696 (0.5263 -- 3.4804)  data: 0.0024 (0.0005 -- 0.0126)  max mem: 16413
Epoch: [63]  [100/160]  eta: 0:00:57  lr: 0.000019  min_lr: 0.000000  loss: 1.9748 (1.9194)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2548 (7.8796)  time: 1.0135 (0.5146 -- 4.3095)  data: 0.0013 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [63]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 1.9042 (1.9303)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8284 (7.8362)  time: 0.8541 (0.5018 -- 4.5301)  data: 0.0011 (0.0003 -- 0.0056)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 2.0143 (1.9379)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9743 (7.9460)  time: 0.8157 (0.5231 -- 3.6620)  data: 0.0014 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9140 (1.9321)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3658 (8.1420)  time: 0.6901 (0.4958 -- 2.4982)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [63] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.9140 (1.9325)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3658 (8.1420)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.6919 (0.6919)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.6696 (2.6696 -- 2.6696)  data: 2.4245 (2.4245 -- 2.4245)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6919 (0.9610)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4513 (0.2009 -- 2.6696)  data: 0.2310 (0.0007 -- 2.4245)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6794 (0.9020)  acc1: 66.6667 (69.8413)  acc5: 100.0000 (96.2963)  time: 0.2117 (0.1694 -- 0.3239)  data: 0.0082 (0.0001 -- 0.1070)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8441 (0.9543)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (95.4357)  time: 0.1964 (0.1325 -- 0.3239)  data: 0.0079 (0.0001 -- 0.1070)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 71.992 Acc@5 95.436 loss 0.900
Accuracy of the network on the 482 val images: 71.99%
Max accuracy: 72.20%
Epoch: [64]  [  0/160]  eta: 0:20:04  lr: 0.000019  min_lr: 0.000000  loss: 1.7295 (1.7295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7375 (8.7375)  time: 7.5288 (7.5288 -- 7.5288)  data: 6.9641 (6.9641 -- 6.9641)  max mem: 16413
Epoch: [64]  [ 20/160]  eta: 0:02:44  lr: 0.000019  min_lr: 0.000000  loss: 1.9929 (1.9909)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7544 (8.1311)  time: 0.8540 (0.5245 -- 3.9444)  data: 0.3047 (0.0004 -- 3.4297)  max mem: 16413
[2023-08-31 00:13:07,215] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:13:07,215] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:13:07,218] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:13:07,218] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [64]  [ 40/160]  eta: 0:02:02  lr: 0.000019  min_lr: 0.000000  loss: 1.8990 (1.9576)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0199 (7.9686)  time: 0.8685 (0.5427 -- 2.9718)  data: 0.3111 (0.0004 -- 2.4171)  max mem: 16413
[2023-08-31 00:13:14,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10281
[2023-08-31 00:13:14,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10281
[2023-08-31 00:13:14,500] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:13:14,501] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:13:14,501] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [64]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 1.8565 (1.9173)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3463 (7.9164)  time: 0.9191 (0.5229 -- 3.2363)  data: 0.3691 (0.0004 -- 2.6925)  max mem: 16413
Epoch: [64]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 1.8723 (1.9105)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1903 (8.2085)  time: 0.8110 (0.5159 -- 2.8355)  data: 0.2325 (0.0006 -- 2.2873)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000000  loss: 1.9781 (1.9271)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9980 (8.1808)  time: 0.9160 (0.5145 -- 5.4622)  data: 0.3723 (0.0003 -- 4.9471)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 2.0163 (1.9424)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4865 (8.0486)  time: 0.8074 (0.5403 -- 3.9271)  data: 0.2493 (0.0004 -- 3.4013)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.8046 (1.9267)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3654 (7.8599)  time: 0.8951 (0.5270 -- 4.5203)  data: 0.2962 (0.0003 -- 3.9754)  max mem: 16413
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8647 (1.9193)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8609 (7.9295)  time: 0.6564 (0.4964 -- 2.3337)  data: 0.0450 (0.0002 -- 0.8849)  max mem: 16413
Epoch: [64] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8647 (1.9256)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8609 (7.9295)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6904 (0.6904)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4474 (2.4474 -- 2.4474)  data: 2.2354 (2.2354 -- 2.2354)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6904 (0.9551)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (94.9495)  time: 0.4587 (0.2077 -- 2.4474)  data: 0.2428 (0.0004 -- 2.2354)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7086 (0.9046)  acc1: 66.6667 (69.8413)  acc5: 100.0000 (95.7672)  time: 0.2292 (0.1696 -- 0.6366)  data: 0.0219 (0.0001 -- 0.4265)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8537 (0.9563)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (95.4357)  time: 0.2130 (0.1330 -- 0.6366)  data: 0.0216 (0.0001 -- 0.4265)  max mem: 16413
Val: Total time: 0:00:08 (0.2985 s / it)
* Acc@1 71.992 Acc@5 95.851 loss 0.901
Accuracy of the network on the 482 val images: 71.99%
Max accuracy: 72.20%
Epoch: [65]  [  0/160]  eta: 0:18:22  lr: 0.000018  min_lr: 0.000000  loss: 1.7708 (1.7708)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8634 (7.8634)  time: 6.8925 (6.8925 -- 6.8925)  data: 4.7461 (4.7461 -- 4.7461)  max mem: 16413
[2023-08-31 00:15:17,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:15:17,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:15:17,521] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:15:17,521] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [65]  [ 20/160]  eta: 0:02:35  lr: 0.000018  min_lr: 0.000000  loss: 1.8133 (1.8577)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7483 (7.6908)  time: 0.8216 (0.5303 -- 3.5761)  data: 0.0098 (0.0003 -- 0.1496)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:01  lr: 0.000018  min_lr: 0.000000  loss: 1.6656 (1.7681)  loss_scale: 32768.0000 (28771.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8780 (7.6332)  time: 0.9012 (0.5206 -- 3.0710)  data: 0.0482 (0.0003 -- 0.9254)  max mem: 16413
[2023-08-31 00:15:45,776] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10442
[2023-08-31 00:15:45,776] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10442
[2023-08-31 00:15:45,777] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:15:45,777] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:15:45,777] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [65]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 1.9022 (1.7901)  loss_scale: 16384.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4204 (7.7093)  time: 0.9317 (0.5224 -- 2.7568)  data: 0.0163 (0.0003 -- 0.3004)  max mem: 16413
Epoch: [65]  [ 80/160]  eta: 0:01:13  lr: 0.000018  min_lr: 0.000000  loss: 2.0520 (1.8415)  loss_scale: 16384.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6642 (7.7418)  time: 0.6997 (0.5286 -- 2.2561)  data: 0.0295 (0.0003 -- 0.5563)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.7770 (1.8422)  loss_scale: 16384.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2524 (7.8276)  time: 0.9873 (0.5285 -- 2.9316)  data: 0.1301 (0.0005 -- 1.0117)  max mem: 16413
Epoch: [65]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.9381 (1.8627)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1134 (7.8760)  time: 0.8601 (0.5354 -- 2.7868)  data: 0.1626 (0.0002 -- 2.2342)  max mem: 16413
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.7706 (1.8620)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0684 (8.1038)  time: 0.8020 (0.5246 -- 2.5423)  data: 0.1204 (0.0004 -- 1.4112)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 2.0125 (1.8727)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3893 (8.1133)  time: 0.7690 (0.4977 -- 3.4562)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [65] Total time: 0:02:21 (0.8864 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 2.0125 (1.9045)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3893 (8.1133)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.6567 (0.6567)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5512 (2.5512 -- 2.5512)  data: 2.3253 (2.3253 -- 2.3253)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6887 (0.9442)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (94.9495)  time: 0.4295 (0.1929 -- 2.5512)  data: 0.2126 (0.0008 -- 2.3253)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6887 (0.8838)  acc1: 66.6667 (70.3704)  acc5: 100.0000 (95.7672)  time: 0.2193 (0.1695 -- 0.4336)  data: 0.0128 (0.0001 -- 0.2414)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8463 (0.9405)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (95.4357)  time: 0.2043 (0.1329 -- 0.4336)  data: 0.0124 (0.0001 -- 0.2414)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 71.992 Acc@5 95.851 loss 0.887
Accuracy of the network on the 482 val images: 71.99%
Max accuracy: 72.20%
Epoch: [66]  [  0/160]  eta: 0:16:04  lr: 0.000018  min_lr: 0.000000  loss: 1.3902 (1.3902)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4448 (9.4448)  time: 6.0254 (6.0254 -- 6.0254)  data: 5.3501 (5.3501 -- 5.3501)  max mem: 16413
[2023-08-31 00:17:47,741] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:17:47,741] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:17:47,741] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:17:47,741] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [66]  [ 20/160]  eta: 0:02:36  lr: 0.000018  min_lr: 0.000000  loss: 1.9403 (1.8771)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7793 (7.3925)  time: 0.8712 (0.5312 -- 3.0345)  data: 0.3146 (0.0004 -- 2.4881)  max mem: 16413
Epoch: [66]  [ 40/160]  eta: 0:01:57  lr: 0.000018  min_lr: 0.000000  loss: 1.7974 (1.8483)  loss_scale: 32768.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7675 (7.7533)  time: 0.8305 (0.5326 -- 2.8410)  data: 0.2365 (0.0002 -- 1.4560)  max mem: 16413
[2023-08-31 00:18:31,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10620
[2023-08-31 00:18:31,353] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:18:31,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10620
[2023-08-31 00:18:31,353] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:18:31,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 1.8442 (1.8505)  loss_scale: 32768.0000 (29544.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4212 (7.9284)  time: 0.9962 (0.5298 -- 3.7443)  data: 0.4079 (0.0005 -- 2.7477)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:14  lr: 0.000018  min_lr: 0.000000  loss: 2.1255 (1.8964)  loss_scale: 16384.0000 (26295.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3379 (8.0852)  time: 0.7486 (0.5162 -- 2.2556)  data: 0.1956 (0.0006 -- 1.7507)  max mem: 16413
Epoch: [66]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.8436 (1.8924)  loss_scale: 16384.0000 (24332.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9438 (7.9689)  time: 0.9288 (0.5231 -- 3.6824)  data: 0.3749 (0.0004 -- 3.1439)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.9758 (1.9053)  loss_scale: 16384.0000 (23018.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7327 (7.9622)  time: 0.8619 (0.5106 -- 3.9725)  data: 0.3150 (0.0003 -- 3.4490)  max mem: 16413
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.8722 (1.9012)  loss_scale: 16384.0000 (22077.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3798 (7.9765)  time: 0.9673 (0.5180 -- 3.9560)  data: 0.4230 (0.0004 -- 3.4301)  max mem: 16413
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7770 (1.8912)  loss_scale: 16384.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2415 (7.9811)  time: 0.5747 (0.4945 -- 1.4698)  data: 0.0474 (0.0002 -- 0.9320)  max mem: 16413
Epoch: [66] Total time: 0:02:21 (0.8819 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7770 (1.9218)  loss_scale: 16384.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2415 (7.9811)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.7179 (0.7179)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4803 (2.4803 -- 2.4803)  data: 2.2754 (2.2754 -- 2.2754)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7179 (0.9502)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (93.9394)  time: 0.4370 (0.1996 -- 2.4803)  data: 0.2204 (0.0004 -- 2.2754)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7773 (0.8941)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (95.2381)  time: 0.2233 (0.1692 -- 0.3780)  data: 0.0175 (0.0001 -- 0.1980)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8113 (0.9496)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (95.0207)  time: 0.2077 (0.1322 -- 0.3780)  data: 0.0171 (0.0001 -- 0.1980)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 74.481 Acc@5 95.228 loss 0.896
Accuracy of the network on the 482 val images: 74.48%
[2023-08-31 00:20:00,404] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:20:00,406] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:20:00,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:20:00,406] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:20:01,759] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:20:01,759] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.48%
Epoch: [67]  [  0/160]  eta: 0:19:53  lr: 0.000018  min_lr: 0.000000  loss: 2.0558 (2.0558)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3450 (5.3450)  time: 7.4599 (7.4599 -- 7.4599)  data: 6.4261 (6.4261 -- 6.4261)  max mem: 16413
[2023-08-31 00:20:17,781] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10728
[2023-08-31 00:20:17,781] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:20:17,781] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10728
[2023-08-31 00:20:17,781] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:20:17,781] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [67]  [ 20/160]  eta: 0:02:48  lr: 0.000018  min_lr: 0.000000  loss: 1.8312 (1.9583)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4141 (7.7257)  time: 0.8920 (0.5035 -- 4.5413)  data: 0.3312 (0.0004 -- 4.0241)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:02:12  lr: 0.000018  min_lr: 0.000000  loss: 1.7869 (1.9025)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1299 (7.7343)  time: 0.9924 (0.5104 -- 4.1476)  data: 0.4492 (0.0002 -- 3.6261)  max mem: 16413
Epoch: [67]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.9456 (1.8950)  loss_scale: 8192.0000 (9266.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8483 (7.7932)  time: 0.6909 (0.5187 -- 2.8216)  data: 0.1437 (0.0005 -- 2.2798)  max mem: 16413
Epoch: [67]  [ 80/160]  eta: 0:01:17  lr: 0.000018  min_lr: 0.000000  loss: 1.9333 (1.9062)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0747 (7.9717)  time: 0.9718 (0.5113 -- 4.7139)  data: 0.4299 (0.0004 -- 4.2016)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000000  loss: 1.9105 (1.9123)  loss_scale: 8192.0000 (8840.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8771 (8.1424)  time: 0.8890 (0.5157 -- 4.3088)  data: 0.3445 (0.0003 -- 3.7705)  max mem: 16413
Epoch: [67]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.8196 (1.9026)  loss_scale: 8192.0000 (8733.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6284 (8.0878)  time: 0.9018 (0.5173 -- 3.5789)  data: 0.3552 (0.0004 -- 3.0554)  max mem: 16413
[2023-08-31 00:22:09,556] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:22:09,557] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 00:22:09,557] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:22:09,558] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.8488 (1.9030)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9812 (7.9956)  time: 0.7624 (0.5250 -- 3.0203)  data: 0.2172 (0.0002 -- 2.4818)  max mem: 16413
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9545 (1.9073)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8134 (8.0606)  time: 0.6449 (0.4952 -- 1.8948)  data: 0.1165 (0.0002 -- 1.3635)  max mem: 16413
Epoch: [67] Total time: 0:02:21 (0.8864 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9545 (1.9066)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8134 (8.0606)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.6470 (0.6470)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5864 (2.5864 -- 2.5864)  data: 2.3384 (2.3384 -- 2.3384)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6470 (0.9478)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (94.9495)  time: 0.4452 (0.1912 -- 2.5864)  data: 0.2306 (0.0004 -- 2.3384)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6823 (0.8865)  acc1: 66.6667 (70.8995)  acc5: 100.0000 (95.7672)  time: 0.2153 (0.1697 -- 0.4060)  data: 0.0141 (0.0001 -- 0.1902)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8332 (0.9472)  acc1: 55.5556 (67.2199)  acc5: 100.0000 (95.4357)  time: 0.2007 (0.1330 -- 0.4060)  data: 0.0138 (0.0001 -- 0.1902)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 72.407 Acc@5 95.851 loss 0.890
Accuracy of the network on the 482 val images: 72.41%
Max accuracy: 74.48%
Epoch: [68]  [  0/160]  eta: 0:19:56  lr: 0.000018  min_lr: 0.000000  loss: 2.1058 (2.1058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1613 (6.1613)  time: 7.4807 (7.4807 -- 7.4807)  data: 6.9536 (6.9536 -- 6.9536)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:39  lr: 0.000018  min_lr: 0.000000  loss: 1.6502 (1.8020)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1437 (7.7344)  time: 0.8207 (0.5397 -- 3.5201)  data: 0.2606 (0.0008 -- 2.9674)  max mem: 16413
Epoch: [68]  [ 40/160]  eta: 0:02:08  lr: 0.000018  min_lr: 0.000000  loss: 1.8420 (1.8154)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8899 (7.9525)  time: 1.0057 (0.5127 -- 4.7467)  data: 0.1126 (0.0001 -- 1.7507)  max mem: 16413
Epoch: [68]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 1.8551 (1.8205)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9672 (7.7994)  time: 0.8153 (0.5221 -- 4.4938)  data: 0.0016 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 2.0274 (1.8763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9565 (7.9409)  time: 0.8608 (0.5335 -- 3.2231)  data: 0.0013 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.9085 (1.8829)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7039 (7.8706)  time: 0.8235 (0.5238 -- 4.2915)  data: 0.0639 (0.0004 -- 1.1659)  max mem: 16413
[2023-08-31 00:24:09,941] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:24:09,941] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:24:09,941] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:24:09,941] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:24:20,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=61, lr=[4.2272779146247105e-07, 4.2272779146247105e-07, 5.636370552832948e-07, 5.636370552832948e-07, 7.515160737110597e-07, 7.515160737110597e-07, 1.0020214316147463e-06, 1.0020214316147463e-06, 1.3360285754863282e-06, 1.3360285754863282e-06, 1.7813714339817711e-06, 1.7813714339817711e-06, 2.375161911975695e-06, 2.375161911975695e-06, 3.1668825493009265e-06, 3.1668825493009265e-06, 4.222510065734568e-06, 4.222510065734568e-06, 5.6300134209794245e-06, 5.6300134209794245e-06, 7.5066845613059e-06, 7.5066845613059e-06, 1.0008912748407866e-05, 1.0008912748407866e-05, 1.3345216997877155e-05, 1.3345216997877155e-05, 1.7793622663836206e-05, 1.7793622663836206e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 00:24:20,408] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=17.150707304891977, CurrSamplesPerSec=22.135983407132187, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.9702 (1.8930)  loss_scale: 32768.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0539 (7.8532)  time: 0.7831 (0.5336 -- 2.2742)  data: 0.0015 (0.0004 -- 0.0027)  max mem: 16413
[2023-08-31 00:24:38,958] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11020
[2023-08-31 00:24:38,958] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11020
[2023-08-31 00:24:38,959] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:24:38,959] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:24:38,959] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 2.1564 (1.9242)  loss_scale: 32768.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0550 (7.9999)  time: 0.8902 (0.5115 -- 2.6019)  data: 0.0077 (0.0003 -- 0.1286)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9280 (1.9203)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3494 (8.0298)  time: 0.6622 (0.4965 -- 2.4006)  data: 0.0010 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [68] Total time: 0:02:20 (0.8764 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9280 (1.9205)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3494 (8.0298)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6369 (0.6369)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3579 (2.3579 -- 2.3579)  data: 2.1179 (2.1179 -- 2.1179)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6492 (0.9498)  acc1: 66.6667 (69.6970)  acc5: 100.0000 (94.9495)  time: 0.4556 (0.2033 -- 2.3579)  data: 0.2327 (0.0009 -- 2.1179)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7084 (0.8859)  acc1: 66.6667 (73.0159)  acc5: 100.0000 (95.7672)  time: 0.2299 (0.1704 -- 0.7139)  data: 0.0223 (0.0001 -- 0.4291)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7887 (0.9421)  acc1: 66.6667 (70.5394)  acc5: 100.0000 (95.4357)  time: 0.2148 (0.1327 -- 0.7139)  data: 0.0219 (0.0001 -- 0.4291)  max mem: 16413
Val: Total time: 0:00:07 (0.2941 s / it)
* Acc@1 74.689 Acc@5 96.058 loss 0.891
Accuracy of the network on the 482 val images: 74.69%
[2023-08-31 00:24:59,641] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:24:59,643] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:24:59,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:24:59,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:25:00,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:25:00,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.69%
Epoch: [69]  [  0/160]  eta: 0:18:12  lr: 0.000018  min_lr: 0.000000  loss: 2.4389 (2.4389)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5406 (5.5406)  time: 6.8252 (6.8252 -- 6.8252)  data: 5.6403 (5.6403 -- 5.6403)  max mem: 16413
Epoch: [69]  [ 20/160]  eta: 0:02:50  lr: 0.000018  min_lr: 0.000000  loss: 1.9539 (1.9568)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5953 (7.7082)  time: 0.9370 (0.5341 -- 3.6616)  data: 0.3160 (0.0008 -- 3.1405)  max mem: 16413
Epoch: [69]  [ 40/160]  eta: 0:02:13  lr: 0.000018  min_lr: 0.000000  loss: 1.8579 (1.9376)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0629 (8.1400)  time: 0.9951 (0.5325 -- 4.6470)  data: 0.1184 (0.0005 -- 0.6970)  max mem: 16413
[2023-08-31 00:25:54,453] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11092
[2023-08-31 00:25:54,453] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11092
[2023-08-31 00:25:54,453] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:25:54,453] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:25:54,453] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [69]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 2.0938 (1.9840)  loss_scale: 16384.0000 (15175.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3224 (7.9856)  time: 0.7189 (0.5171 -- 2.4114)  data: 0.0217 (0.0003 -- 0.2563)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:19  lr: 0.000018  min_lr: 0.000000  loss: 1.9999 (1.9805)  loss_scale: 8192.0000 (13451.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7330 (8.4303)  time: 1.0232 (0.5192 -- 5.1443)  data: 0.0633 (0.0003 -- 0.9692)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000000  loss: 1.9807 (1.9676)  loss_scale: 8192.0000 (12409.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7032 (8.5688)  time: 0.7596 (0.5198 -- 3.2994)  data: 0.0015 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [69]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.9727 (1.9745)  loss_scale: 8192.0000 (11712.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0787 (8.4846)  time: 0.9004 (0.5415 -- 3.0685)  data: 0.0018 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.9383 (1.9639)  loss_scale: 8192.0000 (11213.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0228 (8.4285)  time: 0.8389 (0.5233 -- 3.5897)  data: 0.0022 (0.0004 -- 0.0154)  max mem: 16413
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7984 (1.9459)  loss_scale: 8192.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0765 (8.2816)  time: 0.5806 (0.4952 -- 1.5945)  data: 0.0010 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [69] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7984 (1.9271)  loss_scale: 8192.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0765 (8.2816)
Val:  [ 0/27]  eta: 0:01:15  loss: 0.6526 (0.6526)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.7930 (2.7930 -- 2.7930)  data: 2.5184 (2.5184 -- 2.5184)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6526 (0.9314)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (94.9495)  time: 0.4592 (0.2034 -- 2.7930)  data: 0.2405 (0.0004 -- 2.5184)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6782 (0.8656)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (95.7672)  time: 0.2072 (0.1696 -- 0.3142)  data: 0.0065 (0.0001 -- 0.1050)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7872 (0.9239)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (95.4357)  time: 0.1918 (0.1327 -- 0.3142)  data: 0.0062 (0.0001 -- 0.1050)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 75.519 Acc@5 96.058 loss 0.876
Accuracy of the network on the 482 val images: 75.52%
[2023-08-31 00:27:30,244] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:27:30,246] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:27:30,246] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:27:30,246] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:27:31,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:27:31,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.52%
Epoch: [70]  [  0/160]  eta: 0:16:26  lr: 0.000018  min_lr: 0.000000  loss: 1.8804 (1.8804)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3476 (6.3476)  time: 6.1647 (6.1647 -- 6.1647)  data: 5.5970 (5.5970 -- 5.5970)  max mem: 16413
Epoch: [70]  [ 20/160]  eta: 0:02:43  lr: 0.000018  min_lr: 0.000000  loss: 1.8437 (1.9159)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1044 (8.2404)  time: 0.9200 (0.5192 -- 3.2467)  data: 0.2370 (0.0010 -- 2.7103)  max mem: 16413
[2023-08-31 00:27:56,795] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:27:56,796] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 00:27:56,796] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:27:56,797] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [70]  [ 40/160]  eta: 0:01:54  lr: 0.000018  min_lr: 0.000000  loss: 1.7286 (1.8624)  loss_scale: 16384.0000 (12188.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6703 (8.2100)  time: 0.7320 (0.5279 -- 2.3515)  data: 0.1709 (0.0003 -- 1.7740)  max mem: 16413
Epoch: [70]  [ 60/160]  eta: 0:01:35  lr: 0.000018  min_lr: 0.000000  loss: 1.9228 (1.8527)  loss_scale: 16384.0000 (13563.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6567 (8.0980)  time: 0.9675 (0.5421 -- 2.9639)  data: 0.2306 (0.0005 -- 2.4176)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 1.9785 (1.8976)  loss_scale: 16384.0000 (14260.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2507 (8.3203)  time: 0.9430 (0.5259 -- 3.5150)  data: 0.0191 (0.0002 -- 0.3463)  max mem: 16413
Epoch: [70]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000000  loss: 2.1144 (1.9337)  loss_scale: 16384.0000 (14680.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5686 (8.1899)  time: 0.8470 (0.5197 -- 3.5146)  data: 0.0013 (0.0007 -- 0.0022)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.9389 (1.9364)  loss_scale: 16384.0000 (14962.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6172 (8.1176)  time: 0.8270 (0.5258 -- 3.3003)  data: 0.2324 (0.0003 -- 2.7535)  max mem: 16413
Epoch: [70]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.8855 (1.9369)  loss_scale: 16384.0000 (15163.9149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5964 (8.1293)  time: 0.8368 (0.5247 -- 2.8107)  data: 0.2259 (0.0004 -- 2.2786)  max mem: 16413
[2023-08-31 00:29:45,643] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:29:45,643] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:29:45,644] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:29:45,644] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.9721 (1.9403)  loss_scale: 32768.0000 (16435.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6989 (8.1562)  time: 0.6915 (0.4964 -- 2.3836)  data: 0.1658 (0.0002 -- 1.8835)  max mem: 16413
Epoch: [70] Total time: 0:02:20 (0.8810 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.9721 (1.9128)  loss_scale: 32768.0000 (16435.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6989 (8.1562)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.6094 (0.6094)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5575 (2.5575 -- 2.5575)  data: 2.2796 (2.2796 -- 2.2796)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6180 (0.9231)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4471 (0.2025 -- 2.5575)  data: 0.2191 (0.0003 -- 2.2796)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6969 (0.8663)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.8254)  time: 0.2187 (0.1691 -- 0.3958)  data: 0.0080 (0.0001 -- 0.1203)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8140 (0.9237)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (96.2656)  time: 0.2030 (0.1327 -- 0.3958)  data: 0.0078 (0.0001 -- 0.1203)  max mem: 16413
Val: Total time: 0:00:07 (0.2933 s / it)
* Acc@1 75.519 Acc@5 96.473 loss 0.868
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 75.52%
Epoch: [71]  [  0/160]  eta: 0:19:27  lr: 0.000017  min_lr: 0.000000  loss: 1.4295 (1.4295)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4326 (9.4326)  time: 7.2975 (7.2975 -- 7.2975)  data: 5.9856 (5.9856 -- 5.9856)  max mem: 16413
[2023-08-31 00:30:11,741] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11367
[2023-08-31 00:30:11,742] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:30:11,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 00:30:11,744] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11367
[2023-08-31 00:30:11,745] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [71]  [ 20/160]  eta: 0:02:43  lr: 0.000017  min_lr: 0.000000  loss: 1.8940 (1.8335)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4620 (8.5376)  time: 0.8597 (0.5172 -- 2.8462)  data: 0.2889 (0.0009 -- 2.2966)  max mem: 16413
Epoch: [71]  [ 40/160]  eta: 0:02:04  lr: 0.000017  min_lr: 0.000000  loss: 1.7206 (1.8149)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2121 (8.0129)  time: 0.8975 (0.5140 -- 4.2837)  data: 0.0668 (0.0002 -- 1.3106)  max mem: 16413
Epoch: [71]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000000  loss: 2.0399 (1.8679)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0130 (8.2358)  time: 0.8748 (0.5126 -- 4.6648)  data: 0.1128 (0.0006 -- 1.0279)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 1.6816 (1.8303)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5447 (8.0473)  time: 0.8345 (0.5275 -- 3.5181)  data: 0.2838 (0.0005 -- 2.9902)  max mem: 16413
Epoch: [71]  [100/160]  eta: 0:00:54  lr: 0.000017  min_lr: 0.000000  loss: 1.9910 (1.8434)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0125 (8.1067)  time: 0.7889 (0.5288 -- 2.2804)  data: 0.2336 (0.0002 -- 1.7265)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.9266 (1.8638)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3045 (7.9908)  time: 0.9840 (0.5227 -- 3.6946)  data: 0.4372 (0.0002 -- 3.1386)  max mem: 16413
[2023-08-31 00:32:07,012] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:32:07,012] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:32:07,012] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:32:07,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 2.1095 (1.8919)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3370 (8.0221)  time: 0.8265 (0.5391 -- 4.3554)  data: 0.2732 (0.0003 -- 3.8276)  max mem: 16413
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8932 (1.9032)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0822 (8.0193)  time: 0.6591 (0.4958 -- 3.3421)  data: 0.1414 (0.0003 -- 2.8124)  max mem: 16413
Epoch: [71] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8932 (1.8855)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0822 (8.0193)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6273 (0.6273)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4646 (2.4646 -- 2.4646)  data: 2.2355 (2.2355 -- 2.2355)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6562 (0.9368)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4222 (0.1963 -- 2.4646)  data: 0.2053 (0.0004 -- 2.2355)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7319 (0.8799)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (96.8254)  time: 0.2163 (0.1681 -- 0.3809)  data: 0.0106 (0.0001 -- 0.1861)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7905 (0.9277)  acc1: 66.6667 (70.5394)  acc5: 100.0000 (95.8506)  time: 0.2006 (0.1327 -- 0.3809)  data: 0.0103 (0.0001 -- 0.1861)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 75.726 Acc@5 96.266 loss 0.871
Accuracy of the network on the 482 val images: 75.73%
[2023-08-31 00:32:29,640] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:32:29,642] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:32:29,642] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:32:29,642] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:32:31,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:32:31,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.73%
Epoch: [72]  [  0/160]  eta: 0:19:01  lr: 0.000017  min_lr: 0.000000  loss: 1.5767 (1.5767)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4796 (7.4796)  time: 7.1319 (7.1319 -- 7.1319)  data: 4.9017 (4.9017 -- 4.9017)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:02:42  lr: 0.000017  min_lr: 0.000000  loss: 1.8626 (1.8556)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1330 (8.7783)  time: 0.8647 (0.5354 -- 2.9598)  data: 0.3058 (0.0005 -- 2.4410)  max mem: 16413
[2023-08-31 00:33:04,964] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11552
[2023-08-31 00:33:04,964] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11552
[2023-08-31 00:33:04,964] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:33:04,964] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:33:04,964] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [72]  [ 40/160]  eta: 0:02:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8942 (1.8771)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2868 (8.9143)  time: 0.8442 (0.5253 -- 2.6020)  data: 0.2260 (0.0003 -- 2.0685)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:36  lr: 0.000017  min_lr: 0.000000  loss: 1.7661 (1.8567)  loss_scale: 16384.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9799 (9.0375)  time: 0.8807 (0.5212 -- 3.2225)  data: 0.1989 (0.0003 -- 2.7009)  max mem: 16413
Epoch: [72]  [ 80/160]  eta: 0:01:17  lr: 0.000017  min_lr: 0.000000  loss: 1.8631 (1.8721)  loss_scale: 16384.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3742 (8.6821)  time: 0.9689 (0.5226 -- 4.3200)  data: 0.0016 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [72]  [100/160]  eta: 0:00:54  lr: 0.000017  min_lr: 0.000000  loss: 1.8423 (1.8703)  loss_scale: 16384.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4334 (8.4843)  time: 0.7080 (0.5393 -- 2.5492)  data: 0.0320 (0.0002 -- 0.5987)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.8896 (1.8635)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7998 (8.2891)  time: 0.8892 (0.5277 -- 3.0131)  data: 0.1581 (0.0007 -- 2.4706)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.9473 (1.8729)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4448 (8.2591)  time: 0.9374 (0.5315 -- 3.1200)  data: 0.1352 (0.0007 -- 1.5365)  max mem: 16413
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8683 (1.8722)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2862 (8.2870)  time: 0.7098 (0.4933 -- 2.8677)  data: 0.1790 (0.0001 -- 2.3555)  max mem: 16413
Epoch: [72] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8683 (1.8880)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2862 (8.2870)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.6886 (0.6886)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5312 (2.5312 -- 2.5312)  data: 2.2416 (2.2416 -- 2.2416)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6886 (0.9373)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (94.9495)  time: 0.4477 (0.1914 -- 2.5312)  data: 0.2306 (0.0006 -- 2.2416)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7435 (0.8844)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (95.7672)  time: 0.2219 (0.1697 -- 0.4940)  data: 0.0216 (0.0001 -- 0.2687)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8020 (0.9313)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (95.0207)  time: 0.2079 (0.1325 -- 0.4940)  data: 0.0205 (0.0001 -- 0.2687)  max mem: 16413
Val: Total time: 0:00:07 (0.2945 s / it)
* Acc@1 75.519 Acc@5 95.851 loss 0.878
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 75.73%
Epoch: [73]  [  0/160]  eta: 0:18:28  lr: 0.000017  min_lr: 0.000000  loss: 2.2936 (2.2936)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3966 (8.3966)  time: 6.9260 (6.9260 -- 6.9260)  data: 6.3696 (6.3696 -- 6.3696)  max mem: 16413
[2023-08-31 00:35:10,120] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:35:10,120] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:35:10,121] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:35:10,121] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [73]  [ 20/160]  eta: 0:02:51  lr: 0.000017  min_lr: 0.000000  loss: 1.9762 (1.8784)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2033 (7.8820)  time: 0.9385 (0.5307 -- 3.1913)  data: 0.0279 (0.0004 -- 0.5237)  max mem: 16413
[2023-08-31 00:35:27,915] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11701
[2023-08-31 00:35:27,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:35:27,915] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11701
[2023-08-31 00:35:27,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:35:27,916] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [73]  [ 40/160]  eta: 0:02:07  lr: 0.000017  min_lr: 0.000000  loss: 1.8414 (1.8205)  loss_scale: 16384.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0116 (7.8980)  time: 0.8961 (0.5351 -- 2.9162)  data: 0.0026 (0.0005 -- 0.0172)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:40  lr: 0.000017  min_lr: 0.000000  loss: 1.9091 (1.8501)  loss_scale: 16384.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4714 (8.1012)  time: 0.8840 (0.5141 -- 4.6368)  data: 0.0022 (0.0002 -- 0.0173)  max mem: 16413
Epoch: [73]  [ 80/160]  eta: 0:01:17  lr: 0.000017  min_lr: 0.000000  loss: 1.9204 (1.8499)  loss_scale: 16384.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0188 (8.2349)  time: 0.8708 (0.5197 -- 4.2795)  data: 0.0011 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.9407 (1.8634)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8005 (7.9369)  time: 0.7639 (0.5289 -- 3.1027)  data: 0.0017 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.8126 (1.8513)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4952 (8.0355)  time: 0.8989 (0.5245 -- 3.1920)  data: 0.0946 (0.0002 -- 1.3091)  max mem: 16413
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7635 (1.8561)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8998 (8.1605)  time: 0.9169 (0.5195 -- 5.1672)  data: 0.1380 (0.0003 -- 2.5164)  max mem: 16413
[2023-08-31 00:37:20,131] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:37:20,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:37:20,131] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:37:20,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.7959 (1.8599)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1963 (8.0657)  time: 0.6568 (0.4946 -- 3.3642)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [73] Total time: 0:02:22 (0.8934 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.7959 (1.8972)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1963 (8.0657)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6001 (0.6001)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3537 (2.3537 -- 2.3537)  data: 2.1168 (2.1168 -- 2.1168)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6001 (0.9164)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4247 (0.2021 -- 2.3537)  data: 0.2107 (0.0004 -- 2.1168)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7502 (0.8683)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.2963)  time: 0.2248 (0.1695 -- 0.4300)  data: 0.0230 (0.0001 -- 0.2554)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8004 (0.9174)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (95.4357)  time: 0.2110 (0.1322 -- 0.4300)  data: 0.0227 (0.0001 -- 0.2554)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 75.104 Acc@5 96.266 loss 0.861
Accuracy of the network on the 482 val images: 75.10%
Max accuracy: 75.73%
Epoch: [74]  [  0/160]  eta: 0:20:59  lr: 0.000017  min_lr: 0.000000  loss: 1.8985 (1.8985)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6626 (5.6626)  time: 7.8738 (7.8738 -- 7.8738)  data: 7.3247 (7.3247 -- 7.3247)  max mem: 16413
[2023-08-31 00:37:41,471] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11842
[2023-08-31 00:37:41,471] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11842
[2023-08-31 00:37:41,471] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:37:41,471] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:37:41,471] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [74]  [ 20/160]  eta: 0:02:42  lr: 0.000017  min_lr: 0.000000  loss: 1.9117 (1.8580)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4624 (8.2518)  time: 0.8268 (0.5271 -- 2.3544)  data: 0.1957 (0.0005 -- 1.1628)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:01:59  lr: 0.000017  min_lr: 0.000000  loss: 1.7512 (1.8339)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9875 (8.2512)  time: 0.8187 (0.5340 -- 2.5465)  data: 0.1884 (0.0003 -- 1.7375)  max mem: 16413
[2023-08-31 00:38:27,532] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11895
[2023-08-31 00:38:27,532] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11895
[2023-08-31 00:38:27,532] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:38:27,533] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 00:38:27,533] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [74]  [ 60/160]  eta: 0:01:37  lr: 0.000017  min_lr: 0.000000  loss: 2.0009 (1.8708)  loss_scale: 16384.0000 (16115.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1613 (8.2144)  time: 0.9348 (0.5231 -- 5.0585)  data: 0.0759 (0.0004 -- 1.4791)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 2.0972 (1.8992)  loss_scale: 8192.0000 (14159.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7480 (8.1281)  time: 0.8505 (0.5281 -- 2.2617)  data: 0.0962 (0.0001 -- 1.3705)  max mem: 16413
Epoch: [74]  [100/160]  eta: 0:00:54  lr: 0.000017  min_lr: 0.000000  loss: 1.9582 (1.9053)  loss_scale: 8192.0000 (12977.4257)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8120 (8.2130)  time: 0.7935 (0.5360 -- 2.3394)  data: 0.0617 (0.0006 -- 1.1934)  max mem: 16413
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 2.0333 (1.9149)  loss_scale: 8192.0000 (12186.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2301 (8.0777)  time: 0.9126 (0.5313 -- 3.1330)  data: 0.0096 (0.0004 -- 0.1603)  max mem: 16413
Epoch: [74]  [140/160]  eta: 0:00:17  lr: 0.000017  min_lr: 0.000000  loss: 1.9295 (1.9281)  loss_scale: 8192.0000 (11619.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8154 (8.1136)  time: 0.7553 (0.5336 -- 1.8306)  data: 0.0655 (0.0003 -- 1.2833)  max mem: 16413
[2023-08-31 00:39:52,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=68, lr=[3.980991634758528e-07, 3.980991634758528e-07, 5.307988846344704e-07, 5.307988846344704e-07, 7.077318461792938e-07, 7.077318461792938e-07, 9.436424615723918e-07, 9.436424615723918e-07, 1.258189948763189e-06, 1.258189948763189e-06, 1.6775865983509187e-06, 1.6775865983509187e-06, 2.2367821311345584e-06, 2.2367821311345584e-06, 2.9823761748460777e-06, 2.9823761748460777e-06, 3.976501566461437e-06, 3.976501566461437e-06, 5.302002088615249e-06, 5.302002088615249e-06, 7.069336118153666e-06, 7.069336118153666e-06, 9.425781490871555e-06, 9.425781490871555e-06, 1.2567708654495406e-05, 1.2567708654495406e-05, 1.675694487266054e-05, 1.675694487266054e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 00:39:52,027] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=17.12622671026013, CurrSamplesPerSec=24.73985320747372, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.6997 (1.9124)  loss_scale: 8192.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9060 (8.1109)  time: 0.7191 (0.4970 -- 3.1122)  data: 0.1285 (0.0002 -- 2.5578)  max mem: 16413
Epoch: [74] Total time: 0:02:19 (0.8723 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.6997 (1.9165)  loss_scale: 8192.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9060 (8.1109)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6524 (0.6524)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4538 (2.4538 -- 2.4538)  data: 2.2376 (2.2376 -- 2.2376)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6524 (0.9267)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (94.9495)  time: 0.4399 (0.1834 -- 2.4538)  data: 0.2264 (0.0005 -- 2.2376)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6973 (0.8692)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (95.7672)  time: 0.2241 (0.1682 -- 0.4819)  data: 0.0201 (0.0001 -- 0.2442)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7779 (0.9192)  acc1: 77.7778 (71.3693)  acc5: 100.0000 (95.0207)  time: 0.2101 (0.1332 -- 0.4819)  data: 0.0199 (0.0001 -- 0.2442)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 76.971 Acc@5 95.851 loss 0.869
Accuracy of the network on the 482 val images: 76.97%
[2023-08-31 00:39:59,957] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:39:59,958] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:39:59,958] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:39:59,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:40:01,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:40:01,372] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.97%
Epoch: [75]  [  0/160]  eta: 0:22:14  lr: 0.000017  min_lr: 0.000000  loss: 2.1230 (2.1230)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2786 (4.2786)  time: 8.3421 (8.3421 -- 8.3421)  data: 4.9309 (4.9309 -- 4.9309)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:03:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8591 (1.8541)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4905 (7.9507)  time: 0.9377 (0.5278 -- 4.1988)  data: 0.0026 (0.0003 -- 0.0132)  max mem: 16413
[2023-08-31 00:40:30,628] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:40:30,629] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 00:40:30,630] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:40:30,630] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [75]  [ 40/160]  eta: 0:02:08  lr: 0.000017  min_lr: 0.000000  loss: 1.9997 (1.9129)  loss_scale: 16384.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5453 (7.9140)  time: 0.8327 (0.5250 -- 4.1078)  data: 0.0012 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:39  lr: 0.000017  min_lr: 0.000000  loss: 2.0035 (1.9051)  loss_scale: 16384.0000 (13160.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7729 (8.2956)  time: 0.8569 (0.5297 -- 2.3382)  data: 0.0020 (0.0001 -- 0.0081)  max mem: 16413
Epoch: [75]  [ 80/160]  eta: 0:01:18  lr: 0.000017  min_lr: 0.000000  loss: 1.9073 (1.8956)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2332 (8.3072)  time: 0.9500 (0.5093 -- 3.9976)  data: 0.0013 (0.0003 -- 0.0027)  max mem: 16413
[2023-08-31 00:41:36,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12097
[2023-08-31 00:41:36,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12097
[2023-08-31 00:41:36,515] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:41:36,515] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:41:36,515] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [75]  [100/160]  eta: 0:00:57  lr: 0.000017  min_lr: 0.000000  loss: 1.8317 (1.8729)  loss_scale: 16384.0000 (14112.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0489 (8.3591)  time: 0.8456 (0.5042 -- 4.7288)  data: 0.0014 (0.0003 -- 0.0081)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.9438 (1.8693)  loss_scale: 8192.0000 (13134.2810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3870 (8.3095)  time: 0.8016 (0.5318 -- 2.6496)  data: 0.0021 (0.0006 -- 0.0081)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.8855 (1.8710)  loss_scale: 8192.0000 (12433.2482)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2623 (8.3717)  time: 0.8846 (0.5223 -- 4.0307)  data: 0.0021 (0.0002 -- 0.0154)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.7338 (1.8639)  loss_scale: 8192.0000 (11929.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8605 (8.4045)  time: 0.6826 (0.4953 -- 3.2675)  data: 0.0009 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [75] Total time: 0:02:23 (0.8979 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.7338 (1.8735)  loss_scale: 8192.0000 (11929.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8605 (8.4045)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.5879 (0.5879)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4534 (2.4534 -- 2.4534)  data: 2.2169 (2.2169 -- 2.2169)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6213 (0.9072)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4281 (0.2113 -- 2.4534)  data: 0.2079 (0.0006 -- 2.2169)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7103 (0.8523)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (96.2963)  time: 0.2262 (0.1694 -- 0.4982)  data: 0.0192 (0.0001 -- 0.3112)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7641 (0.8981)  acc1: 66.6667 (71.3693)  acc5: 100.0000 (95.4357)  time: 0.2102 (0.1322 -- 0.4982)  data: 0.0189 (0.0001 -- 0.3112)  max mem: 16413
Val: Total time: 0:00:07 (0.2948 s / it)
* Acc@1 75.934 Acc@5 96.266 loss 0.846
Accuracy of the network on the 482 val images: 75.93%
Max accuracy: 76.97%
Epoch: [76]  [  0/160]  eta: 0:17:00  lr: 0.000017  min_lr: 0.000000  loss: 2.0359 (2.0359)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.9599 (14.9599)  time: 6.3812 (6.3812 -- 6.3812)  data: 4.5774 (4.5774 -- 4.5774)  max mem: 16413
Epoch: [76]  [ 20/160]  eta: 0:02:42  lr: 0.000017  min_lr: 0.000000  loss: 2.0159 (1.9392)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0293 (8.5153)  time: 0.9026 (0.5302 -- 4.0482)  data: 0.3446 (0.0004 -- 3.5178)  max mem: 16413
Epoch: [76]  [ 40/160]  eta: 0:02:01  lr: 0.000017  min_lr: 0.000000  loss: 1.7362 (1.7994)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7539 (8.6257)  time: 0.8608 (0.5257 -- 3.2442)  data: 0.1454 (0.0005 -- 2.6721)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:35  lr: 0.000017  min_lr: 0.000000  loss: 1.8706 (1.8480)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5977 (8.3970)  time: 0.8293 (0.5215 -- 2.0986)  data: 0.1950 (0.0004 -- 1.5518)  max mem: 16413
[2023-08-31 00:43:36,536] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:43:36,536] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 00:43:36,537] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:43:36,537] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [76]  [ 80/160]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000000  loss: 1.9534 (1.8747)  loss_scale: 16384.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9625 (8.0815)  time: 0.8629 (0.5331 -- 2.4333)  data: 0.1825 (0.0002 -- 1.8810)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:54  lr: 0.000016  min_lr: 0.000000  loss: 1.9095 (1.8869)  loss_scale: 16384.0000 (11030.8119)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2218 (8.3321)  time: 0.8022 (0.5246 -- 3.8205)  data: 0.2114 (0.0006 -- 3.2674)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:37  lr: 0.000016  min_lr: 0.000000  loss: 2.0276 (1.9074)  loss_scale: 16384.0000 (11915.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2020 (8.5683)  time: 1.0361 (0.5208 -- 4.2175)  data: 0.2780 (0.0003 -- 3.7042)  max mem: 16413
Epoch: [76]  [140/160]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000000  loss: 2.0621 (1.9119)  loss_scale: 16384.0000 (12549.4468)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7491 (8.5920)  time: 0.6971 (0.5291 -- 2.1933)  data: 0.0020 (0.0003 -- 0.0048)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.8754 (1.9022)  loss_scale: 16384.0000 (13004.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4607 (8.6843)  time: 0.6968 (0.4961 -- 1.9351)  data: 0.0717 (0.0002 -- 0.6454)  max mem: 16413
Epoch: [76] Total time: 0:02:19 (0.8725 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.8754 (1.8972)  loss_scale: 16384.0000 (13004.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4607 (8.6843)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5992 (0.5992)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3762 (2.3762 -- 2.3762)  data: 2.1000 (2.1000 -- 2.1000)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6050 (0.9089)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (95.9596)  time: 0.4294 (0.2029 -- 2.3762)  data: 0.2051 (0.0009 -- 2.1000)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6711 (0.8432)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (96.2963)  time: 0.2306 (0.1697 -- 0.5176)  data: 0.0247 (0.0001 -- 0.3345)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7606 (0.8958)  acc1: 77.7778 (71.3693)  acc5: 100.0000 (95.4357)  time: 0.2141 (0.1366 -- 0.5176)  data: 0.0243 (0.0001 -- 0.3345)  max mem: 16413
Val: Total time: 0:00:07 (0.2955 s / it)
* Acc@1 75.934 Acc@5 96.058 loss 0.846
Accuracy of the network on the 482 val images: 75.93%
Max accuracy: 76.97%
Epoch: [77]  [  0/160]  eta: 0:23:16  lr: 0.000016  min_lr: 0.000000  loss: 1.9899 (1.9899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4702 (7.4702)  time: 8.7250 (8.7250 -- 8.7250)  data: 6.9629 (6.9629 -- 6.9629)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:58  lr: 0.000016  min_lr: 0.000000  loss: 1.9654 (1.9611)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3126 (8.1934)  time: 0.9043 (0.5265 -- 4.5189)  data: 0.2534 (0.0004 -- 3.0584)  max mem: 16413
[2023-08-31 00:45:37,203] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:45:37,203] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:45:37,207] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:45:37,208] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:45:43,574] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12360
[2023-08-31 00:45:43,574] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12360
[2023-08-31 00:45:43,575] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:45:43,574] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:45:43,575] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [77]  [ 40/160]  eta: 0:02:05  lr: 0.000016  min_lr: 0.000000  loss: 1.8982 (1.9057)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5141 (8.0311)  time: 0.8085 (0.5145 -- 3.4070)  data: 0.2201 (0.0001 -- 2.1635)  max mem: 16413
[2023-08-31 00:45:59,415] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12378
[2023-08-31 00:45:59,415] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:45:59,415] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12378
[2023-08-31 00:45:59,415] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:45:59,415] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [77]  [ 60/160]  eta: 0:01:38  lr: 0.000016  min_lr: 0.000000  loss: 1.7226 (1.8435)  loss_scale: 16384.0000 (17592.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9931 (7.9542)  time: 0.8455 (0.5128 -- 3.5320)  data: 0.1206 (0.0005 -- 1.2962)  max mem: 16413
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000000  loss: 1.6994 (1.8414)  loss_scale: 8192.0000 (15271.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8755 (8.0195)  time: 0.8881 (0.5333 -- 5.5158)  data: 0.1255 (0.0004 -- 1.7742)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.8060 (1.8289)  loss_scale: 8192.0000 (13869.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5977 (8.2152)  time: 0.8389 (0.5255 -- 4.1069)  data: 0.0015 (0.0004 -- 0.0050)  max mem: 16413
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7404 (1.8233)  loss_scale: 8192.0000 (12931.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6648 (8.4300)  time: 0.7770 (0.5540 -- 2.4801)  data: 0.0017 (0.0005 -- 0.0043)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000000  loss: 1.9533 (1.8314)  loss_scale: 8192.0000 (12258.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5130 (8.4287)  time: 0.7621 (0.5200 -- 2.6704)  data: 0.0018 (0.0005 -- 0.0041)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9448 (1.8484)  loss_scale: 8192.0000 (11776.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1354 (8.3748)  time: 0.7684 (0.4962 -- 2.2904)  data: 0.0010 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [77] Total time: 0:02:20 (0.8755 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9448 (1.8872)  loss_scale: 8192.0000 (11776.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1354 (8.3748)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5686 (0.5686)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4048 (2.4048 -- 2.4048)  data: 2.1789 (2.1789 -- 2.1789)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5875 (0.8906)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4329 (0.1922 -- 2.4048)  data: 0.2198 (0.0006 -- 2.1789)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6605 (0.8413)  acc1: 77.7778 (74.0741)  acc5: 100.0000 (96.2963)  time: 0.2189 (0.1694 -- 0.4282)  data: 0.0121 (0.0001 -- 0.2054)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7578 (0.8875)  acc1: 77.7778 (72.1992)  acc5: 100.0000 (95.4357)  time: 0.2042 (0.1326 -- 0.4282)  data: 0.0113 (0.0001 -- 0.2054)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 76.141 Acc@5 95.851 loss 0.833
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 76.97%
Epoch: [78]  [  0/160]  eta: 0:21:08  lr: 0.000016  min_lr: 0.000000  loss: 1.8083 (1.8083)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3284 (7.3284)  time: 7.9282 (7.9282 -- 7.9282)  data: 7.4059 (7.4059 -- 7.4059)  max mem: 16413
Epoch: [78]  [ 20/160]  eta: 0:02:33  lr: 0.000016  min_lr: 0.000000  loss: 1.8353 (1.8200)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3260 (8.9614)  time: 0.7536 (0.5307 -- 2.5632)  data: 0.1374 (0.0004 -- 1.9051)  max mem: 16413
[2023-08-31 00:47:57,827] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:47:57,828] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 00:47:57,829] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:47:57,829] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [78]  [ 40/160]  eta: 0:02:04  lr: 0.000016  min_lr: 0.000000  loss: 2.0670 (1.9170)  loss_scale: 16384.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3240 (9.0870)  time: 0.9835 (0.5267 -- 4.3167)  data: 0.3594 (0.0005 -- 2.4920)  max mem: 16413
Epoch: [78]  [ 60/160]  eta: 0:01:34  lr: 0.000016  min_lr: 0.000000  loss: 1.8790 (1.9103)  loss_scale: 16384.0000 (12758.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (9.1523)  time: 0.7454 (0.5199 -- 3.1924)  data: 0.0890 (0.0005 -- 0.7406)  max mem: 16413
Epoch: [78]  [ 80/160]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000000  loss: 1.8800 (1.9143)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5971 (9.0134)  time: 0.8895 (0.5368 -- 2.3161)  data: 0.2731 (0.0008 -- 1.7844)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:54  lr: 0.000016  min_lr: 0.000000  loss: 1.8190 (1.9126)  loss_scale: 16384.0000 (14194.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0590 (9.0064)  time: 0.8590 (0.5425 -- 3.7209)  data: 0.3054 (0.0005 -- 3.2032)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.9163 (1.9136)  loss_scale: 16384.0000 (14556.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2610 (8.8821)  time: 0.8582 (0.5231 -- 3.4561)  data: 0.3083 (0.0004 -- 2.9469)  max mem: 16413
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.8518 (1.9044)  loss_scale: 16384.0000 (14815.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3069 (8.9243)  time: 0.9860 (0.5275 -- 3.6477)  data: 0.4416 (0.0004 -- 3.1158)  max mem: 16413
[2023-08-31 00:49:47,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:49:47,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:49:47,553] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:49:47,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:49:48,047] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12636
[2023-08-31 00:49:48,047] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:49:48,047] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12636
[2023-08-31 00:49:48,047] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 00:49:48,047] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9422 (1.8975)  loss_scale: 16384.0000 (15104.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3347 (8.8108)  time: 0.6068 (0.4851 -- 1.9938)  data: 0.0875 (0.0001 -- 1.4617)  max mem: 16413
Epoch: [78] Total time: 0:02:21 (0.8817 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9422 (1.9065)  loss_scale: 16384.0000 (15104.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3347 (8.8108)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.6227 (0.6227)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5354 (2.5354 -- 2.5354)  data: 2.3115 (2.3115 -- 2.3115)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6227 (0.9019)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4402 (0.2086 -- 2.5354)  data: 0.2114 (0.0007 -- 2.3115)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6960 (0.8452)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (96.2963)  time: 0.2170 (0.1689 -- 0.3279)  data: 0.0086 (0.0001 -- 0.1556)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7509 (0.8911)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (95.4357)  time: 0.1967 (0.1329 -- 0.3279)  data: 0.0081 (0.0001 -- 0.1556)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 77.386 Acc@5 95.851 loss 0.840
Accuracy of the network on the 482 val images: 77.39%
[2023-08-31 00:49:57,411] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:49:57,412] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:49:57,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:49:57,413] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:49:58,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:49:58,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.39%
Epoch: [79]  [  0/160]  eta: 0:27:08  lr: 0.000016  min_lr: 0.000000  loss: 1.9531 (1.9531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5469 (5.5469)  time: 10.1793 (10.1793 -- 10.1793)  data: 9.6393 (9.6393 -- 9.6393)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:50  lr: 0.000016  min_lr: 0.000000  loss: 1.9574 (1.9299)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7945 (8.0628)  time: 0.7732 (0.5172 -- 3.1539)  data: 0.2236 (0.0003 -- 2.6359)  max mem: 16413
Epoch: [79]  [ 40/160]  eta: 0:02:04  lr: 0.000016  min_lr: 0.000000  loss: 2.0330 (1.9636)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0443 (8.3975)  time: 0.8485 (0.5266 -- 3.7175)  data: 0.2711 (0.0002 -- 3.1428)  max mem: 16413
Epoch: [79]  [ 60/160]  eta: 0:01:35  lr: 0.000016  min_lr: 0.000000  loss: 1.9751 (1.9705)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4944 (8.7044)  time: 0.7960 (0.5218 -- 2.4604)  data: 0.1184 (0.0002 -- 1.9052)  max mem: 16413
Epoch: [79]  [ 80/160]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000000  loss: 1.9542 (1.9612)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6708 (8.5336)  time: 0.8404 (0.5204 -- 2.5092)  data: 0.2886 (0.0003 -- 1.9572)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.9235 (1.9526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0715 (8.4150)  time: 0.9788 (0.5306 -- 2.4283)  data: 0.0707 (0.0003 -- 1.3607)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.6603 (1.9000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6243 (8.3965)  time: 0.7801 (0.5300 -- 1.8103)  data: 0.0320 (0.0004 -- 0.6122)  max mem: 16413
[2023-08-31 00:51:53,654] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:51:53,654] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:51:53,655] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:51:53,655] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:51:58,825] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12770
[2023-08-31 00:51:58,826] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:51:58,825] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12770
[2023-08-31 00:51:58,826] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:51:58,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.9156 (1.8902)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8147 (8.3055)  time: 0.9392 (0.5118 -- 3.3190)  data: 0.2055 (0.0003 -- 2.8042)  max mem: 16413
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.6365 (1.8691)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2795 (8.2748)  time: 0.6147 (0.4934 -- 1.7052)  data: 0.0750 (0.0001 -- 1.1854)  max mem: 16413
Epoch: [79] Total time: 0:02:21 (0.8818 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.6365 (1.8993)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2795 (8.2748)
[2023-08-31 00:52:19,930] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-08-31 00:52:19,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-08-31 00:52:19,934] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-08-31 00:52:19,934] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-08-31 00:52:20,979] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-08-31 00:52:20,979] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:02  loss: 0.5769 (0.5769)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3331 (2.3331 -- 2.3331)  data: 2.0982 (2.0982 -- 2.0982)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6355 (0.8967)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (96.9697)  time: 0.4437 (0.2068 -- 2.3331)  data: 0.2219 (0.0007 -- 2.0982)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6473 (0.8445)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.8254)  time: 0.2334 (0.1699 -- 0.5495)  data: 0.0261 (0.0001 -- 0.3310)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7531 (0.8853)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (95.8506)  time: 0.2164 (0.1324 -- 0.5495)  data: 0.0258 (0.0001 -- 0.3310)  max mem: 16413
Val: Total time: 0:00:07 (0.2959 s / it)
* Acc@1 77.178 Acc@5 96.266 loss 0.834
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 77.39%
Epoch: [80]  [  0/160]  eta: 0:21:02  lr: 0.000016  min_lr: 0.000000  loss: 2.1061 (2.1061)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.4585 (12.4585)  time: 7.8889 (7.8889 -- 7.8889)  data: 6.0025 (6.0025 -- 6.0025)  max mem: 16413
Epoch: [80]  [ 20/160]  eta: 0:02:43  lr: 0.000016  min_lr: 0.000000  loss: 1.9374 (1.9256)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4003 (8.9759)  time: 0.8312 (0.5216 -- 4.0935)  data: 0.0683 (0.0003 -- 1.1467)  max mem: 16413
Epoch: [80]  [ 40/160]  eta: 0:02:01  lr: 0.000016  min_lr: 0.000000  loss: 1.8038 (1.8889)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9964 (8.9325)  time: 0.8418 (0.5293 -- 3.1420)  data: 0.1168 (0.0002 -- 1.1641)  max mem: 16413
Epoch: [80]  [ 60/160]  eta: 0:01:34  lr: 0.000016  min_lr: 0.000000  loss: 1.7703 (1.8939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6487 (8.7281)  time: 0.8177 (0.5194 -- 2.1957)  data: 0.1455 (0.0001 -- 1.5331)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:17  lr: 0.000016  min_lr: 0.000000  loss: 1.9208 (1.9021)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6298 (8.5529)  time: 1.0591 (0.5238 -- 4.5257)  data: 0.4044 (0.0001 -- 4.0204)  max mem: 16413
[2023-08-31 00:54:01,186] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:54:01,186] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:54:01,186] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:54:01,186] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [80]  [100/160]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000000  loss: 1.9701 (1.9095)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4014 (8.3172)  time: 0.6937 (0.5208 -- 2.1212)  data: 0.1406 (0.0002 -- 1.5904)  max mem: 16413
[2023-08-31 00:54:08,611] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12909
[2023-08-31 00:54:08,611] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12909
[2023-08-31 00:54:08,611] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:54:08,611] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:54:08,611] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.9582 (1.9233)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2218 (8.5415)  time: 0.8248 (0.5327 -- 3.3166)  data: 0.2782 (0.0005 -- 2.7688)  max mem: 16413
Epoch: [80]  [140/160]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000000  loss: 1.6265 (1.8901)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5565 (8.5688)  time: 0.8809 (0.5294 -- 3.3120)  data: 0.3243 (0.0002 -- 2.7228)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.8955 (1.9045)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0287 (8.7046)  time: 0.6839 (0.4982 -- 1.7821)  data: 0.0910 (0.0003 -- 1.0401)  max mem: 16413
Epoch: [80] Total time: 0:02:20 (0.8752 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.8955 (1.8986)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0287 (8.7046)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.5390 (0.5390)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5244 (2.5244 -- 2.5244)  data: 2.2720 (2.2720 -- 2.2720)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5953 (0.8746)  acc1: 88.8889 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4519 (0.1968 -- 2.5244)  data: 0.2321 (0.0006 -- 2.2720)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6319 (0.8243)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2211 (0.1697 -- 0.4864)  data: 0.0142 (0.0001 -- 0.2721)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7342 (0.8695)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.4357)  time: 0.2061 (0.1327 -- 0.4864)  data: 0.0139 (0.0001 -- 0.2721)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 77.386 Acc@5 96.266 loss 0.821
Accuracy of the network on the 482 val images: 77.39%
[2023-08-31 00:54:56,931] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 00:54:56,933] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 00:54:56,933] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 00:54:56,933] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 00:54:58,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 00:54:58,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.39%
Epoch: [81]  [  0/160]  eta: 0:20:31  lr: 0.000016  min_lr: 0.000000  loss: 2.3146 (2.3146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5788 (6.5788)  time: 7.6972 (7.6972 -- 7.6972)  data: 7.1499 (7.1499 -- 7.1499)  max mem: 16413
Epoch: [81]  [ 20/160]  eta: 0:02:49  lr: 0.000016  min_lr: 0.000000  loss: 1.9190 (1.9454)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3774 (8.0087)  time: 0.8830 (0.5195 -- 3.2214)  data: 0.0563 (0.0002 -- 1.0888)  max mem: 16413
[2023-08-31 00:55:40,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=74, lr=[3.7226363924115567e-07, 3.7226363924115567e-07, 4.963515189882075e-07, 4.963515189882075e-07, 6.618020253176101e-07, 6.618020253176101e-07, 8.824027004234801e-07, 8.824027004234801e-07, 1.1765369338979734e-06, 1.1765369338979734e-06, 1.5687159118639646e-06, 1.5687159118639646e-06, 2.0916212158186196e-06, 2.0916212158186196e-06, 2.7888282877581594e-06, 2.7888282877581594e-06, 3.718437717010879e-06, 3.718437717010879e-06, 4.957916956014506e-06, 4.957916956014506e-06, 6.610555941352674e-06, 6.610555941352674e-06, 8.814074588470233e-06, 8.814074588470233e-06, 1.1752099451293643e-05, 1.1752099451293643e-05, 1.566946593505819e-05, 1.566946593505819e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 00:55:40,415] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=17.15331775800677, CurrSamplesPerSec=21.348290644512335, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:04  lr: 0.000016  min_lr: 0.000000  loss: 1.6531 (1.8426)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1662 (8.2347)  time: 0.8655 (0.5148 -- 4.1766)  data: 0.0015 (0.0003 -- 0.0049)  max mem: 16413
Epoch: [81]  [ 60/160]  eta: 0:01:38  lr: 0.000016  min_lr: 0.000000  loss: 2.0514 (1.9026)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6679 (8.1314)  time: 0.8795 (0.5338 -- 2.7834)  data: 0.2053 (0.0003 -- 2.2346)  max mem: 16413
[2023-08-31 00:56:16,384] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:56:16,384] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:56:16,384] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 00:56:16,384] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [81]  [ 80/160]  eta: 0:01:18  lr: 0.000016  min_lr: 0.000000  loss: 1.9486 (1.8893)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9932 (7.9326)  time: 0.9422 (0.5149 -- 3.8536)  data: 0.2855 (0.0003 -- 3.3427)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.8453 (1.8850)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9144 (7.9578)  time: 0.8221 (0.5256 -- 2.3335)  data: 0.1833 (0.0002 -- 1.8186)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7876 (1.8699)  loss_scale: 32768.0000 (22206.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9933 (7.8560)  time: 0.7666 (0.5205 -- 2.8856)  data: 0.0019 (0.0004 -- 0.0051)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.9126 (1.8722)  loss_scale: 32768.0000 (23704.5106)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8330 (7.8891)  time: 0.9254 (0.5180 -- 4.4343)  data: 0.0012 (0.0002 -- 0.0023)  max mem: 16413
[2023-08-31 00:57:09,903] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13104
[2023-08-31 00:57:09,903] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13104
[2023-08-31 00:57:09,903] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:57:09,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 00:57:09,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 00:57:19,664] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13115
[2023-08-31 00:57:19,665] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:57:19,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13115
[2023-08-31 00:57:19,665] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 00:57:19,665] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7442 (1.8579)  loss_scale: 16384.0000 (22886.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9157 (7.9541)  time: 0.7240 (0.4834 -- 3.7082)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [81] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7442 (1.8574)  loss_scale: 16384.0000 (22886.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9157 (7.9541)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.6539 (0.6539)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.6423 (2.6423 -- 2.6423)  data: 2.3835 (2.3835 -- 2.3835)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6539 (0.8952)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4502 (0.2001 -- 2.6423)  data: 0.2341 (0.0004 -- 2.3835)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6616 (0.8332)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2110 (0.1696 -- 0.4122)  data: 0.0097 (0.0001 -- 0.1818)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7340 (0.8836)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (95.4357)  time: 0.1973 (0.1327 -- 0.4122)  data: 0.0095 (0.0001 -- 0.1818)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 76.556 Acc@5 96.266 loss 0.841
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 77.39%
Epoch: [82]  [  0/160]  eta: 0:21:43  lr: 0.000016  min_lr: 0.000000  loss: 2.0928 (2.0928)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9911 (9.9911)  time: 8.1456 (8.1456 -- 8.1456)  data: 6.0767 (6.0767 -- 6.0767)  max mem: 16413
Epoch: [82]  [ 20/160]  eta: 0:02:42  lr: 0.000016  min_lr: 0.000000  loss: 1.8918 (1.9484)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4554 (8.5394)  time: 0.8110 (0.5176 -- 3.3030)  data: 0.0586 (0.0005 -- 0.6375)  max mem: 16413
Epoch: [82]  [ 40/160]  eta: 0:02:12  lr: 0.000015  min_lr: 0.000000  loss: 1.8278 (1.8981)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4735 (8.5358)  time: 1.0461 (0.5302 -- 4.4373)  data: 0.4950 (0.0004 -- 3.9027)  max mem: 16413
Epoch: [82]  [ 60/160]  eta: 0:01:38  lr: 0.000015  min_lr: 0.000000  loss: 1.8913 (1.8843)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7218 (8.5883)  time: 0.7486 (0.5140 -- 2.8323)  data: 0.2056 (0.0002 -- 2.3240)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:14  lr: 0.000015  min_lr: 0.000000  loss: 1.9169 (1.9116)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4415 (8.5228)  time: 0.7395 (0.5307 -- 3.8057)  data: 0.1861 (0.0003 -- 3.2823)  max mem: 16413
Epoch: [82]  [100/160]  eta: 0:00:54  lr: 0.000015  min_lr: 0.000000  loss: 2.0684 (1.9286)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8236 (8.4098)  time: 0.8383 (0.5251 -- 3.0622)  data: 0.1153 (0.0003 -- 1.5233)  max mem: 16413
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.8610 (1.9120)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9653 (8.3759)  time: 0.9197 (0.5302 -- 2.7933)  data: 0.2262 (0.0004 -- 2.2804)  max mem: 16413
[2023-08-31 00:59:22,700] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:59:22,700] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 00:59:22,701] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 00:59:22,741] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.6871 (1.8989)  loss_scale: 16384.0000 (9179.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6973 (8.2758)  time: 0.9443 (0.5081 -- 3.4849)  data: 0.0892 (0.0002 -- 1.7449)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.7652 (1.8898)  loss_scale: 16384.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9191 (8.2857)  time: 0.7612 (0.4968 -- 3.0161)  data: 0.0007 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [82] Total time: 0:02:21 (0.8834 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.7652 (1.8943)  loss_scale: 16384.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9191 (8.2857)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6192 (0.6192)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3236 (2.3236 -- 2.3236)  data: 2.0904 (2.0904 -- 2.0904)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6192 (0.8798)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (93.9394)  time: 0.4200 (0.1970 -- 2.3236)  data: 0.2012 (0.0008 -- 2.0904)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6389 (0.8283)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (95.2381)  time: 0.2219 (0.1697 -- 0.3572)  data: 0.0159 (0.0001 -- 0.1764)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7566 (0.8733)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (95.0207)  time: 0.2067 (0.1332 -- 0.3572)  data: 0.0155 (0.0001 -- 0.1764)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 76.763 Acc@5 95.643 loss 0.830
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 77.39%
Epoch: [83]  [  0/160]  eta: 0:22:48  lr: 0.000015  min_lr: 0.000000  loss: 1.9655 (1.9655)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8587 (6.8587)  time: 8.5521 (8.5521 -- 8.5521)  data: 7.5028 (7.5028 -- 7.5028)  max mem: 16413
Epoch: [83]  [ 20/160]  eta: 0:02:41  lr: 0.000015  min_lr: 0.000000  loss: 1.8743 (1.8776)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4457 (8.7616)  time: 0.7831 (0.5126 -- 3.2828)  data: 0.2247 (0.0007 -- 2.6183)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:01:57  lr: 0.000015  min_lr: 0.000000  loss: 1.8164 (1.8132)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1096 (8.5639)  time: 0.7901 (0.5282 -- 2.7041)  data: 0.2112 (0.0004 -- 2.1891)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:37  lr: 0.000015  min_lr: 0.000000  loss: 1.9662 (1.8612)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9013 (8.3460)  time: 0.9706 (0.5231 -- 3.2026)  data: 0.1403 (0.0003 -- 1.2363)  max mem: 16413
Epoch: [83]  [ 80/160]  eta: 0:01:14  lr: 0.000015  min_lr: 0.000000  loss: 1.9394 (1.8620)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6446 (8.3193)  time: 0.7841 (0.5301 -- 2.3810)  data: 0.0281 (0.0004 -- 0.5225)  max mem: 16413
[2023-08-31 01:01:22,824] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:01:22,824] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:01:22,824] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:01:22,825] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [83]  [100/160]  eta: 0:00:54  lr: 0.000015  min_lr: 0.000000  loss: 2.0400 (1.8979)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2017 (8.1017)  time: 0.8307 (0.5245 -- 2.7968)  data: 0.2097 (0.0003 -- 2.2892)  max mem: 16413
[2023-08-31 01:01:44,040] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13395
[2023-08-31 01:01:44,040] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:01:44,040] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13395
[2023-08-31 01:01:44,040] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 01:01:44,040] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [83]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.6580 (1.8717)  loss_scale: 32768.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6182 (8.0944)  time: 0.9597 (0.5254 -- 2.9768)  data: 0.3390 (0.0003 -- 2.4411)  max mem: 16413
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.7156 (1.8577)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8534 (8.2585)  time: 0.8092 (0.5168 -- 2.4323)  data: 0.1351 (0.0003 -- 1.8788)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 2.0285 (1.8724)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8534 (8.3379)  time: 0.7152 (0.4947 -- 3.1274)  data: 0.0215 (0.0001 -- 0.2367)  max mem: 16413
Epoch: [83] Total time: 0:02:20 (0.8807 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 2.0285 (1.8934)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8534 (8.3379)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6254 (0.6254)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4546 (2.4546 -- 2.4546)  data: 2.2524 (2.2524 -- 2.2524)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6254 (0.8742)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4327 (0.2033 -- 2.4546)  data: 0.2189 (0.0006 -- 2.2524)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6345 (0.8258)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (96.2963)  time: 0.2197 (0.1685 -- 0.3667)  data: 0.0122 (0.0001 -- 0.1262)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7362 (0.8712)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (95.4357)  time: 0.2038 (0.1332 -- 0.3667)  data: 0.0116 (0.0001 -- 0.1262)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 76.349 Acc@5 96.473 loss 0.828
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 77.39%
Epoch: [84]  [  0/160]  eta: 0:22:24  lr: 0.000015  min_lr: 0.000000  loss: 2.4591 (2.4591)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1716 (9.1716)  time: 8.4005 (8.4005 -- 8.4005)  data: 7.8520 (7.8520 -- 7.8520)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:56  lr: 0.000015  min_lr: 0.000000  loss: 1.6322 (1.7409)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2703 (7.5726)  time: 0.9013 (0.5225 -- 4.3032)  data: 0.3484 (0.0003 -- 3.7912)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:06  lr: 0.000015  min_lr: 0.000000  loss: 1.9252 (1.8129)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6924 (7.7970)  time: 0.8375 (0.5351 -- 3.4293)  data: 0.2819 (0.0002 -- 2.8544)  max mem: 16413
Epoch: [84]  [ 60/160]  eta: 0:01:37  lr: 0.000015  min_lr: 0.000000  loss: 1.9466 (1.8632)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7199 (7.8117)  time: 0.8186 (0.5171 -- 3.3382)  data: 0.2644 (0.0005 -- 2.7861)  max mem: 16413
Epoch: [84]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 1.7090 (1.8437)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5068 (7.8279)  time: 0.9194 (0.5281 -- 3.8713)  data: 0.3678 (0.0003 -- 3.3598)  max mem: 16413
[2023-08-31 01:03:47,879] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:03:47,879] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:03:47,881] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:03:47,882] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [84]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.8501 (1.8380)  loss_scale: 32768.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1377 (7.6906)  time: 0.7818 (0.5311 -- 3.3764)  data: 0.2138 (0.0003 -- 2.8545)  max mem: 16413
Epoch: [84]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.9386 (1.8518)  loss_scale: 32768.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6179 (7.7957)  time: 0.8665 (0.5184 -- 2.4978)  data: 0.2274 (0.0005 -- 1.9339)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.8030 (1.8473)  loss_scale: 32768.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0086 (7.9801)  time: 0.8500 (0.5295 -- 2.7292)  data: 0.0018 (0.0004 -- 0.0055)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.8583 (1.8433)  loss_scale: 32768.0000 (24166.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4313 (7.9432)  time: 0.7434 (0.4972 -- 2.4526)  data: 0.0525 (0.0003 -- 0.6226)  max mem: 16413
Epoch: [84] Total time: 0:02:20 (0.8772 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.8583 (1.8747)  loss_scale: 32768.0000 (24166.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4313 (7.9432)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.5510 (0.5510)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2554 (2.2554 -- 2.2554)  data: 2.0124 (2.0124 -- 2.0124)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5889 (0.8740)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (95.9596)  time: 0.4385 (0.1992 -- 2.2554)  data: 0.2295 (0.0005 -- 2.0124)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6006 (0.8161)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (96.2963)  time: 0.2300 (0.1693 -- 0.7194)  data: 0.0331 (0.0001 -- 0.5036)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7328 (0.8642)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (95.4357)  time: 0.2178 (0.1332 -- 0.7194)  data: 0.0329 (0.0001 -- 0.5036)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 76.971 Acc@5 96.058 loss 0.816
Accuracy of the network on the 482 val images: 76.97%
Max accuracy: 77.39%
Epoch: [85]  [  0/160]  eta: 0:23:03  lr: 0.000015  min_lr: 0.000000  loss: 1.6290 (1.6290)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1334 (8.1334)  time: 8.6455 (8.6455 -- 8.6455)  data: 6.8992 (6.8992 -- 6.8992)  max mem: 16413
[2023-08-31 01:05:05,791] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13603
[2023-08-31 01:05:05,791] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13603
[2023-08-31 01:05:05,791] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:05:05,791] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:05:05,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [ 20/160]  eta: 0:02:39  lr: 0.000015  min_lr: 0.000000  loss: 1.8686 (1.8317)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9070 (7.4972)  time: 0.7656 (0.5127 -- 2.4207)  data: 0.0015 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [85]  [ 40/160]  eta: 0:02:02  lr: 0.000015  min_lr: 0.000000  loss: 1.9446 (1.8423)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6791 (7.7308)  time: 0.9004 (0.5281 -- 2.4603)  data: 0.0013 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [85]  [ 60/160]  eta: 0:01:34  lr: 0.000015  min_lr: 0.000000  loss: 1.5609 (1.7886)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5507 (7.7486)  time: 0.7932 (0.5128 -- 2.6711)  data: 0.0995 (0.0004 -- 1.8157)  max mem: 16413
Epoch: [85]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 2.0046 (1.8337)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5982 (7.8976)  time: 0.9771 (0.5246 -- 3.1262)  data: 0.2555 (0.0005 -- 2.5779)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000000  loss: 1.6590 (1.8090)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1458 (7.9088)  time: 0.8600 (0.5216 -- 4.1163)  data: 0.3148 (0.0006 -- 3.5913)  max mem: 16413
Epoch: [85]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8003 (1.8211)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0918 (8.2004)  time: 0.9058 (0.5173 -- 3.8295)  data: 0.3665 (0.0002 -- 3.2912)  max mem: 16413
[2023-08-31 01:06:57,783] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:06:57,783] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:06:57,784] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:06:57,784] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:07:02,513] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13737
[2023-08-31 01:07:02,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:07:02,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13737
[2023-08-31 01:07:02,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:07:02,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.8206 (1.8265)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1584 (8.2198)  time: 0.7947 (0.5193 -- 3.3450)  data: 0.2452 (0.0001 -- 2.8147)  max mem: 16413
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.9694 (1.8424)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0447 (8.2762)  time: 0.6492 (0.4970 -- 1.2594)  data: 0.0558 (0.0002 -- 0.6303)  max mem: 16413
Epoch: [85] Total time: 0:02:21 (0.8816 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.9694 (1.8805)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0447 (8.2762)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5177 (0.5177)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3746 (2.3746 -- 2.3746)  data: 2.1551 (2.1551 -- 2.1551)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5759 (0.8684)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (94.9495)  time: 0.4247 (0.2070 -- 2.3746)  data: 0.1996 (0.0004 -- 2.1551)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6363 (0.8178)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (95.7672)  time: 0.2225 (0.1686 -- 0.4114)  data: 0.0130 (0.0001 -- 0.2144)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7038 (0.8668)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (94.6058)  time: 0.2031 (0.1331 -- 0.4114)  data: 0.0126 (0.0001 -- 0.2144)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 77.178 Acc@5 96.058 loss 0.816
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 77.39%
Epoch: [86]  [  0/160]  eta: 0:17:45  lr: 0.000015  min_lr: 0.000000  loss: 1.9329 (1.9329)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8402 (7.8402)  time: 6.6621 (6.6621 -- 6.6621)  data: 6.1223 (6.1223 -- 6.1223)  max mem: 16413
Epoch: [86]  [ 20/160]  eta: 0:02:45  lr: 0.000015  min_lr: 0.000000  loss: 1.9669 (1.9210)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5952 (8.7059)  time: 0.9100 (0.5458 -- 2.5397)  data: 0.0920 (0.0004 -- 1.3806)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000000  loss: 1.7422 (1.8622)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3783 (8.7453)  time: 0.9057 (0.5087 -- 3.6993)  data: 0.0325 (0.0004 -- 0.6274)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:40  lr: 0.000015  min_lr: 0.000000  loss: 1.8354 (1.8561)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9745 (8.8419)  time: 0.9311 (0.5250 -- 4.9290)  data: 0.0021 (0.0002 -- 0.0167)  max mem: 16413
Epoch: [86]  [ 80/160]  eta: 0:01:15  lr: 0.000015  min_lr: 0.000000  loss: 1.9541 (1.8813)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7636 (8.8978)  time: 0.7393 (0.5219 -- 2.7586)  data: 0.0017 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [86]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.8949 (1.8792)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3899 (8.9606)  time: 0.8146 (0.5225 -- 3.5926)  data: 0.0766 (0.0001 -- 1.5044)  max mem: 16413
[2023-08-31 01:09:03,317] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:09:03,318] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:09:03,318] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:09:03,318] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:09:14,954] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13879
[2023-08-31 01:09:14,954] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13879
[2023-08-31 01:09:14,954] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:09:14,954] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:09:14,955] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [86]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.9869 (1.9030)  loss_scale: 32768.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2700 (8.8864)  time: 1.0844 (0.5308 -- 3.8525)  data: 0.5399 (0.0005 -- 3.3273)  max mem: 16413
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.8466 (1.8886)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8971 (8.8928)  time: 0.8303 (0.5281 -- 3.7357)  data: 0.2852 (0.0004 -- 3.2113)  max mem: 16413
[2023-08-31 01:09:44,109] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13911
[2023-08-31 01:09:44,109] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13911
[2023-08-31 01:09:44,109] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:09:44,109] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:09:44,109] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.8029 (1.8770)  loss_scale: 16384.0000 (17254.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5427 (8.8348)  time: 0.6607 (0.4798 -- 3.5699)  data: 0.1534 (0.0002 -- 3.0597)  max mem: 16413
Epoch: [86] Total time: 0:02:23 (0.8980 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.8029 (1.8518)  loss_scale: 16384.0000 (17254.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5427 (8.8348)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.5084 (0.5084)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2771 (2.2771 -- 2.2771)  data: 2.0618 (2.0618 -- 2.0618)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5952 (0.8803)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (94.9495)  time: 0.4313 (0.1985 -- 2.2771)  data: 0.2187 (0.0006 -- 2.0618)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6344 (0.8187)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (95.7672)  time: 0.2243 (0.1687 -- 0.5616)  data: 0.0179 (0.0001 -- 0.3349)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7652 (0.8768)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (95.0207)  time: 0.2094 (0.1327 -- 0.5616)  data: 0.0176 (0.0001 -- 0.3349)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 77.386 Acc@5 96.058 loss 0.826
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 77.39%
Epoch: [87]  [  0/160]  eta: 0:22:02  lr: 0.000015  min_lr: 0.000000  loss: 1.8402 (1.8402)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7108 (8.7108)  time: 8.2655 (8.2655 -- 8.2655)  data: 7.6888 (7.6888 -- 7.6888)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:52  lr: 0.000015  min_lr: 0.000000  loss: 1.8945 (1.8220)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3464 (9.0314)  time: 0.8803 (0.5312 -- 4.2300)  data: 0.3222 (0.0002 -- 3.7045)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000000  loss: 1.7505 (1.8149)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4397 (8.4336)  time: 0.8463 (0.5252 -- 3.1776)  data: 0.2914 (0.0009 -- 2.6460)  max mem: 16413
Epoch: [87]  [ 60/160]  eta: 0:01:36  lr: 0.000015  min_lr: 0.000000  loss: 1.8588 (1.8207)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7292 (8.6832)  time: 0.8056 (0.5304 -- 3.1192)  data: 0.2539 (0.0007 -- 2.6004)  max mem: 16413
[2023-08-31 01:11:09,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=81, lr=[3.454829411413636e-07, 3.454829411413636e-07, 4.6064392152181815e-07, 4.6064392152181815e-07, 6.141918953624241e-07, 6.141918953624241e-07, 8.189225271498989e-07, 8.189225271498989e-07, 1.0918967028665318e-06, 1.0918967028665318e-06, 1.455862270488709e-06, 1.455862270488709e-06, 1.9411496939849457e-06, 1.9411496939849457e-06, 2.5881995919799273e-06, 2.5881995919799273e-06, 3.4509327893065697e-06, 3.4509327893065697e-06, 4.6012437190754266e-06, 4.6012437190754266e-06, 6.134991625433902e-06, 6.134991625433902e-06, 8.179988833911868e-06, 8.179988833911868e-06, 1.0906651778549159e-05, 1.0906651778549159e-05, 1.4542202371398879e-05, 1.4542202371398879e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 01:11:09,607] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=17.235821961799946, CurrSamplesPerSec=21.595726477390233, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:14  lr: 0.000015  min_lr: 0.000000  loss: 2.1465 (1.8956)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3524 (8.5041)  time: 0.8035 (0.5279 -- 3.1119)  data: 0.2170 (0.0004 -- 2.5696)  max mem: 16413
Epoch: [87]  [100/160]  eta: 0:00:54  lr: 0.000015  min_lr: 0.000000  loss: 1.8043 (1.8748)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1413 (8.5380)  time: 0.8797 (0.5252 -- 3.1970)  data: 0.0545 (0.0003 -- 0.6114)  max mem: 16413
[2023-08-31 01:11:46,365] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:11:46,365] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:11:46,366] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:11:46,366] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [87]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.9177 (1.8803)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1995 (8.3799)  time: 0.8963 (0.5142 -- 2.6903)  data: 0.3292 (0.0004 -- 2.1676)  max mem: 16413
Epoch: [87]  [140/160]  eta: 0:00:17  lr: 0.000014  min_lr: 0.000000  loss: 1.9288 (1.8913)  loss_scale: 16384.0000 (9412.0851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5388 (8.1619)  time: 0.7616 (0.5263 -- 2.9842)  data: 0.1791 (0.0003 -- 2.4356)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.8678 (1.8935)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9739 (8.1475)  time: 0.7934 (0.4967 -- 4.5684)  data: 0.2511 (0.0003 -- 4.0361)  max mem: 16413
Epoch: [87] Total time: 0:02:20 (0.8787 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.8678 (1.8635)  loss_scale: 16384.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9739 (8.1475)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.5001 (0.5001)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4594 (2.4594 -- 2.4594)  data: 2.1705 (2.1705 -- 2.1705)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5982 (0.8646)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4176 (0.2010 -- 2.4594)  data: 0.1987 (0.0009 -- 2.1705)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6133 (0.8057)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (96.2963)  time: 0.2117 (0.1702 -- 0.3115)  data: 0.0092 (0.0001 -- 0.1350)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7638 (0.8605)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.0207)  time: 0.1962 (0.1335 -- 0.3115)  data: 0.0089 (0.0001 -- 0.1350)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 77.593 Acc@5 96.266 loss 0.812
Accuracy of the network on the 482 val images: 77.59%
[2023-08-31 01:12:24,160] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 01:12:24,161] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 01:12:24,162] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 01:12:24,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 01:12:25,526] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 01:12:25,526] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.59%
Epoch: [88]  [  0/160]  eta: 0:16:04  lr: 0.000014  min_lr: 0.000000  loss: 2.0499 (2.0499)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9776 (3.9776)  time: 6.0299 (6.0299 -- 6.0299)  data: 5.4674 (5.4674 -- 5.4674)  max mem: 16413
Epoch: [88]  [ 20/160]  eta: 0:02:42  lr: 0.000014  min_lr: 0.000000  loss: 2.0711 (1.9816)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0361 (8.6106)  time: 0.9158 (0.5250 -- 3.0023)  data: 0.3572 (0.0003 -- 2.4635)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:01:57  lr: 0.000014  min_lr: 0.000000  loss: 1.7219 (1.8374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8409 (8.7584)  time: 0.7888 (0.5323 -- 3.0643)  data: 0.2370 (0.0007 -- 2.5047)  max mem: 16413
Epoch: [88]  [ 60/160]  eta: 0:01:33  lr: 0.000014  min_lr: 0.000000  loss: 1.8709 (1.8532)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0437 (8.6471)  time: 0.8423 (0.5474 -- 2.1218)  data: 0.2581 (0.0005 -- 1.5989)  max mem: 16413
Epoch: [88]  [ 80/160]  eta: 0:01:13  lr: 0.000014  min_lr: 0.000000  loss: 1.9023 (1.8544)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7570 (8.5506)  time: 0.8743 (0.5257 -- 2.3990)  data: 0.2097 (0.0005 -- 1.4119)  max mem: 16413
[2023-08-31 01:13:47,463] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:13:47,463] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:13:47,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:13:47,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:13:50,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14172
[2023-08-31 01:13:50,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14172
[2023-08-31 01:13:50,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:13:50,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:13:50,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [88]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 2.0082 (1.8701)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6729 (8.4829)  time: 0.9315 (0.5302 -- 3.8548)  data: 0.2403 (0.0002 -- 3.3080)  max mem: 16413
Epoch: [88]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.9651 (1.8804)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4733 (8.5822)  time: 0.9287 (0.5118 -- 3.7139)  data: 0.3840 (0.0004 -- 3.1980)  max mem: 16413
Epoch: [88]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.8011 (1.8678)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1055 (8.6029)  time: 0.8330 (0.5178 -- 3.5359)  data: 0.2894 (0.0003 -- 3.0215)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.8563 (1.8622)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2634 (8.5838)  time: 0.6694 (0.4975 -- 2.7346)  data: 0.1544 (0.0002 -- 2.2038)  max mem: 16413
Epoch: [88] Total time: 0:02:21 (0.8822 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.8563 (1.8678)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2634 (8.5838)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.4970 (0.4970)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5285 (2.5285 -- 2.5285)  data: 2.2624 (2.2624 -- 2.2624)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5837 (0.8707)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4446 (0.2015 -- 2.5285)  data: 0.2219 (0.0005 -- 2.2624)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5879 (0.8050)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (96.2963)  time: 0.2200 (0.1698 -- 0.3778)  data: 0.0162 (0.0001 -- 0.1513)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7197 (0.8592)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (95.0207)  time: 0.2035 (0.1350 -- 0.3778)  data: 0.0150 (0.0001 -- 0.1513)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 76.141 Acc@5 96.058 loss 0.813
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 77.59%
Epoch: [89]  [  0/160]  eta: 0:22:27  lr: 0.000014  min_lr: 0.000000  loss: 1.8815 (1.8815)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6517 (9.6517)  time: 8.4224 (8.4224 -- 8.4224)  data: 7.8880 (7.8880 -- 7.8880)  max mem: 16413
Epoch: [89]  [ 20/160]  eta: 0:02:50  lr: 0.000014  min_lr: 0.000000  loss: 1.8187 (1.8400)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4708 (8.5406)  time: 0.8580 (0.5251 -- 4.0880)  data: 0.3128 (0.0003 -- 3.5405)  max mem: 16413
Epoch: [89]  [ 40/160]  eta: 0:02:08  lr: 0.000014  min_lr: 0.000000  loss: 1.8816 (1.8616)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9635 (8.7497)  time: 0.9107 (0.5185 -- 3.3836)  data: 0.3580 (0.0002 -- 2.8347)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000000  loss: 1.7585 (1.8610)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9679 (8.6236)  time: 0.7812 (0.5312 -- 2.2657)  data: 0.2317 (0.0008 -- 1.7128)  max mem: 16413
[2023-08-31 01:15:54,606] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:15:54,606] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:15:54,608] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:15:54,608] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [89]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000000  loss: 1.7959 (1.8703)  loss_scale: 32768.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8236 (8.5045)  time: 0.8965 (0.5269 -- 2.9908)  data: 0.3190 (0.0005 -- 2.4584)  max mem: 16413
[2023-08-31 01:16:18,755] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14329
[2023-08-31 01:16:18,756] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:16:18,755] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14329
[2023-08-31 01:16:18,756] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:16:18,756] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [89]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 1.8046 (1.8619)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8430 (8.5518)  time: 0.8128 (0.5208 -- 2.2513)  data: 0.2577 (0.0002 -- 1.7265)  max mem: 16413
Epoch: [89]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.6957 (1.8309)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9263 (8.4359)  time: 0.8606 (0.5267 -- 2.1653)  data: 0.2415 (0.0003 -- 1.6425)  max mem: 16413
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 2.0509 (1.8661)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9231 (8.3481)  time: 0.9692 (0.5306 -- 3.2576)  data: 0.3244 (0.0003 -- 2.7246)  max mem: 16413
[2023-08-31 01:17:13,765] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14395
[2023-08-31 01:17:13,765] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14395
[2023-08-31 01:17:13,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:17:13,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:17:13,765] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.8263 (1.8620)  loss_scale: 16384.0000 (18995.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9678 (8.6644)  time: 0.6100 (0.4828 -- 1.2492)  data: 0.0161 (0.0002 -- 0.3068)  max mem: 16413
Epoch: [89] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.8263 (1.8985)  loss_scale: 16384.0000 (18995.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9678 (8.6644)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.5308 (0.5308)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2512 (2.2512 -- 2.2512)  data: 2.0471 (2.0471 -- 2.0471)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5897 (0.8736)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4282 (0.1992 -- 2.2512)  data: 0.2115 (0.0004 -- 2.0471)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6113 (0.8041)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.2963)  time: 0.2340 (0.1689 -- 0.5047)  data: 0.0273 (0.0001 -- 0.2710)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7444 (0.8625)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.4357)  time: 0.2177 (0.1325 -- 0.5047)  data: 0.0270 (0.0001 -- 0.2710)  max mem: 16413
Val: Total time: 0:00:07 (0.2932 s / it)
* Acc@1 77.178 Acc@5 96.058 loss 0.813
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 77.59%
Epoch: [90]  [  0/160]  eta: 0:21:06  lr: 0.000014  min_lr: 0.000000  loss: 2.2316 (2.2316)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9901 (10.9901)  time: 7.9145 (7.9145 -- 7.9145)  data: 7.3808 (7.3808 -- 7.3808)  max mem: 16413
Epoch: [90]  [ 20/160]  eta: 0:02:38  lr: 0.000014  min_lr: 0.000000  loss: 1.9002 (1.8869)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6061 (8.1311)  time: 0.7913 (0.5362 -- 3.1441)  data: 0.2313 (0.0005 -- 2.5918)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:06  lr: 0.000014  min_lr: 0.000000  loss: 2.0249 (1.9597)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6575 (8.2711)  time: 0.9742 (0.5321 -- 3.8200)  data: 0.4238 (0.0002 -- 3.2931)  max mem: 16413
Epoch: [90]  [ 60/160]  eta: 0:01:36  lr: 0.000014  min_lr: 0.000000  loss: 1.9690 (1.9818)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2575 (8.3521)  time: 0.7876 (0.5158 -- 2.9506)  data: 0.2188 (0.0003 -- 2.4212)  max mem: 16413
Epoch: [90]  [ 80/160]  eta: 0:01:14  lr: 0.000014  min_lr: 0.000000  loss: 1.9620 (1.9757)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7821 (8.4489)  time: 0.8186 (0.5062 -- 3.5597)  data: 0.2312 (0.0003 -- 3.0366)  max mem: 16413
Epoch: [90]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 1.8841 (1.9514)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8228 (8.3705)  time: 0.9999 (0.5229 -- 3.4113)  data: 0.3509 (0.0002 -- 2.8772)  max mem: 16413
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.7924 (1.9293)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9137 (8.4124)  time: 0.7404 (0.5201 -- 2.9510)  data: 0.1914 (0.0002 -- 2.4182)  max mem: 16413
[2023-08-31 01:19:19,351] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:19:19,352] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:19:19,352] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:19:19,352] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [90]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.8415 (1.9200)  loss_scale: 16384.0000 (9179.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7654 (8.3755)  time: 0.8948 (0.5289 -- 3.7943)  data: 0.3388 (0.0003 -- 3.2471)  max mem: 16413
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.9788 (1.9245)  loss_scale: 16384.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6252 (8.4615)  time: 0.6724 (0.4949 -- 2.0862)  data: 0.1467 (0.0002 -- 1.5510)  max mem: 16413
Epoch: [90] Total time: 0:02:20 (0.8809 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.9788 (1.9086)  loss_scale: 16384.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6252 (8.4615)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.4749 (0.4749)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2391 (2.2391 -- 2.2391)  data: 2.0102 (2.0102 -- 2.0102)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5885 (0.8635)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4004 (0.1935 -- 2.2391)  data: 0.1838 (0.0008 -- 2.0102)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6318 (0.8069)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2278 (0.1693 -- 0.5785)  data: 0.0218 (0.0001 -- 0.3940)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7527 (0.8605)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.4357)  time: 0.2121 (0.1333 -- 0.5785)  data: 0.0215 (0.0001 -- 0.3940)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 77.593 Acc@5 96.266 loss 0.812
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 77.59%
Epoch: [91]  [  0/160]  eta: 0:21:48  lr: 0.000014  min_lr: 0.000000  loss: 2.1642 (2.1642)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9868 (6.9868)  time: 8.1811 (8.1811 -- 8.1811)  data: 6.6870 (6.6870 -- 6.6870)  max mem: 16413
Epoch: [91]  [ 20/160]  eta: 0:02:45  lr: 0.000014  min_lr: 0.000000  loss: 1.8162 (1.8266)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9129 (8.2958)  time: 0.8329 (0.5276 -- 5.6193)  data: 0.1026 (0.0003 -- 2.0249)  max mem: 16413
Epoch: [91]  [ 40/160]  eta: 0:02:07  lr: 0.000014  min_lr: 0.000000  loss: 1.9420 (1.8569)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7826 (8.3724)  time: 0.9302 (0.5171 -- 4.1325)  data: 0.0112 (0.0008 -- 0.1907)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000000  loss: 2.1307 (1.9204)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1694 (8.2902)  time: 0.8032 (0.5310 -- 3.3055)  data: 0.0016 (0.0002 -- 0.0055)  max mem: 16413
Epoch: [91]  [ 80/160]  eta: 0:01:17  lr: 0.000014  min_lr: 0.000000  loss: 1.7778 (1.8885)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1334 (8.1725)  time: 0.9415 (0.5135 -- 4.1180)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16413
[2023-08-31 01:21:21,365] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:21:21,365] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:21:21,366] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:21:21,366] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [91]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 1.8230 (1.8783)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4312 (8.2709)  time: 0.7298 (0.5222 -- 3.7403)  data: 0.0023 (0.0005 -- 0.0168)  max mem: 16413
[2023-08-31 01:21:39,238] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14669
[2023-08-31 01:21:39,238] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14669
[2023-08-31 01:21:39,238] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:21:39,238] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:21:39,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [91]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 1.6436 (1.8471)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4939 (8.1246)  time: 1.0638 (0.5144 -- 4.7627)  data: 0.0017 (0.0004 -- 0.0067)  max mem: 16413
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.8152 (1.8379)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2665 (8.1967)  time: 0.7754 (0.5242 -- 3.0171)  data: 0.0023 (0.0004 -- 0.0156)  max mem: 16413
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.6832 (1.8213)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5245 (8.2683)  time: 0.6784 (0.4947 -- 1.9029)  data: 0.0010 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [91] Total time: 0:02:22 (0.8925 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.6832 (1.8458)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5245 (8.2683)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4866 (0.4866)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2992 (2.2992 -- 2.2992)  data: 2.0886 (2.0886 -- 2.0886)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6216 (0.8532)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4275 (0.1983 -- 2.2992)  data: 0.2116 (0.0006 -- 2.0886)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6238 (0.7949)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2255 (0.1701 -- 0.4391)  data: 0.0149 (0.0001 -- 0.2303)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6990 (0.8458)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.4357)  time: 0.2100 (0.1330 -- 0.4391)  data: 0.0146 (0.0001 -- 0.2303)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 77.386 Acc@5 96.058 loss 0.799
Accuracy of the network on the 482 val images: 77.39%
Max accuracy: 77.59%
Epoch: [92]  [  0/160]  eta: 0:20:15  lr: 0.000014  min_lr: 0.000000  loss: 1.7807 (1.7807)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0264 (13.0264)  time: 7.5962 (7.5962 -- 7.5962)  data: 6.8350 (6.8350 -- 6.8350)  max mem: 16413
Epoch: [92]  [ 20/160]  eta: 0:02:48  lr: 0.000014  min_lr: 0.000000  loss: 1.9624 (1.9025)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8404 (8.3384)  time: 0.8874 (0.5145 -- 2.8756)  data: 0.0926 (0.0005 -- 1.2421)  max mem: 16413
Epoch: [92]  [ 40/160]  eta: 0:02:05  lr: 0.000014  min_lr: 0.000000  loss: 1.8173 (1.8551)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4966 (8.4351)  time: 0.8810 (0.5196 -- 3.5336)  data: 0.0357 (0.0001 -- 0.3814)  max mem: 16413
Epoch: [92]  [ 60/160]  eta: 0:01:40  lr: 0.000014  min_lr: 0.000000  loss: 1.6793 (1.8462)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1454 (8.4429)  time: 0.9265 (0.5320 -- 2.7537)  data: 0.0677 (0.0004 -- 1.3219)  max mem: 16413
[2023-08-31 01:23:38,843] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:23:38,844] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:23:38,844] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:23:38,845] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [92]  [ 80/160]  eta: 0:01:15  lr: 0.000014  min_lr: 0.000000  loss: 1.9682 (1.8970)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7737 (8.3514)  time: 0.7351 (0.5304 -- 1.9522)  data: 0.0021 (0.0003 -- 0.0121)  max mem: 16413
Epoch: [92]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 1.8759 (1.8767)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7572 (8.2657)  time: 0.9104 (0.5230 -- 3.3876)  data: 0.0385 (0.0003 -- 0.7406)  max mem: 16413
[2023-08-31 01:24:05,901] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14830
[2023-08-31 01:24:05,901] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:24:05,901] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14830
[2023-08-31 01:24:05,901] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 01:24:05,901] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.7759 (1.8648)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4782 (8.4138)  time: 0.8359 (0.5075 -- 2.2224)  data: 0.0363 (0.0004 -- 0.6888)  max mem: 16413
Epoch: [92]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.9023 (1.8637)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3828 (8.4184)  time: 0.8658 (0.5334 -- 2.6295)  data: 0.0171 (0.0004 -- 0.2689)  max mem: 16413
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7470 (1.8639)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5622 (8.4797)  time: 0.7245 (0.4976 -- 3.5914)  data: 0.0006 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [92] Total time: 0:02:22 (0.8902 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7470 (1.8700)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5622 (8.4797)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4572 (0.4572)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4796 (2.4796 -- 2.4796)  data: 2.2430 (2.2430 -- 2.2430)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6202 (0.8563)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4204 (0.2014 -- 2.4796)  data: 0.2063 (0.0009 -- 2.2430)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6175 (0.7966)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2162 (0.1701 -- 0.4289)  data: 0.0130 (0.0001 -- 0.2280)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7142 (0.8533)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.4357)  time: 0.2028 (0.1336 -- 0.4289)  data: 0.0127 (0.0001 -- 0.2280)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 77.593 Acc@5 96.266 loss 0.804
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 77.59%
Epoch: [93]  [  0/160]  eta: 0:17:27  lr: 0.000014  min_lr: 0.000000  loss: 1.5427 (1.5427)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8353 (4.8353)  time: 6.5479 (6.5479 -- 6.5479)  data: 5.4052 (5.4052 -- 5.4052)  max mem: 16413
Epoch: [93]  [ 20/160]  eta: 0:02:47  lr: 0.000014  min_lr: 0.000000  loss: 1.8734 (1.8590)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5297 (9.0537)  time: 0.9306 (0.5223 -- 5.1494)  data: 0.0664 (0.0002 -- 0.6854)  max mem: 16413
[2023-08-31 01:25:25,216] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14907
[2023-08-31 01:25:25,216] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:25:25,216] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14907
[2023-08-31 01:25:25,216] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:25:25,217] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [93]  [ 40/160]  eta: 0:02:10  lr: 0.000013  min_lr: 0.000000  loss: 1.7091 (1.8291)  loss_scale: 8192.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4765 (8.5930)  time: 0.9651 (0.5229 -- 4.3647)  data: 0.0016 (0.0005 -- 0.0047)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:39  lr: 0.000013  min_lr: 0.000000  loss: 1.9229 (1.8378)  loss_scale: 8192.0000 (11817.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3829 (8.5119)  time: 0.8057 (0.5214 -- 3.2204)  data: 0.0016 (0.0001 -- 0.0040)  max mem: 16413
Epoch: [93]  [ 80/160]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000000  loss: 1.8533 (1.8448)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9258 (8.5932)  time: 0.8774 (0.5140 -- 3.0785)  data: 0.0010 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [93]  [100/160]  eta: 0:00:57  lr: 0.000013  min_lr: 0.000000  loss: 1.8024 (1.8391)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9313 (8.5600)  time: 0.9183 (0.5287 -- 3.0233)  data: 0.0017 (0.0002 -- 0.0068)  max mem: 16413
[2023-08-31 01:26:44,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=87, lr=[3.1802836648250215e-07, 3.1802836648250215e-07, 4.2403782197666955e-07, 4.2403782197666955e-07, 5.653837626355594e-07, 5.653837626355594e-07, 7.538450168474125e-07, 7.538450168474125e-07, 1.0051266891298835e-06, 1.0051266891298835e-06, 1.3401689188398446e-06, 1.3401689188398446e-06, 1.7868918917864594e-06, 1.7868918917864594e-06, 2.382522522381946e-06, 2.382522522381946e-06, 3.176696696509261e-06, 3.176696696509261e-06, 4.235595595345682e-06, 4.235595595345682e-06, 5.647460793794242e-06, 5.647460793794242e-06, 7.529947725058989e-06, 7.529947725058989e-06, 1.0039930300078653e-05, 1.0039930300078653e-05, 1.3386573733438203e-05, 1.3386573733438203e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 01:26:44,868] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=17.25973325852245, CurrSamplesPerSec=19.631322342545023, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [93]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.9286 (1.8534)  loss_scale: 8192.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6479 (8.5148)  time: 0.7433 (0.5228 -- 2.5868)  data: 0.0017 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 2.0727 (1.8636)  loss_scale: 8192.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2617 (8.7110)  time: 0.9006 (0.5367 -- 4.3119)  data: 0.0015 (0.0003 -- 0.0058)  max mem: 16413
[2023-08-31 01:27:14,439] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:27:14,439] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:27:14,440] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:27:14,440] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7553 (1.8516)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4855 (8.6768)  time: 0.6855 (0.4957 -- 2.1425)  data: 0.0384 (0.0002 -- 0.5975)  max mem: 16413
Epoch: [93] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.7553 (1.8738)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4855 (8.6768)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5048 (0.5048)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4081 (2.4081 -- 2.4081)  data: 2.1907 (2.1907 -- 2.1907)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6113 (0.8501)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4386 (0.1969 -- 2.4081)  data: 0.2237 (0.0008 -- 2.1907)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6113 (0.7951)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.2963)  time: 0.2181 (0.1728 -- 0.4741)  data: 0.0137 (0.0001 -- 0.2590)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7084 (0.8474)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.0207)  time: 0.2039 (0.1328 -- 0.4741)  data: 0.0133 (0.0001 -- 0.2590)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 77.593 Acc@5 95.851 loss 0.798
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 77.59%
Epoch: [94]  [  0/160]  eta: 0:20:26  lr: 0.000013  min_lr: 0.000000  loss: 2.1406 (2.1406)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2344 (8.2344)  time: 7.6658 (7.6658 -- 7.6658)  data: 6.9143 (6.9143 -- 6.9143)  max mem: 16413
Epoch: [94]  [ 20/160]  eta: 0:02:52  lr: 0.000013  min_lr: 0.000000  loss: 1.6856 (1.8034)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4735 (8.1635)  time: 0.9129 (0.5345 -- 4.6126)  data: 0.1026 (0.0004 -- 2.0228)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:01:57  lr: 0.000013  min_lr: 0.000000  loss: 1.9670 (1.8156)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5961 (8.1525)  time: 0.7168 (0.5221 -- 1.7898)  data: 0.0149 (0.0002 -- 0.1448)  max mem: 16413
Epoch: [94]  [ 60/160]  eta: 0:01:36  lr: 0.000013  min_lr: 0.000000  loss: 1.9311 (1.8297)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8841 (8.1682)  time: 0.9417 (0.5218 -- 3.9456)  data: 0.0022 (0.0002 -- 0.0143)  max mem: 16413
[2023-08-31 01:28:31,341] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15109
[2023-08-31 01:28:31,342] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:28:31,342] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15109
[2023-08-31 01:28:31,342] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:28:31,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [94]  [ 80/160]  eta: 0:01:14  lr: 0.000013  min_lr: 0.000000  loss: 1.8171 (1.8303)  loss_scale: 8192.0000 (15170.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5622 (8.2494)  time: 0.8207 (0.5267 -- 2.0524)  data: 0.1404 (0.0003 -- 1.5280)  max mem: 16413
Epoch: [94]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000000  loss: 1.9223 (1.8461)  loss_scale: 8192.0000 (13788.5149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8204 (8.2347)  time: 0.9524 (0.5242 -- 3.9120)  data: 0.3987 (0.0004 -- 3.3675)  max mem: 16413
Epoch: [94]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.8436 (1.8622)  loss_scale: 8192.0000 (12863.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1231 (8.2431)  time: 0.8141 (0.5190 -- 3.4629)  data: 0.2727 (0.0005 -- 2.9120)  max mem: 16413
Epoch: [94]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 2.0247 (1.8898)  loss_scale: 8192.0000 (12200.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5513 (8.3253)  time: 0.8198 (0.5254 -- 2.4508)  data: 0.2677 (0.0005 -- 1.9383)  max mem: 16413
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8477 (1.8863)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1902 (8.3605)  time: 0.6788 (0.4959 -- 1.6407)  data: 0.1132 (0.0002 -- 1.1245)  max mem: 16413
Epoch: [94] Total time: 0:02:20 (0.8771 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8477 (1.8653)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1902 (8.3605)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.5203 (0.5203)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4859 (2.4859 -- 2.4859)  data: 2.2120 (2.2120 -- 2.2120)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6097 (0.8477)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4505 (0.2030 -- 2.4859)  data: 0.2321 (0.0007 -- 2.2120)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6328 (0.7931)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2215 (0.1691 -- 0.5596)  data: 0.0189 (0.0001 -- 0.3296)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7455 (0.8450)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.2077 (0.1331 -- 0.5596)  data: 0.0185 (0.0001 -- 0.3296)  max mem: 16413
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 78.631 Acc@5 96.473 loss 0.800
Accuracy of the network on the 482 val images: 78.63%
[2023-08-31 01:29:52,587] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 01:29:52,589] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 01:29:52,589] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 01:29:52,589] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 01:29:54,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 01:29:54,029] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.63%
Epoch: [95]  [  0/160]  eta: 0:19:25  lr: 0.000013  min_lr: 0.000000  loss: 1.8305 (1.8305)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0489 (7.0489)  time: 7.2848 (7.2848 -- 7.2848)  data: 6.4567 (6.4567 -- 6.4567)  max mem: 16413
Epoch: [95]  [ 20/160]  eta: 0:02:57  lr: 0.000013  min_lr: 0.000000  loss: 2.0540 (1.9923)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2846 (7.6316)  time: 0.9675 (0.5115 -- 4.7582)  data: 0.0896 (0.0002 -- 1.7575)  max mem: 16413
[2023-08-31 01:30:34,834] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:30:34,834] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:30:34,834] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:30:34,834] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [95]  [ 40/160]  eta: 0:02:07  lr: 0.000013  min_lr: 0.000000  loss: 1.8712 (1.9445)  loss_scale: 8192.0000 (8791.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8585 (8.0606)  time: 0.8531 (0.5195 -- 4.2926)  data: 0.0015 (0.0002 -- 0.0065)  max mem: 16413
Epoch: [95]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000000  loss: 1.8758 (1.9296)  loss_scale: 16384.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6452 (8.3510)  time: 0.7993 (0.5216 -- 3.5754)  data: 0.0022 (0.0003 -- 0.0132)  max mem: 16413
Epoch: [95]  [ 80/160]  eta: 0:01:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8604 (1.9157)  loss_scale: 16384.0000 (12540.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4650 (8.3283)  time: 0.9674 (0.5177 -- 5.0263)  data: 0.0020 (0.0004 -- 0.0143)  max mem: 16413
Epoch: [95]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.8442 (1.9064)  loss_scale: 16384.0000 (13301.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7912 (8.3430)  time: 0.7366 (0.5153 -- 2.6408)  data: 0.0013 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [95]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.7674 (1.8920)  loss_scale: 16384.0000 (13811.3058)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7760 (8.3051)  time: 0.8947 (0.5277 -- 3.1351)  data: 0.1332 (0.0002 -- 2.5648)  max mem: 16413
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8192 (1.8920)  loss_scale: 16384.0000 (14176.2270)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1513 (8.2589)  time: 0.9293 (0.5186 -- 3.3633)  data: 0.1123 (0.0003 -- 1.3480)  max mem: 16413
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8662 (1.8930)  loss_scale: 16384.0000 (14438.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5906 (8.2848)  time: 0.7135 (0.4943 -- 3.3633)  data: 0.0007 (0.0001 -- 0.0016)  max mem: 16413
Epoch: [95] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8662 (1.8834)  loss_scale: 16384.0000 (14438.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5906 (8.2848)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5017 (0.5017)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4385 (2.4385 -- 2.4385)  data: 2.2126 (2.2126 -- 2.2126)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5914 (0.8489)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4206 (0.1974 -- 2.4385)  data: 0.2029 (0.0006 -- 2.2126)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5914 (0.7829)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2222 (0.1698 -- 0.4455)  data: 0.0137 (0.0001 -- 0.2512)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6950 (0.8382)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.0207)  time: 0.2054 (0.1325 -- 0.4455)  data: 0.0130 (0.0001 -- 0.2512)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 78.008 Acc@5 95.851 loss 0.792
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 78.63%
Epoch: [96]  [  0/160]  eta: 0:19:19  lr: 0.000013  min_lr: 0.000000  loss: 1.6810 (1.6810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2134 (9.2134)  time: 7.2493 (7.2493 -- 7.2493)  data: 5.9509 (5.9509 -- 5.9509)  max mem: 16413
[2023-08-31 01:32:33,604] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:32:33,604] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:32:33,604] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:32:33,604] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [96]  [ 20/160]  eta: 0:02:43  lr: 0.000013  min_lr: 0.000000  loss: 1.8898 (1.9457)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3298 (8.3843)  time: 0.8604 (0.5285 -- 4.4244)  data: 0.1942 (0.0001 -- 2.3367)  max mem: 16413
Epoch: [96]  [ 40/160]  eta: 0:02:05  lr: 0.000013  min_lr: 0.000000  loss: 1.6742 (1.8582)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8669 (8.1544)  time: 0.9234 (0.5353 -- 3.1880)  data: 0.2919 (0.0005 -- 2.2677)  max mem: 16413
Epoch: [96]  [ 60/160]  eta: 0:01:35  lr: 0.000013  min_lr: 0.000000  loss: 1.7880 (1.8428)  loss_scale: 32768.0000 (31156.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7292 (8.0042)  time: 0.7795 (0.5154 -- 3.2880)  data: 0.1383 (0.0003 -- 2.7399)  max mem: 16413
[2023-08-31 01:33:22,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15421
[2023-08-31 01:33:22,147] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:33:22,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15421
[2023-08-31 01:33:22,147] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:33:22,147] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [96]  [ 80/160]  eta: 0:01:16  lr: 0.000013  min_lr: 0.000000  loss: 1.8124 (1.8429)  loss_scale: 16384.0000 (27508.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5759 (8.0783)  time: 0.9243 (0.5152 -- 2.6495)  data: 0.1082 (0.0003 -- 1.3382)  max mem: 16413
Epoch: [96]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.8412 (1.8395)  loss_scale: 16384.0000 (25305.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2495 (8.2283)  time: 0.8205 (0.5345 -- 2.5546)  data: 0.2344 (0.0004 -- 1.9876)  max mem: 16413
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.8350 (1.8245)  loss_scale: 16384.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6361 (8.3159)  time: 0.8387 (0.5352 -- 2.9268)  data: 0.1161 (0.0007 -- 2.0299)  max mem: 16413
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8647 (1.8279)  loss_scale: 16384.0000 (22774.9220)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1855 (8.3457)  time: 0.9605 (0.5277 -- 2.8745)  data: 0.2377 (0.0004 -- 2.3421)  max mem: 16413
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.9537 (1.8455)  loss_scale: 16384.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5409 (8.3970)  time: 0.6505 (0.4953 -- 2.1156)  data: 0.0525 (0.0002 -- 0.7675)  max mem: 16413
Epoch: [96] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.9537 (1.8458)  loss_scale: 16384.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5409 (8.3970)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.5067 (0.5067)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5568 (2.5568 -- 2.5568)  data: 2.2972 (2.2972 -- 2.2972)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5797 (0.8447)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4427 (0.2003 -- 2.5568)  data: 0.2156 (0.0009 -- 2.2972)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5840 (0.7831)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2137 (0.1696 -- 0.2940)  data: 0.0046 (0.0001 -- 0.0622)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6921 (0.8349)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.0207)  time: 0.1937 (0.1328 -- 0.2940)  data: 0.0042 (0.0001 -- 0.0622)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 78.008 Acc@5 96.058 loss 0.788
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 78.63%
Epoch: [97]  [  0/160]  eta: 0:20:12  lr: 0.000013  min_lr: 0.000000  loss: 2.1030 (2.1030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.1619 (15.1619)  time: 7.5795 (7.5795 -- 7.5795)  data: 7.0593 (7.0593 -- 7.0593)  max mem: 16413
Epoch: [97]  [ 20/160]  eta: 0:02:43  lr: 0.000013  min_lr: 0.000000  loss: 1.8101 (1.8382)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8685 (8.5624)  time: 0.8468 (0.5308 -- 5.3176)  data: 0.2935 (0.0002 -- 4.7558)  max mem: 16413
[2023-08-31 01:35:25,157] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:35:25,157] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:35:25,157] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:35:25,157] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:35:26,904] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15552
[2023-08-31 01:35:26,904] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15552
[2023-08-31 01:35:26,905] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:35:26,905] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:35:26,905] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [97]  [ 40/160]  eta: 0:01:58  lr: 0.000013  min_lr: 0.000000  loss: 1.9659 (1.8903)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6522 (8.1052)  time: 0.7949 (0.5201 -- 2.1007)  data: 0.0621 (0.0008 -- 0.6874)  max mem: 16413
Epoch: [97]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000000  loss: 1.8443 (1.8772)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8297 (8.3392)  time: 0.9477 (0.5272 -- 3.5711)  data: 0.1202 (0.0003 -- 1.3649)  max mem: 16413
Epoch: [97]  [ 80/160]  eta: 0:01:16  lr: 0.000013  min_lr: 0.000000  loss: 1.9304 (1.8708)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3208 (8.2963)  time: 0.8836 (0.5260 -- 5.0653)  data: 0.2201 (0.0004 -- 2.3482)  max mem: 16413
Epoch: [97]  [100/160]  eta: 0:00:54  lr: 0.000013  min_lr: 0.000000  loss: 1.8254 (1.8547)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9659 (8.2776)  time: 0.7308 (0.5334 -- 2.9257)  data: 0.1764 (0.0004 -- 2.3748)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.9446 (1.8617)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2319 (8.3436)  time: 0.9263 (0.5249 -- 5.4427)  data: 0.2139 (0.0002 -- 3.1121)  max mem: 16413
Epoch: [97]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8355 (1.8722)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4546 (8.3524)  time: 0.9201 (0.5226 -- 5.2135)  data: 0.0015 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8867 (1.8688)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9925 (8.4194)  time: 0.6672 (0.4961 -- 2.7032)  data: 0.0008 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [97] Total time: 0:02:21 (0.8841 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8867 (1.8598)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9925 (8.4194)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.4879 (0.4879)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5477 (2.5477 -- 2.5477)  data: 2.3012 (2.3012 -- 2.3012)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5719 (0.8484)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4386 (0.2098 -- 2.5477)  data: 0.2170 (0.0008 -- 2.3012)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6047 (0.7842)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.2963)  time: 0.2188 (0.1688 -- 0.3471)  data: 0.0124 (0.0001 -- 0.1596)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6955 (0.8374)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.0207)  time: 0.2026 (0.1329 -- 0.3471)  data: 0.0121 (0.0001 -- 0.1596)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 77.593 Acc@5 95.851 loss 0.789
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 78.63%
Epoch: [98]  [  0/160]  eta: 0:20:09  lr: 0.000013  min_lr: 0.000000  loss: 1.3197 (1.3197)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5515 (11.5515)  time: 7.5564 (7.5564 -- 7.5564)  data: 6.4486 (6.4486 -- 6.4486)  max mem: 16413
[2023-08-31 01:37:30,258] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:37:30,258] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:37:30,258] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:37:30,258] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [98]  [ 20/160]  eta: 0:02:40  lr: 0.000013  min_lr: 0.000000  loss: 1.9016 (1.8554)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4030 (7.9058)  time: 0.8296 (0.5183 -- 3.8279)  data: 0.2792 (0.0003 -- 3.2882)  max mem: 16413
[2023-08-31 01:37:54,758] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15709
[2023-08-31 01:37:54,758] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15709
[2023-08-31 01:37:54,758] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:37:54,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:37:54,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [98]  [ 40/160]  eta: 0:02:06  lr: 0.000013  min_lr: 0.000000  loss: 1.8952 (1.9012)  loss_scale: 16384.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7050 (8.1587)  time: 0.9571 (0.5091 -- 5.3532)  data: 0.2463 (0.0003 -- 2.6518)  max mem: 16413
Epoch: [98]  [ 60/160]  eta: 0:01:38  lr: 0.000013  min_lr: 0.000000  loss: 1.9178 (1.8709)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6851 (8.3338)  time: 0.8272 (0.5148 -- 4.5663)  data: 0.2808 (0.0002 -- 4.0524)  max mem: 16413
Epoch: [98]  [ 80/160]  eta: 0:01:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8192 (1.8750)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9490 (8.3087)  time: 0.9744 (0.5240 -- 4.2859)  data: 0.4277 (0.0004 -- 3.7601)  max mem: 16413
[2023-08-31 01:38:43,128] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15763
[2023-08-31 01:38:43,128] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15763
[2023-08-31 01:38:43,129] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:38:43,129] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:38:43,129] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [98]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.7097 (1.8459)  loss_scale: 8192.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1043 (8.2942)  time: 0.8148 (0.5031 -- 3.5045)  data: 0.2728 (0.0003 -- 3.0074)  max mem: 16413
Epoch: [98]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.9233 (1.8457)  loss_scale: 8192.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5472 (8.1812)  time: 0.8924 (0.5414 -- 3.2419)  data: 0.3337 (0.0004 -- 2.7087)  max mem: 16413
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8877 (1.8560)  loss_scale: 8192.0000 (16267.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8850 (8.2020)  time: 0.7382 (0.5331 -- 2.4208)  data: 0.1901 (0.0007 -- 1.8461)  max mem: 16413
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.9537 (1.8643)  loss_scale: 8192.0000 (15308.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3534 (8.3504)  time: 0.7249 (0.4960 -- 2.0610)  data: 0.2026 (0.0002 -- 1.5332)  max mem: 16413
Epoch: [98] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.9537 (1.8624)  loss_scale: 8192.0000 (15308.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3534 (8.3504)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4498 (0.4498)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3474 (2.3474 -- 2.3474)  data: 2.1551 (2.1551 -- 2.1551)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5882 (0.8459)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (96.9697)  time: 0.4288 (0.2035 -- 2.3474)  data: 0.2169 (0.0007 -- 2.1551)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5882 (0.7814)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.8254)  time: 0.2232 (0.1688 -- 0.4342)  data: 0.0202 (0.0001 -- 0.2045)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6975 (0.8379)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.4357)  time: 0.2074 (0.1331 -- 0.4342)  data: 0.0191 (0.0001 -- 0.2045)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 78.216 Acc@5 96.266 loss 0.786
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 78.63%
Epoch: [99]  [  0/160]  eta: 0:17:13  lr: 0.000012  min_lr: 0.000000  loss: 2.2418 (2.2418)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0458 (6.0458)  time: 6.4597 (6.4597 -- 6.4597)  data: 5.3096 (5.3096 -- 5.3096)  max mem: 16413
Epoch: [99]  [ 20/160]  eta: 0:02:38  lr: 0.000012  min_lr: 0.000000  loss: 1.8698 (1.8206)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0032 (10.2173)  time: 0.8692 (0.5305 -- 2.9551)  data: 0.0875 (0.0004 -- 1.3016)  max mem: 16413
Epoch: [99]  [ 40/160]  eta: 0:02:08  lr: 0.000012  min_lr: 0.000000  loss: 1.9311 (1.8580)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1514 (9.3084)  time: 1.0040 (0.5376 -- 3.8207)  data: 0.1173 (0.0004 -- 2.0233)  max mem: 16413
[2023-08-31 01:40:44,642] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:40:44,642] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:40:44,642] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:40:44,642] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [99]  [ 60/160]  eta: 0:01:36  lr: 0.000012  min_lr: 0.000000  loss: 1.9597 (1.8885)  loss_scale: 8192.0000 (9400.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0953 (8.9503)  time: 0.7528 (0.5300 -- 1.6952)  data: 0.0652 (0.0004 -- 0.7091)  max mem: 16413
Epoch: [99]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 1.8277 (1.8454)  loss_scale: 16384.0000 (11124.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7664 (8.9319)  time: 0.8834 (0.5110 -- 3.6882)  data: 0.2869 (0.0004 -- 3.1678)  max mem: 16413
Epoch: [99]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.9212 (1.8395)  loss_scale: 16384.0000 (12166.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8526 (8.9212)  time: 0.8893 (0.5242 -- 3.6305)  data: 0.3417 (0.0004 -- 3.0762)  max mem: 16413
Epoch: [99]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.7630 (1.8283)  loss_scale: 16384.0000 (12863.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1144 (8.7589)  time: 0.7988 (0.5296 -- 4.1261)  data: 0.2470 (0.0002 -- 3.5962)  max mem: 16413
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8465 (1.8181)  loss_scale: 16384.0000 (13362.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1267 (8.5934)  time: 0.9884 (0.5195 -- 4.7067)  data: 0.0969 (0.0003 -- 1.9053)  max mem: 16413
[2023-08-31 01:42:13,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=92, lr=[2.9017803916243546e-07, 2.9017803916243546e-07, 3.8690405221658065e-07, 3.8690405221658065e-07, 5.158720696221075e-07, 5.158720696221075e-07, 6.8782942616281e-07, 6.8782942616281e-07, 9.171059015504133e-07, 9.171059015504133e-07, 1.2228078687338845e-06, 1.2228078687338845e-06, 1.6304104916451792e-06, 1.6304104916451792e-06, 2.1738806555269056e-06, 2.1738806555269056e-06, 2.8985075407025407e-06, 2.8985075407025407e-06, 3.864676720936721e-06, 3.864676720936721e-06, 5.152902294582295e-06, 5.152902294582295e-06, 6.870536392776393e-06, 6.870536392776393e-06, 9.160715190368523e-06, 9.160715190368523e-06, 1.2214286920491366e-05, 1.2214286920491366e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 01:42:13,741] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=17.288777568479382, CurrSamplesPerSec=24.47351714694716, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.8505 (1.8122)  loss_scale: 16384.0000 (13721.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6678 (8.5359)  time: 0.5955 (0.4958 -- 1.9894)  data: 0.0009 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [99] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.8505 (1.8429)  loss_scale: 16384.0000 (13721.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6678 (8.5359)
[2023-08-31 01:42:13,744] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-08-31 01:42:13,746] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-08-31 01:42:13,746] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-08-31 01:42:13,746] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-08-31 01:42:14,744] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-08-31 01:42:14,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4468 (0.4468)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3494 (2.3494 -- 2.3494)  data: 2.1086 (2.1086 -- 2.1086)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5418 (0.8294)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4508 (0.1910 -- 2.3494)  data: 0.2391 (0.0005 -- 2.1086)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5874 (0.7665)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2302 (0.1696 -- 0.7439)  data: 0.0263 (0.0001 -- 0.5142)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6875 (0.8236)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.0207)  time: 0.2191 (0.1354 -- 0.7439)  data: 0.0261 (0.0001 -- 0.5142)  max mem: 16413
Val: Total time: 0:00:07 (0.2942 s / it)
* Acc@1 77.801 Acc@5 96.266 loss 0.782
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 78.63%
Epoch: [100]  [  0/160]  eta: 0:18:14  lr: 0.000012  min_lr: 0.000000  loss: 2.0731 (2.0731)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3812 (7.3812)  time: 6.8388 (6.8388 -- 6.8388)  data: 5.3929 (5.3929 -- 5.3929)  max mem: 16413
[2023-08-31 01:42:46,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:42:46,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:42:46,735] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:42:46,735] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [100]  [ 20/160]  eta: 0:02:40  lr: 0.000012  min_lr: 0.000000  loss: 1.7311 (1.8469)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2832 (8.2359)  time: 0.8607 (0.5316 -- 3.3882)  data: 0.0269 (0.0006 -- 0.3361)  max mem: 16413
Epoch: [100]  [ 40/160]  eta: 0:02:10  lr: 0.000012  min_lr: 0.000000  loss: 1.6873 (1.8012)  loss_scale: 32768.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0364 (8.1713)  time: 1.0325 (0.5127 -- 4.5861)  data: 0.0014 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [100]  [ 60/160]  eta: 0:01:39  lr: 0.000012  min_lr: 0.000000  loss: 1.8968 (1.8531)  loss_scale: 32768.0000 (27396.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9815 (8.2038)  time: 0.7882 (0.5261 -- 3.0192)  data: 0.0019 (0.0002 -- 0.0160)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:19  lr: 0.000012  min_lr: 0.000000  loss: 1.9344 (1.8565)  loss_scale: 32768.0000 (28722.5679)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4824 (8.4931)  time: 0.9900 (0.5219 -- 3.3361)  data: 0.0632 (0.0004 -- 1.2305)  max mem: 16413
[2023-08-31 01:43:46,199] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16086
[2023-08-31 01:43:46,199] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16086
[2023-08-31 01:43:46,199] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:43:46,199] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:43:46,199] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.9339 (1.8661)  loss_scale: 16384.0000 (27090.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4990 (8.5128)  time: 0.7227 (0.5187 -- 3.1288)  data: 0.0370 (0.0003 -- 0.7138)  max mem: 16413
[2023-08-31 01:44:00,806] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16105
[2023-08-31 01:44:00,806] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16105
[2023-08-31 01:44:00,806] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:44:00,807] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 01:44:00,806] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [100]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.8419 (1.8702)  loss_scale: 8192.0000 (24237.4876)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4545 (8.5913)  time: 0.9084 (0.5353 -- 3.4620)  data: 0.1169 (0.0003 -- 1.1598)  max mem: 16413
Epoch: [100]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.9140 (1.8673)  loss_scale: 8192.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2575 (8.4899)  time: 0.8906 (0.5299 -- 3.2963)  data: 0.0854 (0.0003 -- 0.9387)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 2.0139 (1.8782)  loss_scale: 8192.0000 (20326.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8633 (8.4984)  time: 0.6693 (0.4970 -- 2.0587)  data: 0.1250 (0.0001 -- 1.5504)  max mem: 16413
Epoch: [100] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 2.0139 (1.8788)  loss_scale: 8192.0000 (20326.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8633 (8.4984)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4664 (0.4664)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2672 (2.2672 -- 2.2672)  data: 2.0285 (2.0285 -- 2.0285)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5516 (0.8261)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4119 (0.2000 -- 2.2672)  data: 0.1985 (0.0004 -- 2.0285)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5763 (0.7626)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2333 (0.1714 -- 0.6492)  data: 0.0303 (0.0001 -- 0.4486)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6953 (0.8153)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.4357)  time: 0.2180 (0.1365 -- 0.6492)  data: 0.0301 (0.0001 -- 0.4486)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 78.838 Acc@5 96.473 loss 0.775
Accuracy of the network on the 482 val images: 78.84%
[2023-08-31 01:44:53,289] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 01:44:53,291] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 01:44:53,291] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 01:44:53,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 01:44:54,795] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 01:44:54,795] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.84%
Epoch: [101]  [  0/160]  eta: 0:19:28  lr: 0.000012  min_lr: 0.000000  loss: 1.6115 (1.6115)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5748 (10.5748)  time: 7.3009 (7.3009 -- 7.3009)  data: 6.7599 (6.7599 -- 6.7599)  max mem: 16413
Epoch: [101]  [ 20/160]  eta: 0:02:43  lr: 0.000012  min_lr: 0.000000  loss: 1.9159 (1.8612)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1755 (8.9419)  time: 0.8609 (0.5259 -- 3.9906)  data: 0.0634 (0.0005 -- 1.1513)  max mem: 16413
Epoch: [101]  [ 40/160]  eta: 0:02:00  lr: 0.000012  min_lr: 0.000000  loss: 1.8604 (1.8253)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9091 (8.6970)  time: 0.8410 (0.5166 -- 3.1042)  data: 0.0753 (0.0002 -- 1.2688)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:39  lr: 0.000012  min_lr: 0.000000  loss: 1.8070 (1.8163)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8182 (8.3824)  time: 0.9551 (0.5122 -- 4.0521)  data: 0.0013 (0.0005 -- 0.0025)  max mem: 16413
[2023-08-31 01:46:09,571] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:46:09,571] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:46:09,574] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:46:09,612] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [101]  [ 80/160]  eta: 0:01:17  lr: 0.000012  min_lr: 0.000000  loss: 1.8150 (1.8119)  loss_scale: 8192.0000 (8899.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4026 (8.2129)  time: 0.8815 (0.5124 -- 5.2184)  data: 0.0013 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [101]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.8378 (1.8322)  loss_scale: 16384.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3797 (8.2837)  time: 0.8605 (0.5393 -- 3.0359)  data: 0.0025 (0.0006 -- 0.0168)  max mem: 16413
[2023-08-31 01:46:36,575] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16267
[2023-08-31 01:46:36,575] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16267
[2023-08-31 01:46:36,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:46:36,576] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:46:36,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [101]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.7216 (1.8221)  loss_scale: 8192.0000 (10426.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5446 (8.3373)  time: 0.8650 (0.5223 -- 4.1988)  data: 0.0024 (0.0003 -- 0.0122)  max mem: 16413
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8356 (1.8258)  loss_scale: 8192.0000 (10109.2766)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1793 (8.3823)  time: 0.9025 (0.5192 -- 3.8145)  data: 0.0012 (0.0005 -- 0.0045)  max mem: 16413
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.8186 (1.8231)  loss_scale: 8192.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1504 (8.3222)  time: 0.5891 (0.4983 -- 1.6959)  data: 0.0005 (0.0002 -- 0.0011)  max mem: 16413
Epoch: [101] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.8186 (1.8604)  loss_scale: 8192.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1504 (8.3222)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4063 (0.4063)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4661 (2.4661 -- 2.4661)  data: 2.2403 (2.2403 -- 2.2403)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5788 (0.8328)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4459 (0.2027 -- 2.4661)  data: 0.2341 (0.0007 -- 2.2403)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5851 (0.7666)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2228 (0.1688 -- 0.5233)  data: 0.0230 (0.0001 -- 0.3254)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6753 (0.8261)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (95.0207)  time: 0.2074 (0.1385 -- 0.5233)  data: 0.0227 (0.0001 -- 0.3254)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 76.556 Acc@5 96.058 loss 0.780
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 78.84%
Epoch: [102]  [  0/160]  eta: 0:22:50  lr: 0.000012  min_lr: 0.000000  loss: 1.8800 (1.8800)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4720 (8.4720)  time: 8.5655 (8.5655 -- 8.5655)  data: 8.0351 (8.0351 -- 8.0351)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:42  lr: 0.000012  min_lr: 0.000000  loss: 1.6656 (1.8106)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3677 (8.0918)  time: 0.7941 (0.5292 -- 4.1755)  data: 0.2280 (0.0003 -- 3.6457)  max mem: 16413
Epoch: [102]  [ 40/160]  eta: 0:02:00  lr: 0.000012  min_lr: 0.000000  loss: 1.8550 (1.8448)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2611 (8.2146)  time: 0.8346 (0.5220 -- 3.1386)  data: 0.2840 (0.0008 -- 2.5833)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 1.7909 (1.8318)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3341 (8.3261)  time: 0.9247 (0.5177 -- 3.5339)  data: 0.2957 (0.0002 -- 3.0094)  max mem: 16413
[2023-08-31 01:48:37,868] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:48:37,869] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:48:37,873] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:48:37,874] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [102]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 1.8992 (1.8514)  loss_scale: 8192.0000 (8697.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8568 (8.4651)  time: 0.9150 (0.5346 -- 3.8846)  data: 0.3551 (0.0010 -- 3.3231)  max mem: 16413
Epoch: [102]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.7886 (1.8533)  loss_scale: 16384.0000 (10219.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3403 (8.4003)  time: 0.8330 (0.5217 -- 3.1973)  data: 0.2823 (0.0005 -- 2.6569)  max mem: 16413
Epoch: [102]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.9587 (1.8712)  loss_scale: 16384.0000 (11238.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6659 (8.3391)  time: 0.8572 (0.5234 -- 4.2060)  data: 0.0315 (0.0002 -- 0.6046)  max mem: 16413
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8802 (1.8642)  loss_scale: 16384.0000 (11968.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3084 (8.2350)  time: 0.8317 (0.5339 -- 3.1819)  data: 0.0119 (0.0004 -- 0.1943)  max mem: 16413
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.8524 (1.8609)  loss_scale: 16384.0000 (12492.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8384 (8.1954)  time: 0.7362 (0.4931 -- 4.3048)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [102] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.8524 (1.8470)  loss_scale: 16384.0000 (12492.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8384 (8.1954)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.4406 (0.4406)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5294 (2.5294 -- 2.5294)  data: 2.2146 (2.2146 -- 2.2146)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5861 (0.8229)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4245 (0.2008 -- 2.5294)  data: 0.2031 (0.0009 -- 2.2146)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5861 (0.7646)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2180 (0.1701 -- 0.4731)  data: 0.0158 (0.0001 -- 0.2933)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6909 (0.8168)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.0207)  time: 0.2028 (0.1324 -- 0.4731)  data: 0.0151 (0.0001 -- 0.2933)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 78.008 Acc@5 96.058 loss 0.774
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 78.84%
Epoch: [103]  [  0/160]  eta: 0:18:58  lr: 0.000012  min_lr: 0.000000  loss: 1.8185 (1.8185)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1953 (11.1953)  time: 7.1133 (7.1133 -- 7.1133)  data: 6.0034 (6.0034 -- 6.0034)  max mem: 16413
Epoch: [103]  [ 20/160]  eta: 0:02:42  lr: 0.000012  min_lr: 0.000000  loss: 1.8992 (1.8759)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7454 (8.3837)  time: 0.8603 (0.5321 -- 3.4215)  data: 0.2169 (0.0005 -- 2.8780)  max mem: 16413
Epoch: [103]  [ 40/160]  eta: 0:02:01  lr: 0.000012  min_lr: 0.000000  loss: 1.9974 (1.8966)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8111 (8.1280)  time: 0.8531 (0.5315 -- 3.3699)  data: 0.2308 (0.0004 -- 2.8380)  max mem: 16413
[2023-08-31 01:50:41,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:50:41,077] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:50:41,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:50:41,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [103]  [ 60/160]  eta: 0:01:41  lr: 0.000012  min_lr: 0.000000  loss: 1.8733 (1.8914)  loss_scale: 32768.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3077 (8.1349)  time: 1.0148 (0.5288 -- 3.5485)  data: 0.0524 (0.0005 -- 0.8311)  max mem: 16413
Epoch: [103]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 1.7095 (1.8587)  loss_scale: 32768.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0498 (8.1436)  time: 0.8099 (0.5183 -- 3.0495)  data: 0.0015 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.9582 (1.8740)  loss_scale: 32768.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2847 (8.1675)  time: 0.8980 (0.5350 -- 3.5186)  data: 0.0015 (0.0003 -- 0.0049)  max mem: 16413
[2023-08-31 01:51:37,238] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16588
[2023-08-31 01:51:37,238] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:51:37,238] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16588
[2023-08-31 01:51:37,238] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:51:37,238] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [103]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.9124 (1.8693)  loss_scale: 16384.0000 (25049.9174)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5876 (8.2391)  time: 0.7601 (0.5314 -- 1.8169)  data: 0.0023 (0.0005 -- 0.0158)  max mem: 16413
Epoch: [103]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8870 (1.8656)  loss_scale: 16384.0000 (23820.7092)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7462 (8.2826)  time: 0.9080 (0.5442 -- 2.3864)  data: 0.0670 (0.0004 -- 1.3011)  max mem: 16413
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7918 (1.8625)  loss_scale: 16384.0000 (22937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5368 (8.2494)  time: 0.7166 (0.4957 -- 2.6146)  data: 0.0327 (0.0002 -- 0.6409)  max mem: 16413
Epoch: [103] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7918 (1.8316)  loss_scale: 16384.0000 (22937.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5368 (8.2494)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.4208 (0.4208)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5612 (2.5612 -- 2.5612)  data: 2.2968 (2.2968 -- 2.2968)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5652 (0.8283)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4594 (0.2048 -- 2.5612)  data: 0.2390 (0.0005 -- 2.2968)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5721 (0.7629)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2200 (0.1694 -- 0.5587)  data: 0.0168 (0.0001 -- 0.3209)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6874 (0.8206)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (95.0207)  time: 0.2055 (0.1328 -- 0.5587)  data: 0.0165 (0.0001 -- 0.3209)  max mem: 16413
Val: Total time: 0:00:07 (0.2946 s / it)
* Acc@1 76.763 Acc@5 96.058 loss 0.774
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 78.84%
Epoch: [104]  [  0/160]  eta: 0:22:14  lr: 0.000011  min_lr: 0.000000  loss: 2.0800 (2.0800)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8278 (11.8278)  time: 8.3413 (8.3413 -- 8.3413)  data: 6.3201 (6.3201 -- 6.3201)  max mem: 16413
[2023-08-31 01:52:36,076] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16645
[2023-08-31 01:52:36,076] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16645
[2023-08-31 01:52:36,076] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:52:36,076] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:52:36,076] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [104]  [ 20/160]  eta: 0:02:57  lr: 0.000011  min_lr: 0.000000  loss: 1.7448 (1.8353)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2081 (8.8985)  time: 0.9151 (0.5252 -- 5.0112)  data: 0.0033 (0.0006 -- 0.0305)  max mem: 16413
Epoch: [104]  [ 40/160]  eta: 0:02:15  lr: 0.000011  min_lr: 0.000000  loss: 1.7324 (1.8153)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0847 (8.7230)  time: 0.9799 (0.5282 -- 3.8679)  data: 0.0018 (0.0004 -- 0.0130)  max mem: 16413
Epoch: [104]  [ 60/160]  eta: 0:01:41  lr: 0.000011  min_lr: 0.000000  loss: 1.8831 (1.8348)  loss_scale: 8192.0000 (8863.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2708 (8.7199)  time: 0.7787 (0.5288 -- 3.8789)  data: 0.0016 (0.0004 -- 0.0060)  max mem: 16413
Epoch: [104]  [ 80/160]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000000  loss: 1.8551 (1.8161)  loss_scale: 8192.0000 (8697.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6684 (8.3873)  time: 0.8161 (0.5294 -- 2.7443)  data: 0.0015 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.7549 (1.8096)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2087 (8.3303)  time: 0.8720 (0.5217 -- 3.8570)  data: 0.0014 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [104]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 2.1251 (1.8443)  loss_scale: 8192.0000 (8530.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4159 (8.3020)  time: 0.7796 (0.5303 -- 2.3214)  data: 0.0016 (0.0004 -- 0.0034)  max mem: 16413
[2023-08-31 01:54:27,333] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:54:27,333] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:54:27,375] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:54:27,375] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [104]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8084 (1.8470)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1139 (8.2404)  time: 0.9011 (0.5350 -- 4.0640)  data: 0.0118 (0.0005 -- 0.1969)  max mem: 16413
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.8478 (1.8417)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3952 (8.4847)  time: 0.6923 (0.4937 -- 3.4993)  data: 0.0007 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [104] Total time: 0:02:22 (0.8907 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.8478 (1.8405)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3952 (8.4847)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.4232 (0.4232)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6807 (2.6807 -- 2.6807)  data: 2.4607 (2.4607 -- 2.4607)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5729 (0.8179)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4398 (0.2037 -- 2.6807)  data: 0.2252 (0.0006 -- 2.4607)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5789 (0.7467)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2138 (0.1690 -- 0.3955)  data: 0.0117 (0.0001 -- 0.2142)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6744 (0.8081)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.4357)  time: 0.1978 (0.1330 -- 0.3955)  data: 0.0114 (0.0001 -- 0.2142)  max mem: 16413
Val: Total time: 0:00:07 (0.2941 s / it)
* Acc@1 79.253 Acc@5 96.058 loss 0.761
Accuracy of the network on the 482 val images: 79.25%
[2023-08-31 01:54:55,460] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 01:54:55,462] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 01:54:55,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 01:54:55,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 01:54:56,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 01:54:56,881] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.25%
Epoch: [105]  [  0/160]  eta: 0:22:56  lr: 0.000011  min_lr: 0.000000  loss: 1.6721 (1.6721)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7288 (11.7288)  time: 8.6037 (8.6037 -- 8.6037)  data: 8.0799 (8.0799 -- 8.0799)  max mem: 16413
Epoch: [105]  [ 20/160]  eta: 0:02:46  lr: 0.000011  min_lr: 0.000000  loss: 1.7226 (1.6822)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8544 (8.4722)  time: 0.8212 (0.5007 -- 3.6954)  data: 0.2777 (0.0002 -- 3.1604)  max mem: 16413
[2023-08-31 01:55:27,978] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16824
[2023-08-31 01:55:27,978] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16824
[2023-08-31 01:55:27,979] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:55:27,979] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:55:28,020] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [105]  [ 40/160]  eta: 0:02:09  lr: 0.000011  min_lr: 0.000000  loss: 1.9218 (1.8294)  loss_scale: 8192.0000 (12987.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9769 (8.6189)  time: 0.9531 (0.5072 -- 4.4622)  data: 0.0764 (0.0004 -- 1.5067)  max mem: 16413
Epoch: [105]  [ 60/160]  eta: 0:01:38  lr: 0.000011  min_lr: 0.000000  loss: 1.8453 (1.8274)  loss_scale: 8192.0000 (11415.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1222 (8.5015)  time: 0.7860 (0.5381 -- 3.5568)  data: 0.0018 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [105]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000000  loss: 2.1021 (1.8663)  loss_scale: 8192.0000 (10619.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7630 (8.5619)  time: 0.8689 (0.5259 -- 3.5164)  data: 0.0014 (0.0001 -- 0.0036)  max mem: 16413
Epoch: [105]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.8022 (1.8478)  loss_scale: 8192.0000 (10138.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3841 (8.6151)  time: 0.8906 (0.5228 -- 3.7106)  data: 0.0012 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [105]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.6632 (1.8414)  loss_scale: 8192.0000 (9816.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0522 (8.5317)  time: 0.7878 (0.5227 -- 3.1562)  data: 0.0024 (0.0004 -- 0.0143)  max mem: 16413
Epoch: [105]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.7865 (1.8354)  loss_scale: 8192.0000 (9586.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3734 (8.5515)  time: 0.9083 (0.5341 -- 2.4254)  data: 0.0571 (0.0001 -- 0.7069)  max mem: 16413
[2023-08-31 01:57:15,531] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:57:15,531] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 01:57:15,532] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:57:15,532] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.9683 (1.8356)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8613 (8.5078)  time: 0.6626 (0.4951 -- 3.4441)  data: 0.0006 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [105] Total time: 0:02:21 (0.8854 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.9683 (1.8361)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8613 (8.5078)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4035 (0.4035)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4717 (2.4717 -- 2.4717)  data: 2.2533 (2.2533 -- 2.2533)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5316 (0.8295)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4421 (0.2049 -- 2.4717)  data: 0.2277 (0.0008 -- 2.2533)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5573 (0.7538)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2231 (0.1693 -- 0.4428)  data: 0.0187 (0.0001 -- 0.2387)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6755 (0.8142)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.0207)  time: 0.2076 (0.1318 -- 0.4428)  data: 0.0182 (0.0001 -- 0.2387)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 79.046 Acc@5 95.851 loss 0.767
Accuracy of the network on the 482 val images: 79.05%
Max accuracy: 79.25%
Epoch: [106]  [  0/160]  eta: 0:15:55  lr: 0.000011  min_lr: 0.000000  loss: 1.2925 (1.2925)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0525 (11.0525)  time: 5.9708 (5.9708 -- 5.9708)  data: 5.4325 (5.4325 -- 5.4325)  max mem: 16413
Epoch: [106]  [ 20/160]  eta: 0:02:39  lr: 0.000011  min_lr: 0.000000  loss: 1.8416 (1.7975)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8432 (8.4451)  time: 0.8994 (0.5403 -- 3.6842)  data: 0.0224 (0.0004 -- 0.3052)  max mem: 16413
[2023-08-31 01:58:06,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=98, lr=[2.622140921840345e-07, 2.622140921840345e-07, 3.4961878957871263e-07, 3.4961878957871263e-07, 4.661583861049502e-07, 4.661583861049502e-07, 6.215445148066003e-07, 6.215445148066003e-07, 8.287260197421337e-07, 8.287260197421337e-07, 1.104968026322845e-06, 1.104968026322845e-06, 1.4732907017637933e-06, 1.4732907017637933e-06, 1.9643876023517244e-06, 1.9643876023517244e-06, 2.619183469802299e-06, 2.619183469802299e-06, 3.4922446264030654e-06, 3.4922446264030654e-06, 4.656326168537421e-06, 4.656326168537421e-06, 6.2084348913832275e-06, 6.2084348913832275e-06, 8.27791318851097e-06, 8.27791318851097e-06, 1.1037217584681293e-05, 1.1037217584681293e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 01:58:06,530] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=17.176481769983347, CurrSamplesPerSec=22.754053597084248, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:01:58  lr: 0.000011  min_lr: 0.000000  loss: 1.9991 (1.8873)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2897 (8.2817)  time: 0.8341 (0.5275 -- 4.9008)  data: 0.0336 (0.0003 -- 0.6211)  max mem: 16413
Epoch: [106]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000000  loss: 1.8519 (1.8687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4646 (8.3027)  time: 0.9544 (0.5369 -- 3.9714)  data: 0.0043 (0.0003 -- 0.0377)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:14  lr: 0.000011  min_lr: 0.000000  loss: 1.6959 (1.8372)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8731 (8.3732)  time: 0.8090 (0.5250 -- 2.4582)  data: 0.0879 (0.0001 -- 1.7307)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 1.8634 (1.8170)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0617 (8.5200)  time: 0.9000 (0.5262 -- 3.4100)  data: 0.0167 (0.0002 -- 0.2732)  max mem: 16413
Epoch: [106]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.7100 (1.8017)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3921 (8.4230)  time: 0.8338 (0.5062 -- 2.8868)  data: 0.1791 (0.0002 -- 2.3382)  max mem: 16413
[2023-08-31 01:59:19,370] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:59:19,370] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:59:19,372] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 01:59:19,372] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 01:59:22,749] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17084
[2023-08-31 01:59:22,749] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17084
[2023-08-31 01:59:22,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:59:22,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 01:59:22,749] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.9719 (1.8178)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3482 (8.4165)  time: 1.0789 (0.5150 -- 5.4699)  data: 0.4473 (0.0002 -- 4.9587)  max mem: 16413
[2023-08-31 01:59:39,671] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17102
[2023-08-31 01:59:39,671] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:59:39,671] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17102
[2023-08-31 01:59:39,671] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 01:59:39,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.9616 (1.8328)  loss_scale: 8192.0000 (15769.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6433 (8.3879)  time: 0.5530 (0.4941 -- 1.1794)  data: 0.0437 (0.0002 -- 0.6850)  max mem: 16413
Epoch: [106] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.9616 (1.8451)  loss_scale: 8192.0000 (15769.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6433 (8.3879)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3694 (0.3694)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2294 (2.2294 -- 2.2294)  data: 2.0192 (2.0192 -- 2.0192)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5641 (0.8360)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4232 (0.1941 -- 2.2294)  data: 0.2072 (0.0005 -- 2.0192)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5795 (0.7690)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.2963)  time: 0.2292 (0.1728 -- 0.4463)  data: 0.0236 (0.0001 -- 0.2445)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6825 (0.8299)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.0207)  time: 0.2132 (0.1325 -- 0.4463)  data: 0.0230 (0.0001 -- 0.2445)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 77.178 Acc@5 95.851 loss 0.779
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 79.25%
Epoch: [107]  [  0/160]  eta: 0:19:54  lr: 0.000011  min_lr: 0.000000  loss: 1.5740 (1.5740)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5125 (6.5125)  time: 7.4684 (7.4684 -- 7.4684)  data: 6.9419 (6.9419 -- 6.9419)  max mem: 16413
Epoch: [107]  [ 20/160]  eta: 0:02:41  lr: 0.000011  min_lr: 0.000000  loss: 1.8403 (1.8377)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2499 (8.2844)  time: 0.8388 (0.5295 -- 4.1183)  data: 0.2855 (0.0002 -- 3.5958)  max mem: 16413
Epoch: [107]  [ 40/160]  eta: 0:02:07  lr: 0.000011  min_lr: 0.000000  loss: 1.8475 (1.8866)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3094 (8.9041)  time: 0.9582 (0.5184 -- 4.4548)  data: 0.4114 (0.0005 -- 3.9337)  max mem: 16413
Epoch: [107]  [ 60/160]  eta: 0:01:36  lr: 0.000011  min_lr: 0.000000  loss: 1.8893 (1.8860)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2556 (9.0125)  time: 0.7679 (0.5249 -- 3.0106)  data: 0.2069 (0.0003 -- 2.4875)  max mem: 16413
Epoch: [107]  [ 80/160]  eta: 0:01:15  lr: 0.000011  min_lr: 0.000000  loss: 1.7068 (1.8573)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8985 (8.7958)  time: 0.8961 (0.5260 -- 2.3689)  data: 0.0661 (0.0002 -- 0.8099)  max mem: 16413
Epoch: [107]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 1.8242 (1.8770)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2683 (8.4989)  time: 0.8328 (0.5246 -- 2.2163)  data: 0.1395 (0.0002 -- 1.6914)  max mem: 16413
[2023-08-31 02:01:39,736] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:01:39,736] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:01:39,737] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:01:39,737] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [107]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.6791 (1.8534)  loss_scale: 8192.0000 (8869.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9332 (8.4273)  time: 0.8343 (0.5259 -- 3.4863)  data: 0.0023 (0.0002 -- 0.0131)  max mem: 16413
Epoch: [107]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.9150 (1.8633)  loss_scale: 16384.0000 (9934.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5733 (8.4515)  time: 0.8526 (0.5330 -- 3.3693)  data: 0.0017 (0.0005 -- 0.0048)  max mem: 16413
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7944 (1.8460)  loss_scale: 16384.0000 (10700.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4333 (8.5489)  time: 0.6999 (0.4986 -- 2.1740)  data: 0.0008 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [107] Total time: 0:02:20 (0.8784 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7944 (1.8332)  loss_scale: 16384.0000 (10700.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4333 (8.5489)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3910 (0.3910)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5540 (2.5540 -- 2.5540)  data: 2.3459 (2.3459 -- 2.3459)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5556 (0.8223)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4566 (0.2040 -- 2.5540)  data: 0.2421 (0.0007 -- 2.3459)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5795 (0.7551)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2186 (0.1709 -- 0.5429)  data: 0.0160 (0.0001 -- 0.3079)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6664 (0.8159)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.0207)  time: 0.2034 (0.1329 -- 0.5429)  data: 0.0157 (0.0001 -- 0.3079)  max mem: 16413
Val: Total time: 0:00:07 (0.2933 s / it)
* Acc@1 78.216 Acc@5 95.851 loss 0.767
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 79.25%
Epoch: [108]  [  0/160]  eta: 0:16:42  lr: 0.000011  min_lr: 0.000000  loss: 1.9190 (1.9190)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0361 (7.0361)  time: 6.2632 (6.2632 -- 6.2632)  data: 5.2345 (5.2345 -- 5.2345)  max mem: 16413
Epoch: [108]  [ 20/160]  eta: 0:02:42  lr: 0.000011  min_lr: 0.000000  loss: 1.9539 (1.8683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8689 (9.6580)  time: 0.9089 (0.5290 -- 3.4802)  data: 0.2181 (0.0002 -- 2.9398)  max mem: 16413
Epoch: [108]  [ 40/160]  eta: 0:02:06  lr: 0.000011  min_lr: 0.000000  loss: 1.9546 (1.9023)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5140 (9.3636)  time: 0.9424 (0.5246 -- 4.2867)  data: 0.1700 (0.0004 -- 3.1898)  max mem: 16413
[2023-08-31 02:03:23,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17339
[2023-08-31 02:03:23,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17339
[2023-08-31 02:03:23,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:03:23,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:03:23,700] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [108]  [ 60/160]  eta: 0:01:40  lr: 0.000011  min_lr: 0.000000  loss: 1.8962 (1.8977)  loss_scale: 16384.0000 (16115.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9220 (9.0993)  time: 0.8999 (0.5192 -- 3.7526)  data: 0.0023 (0.0003 -- 0.0157)  max mem: 16413
Epoch: [108]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000000  loss: 1.8313 (1.8883)  loss_scale: 8192.0000 (14159.0123)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4856 (8.9559)  time: 0.8106 (0.5267 -- 3.0699)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [108]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.6639 (1.8641)  loss_scale: 8192.0000 (12977.4257)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9464 (8.8414)  time: 0.8591 (0.5249 -- 2.3267)  data: 0.0795 (0.0005 -- 1.5296)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.8061 (1.8696)  loss_scale: 8192.0000 (12186.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4070 (8.8020)  time: 0.9550 (0.5089 -- 4.3384)  data: 0.4190 (0.0003 -- 3.8088)  max mem: 16413
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 2.0092 (1.8912)  loss_scale: 8192.0000 (11619.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5819 (8.8791)  time: 0.7622 (0.5095 -- 2.6946)  data: 0.2170 (0.0002 -- 2.1587)  max mem: 16413
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.8365 (1.8844)  loss_scale: 8192.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4851 (8.8144)  time: 0.6784 (0.4959 -- 2.8938)  data: 0.1501 (0.0002 -- 2.3684)  max mem: 16413
Epoch: [108] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.8365 (1.8561)  loss_scale: 8192.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4851 (8.8144)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3969 (0.3969)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4390 (2.4390 -- 2.4390)  data: 2.1989 (2.1989 -- 2.1989)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5659 (0.8175)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4475 (0.2004 -- 2.4390)  data: 0.2314 (0.0007 -- 2.1989)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5671 (0.7541)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.2963)  time: 0.2200 (0.1697 -- 0.5304)  data: 0.0175 (0.0001 -- 0.3245)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6709 (0.8100)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.0207)  time: 0.2026 (0.1330 -- 0.5304)  data: 0.0165 (0.0001 -- 0.3245)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 77.593 Acc@5 95.851 loss 0.764
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.25%
Epoch: [109]  [  0/160]  eta: 0:17:41  lr: 0.000011  min_lr: 0.000000  loss: 2.2718 (2.2718)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9089 (9.9089)  time: 6.6346 (6.6346 -- 6.6346)  data: 6.0597 (6.0597 -- 6.0597)  max mem: 16413
Epoch: [109]  [ 20/160]  eta: 0:02:48  lr: 0.000010  min_lr: 0.000000  loss: 1.7621 (1.8902)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8761 (9.4836)  time: 0.9349 (0.5454 -- 2.6950)  data: 0.0024 (0.0004 -- 0.0161)  max mem: 16413
[2023-08-31 02:05:26,934] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:05:26,934] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:05:26,935] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:05:26,935] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [109]  [ 40/160]  eta: 0:02:00  lr: 0.000010  min_lr: 0.000000  loss: 2.0463 (1.9148)  loss_scale: 16384.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2841 (8.5218)  time: 0.7970 (0.5290 -- 2.4195)  data: 0.1284 (0.0003 -- 1.5362)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:37  lr: 0.000010  min_lr: 0.000000  loss: 1.9150 (1.8994)  loss_scale: 16384.0000 (12623.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4885 (8.3536)  time: 0.9070 (0.5197 -- 3.0561)  data: 0.0770 (0.0004 -- 0.8226)  max mem: 16413
Epoch: [109]  [ 80/160]  eta: 0:01:15  lr: 0.000010  min_lr: 0.000000  loss: 1.9725 (1.9092)  loss_scale: 16384.0000 (13552.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6034 (8.5681)  time: 0.8408 (0.5226 -- 2.3369)  data: 0.2414 (0.0004 -- 1.7967)  max mem: 16413
Epoch: [109]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.9825 (1.8946)  loss_scale: 16384.0000 (14112.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7560 (8.6321)  time: 0.8632 (0.5246 -- 2.7377)  data: 0.1138 (0.0007 -- 1.2348)  max mem: 16413
Epoch: [109]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.8296 (1.8880)  loss_scale: 16384.0000 (14488.3306)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5128 (8.4614)  time: 0.9741 (0.5191 -- 4.3940)  data: 0.0077 (0.0005 -- 0.1220)  max mem: 16413
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.9189 (1.8948)  loss_scale: 16384.0000 (14757.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0866 (8.7127)  time: 0.7778 (0.5287 -- 2.7946)  data: 0.0023 (0.0005 -- 0.0109)  max mem: 16413
[2023-08-31 02:07:17,210] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:07:17,210] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:07:17,211] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:07:17,211] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8514 (1.8880)  loss_scale: 16384.0000 (15360.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4609 (8.7005)  time: 0.7642 (0.4944 -- 3.2160)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [109] Total time: 0:02:23 (0.8961 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.8514 (1.8433)  loss_scale: 16384.0000 (15360.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4609 (8.7005)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3817 (0.3817)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4606 (2.4606 -- 2.4606)  data: 2.2144 (2.2144 -- 2.2144)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5272 (0.8144)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4454 (0.2027 -- 2.4606)  data: 0.2270 (0.0008 -- 2.2144)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5496 (0.7470)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.2963)  time: 0.2223 (0.1699 -- 0.4770)  data: 0.0166 (0.0001 -- 0.2707)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6790 (0.8033)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.0207)  time: 0.2050 (0.1326 -- 0.4770)  data: 0.0162 (0.0001 -- 0.2707)  max mem: 16413
Val: Total time: 0:00:07 (0.2923 s / it)
* Acc@1 78.216 Acc@5 96.058 loss 0.757
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 79.25%
Epoch: [110]  [  0/160]  eta: 0:21:58  lr: 0.000010  min_lr: 0.000000  loss: 2.1764 (2.1764)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5880 (7.5880)  time: 8.2435 (8.2435 -- 8.2435)  data: 3.6667 (3.6667 -- 3.6667)  max mem: 16413
[2023-08-31 02:07:46,703] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17614
[2023-08-31 02:07:46,703] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17614
[2023-08-31 02:07:46,703] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:07:46,703] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:07:46,703] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [110]  [ 20/160]  eta: 0:02:52  lr: 0.000010  min_lr: 0.000000  loss: 1.8931 (1.9224)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4062 (8.3008)  time: 0.8798 (0.5073 -- 4.0111)  data: 0.0014 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [110]  [ 40/160]  eta: 0:02:10  lr: 0.000010  min_lr: 0.000000  loss: 1.8350 (1.9189)  loss_scale: 16384.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3718 (8.4142)  time: 0.9388 (0.5209 -- 3.7160)  data: 0.0018 (0.0004 -- 0.0128)  max mem: 16413
Epoch: [110]  [ 60/160]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000000  loss: 1.7888 (1.9002)  loss_scale: 16384.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8262 (8.0726)  time: 0.8229 (0.5213 -- 3.2614)  data: 0.0011 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [110]  [ 80/160]  eta: 0:01:19  lr: 0.000010  min_lr: 0.000000  loss: 1.8627 (1.8919)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0762 (8.2397)  time: 0.9589 (0.5192 -- 5.1925)  data: 0.0017 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [110]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 1.4435 (1.8555)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4318 (8.1743)  time: 0.7636 (0.5164 -- 2.8018)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [110]  [120/160]  eta: 0:00:38  lr: 0.000010  min_lr: 0.000000  loss: 1.7234 (1.8460)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9038 (8.2603)  time: 1.0440 (0.5098 -- 4.7991)  data: 0.0012 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.8355 (1.8412)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6644 (8.3205)  time: 0.7775 (0.5230 -- 3.3583)  data: 0.0011 (0.0002 -- 0.0028)  max mem: 16413
[2023-08-31 02:09:40,274] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:09:40,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:09:40,275] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:09:40,276] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 2.0362 (1.8508)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8146 (8.2397)  time: 0.6335 (0.4959 -- 2.8820)  data: 0.0008 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [110] Total time: 0:02:24 (0.9006 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 2.0362 (1.8394)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8146 (8.2397)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3713 (0.3713)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5735 (2.5735 -- 2.5735)  data: 2.3176 (2.3176 -- 2.3176)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5481 (0.8062)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4514 (0.1949 -- 2.5735)  data: 0.2396 (0.0006 -- 2.3176)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5643 (0.7507)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.2963)  time: 0.2160 (0.1696 -- 0.5253)  data: 0.0160 (0.0001 -- 0.3083)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6541 (0.8064)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.4357)  time: 0.2039 (0.1333 -- 0.5253)  data: 0.0157 (0.0001 -- 0.3083)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 77.593 Acc@5 96.058 loss 0.758
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.25%
Epoch: [111]  [  0/160]  eta: 0:19:39  lr: 0.000010  min_lr: 0.000000  loss: 1.6192 (1.6192)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8971 (7.8971)  time: 7.3702 (7.3702 -- 7.3702)  data: 6.8418 (6.8418 -- 6.8418)  max mem: 16413
[2023-08-31 02:10:16,119] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17770
[2023-08-31 02:10:16,119] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17770
[2023-08-31 02:10:16,119] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:10:16,119] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:10:16,119] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [111]  [ 20/160]  eta: 0:02:56  lr: 0.000010  min_lr: 0.000000  loss: 1.9054 (1.8320)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3191 (9.2432)  time: 0.9540 (0.5092 -- 4.9712)  data: 0.3982 (0.0004 -- 4.4590)  max mem: 16413
Epoch: [111]  [ 40/160]  eta: 0:02:05  lr: 0.000010  min_lr: 0.000000  loss: 1.6435 (1.7728)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8441 (8.7988)  time: 0.8226 (0.5209 -- 4.0291)  data: 0.2788 (0.0003 -- 3.4621)  max mem: 16413
Epoch: [111]  [ 60/160]  eta: 0:01:35  lr: 0.000010  min_lr: 0.000000  loss: 1.6908 (1.7685)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1292 (8.7090)  time: 0.7711 (0.5205 -- 3.4390)  data: 0.2225 (0.0002 -- 2.8924)  max mem: 16413
Epoch: [111]  [ 80/160]  eta: 0:01:13  lr: 0.000010  min_lr: 0.000000  loss: 1.8725 (1.7942)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6827 (8.4164)  time: 0.7795 (0.5233 -- 3.3851)  data: 0.2284 (0.0003 -- 2.8616)  max mem: 16413
Epoch: [111]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.8565 (1.8077)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0072 (8.3947)  time: 0.9701 (0.5157 -- 4.5601)  data: 0.0725 (0.0007 -- 0.6794)  max mem: 16413
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.8494 (1.8037)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6467 (8.4343)  time: 0.7948 (0.5304 -- 2.6993)  data: 0.0023 (0.0002 -- 0.0164)  max mem: 16413
[2023-08-31 02:11:48,995] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17882
[2023-08-31 02:11:48,995] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17882
[2023-08-31 02:11:48,995] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:11:48,995] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:11:48,996] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.8989 (1.8123)  loss_scale: 8192.0000 (16442.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4510 (8.4811)  time: 0.9320 (0.5136 -- 3.1604)  data: 0.0905 (0.0005 -- 1.1636)  max mem: 16413
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8429 (1.8119)  loss_scale: 8192.0000 (15462.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6513 (8.5257)  time: 0.7410 (0.4982 -- 3.4178)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [111] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.8429 (1.8366)  loss_scale: 8192.0000 (15462.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6513 (8.5257)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3784 (0.3784)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4365 (2.4365 -- 2.4365)  data: 2.2226 (2.2226 -- 2.2226)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5497 (0.8098)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4438 (0.1857 -- 2.4365)  data: 0.2302 (0.0007 -- 2.2226)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5497 (0.7385)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2252 (0.1695 -- 0.5474)  data: 0.0209 (0.0001 -- 0.2994)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6597 (0.7964)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.0207)  time: 0.2111 (0.1326 -- 0.5474)  data: 0.0206 (0.0001 -- 0.2994)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 78.423 Acc@5 95.851 loss 0.753
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 79.25%
Epoch: [112]  [  0/160]  eta: 0:22:25  lr: 0.000010  min_lr: 0.000000  loss: 1.6846 (1.6846)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3404 (6.3404)  time: 8.4073 (8.4073 -- 8.4073)  data: 7.8637 (7.8637 -- 7.8637)  max mem: 16413
Epoch: [112]  [ 20/160]  eta: 0:02:45  lr: 0.000010  min_lr: 0.000000  loss: 1.9313 (1.8181)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1793 (8.8713)  time: 0.8215 (0.5401 -- 2.5630)  data: 0.0893 (0.0005 -- 1.0107)  max mem: 16413
Epoch: [112]  [ 40/160]  eta: 0:02:01  lr: 0.000010  min_lr: 0.000000  loss: 1.7452 (1.8282)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8365 (8.6865)  time: 0.8371 (0.5171 -- 2.6599)  data: 0.2070 (0.0003 -- 2.1313)  max mem: 16413
Epoch: [112]  [ 60/160]  eta: 0:01:37  lr: 0.000010  min_lr: 0.000000  loss: 1.8892 (1.8406)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9803 (8.6073)  time: 0.8883 (0.5319 -- 2.9248)  data: 0.0941 (0.0004 -- 1.4224)  max mem: 16413
[2023-08-31 02:13:43,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=104, lr=[2.3441980955478927e-07, 2.3441980955478927e-07, 3.125597460730524e-07, 3.125597460730524e-07, 4.1674632809740317e-07, 4.1674632809740317e-07, 5.556617707965376e-07, 5.556617707965376e-07, 7.408823610620501e-07, 7.408823610620501e-07, 9.878431480827334e-07, 9.878431480827334e-07, 1.3171241974436446e-06, 1.3171241974436446e-06, 1.756165596591526e-06, 1.756165596591526e-06, 2.3415541287887014e-06, 2.3415541287887014e-06, 3.1220721717182684e-06, 3.1220721717182684e-06, 4.162762895624358e-06, 4.162762895624358e-06, 5.550350527499144e-06, 5.550350527499144e-06, 7.400467369998859e-06, 7.400467369998859e-06, 9.867289826665145e-06, 9.867289826665145e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 02:13:43,407] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=17.13206878069954, CurrSamplesPerSec=23.32783550451687, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [112]  [ 80/160]  eta: 0:01:15  lr: 0.000010  min_lr: 0.000000  loss: 1.8633 (1.8438)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8929 (8.6313)  time: 0.8719 (0.5189 -- 2.2169)  data: 0.0217 (0.0006 -- 0.3929)  max mem: 16413
[2023-08-31 02:13:54,716] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:13:54,716] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:13:54,719] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:13:54,720] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [112]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.9941 (1.8586)  loss_scale: 8192.0000 (9003.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1211 (8.5306)  time: 0.8599 (0.5287 -- 3.5437)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.6390 (1.8333)  loss_scale: 16384.0000 (10223.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9516 (8.5545)  time: 0.9257 (0.5289 -- 3.5624)  data: 0.0014 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [112]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.7891 (1.8313)  loss_scale: 16384.0000 (11096.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2306 (8.7186)  time: 0.8207 (0.5263 -- 3.7168)  data: 0.0018 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.9640 (1.8385)  loss_scale: 16384.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9483 (8.7136)  time: 0.6526 (0.4951 -- 3.1613)  data: 0.0013 (0.0002 -- 0.0088)  max mem: 16413
Epoch: [112] Total time: 0:02:21 (0.8839 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.9640 (1.8265)  loss_scale: 16384.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9483 (8.7136)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3782 (0.3782)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4285 (2.4285 -- 2.4285)  data: 2.2187 (2.2187 -- 2.2187)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5438 (0.8002)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4331 (0.2027 -- 2.4285)  data: 0.2183 (0.0007 -- 2.2187)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5587 (0.7355)  acc1: 88.8889 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2225 (0.1708 -- 0.3955)  data: 0.0170 (0.0001 -- 0.1643)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6418 (0.7907)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.4357)  time: 0.2066 (0.1333 -- 0.3955)  data: 0.0162 (0.0001 -- 0.1643)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 79.253 Acc@5 96.266 loss 0.745
Accuracy of the network on the 482 val images: 79.25%
[2023-08-31 02:14:57,982] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 02:14:57,983] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 02:14:57,983] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 02:14:57,983] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 02:14:59,399] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 02:14:59,399] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.25%
Epoch: [113]  [  0/160]  eta: 0:19:03  lr: 0.000010  min_lr: 0.000000  loss: 1.1107 (1.1107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0478 (5.0478)  time: 7.1499 (7.1499 -- 7.1499)  data: 6.6180 (6.6180 -- 6.6180)  max mem: 16413
Epoch: [113]  [ 20/160]  eta: 0:02:38  lr: 0.000010  min_lr: 0.000000  loss: 1.9112 (1.9277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6121 (9.0698)  time: 0.8342 (0.5288 -- 3.8415)  data: 0.2716 (0.0005 -- 3.3127)  max mem: 16413
Epoch: [113]  [ 40/160]  eta: 0:02:03  lr: 0.000010  min_lr: 0.000000  loss: 1.8482 (1.8603)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7280 (8.6515)  time: 0.9263 (0.5202 -- 4.3277)  data: 0.3766 (0.0003 -- 3.8009)  max mem: 16413
[2023-08-31 02:15:57,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:15:57,547] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:15:57,548] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:15:57,548] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [113]  [ 60/160]  eta: 0:01:36  lr: 0.000010  min_lr: 0.000000  loss: 1.7796 (1.8736)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7549 (8.8529)  time: 0.8179 (0.5439 -- 2.8122)  data: 0.1339 (0.0007 -- 1.8308)  max mem: 16413
Epoch: [113]  [ 80/160]  eta: 0:01:14  lr: 0.000010  min_lr: 0.000000  loss: 1.9091 (1.9005)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5045 (8.9507)  time: 0.8246 (0.5351 -- 2.4647)  data: 0.0344 (0.0009 -- 0.6343)  max mem: 16413
[2023-08-31 02:16:32,923] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18180
[2023-08-31 02:16:32,923] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18180
[2023-08-31 02:16:32,923] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:16:32,923] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 02:16:32,923] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [113]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.8984 (1.8969)  loss_scale: 32768.0000 (23034.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1927 (8.9500)  time: 0.9144 (0.5236 -- 3.2247)  data: 0.0406 (0.0003 -- 0.7855)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.5967 (1.8681)  loss_scale: 16384.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5975 (8.8270)  time: 0.7842 (0.5309 -- 2.1981)  data: 0.0019 (0.0005 -- 0.0043)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.9005 (1.8715)  loss_scale: 16384.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7295 (8.8355)  time: 0.9776 (0.5113 -- 4.2213)  data: 0.0013 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 2.0555 (1.8920)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3680 (8.7339)  time: 0.6234 (0.4973 -- 1.9676)  data: 0.0011 (0.0002 -- 0.0060)  max mem: 16413
Epoch: [113] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 2.0555 (1.8588)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3680 (8.7339)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4168 (0.4168)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4502 (2.4502 -- 2.4502)  data: 2.2037 (2.2037 -- 2.2037)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5625 (0.8049)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4300 (0.2076 -- 2.4502)  data: 0.2046 (0.0008 -- 2.2037)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5625 (0.7375)  acc1: 88.8889 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2218 (0.1693 -- 0.3922)  data: 0.0127 (0.0001 -- 0.2044)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6379 (0.7944)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2040 (0.1330 -- 0.3922)  data: 0.0124 (0.0001 -- 0.2044)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 78.423 Acc@5 96.473 loss 0.748
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 79.25%
Epoch: [114]  [  0/160]  eta: 0:18:23  lr: 0.000010  min_lr: 0.000000  loss: 1.7957 (1.7957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0958 (11.0958)  time: 6.8954 (6.8954 -- 6.8954)  data: 4.0922 (4.0922 -- 4.0922)  max mem: 16413
Epoch: [114]  [ 20/160]  eta: 0:02:41  lr: 0.000010  min_lr: 0.000000  loss: 1.8034 (1.8116)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6153 (9.1262)  time: 0.8643 (0.5299 -- 2.3651)  data: 0.1262 (0.0008 -- 1.8335)  max mem: 16413
Epoch: [114]  [ 40/160]  eta: 0:02:03  lr: 0.000010  min_lr: 0.000000  loss: 1.9007 (1.8610)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3124 (9.0889)  time: 0.8939 (0.5246 -- 3.7052)  data: 0.0445 (0.0003 -- 0.7255)  max mem: 16413
[2023-08-31 02:18:18,307] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18289
[2023-08-31 02:18:18,307] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:18:18,307] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18289
[2023-08-31 02:18:18,307] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:18:18,307] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [114]  [ 60/160]  eta: 0:01:37  lr: 0.000010  min_lr: 0.000000  loss: 1.6986 (1.8149)  loss_scale: 8192.0000 (14772.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6856 (8.8322)  time: 0.8835 (0.5256 -- 3.9085)  data: 0.0017 (0.0004 -- 0.0084)  max mem: 16413
Epoch: [114]  [ 80/160]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000000  loss: 1.8819 (1.8320)  loss_scale: 8192.0000 (13147.6543)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0463 (9.0964)  time: 0.8493 (0.5151 -- 4.1923)  data: 0.0020 (0.0003 -- 0.0124)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.9250 (1.8385)  loss_scale: 8192.0000 (12166.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8415 (9.1662)  time: 0.9002 (0.5224 -- 3.0907)  data: 0.0927 (0.0004 -- 0.9238)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.6904 (1.8271)  loss_scale: 8192.0000 (11509.4215)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8403 (9.0882)  time: 0.8393 (0.5393 -- 2.8530)  data: 0.0025 (0.0005 -- 0.0162)  max mem: 16413
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.9120 (1.8453)  loss_scale: 8192.0000 (11038.8652)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7198 (9.3241)  time: 0.8404 (0.5250 -- 3.4952)  data: 0.1319 (0.0002 -- 2.0563)  max mem: 16413
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.9611 (1.8545)  loss_scale: 8192.0000 (10700.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7166 (9.3246)  time: 0.6782 (0.4939 -- 3.1363)  data: 0.0261 (0.0002 -- 0.5088)  max mem: 16413
Epoch: [114] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.9611 (1.8448)  loss_scale: 8192.0000 (10700.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7166 (9.3246)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3649 (0.3649)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5942 (2.5942 -- 2.5942)  data: 2.3387 (2.3387 -- 2.3387)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5540 (0.8062)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4668 (0.2046 -- 2.5942)  data: 0.2314 (0.0006 -- 2.3387)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5754 (0.7337)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2222 (0.1690 -- 0.3939)  data: 0.0105 (0.0001 -- 0.1957)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6448 (0.7941)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.1995 (0.1329 -- 0.3939)  data: 0.0102 (0.0001 -- 0.1957)  max mem: 16413
Val: Total time: 0:00:08 (0.2972 s / it)
* Acc@1 79.046 Acc@5 96.266 loss 0.745
Accuracy of the network on the 482 val images: 79.05%
Max accuracy: 79.25%
Epoch: [115]  [  0/160]  eta: 0:20:16  lr: 0.000009  min_lr: 0.000000  loss: 1.6863 (1.6863)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5471 (7.5471)  time: 7.6034 (7.6034 -- 7.6034)  data: 5.5001 (5.5001 -- 5.5001)  max mem: 16413
[2023-08-31 02:20:21,503] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:20:21,503] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:20:21,503] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:20:21,503] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [115]  [ 20/160]  eta: 0:02:48  lr: 0.000009  min_lr: 0.000000  loss: 1.7899 (1.7944)  loss_scale: 8192.0000 (9362.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6806 (8.2839)  time: 0.8831 (0.5304 -- 2.2939)  data: 0.1139 (0.0002 -- 1.7523)  max mem: 16413
Epoch: [115]  [ 40/160]  eta: 0:02:07  lr: 0.000009  min_lr: 0.000000  loss: 1.6888 (1.7824)  loss_scale: 16384.0000 (12787.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9190 (8.8313)  time: 0.9179 (0.5281 -- 4.2496)  data: 0.0855 (0.0004 -- 1.6679)  max mem: 16413
Epoch: [115]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000000  loss: 1.7226 (1.7709)  loss_scale: 16384.0000 (13966.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1181 (9.0038)  time: 0.8107 (0.5320 -- 2.6225)  data: 0.0015 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 1.8897 (1.7950)  loss_scale: 16384.0000 (14563.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4190 (8.9008)  time: 0.8691 (0.5293 -- 4.0098)  data: 0.0014 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [115]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.9292 (1.8057)  loss_scale: 16384.0000 (14924.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2888 (8.9384)  time: 0.8304 (0.5399 -- 3.9984)  data: 0.0030 (0.0005 -- 0.0152)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.9895 (1.8146)  loss_scale: 16384.0000 (15165.3554)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6310 (8.9726)  time: 0.8135 (0.5356 -- 3.9055)  data: 0.0013 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [115]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.8156 (1.8179)  loss_scale: 16384.0000 (15338.2128)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8549 (8.9227)  time: 0.8685 (0.5236 -- 3.3110)  data: 0.0016 (0.0002 -- 0.0047)  max mem: 16413
[2023-08-31 02:22:10,683] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:22:10,683] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:22:10,685] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:22:10,685] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8791 (1.8223)  loss_scale: 32768.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1509 (8.8468)  time: 0.6571 (0.4959 -- 3.0094)  data: 0.0009 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [115] Total time: 0:02:20 (0.8753 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.8791 (1.8205)  loss_scale: 32768.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1509 (8.8468)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3621 (0.3621)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2663 (2.2663 -- 2.2663)  data: 1.9850 (1.9850 -- 1.9850)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5568 (0.8046)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4557 (0.2018 -- 2.2663)  data: 0.2364 (0.0007 -- 1.9850)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5568 (0.7298)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2331 (0.1691 -- 0.8228)  data: 0.0309 (0.0001 -- 0.6014)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6536 (0.7901)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.2173 (0.1327 -- 0.8228)  data: 0.0304 (0.0001 -- 0.6014)  max mem: 16413
Val: Total time: 0:00:07 (0.2933 s / it)
* Acc@1 78.423 Acc@5 96.473 loss 0.745
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 79.25%
Epoch: [116]  [  0/160]  eta: 0:22:04  lr: 0.000009  min_lr: 0.000000  loss: 1.9734 (1.9734)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8678 (7.8678)  time: 8.2788 (8.2788 -- 8.2788)  data: 7.7376 (7.7376 -- 7.7376)  max mem: 16413
[2023-08-31 02:22:43,865] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18575
[2023-08-31 02:22:43,865] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18575
[2023-08-31 02:22:43,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:22:43,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:22:43,866] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [ 20/160]  eta: 0:02:44  lr: 0.000009  min_lr: 0.000000  loss: 1.7051 (1.8212)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5523 (8.1938)  time: 0.8183 (0.5301 -- 3.9150)  data: 0.2694 (0.0007 -- 3.3821)  max mem: 16413
Epoch: [116]  [ 40/160]  eta: 0:02:05  lr: 0.000009  min_lr: 0.000000  loss: 1.7879 (1.8419)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5900 (8.2602)  time: 0.9187 (0.5232 -- 3.4686)  data: 0.3748 (0.0004 -- 2.9373)  max mem: 16413
Epoch: [116]  [ 60/160]  eta: 0:01:36  lr: 0.000009  min_lr: 0.000000  loss: 1.8645 (1.8334)  loss_scale: 16384.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7952 (8.2243)  time: 0.8016 (0.5236 -- 2.3670)  data: 0.1789 (0.0004 -- 1.8545)  max mem: 16413
Epoch: [116]  [ 80/160]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000000  loss: 1.7703 (1.8166)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3985 (8.3461)  time: 1.0008 (0.5255 -- 5.3993)  data: 0.2024 (0.0002 -- 1.4995)  max mem: 16413
Epoch: [116]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.7968 (1.8194)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3973 (8.2815)  time: 0.6833 (0.5212 -- 1.7769)  data: 0.0475 (0.0002 -- 0.9265)  max mem: 16413
Epoch: [116]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.5718 (1.7976)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0458 (8.2706)  time: 0.9554 (0.5354 -- 3.2039)  data: 0.2008 (0.0005 -- 2.0550)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:17  lr: 0.000009  min_lr: 0.000000  loss: 1.8325 (1.8108)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1941 (8.3469)  time: 0.7307 (0.5326 -- 2.3350)  data: 0.0024 (0.0002 -- 0.0150)  max mem: 16413
[2023-08-31 02:24:36,917] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:24:36,917] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:24:36,918] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:24:36,918] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:24:42,658] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18712
[2023-08-31 02:24:42,658] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18712
[2023-08-31 02:24:42,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:24:42,658] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:24:42,658] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7333 (1.8035)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4207 (8.3245)  time: 0.7438 (0.4946 -- 3.1558)  data: 0.0278 (0.0001 -- 0.2743)  max mem: 16413
Epoch: [116] Total time: 0:02:20 (0.8801 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7333 (1.8174)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4207 (8.3245)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3656 (0.3656)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4663 (2.4663 -- 2.4663)  data: 2.2300 (2.2300 -- 2.2300)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5857 (0.8172)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (96.9697)  time: 0.4337 (0.1968 -- 2.4663)  data: 0.2138 (0.0006 -- 2.2300)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5517 (0.7361)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.8254)  time: 0.2145 (0.1730 -- 0.3447)  data: 0.0074 (0.0001 -- 0.1112)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6595 (0.7979)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.8506)  time: 0.2002 (0.1324 -- 0.3447)  data: 0.0071 (0.0001 -- 0.1112)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 77.593 Acc@5 96.266 loss 0.752
Accuracy of the network on the 482 val images: 77.59%
Max accuracy: 79.25%
Epoch: [117]  [  0/160]  eta: 0:17:14  lr: 0.000009  min_lr: 0.000000  loss: 1.4746 (1.4746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2854 (7.2854)  time: 6.4655 (6.4655 -- 6.4655)  data: 5.9219 (5.9219 -- 5.9219)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:34  lr: 0.000009  min_lr: 0.000000  loss: 1.8202 (1.7224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6682 (8.8227)  time: 0.8345 (0.5284 -- 2.0393)  data: 0.0287 (0.0004 -- 0.4289)  max mem: 16413
Epoch: [117]  [ 40/160]  eta: 0:02:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8401 (1.7617)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6251 (8.6226)  time: 0.8944 (0.5320 -- 2.9696)  data: 0.1105 (0.0003 -- 1.0262)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:39  lr: 0.000009  min_lr: 0.000000  loss: 1.9589 (1.8351)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9532 (8.7873)  time: 0.9867 (0.5232 -- 4.2202)  data: 0.0013 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [117]  [ 80/160]  eta: 0:01:17  lr: 0.000009  min_lr: 0.000000  loss: 1.9133 (1.8482)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1656 (8.6108)  time: 0.8636 (0.5210 -- 4.2630)  data: 0.0017 (0.0003 -- 0.0092)  max mem: 16413
[2023-08-31 02:26:25,892] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18815
[2023-08-31 02:26:25,892] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18815
[2023-08-31 02:26:25,892] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:26:25,892] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:26:25,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [117]  [100/160]  eta: 0:00:57  lr: 0.000009  min_lr: 0.000000  loss: 1.7361 (1.8380)  loss_scale: 16384.0000 (15897.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2913 (8.3514)  time: 0.9437 (0.5159 -- 4.3803)  data: 0.0018 (0.0002 -- 0.0154)  max mem: 16413
Epoch: [117]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.8337 (1.8341)  loss_scale: 8192.0000 (14623.7355)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2099 (8.3915)  time: 0.7654 (0.5277 -- 4.3925)  data: 0.0013 (0.0002 -- 0.0064)  max mem: 16413
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.9023 (1.8387)  loss_scale: 8192.0000 (13711.4326)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5878 (8.3396)  time: 0.8480 (0.5382 -- 2.7666)  data: 0.0016 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8505 (1.8334)  loss_scale: 8192.0000 (13056.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8786 (8.3435)  time: 0.7225 (0.4964 -- 4.0827)  data: 0.0008 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [117] Total time: 0:02:23 (0.8944 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.8505 (1.8351)  loss_scale: 8192.0000 (13056.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8786 (8.3435)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3847 (0.3847)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6012 (2.6012 -- 2.6012)  data: 2.3361 (2.3361 -- 2.3361)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5598 (0.8022)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4580 (0.2023 -- 2.6012)  data: 0.2357 (0.0004 -- 2.3361)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5560 (0.7314)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2191 (0.1697 -- 0.4888)  data: 0.0130 (0.0001 -- 0.2495)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6470 (0.7907)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.4357)  time: 0.2024 (0.1333 -- 0.4888)  data: 0.0128 (0.0001 -- 0.2495)  max mem: 16413
Val: Total time: 0:00:07 (0.2958 s / it)
* Acc@1 77.801 Acc@5 96.058 loss 0.746
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.25%
Epoch: [118]  [  0/160]  eta: 0:25:28  lr: 0.000009  min_lr: 0.000000  loss: 1.4985 (1.4985)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8447 (6.8447)  time: 9.5505 (9.5505 -- 9.5505)  data: 9.0181 (9.0181 -- 9.0181)  max mem: 16413
Epoch: [118]  [ 20/160]  eta: 0:02:46  lr: 0.000009  min_lr: 0.000000  loss: 1.8346 (1.8562)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0711 (9.0206)  time: 0.7741 (0.5156 -- 2.9024)  data: 0.2267 (0.0003 -- 2.3426)  max mem: 16413
Epoch: [118]  [ 40/160]  eta: 0:02:01  lr: 0.000009  min_lr: 0.000000  loss: 1.7714 (1.8493)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9637 (8.5290)  time: 0.8200 (0.5333 -- 2.7636)  data: 0.2677 (0.0002 -- 2.2019)  max mem: 16413
Epoch: [118]  [ 60/160]  eta: 0:01:36  lr: 0.000009  min_lr: 0.000000  loss: 1.8218 (1.8386)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9340 (8.7743)  time: 0.8686 (0.5293 -- 3.3092)  data: 0.2836 (0.0008 -- 2.7816)  max mem: 16413
[2023-08-31 02:28:27,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:28:27,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:28:27,473] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:28:27,473] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [118]  [ 80/160]  eta: 0:01:14  lr: 0.000009  min_lr: 0.000000  loss: 2.0769 (1.8604)  loss_scale: 16384.0000 (9911.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9258 (8.7182)  time: 0.8480 (0.5178 -- 2.5429)  data: 0.2450 (0.0005 -- 1.4149)  max mem: 16413
Epoch: [118]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.6766 (1.8415)  loss_scale: 16384.0000 (11193.0297)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9081 (8.7614)  time: 0.9350 (0.5212 -- 4.0506)  data: 0.3643 (0.0008 -- 3.5174)  max mem: 16413
[2023-08-31 02:29:16,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=109, lr=[2.0707675652636497e-07, 2.0707675652636497e-07, 2.761023420351533e-07, 2.761023420351533e-07, 3.6813645604687107e-07, 3.6813645604687107e-07, 4.908486080624948e-07, 4.908486080624948e-07, 6.54464810749993e-07, 6.54464810749993e-07, 8.726197476666574e-07, 8.726197476666574e-07, 1.1634929968888765e-06, 1.1634929968888765e-06, 1.5513239958518352e-06, 1.5513239958518352e-06, 2.0684319944691137e-06, 2.0684319944691137e-06, 2.757909325958818e-06, 2.757909325958818e-06, 3.677212434611758e-06, 3.677212434611758e-06, 4.9029499128156765e-06, 4.9029499128156765e-06, 6.537266550420903e-06, 6.537266550420903e-06, 8.716355400561203e-06, 8.716355400561203e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 02:29:16,626] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=17.080606995034614, CurrSamplesPerSec=22.98435580608835, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.8912 (1.8283)  loss_scale: 16384.0000 (12051.0413)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8753 (8.8684)  time: 0.8850 (0.5179 -- 5.2196)  data: 0.2550 (0.0004 -- 3.2125)  max mem: 16413
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.8248 (1.8283)  loss_scale: 16384.0000 (12665.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1926 (8.8239)  time: 0.8496 (0.5148 -- 2.7023)  data: 0.1298 (0.0004 -- 1.3041)  max mem: 16413
[2023-08-31 02:29:43,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19034
[2023-08-31 02:29:43,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:29:43,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19034
[2023-08-31 02:29:43,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:29:43,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7385 (1.8084)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1269 (8.6969)  time: 0.7165 (0.4916 -- 2.4285)  data: 0.0374 (0.0002 -- 0.7338)  max mem: 16413
Epoch: [118] Total time: 0:02:21 (0.8818 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7385 (1.8248)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1269 (8.6969)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3635 (0.3635)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4554 (2.4554 -- 2.4554)  data: 2.1808 (2.1808 -- 2.1808)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5395 (0.7910)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4371 (0.1906 -- 2.4554)  data: 0.2077 (0.0008 -- 2.1808)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5587 (0.7254)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2184 (0.1692 -- 0.2971)  data: 0.0075 (0.0001 -- 0.0929)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6447 (0.7847)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.1996 (0.1327 -- 0.2971)  data: 0.0071 (0.0001 -- 0.0929)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 78.631 Acc@5 96.058 loss 0.739
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 79.25%
Epoch: [119]  [  0/160]  eta: 0:22:41  lr: 0.000009  min_lr: 0.000000  loss: 1.8109 (1.8109)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9889 (11.9889)  time: 8.5067 (8.5067 -- 8.5067)  data: 7.1150 (7.1150 -- 7.1150)  max mem: 16413
Epoch: [119]  [ 20/160]  eta: 0:02:34  lr: 0.000009  min_lr: 0.000000  loss: 1.8726 (1.8416)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8430 (9.4494)  time: 0.7298 (0.5236 -- 2.8453)  data: 0.1200 (0.0004 -- 1.2159)  max mem: 16413
Epoch: [119]  [ 40/160]  eta: 0:02:02  lr: 0.000009  min_lr: 0.000000  loss: 2.0206 (1.9011)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7921 (9.5880)  time: 0.9422 (0.5318 -- 4.0347)  data: 0.1138 (0.0003 -- 1.3849)  max mem: 16413
Epoch: [119]  [ 60/160]  eta: 0:01:40  lr: 0.000009  min_lr: 0.000000  loss: 1.9975 (1.8934)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5896 (9.2395)  time: 0.9822 (0.5237 -- 4.4497)  data: 0.0015 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:17  lr: 0.000009  min_lr: 0.000000  loss: 1.9666 (1.9032)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3580 (9.1235)  time: 0.8573 (0.5106 -- 4.0994)  data: 0.0011 (0.0003 -- 0.0022)  max mem: 16413
Epoch: [119]  [100/160]  eta: 0:00:57  lr: 0.000009  min_lr: 0.000000  loss: 1.6754 (1.8766)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3381 (8.9936)  time: 0.9439 (0.5213 -- 3.5199)  data: 0.0011 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [119]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.6324 (1.8505)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5162 (9.0214)  time: 0.7524 (0.5299 -- 2.7592)  data: 0.0024 (0.0002 -- 0.0137)  max mem: 16413
[2023-08-31 02:31:49,212] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:31:49,212] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:31:49,212] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:31:49,212] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.9445 (1.8638)  loss_scale: 16384.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6558 (8.9314)  time: 0.8141 (0.5247 -- 2.7183)  data: 0.0017 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.8578 (1.8560)  loss_scale: 16384.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7643 (8.8445)  time: 0.6723 (0.4974 -- 2.6498)  data: 0.0238 (0.0002 -- 0.4603)  max mem: 16413
Epoch: [119] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.8578 (1.8495)  loss_scale: 16384.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7643 (8.8445)
[2023-08-31 02:32:15,784] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-08-31 02:32:15,786] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-08-31 02:32:15,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-08-31 02:32:15,786] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-08-31 02:32:16,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-08-31 02:32:16,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3637 (0.3637)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3111 (2.3111 -- 2.3111)  data: 2.1005 (2.1005 -- 2.1005)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5516 (0.7918)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4341 (0.2040 -- 2.3111)  data: 0.2145 (0.0008 -- 2.1005)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5354 (0.7234)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2291 (0.1703 -- 0.4726)  data: 0.0213 (0.0001 -- 0.2436)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6402 (0.7843)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.4357)  time: 0.2122 (0.1330 -- 0.4726)  data: 0.0209 (0.0001 -- 0.2436)  max mem: 16413
Val: Total time: 0:00:07 (0.2921 s / it)
* Acc@1 77.801 Acc@5 96.266 loss 0.743
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 79.25%
Epoch: [120]  [  0/160]  eta: 0:18:27  lr: 0.000008  min_lr: 0.000000  loss: 1.9813 (1.9813)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0056 (10.0056)  time: 6.9247 (6.9247 -- 6.9247)  data: 6.0629 (6.0629 -- 6.0629)  max mem: 16413
Epoch: [120]  [ 20/160]  eta: 0:02:40  lr: 0.000008  min_lr: 0.000000  loss: 1.9183 (1.9324)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6921 (8.7361)  time: 0.8603 (0.5395 -- 3.3523)  data: 0.0028 (0.0004 -- 0.0124)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:07  lr: 0.000008  min_lr: 0.000000  loss: 1.8721 (1.8979)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8581 (8.6998)  time: 0.9740 (0.5223 -- 3.8566)  data: 0.3730 (0.0003 -- 3.3186)  max mem: 16413
Epoch: [120]  [ 60/160]  eta: 0:01:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9155 (1.8905)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4368 (8.9319)  time: 0.8022 (0.5085 -- 4.3772)  data: 0.2605 (0.0003 -- 3.8317)  max mem: 16413
Epoch: [120]  [ 80/160]  eta: 0:01:15  lr: 0.000008  min_lr: 0.000000  loss: 1.6074 (1.8401)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4225 (8.7889)  time: 0.8452 (0.5184 -- 3.7408)  data: 0.0145 (0.0003 -- 0.2657)  max mem: 16413
[2023-08-31 02:33:45,545] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19286
[2023-08-31 02:33:45,545] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19286
[2023-08-31 02:33:45,545] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:33:45,545] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:33:45,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [120]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 1.9194 (1.8638)  loss_scale: 8192.0000 (15167.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9950 (8.6871)  time: 0.8252 (0.5171 -- 2.8427)  data: 0.1953 (0.0003 -- 2.3193)  max mem: 16413
Epoch: [120]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.7214 (1.8561)  loss_scale: 8192.0000 (14014.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6552 (8.6117)  time: 0.8412 (0.5243 -- 2.3470)  data: 0.1338 (0.0004 -- 1.7188)  max mem: 16413
Epoch: [120]  [140/160]  eta: 0:00:17  lr: 0.000008  min_lr: 0.000000  loss: 1.8389 (1.8476)  loss_scale: 8192.0000 (13188.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2939 (8.6071)  time: 0.8419 (0.5353 -- 2.8072)  data: 0.2011 (0.0004 -- 2.2753)  max mem: 16413
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 2.0097 (1.8509)  loss_scale: 8192.0000 (12595.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1802 (8.6427)  time: 0.7228 (0.4973 -- 3.1828)  data: 0.1812 (0.0002 -- 2.6389)  max mem: 16413
Epoch: [120] Total time: 0:02:20 (0.8794 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 2.0097 (1.8433)  loss_scale: 8192.0000 (12595.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1802 (8.6427)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3933 (0.3933)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4459 (2.4459 -- 2.4459)  data: 2.2288 (2.2288 -- 2.2288)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5212 (0.7875)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4419 (0.1920 -- 2.4459)  data: 0.2290 (0.0007 -- 2.2288)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5555 (0.7212)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2264 (0.1688 -- 0.4916)  data: 0.0234 (0.0001 -- 0.2802)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6267 (0.7795)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.8506)  time: 0.2120 (0.1327 -- 0.4916)  data: 0.0231 (0.0001 -- 0.2802)  max mem: 16413
Val: Total time: 0:00:07 (0.2947 s / it)
* Acc@1 78.838 Acc@5 96.473 loss 0.739
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 79.25%
Epoch: [121]  [  0/160]  eta: 0:21:30  lr: 0.000008  min_lr: 0.000000  loss: 2.3242 (2.3242)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9689 (7.9689)  time: 8.0626 (8.0626 -- 8.0626)  data: 6.0850 (6.0850 -- 6.0850)  max mem: 16413
Epoch: [121]  [ 20/160]  eta: 0:02:45  lr: 0.000008  min_lr: 0.000000  loss: 1.8597 (1.8223)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1819 (8.6865)  time: 0.8400 (0.5296 -- 2.6601)  data: 0.0176 (0.0005 -- 0.3022)  max mem: 16413
Epoch: [121]  [ 40/160]  eta: 0:02:12  lr: 0.000008  min_lr: 0.000000  loss: 1.6096 (1.7449)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7373 (8.5161)  time: 1.0279 (0.5220 -- 3.3781)  data: 0.0015 (0.0004 -- 0.0032)  max mem: 16413
[2023-08-31 02:35:49,986] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:35:49,987] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:35:49,989] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:35:49,989] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [121]  [ 60/160]  eta: 0:01:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9040 (1.7928)  loss_scale: 8192.0000 (8997.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0104 (8.4576)  time: 0.7076 (0.5202 -- 3.1993)  data: 0.0015 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [121]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000000  loss: 1.7447 (1.7930)  loss_scale: 16384.0000 (10821.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9640 (8.5625)  time: 0.8867 (0.5223 -- 3.9306)  data: 0.0014 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [121]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.6383 (1.7941)  loss_scale: 16384.0000 (11923.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2286 (8.5444)  time: 0.8566 (0.5303 -- 2.7902)  data: 0.1171 (0.0003 -- 1.6980)  max mem: 16413
[2023-08-31 02:36:34,317] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19469
[2023-08-31 02:36:34,317] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19469
[2023-08-31 02:36:34,317] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:36:34,317] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:36:34,317] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [121]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.8799 (1.8145)  loss_scale: 8192.0000 (11847.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0697 (8.4581)  time: 0.8411 (0.5267 -- 2.7038)  data: 0.0037 (0.0003 -- 0.0391)  max mem: 16413
Epoch: [121]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.9294 (1.8246)  loss_scale: 8192.0000 (11329.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6582 (8.6129)  time: 0.8732 (0.5278 -- 4.5175)  data: 0.0269 (0.0005 -- 0.4726)  max mem: 16413
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7409 (1.8159)  loss_scale: 8192.0000 (10956.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5594 (8.6711)  time: 0.6607 (0.4950 -- 1.5741)  data: 0.0833 (0.0002 -- 0.7824)  max mem: 16413
Epoch: [121] Total time: 0:02:21 (0.8839 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7409 (1.8270)  loss_scale: 8192.0000 (10956.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5594 (8.6711)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3982 (0.3982)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4936 (2.4936 -- 2.4936)  data: 2.2397 (2.2397 -- 2.2397)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5141 (0.7864)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4353 (0.2066 -- 2.4936)  data: 0.2105 (0.0004 -- 2.2397)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5379 (0.7187)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2181 (0.1698 -- 0.3201)  data: 0.0108 (0.0001 -- 0.1381)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6309 (0.7774)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.8506)  time: 0.2003 (0.1366 -- 0.3201)  data: 0.0106 (0.0001 -- 0.1381)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 79.461 Acc@5 96.266 loss 0.738
Accuracy of the network on the 482 val images: 79.46%
[2023-08-31 02:37:22,494] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 02:37:22,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 02:37:22,496] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 02:37:22,496] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 02:37:23,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 02:37:23,996] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.46%
Epoch: [122]  [  0/160]  eta: 0:21:04  lr: 0.000008  min_lr: 0.000000  loss: 2.0331 (2.0331)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3110 (7.3110)  time: 7.9003 (7.9003 -- 7.9003)  data: 7.3326 (7.3326 -- 7.3326)  max mem: 16413
Epoch: [122]  [ 20/160]  eta: 0:02:44  lr: 0.000008  min_lr: 0.000000  loss: 1.8241 (1.8195)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2631 (8.9026)  time: 0.8373 (0.5267 -- 4.1365)  data: 0.2814 (0.0009 -- 3.5784)  max mem: 16413
Epoch: [122]  [ 40/160]  eta: 0:02:09  lr: 0.000008  min_lr: 0.000000  loss: 1.9307 (1.8777)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0807 (8.8159)  time: 0.9865 (0.5211 -- 3.9611)  data: 0.4473 (0.0002 -- 3.4408)  max mem: 16413
Epoch: [122]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000000  loss: 1.8741 (1.8805)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6145 (8.5129)  time: 0.7820 (0.5249 -- 3.0668)  data: 0.2372 (0.0003 -- 2.5275)  max mem: 16413
[2023-08-31 02:38:39,278] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:38:39,278] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:38:39,279] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:38:39,279] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [122]  [ 80/160]  eta: 0:01:18  lr: 0.000008  min_lr: 0.000000  loss: 1.9027 (1.8814)  loss_scale: 8192.0000 (8495.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5196 (8.6377)  time: 0.9903 (0.5243 -- 3.9933)  data: 0.4439 (0.0002 -- 3.4729)  max mem: 16413
Epoch: [122]  [100/160]  eta: 0:00:57  lr: 0.000008  min_lr: 0.000000  loss: 1.8257 (1.8577)  loss_scale: 16384.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6847 (8.4778)  time: 0.8520 (0.5099 -- 4.5987)  data: 0.3164 (0.0004 -- 4.0806)  max mem: 16413
Epoch: [122]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.8153 (1.8578)  loss_scale: 16384.0000 (11103.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2358 (8.4741)  time: 0.8425 (0.5252 -- 3.0428)  data: 0.2843 (0.0004 -- 2.5002)  max mem: 16413
Epoch: [122]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7187 (1.8414)  loss_scale: 16384.0000 (11852.2553)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0972 (8.4334)  time: 0.7376 (0.5316 -- 2.9593)  data: 0.1781 (0.0006 -- 2.3962)  max mem: 16413
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7277 (1.8283)  loss_scale: 16384.0000 (12390.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9974 (8.5041)  time: 0.7016 (0.4953 -- 1.6936)  data: 0.0200 (0.0002 -- 0.3857)  max mem: 16413
Epoch: [122] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7277 (1.8007)  loss_scale: 16384.0000 (12390.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9974 (8.5041)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3687 (0.3687)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1854 (2.1854 -- 2.1854)  data: 1.9604 (1.9604 -- 1.9604)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4994 (0.7834)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4271 (0.2003 -- 2.1854)  data: 0.2080 (0.0006 -- 1.9604)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5259 (0.7145)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2264 (0.1715 -- 0.5610)  data: 0.0179 (0.0001 -- 0.3179)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6395 (0.7704)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.8506)  time: 0.2126 (0.1331 -- 0.5610)  data: 0.0176 (0.0001 -- 0.3179)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 79.253 Acc@5 96.266 loss 0.730
Accuracy of the network on the 482 val images: 79.25%
Max accuracy: 79.46%
Epoch: [123]  [  0/160]  eta: 0:18:29  lr: 0.000008  min_lr: 0.000000  loss: 1.0720 (1.0720)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9826 (6.9826)  time: 6.9354 (6.9354 -- 6.9354)  data: 6.3861 (6.3861 -- 6.3861)  max mem: 16413
Epoch: [123]  [ 20/160]  eta: 0:02:41  lr: 0.000008  min_lr: 0.000000  loss: 1.9450 (1.8675)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2386 (8.8794)  time: 0.8654 (0.5336 -- 3.1334)  data: 0.0953 (0.0005 -- 1.1202)  max mem: 16413
Epoch: [123]  [ 40/160]  eta: 0:02:01  lr: 0.000008  min_lr: 0.000000  loss: 1.7853 (1.8503)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8735 (8.4927)  time: 0.8690 (0.5192 -- 3.2625)  data: 0.0974 (0.0003 -- 1.5777)  max mem: 16413
[2023-08-31 02:40:40,886] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:40:40,887] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:40:40,909] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:40:40,909] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:40:48,323] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19734
[2023-08-31 02:40:48,323] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19734
[2023-08-31 02:40:48,324] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:40:48,324] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 02:40:48,324] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [123]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000000  loss: 2.0255 (1.8854)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0300 (8.4318)  time: 0.9252 (0.5317 -- 3.5054)  data: 0.0234 (0.0004 -- 0.3678)  max mem: 16413
Epoch: [123]  [ 80/160]  eta: 0:01:14  lr: 0.000008  min_lr: 0.000000  loss: 1.8602 (1.8569)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9888 (8.4025)  time: 0.7871 (0.5256 -- 2.4275)  data: 0.2330 (0.0002 -- 1.9072)  max mem: 16413
Epoch: [123]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 2.0153 (1.8769)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6553 (8.5241)  time: 0.8835 (0.5273 -- 2.2086)  data: 0.0613 (0.0002 -- 1.1967)  max mem: 16413
Epoch: [123]  [120/160]  eta: 0:00:35  lr: 0.000008  min_lr: 0.000000  loss: 1.9411 (1.8798)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9387 (8.6156)  time: 0.7582 (0.5378 -- 2.1563)  data: 0.0023 (0.0005 -- 0.0089)  max mem: 16413
Epoch: [123]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7916 (1.8661)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0061 (8.6101)  time: 0.9525 (0.5255 -- 2.5616)  data: 0.1029 (0.0002 -- 1.9088)  max mem: 16413
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.8687 (1.8536)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9637 (8.6433)  time: 0.7080 (0.4965 -- 2.1867)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [123] Total time: 0:02:21 (0.8839 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.8687 (1.8413)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9637 (8.6433)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3460 (0.3460)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4477 (2.4477 -- 2.4477)  data: 2.2143 (2.2143 -- 2.2143)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5139 (0.8015)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4526 (0.1962 -- 2.4477)  data: 0.2320 (0.0007 -- 2.2143)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5139 (0.7183)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2243 (0.1704 -- 0.5518)  data: 0.0195 (0.0001 -- 0.3272)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6236 (0.7816)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.2075 (0.1328 -- 0.5518)  data: 0.0192 (0.0001 -- 0.3272)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 79.046 Acc@5 96.473 loss 0.739
Accuracy of the network on the 482 val images: 79.05%
Max accuracy: 79.46%
Epoch: [124]  [  0/160]  eta: 0:22:12  lr: 0.000008  min_lr: 0.000000  loss: 2.4400 (2.4400)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6302 (7.6302)  time: 8.3302 (8.3302 -- 8.3302)  data: 6.8156 (6.8156 -- 6.8156)  max mem: 16413
Epoch: [124]  [ 20/160]  eta: 0:02:59  lr: 0.000008  min_lr: 0.000000  loss: 1.8656 (1.8543)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6430 (8.6031)  time: 0.9332 (0.5113 -- 4.9214)  data: 0.0049 (0.0004 -- 0.0749)  max mem: 16413
[2023-08-31 02:42:51,658] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:42:51,658] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:42:51,659] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:42:51,659] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:43:07,818] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19880
[2023-08-31 02:43:07,818] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19880
[2023-08-31 02:43:07,818] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:43:07,818] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:43:07,818] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [124]  [ 40/160]  eta: 0:02:10  lr: 0.000008  min_lr: 0.000000  loss: 1.8940 (1.8508)  loss_scale: 32768.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8838 (8.6413)  time: 0.8856 (0.5123 -- 3.1647)  data: 0.0028 (0.0003 -- 0.0173)  max mem: 16413
Epoch: [124]  [ 60/160]  eta: 0:01:40  lr: 0.000008  min_lr: 0.000000  loss: 1.8889 (1.8495)  loss_scale: 16384.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7555 (8.5375)  time: 0.8162 (0.5216 -- 4.0085)  data: 0.0852 (0.0004 -- 1.0118)  max mem: 16413
Epoch: [124]  [ 80/160]  eta: 0:01:17  lr: 0.000008  min_lr: 0.000000  loss: 1.9081 (1.8381)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1963 (8.4787)  time: 0.8503 (0.5365 -- 3.5066)  data: 0.0939 (0.0003 -- 1.2857)  max mem: 16413
Epoch: [124]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.9006 (1.8454)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1517 (8.5691)  time: 0.8451 (0.5286 -- 4.3198)  data: 0.1937 (0.0003 -- 2.1383)  max mem: 16413
Epoch: [124]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9277 (1.8467)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0433 (8.6547)  time: 0.8558 (0.5244 -- 3.8730)  data: 0.3055 (0.0004 -- 3.3691)  max mem: 16413
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7968 (1.8410)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7776 (8.7821)  time: 0.8450 (0.5238 -- 4.7000)  data: 0.3009 (0.0004 -- 4.1836)  max mem: 16413
[2023-08-31 02:44:46,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=114, lr=[1.8046192724572118e-07, 1.8046192724572118e-07, 2.406159029942949e-07, 2.406159029942949e-07, 3.2082120399239323e-07, 3.2082120399239323e-07, 4.2776160532319094e-07, 4.2776160532319094e-07, 5.703488070975879e-07, 5.703488070975879e-07, 7.604650761301172e-07, 7.604650761301172e-07, 1.0139534348401563e-06, 1.0139534348401563e-06, 1.3519379131202085e-06, 1.3519379131202085e-06, 1.8025838841602778e-06, 1.8025838841602778e-06, 2.4034451788803704e-06, 2.4034451788803704e-06, 3.204593571840494e-06, 3.204593571840494e-06, 4.272791429120658e-06, 4.272791429120658e-06, 5.697055238827545e-06, 5.697055238827545e-06, 7.59607365177006e-06, 7.59607365177006e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 02:44:46,704] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=17.075794341723526, CurrSamplesPerSec=24.602199901359796, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7577 (1.8280)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4832 (8.7941)  time: 0.7571 (0.4964 -- 3.2023)  data: 0.2352 (0.0002 -- 2.6711)  max mem: 16413
Epoch: [124] Total time: 0:02:23 (0.8975 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7577 (1.7953)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4832 (8.7941)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3360 (0.3360)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5042 (2.5042 -- 2.5042)  data: 2.2759 (2.2759 -- 2.2759)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4876 (0.7829)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4348 (0.2080 -- 2.5042)  data: 0.2108 (0.0004 -- 2.2759)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5177 (0.7106)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2165 (0.1721 -- 0.2809)  data: 0.0071 (0.0001 -- 0.0959)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6199 (0.7727)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.4357)  time: 0.1955 (0.1330 -- 0.2809)  data: 0.0068 (0.0001 -- 0.0959)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 80.290 Acc@5 96.266 loss 0.727
Accuracy of the network on the 482 val images: 80.29%
[2023-08-31 02:44:54,527] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 02:44:54,529] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 02:44:54,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 02:44:54,529] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 02:44:55,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 02:44:55,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.29%
Epoch: [125]  [  0/160]  eta: 0:18:20  lr: 0.000008  min_lr: 0.000000  loss: 2.1489 (2.1489)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6699 (6.6699)  time: 6.8807 (6.8807 -- 6.8807)  data: 5.5432 (5.5432 -- 5.5432)  max mem: 16413
[2023-08-31 02:45:13,412] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:45:13,413] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:45:13,414] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:45:13,414] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [125]  [ 20/160]  eta: 0:02:53  lr: 0.000008  min_lr: 0.000000  loss: 1.9057 (1.9116)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2581 (9.6708)  time: 0.9542 (0.5384 -- 3.0989)  data: 0.0018 (0.0003 -- 0.0061)  max mem: 16413
[2023-08-31 02:45:38,068] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20039
[2023-08-31 02:45:38,068] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20039
[2023-08-31 02:45:38,069] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:45:38,069] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:45:38,069] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [125]  [ 40/160]  eta: 0:02:05  lr: 0.000008  min_lr: 0.000000  loss: 1.9124 (1.8602)  loss_scale: 32768.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0295 (9.0731)  time: 0.8384 (0.5219 -- 2.9095)  data: 0.0015 (0.0002 -- 0.0097)  max mem: 16413
Epoch: [125]  [ 60/160]  eta: 0:01:41  lr: 0.000008  min_lr: 0.000000  loss: 1.8063 (1.8519)  loss_scale: 16384.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2262 (8.8048)  time: 0.9482 (0.5203 -- 3.6589)  data: 0.0011 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [125]  [ 80/160]  eta: 0:01:15  lr: 0.000008  min_lr: 0.000000  loss: 2.0019 (1.8853)  loss_scale: 16384.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5906 (9.0063)  time: 0.7340 (0.5188 -- 3.5330)  data: 0.0015 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [125]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.9407 (1.8897)  loss_scale: 16384.0000 (21250.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8040 (8.8139)  time: 0.8493 (0.5356 -- 4.0032)  data: 0.0020 (0.0003 -- 0.0159)  max mem: 16413
Epoch: [125]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.6095 (1.8501)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7035 (8.7659)  time: 1.0657 (0.5160 -- 5.0257)  data: 0.0023 (0.0003 -- 0.0123)  max mem: 16413
Epoch: [125]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8740 (1.8454)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5532 (8.6956)  time: 0.7806 (0.5189 -- 3.4045)  data: 0.0017 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8027 (1.8454)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3870 (8.6050)  time: 0.6923 (0.4977 -- 3.2247)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [125] Total time: 0:02:23 (0.8977 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8027 (1.8413)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3870 (8.6050)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3588 (0.3588)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3848 (2.3848 -- 2.3848)  data: 2.1228 (2.1228 -- 2.1228)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5161 (0.7886)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4581 (0.1982 -- 2.3848)  data: 0.2268 (0.0009 -- 2.1228)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5313 (0.7112)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2289 (0.1695 -- 0.6118)  data: 0.0188 (0.0001 -- 0.3479)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6179 (0.7710)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.4357)  time: 0.2125 (0.1331 -- 0.6118)  data: 0.0178 (0.0001 -- 0.3479)  max mem: 16413
Val: Total time: 0:00:07 (0.2946 s / it)
* Acc@1 79.876 Acc@5 96.058 loss 0.732
Accuracy of the network on the 482 val images: 79.88%
Max accuracy: 80.29%
Epoch: [126]  [  0/160]  eta: 0:25:49  lr: 0.000007  min_lr: 0.000000  loss: 1.7538 (1.7538)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0811 (6.0811)  time: 9.6860 (9.6860 -- 9.6860)  data: 9.1673 (9.1673 -- 9.1673)  max mem: 16413
[2023-08-31 02:47:45,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:47:45,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:47:45,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:47:45,681] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [126]  [ 20/160]  eta: 0:03:04  lr: 0.000007  min_lr: 0.000000  loss: 1.6510 (1.7353)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9205 (8.6069)  time: 0.8994 (0.5097 -- 4.6840)  data: 0.3554 (0.0002 -- 4.1376)  max mem: 16413
[2023-08-31 02:47:56,823] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20183
[2023-08-31 02:47:56,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20183
[2023-08-31 02:47:56,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:47:56,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:47:56,824] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [126]  [ 40/160]  eta: 0:02:14  lr: 0.000007  min_lr: 0.000000  loss: 1.8512 (1.7505)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4265 (8.6801)  time: 0.9106 (0.5252 -- 3.6040)  data: 0.3655 (0.0002 -- 3.0837)  max mem: 16413
Epoch: [126]  [ 60/160]  eta: 0:01:41  lr: 0.000007  min_lr: 0.000000  loss: 1.7152 (1.7459)  loss_scale: 16384.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3680 (8.3813)  time: 0.7977 (0.5174 -- 3.1668)  data: 0.2573 (0.0002 -- 2.6436)  max mem: 16413
Epoch: [126]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.9628 (1.7906)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3459 (8.4599)  time: 0.7838 (0.5252 -- 2.1017)  data: 0.1717 (0.0003 -- 1.5576)  max mem: 16413
Epoch: [126]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.7843 (1.7912)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3195 (8.5112)  time: 0.8736 (0.5288 -- 3.1547)  data: 0.0018 (0.0004 -- 0.0059)  max mem: 16413
Epoch: [126]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.8551 (1.8111)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3862 (8.6277)  time: 0.8175 (0.5247 -- 2.8497)  data: 0.0016 (0.0002 -- 0.0034)  max mem: 16413
[2023-08-31 02:49:22,258] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20285
[2023-08-31 02:49:22,258] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20285
[2023-08-31 02:49:22,258] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:49:22,258] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 02:49:22,259] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [126]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.9783 (1.8285)  loss_scale: 8192.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8719 (8.5910)  time: 0.8403 (0.5267 -- 1.9866)  data: 0.0873 (0.0002 -- 1.0023)  max mem: 16413
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8227 (1.8336)  loss_scale: 8192.0000 (16128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1968 (8.6595)  time: 0.7877 (0.4963 -- 2.8049)  data: 0.0009 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [126] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8227 (1.8210)  loss_scale: 8192.0000 (16128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1968 (8.6595)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3558 (0.3558)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3900 (2.3900 -- 2.3900)  data: 2.1524 (2.1524 -- 2.1524)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5027 (0.7900)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4378 (0.2040 -- 2.3900)  data: 0.2178 (0.0007 -- 2.1524)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5430 (0.7170)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2187 (0.1723 -- 0.4333)  data: 0.0124 (0.0001 -- 0.2273)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6360 (0.7777)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.4357)  time: 0.2013 (0.1325 -- 0.4333)  data: 0.0117 (0.0001 -- 0.2273)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 79.461 Acc@5 96.058 loss 0.735
Accuracy of the network on the 482 val images: 79.46%
Max accuracy: 80.29%
Epoch: [127]  [  0/160]  eta: 0:19:56  lr: 0.000007  min_lr: 0.000000  loss: 2.1207 (2.1207)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3190 (5.3190)  time: 7.4769 (7.4769 -- 7.4769)  data: 6.9338 (6.9338 -- 6.9338)  max mem: 16413
Epoch: [127]  [ 20/160]  eta: 0:02:42  lr: 0.000007  min_lr: 0.000000  loss: 2.0176 (1.9240)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8710 (7.8256)  time: 0.8420 (0.5288 -- 3.7655)  data: 0.1013 (0.0007 -- 1.2560)  max mem: 16413
Epoch: [127]  [ 40/160]  eta: 0:02:07  lr: 0.000007  min_lr: 0.000000  loss: 1.5402 (1.7794)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0816 (8.1858)  time: 0.9695 (0.5364 -- 5.0720)  data: 0.4051 (0.0009 -- 4.5662)  max mem: 16413
Epoch: [127]  [ 60/160]  eta: 0:01:36  lr: 0.000007  min_lr: 0.000000  loss: 1.7988 (1.8054)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3904 (8.3525)  time: 0.7536 (0.5337 -- 3.2615)  data: 0.1852 (0.0001 -- 2.7217)  max mem: 16413
Epoch: [127]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000000  loss: 1.7980 (1.8202)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4326 (8.3756)  time: 0.8922 (0.5171 -- 2.9759)  data: 0.3253 (0.0004 -- 2.4673)  max mem: 16413
[2023-08-31 02:51:26,043] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:51:26,043] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 02:51:26,043] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:51:26,043] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [127]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.7827 (1.8197)  loss_scale: 8192.0000 (8759.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0297 (8.3408)  time: 0.8111 (0.5190 -- 4.1598)  data: 0.1211 (0.0003 -- 1.6673)  max mem: 16413
Epoch: [127]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.8969 (1.8236)  loss_scale: 16384.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5020 (8.4604)  time: 0.9431 (0.5227 -- 4.1996)  data: 0.3709 (0.0002 -- 3.6758)  max mem: 16413
Epoch: [127]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8202 (1.8239)  loss_scale: 16384.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3842 (8.4982)  time: 0.8293 (0.5168 -- 3.4718)  data: 0.2858 (0.0002 -- 2.9572)  max mem: 16413
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8713 (1.8381)  loss_scale: 16384.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2900 (8.5844)  time: 0.6418 (0.4962 -- 1.6447)  data: 0.0659 (0.0002 -- 1.1232)  max mem: 16413
Epoch: [127] Total time: 0:02:20 (0.8789 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8713 (1.8444)  loss_scale: 16384.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2900 (8.5844)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3795 (0.3795)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4256 (2.4256 -- 2.4256)  data: 2.1993 (2.1993 -- 2.1993)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5065 (0.7843)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4172 (0.1892 -- 2.4256)  data: 0.2163 (0.0005 -- 2.1993)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5423 (0.7104)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2266 (0.1691 -- 0.6132)  data: 0.0305 (0.0001 -- 0.4280)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6286 (0.7699)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.4357)  time: 0.2169 (0.1326 -- 0.6132)  data: 0.0304 (0.0001 -- 0.4280)  max mem: 16413
Val: Total time: 0:00:07 (0.2942 s / it)
* Acc@1 79.253 Acc@5 96.266 loss 0.728
Accuracy of the network on the 482 val images: 79.25%
Max accuracy: 80.29%
Epoch: [128]  [  0/160]  eta: 0:19:40  lr: 0.000007  min_lr: 0.000000  loss: 2.0670 (2.0670)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4048 (6.4048)  time: 7.3786 (7.3786 -- 7.3786)  data: 6.8488 (6.8488 -- 6.8488)  max mem: 16413
Epoch: [128]  [ 20/160]  eta: 0:02:47  lr: 0.000007  min_lr: 0.000000  loss: 1.7576 (1.7820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2094 (8.1393)  time: 0.8873 (0.5250 -- 3.4867)  data: 0.3358 (0.0003 -- 2.9708)  max mem: 16413
Epoch: [128]  [ 40/160]  eta: 0:02:10  lr: 0.000007  min_lr: 0.000000  loss: 1.8261 (1.8284)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9086 (8.2516)  time: 0.9718 (0.5217 -- 3.8339)  data: 0.4243 (0.0006 -- 3.2839)  max mem: 16413
Epoch: [128]  [ 60/160]  eta: 0:01:39  lr: 0.000007  min_lr: 0.000000  loss: 1.7813 (1.8208)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4782 (8.1660)  time: 0.8205 (0.5155 -- 3.3616)  data: 0.2799 (0.0002 -- 2.8153)  max mem: 16413
[2023-08-31 02:53:28,708] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:53:28,708] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:53:28,710] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:53:28,710] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [128]  [ 80/160]  eta: 0:01:18  lr: 0.000007  min_lr: 0.000000  loss: 1.7130 (1.8003)  loss_scale: 32768.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3102 (8.2783)  time: 0.9420 (0.5156 -- 3.5001)  data: 0.3944 (0.0002 -- 2.9577)  max mem: 16413
[2023-08-31 02:53:55,758] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20573
[2023-08-31 02:53:55,758] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20573
[2023-08-31 02:53:55,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:53:55,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:53:55,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [128]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 2.0412 (1.8287)  loss_scale: 32768.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3527 (8.3783)  time: 0.7672 (0.5247 -- 2.8442)  data: 0.2237 (0.0003 -- 2.3094)  max mem: 16413
Epoch: [128]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.8430 (1.8476)  loss_scale: 16384.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6319 (8.3857)  time: 0.8977 (0.5383 -- 2.7502)  data: 0.3461 (0.0004 -- 2.1925)  max mem: 16413
Epoch: [128]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8134 (1.8392)  loss_scale: 16384.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3208 (8.3770)  time: 0.8520 (0.5314 -- 2.6659)  data: 0.2315 (0.0004 -- 2.1521)  max mem: 16413
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.6136 (1.8300)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6134 (8.4478)  time: 0.6691 (0.4942 -- 2.4419)  data: 0.1413 (0.0001 -- 1.9378)  max mem: 16413
Epoch: [128] Total time: 0:02:23 (0.8938 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.6136 (1.8253)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6134 (8.4478)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3911 (0.3911)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4076 (2.4076 -- 2.4076)  data: 2.1855 (2.1855 -- 2.1855)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4887 (0.7896)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4353 (0.2014 -- 2.4076)  data: 0.2175 (0.0005 -- 2.1855)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5383 (0.7141)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2176 (0.1690 -- 0.3925)  data: 0.0115 (0.0001 -- 0.1832)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6363 (0.7734)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.1984 (0.1329 -- 0.3925)  data: 0.0104 (0.0001 -- 0.1832)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 78.631 Acc@5 96.058 loss 0.739
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 80.29%
Epoch: [129]  [  0/160]  eta: 0:18:48  lr: 0.000007  min_lr: 0.000000  loss: 1.0995 (1.0995)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0917 (12.0917)  time: 7.0552 (7.0552 -- 7.0552)  data: 6.4870 (6.4870 -- 6.4870)  max mem: 16413
Epoch: [129]  [ 20/160]  eta: 0:02:39  lr: 0.000007  min_lr: 0.000000  loss: 1.9185 (1.9268)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6811 (8.8741)  time: 0.8437 (0.5205 -- 3.1002)  data: 0.2562 (0.0002 -- 2.5624)  max mem: 16413
Epoch: [129]  [ 40/160]  eta: 0:02:09  lr: 0.000007  min_lr: 0.000000  loss: 1.9826 (1.9277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9449 (8.9754)  time: 1.0140 (0.5181 -- 3.0139)  data: 0.2978 (0.0004 -- 2.4762)  max mem: 16413
Epoch: [129]  [ 60/160]  eta: 0:01:41  lr: 0.000007  min_lr: 0.000000  loss: 1.8099 (1.9006)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4685 (8.9081)  time: 0.8772 (0.5138 -- 4.3700)  data: 0.3323 (0.0003 -- 3.8510)  max mem: 16413
[2023-08-31 02:56:00,279] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:56:00,279] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:56:00,280] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:56:00,281] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [129]  [ 80/160]  eta: 0:01:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8451 (1.8910)  loss_scale: 32768.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8009 (8.7522)  time: 0.8842 (0.5302 -- 4.5995)  data: 0.3335 (0.0003 -- 4.0794)  max mem: 16413
[2023-08-31 02:56:19,064] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20724
[2023-08-31 02:56:19,064] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20724
[2023-08-31 02:56:19,065] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:56:19,065] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:56:19,065] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [129]  [100/160]  eta: 0:00:57  lr: 0.000007  min_lr: 0.000000  loss: 1.8211 (1.8786)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0405 (8.5478)  time: 0.8613 (0.5258 -- 4.8548)  data: 0.3191 (0.0004 -- 4.3347)  max mem: 16413
Epoch: [129]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 2.0317 (1.8976)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9292 (8.8566)  time: 0.8401 (0.5265 -- 3.6157)  data: 0.2941 (0.0003 -- 3.0694)  max mem: 16413
Epoch: [129]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.7974 (1.8913)  loss_scale: 16384.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1653 (8.7421)  time: 0.7426 (0.5303 -- 2.3694)  data: 0.1926 (0.0003 -- 1.8384)  max mem: 16413
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8236 (1.8760)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5089 (8.7161)  time: 0.7192 (0.4967 -- 2.8352)  data: 0.1914 (0.0002 -- 2.2853)  max mem: 16413
Epoch: [129] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8236 (1.8667)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5089 (8.7161)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3909 (0.3909)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4912 (2.4912 -- 2.4912)  data: 2.2498 (2.2498 -- 2.2498)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5024 (0.7847)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4418 (0.1951 -- 2.4912)  data: 0.2184 (0.0008 -- 2.2498)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5577 (0.7139)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2206 (0.1698 -- 0.3715)  data: 0.0137 (0.0001 -- 0.1386)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6276 (0.7700)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.2021 (0.1331 -- 0.3715)  data: 0.0132 (0.0001 -- 0.1386)  max mem: 16413
Val: Total time: 0:00:07 (0.2926 s / it)
* Acc@1 78.423 Acc@5 96.266 loss 0.739
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 80.29%
Epoch: [130]  [  0/160]  eta: 0:15:33  lr: 0.000007  min_lr: 0.000000  loss: 1.9346 (1.9346)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5234 (9.5234)  time: 5.8342 (5.8342 -- 5.8342)  data: 5.3056 (5.3056 -- 5.3056)  max mem: 16413
Epoch: [130]  [ 20/160]  eta: 0:02:45  lr: 0.000007  min_lr: 0.000000  loss: 2.0137 (1.8735)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0367 (8.2542)  time: 0.9521 (0.5262 -- 4.1437)  data: 0.2543 (0.0006 -- 3.5890)  max mem: 16413
Epoch: [130]  [ 40/160]  eta: 0:02:12  lr: 0.000007  min_lr: 0.000000  loss: 1.9571 (1.8947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9091 (8.1659)  time: 1.0163 (0.5270 -- 4.1108)  data: 0.4680 (0.0006 -- 3.5828)  max mem: 16413
[2023-08-31 02:58:22,655] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:58:22,655] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:58:22,655] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 02:58:22,655] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 02:58:26,595] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20856
[2023-08-31 02:58:26,595] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20856
[2023-08-31 02:58:26,595] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:58:26,595] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 02:58:26,595] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [130]  [ 60/160]  eta: 0:01:40  lr: 0.000007  min_lr: 0.000000  loss: 1.8006 (1.8404)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7897 (8.0982)  time: 0.8056 (0.5184 -- 3.2371)  data: 0.2501 (0.0003 -- 2.7159)  max mem: 16413
Epoch: [130]  [ 80/160]  eta: 0:01:17  lr: 0.000007  min_lr: 0.000000  loss: 1.7812 (1.8252)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2790 (8.1776)  time: 0.8509 (0.5237 -- 3.2804)  data: 0.3005 (0.0008 -- 2.7263)  max mem: 16413
Epoch: [130]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.8761 (1.8466)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3127 (8.2887)  time: 0.7949 (0.5360 -- 4.2597)  data: 0.2405 (0.0002 -- 3.7231)  max mem: 16413
Epoch: [130]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.8211 (1.8549)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0646 (8.4294)  time: 1.0087 (0.5260 -- 4.3311)  data: 0.4616 (0.0002 -- 3.8171)  max mem: 16413
Epoch: [130]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.6367 (1.8376)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2673 (8.4195)  time: 0.8383 (0.5268 -- 3.8572)  data: 0.2932 (0.0002 -- 3.3146)  max mem: 16413
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.6744 (1.8239)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1961 (8.4602)  time: 0.6321 (0.4954 -- 2.7859)  data: 0.1134 (0.0002 -- 2.2556)  max mem: 16413
Epoch: [130] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.6744 (1.8545)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1961 (8.4602)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.4099 (0.4099)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6263 (2.6263 -- 2.6263)  data: 2.3897 (2.3897 -- 2.3897)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4956 (0.7829)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4351 (0.1982 -- 2.6263)  data: 0.2192 (0.0010 -- 2.3897)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5563 (0.7113)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2108 (0.1697 -- 0.2715)  data: 0.0064 (0.0001 -- 0.0924)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6261 (0.7684)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.8506)  time: 0.1960 (0.1325 -- 0.2715)  data: 0.0061 (0.0001 -- 0.0924)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 79.461 Acc@5 96.266 loss 0.737
Accuracy of the network on the 482 val images: 79.46%
Max accuracy: 80.29%
Epoch: [131]  [  0/160]  eta: 0:21:07  lr: 0.000007  min_lr: 0.000000  loss: 1.3402 (1.3402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8192 (5.8192)  time: 7.9188 (7.9188 -- 7.9188)  data: 7.4033 (7.4033 -- 7.4033)  max mem: 16413
Epoch: [131]  [ 20/160]  eta: 0:02:57  lr: 0.000007  min_lr: 0.000000  loss: 1.8220 (1.7844)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6088 (8.5599)  time: 0.9321 (0.5242 -- 3.2314)  data: 0.2432 (0.0002 -- 2.6940)  max mem: 16413
[2023-08-31 03:00:29,225] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:00:29,228] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:00:29,267] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:00:29,267] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:00:39,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=120, lr=[1.5484493871302982e-07, 1.5484493871302982e-07, 2.0645991828403974e-07, 2.0645991828403974e-07, 2.7527989104538634e-07, 2.7527989104538634e-07, 3.670398547271818e-07, 3.670398547271818e-07, 4.893864729695757e-07, 4.893864729695757e-07, 6.525152972927676e-07, 6.525152972927676e-07, 8.700203963903568e-07, 8.700203963903568e-07, 1.1600271951871424e-06, 1.1600271951871424e-06, 1.5467029269161898e-06, 1.5467029269161898e-06, 2.0622705692215864e-06, 2.0622705692215864e-06, 2.7496940922954488e-06, 2.7496940922954488e-06, 3.666258789727265e-06, 3.666258789727265e-06, 4.8883450529696865e-06, 4.8883450529696865e-06, 6.517793403959582e-06, 6.517793403959582e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 03:00:39,067] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=17.165908462137832, CurrSamplesPerSec=22.76431406940514, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [131]  [ 40/160]  eta: 0:02:08  lr: 0.000007  min_lr: 0.000000  loss: 1.8977 (1.8524)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3222 (8.5617)  time: 0.8751 (0.5319 -- 3.7494)  data: 0.3290 (0.0006 -- 3.2404)  max mem: 16413
[2023-08-31 03:00:56,709] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21016
[2023-08-31 03:00:56,709] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:00:56,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 03:00:56,709] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21016
[2023-08-31 03:00:56,709] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [131]  [ 60/160]  eta: 0:01:38  lr: 0.000006  min_lr: 0.000000  loss: 1.8819 (1.8474)  loss_scale: 32768.0000 (24710.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2753 (8.6390)  time: 0.8064 (0.5262 -- 3.0880)  data: 0.2568 (0.0004 -- 2.5611)  max mem: 16413
[2023-08-31 03:01:03,391] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21026
[2023-08-31 03:01:03,391] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21026
[2023-08-31 03:01:03,392] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:01:03,392] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:01:03,392] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [131]  [ 80/160]  eta: 0:01:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7177 (1.8420)  loss_scale: 8192.0000 (21137.3827)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6250 (8.7986)  time: 0.9415 (0.5286 -- 4.0041)  data: 0.3914 (0.0002 -- 3.4844)  max mem: 16413
Epoch: [131]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7416 (1.8385)  loss_scale: 8192.0000 (18573.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0312 (8.7724)  time: 0.7148 (0.5280 -- 3.5840)  data: 0.1636 (0.0003 -- 3.0749)  max mem: 16413
Epoch: [131]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.7522 (1.8238)  loss_scale: 8192.0000 (16857.9174)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2963 (8.7904)  time: 0.9294 (0.5331 -- 3.7619)  data: 0.3784 (0.0003 -- 3.1786)  max mem: 16413
Epoch: [131]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.6258 (1.8185)  loss_scale: 8192.0000 (15628.7092)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7809 (8.7476)  time: 0.8399 (0.5182 -- 3.6450)  data: 0.2917 (0.0004 -- 3.1304)  max mem: 16413
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6885 (1.7996)  loss_scale: 8192.0000 (14745.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3794 (8.8624)  time: 0.6854 (0.4958 -- 2.4052)  data: 0.1575 (0.0002 -- 1.8455)  max mem: 16413
Epoch: [131] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6885 (1.8175)  loss_scale: 8192.0000 (14745.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3794 (8.8624)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3751 (0.3751)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4369 (2.4369 -- 2.4369)  data: 2.2423 (2.2423 -- 2.2423)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5060 (0.7803)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4401 (0.2014 -- 2.4369)  data: 0.2229 (0.0008 -- 2.2423)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5488 (0.7141)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2207 (0.1695 -- 0.4346)  data: 0.0163 (0.0001 -- 0.1971)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6358 (0.7700)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.8506)  time: 0.2037 (0.1331 -- 0.4346)  data: 0.0158 (0.0001 -- 0.1971)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 79.668 Acc@5 96.266 loss 0.730
Accuracy of the network on the 482 val images: 79.67%
Max accuracy: 80.29%
Epoch: [132]  [  0/160]  eta: 0:16:48  lr: 0.000006  min_lr: 0.000000  loss: 1.7674 (1.7674)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7434 (10.7434)  time: 6.3052 (6.3052 -- 6.3052)  data: 4.9347 (4.9347 -- 4.9347)  max mem: 16413
Epoch: [132]  [ 20/160]  eta: 0:02:39  lr: 0.000006  min_lr: 0.000000  loss: 1.8227 (1.7807)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9813 (8.7732)  time: 0.8841 (0.5380 -- 1.9409)  data: 0.2779 (0.0008 -- 1.4006)  max mem: 16413
[2023-08-31 03:03:07,791] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:03:07,791] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:03:07,791] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:03:07,791] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [132]  [ 40/160]  eta: 0:02:03  lr: 0.000006  min_lr: 0.000000  loss: 1.8983 (1.8436)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8236 (8.3147)  time: 0.9074 (0.5270 -- 2.8880)  data: 0.3233 (0.0004 -- 2.3418)  max mem: 16413
[2023-08-31 03:03:22,835] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21171
[2023-08-31 03:03:22,835] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21171
[2023-08-31 03:03:22,835] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:03:22,835] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:03:22,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [132]  [ 60/160]  eta: 0:01:41  lr: 0.000006  min_lr: 0.000000  loss: 1.7606 (1.8262)  loss_scale: 8192.0000 (10340.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6714 (8.2873)  time: 0.9814 (0.5238 -- 3.7024)  data: 0.4346 (0.0005 -- 3.1927)  max mem: 16413
Epoch: [132]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000000  loss: 1.9441 (1.8529)  loss_scale: 8192.0000 (9810.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0364 (8.3815)  time: 0.6849 (0.5220 -- 1.8889)  data: 0.1367 (0.0002 -- 1.3672)  max mem: 16413
Epoch: [132]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 1.8064 (1.8359)  loss_scale: 8192.0000 (9489.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8244 (8.3625)  time: 0.9936 (0.5216 -- 2.6369)  data: 0.1059 (0.0003 -- 2.0251)  max mem: 16413
Epoch: [132]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.7388 (1.8290)  loss_scale: 8192.0000 (9275.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8220 (8.3464)  time: 0.9430 (0.5202 -- 3.3758)  data: 0.0015 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [132]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.9295 (1.8480)  loss_scale: 8192.0000 (9121.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7405 (8.3638)  time: 0.7737 (0.5278 -- 3.1281)  data: 0.0013 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8804 (1.8544)  loss_scale: 8192.0000 (9011.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2235 (8.3784)  time: 0.6620 (0.4947 -- 2.5870)  data: 0.0014 (0.0002 -- 0.0131)  max mem: 16413
Epoch: [132] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8804 (1.8241)  loss_scale: 8192.0000 (9011.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2235 (8.3784)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3541 (0.3541)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4722 (2.4722 -- 2.4722)  data: 2.2471 (2.2471 -- 2.2471)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4808 (0.7898)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4317 (0.2004 -- 2.4722)  data: 0.2098 (0.0006 -- 2.2471)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5272 (0.7116)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1696 -- 0.3177)  data: 0.0104 (0.0001 -- 0.1443)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6463 (0.7696)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.2030 (0.1331 -- 0.3177)  data: 0.0101 (0.0001 -- 0.1443)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 79.668 Acc@5 96.266 loss 0.728
Accuracy of the network on the 482 val images: 79.67%
Max accuracy: 80.29%
Epoch: [133]  [  0/160]  eta: 0:17:09  lr: 0.000006  min_lr: 0.000000  loss: 2.0198 (2.0198)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1775 (11.1775)  time: 6.4361 (6.4361 -- 6.4361)  data: 5.5274 (5.5274 -- 5.5274)  max mem: 16413
[2023-08-31 03:05:23,527] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:05:23,527] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:05:23,528] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:05:23,529] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [133]  [ 20/160]  eta: 0:02:45  lr: 0.000006  min_lr: 0.000000  loss: 1.6708 (1.6727)  loss_scale: 8192.0000 (8582.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9699 (8.4839)  time: 0.9179 (0.5274 -- 3.4018)  data: 0.2207 (0.0009 -- 1.3917)  max mem: 16413
Epoch: [133]  [ 40/160]  eta: 0:02:06  lr: 0.000006  min_lr: 0.000000  loss: 1.6406 (1.6502)  loss_scale: 16384.0000 (12387.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8676 (8.1108)  time: 0.9143 (0.5208 -- 2.1328)  data: 0.1456 (0.0003 -- 1.6159)  max mem: 16413
Epoch: [133]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000000  loss: 2.0141 (1.7533)  loss_scale: 16384.0000 (13698.0984)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0448 (8.2470)  time: 0.8172 (0.5246 -- 2.8333)  data: 0.1174 (0.0003 -- 1.2362)  max mem: 16413
Epoch: [133]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 2.0118 (1.8106)  loss_scale: 16384.0000 (14361.2840)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6234 (8.3469)  time: 0.9213 (0.5193 -- 3.4717)  data: 0.0010 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [133]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7576 (1.8099)  loss_scale: 16384.0000 (14761.8218)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4787 (8.2421)  time: 0.8189 (0.5237 -- 3.4133)  data: 0.0013 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [133]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.8917 (1.8195)  loss_scale: 16384.0000 (15029.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2080 (8.3808)  time: 0.8111 (0.5294 -- 2.3406)  data: 0.0013 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [133]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7932 (1.8098)  loss_scale: 16384.0000 (15222.0142)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7121 (8.3593)  time: 0.8962 (0.5271 -- 3.7565)  data: 0.0018 (0.0002 -- 0.0051)  max mem: 16413
[2023-08-31 03:07:14,156] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:07:14,156] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:07:14,156] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:07:14,157] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:07:16,712] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21433
[2023-08-31 03:07:16,712] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:07:16,712] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21433
[2023-08-31 03:07:16,712] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:07:16,712] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8171 (1.8042)  loss_scale: 16384.0000 (15872.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6743 (8.3685)  time: 0.6973 (0.4966 -- 1.9051)  data: 0.0009 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [133] Total time: 0:02:21 (0.8844 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8171 (1.8275)  loss_scale: 16384.0000 (15872.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6743 (8.3685)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3563 (0.3563)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5617 (2.5617 -- 2.5617)  data: 2.3085 (2.3085 -- 2.3085)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4833 (0.7791)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4435 (0.1912 -- 2.5617)  data: 0.2281 (0.0006 -- 2.3085)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5337 (0.7044)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2197 (0.1695 -- 0.4126)  data: 0.0192 (0.0001 -- 0.1931)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6320 (0.7608)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.8506)  time: 0.2064 (0.1329 -- 0.4126)  data: 0.0189 (0.0001 -- 0.1931)  max mem: 16413
Val: Total time: 0:00:07 (0.2941 s / it)
* Acc@1 80.498 Acc@5 96.266 loss 0.717
Accuracy of the network on the 482 val images: 80.50%
[2023-08-31 03:07:28,190] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 03:07:28,192] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 03:07:28,192] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 03:07:28,192] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 03:07:29,672] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 03:07:29,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.50%
Epoch: [134]  [  0/160]  eta: 0:20:45  lr: 0.000006  min_lr: 0.000000  loss: 2.3504 (2.3504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5515 (12.5515)  time: 7.7822 (7.7822 -- 7.7822)  data: 5.4958 (5.4958 -- 5.4958)  max mem: 16413
Epoch: [134]  [ 20/160]  eta: 0:02:39  lr: 0.000006  min_lr: 0.000000  loss: 1.8865 (1.8697)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8980 (9.2346)  time: 0.8098 (0.5294 -- 2.6820)  data: 0.1685 (0.0005 -- 1.6560)  max mem: 16413
Epoch: [134]  [ 40/160]  eta: 0:02:01  lr: 0.000006  min_lr: 0.000000  loss: 1.8313 (1.8306)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6587 (8.5841)  time: 0.8818 (0.5252 -- 2.2398)  data: 0.0681 (0.0003 -- 0.5432)  max mem: 16413
[2023-08-31 03:08:30,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21500
[2023-08-31 03:08:30,359] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:08:30,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21500
[2023-08-31 03:08:30,360] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:08:30,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [134]  [ 60/160]  eta: 0:01:39  lr: 0.000006  min_lr: 0.000000  loss: 1.7641 (1.8155)  loss_scale: 16384.0000 (16249.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5419 (8.4375)  time: 0.9523 (0.5087 -- 3.8448)  data: 0.0011 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [134]  [ 80/160]  eta: 0:01:15  lr: 0.000006  min_lr: 0.000000  loss: 1.8345 (1.8051)  loss_scale: 8192.0000 (14260.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5122 (8.4769)  time: 0.8130 (0.5249 -- 3.4554)  data: 0.0013 (0.0005 -- 0.0049)  max mem: 16413
Epoch: [134]  [100/160]  eta: 0:00:57  lr: 0.000006  min_lr: 0.000000  loss: 1.7669 (1.7960)  loss_scale: 8192.0000 (13058.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6907 (8.7269)  time: 0.9664 (0.5354 -- 3.1015)  data: 0.0014 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [134]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.7849 (1.7760)  loss_scale: 8192.0000 (12254.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7816 (8.8873)  time: 0.7300 (0.5278 -- 2.5417)  data: 0.0015 (0.0005 -- 0.0052)  max mem: 16413
Epoch: [134]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.9667 (1.8034)  loss_scale: 8192.0000 (11677.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1063 (8.7530)  time: 0.9834 (0.5135 -- 4.3408)  data: 0.0011 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7070 (1.7957)  loss_scale: 8192.0000 (11264.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3007 (8.7611)  time: 0.6366 (0.4967 -- 2.3440)  data: 0.0006 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [134] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7070 (1.7911)  loss_scale: 8192.0000 (11264.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3007 (8.7611)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3430 (0.3430)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5128 (2.5128 -- 2.5128)  data: 2.3033 (2.3033 -- 2.3033)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4894 (0.7767)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4472 (0.1946 -- 2.5128)  data: 0.2309 (0.0005 -- 2.3033)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5328 (0.7076)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2185 (0.1685 -- 0.4751)  data: 0.0148 (0.0001 -- 0.2281)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6326 (0.7643)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.4357)  time: 0.2025 (0.1325 -- 0.4751)  data: 0.0145 (0.0001 -- 0.2281)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 80.290 Acc@5 96.266 loss 0.720
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 80.50%
Epoch: [135]  [  0/160]  eta: 0:20:07  lr: 0.000006  min_lr: 0.000000  loss: 1.4628 (1.4628)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7942 (5.7942)  time: 7.5464 (7.5464 -- 7.5464)  data: 7.0085 (7.0085 -- 7.0085)  max mem: 16413
Epoch: [135]  [ 20/160]  eta: 0:02:42  lr: 0.000006  min_lr: 0.000000  loss: 1.7782 (1.8163)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5258 (9.5912)  time: 0.8425 (0.5289 -- 3.6749)  data: 0.2960 (0.0005 -- 3.1472)  max mem: 16413
[2023-08-31 03:10:33,325] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:10:33,325] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:10:33,326] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:10:33,326] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:10:35,547] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21633
[2023-08-31 03:10:35,547] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21633
[2023-08-31 03:10:35,548] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:10:35,548] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:10:35,548] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [135]  [ 40/160]  eta: 0:02:03  lr: 0.000006  min_lr: 0.000000  loss: 1.7664 (1.8217)  loss_scale: 8192.0000 (8991.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1559 (9.0808)  time: 0.8915 (0.5309 -- 3.7712)  data: 0.3381 (0.0002 -- 3.2230)  max mem: 16413
Epoch: [135]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000000  loss: 1.7953 (1.8124)  loss_scale: 8192.0000 (8729.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6079 (8.9182)  time: 0.8708 (0.5269 -- 3.2027)  data: 0.3241 (0.0004 -- 2.6615)  max mem: 16413
Epoch: [135]  [ 80/160]  eta: 0:01:15  lr: 0.000006  min_lr: 0.000000  loss: 1.9580 (1.8265)  loss_scale: 8192.0000 (8596.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7448 (8.8265)  time: 0.8159 (0.5306 -- 3.6480)  data: 0.0173 (0.0004 -- 0.3176)  max mem: 16413
Epoch: [135]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7000 (1.8189)  loss_scale: 8192.0000 (8516.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7171 (8.7718)  time: 0.8877 (0.5321 -- 3.3493)  data: 0.0521 (0.0005 -- 0.5906)  max mem: 16413
Epoch: [135]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.8719 (1.8250)  loss_scale: 8192.0000 (8462.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6929 (8.7320)  time: 0.8243 (0.5317 -- 2.5961)  data: 0.1340 (0.0004 -- 2.0495)  max mem: 16413
Epoch: [135]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.6276 (1.8058)  loss_scale: 8192.0000 (8424.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9352 (8.7180)  time: 0.8663 (0.5387 -- 2.5684)  data: 0.2586 (0.0003 -- 2.0445)  max mem: 16413
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8456 (1.8150)  loss_scale: 8192.0000 (8396.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6076 (8.5872)  time: 0.6781 (0.4957 -- 2.2576)  data: 0.0868 (0.0002 -- 1.7204)  max mem: 16413
Epoch: [135] Total time: 0:02:20 (0.8786 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8456 (1.8229)  loss_scale: 8192.0000 (8396.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6076 (8.5872)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.3511 (0.3511)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6460 (2.6460 -- 2.6460)  data: 2.4006 (2.4006 -- 2.4006)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4863 (0.7739)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4453 (0.1977 -- 2.6460)  data: 0.2302 (0.0006 -- 2.4006)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5442 (0.7076)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (95.7672)  time: 0.2102 (0.1706 -- 0.3324)  data: 0.0080 (0.0001 -- 0.1169)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6303 (0.7640)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.0207)  time: 0.1955 (0.1323 -- 0.3324)  data: 0.0077 (0.0001 -- 0.1169)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 79.876 Acc@5 96.058 loss 0.719
Accuracy of the network on the 482 val images: 79.88%
Max accuracy: 80.50%
Epoch: [136]  [  0/160]  eta: 0:20:28  lr: 0.000006  min_lr: 0.000000  loss: 2.2822 (2.2822)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4945 (11.4945)  time: 7.6795 (7.6795 -- 7.6795)  data: 7.1394 (7.1394 -- 7.1394)  max mem: 16413
[2023-08-31 03:12:37,559] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:12:37,559] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:12:37,559] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:12:37,560] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [136]  [ 20/160]  eta: 0:02:38  lr: 0.000006  min_lr: 0.000000  loss: 1.8350 (1.8481)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3821 (9.0323)  time: 0.8066 (0.5406 -- 2.1041)  data: 0.1568 (0.0005 -- 1.4704)  max mem: 16413
Epoch: [136]  [ 40/160]  eta: 0:01:59  lr: 0.000006  min_lr: 0.000000  loss: 1.7788 (1.8450)  loss_scale: 16384.0000 (15984.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1995 (8.4932)  time: 0.8492 (0.5259 -- 3.6541)  data: 0.1386 (0.0008 -- 0.8450)  max mem: 16413
Epoch: [136]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000000  loss: 1.9788 (1.8826)  loss_scale: 16384.0000 (16115.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5101 (8.3981)  time: 0.9241 (0.5235 -- 2.5328)  data: 0.2239 (0.0002 -- 1.9683)  max mem: 16413
Epoch: [136]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 1.6658 (1.8409)  loss_scale: 16384.0000 (16181.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1479 (8.3347)  time: 0.9340 (0.5299 -- 4.2226)  data: 0.1875 (0.0008 -- 3.6961)  max mem: 16413
Epoch: [136]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7750 (1.8129)  loss_scale: 16384.0000 (16221.7822)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2873 (8.3831)  time: 0.7832 (0.5253 -- 3.2114)  data: 0.2260 (0.0004 -- 2.6025)  max mem: 16413
Epoch: [136]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.9273 (1.8383)  loss_scale: 16384.0000 (16248.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5213 (8.5979)  time: 0.9363 (0.5349 -- 4.3189)  data: 0.3797 (0.0004 -- 3.7856)  max mem: 16413
[2023-08-31 03:14:27,803] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:14:27,803] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:14:27,803] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:14:27,803] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [136]  [140/160]  eta: 0:00:17  lr: 0.000006  min_lr: 0.000000  loss: 1.7272 (1.8246)  loss_scale: 32768.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6874 (8.6415)  time: 0.6728 (0.5435 -- 1.7410)  data: 0.1119 (0.0002 -- 1.1911)  max mem: 16413
[2023-08-31 03:14:45,630] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21914
[2023-08-31 03:14:45,630] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:14:45,630] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21914
[2023-08-31 03:14:45,630] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:14:45,630] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8162 (1.8287)  loss_scale: 32768.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8042 (8.6600)  time: 0.7030 (0.4931 -- 3.2277)  data: 0.0905 (0.0002 -- 1.0934)  max mem: 16413
Epoch: [136] Total time: 0:02:19 (0.8709 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8162 (1.8259)  loss_scale: 32768.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8042 (8.6600)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3497 (0.3497)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2152 (2.2152 -- 2.2152)  data: 2.0042 (2.0042 -- 2.0042)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4984 (0.7771)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4265 (0.1872 -- 2.2152)  data: 0.2105 (0.0006 -- 2.0042)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5288 (0.7078)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2296 (0.1698 -- 0.5368)  data: 0.0221 (0.0001 -- 0.2991)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6271 (0.7655)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.2163 (0.1329 -- 0.5368)  data: 0.0218 (0.0001 -- 0.2991)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 80.913 Acc@5 96.266 loss 0.720
Accuracy of the network on the 482 val images: 80.91%
[2023-08-31 03:14:56,019] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 03:14:56,021] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 03:14:56,021] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 03:14:56,021] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 03:14:57,497] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 03:14:57,497] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.91%
Epoch: [137]  [  0/160]  eta: 0:20:36  lr: 0.000006  min_lr: 0.000000  loss: 2.2000 (2.2000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9093 (9.9093)  time: 7.7305 (7.7305 -- 7.7305)  data: 7.1904 (7.1904 -- 7.1904)  max mem: 16413
Epoch: [137]  [ 20/160]  eta: 0:02:44  lr: 0.000006  min_lr: 0.000000  loss: 1.7728 (1.8873)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2009 (9.4061)  time: 0.8485 (0.5255 -- 3.6086)  data: 0.2980 (0.0003 -- 3.0721)  max mem: 16413
Epoch: [137]  [ 40/160]  eta: 0:02:07  lr: 0.000006  min_lr: 0.000000  loss: 1.7681 (1.8741)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9466 (8.7130)  time: 0.9467 (0.5272 -- 3.0625)  data: 0.3524 (0.0004 -- 2.5433)  max mem: 16413
Epoch: [137]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000000  loss: 1.8038 (1.8687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4418 (8.7765)  time: 0.8025 (0.5338 -- 3.2469)  data: 0.1562 (0.0003 -- 2.7114)  max mem: 16413
[2023-08-31 03:15:58,849] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21983
[2023-08-31 03:15:58,850] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:15:58,850] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21983
[2023-08-31 03:15:58,850] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:15:58,850] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 03:16:13,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=128, lr=[1.304852994725238e-07, 1.304852994725238e-07, 1.7398039929669843e-07, 1.7398039929669843e-07, 2.3197386572893125e-07, 2.3197386572893125e-07, 3.09298487638575e-07, 3.09298487638575e-07, 4.123979835181e-07, 4.123979835181e-07, 5.498639780241332e-07, 5.498639780241332e-07, 7.331519706988444e-07, 7.331519706988444e-07, 9.775359609317925e-07, 9.775359609317925e-07, 1.30338128124239e-06, 1.30338128124239e-06, 1.7378417083231867e-06, 1.7378417083231867e-06, 2.317122277764249e-06, 2.317122277764249e-06, 3.089496370352332e-06, 3.089496370352332e-06, 4.11932849380311e-06, 4.11932849380311e-06, 5.492437991737479e-06, 5.492437991737479e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 03:16:13,276] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=17.18953589319277, CurrSamplesPerSec=21.717404248929483, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [137]  [ 80/160]  eta: 0:01:17  lr: 0.000005  min_lr: 0.000000  loss: 1.7243 (1.8462)  loss_scale: 8192.0000 (14563.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7528 (8.4126)  time: 0.9435 (0.5208 -- 2.7930)  data: 0.2546 (0.0002 -- 2.2713)  max mem: 16413
Epoch: [137]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.9361 (1.8604)  loss_scale: 8192.0000 (13301.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8978 (8.5640)  time: 0.8492 (0.5308 -- 2.4628)  data: 0.1042 (0.0003 -- 1.2549)  max mem: 16413
Epoch: [137]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.9138 (1.8581)  loss_scale: 8192.0000 (12457.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6532 (8.6056)  time: 0.8321 (0.5365 -- 3.2187)  data: 0.0472 (0.0002 -- 0.9215)  max mem: 16413
Epoch: [137]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8472 (1.8484)  loss_scale: 8192.0000 (11852.2553)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7499 (8.5126)  time: 0.8859 (0.5138 -- 4.2707)  data: 0.0012 (0.0003 -- 0.0019)  max mem: 16413
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.9862 (1.8595)  loss_scale: 8192.0000 (11417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2378 (8.5152)  time: 0.6311 (0.4946 -- 2.7184)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [137] Total time: 0:02:21 (0.8874 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.9862 (1.8276)  loss_scale: 8192.0000 (11417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2378 (8.5152)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3721 (0.3721)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4549 (2.4549 -- 2.4549)  data: 2.1864 (2.1864 -- 2.1864)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4884 (0.7818)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4333 (0.1994 -- 2.4549)  data: 0.2116 (0.0010 -- 2.1864)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5483 (0.7107)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2224 (0.1711 -- 0.3896)  data: 0.0176 (0.0001 -- 0.2081)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6237 (0.7703)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.2080 (0.1326 -- 0.3896)  data: 0.0172 (0.0001 -- 0.2081)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 80.498 Acc@5 96.266 loss 0.724
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 80.91%
Epoch: [138]  [  0/160]  eta: 0:21:55  lr: 0.000005  min_lr: 0.000000  loss: 2.0283 (2.0283)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1887 (8.1887)  time: 8.2197 (8.2197 -- 8.2197)  data: 7.6540 (7.6540 -- 7.6540)  max mem: 16413
Epoch: [138]  [ 20/160]  eta: 0:02:46  lr: 0.000005  min_lr: 0.000000  loss: 1.6995 (1.7832)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0241 (8.7096)  time: 0.8373 (0.5210 -- 3.7753)  data: 0.2917 (0.0006 -- 3.2499)  max mem: 16413
[2023-08-31 03:18:03,336] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:18:03,337] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:18:03,337] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:18:03,337] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [138]  [ 40/160]  eta: 0:02:03  lr: 0.000005  min_lr: 0.000000  loss: 1.7417 (1.7355)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1913 (8.4238)  time: 0.8687 (0.5184 -- 3.3390)  data: 0.2725 (0.0003 -- 2.7811)  max mem: 16413
Epoch: [138]  [ 60/160]  eta: 0:01:38  lr: 0.000005  min_lr: 0.000000  loss: 1.5616 (1.7224)  loss_scale: 16384.0000 (12086.5574)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8467 (8.8775)  time: 0.8932 (0.5157 -- 3.2331)  data: 0.1995 (0.0006 -- 1.4236)  max mem: 16413
Epoch: [138]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.6720 (1.7255)  loss_scale: 16384.0000 (13147.6543)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6067 (8.8228)  time: 0.8358 (0.5174 -- 3.5638)  data: 0.2948 (0.0004 -- 3.0502)  max mem: 16413
Epoch: [138]  [100/160]  eta: 0:00:54  lr: 0.000005  min_lr: 0.000000  loss: 1.9106 (1.7624)  loss_scale: 16384.0000 (13788.5149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7656 (8.9635)  time: 0.7682 (0.5227 -- 1.6089)  data: 0.1682 (0.0003 -- 1.0954)  max mem: 16413
Epoch: [138]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.7764 (1.7724)  loss_scale: 16384.0000 (14217.5207)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1061 (9.0462)  time: 0.8418 (0.5241 -- 2.0470)  data: 0.0953 (0.0003 -- 1.0766)  max mem: 16413
Epoch: [138]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6394 (1.7574)  loss_scale: 16384.0000 (14524.8227)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1045 (9.0254)  time: 0.9380 (0.5259 -- 2.1359)  data: 0.2680 (0.0004 -- 1.5346)  max mem: 16413
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8392 (1.7544)  loss_scale: 16384.0000 (14745.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7261 (9.0010)  time: 0.6502 (0.4966 -- 2.1315)  data: 0.0179 (0.0002 -- 0.3312)  max mem: 16413
Epoch: [138] Total time: 0:02:20 (0.8771 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8392 (1.7980)  loss_scale: 16384.0000 (14745.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7261 (9.0010)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3560 (0.3560)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4585 (2.4585 -- 2.4585)  data: 2.2410 (2.2410 -- 2.2410)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4864 (0.7793)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4456 (0.1922 -- 2.4585)  data: 0.2335 (0.0007 -- 2.2410)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5365 (0.7067)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2244 (0.1697 -- 0.5287)  data: 0.0214 (0.0001 -- 0.3172)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6212 (0.7663)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2109 (0.1324 -- 0.5287)  data: 0.0211 (0.0001 -- 0.3172)  max mem: 16413
Val: Total time: 0:00:07 (0.2938 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.720
Accuracy of the network on the 482 val images: 81.54%
[2023-08-31 03:19:55,696] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 03:19:55,698] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 03:19:55,698] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 03:19:55,698] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 03:19:56,978] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 03:19:56,978] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.54%
[2023-08-31 03:20:04,374] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:20:04,374] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:20:04,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:20:04,377] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [139]  [  0/160]  eta: 0:19:46  lr: 0.000005  min_lr: 0.000000  loss: 1.4042 (1.4042)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5241 (11.5241)  time: 7.4130 (7.4130 -- 7.4130)  data: 5.3830 (5.3830 -- 5.3830)  max mem: 16413
[2023-08-31 03:20:06,717] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22242
[2023-08-31 03:20:06,717] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:20:06,717] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22242
[2023-08-31 03:20:06,717] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:20:06,717] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 03:20:21,114] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22260
[2023-08-31 03:20:21,114] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22260
[2023-08-31 03:20:21,114] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:20:21,114] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:20:21,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [139]  [ 20/160]  eta: 0:02:40  lr: 0.000005  min_lr: 0.000000  loss: 1.8129 (1.7465)  loss_scale: 16384.0000 (17554.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9171 (8.9428)  time: 0.8360 (0.5195 -- 2.8846)  data: 0.0016 (0.0005 -- 0.0036)  max mem: 16413
Epoch: [139]  [ 40/160]  eta: 0:02:03  lr: 0.000005  min_lr: 0.000000  loss: 2.0317 (1.8422)  loss_scale: 8192.0000 (12987.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7856 (9.1479)  time: 0.8983 (0.5227 -- 3.0233)  data: 0.1147 (0.0003 -- 1.1530)  max mem: 16413
Epoch: [139]  [ 60/160]  eta: 0:01:33  lr: 0.000005  min_lr: 0.000000  loss: 1.7225 (1.8264)  loss_scale: 8192.0000 (11415.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8497 (9.4565)  time: 0.7595 (0.5300 -- 1.8811)  data: 0.1259 (0.0004 -- 1.3550)  max mem: 16413
Epoch: [139]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.6207 (1.7999)  loss_scale: 8192.0000 (10619.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2086 (9.0564)  time: 0.9605 (0.5408 -- 3.3503)  data: 0.3585 (0.0006 -- 2.7992)  max mem: 16413
Epoch: [139]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.9148 (1.8254)  loss_scale: 8192.0000 (10138.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2253 (8.9250)  time: 0.8068 (0.5221 -- 3.0868)  data: 0.2497 (0.0002 -- 2.5515)  max mem: 16413
Epoch: [139]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.8251 (1.8253)  loss_scale: 8192.0000 (9816.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6560 (8.9848)  time: 0.8735 (0.5168 -- 3.6422)  data: 0.1023 (0.0004 -- 1.9312)  max mem: 16413
Epoch: [139]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8652 (1.8270)  loss_scale: 8192.0000 (9586.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0860 (9.1095)  time: 0.9992 (0.5156 -- 3.8537)  data: 0.0012 (0.0003 -- 0.0041)  max mem: 16413
[2023-08-31 03:22:12,814] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:22:12,814] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:22:12,816] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:22:12,816] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.9220 (1.8364)  loss_scale: 16384.0000 (9984.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3811 (9.0505)  time: 0.5799 (0.4944 -- 1.3261)  data: 0.0007 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [139] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.9220 (1.8529)  loss_scale: 16384.0000 (9984.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3811 (9.0505)
[2023-08-31 03:22:18,155] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-08-31 03:22:18,156] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-08-31 03:22:18,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-08-31 03:22:18,157] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-08-31 03:22:19,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-08-31 03:22:19,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:13  loss: 0.3455 (0.3455)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7353 (2.7353 -- 2.7353)  data: 2.4182 (2.4182 -- 2.4182)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4940 (0.7787)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4557 (0.2013 -- 2.7353)  data: 0.2353 (0.0007 -- 2.4182)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5302 (0.7040)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2112 (0.1694 -- 0.3918)  data: 0.0119 (0.0001 -- 0.1600)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6253 (0.7636)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.1987 (0.1327 -- 0.3918)  data: 0.0115 (0.0001 -- 0.1600)  max mem: 16413
Val: Total time: 0:00:07 (0.2944 s / it)
* Acc@1 81.328 Acc@5 96.266 loss 0.719
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.54%
Epoch: [140]  [  0/160]  eta: 0:17:07  lr: 0.000005  min_lr: 0.000000  loss: 2.0058 (2.0058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.6985 (15.6985)  time: 6.4249 (6.4249 -- 6.4249)  data: 5.6877 (5.6877 -- 5.6877)  max mem: 16413
Epoch: [140]  [ 20/160]  eta: 0:02:54  lr: 0.000005  min_lr: 0.000000  loss: 1.9726 (1.9049)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8932 (8.4901)  time: 0.9876 (0.5271 -- 3.7541)  data: 0.0486 (0.0003 -- 0.9457)  max mem: 16413
Epoch: [140]  [ 40/160]  eta: 0:02:06  lr: 0.000005  min_lr: 0.000000  loss: 1.8403 (1.8747)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6001 (8.6016)  time: 0.8552 (0.5142 -- 4.4584)  data: 0.0016 (0.0003 -- 0.0062)  max mem: 16413
Epoch: [140]  [ 60/160]  eta: 0:01:41  lr: 0.000005  min_lr: 0.000000  loss: 1.9878 (1.9069)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4358 (8.7128)  time: 0.9445 (0.5145 -- 3.7437)  data: 0.0020 (0.0003 -- 0.0138)  max mem: 16413
Epoch: [140]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 1.6578 (1.8651)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1709 (8.8777)  time: 0.7778 (0.5229 -- 2.7858)  data: 0.0020 (0.0002 -- 0.0139)  max mem: 16413
Epoch: [140]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.7619 (1.8466)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7331 (8.9458)  time: 0.8464 (0.5183 -- 4.1131)  data: 0.0014 (0.0003 -- 0.0048)  max mem: 16413
[2023-08-31 03:24:17,176] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:24:17,176] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:24:17,176] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:24:17,176] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [140]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 2.1018 (1.8857)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7847 (9.0217)  time: 0.8566 (0.5335 -- 3.6978)  data: 0.0018 (0.0005 -- 0.0056)  max mem: 16413
[2023-08-31 03:24:22,738] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22524
[2023-08-31 03:24:22,738] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22524
[2023-08-31 03:24:22,738] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:24:22,738] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:24:22,738] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [140]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8172 (1.8789)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7739 (9.0215)  time: 0.8252 (0.5183 -- 3.5210)  data: 0.0015 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8280 (1.8800)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5604 (8.9707)  time: 0.7165 (0.4963 -- 2.2532)  data: 0.0382 (0.0002 -- 0.7533)  max mem: 16413
Epoch: [140] Total time: 0:02:21 (0.8856 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8280 (1.8629)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5604 (8.9707)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3526 (0.3526)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5638 (2.5638 -- 2.5638)  data: 2.3343 (2.3343 -- 2.3343)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4645 (0.7741)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4448 (0.2021 -- 2.5638)  data: 0.2347 (0.0006 -- 2.3343)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5476 (0.6997)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2125 (0.1692 -- 0.4460)  data: 0.0128 (0.0001 -- 0.2380)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6227 (0.7606)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.1988 (0.1328 -- 0.4460)  data: 0.0125 (0.0001 -- 0.2380)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 80.705 Acc@5 96.266 loss 0.718
Accuracy of the network on the 482 val images: 80.71%
Max accuracy: 81.54%
Epoch: [141]  [  0/160]  eta: 0:23:19  lr: 0.000005  min_lr: 0.000000  loss: 1.4568 (1.4568)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2535 (6.2535)  time: 8.7464 (8.7464 -- 8.7464)  data: 6.9423 (6.9423 -- 6.9423)  max mem: 16413
Epoch: [141]  [ 20/160]  eta: 0:02:44  lr: 0.000005  min_lr: 0.000000  loss: 1.8702 (1.8382)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4935 (8.3938)  time: 0.7956 (0.5218 -- 3.7008)  data: 0.1064 (0.0004 -- 1.2019)  max mem: 16413
Epoch: [141]  [ 40/160]  eta: 0:02:05  lr: 0.000005  min_lr: 0.000000  loss: 1.7741 (1.8194)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4100 (8.9378)  time: 0.9126 (0.5211 -- 4.0346)  data: 0.0013 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [141]  [ 60/160]  eta: 0:01:41  lr: 0.000005  min_lr: 0.000000  loss: 1.7948 (1.8110)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8761 (9.1826)  time: 0.9402 (0.5215 -- 2.4975)  data: 0.0019 (0.0005 -- 0.0125)  max mem: 16413
Epoch: [141]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.8237 (1.8253)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2884 (9.1328)  time: 0.7513 (0.5242 -- 2.6363)  data: 0.0020 (0.0005 -- 0.0052)  max mem: 16413
[2023-08-31 03:26:28,019] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:26:28,020] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:26:28,020] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:26:28,020] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [141]  [100/160]  eta: 0:00:57  lr: 0.000005  min_lr: 0.000000  loss: 1.8978 (1.8348)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9975 (8.9292)  time: 1.0339 (0.5171 -- 4.4746)  data: 0.0018 (0.0003 -- 0.0153)  max mem: 16413
[2023-08-31 03:26:48,909] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22678
[2023-08-31 03:26:48,909] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22678
[2023-08-31 03:26:48,910] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:26:48,910] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:26:48,910] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [141]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.6989 (1.8166)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1239 (8.8319)  time: 0.8041 (0.5225 -- 3.4467)  data: 0.0029 (0.0004 -- 0.0163)  max mem: 16413
Epoch: [141]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.7588 (1.7880)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5931 (8.9375)  time: 0.9006 (0.5285 -- 3.1956)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
[2023-08-31 03:27:19,177] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22719
[2023-08-31 03:27:19,177] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22719
[2023-08-31 03:27:19,177] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:27:19,177] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:27:19,177] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7638 (1.7831)  loss_scale: 16384.0000 (18892.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7094 (8.9343)  time: 0.6879 (0.4844 -- 2.6501)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [141] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7638 (1.8106)  loss_scale: 16384.0000 (18892.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7094 (8.9343)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3444 (0.3444)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5288 (2.5288 -- 2.5288)  data: 2.2912 (2.2912 -- 2.2912)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4873 (0.7813)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4362 (0.1991 -- 2.5288)  data: 0.2178 (0.0009 -- 2.2912)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5356 (0.7062)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2166 (0.1704 -- 0.3481)  data: 0.0126 (0.0001 -- 0.1451)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6191 (0.7675)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2011 (0.1325 -- 0.3481)  data: 0.0122 (0.0001 -- 0.1451)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 81.120 Acc@5 96.266 loss 0.724
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.54%
Epoch: [142]  [  0/160]  eta: 0:17:05  lr: 0.000005  min_lr: 0.000000  loss: 2.2058 (2.2058)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9726 (9.9726)  time: 6.4105 (6.4105 -- 6.4105)  data: 5.5328 (5.5328 -- 5.5328)  max mem: 16413
Epoch: [142]  [ 20/160]  eta: 0:02:45  lr: 0.000005  min_lr: 0.000000  loss: 1.7794 (1.8944)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9161 (8.2219)  time: 0.9210 (0.5463 -- 2.2764)  data: 0.0019 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [142]  [ 40/160]  eta: 0:01:58  lr: 0.000005  min_lr: 0.000000  loss: 1.9816 (1.9249)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9670 (8.2805)  time: 0.7863 (0.5297 -- 2.1809)  data: 0.0475 (0.0004 -- 0.9201)  max mem: 16413
Epoch: [142]  [ 60/160]  eta: 0:01:37  lr: 0.000005  min_lr: 0.000000  loss: 1.8446 (1.8926)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4809 (8.3519)  time: 0.9361 (0.5095 -- 4.0184)  data: 0.0336 (0.0004 -- 0.4698)  max mem: 16413
Epoch: [142]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 2.0067 (1.9072)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3084 (8.4940)  time: 0.8903 (0.5161 -- 3.1995)  data: 0.0020 (0.0002 -- 0.0136)  max mem: 16413
Epoch: [142]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.8704 (1.9012)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3635 (8.5224)  time: 0.7840 (0.5238 -- 2.1365)  data: 0.0013 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [142]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.6751 (1.8646)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1396 (8.7287)  time: 0.8866 (0.5218 -- 2.9843)  data: 0.0018 (0.0003 -- 0.0042)  max mem: 16413
[2023-08-31 03:29:25,385] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:29:25,386] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:29:25,387] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:29:25,388] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [142]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8811 (1.8681)  loss_scale: 16384.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2112 (8.8035)  time: 0.8659 (0.5442 -- 3.5749)  data: 0.0212 (0.0002 -- 0.3827)  max mem: 16413
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.6676 (1.8498)  loss_scale: 16384.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2112 (8.9449)  time: 0.7623 (0.4955 -- 2.2522)  data: 0.0009 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [142] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.6676 (1.8392)  loss_scale: 16384.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2112 (8.9449)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3622 (0.3622)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3050 (2.3050 -- 2.3050)  data: 2.0633 (2.0633 -- 2.0633)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4979 (0.7801)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4301 (0.1975 -- 2.3050)  data: 0.2129 (0.0007 -- 2.0633)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5445 (0.7063)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2244 (0.1686 -- 0.5149)  data: 0.0202 (0.0001 -- 0.2682)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6174 (0.7645)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.2103 (0.1331 -- 0.5149)  data: 0.0199 (0.0001 -- 0.2682)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 80.913 Acc@5 96.473 loss 0.721
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 81.54%
Epoch: [143]  [  0/160]  eta: 0:23:10  lr: 0.000005  min_lr: 0.000000  loss: 1.6245 (1.6245)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5977 (8.5977)  time: 8.6928 (8.6928 -- 8.6928)  data: 5.8082 (5.8082 -- 5.8082)  max mem: 16413
Epoch: [143]  [ 20/160]  eta: 0:02:44  lr: 0.000005  min_lr: 0.000000  loss: 1.7627 (1.7999)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8731 (8.4835)  time: 0.7972 (0.5287 -- 3.0633)  data: 0.0456 (0.0004 -- 0.8757)  max mem: 16413
Epoch: [143]  [ 40/160]  eta: 0:02:03  lr: 0.000005  min_lr: 0.000000  loss: 1.9555 (1.8440)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9713 (8.3365)  time: 0.8732 (0.5254 -- 3.2749)  data: 0.0025 (0.0007 -- 0.0127)  max mem: 16413
Epoch: [143]  [ 60/160]  eta: 0:01:36  lr: 0.000005  min_lr: 0.000000  loss: 1.6867 (1.7800)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5997 (8.2654)  time: 0.8523 (0.5283 -- 3.7137)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [143]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.8139 (1.8026)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1470 (8.2496)  time: 0.8796 (0.5249 -- 2.3588)  data: 0.0019 (0.0002 -- 0.0061)  max mem: 16413
[2023-08-31 03:31:27,577] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:31:27,577] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:31:27,577] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:31:27,577] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [143]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.7980 (1.8164)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9089 (8.3511)  time: 0.8276 (0.5191 -- 2.1622)  data: 0.1271 (0.0003 -- 1.5412)  max mem: 16413
[2023-08-31 03:31:37,264] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22988
[2023-08-31 03:31:37,264] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22988
[2023-08-31 03:31:37,264] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:31:37,264] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:31:37,264] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 03:31:47,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=134, lr=[1.0762978070533814e-07, 1.0762978070533814e-07, 1.4350637427378418e-07, 1.4350637427378418e-07, 1.9134183236504557e-07, 1.9134183236504557e-07, 2.551224431533941e-07, 2.551224431533941e-07, 3.401632575378588e-07, 3.401632575378588e-07, 4.535510100504784e-07, 4.535510100504784e-07, 6.047346800673045e-07, 6.047346800673045e-07, 8.063129067564061e-07, 8.063129067564061e-07, 1.075083875675208e-06, 1.075083875675208e-06, 1.433445167566944e-06, 1.433445167566944e-06, 1.911260223422592e-06, 1.911260223422592e-06, 2.548346964563456e-06, 2.548346964563456e-06, 3.3977959527512746e-06, 3.3977959527512746e-06, 4.5303946036683665e-06, 4.5303946036683665e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 03:31:47,404] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=17.11729645387462, CurrSamplesPerSec=4.1303832218543866, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [143]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.8294 (1.8200)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1892 (8.4365)  time: 0.8602 (0.5232 -- 2.9222)  data: 0.1033 (0.0009 -- 1.4047)  max mem: 16413
Epoch: [143]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.9768 (1.8382)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4756 (8.5973)  time: 0.8377 (0.5184 -- 2.3768)  data: 0.0184 (0.0004 -- 0.3362)  max mem: 16413
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7957 (1.8385)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6262 (8.5719)  time: 0.6979 (0.4944 -- 1.7433)  data: 0.0217 (0.0001 -- 0.4162)  max mem: 16413
Epoch: [143] Total time: 0:02:20 (0.8800 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7957 (1.8338)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6262 (8.5719)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3763 (0.3763)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4467 (2.4467 -- 2.4467)  data: 2.2043 (2.2043 -- 2.2043)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4723 (0.7747)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4175 (0.1975 -- 2.4467)  data: 0.2021 (0.0008 -- 2.2043)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5623 (0.6997)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2224 (0.1694 -- 0.5699)  data: 0.0194 (0.0001 -- 0.3673)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6170 (0.7561)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.4357)  time: 0.2090 (0.1323 -- 0.5699)  data: 0.0190 (0.0001 -- 0.3673)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 81.328 Acc@5 96.266 loss 0.714
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.54%
Epoch: [144]  [  0/160]  eta: 0:21:15  lr: 0.000004  min_lr: 0.000000  loss: 0.9830 (0.9830)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7976 (8.7976)  time: 7.9702 (7.9702 -- 7.9702)  data: 7.3160 (7.3160 -- 7.3160)  max mem: 16413
Epoch: [144]  [ 20/160]  eta: 0:02:38  lr: 0.000004  min_lr: 0.000000  loss: 1.7909 (1.8048)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4918 (9.0720)  time: 0.7929 (0.5255 -- 3.7208)  data: 0.1243 (0.0005 -- 1.3394)  max mem: 16413
[2023-08-31 03:33:08,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23080
[2023-08-31 03:33:08,318] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23080
[2023-08-31 03:33:08,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:33:08,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:33:08,319] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [144]  [ 40/160]  eta: 0:02:03  lr: 0.000004  min_lr: 0.000000  loss: 1.8719 (1.7825)  loss_scale: 16384.0000 (16184.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8873 (8.7413)  time: 0.9200 (0.5269 -- 3.6714)  data: 0.1826 (0.0008 -- 2.5013)  max mem: 16413
Epoch: [144]  [ 60/160]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000000  loss: 1.9534 (1.8376)  loss_scale: 8192.0000 (13563.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3521 (8.7909)  time: 0.9343 (0.5146 -- 4.0724)  data: 0.0991 (0.0004 -- 1.9579)  max mem: 16413
Epoch: [144]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.7519 (1.8001)  loss_scale: 8192.0000 (12237.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0726 (8.8964)  time: 0.8213 (0.5210 -- 3.8935)  data: 0.2781 (0.0005 -- 3.3775)  max mem: 16413
Epoch: [144]  [100/160]  eta: 0:00:57  lr: 0.000004  min_lr: 0.000000  loss: 1.8917 (1.8231)  loss_scale: 8192.0000 (11436.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6064 (8.8103)  time: 0.9400 (0.5352 -- 3.3807)  data: 0.3858 (0.0003 -- 2.8619)  max mem: 16413
[2023-08-31 03:34:17,279] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23157
[2023-08-31 03:34:17,279] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23157
[2023-08-31 03:34:17,279] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 03:34:17,279] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 03:34:17,279] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [144]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.8096 (1.8264)  loss_scale: 8192.0000 (10764.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3719 (8.7958)  time: 0.8315 (0.5168 -- 3.9038)  data: 0.2929 (0.0004 -- 3.3541)  max mem: 16413
Epoch: [144]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7133 (1.8081)  loss_scale: 4096.0000 (9818.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9322 (8.8086)  time: 0.9616 (0.5126 -- 3.7064)  data: 0.4173 (0.0002 -- 3.1781)  max mem: 16413
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6881 (1.7942)  loss_scale: 4096.0000 (9139.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1624 (8.7780)  time: 0.5445 (0.4977 -- 1.0419)  data: 0.0268 (0.0002 -- 0.5232)  max mem: 16413
Epoch: [144] Total time: 0:02:22 (0.8901 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6881 (1.7974)  loss_scale: 4096.0000 (9139.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1624 (8.7780)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3508 (0.3508)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2716 (2.2716 -- 2.2716)  data: 2.0527 (2.0527 -- 2.0527)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4871 (0.7759)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4294 (0.2043 -- 2.2716)  data: 0.2135 (0.0008 -- 2.0527)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5454 (0.7033)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2249 (0.1689 -- 0.4343)  data: 0.0183 (0.0001 -- 0.2351)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6169 (0.7601)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.2088 (0.1325 -- 0.4343)  data: 0.0174 (0.0001 -- 0.2351)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 81.120 Acc@5 96.473 loss 0.718
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.54%
Epoch: [145]  [  0/160]  eta: 0:19:49  lr: 0.000004  min_lr: 0.000000  loss: 1.5816 (1.5816)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6354 (6.6354)  time: 7.4327 (7.4327 -- 7.4327)  data: 5.6274 (5.6274 -- 5.6274)  max mem: 16413
Epoch: [145]  [ 20/160]  eta: 0:03:01  lr: 0.000004  min_lr: 0.000000  loss: 1.8568 (1.8405)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5840 (9.3896)  time: 0.9866 (0.5261 -- 3.6291)  data: 0.0848 (0.0007 -- 1.6618)  max mem: 16413
Epoch: [145]  [ 40/160]  eta: 0:02:08  lr: 0.000004  min_lr: 0.000000  loss: 1.8578 (1.8303)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4826 (9.1471)  time: 0.8353 (0.5209 -- 3.5919)  data: 0.0016 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [145]  [ 60/160]  eta: 0:01:42  lr: 0.000004  min_lr: 0.000000  loss: 1.7378 (1.8121)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3575 (8.8297)  time: 0.9455 (0.5196 -- 3.5062)  data: 0.0018 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [145]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.9764 (1.8434)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9783 (8.8459)  time: 0.7360 (0.5172 -- 3.1294)  data: 0.0014 (0.0003 -- 0.0031)  max mem: 16413
[2023-08-31 03:36:18,920] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:36:18,920] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:36:18,921] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-31 03:36:18,921] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [145]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.6492 (1.8073)  loss_scale: 8192.0000 (4704.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0291 (8.7332)  time: 0.8249 (0.5280 -- 2.4788)  data: 0.0023 (0.0003 -- 0.0131)  max mem: 16413
Epoch: [145]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.9199 (1.8256)  loss_scale: 8192.0000 (5280.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1208 (8.7813)  time: 0.9042 (0.5234 -- 3.1653)  data: 0.2507 (0.0007 -- 2.6423)  max mem: 16413
Epoch: [145]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8146 (1.8280)  loss_scale: 8192.0000 (5693.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4096 (8.7480)  time: 0.9551 (0.5168 -- 5.7358)  data: 0.3896 (0.0004 -- 5.2138)  max mem: 16413
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7600 (1.8178)  loss_scale: 8192.0000 (5990.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0349 (8.7763)  time: 0.6910 (0.4954 -- 2.1765)  data: 0.1712 (0.0002 -- 1.6343)  max mem: 16413
Epoch: [145] Total time: 0:02:24 (0.9031 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7600 (1.8279)  loss_scale: 8192.0000 (5990.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0349 (8.7763)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3771 (0.3771)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4636 (2.4636 -- 2.4636)  data: 2.2451 (2.2451 -- 2.2451)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4740 (0.7677)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4457 (0.2027 -- 2.4636)  data: 0.2314 (0.0005 -- 2.2451)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5560 (0.6960)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2232 (0.1700 -- 0.5157)  data: 0.0204 (0.0001 -- 0.2916)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6163 (0.7524)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2084 (0.1326 -- 0.5157)  data: 0.0201 (0.0001 -- 0.2916)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 81.535 Acc@5 96.888 loss 0.711
Accuracy of the network on the 482 val images: 81.54%
[2023-08-31 03:37:28,629] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 03:37:28,631] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 03:37:28,631] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 03:37:28,631] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 03:37:30,054] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 03:37:30,054] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.54%
Epoch: [146]  [  0/160]  eta: 0:21:17  lr: 0.000004  min_lr: 0.000000  loss: 1.8447 (1.8447)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6556 (6.6556)  time: 7.9843 (7.9843 -- 7.9843)  data: 7.4213 (7.4213 -- 7.4213)  max mem: 16413
Epoch: [146]  [ 20/160]  eta: 0:03:01  lr: 0.000004  min_lr: 0.000000  loss: 1.7245 (1.7282)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2065 (9.1841)  time: 0.9624 (0.5170 -- 5.0997)  data: 0.4124 (0.0005 -- 4.5839)  max mem: 16413
Epoch: [146]  [ 40/160]  eta: 0:02:11  lr: 0.000004  min_lr: 0.000000  loss: 1.7384 (1.7238)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1824 (8.8751)  time: 0.8920 (0.5102 -- 3.6702)  data: 0.3466 (0.0002 -- 3.1476)  max mem: 16413
[2023-08-31 03:38:26,015] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:38:26,015] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:38:26,016] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:38:26,016] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [146]  [ 60/160]  eta: 0:01:40  lr: 0.000004  min_lr: 0.000000  loss: 1.7386 (1.7290)  loss_scale: 8192.0000 (9132.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7347 (8.7968)  time: 0.8041 (0.5266 -- 3.7507)  data: 0.2514 (0.0004 -- 3.1844)  max mem: 16413
Epoch: [146]  [ 80/160]  eta: 0:01:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7777 (1.7509)  loss_scale: 16384.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4177 (8.7844)  time: 0.9252 (0.5244 -- 4.0654)  data: 0.3701 (0.0002 -- 3.5389)  max mem: 16413
Epoch: [146]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 2.0000 (1.7980)  loss_scale: 16384.0000 (12004.1188)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9030 (8.7216)  time: 0.7265 (0.5286 -- 3.2113)  data: 0.1688 (0.0002 -- 2.6768)  max mem: 16413
Epoch: [146]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7678 (1.8098)  loss_scale: 16384.0000 (12728.0661)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1796 (8.6468)  time: 0.8545 (0.5249 -- 3.5656)  data: 0.3063 (0.0006 -- 3.0393)  max mem: 16413
Epoch: [146]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7334 (1.7898)  loss_scale: 16384.0000 (13246.6383)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6147 (8.7240)  time: 0.8429 (0.5235 -- 3.7384)  data: 0.2751 (0.0003 -- 3.1967)  max mem: 16413
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7334 (1.8032)  loss_scale: 16384.0000 (13619.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6468 (8.7757)  time: 0.7206 (0.4970 -- 4.0630)  data: 0.1988 (0.0002 -- 3.5446)  max mem: 16413
Epoch: [146] Total time: 0:02:22 (0.8877 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7334 (1.8017)  loss_scale: 16384.0000 (13619.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6468 (8.7757)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3744 (0.3744)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3445 (2.3445 -- 2.3445)  data: 2.1091 (2.1091 -- 2.1091)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4671 (0.7728)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4371 (0.1981 -- 2.3445)  data: 0.2185 (0.0005 -- 2.1091)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5576 (0.6955)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2243 (0.1696 -- 0.5327)  data: 0.0172 (0.0001 -- 0.2840)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6210 (0.7542)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.4357)  time: 0.2094 (0.1331 -- 0.5327)  data: 0.0169 (0.0001 -- 0.2840)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 81.120 Acc@5 96.058 loss 0.712
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.54%
Epoch: [147]  [  0/160]  eta: 0:20:51  lr: 0.000004  min_lr: 0.000000  loss: 1.5548 (1.5548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1812 (8.1812)  time: 7.8243 (7.8243 -- 7.8243)  data: 7.2996 (7.2996 -- 7.2996)  max mem: 16413
Epoch: [147]  [ 20/160]  eta: 0:02:47  lr: 0.000004  min_lr: 0.000000  loss: 1.7805 (1.7543)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0620 (7.9278)  time: 0.8641 (0.5368 -- 4.6192)  data: 0.3111 (0.0003 -- 4.0837)  max mem: 16413
[2023-08-31 03:40:26,134] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:40:26,135] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:40:26,138] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:40:26,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:40:36,106] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23553
[2023-08-31 03:40:36,106] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23553
[2023-08-31 03:40:36,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:40:36,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:40:36,106] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [147]  [ 40/160]  eta: 0:02:06  lr: 0.000004  min_lr: 0.000000  loss: 1.8812 (1.8307)  loss_scale: 32768.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3528 (8.5419)  time: 0.9111 (0.5208 -- 2.9899)  data: 0.3629 (0.0008 -- 2.4722)  max mem: 16413
Epoch: [147]  [ 60/160]  eta: 0:01:37  lr: 0.000004  min_lr: 0.000000  loss: 1.8320 (1.8046)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4246 (8.6805)  time: 0.7989 (0.5328 -- 2.7064)  data: 0.2431 (0.0004 -- 2.1825)  max mem: 16413
Epoch: [147]  [ 80/160]  eta: 0:01:18  lr: 0.000004  min_lr: 0.000000  loss: 1.9379 (1.8364)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1433 (8.8165)  time: 1.0152 (0.5112 -- 4.0912)  data: 0.4715 (0.0003 -- 3.5252)  max mem: 16413
[2023-08-31 03:41:27,511] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23610
[2023-08-31 03:41:27,511] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23610
[2023-08-31 03:41:27,511] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:41:27,511] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:41:27,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [147]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 2.0179 (1.8623)  loss_scale: 8192.0000 (17276.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9749 (8.7775)  time: 0.7871 (0.5195 -- 3.1594)  data: 0.2455 (0.0002 -- 2.6037)  max mem: 16413
Epoch: [147]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.8572 (1.8526)  loss_scale: 8192.0000 (15774.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9011 (8.7666)  time: 0.9122 (0.5238 -- 3.0922)  data: 0.3658 (0.0003 -- 2.5764)  max mem: 16413
Epoch: [147]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7144 (1.8395)  loss_scale: 8192.0000 (14699.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4441 (8.8804)  time: 0.8504 (0.5297 -- 3.7635)  data: 0.3004 (0.0004 -- 3.2313)  max mem: 16413
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8743 (1.8459)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3227 (8.9012)  time: 0.6312 (0.4967 -- 1.7285)  data: 0.1103 (0.0002 -- 1.1959)  max mem: 16413
Epoch: [147] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8743 (1.8242)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3227 (8.9012)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3707 (0.3707)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4133 (2.4133 -- 2.4133)  data: 2.1997 (2.1997 -- 2.1997)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4566 (0.7689)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4419 (0.1995 -- 2.4133)  data: 0.2273 (0.0008 -- 2.1997)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5610 (0.6944)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2282 (0.1693 -- 0.5121)  data: 0.0258 (0.0001 -- 0.2786)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6167 (0.7545)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.4357)  time: 0.2129 (0.1324 -- 0.5121)  data: 0.0255 (0.0001 -- 0.2786)  max mem: 16413
Val: Total time: 0:00:07 (0.2948 s / it)
* Acc@1 81.120 Acc@5 96.058 loss 0.713
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.54%
Epoch: [148]  [  0/160]  eta: 0:20:32  lr: 0.000004  min_lr: 0.000000  loss: 1.6090 (1.6090)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2581 (8.2581)  time: 7.7060 (7.7060 -- 7.7060)  data: 7.1268 (7.1268 -- 7.1268)  max mem: 16413
Epoch: [148]  [ 20/160]  eta: 0:02:54  lr: 0.000004  min_lr: 0.000000  loss: 1.8615 (1.8181)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6520 (9.4317)  time: 0.9219 (0.5253 -- 4.4143)  data: 0.3789 (0.0004 -- 3.8903)  max mem: 16413
Epoch: [148]  [ 40/160]  eta: 0:02:08  lr: 0.000004  min_lr: 0.000000  loss: 1.7528 (1.7936)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2018 (8.8349)  time: 0.8797 (0.5299 -- 3.0886)  data: 0.3342 (0.0002 -- 2.5531)  max mem: 16413
[2023-08-31 03:43:31,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:43:31,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:43:31,239] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:43:31,240] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [148]  [ 60/160]  eta: 0:01:40  lr: 0.000004  min_lr: 0.000000  loss: 1.8024 (1.7872)  loss_scale: 8192.0000 (8460.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9368 (8.5884)  time: 0.8717 (0.5234 -- 3.0547)  data: 0.3246 (0.0003 -- 2.4529)  max mem: 16413
Epoch: [148]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.7262 (1.7845)  loss_scale: 16384.0000 (10416.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5631 (8.5271)  time: 0.8176 (0.5100 -- 3.1357)  data: 0.2515 (0.0003 -- 2.6048)  max mem: 16413
Epoch: [148]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.8262 (1.7939)  loss_scale: 16384.0000 (11598.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6131 (8.5385)  time: 0.8208 (0.5320 -- 2.2835)  data: 0.1892 (0.0005 -- 1.7204)  max mem: 16413
Epoch: [148]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7797 (1.7887)  loss_scale: 16384.0000 (12389.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4561 (8.6287)  time: 0.8667 (0.5089 -- 4.2672)  data: 0.0012 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [148]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7820 (1.7989)  loss_scale: 16384.0000 (12956.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4112 (8.5441)  time: 0.8427 (0.5155 -- 4.0429)  data: 0.1503 (0.0004 -- 1.6215)  max mem: 16413
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8625 (1.8066)  loss_scale: 16384.0000 (13363.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5554 (8.6019)  time: 0.7099 (0.4981 -- 2.1500)  data: 0.1380 (0.0002 -- 1.2415)  max mem: 16413
Epoch: [148] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8625 (1.8224)  loss_scale: 16384.0000 (13363.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5554 (8.6019)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.3590 (0.3590)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6878 (2.6878 -- 2.6878)  data: 2.4694 (2.4694 -- 2.4694)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4676 (0.7704)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4612 (0.1987 -- 2.6878)  data: 0.2452 (0.0005 -- 2.4694)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5531 (0.6991)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2170 (0.1687 -- 0.4221)  data: 0.0115 (0.0001 -- 0.2169)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6084 (0.7592)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.4357)  time: 0.2015 (0.1327 -- 0.4221)  data: 0.0111 (0.0001 -- 0.2169)  max mem: 16413
Val: Total time: 0:00:08 (0.2967 s / it)
* Acc@1 81.120 Acc@5 96.058 loss 0.715
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.54%
Epoch: [149]  [  0/160]  eta: 0:15:46  lr: 0.000004  min_lr: 0.000000  loss: 1.5276 (1.5276)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3432 (9.3432)  time: 5.9126 (5.9126 -- 5.9126)  data: 5.3937 (5.3937 -- 5.3937)  max mem: 16413
Epoch: [149]  [ 20/160]  eta: 0:02:42  lr: 0.000004  min_lr: 0.000000  loss: 1.7301 (1.7667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8387 (8.8220)  time: 0.9235 (0.5357 -- 3.1313)  data: 0.1477 (0.0009 -- 1.1687)  max mem: 16413
[2023-08-31 03:45:31,186] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:45:31,186] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:45:31,186] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:45:31,187] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:45:36,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23872
[2023-08-31 03:45:36,665] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23872
[2023-08-31 03:45:36,666] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:45:36,666] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:45:36,666] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [149]  [ 40/160]  eta: 0:02:04  lr: 0.000004  min_lr: 0.000000  loss: 1.8886 (1.7971)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8006 (8.7861)  time: 0.9137 (0.5453 -- 3.2116)  data: 0.2267 (0.0007 -- 2.6304)  max mem: 16413
Epoch: [149]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.9288 (1.8304)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9804 (8.6965)  time: 0.8599 (0.5221 -- 2.6960)  data: 0.1608 (0.0006 -- 2.1611)  max mem: 16413
Epoch: [149]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.8386 (1.8237)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1947 (8.6751)  time: 0.8958 (0.5101 -- 2.8404)  data: 0.0912 (0.0004 -- 0.9922)  max mem: 16413
Epoch: [149]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.7022 (1.8178)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3486 (8.7003)  time: 0.9059 (0.5291 -- 3.9067)  data: 0.0015 (0.0005 -- 0.0064)  max mem: 16413
Epoch: [149]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.8851 (1.8232)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6394 (8.6047)  time: 0.6983 (0.5243 -- 2.3282)  data: 0.0015 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [149]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7737 (1.8135)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7419 (8.7731)  time: 0.9398 (0.5271 -- 4.8402)  data: 0.0023 (0.0004 -- 0.0076)  max mem: 16413
[2023-08-31 03:47:23,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=139, lr=[8.650991635604222e-08, 8.650991635604222e-08, 1.1534655514138963e-07, 1.1534655514138963e-07, 1.5379540685518617e-07, 1.5379540685518617e-07, 2.0506054247358157e-07, 2.0506054247358157e-07, 2.734140566314421e-07, 2.734140566314421e-07, 3.6455207550858947e-07, 3.6455207550858947e-07, 4.860694340114526e-07, 4.860694340114526e-07, 6.480925786819368e-07, 6.480925786819368e-07, 8.641234382425824e-07, 8.641234382425824e-07, 1.1521645843234432e-06, 1.1521645843234432e-06, 1.536219445764591e-06, 1.536219445764591e-06, 2.048292594352788e-06, 2.048292594352788e-06, 2.731056792470384e-06, 2.731056792470384e-06, 3.6414090566271786e-06, 3.6414090566271786e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 03:47:23,225] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=17.183785368343653, CurrSamplesPerSec=24.45017947194588, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8963 (1.8257)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4942 (8.7424)  time: 0.7332 (0.4968 -- 3.9164)  data: 0.0010 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [149] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8963 (1.8492)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4942 (8.7424)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3660 (0.3660)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5545 (2.5545 -- 2.5545)  data: 2.3122 (2.3122 -- 2.3122)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4718 (0.7637)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4445 (0.2013 -- 2.5545)  data: 0.2235 (0.0007 -- 2.3122)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5505 (0.6960)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2146 (0.1708 -- 0.3329)  data: 0.0082 (0.0001 -- 0.1184)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6045 (0.7543)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.1989 (0.1329 -- 0.3329)  data: 0.0071 (0.0001 -- 0.1184)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 81.743 Acc@5 96.473 loss 0.711
Accuracy of the network on the 482 val images: 81.74%
[2023-08-31 03:47:31,076] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 03:47:31,077] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 03:47:31,078] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 03:47:31,078] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 03:47:32,646] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 03:47:32,646] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.74%
Epoch: [150]  [  0/160]  eta: 0:21:08  lr: 0.000004  min_lr: 0.000000  loss: 2.2310 (2.2310)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1323 (5.1323)  time: 7.9284 (7.9284 -- 7.9284)  data: 6.6622 (6.6622 -- 6.6622)  max mem: 16413
[2023-08-31 03:47:41,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:47:41,131] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 03:47:41,132] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:47:41,132] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [150]  [ 20/160]  eta: 0:02:45  lr: 0.000004  min_lr: 0.000000  loss: 1.8303 (1.8977)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0265 (7.9594)  time: 0.8435 (0.5284 -- 2.5704)  data: 0.2516 (0.0005 -- 1.7911)  max mem: 16413
Epoch: [150]  [ 40/160]  eta: 0:02:02  lr: 0.000004  min_lr: 0.000000  loss: 1.7924 (1.8223)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5525 (8.6161)  time: 0.8539 (0.5302 -- 2.6088)  data: 0.3033 (0.0002 -- 2.0640)  max mem: 16413
[2023-08-31 03:48:16,492] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24042
[2023-08-31 03:48:16,492] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:48:16,492] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24042
[2023-08-31 03:48:16,493] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 03:48:16,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [150]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.8896 (1.8187)  loss_scale: 16384.0000 (27396.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3236 (8.6322)  time: 0.9137 (0.5135 -- 2.5686)  data: 0.3700 (0.0005 -- 2.0356)  max mem: 16413
Epoch: [150]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.6436 (1.8041)  loss_scale: 16384.0000 (24677.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1982 (8.6485)  time: 0.8603 (0.5266 -- 3.1694)  data: 0.2806 (0.0002 -- 2.6362)  max mem: 16413
Epoch: [150]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 2.0019 (1.8309)  loss_scale: 16384.0000 (23034.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6490 (8.9048)  time: 0.7940 (0.5168 -- 2.7118)  data: 0.2195 (0.0005 -- 2.1576)  max mem: 16413
Epoch: [150]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7281 (1.8125)  loss_scale: 16384.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4624 (8.8028)  time: 0.8373 (0.5273 -- 3.4209)  data: 0.2689 (0.0003 -- 2.9146)  max mem: 16413
Epoch: [150]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.9080 (1.8249)  loss_scale: 16384.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2945 (8.6670)  time: 0.8937 (0.5225 -- 2.6504)  data: 0.3375 (0.0004 -- 2.1006)  max mem: 16413
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8341 (1.8246)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9433 (8.6719)  time: 0.7067 (0.4965 -- 2.4313)  data: 0.1347 (0.0003 -- 1.9208)  max mem: 16413
Epoch: [150] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8341 (1.8120)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9433 (8.6719)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3672 (0.3672)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4228 (2.4228 -- 2.4228)  data: 2.2223 (2.2223 -- 2.2223)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4520 (0.7645)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4395 (0.1849 -- 2.4228)  data: 0.2291 (0.0006 -- 2.2223)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5622 (0.6962)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1703 -- 0.4992)  data: 0.0150 (0.0001 -- 0.2887)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6038 (0.7556)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2047 (0.1331 -- 0.4992)  data: 0.0147 (0.0001 -- 0.2887)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 81.743 Acc@5 96.680 loss 0.712
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 81.74%
Epoch: [151]  [  0/160]  eta: 0:16:23  lr: 0.000004  min_lr: 0.000000  loss: 1.4566 (1.4566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1225 (7.1225)  time: 6.1440 (6.1440 -- 6.1440)  data: 5.4999 (5.4999 -- 5.4999)  max mem: 16413
[2023-08-31 03:50:09,573] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24162
[2023-08-31 03:50:09,573] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24162
[2023-08-31 03:50:09,573] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:50:09,573] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:50:09,573] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [151]  [ 20/160]  eta: 0:02:46  lr: 0.000003  min_lr: 0.000000  loss: 1.8316 (1.7610)  loss_scale: 8192.0000 (8972.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9637 (9.0718)  time: 0.9428 (0.5035 -- 2.8116)  data: 0.1289 (0.0004 -- 1.1631)  max mem: 16413
Epoch: [151]  [ 40/160]  eta: 0:01:58  lr: 0.000003  min_lr: 0.000000  loss: 1.8947 (1.7807)  loss_scale: 8192.0000 (8591.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1479 (9.2860)  time: 0.7749 (0.5213 -- 2.9490)  data: 0.0310 (0.0003 -- 0.5704)  max mem: 16413
Epoch: [151]  [ 60/160]  eta: 0:01:35  lr: 0.000003  min_lr: 0.000000  loss: 1.7227 (1.7661)  loss_scale: 8192.0000 (8460.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5797 (8.8927)  time: 0.8915 (0.5249 -- 2.4523)  data: 0.0075 (0.0008 -- 0.1039)  max mem: 16413
Epoch: [151]  [ 80/160]  eta: 0:01:13  lr: 0.000003  min_lr: 0.000000  loss: 1.7838 (1.7723)  loss_scale: 8192.0000 (8394.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2578 (8.9078)  time: 0.8191 (0.5243 -- 2.0612)  data: 0.1092 (0.0009 -- 1.5122)  max mem: 16413
Epoch: [151]  [100/160]  eta: 0:00:54  lr: 0.000003  min_lr: 0.000000  loss: 1.7897 (1.7712)  loss_scale: 8192.0000 (8354.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9141 (9.0831)  time: 0.8667 (0.5322 -- 2.3581)  data: 0.2836 (0.0002 -- 1.8199)  max mem: 16413
Epoch: [151]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.8939 (1.7908)  loss_scale: 8192.0000 (8327.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7579 (8.9840)  time: 0.9019 (0.5282 -- 2.8526)  data: 0.3431 (0.0003 -- 2.3047)  max mem: 16413
[2023-08-31 03:51:59,705] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:51:59,706] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:51:59,707] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:51:59,707] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [151]  [140/160]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 1.5618 (1.7704)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1725 (8.9724)  time: 0.8149 (0.5305 -- 2.4219)  data: 0.1173 (0.0004 -- 0.9886)  max mem: 16413
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.9853 (1.7985)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6641 (8.9492)  time: 0.7823 (0.4936 -- 2.8968)  data: 0.1647 (0.0003 -- 2.3382)  max mem: 16413
Epoch: [151] Total time: 0:02:19 (0.8730 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.9853 (1.8129)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6641 (8.9492)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3592 (0.3592)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3752 (2.3752 -- 2.3752)  data: 2.1483 (2.1483 -- 2.1483)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4673 (0.7715)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4357 (0.2064 -- 2.3752)  data: 0.2106 (0.0006 -- 2.1483)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5404 (0.6973)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2237 (0.1698 -- 0.4443)  data: 0.0149 (0.0001 -- 0.1577)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6046 (0.7594)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.2077 (0.1333 -- 0.4443)  data: 0.0146 (0.0001 -- 0.1577)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 81.328 Acc@5 96.266 loss 0.714
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [152]  [  0/160]  eta: 0:19:59  lr: 0.000003  min_lr: 0.000000  loss: 1.8610 (1.8610)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0764 (9.0764)  time: 7.4939 (7.4939 -- 7.4939)  data: 6.4659 (6.4659 -- 6.4659)  max mem: 16413
Epoch: [152]  [ 20/160]  eta: 0:02:35  lr: 0.000003  min_lr: 0.000000  loss: 1.8146 (1.7910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5990 (8.4635)  time: 0.7941 (0.5378 -- 3.2100)  data: 0.2407 (0.0004 -- 2.6509)  max mem: 16413
[2023-08-31 03:52:54,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24343
[2023-08-31 03:52:54,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24343
[2023-08-31 03:52:54,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:52:54,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:52:54,506] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [152]  [ 40/160]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8683 (1.8479)  loss_scale: 8192.0000 (12787.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9846 (8.7086)  time: 0.8892 (0.5327 -- 3.6722)  data: 0.1063 (0.0005 -- 2.0885)  max mem: 16413
Epoch: [152]  [ 60/160]  eta: 0:01:38  lr: 0.000003  min_lr: 0.000000  loss: 1.8721 (1.8253)  loss_scale: 8192.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9573 (8.8322)  time: 0.9562 (0.5272 -- 2.5478)  data: 0.2106 (0.0002 -- 1.6664)  max mem: 16413
Epoch: [152]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.7757 (1.8124)  loss_scale: 8192.0000 (10518.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7684 (8.9540)  time: 0.8196 (0.5204 -- 3.6260)  data: 0.2791 (0.0003 -- 3.1147)  max mem: 16413
Epoch: [152]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.9745 (1.8185)  loss_scale: 8192.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5753 (8.8863)  time: 0.9060 (0.5303 -- 3.9453)  data: 0.3213 (0.0003 -- 3.4029)  max mem: 16413
Epoch: [152]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7749 (1.8140)  loss_scale: 8192.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8794 (8.8709)  time: 0.7920 (0.5369 -- 3.3819)  data: 0.0271 (0.0003 -- 0.5142)  max mem: 16413
Epoch: [152]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7971 (1.8091)  loss_scale: 8192.0000 (9528.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4932 (8.8812)  time: 0.8955 (0.5415 -- 2.6706)  data: 0.0832 (0.0006 -- 1.6314)  max mem: 16413
[2023-08-31 03:54:46,955] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:54:46,955] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:54:46,955] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:54:46,955] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8564 (1.8151)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7917 (8.8578)  time: 0.6737 (0.4968 -- 2.5341)  data: 0.0119 (0.0002 -- 0.2252)  max mem: 16413
Epoch: [152] Total time: 0:02:21 (0.8844 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8564 (1.8082)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7917 (8.8578)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3463 (0.3463)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5336 (2.5336 -- 2.5336)  data: 2.2959 (2.2959 -- 2.2959)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4725 (0.7723)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4363 (0.2013 -- 2.5336)  data: 0.2105 (0.0009 -- 2.2959)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5177 (0.6943)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2189 (0.1685 -- 0.3704)  data: 0.0092 (0.0001 -- 0.1617)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6045 (0.7572)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.1989 (0.1324 -- 0.3704)  data: 0.0086 (0.0001 -- 0.1617)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 80.913 Acc@5 96.266 loss 0.713
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 81.74%
Epoch: [153]  [  0/160]  eta: 0:20:46  lr: 0.000003  min_lr: 0.000000  loss: 1.4820 (1.4820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0083 (8.0083)  time: 7.7879 (7.7879 -- 7.7879)  data: 7.2429 (7.2429 -- 7.2429)  max mem: 16413
Epoch: [153]  [ 20/160]  eta: 0:02:57  lr: 0.000003  min_lr: 0.000000  loss: 1.8038 (1.7692)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4395 (8.8776)  time: 0.9412 (0.5157 -- 4.7342)  data: 0.3973 (0.0003 -- 4.1931)  max mem: 16413
Epoch: [153]  [ 40/160]  eta: 0:02:09  lr: 0.000003  min_lr: 0.000000  loss: 1.9001 (1.8244)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4970 (9.0646)  time: 0.8873 (0.5187 -- 4.8799)  data: 0.3491 (0.0003 -- 4.3769)  max mem: 16413
Epoch: [153]  [ 60/160]  eta: 0:01:41  lr: 0.000003  min_lr: 0.000000  loss: 1.9032 (1.8412)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5178 (8.8329)  time: 0.8712 (0.5195 -- 4.3074)  data: 0.3210 (0.0003 -- 3.7321)  max mem: 16413
[2023-08-31 03:56:12,458] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24557
[2023-08-31 03:56:12,458] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24557
[2023-08-31 03:56:12,458] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:56:12,458] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:56:12,458] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [153]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000000  loss: 1.8329 (1.8430)  loss_scale: 16384.0000 (15979.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8761 (8.8850)  time: 0.8429 (0.5210 -- 3.9268)  data: 0.2867 (0.0004 -- 3.3910)  max mem: 16413
Epoch: [153]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.9492 (1.8620)  loss_scale: 8192.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3083 (9.2373)  time: 0.8095 (0.5280 -- 1.8857)  data: 0.0454 (0.0005 -- 0.6269)  max mem: 16413
Epoch: [153]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7993 (1.8523)  loss_scale: 8192.0000 (13405.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7342 (9.1192)  time: 0.8153 (0.5293 -- 2.3674)  data: 0.0693 (0.0003 -- 0.7368)  max mem: 16413
Epoch: [153]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.9250 (1.8476)  loss_scale: 8192.0000 (12665.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4812 (9.0856)  time: 0.9419 (0.5287 -- 2.1353)  data: 0.1792 (0.0004 -- 1.5967)  max mem: 16413
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8303 (1.8380)  loss_scale: 8192.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5825 (9.0650)  time: 0.7710 (0.4934 -- 3.3379)  data: 0.0804 (0.0002 -- 1.5967)  max mem: 16413
Epoch: [153] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8303 (1.8510)  loss_scale: 8192.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5825 (9.0650)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3662 (0.3662)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5543 (2.5543 -- 2.5543)  data: 2.3301 (2.3301 -- 2.3301)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4689 (0.7645)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4331 (0.2092 -- 2.5543)  data: 0.2127 (0.0007 -- 2.3301)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5281 (0.6900)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2167 (0.1696 -- 0.3252)  data: 0.0076 (0.0001 -- 0.1394)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5991 (0.7510)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.2656)  time: 0.1989 (0.1328 -- 0.3252)  data: 0.0073 (0.0001 -- 0.1394)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 80.913 Acc@5 96.680 loss 0.712
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 81.74%
Epoch: [154]  [  0/160]  eta: 0:20:17  lr: 0.000003  min_lr: 0.000000  loss: 1.4598 (1.4598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6554 (7.6554)  time: 7.6111 (7.6111 -- 7.6111)  data: 4.8153 (4.8153 -- 4.8153)  max mem: 16413
Epoch: [154]  [ 20/160]  eta: 0:02:45  lr: 0.000003  min_lr: 0.000000  loss: 1.8953 (1.8459)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5573 (8.9961)  time: 0.8630 (0.5312 -- 2.2365)  data: 0.0070 (0.0003 -- 0.0965)  max mem: 16413
Epoch: [154]  [ 40/160]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000000  loss: 1.8417 (1.8794)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2288 (9.3504)  time: 0.8295 (0.5357 -- 3.9124)  data: 0.0019 (0.0002 -- 0.0132)  max mem: 16413
[2023-08-31 03:58:17,881] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:58:17,881] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 03:58:17,882] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 03:58:17,882] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [154]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7819 (1.8585)  loss_scale: 16384.0000 (10206.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8605 (9.1005)  time: 0.8610 (0.5312 -- 3.5585)  data: 0.0256 (0.0003 -- 0.3125)  max mem: 16413
[2023-08-31 03:58:31,358] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24704
[2023-08-31 03:58:31,358] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24704
[2023-08-31 03:58:31,358] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:58:31,358] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 03:58:31,358] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [154]  [ 80/160]  eta: 0:01:13  lr: 0.000003  min_lr: 0.000000  loss: 1.8396 (1.8509)  loss_scale: 8192.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4710 (9.2449)  time: 0.7985 (0.5226 -- 3.2474)  data: 0.0284 (0.0004 -- 0.4328)  max mem: 16413
Epoch: [154]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.9053 (1.8468)  loss_scale: 8192.0000 (9651.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6223 (9.2585)  time: 0.9076 (0.5385 -- 3.3538)  data: 0.0737 (0.0004 -- 0.6675)  max mem: 16413
Epoch: [154]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.9437 (1.8651)  loss_scale: 8192.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8567 (9.1099)  time: 0.9225 (0.5400 -- 3.0733)  data: 0.1284 (0.0005 -- 1.7797)  max mem: 16413
Epoch: [154]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.8755 (1.8675)  loss_scale: 8192.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4549 (9.1643)  time: 0.8914 (0.5229 -- 3.7156)  data: 0.0014 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8188 (1.8513)  loss_scale: 8192.0000 (9113.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1438 (9.0952)  time: 0.7223 (0.4958 -- 4.6173)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [154] Total time: 0:02:23 (0.8938 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8188 (1.8302)  loss_scale: 8192.0000 (9113.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1438 (9.0952)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3620 (0.3620)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4946 (2.4946 -- 2.4946)  data: 2.2445 (2.2445 -- 2.2445)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4703 (0.7630)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4439 (0.1906 -- 2.4946)  data: 0.2234 (0.0004 -- 2.2445)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5261 (0.6892)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2156 (0.1690 -- 0.4270)  data: 0.0108 (0.0001 -- 0.2031)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6014 (0.7495)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.2656)  time: 0.2007 (0.1325 -- 0.4270)  data: 0.0106 (0.0001 -- 0.2031)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 80.913 Acc@5 96.680 loss 0.711
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 81.74%
Epoch: [155]  [  0/160]  eta: 0:21:25  lr: 0.000003  min_lr: 0.000000  loss: 1.4796 (1.4796)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.4029 (12.4029)  time: 8.0336 (8.0336 -- 8.0336)  data: 7.4710 (7.4710 -- 7.4710)  max mem: 16413
Epoch: [155]  [ 20/160]  eta: 0:02:48  lr: 0.000003  min_lr: 0.000000  loss: 1.8481 (1.8016)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3824 (9.0808)  time: 0.8646 (0.5363 -- 3.8412)  data: 0.2666 (0.0007 -- 2.8741)  max mem: 16413
[2023-08-31 04:00:38,361] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:00:38,362] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:00:38,364] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:00:38,365] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [155]  [ 40/160]  eta: 0:02:08  lr: 0.000003  min_lr: 0.000000  loss: 1.8241 (1.7860)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3556 (8.6577)  time: 0.9283 (0.5306 -- 3.1491)  data: 0.0334 (0.0003 -- 0.2792)  max mem: 16413
Epoch: [155]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 1.9131 (1.8255)  loss_scale: 16384.0000 (11952.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6420 (8.6744)  time: 0.7637 (0.5272 -- 4.4146)  data: 0.0011 (0.0005 -- 0.0022)  max mem: 16413
Epoch: [155]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.7686 (1.8153)  loss_scale: 16384.0000 (13046.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3214 (8.6236)  time: 0.9177 (0.5227 -- 3.7955)  data: 0.0013 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [155]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.9039 (1.8139)  loss_scale: 16384.0000 (13707.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3209 (8.6631)  time: 0.7995 (0.5367 -- 3.2853)  data: 0.0013 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [155]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7458 (1.7948)  loss_scale: 16384.0000 (14149.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9912 (8.6318)  time: 0.8424 (0.5265 -- 3.5376)  data: 0.0016 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [155]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7412 (1.7908)  loss_scale: 16384.0000 (14466.7234)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8221 (8.5586)  time: 0.8918 (0.5145 -- 3.1365)  data: 0.0613 (0.0004 -- 1.1854)  max mem: 16413
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.9705 (1.8084)  loss_scale: 16384.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1782 (8.5730)  time: 0.6634 (0.4971 -- 1.6804)  data: 0.0007 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [155] Total time: 0:02:20 (0.8812 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.9705 (1.8298)  loss_scale: 16384.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1782 (8.5730)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3650 (0.3650)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3045 (2.3045 -- 2.3045)  data: 2.0951 (2.0951 -- 2.0951)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4666 (0.7640)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4374 (0.1989 -- 2.3045)  data: 0.2265 (0.0006 -- 2.0951)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5316 (0.6888)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2303 (0.1693 -- 0.6085)  data: 0.0265 (0.0001 -- 0.3879)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6053 (0.7505)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.8506)  time: 0.2173 (0.1333 -- 0.6085)  data: 0.0263 (0.0001 -- 0.3879)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 80.705 Acc@5 96.680 loss 0.709
Accuracy of the network on the 482 val images: 80.71%
Max accuracy: 81.74%
Epoch: [156]  [  0/160]  eta: 0:18:01  lr: 0.000003  min_lr: 0.000000  loss: 2.0963 (2.0963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5352 (7.5352)  time: 6.7579 (6.7579 -- 6.7579)  data: 5.5455 (5.5455 -- 5.5455)  max mem: 16413
[2023-08-31 04:02:37,054] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:02:37,055] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:02:37,059] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:02:37,060] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:02:48,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24972
[2023-08-31 04:02:48,082] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:02:48,082] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 04:02:48,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24972
[2023-08-31 04:02:48,082] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [156]  [ 20/160]  eta: 0:02:48  lr: 0.000003  min_lr: 0.000000  loss: 1.9036 (1.9458)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5665 (8.8795)  time: 0.9254 (0.5098 -- 3.5308)  data: 0.3822 (0.0002 -- 3.0032)  max mem: 16413
[2023-08-31 04:03:08,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=145, lr=[6.733965761740349e-08, 6.733965761740349e-08, 8.978621015653799e-08, 8.978621015653799e-08, 1.1971494687538399e-07, 1.1971494687538399e-07, 1.5961992916717863e-07, 1.5961992916717863e-07, 2.1282657222290486e-07, 2.1282657222290486e-07, 2.8376876296387316e-07, 2.8376876296387316e-07, 3.7835835061849753e-07, 3.7835835061849753e-07, 5.044778008246634e-07, 5.044778008246634e-07, 6.726370677662179e-07, 6.726370677662179e-07, 8.968494236882905e-07, 8.968494236882905e-07, 1.1957992315843872e-06, 1.1957992315843872e-06, 1.5943989754458497e-06, 1.5943989754458497e-06, 2.125865300594466e-06, 2.125865300594466e-06, 2.8344870674592883e-06, 2.8344870674592883e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 04:03:08,522] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=17.19876306734141, CurrSamplesPerSec=21.204960104550974, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [156]  [ 40/160]  eta: 0:01:55  lr: 0.000003  min_lr: 0.000000  loss: 1.8371 (1.8559)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6301 (8.6274)  time: 0.7068 (0.5249 -- 2.5484)  data: 0.1453 (0.0007 -- 1.9730)  max mem: 16413
Epoch: [156]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7996 (1.8610)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4552 (8.8007)  time: 0.9761 (0.5279 -- 3.8952)  data: 0.3610 (0.0002 -- 3.3745)  max mem: 16413
Epoch: [156]  [ 80/160]  eta: 0:01:14  lr: 0.000003  min_lr: 0.000000  loss: 1.7552 (1.8362)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7452 (8.8473)  time: 0.8332 (0.5322 -- 3.6507)  data: 0.0170 (0.0004 -- 0.2433)  max mem: 16413
Epoch: [156]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.7590 (1.8202)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9111 (8.9580)  time: 0.8503 (0.5272 -- 2.2038)  data: 0.0015 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [156]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7729 (1.8111)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9632 (8.8148)  time: 0.8533 (0.5265 -- 2.8393)  data: 0.0735 (0.0009 -- 0.9241)  max mem: 16413
Epoch: [156]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.9278 (1.8251)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7368 (8.7207)  time: 0.9912 (0.5233 -- 4.7956)  data: 0.4469 (0.0003 -- 4.2760)  max mem: 16413
[2023-08-31 04:04:39,798] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:04:39,798] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:04:39,798] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:04:39,798] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:04:50,612] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25117
[2023-08-31 04:04:50,612] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25117
[2023-08-31 04:04:50,613] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:04:50,613] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:04:50,613] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.9360 (1.8206)  loss_scale: 32768.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0454 (8.6587)  time: 0.6971 (0.4912 -- 2.2382)  data: 0.1798 (0.0002 -- 1.7387)  max mem: 16413
Epoch: [156] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.9360 (1.8103)  loss_scale: 32768.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0454 (8.6587)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3538 (0.3538)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3175 (2.3175 -- 2.3175)  data: 2.0630 (2.0630 -- 2.0630)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4637 (0.7636)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4132 (0.2037 -- 2.3175)  data: 0.1886 (0.0007 -- 2.0630)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5326 (0.6875)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2258 (0.1722 -- 0.4065)  data: 0.0131 (0.0001 -- 0.1954)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5994 (0.7496)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.2065 (0.1329 -- 0.4065)  data: 0.0128 (0.0001 -- 0.1954)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 80.913 Acc@5 96.680 loss 0.706
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 81.74%
Epoch: [157]  [  0/160]  eta: 0:20:17  lr: 0.000003  min_lr: 0.000000  loss: 1.3589 (1.3589)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8121 (9.8121)  time: 7.6116 (7.6116 -- 7.6116)  data: 7.0494 (7.0494 -- 7.0494)  max mem: 16413
Epoch: [157]  [ 20/160]  eta: 0:02:57  lr: 0.000003  min_lr: 0.000000  loss: 1.8956 (1.7760)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9065 (8.8594)  time: 0.9475 (0.5222 -- 5.8328)  data: 0.1949 (0.0003 -- 1.9638)  max mem: 16413
Epoch: [157]  [ 40/160]  eta: 0:02:15  lr: 0.000003  min_lr: 0.000000  loss: 1.7873 (1.7815)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9958 (8.8492)  time: 0.9903 (0.5126 -- 3.7840)  data: 0.0015 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [157]  [ 60/160]  eta: 0:01:41  lr: 0.000003  min_lr: 0.000000  loss: 1.6988 (1.7809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2859 (8.7620)  time: 0.7695 (0.5087 -- 3.3322)  data: 0.0016 (0.0003 -- 0.0069)  max mem: 16413
Epoch: [157]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000000  loss: 1.7997 (1.7926)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3365 (8.6603)  time: 0.8566 (0.5305 -- 4.0363)  data: 0.0013 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [157]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 2.0160 (1.8319)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5098 (8.6852)  time: 0.7939 (0.5305 -- 3.4001)  data: 0.0248 (0.0004 -- 0.3608)  max mem: 16413
Epoch: [157]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.9216 (1.8425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5733 (8.8201)  time: 0.8168 (0.5147 -- 2.4970)  data: 0.1205 (0.0002 -- 1.4012)  max mem: 16413
[2023-08-31 04:06:55,950] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:06:55,951] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:06:55,951] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:06:55,951] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [157]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7265 (1.8268)  loss_scale: 32768.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6641 (8.8472)  time: 0.9041 (0.5285 -- 3.2987)  data: 0.3455 (0.0003 -- 2.7246)  max mem: 16413
[2023-08-31 04:07:10,891] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25264
[2023-08-31 04:07:10,891] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:07:10,891] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25264
[2023-08-31 04:07:10,892] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:07:10,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5897 (1.8073)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7347 (8.8352)  time: 0.7308 (0.4974 -- 2.2851)  data: 0.0878 (0.0001 -- 1.7405)  max mem: 16413
Epoch: [157] Total time: 0:02:23 (0.8954 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5897 (1.8448)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7347 (8.8352)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3483 (0.3483)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4056 (2.4056 -- 2.4056)  data: 2.1919 (2.1919 -- 2.1919)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4624 (0.7639)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4666 (0.1986 -- 2.4056)  data: 0.2582 (0.0003 -- 2.1919)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5306 (0.6875)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2315 (0.1689 -- 0.8415)  data: 0.0326 (0.0001 -- 0.6406)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5975 (0.7504)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.8506)  time: 0.2197 (0.1328 -- 0.8415)  data: 0.0323 (0.0001 -- 0.6406)  max mem: 16413
Val: Total time: 0:00:08 (0.2972 s / it)
* Acc@1 80.498 Acc@5 96.473 loss 0.708
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 81.74%
Epoch: [158]  [  0/160]  eta: 0:19:56  lr: 0.000003  min_lr: 0.000000  loss: 2.0230 (2.0230)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3594 (8.3594)  time: 7.4805 (7.4805 -- 7.4805)  data: 6.9072 (6.9072 -- 6.9072)  max mem: 16413
Epoch: [158]  [ 20/160]  eta: 0:02:45  lr: 0.000003  min_lr: 0.000000  loss: 1.8407 (1.9014)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1581 (8.4989)  time: 0.8688 (0.5198 -- 2.6881)  data: 0.2485 (0.0002 -- 2.1598)  max mem: 16413
Epoch: [158]  [ 40/160]  eta: 0:02:04  lr: 0.000003  min_lr: 0.000000  loss: 1.7537 (1.8217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0070 (8.4859)  time: 0.8911 (0.5338 -- 2.8025)  data: 0.1403 (0.0005 -- 1.3151)  max mem: 16413
Epoch: [158]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 1.9148 (1.8432)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3590 (8.3979)  time: 0.7981 (0.5365 -- 3.9336)  data: 0.0015 (0.0005 -- 0.0025)  max mem: 16413
Epoch: [158]  [ 80/160]  eta: 0:01:18  lr: 0.000003  min_lr: 0.000000  loss: 1.8144 (1.8479)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2453 (8.5104)  time: 1.0344 (0.5162 -- 4.6412)  data: 0.0383 (0.0005 -- 0.7275)  max mem: 16413
Epoch: [158]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.6749 (1.8271)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3598 (8.6144)  time: 0.6851 (0.5210 -- 2.3719)  data: 0.0013 (0.0004 -- 0.0048)  max mem: 16413
[2023-08-31 04:09:14,943] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:09:14,943] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:09:14,944] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:09:14,945] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [158]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7270 (1.8176)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9648 (8.6747)  time: 0.8712 (0.5426 -- 2.0102)  data: 0.0681 (0.0007 -- 1.3319)  max mem: 16413
Epoch: [158]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7176 (1.8110)  loss_scale: 32768.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5082 (8.6945)  time: 1.0125 (0.5306 -- 4.3872)  data: 0.4561 (0.0004 -- 3.8601)  max mem: 16413
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8405 (1.8120)  loss_scale: 32768.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6708 (8.6236)  time: 0.6407 (0.4972 -- 1.8925)  data: 0.1269 (0.0001 -- 1.3954)  max mem: 16413
Epoch: [158] Total time: 0:02:23 (0.8938 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8405 (1.7970)  loss_scale: 32768.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6708 (8.6236)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3504 (0.3504)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4216 (2.4216 -- 2.4216)  data: 2.1562 (2.1562 -- 2.1562)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4688 (0.7569)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4326 (0.2023 -- 2.4216)  data: 0.2089 (0.0005 -- 2.1562)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5389 (0.6863)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2218 (0.1700 -- 0.3574)  data: 0.0131 (0.0001 -- 0.1269)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5961 (0.7463)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2056 (0.1334 -- 0.3574)  data: 0.0128 (0.0001 -- 0.1269)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 80.498 Acc@5 96.888 loss 0.705
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 81.74%
Epoch: [159]  [  0/160]  eta: 0:20:24  lr: 0.000003  min_lr: 0.000000  loss: 2.2084 (2.2084)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3638 (12.3638)  time: 7.6522 (7.6522 -- 7.6522)  data: 6.7735 (6.7735 -- 6.7735)  max mem: 16413
[2023-08-31 04:10:26,180] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25460
[2023-08-31 04:10:26,180] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25460
[2023-08-31 04:10:26,180] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:10:26,180] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:10:26,181] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [ 20/160]  eta: 0:02:43  lr: 0.000002  min_lr: 0.000000  loss: 1.7006 (1.8151)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1727 (10.1187)  time: 0.8467 (0.5146 -- 3.7990)  data: 0.2692 (0.0002 -- 2.9168)  max mem: 16413
Epoch: [159]  [ 40/160]  eta: 0:02:09  lr: 0.000002  min_lr: 0.000000  loss: 1.7605 (1.8545)  loss_scale: 16384.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6255 (9.6215)  time: 0.9828 (0.5196 -- 3.9858)  data: 0.4383 (0.0001 -- 3.4589)  max mem: 16413
Epoch: [159]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.6923 (1.8038)  loss_scale: 16384.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2959 (9.2425)  time: 0.7332 (0.5254 -- 3.0368)  data: 0.1848 (0.0004 -- 2.5213)  max mem: 16413
Epoch: [159]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.6677 (1.7920)  loss_scale: 16384.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1705 (9.2531)  time: 0.9014 (0.5135 -- 5.1105)  data: 0.3589 (0.0003 -- 4.5894)  max mem: 16413
Epoch: [159]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8152 (1.7887)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3461 (8.9212)  time: 0.8299 (0.5181 -- 2.8810)  data: 0.2843 (0.0002 -- 2.3493)  max mem: 16413
Epoch: [159]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8307 (1.7952)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4181 (8.8748)  time: 0.8702 (0.5287 -- 4.1521)  data: 0.3181 (0.0002 -- 3.6454)  max mem: 16413
Epoch: [159]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8834 (1.8064)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5123 (8.8964)  time: 0.7992 (0.5404 -- 3.5827)  data: 0.0062 (0.0006 -- 0.0900)  max mem: 16413
[2023-08-31 04:12:15,688] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:12:15,688] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:12:15,688] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:12:15,688] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:12:17,989] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25592
[2023-08-31 04:12:17,989] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:12:17,989] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25592
[2023-08-31 04:12:17,989] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:12:17,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8541 (1.8090)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0674 (8.8075)  time: 0.6905 (0.4938 -- 2.5792)  data: 0.0131 (0.0002 -- 0.2343)  max mem: 16413
Epoch: [159] Total time: 0:02:20 (0.8766 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8541 (1.8132)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0674 (8.8075)
[2023-08-31 04:12:21,848] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-08-31 04:12:21,849] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-08-31 04:12:21,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-08-31 04:12:21,850] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-08-31 04:12:22,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-08-31 04:12:22,765] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:01:12  loss: 0.3488 (0.3488)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6970 (2.6970 -- 2.6970)  data: 2.4749 (2.4749 -- 2.4749)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4655 (0.7614)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4490 (0.2050 -- 2.6970)  data: 0.2310 (0.0006 -- 2.4749)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5406 (0.6872)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2142 (0.1696 -- 0.2885)  data: 0.0092 (0.0001 -- 0.1140)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5946 (0.7477)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.1993 (0.1324 -- 0.2885)  data: 0.0089 (0.0001 -- 0.1140)  max mem: 16413
Val: Total time: 0:00:07 (0.2950 s / it)
* Acc@1 81.120 Acc@5 96.888 loss 0.707
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.74%
Epoch: [160]  [  0/160]  eta: 0:23:52  lr: 0.000002  min_lr: 0.000000  loss: 2.0058 (2.0058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3986 (7.3986)  time: 8.9507 (8.9507 -- 8.9507)  data: 6.5874 (6.5874 -- 6.5874)  max mem: 16413
Epoch: [160]  [ 20/160]  eta: 0:02:42  lr: 0.000002  min_lr: 0.000000  loss: 1.8377 (1.8346)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8657 (9.9427)  time: 0.7741 (0.5207 -- 3.4623)  data: 0.2028 (0.0002 -- 2.9122)  max mem: 16413
Epoch: [160]  [ 40/160]  eta: 0:02:13  lr: 0.000002  min_lr: 0.000000  loss: 1.7042 (1.7859)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3030 (9.2220)  time: 1.0606 (0.5320 -- 5.1320)  data: 0.4813 (0.0003 -- 4.6199)  max mem: 16413
Epoch: [160]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.9428 (1.8057)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4027 (9.3042)  time: 0.7373 (0.5191 -- 3.2183)  data: 0.1952 (0.0003 -- 2.7060)  max mem: 16413
Epoch: [160]  [ 80/160]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6653 (1.7674)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0401 (9.3138)  time: 0.9444 (0.5317 -- 3.9032)  data: 0.3939 (0.0004 -- 3.3645)  max mem: 16413
Epoch: [160]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.7073 (1.7750)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3341 (9.1989)  time: 0.7534 (0.5216 -- 3.3015)  data: 0.1422 (0.0003 -- 1.8103)  max mem: 16413
Epoch: [160]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7752 (1.7833)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1839 (9.1560)  time: 0.9009 (0.5352 -- 2.7748)  data: 0.3114 (0.0003 -- 2.2380)  max mem: 16413
[2023-08-31 04:14:23,653] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:14:23,653] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:14:23,653] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:14:23,653] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:14:27,252] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25727
[2023-08-31 04:14:27,253] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25727
[2023-08-31 04:14:27,253] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:14:27,253] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:14:27,253] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 04:14:34,308] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25735
[2023-08-31 04:14:34,308] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25735
[2023-08-31 04:14:34,309] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:14:34,309] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:14:34,310] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [160]  [140/160]  eta: 0:00:17  lr: 0.000002  min_lr: 0.000000  loss: 1.9777 (1.7957)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7527 (9.2355)  time: 0.7258 (0.5101 -- 3.2129)  data: 0.1614 (0.0005 -- 2.6826)  max mem: 16413
[2023-08-31 04:14:43,616] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25745
[2023-08-31 04:14:43,616] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25745
[2023-08-31 04:14:43,617] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 04:14:43,617] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2023-08-31 04:14:43,617] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
Epoch: [160]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7548 (1.7929)  loss_scale: 4096.0000 (15334.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8813 (9.1867)  time: 0.7182 (0.4968 -- 2.3586)  data: 0.0727 (0.0002 -- 0.5848)  max mem: 16413
Epoch: [160] Total time: 0:02:20 (0.8795 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7548 (1.8079)  loss_scale: 4096.0000 (15334.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8813 (9.1867)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3398 (0.3398)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3751 (2.3751 -- 2.3751)  data: 2.1747 (2.1747 -- 2.1747)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4691 (0.7600)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4219 (0.2042 -- 2.3751)  data: 0.2030 (0.0007 -- 2.1747)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5397 (0.6878)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2262 (0.1711 -- 0.4704)  data: 0.0163 (0.0001 -- 0.2643)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5917 (0.7471)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2082 (0.1329 -- 0.4704)  data: 0.0159 (0.0001 -- 0.2643)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.705
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [161]  [  0/160]  eta: 0:19:00  lr: 0.000002  min_lr: 0.000000  loss: 2.6362 (2.6362)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0780 (7.0780)  time: 7.1289 (7.1289 -- 7.1289)  data: 6.5540 (6.5540 -- 6.5540)  max mem: 16413
Epoch: [161]  [ 20/160]  eta: 0:02:43  lr: 0.000002  min_lr: 0.000000  loss: 1.6371 (1.6851)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8116 (8.2439)  time: 0.8720 (0.5260 -- 3.2683)  data: 0.3165 (0.0007 -- 2.7191)  max mem: 16413
Epoch: [161]  [ 40/160]  eta: 0:02:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6755 (1.7062)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1035 (8.2618)  time: 0.8254 (0.5228 -- 3.2467)  data: 0.2763 (0.0005 -- 2.7028)  max mem: 16413
Epoch: [161]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.7610 (1.7486)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1847 (8.2021)  time: 0.9354 (0.5237 -- 3.9651)  data: 0.3876 (0.0003 -- 3.4382)  max mem: 16413
Epoch: [161]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.7280 (1.7747)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8916 (8.3815)  time: 0.8650 (0.5270 -- 3.0633)  data: 0.3105 (0.0003 -- 2.5174)  max mem: 16413
Epoch: [161]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.9282 (1.7954)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2430 (8.3542)  time: 0.8924 (0.5247 -- 2.7525)  data: 0.2530 (0.0004 -- 2.2342)  max mem: 16413
[2023-08-31 04:16:47,184] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:16:47,184] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-31 04:16:47,185] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:16:47,185] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [161]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7623 (1.7878)  loss_scale: 4096.0000 (4332.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7005 (8.3597)  time: 0.8096 (0.5154 -- 3.4285)  data: 0.2516 (0.0004 -- 2.8933)  max mem: 16413
Epoch: [161]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7423 (1.7689)  loss_scale: 8192.0000 (4880.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3685 (8.3517)  time: 0.9427 (0.5271 -- 2.1637)  data: 0.3919 (0.0005 -- 1.5982)  max mem: 16413
Epoch: [161]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8147 (1.7710)  loss_scale: 8192.0000 (5273.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7186 (8.4399)  time: 0.6272 (0.4955 -- 2.4447)  data: 0.0974 (0.0002 -- 1.9323)  max mem: 16413
Epoch: [161] Total time: 0:02:22 (0.8875 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8147 (1.8068)  loss_scale: 8192.0000 (5273.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7186 (8.4399)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3434 (0.3434)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5990 (2.5990 -- 2.5990)  data: 2.2848 (2.2848 -- 2.2848)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4667 (0.7619)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4306 (0.1894 -- 2.5990)  data: 0.2163 (0.0006 -- 2.2848)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5255 (0.6863)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2107 (0.1712 -- 0.2895)  data: 0.0074 (0.0001 -- 0.0783)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5913 (0.7469)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.2656)  time: 0.1976 (0.1325 -- 0.2895)  data: 0.0067 (0.0001 -- 0.0783)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 80.913 Acc@5 96.680 loss 0.707
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 81.74%
Epoch: [162]  [  0/160]  eta: 0:19:13  lr: 0.000002  min_lr: 0.000000  loss: 1.7743 (1.7743)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0155 (8.0155)  time: 7.2095 (7.2095 -- 7.2095)  data: 5.6244 (5.6244 -- 5.6244)  max mem: 16413
Epoch: [162]  [ 20/160]  eta: 0:02:44  lr: 0.000002  min_lr: 0.000000  loss: 1.7872 (1.8452)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1037 (9.8307)  time: 0.8743 (0.5194 -- 3.1314)  data: 0.0696 (0.0007 -- 1.0436)  max mem: 16413
Epoch: [162]  [ 40/160]  eta: 0:01:58  lr: 0.000002  min_lr: 0.000000  loss: 1.7522 (1.7853)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2490 (9.4888)  time: 0.7978 (0.5183 -- 4.1529)  data: 0.0092 (0.0003 -- 0.1316)  max mem: 16413
Epoch: [162]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000000  loss: 1.6712 (1.7667)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1744 (9.0996)  time: 1.0241 (0.5194 -- 5.0072)  data: 0.0536 (0.0008 -- 0.9355)  max mem: 16413
[2023-08-31 04:18:45,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=152, lr=[5.031320553422787e-08, 5.031320553422787e-08, 6.708427404563716e-08, 6.708427404563716e-08, 8.944569872751621e-08, 8.944569872751621e-08, 1.192609316366883e-07, 1.192609316366883e-07, 1.590145755155844e-07, 1.590145755155844e-07, 2.1201943402077917e-07, 2.1201943402077917e-07, 2.8269257869437223e-07, 2.8269257869437223e-07, 3.7692343825916296e-07, 3.7692343825916296e-07, 5.025645843455506e-07, 5.025645843455506e-07, 6.700861124607342e-07, 6.700861124607342e-07, 8.934481499476456e-07, 8.934481499476456e-07, 1.1912641999301942e-06, 1.1912641999301942e-06, 1.5883522665735923e-06, 1.5883522665735923e-06, 2.117803022098123e-06, 2.117803022098123e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 04:18:45,217] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=17.242099042882714, CurrSamplesPerSec=20.69569808378831, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [162]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.8172 (1.7939)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7384 (8.9403)  time: 0.7738 (0.5204 -- 2.6113)  data: 0.0541 (0.0004 -- 1.0544)  max mem: 16413
[2023-08-31 04:18:48,790] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:18:48,790] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:18:48,792] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:18:48,792] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [162]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.8044 (1.7860)  loss_scale: 16384.0000 (9733.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7182 (8.8354)  time: 0.8856 (0.5436 -- 3.3395)  data: 0.0411 (0.0002 -- 0.6729)  max mem: 16413
Epoch: [162]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7351 (1.7811)  loss_scale: 16384.0000 (10832.3967)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0604 (8.7992)  time: 0.8173 (0.5179 -- 2.4448)  data: 0.0963 (0.0004 -- 1.7040)  max mem: 16413
Epoch: [162]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7690 (1.7729)  loss_scale: 16384.0000 (11619.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3572 (8.7254)  time: 0.9370 (0.5377 -- 3.8441)  data: 0.3834 (0.0002 -- 3.3086)  max mem: 16413
Epoch: [162]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6453 (1.7710)  loss_scale: 16384.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6605 (8.6314)  time: 0.6126 (0.4960 -- 1.6682)  data: 0.0893 (0.0002 -- 1.1309)  max mem: 16413
Epoch: [162] Total time: 0:02:21 (0.8821 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6453 (1.8054)  loss_scale: 16384.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6605 (8.6314)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3334 (0.3334)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5854 (2.5854 -- 2.5854)  data: 2.3696 (2.3696 -- 2.3696)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4549 (0.7585)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4520 (0.2034 -- 2.5854)  data: 0.2367 (0.0007 -- 2.3696)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5323 (0.6846)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2137 (0.1693 -- 0.4471)  data: 0.0118 (0.0001 -- 0.2129)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5923 (0.7450)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.1973 (0.1334 -- 0.4471)  data: 0.0109 (0.0001 -- 0.2129)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 81.535 Acc@5 96.888 loss 0.703
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [163]  [  0/160]  eta: 0:15:06  lr: 0.000002  min_lr: 0.000000  loss: 1.0326 (1.0326)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2293 (9.2293)  time: 5.6667 (5.6667 -- 5.6667)  data: 5.0112 (5.0112 -- 5.0112)  max mem: 16413
[2023-08-31 04:20:08,553] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26084
[2023-08-31 04:20:08,553] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26084
[2023-08-31 04:20:08,553] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:20:08,553] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:20:08,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [163]  [ 20/160]  eta: 0:02:50  lr: 0.000002  min_lr: 0.000000  loss: 1.9009 (1.8251)  loss_scale: 8192.0000 (9752.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3709 (8.7261)  time: 0.9950 (0.5260 -- 3.2001)  data: 0.3908 (0.0004 -- 2.6655)  max mem: 16413
Epoch: [163]  [ 40/160]  eta: 0:02:06  lr: 0.000002  min_lr: 0.000000  loss: 1.8517 (1.8131)  loss_scale: 8192.0000 (8991.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0462 (8.7832)  time: 0.8794 (0.5194 -- 4.1837)  data: 0.3294 (0.0003 -- 3.6718)  max mem: 16413
Epoch: [163]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000000  loss: 1.7899 (1.7894)  loss_scale: 8192.0000 (8729.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9533 (8.6148)  time: 0.9182 (0.5205 -- 3.3184)  data: 0.3741 (0.0003 -- 2.7873)  max mem: 16413
Epoch: [163]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.6953 (1.7851)  loss_scale: 8192.0000 (8596.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8671 (8.7140)  time: 0.7675 (0.5340 -- 3.5361)  data: 0.2233 (0.0006 -- 3.0196)  max mem: 16413
Epoch: [163]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8060 (1.7917)  loss_scale: 8192.0000 (8516.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1171 (8.8325)  time: 0.8281 (0.5185 -- 2.9662)  data: 0.2728 (0.0004 -- 2.4330)  max mem: 16413
Epoch: [163]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8001 (1.7863)  loss_scale: 8192.0000 (8462.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0793 (8.6378)  time: 0.8791 (0.5342 -- 2.6334)  data: 0.0437 (0.0009 -- 0.8401)  max mem: 16413
[2023-08-31 04:22:00,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:22:00,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:22:00,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:22:00,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [163]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7794 (1.7919)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3549 (8.7442)  time: 0.9011 (0.5260 -- 3.7280)  data: 0.0515 (0.0003 -- 0.9920)  max mem: 16413
Epoch: [163]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7575 (1.7907)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0480 (8.6670)  time: 0.6884 (0.4947 -- 2.5553)  data: 0.0008 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [163] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7575 (1.8032)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0480 (8.6670)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3399 (0.3399)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4285 (2.4285 -- 2.4285)  data: 2.2025 (2.2025 -- 2.2025)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4479 (0.7590)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4408 (0.1886 -- 2.4285)  data: 0.2232 (0.0002 -- 2.2025)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5322 (0.6846)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2281 (0.1696 -- 0.4732)  data: 0.0256 (0.0001 -- 0.2568)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5924 (0.7451)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2132 (0.1330 -- 0.4732)  data: 0.0251 (0.0001 -- 0.2568)  max mem: 16413
Val: Total time: 0:00:07 (0.2952 s / it)
* Acc@1 81.120 Acc@5 96.888 loss 0.703
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.74%
Epoch: [164]  [  0/160]  eta: 0:20:05  lr: 0.000002  min_lr: 0.000000  loss: 2.1690 (2.1690)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1488 (7.1488)  time: 7.5372 (7.5372 -- 7.5372)  data: 6.9946 (6.9946 -- 6.9946)  max mem: 16413
Epoch: [164]  [ 20/160]  eta: 0:02:36  lr: 0.000002  min_lr: 0.000000  loss: 1.9121 (1.8399)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8621 (9.1636)  time: 0.7967 (0.5339 -- 3.0495)  data: 0.0440 (0.0005 -- 0.8493)  max mem: 16413
Epoch: [164]  [ 40/160]  eta: 0:01:57  lr: 0.000002  min_lr: 0.000000  loss: 1.7247 (1.8144)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4349 (9.0724)  time: 0.8308 (0.5250 -- 3.1982)  data: 0.2770 (0.0005 -- 2.6487)  max mem: 16413
Epoch: [164]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8506 (1.8128)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6043 (9.2777)  time: 0.9754 (0.5302 -- 2.6159)  data: 0.3923 (0.0010 -- 2.0440)  max mem: 16413
Epoch: [164]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.8965 (1.8277)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0128 (9.1687)  time: 0.8426 (0.5217 -- 2.8484)  data: 0.2198 (0.0004 -- 2.3266)  max mem: 16413
Epoch: [164]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8219 (1.8212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4386 (9.3898)  time: 0.8275 (0.5198 -- 2.8369)  data: 0.1176 (0.0002 -- 1.3970)  max mem: 16413
[2023-08-31 04:24:01,978] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:24:01,978] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:24:01,978] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:24:01,978] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:24:03,650] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26344
[2023-08-31 04:24:03,650] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26344
[2023-08-31 04:24:03,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:24:03,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:24:03,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [164]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8192 (1.8138)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9191 (9.1971)  time: 0.8828 (0.5144 -- 3.1343)  data: 0.2038 (0.0010 -- 2.0064)  max mem: 16413
Epoch: [164]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8271 (1.8194)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9011 (9.0590)  time: 0.8420 (0.5188 -- 2.9864)  data: 0.2936 (0.0003 -- 2.4540)  max mem: 16413
Epoch: [164]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7042 (1.8103)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6633 (9.0239)  time: 0.7306 (0.4958 -- 2.5181)  data: 0.1424 (0.0002 -- 2.0169)  max mem: 16413
Epoch: [164] Total time: 0:02:21 (0.8849 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7042 (1.8156)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6633 (9.0239)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3435 (0.3435)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3558 (2.3558 -- 2.3558)  data: 2.1337 (2.1337 -- 2.1337)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4613 (0.7603)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4378 (0.1998 -- 2.3558)  data: 0.2190 (0.0007 -- 2.1337)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5278 (0.6853)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2258 (0.1692 -- 0.5037)  data: 0.0168 (0.0001 -- 0.2651)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5903 (0.7463)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.2105 (0.1325 -- 0.5037)  data: 0.0165 (0.0001 -- 0.2651)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 81.120 Acc@5 96.473 loss 0.704
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.74%
Epoch: [165]  [  0/160]  eta: 0:20:53  lr: 0.000002  min_lr: 0.000000  loss: 2.5329 (2.5329)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3681 (6.3681)  time: 7.8351 (7.8351 -- 7.8351)  data: 6.6076 (6.6076 -- 6.6076)  max mem: 16413
Epoch: [165]  [ 20/160]  eta: 0:02:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8467 (1.8806)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1705 (9.3559)  time: 0.7895 (0.5260 -- 2.8813)  data: 0.0079 (0.0006 -- 0.1141)  max mem: 16413
Epoch: [165]  [ 40/160]  eta: 0:01:59  lr: 0.000002  min_lr: 0.000000  loss: 1.7099 (1.8237)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9613 (8.6528)  time: 0.8587 (0.5329 -- 2.3686)  data: 0.0338 (0.0005 -- 0.4634)  max mem: 16413
Epoch: [165]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000000  loss: 1.7465 (1.8097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9979 (8.4868)  time: 1.0209 (0.5260 -- 5.2883)  data: 0.0018 (0.0005 -- 0.0053)  max mem: 16413
[2023-08-31 04:26:08,433] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:26:08,433] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:26:08,433] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:26:08,433] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:26:15,737] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26480
[2023-08-31 04:26:15,737] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26480
[2023-08-31 04:26:15,737] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:26:15,737] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:26:15,737] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [165]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.7351 (1.7879)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5226 (8.7614)  time: 0.8315 (0.5154 -- 4.0226)  data: 0.0015 (0.0002 -- 0.0049)  max mem: 16413
[2023-08-31 04:26:17,441] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26483
[2023-08-31 04:26:17,441] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26483
[2023-08-31 04:26:17,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:26:17,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:26:17,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [165]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.9163 (1.8034)  loss_scale: 8192.0000 (16059.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3694 (8.9161)  time: 0.8367 (0.5150 -- 3.9725)  data: 0.0015 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [165]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8614 (1.8106)  loss_scale: 8192.0000 (14759.1405)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6379 (9.0090)  time: 0.9219 (0.5228 -- 3.9340)  data: 0.0012 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [165]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8500 (1.8175)  loss_scale: 8192.0000 (13827.6312)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1954 (8.9280)  time: 0.7396 (0.5171 -- 2.7452)  data: 0.0013 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [165]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8703 (1.8089)  loss_scale: 8192.0000 (13158.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2077 (8.8704)  time: 0.7844 (0.4960 -- 2.4225)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [165] Total time: 0:02:22 (0.8936 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8703 (1.8395)  loss_scale: 8192.0000 (13158.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2077 (8.8704)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3459 (0.3459)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3812 (2.3812 -- 2.3812)  data: 2.1754 (2.1754 -- 2.1754)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4644 (0.7593)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4299 (0.1927 -- 2.3812)  data: 0.2191 (0.0008 -- 2.1754)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5317 (0.6853)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2271 (0.1689 -- 0.4535)  data: 0.0245 (0.0001 -- 0.2523)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5897 (0.7454)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.2127 (0.1334 -- 0.4535)  data: 0.0236 (0.0001 -- 0.2523)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 81.120 Acc@5 96.473 loss 0.704
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.74%
Epoch: [166]  [  0/160]  eta: 0:16:49  lr: 0.000002  min_lr: 0.000000  loss: 1.8506 (1.8506)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1768 (7.1768)  time: 6.3123 (6.3123 -- 6.3123)  data: 5.7680 (5.7680 -- 5.7680)  max mem: 16413
Epoch: [166]  [ 20/160]  eta: 0:02:46  lr: 0.000002  min_lr: 0.000000  loss: 1.9687 (1.9057)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5170 (8.5836)  time: 0.9297 (0.5447 -- 3.4964)  data: 0.1853 (0.0002 -- 2.9426)  max mem: 16413
Epoch: [166]  [ 40/160]  eta: 0:01:57  lr: 0.000002  min_lr: 0.000000  loss: 1.8747 (1.8956)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8279 (8.4880)  time: 0.7656 (0.5191 -- 2.0730)  data: 0.1002 (0.0003 -- 1.5348)  max mem: 16413
[2023-08-31 04:28:22,510] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:28:22,510] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:28:22,514] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:28:22,514] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [166]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.6585 (1.8134)  loss_scale: 8192.0000 (9400.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9758 (8.3710)  time: 1.0183 (0.5311 -- 3.9812)  data: 0.0021 (0.0003 -- 0.0068)  max mem: 16413
Epoch: [166]  [ 80/160]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6299 (1.7904)  loss_scale: 16384.0000 (11124.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1358 (8.5786)  time: 0.9276 (0.5250 -- 4.0991)  data: 0.0013 (0.0005 -- 0.0027)  max mem: 16413
Epoch: [166]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8805 (1.8185)  loss_scale: 16384.0000 (12166.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9892 (8.6895)  time: 0.6995 (0.5188 -- 1.8640)  data: 0.0013 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [166]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8622 (1.8154)  loss_scale: 16384.0000 (12863.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4994 (8.6746)  time: 0.8607 (0.5318 -- 4.3603)  data: 0.0017 (0.0003 -- 0.0049)  max mem: 16413
Epoch: [166]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8062 (1.8173)  loss_scale: 16384.0000 (13362.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1290 (8.8173)  time: 0.9245 (0.5243 -- 3.8909)  data: 0.2098 (0.0002 -- 3.3643)  max mem: 16413
Epoch: [166]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7380 (1.8163)  loss_scale: 16384.0000 (13721.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9531 (8.7475)  time: 0.6562 (0.4957 -- 2.0668)  data: 0.1335 (0.0003 -- 1.5645)  max mem: 16413
Epoch: [166] Total time: 0:02:21 (0.8843 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7380 (1.8246)  loss_scale: 16384.0000 (13721.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9531 (8.7475)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3453 (0.3453)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4961 (2.4961 -- 2.4961)  data: 2.2140 (2.2140 -- 2.2140)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4613 (0.7593)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4535 (0.2017 -- 2.4961)  data: 0.2311 (0.0005 -- 2.2140)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5322 (0.6844)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2214 (0.1692 -- 0.5508)  data: 0.0167 (0.0001 -- 0.3171)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5903 (0.7445)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2067 (0.1325 -- 0.5508)  data: 0.0165 (0.0001 -- 0.3171)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.702
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [167]  [  0/160]  eta: 0:20:53  lr: 0.000002  min_lr: 0.000000  loss: 2.1175 (2.1175)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7884 (7.7884)  time: 7.8353 (7.8353 -- 7.8353)  data: 7.0271 (7.0271 -- 7.0271)  max mem: 16413
[2023-08-31 04:30:22,372] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:30:22,373] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:30:22,373] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:30:22,373] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [167]  [ 20/160]  eta: 0:02:41  lr: 0.000002  min_lr: 0.000000  loss: 1.7763 (1.8118)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9536 (9.2543)  time: 0.8191 (0.5293 -- 3.3339)  data: 0.2596 (0.0007 -- 2.7811)  max mem: 16413
Epoch: [167]  [ 40/160]  eta: 0:01:56  lr: 0.000002  min_lr: 0.000000  loss: 1.6998 (1.7633)  loss_scale: 32768.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5565 (8.8384)  time: 0.7808 (0.5317 -- 2.7705)  data: 0.1724 (0.0004 -- 2.2320)  max mem: 16413
[2023-08-31 04:30:40,276] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26761
[2023-08-31 04:30:40,276] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26761
[2023-08-31 04:30:40,277] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:30:40,277] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:30:40,277] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [167]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7979 (1.7818)  loss_scale: 16384.0000 (22024.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0490 (8.6514)  time: 0.9617 (0.5297 -- 2.2726)  data: 0.1508 (0.0004 -- 1.4751)  max mem: 16413
Epoch: [167]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.8645 (1.7825)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5643 (8.8792)  time: 0.8909 (0.5266 -- 3.5915)  data: 0.0551 (0.0002 -- 0.7789)  max mem: 16413
Epoch: [167]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.9787 (1.8043)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3188 (8.9042)  time: 0.9288 (0.5215 -- 2.8787)  data: 0.0019 (0.0003 -- 0.0053)  max mem: 16413
Epoch: [167]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.9259 (1.8065)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5363 (9.0481)  time: 0.8177 (0.5244 -- 2.6997)  data: 0.1352 (0.0003 -- 2.1825)  max mem: 16413
Epoch: [167]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7779 (1.8141)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0232 (9.0108)  time: 0.9305 (0.5293 -- 3.6773)  data: 0.3805 (0.0003 -- 3.1448)  max mem: 16413
Epoch: [167]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8177 (1.8168)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5356 (9.0024)  time: 0.5833 (0.4964 -- 1.2971)  data: 0.0627 (0.0002 -- 0.7537)  max mem: 16413
Epoch: [167] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8177 (1.8170)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5356 (9.0024)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3469 (0.3469)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4215 (2.4215 -- 2.4215)  data: 2.1516 (2.1516 -- 2.1516)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4560 (0.7582)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4622 (0.1904 -- 2.4215)  data: 0.2382 (0.0005 -- 2.1516)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5382 (0.6847)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2320 (0.1722 -- 0.7001)  data: 0.0235 (0.0001 -- 0.4584)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5914 (0.7445)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2186 (0.1326 -- 0.7001)  data: 0.0233 (0.0001 -- 0.4584)  max mem: 16413
Val: Total time: 0:00:08 (0.2989 s / it)
* Acc@1 81.120 Acc@5 96.680 loss 0.703
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.74%
Epoch: [168]  [  0/160]  eta: 0:20:20  lr: 0.000002  min_lr: 0.000000  loss: 2.0947 (2.0947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6341 (7.6341)  time: 7.6294 (7.6294 -- 7.6294)  data: 5.7746 (5.7746 -- 5.7746)  max mem: 16413
[2023-08-31 04:32:44,406] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:32:44,406] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:32:44,408] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:32:44,408] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [168]  [ 20/160]  eta: 0:02:57  lr: 0.000002  min_lr: 0.000000  loss: 1.8376 (1.8000)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9513 (8.2279)  time: 0.9534 (0.5154 -- 5.1308)  data: 0.1460 (0.0004 -- 2.8880)  max mem: 16413
[2023-08-31 04:33:06,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26917
[2023-08-31 04:33:06,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:33:06,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26917
[2023-08-31 04:33:06,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:33:06,506] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [168]  [ 40/160]  eta: 0:01:59  lr: 0.000002  min_lr: 0.000000  loss: 1.7813 (1.8249)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8388 (8.4759)  time: 0.7025 (0.5378 -- 2.0070)  data: 0.0017 (0.0001 -- 0.0038)  max mem: 16413
Epoch: [168]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.9463 (1.8330)  loss_scale: 16384.0000 (23635.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8855 (8.6646)  time: 0.9151 (0.5320 -- 4.1247)  data: 0.0493 (0.0004 -- 0.6105)  max mem: 16413
[2023-08-31 04:33:36,892] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26951
[2023-08-31 04:33:36,892] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:33:36,892] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26951
[2023-08-31 04:33:36,892] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:33:36,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [168]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.8898 (1.8330)  loss_scale: 8192.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9025 (8.6275)  time: 0.9359 (0.5154 -- 3.2966)  data: 0.2203 (0.0003 -- 2.7728)  max mem: 16413
Epoch: [168]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.7945 (1.8270)  loss_scale: 8192.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3624 (8.6206)  time: 0.8263 (0.5220 -- 3.0727)  data: 0.0446 (0.0001 -- 0.4760)  max mem: 16413
[2023-08-31 04:34:16,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=159, lr=[3.560304368271369e-08, 3.560304368271369e-08, 4.747072491028493e-08, 4.747072491028493e-08, 6.32942998803799e-08, 6.32942998803799e-08, 8.439239984050654e-08, 8.439239984050654e-08, 1.1252319978734205e-07, 1.1252319978734205e-07, 1.500309330497894e-07, 1.500309330497894e-07, 2.0004124406638586e-07, 2.0004124406638586e-07, 2.6672165875518115e-07, 2.6672165875518115e-07, 3.5562887834024156e-07, 3.5562887834024156e-07, 4.741718377869887e-07, 4.741718377869887e-07, 6.322291170493183e-07, 6.322291170493183e-07, 8.429721560657577e-07, 8.429721560657577e-07, 1.1239628747543436e-06, 1.1239628747543436e-06, 1.4986171663391248e-06, 1.4986171663391248e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 04:34:16,084] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=17.233831130349017, CurrSamplesPerSec=21.91466318574692, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [168]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7339 (1.8222)  loss_scale: 8192.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6377 (8.6856)  time: 0.7442 (0.5293 -- 2.1805)  data: 0.0492 (0.0002 -- 0.9561)  max mem: 16413
Epoch: [168]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6693 (1.8029)  loss_scale: 8192.0000 (15454.4113)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0597 (8.7945)  time: 0.9635 (0.5256 -- 5.2816)  data: 0.4027 (0.0003 -- 4.7506)  max mem: 16413
Epoch: [168]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8926 (1.8051)  loss_scale: 8192.0000 (14592.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4543 (8.6753)  time: 0.6477 (0.4961 -- 3.0329)  data: 0.1267 (0.0002 -- 2.5196)  max mem: 16413
Epoch: [168] Total time: 0:02:20 (0.8809 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8926 (1.7906)  loss_scale: 8192.0000 (14592.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4543 (8.6753)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3498 (0.3498)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4373 (2.4373 -- 2.4373)  data: 2.2092 (2.2092 -- 2.2092)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4481 (0.7582)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4242 (0.1960 -- 2.4373)  data: 0.2083 (0.0007 -- 2.2092)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5399 (0.6854)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2181 (0.1694 -- 0.3835)  data: 0.0146 (0.0001 -- 0.2060)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5942 (0.7457)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2027 (0.1330 -- 0.3835)  data: 0.0143 (0.0001 -- 0.2060)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 81.120 Acc@5 97.095 loss 0.703
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.74%
Epoch: [169]  [  0/160]  eta: 0:23:19  lr: 0.000001  min_lr: 0.000000  loss: 1.6894 (1.6894)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8578 (5.8578)  time: 8.7474 (8.7474 -- 8.7474)  data: 8.2334 (8.2334 -- 8.2334)  max mem: 16413
Epoch: [169]  [ 20/160]  eta: 0:02:40  lr: 0.000001  min_lr: 0.000000  loss: 1.8529 (1.8847)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5364 (7.8035)  time: 0.7648 (0.5257 -- 3.2265)  data: 0.2224 (0.0002 -- 2.7059)  max mem: 16413
[2023-08-31 04:35:39,636] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:35:39,636] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:35:39,637] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:35:39,637] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [169]  [ 40/160]  eta: 0:02:06  lr: 0.000001  min_lr: 0.000000  loss: 1.6510 (1.7697)  loss_scale: 8192.0000 (8391.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2683 (7.5866)  time: 0.9536 (0.5189 -- 4.0823)  data: 0.3968 (0.0003 -- 3.4983)  max mem: 16413
Epoch: [169]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.9723 (1.8162)  loss_scale: 16384.0000 (11012.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7178 (7.8063)  time: 0.8296 (0.5222 -- 2.2724)  data: 0.2850 (0.0002 -- 1.7297)  max mem: 16413
[2023-08-31 04:36:13,847] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27117
[2023-08-31 04:36:13,847] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27117
[2023-08-31 04:36:13,848] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:36:13,848] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:36:13,848] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [169]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.8426 (1.8256)  loss_scale: 16384.0000 (11934.0247)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2404 (8.1134)  time: 0.9585 (0.4918 -- 4.7538)  data: 0.4148 (0.0003 -- 4.2595)  max mem: 16413
Epoch: [169]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.7844 (1.8288)  loss_scale: 8192.0000 (11193.0297)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5192 (8.4144)  time: 0.8830 (0.5233 -- 4.1494)  data: 0.3374 (0.0004 -- 3.6341)  max mem: 16413
Epoch: [169]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8957 (1.8403)  loss_scale: 8192.0000 (10696.9917)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5182 (8.4733)  time: 0.6449 (0.5327 -- 1.7250)  data: 0.0920 (0.0003 -- 1.1913)  max mem: 16413
Epoch: [169]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7594 (1.8380)  loss_scale: 8192.0000 (10341.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9100 (8.4856)  time: 1.0567 (0.5316 -- 5.3677)  data: 0.5059 (0.0003 -- 4.8387)  max mem: 16413
Epoch: [169]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8538 (1.8402)  loss_scale: 8192.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9605 (8.4755)  time: 0.6158 (0.4949 -- 2.4733)  data: 0.0981 (0.0001 -- 1.9498)  max mem: 16413
Epoch: [169] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8538 (1.8142)  loss_scale: 8192.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9605 (8.4755)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3416 (0.3416)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5922 (2.5922 -- 2.5922)  data: 2.3700 (2.3700 -- 2.3700)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.4548 (0.7588)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4713 (0.1945 -- 2.5922)  data: 0.2579 (0.0005 -- 2.3700)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5383 (0.6853)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2212 (0.1689 -- 0.7106)  data: 0.0235 (0.0001 -- 0.4600)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5912 (0.7463)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2069 (0.1325 -- 0.7106)  data: 0.0233 (0.0001 -- 0.4600)  max mem: 16413
Val: Total time: 0:00:08 (0.2977 s / it)
* Acc@1 81.120 Acc@5 96.473 loss 0.704
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.74%
Epoch: [170]  [  0/160]  eta: 0:18:49  lr: 0.000001  min_lr: 0.000000  loss: 2.0018 (2.0018)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7406 (10.7406)  time: 7.0579 (7.0579 -- 7.0579)  data: 5.0240 (5.0240 -- 5.0240)  max mem: 16413
Epoch: [170]  [ 20/160]  eta: 0:02:34  lr: 0.000001  min_lr: 0.000000  loss: 1.6822 (1.7268)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9546 (9.1563)  time: 0.8038 (0.5212 -- 2.8856)  data: 0.1665 (0.0007 -- 1.6559)  max mem: 16413
Epoch: [170]  [ 40/160]  eta: 0:02:03  lr: 0.000001  min_lr: 0.000000  loss: 1.6010 (1.6650)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9041 (8.8399)  time: 0.9545 (0.5238 -- 3.9293)  data: 0.0748 (0.0006 -- 1.4558)  max mem: 16413
[2023-08-31 04:38:12,492] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:38:12,492] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:38:12,492] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:38:12,492] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:38:26,368] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27259
[2023-08-31 04:38:26,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:38:26,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27259
[2023-08-31 04:38:26,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:38:26,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [170]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.9872 (1.7726)  loss_scale: 16384.0000 (9937.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2454 (8.6492)  time: 0.8881 (0.5289 -- 3.3891)  data: 0.0761 (0.0003 -- 0.9996)  max mem: 16413
Epoch: [170]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.9117 (1.8034)  loss_scale: 8192.0000 (9506.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8980 (8.7044)  time: 0.7949 (0.5226 -- 3.0721)  data: 0.0014 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [170]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7955 (1.8068)  loss_scale: 8192.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2988 (8.9129)  time: 0.9239 (0.5118 -- 3.6291)  data: 0.0014 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [170]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8015 (1.8050)  loss_scale: 8192.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0578 (9.0882)  time: 0.7352 (0.5316 -- 2.8468)  data: 0.0022 (0.0005 -- 0.0117)  max mem: 16413
Epoch: [170]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7329 (1.8045)  loss_scale: 8192.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4187 (9.0828)  time: 0.9062 (0.5272 -- 3.1568)  data: 0.0587 (0.0004 -- 0.7901)  max mem: 16413
Epoch: [170]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7250 (1.7989)  loss_scale: 8192.0000 (8857.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2229 (8.9764)  time: 0.7141 (0.4952 -- 1.5489)  data: 0.1023 (0.0002 -- 1.0331)  max mem: 16413
Epoch: [170] Total time: 0:02:20 (0.8766 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7250 (1.8017)  loss_scale: 8192.0000 (8857.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2229 (8.9764)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3423 (0.3423)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3981 (2.3981 -- 2.3981)  data: 2.1449 (2.1449 -- 2.1449)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4541 (0.7572)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4391 (0.2002 -- 2.3981)  data: 0.2166 (0.0007 -- 2.1449)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5345 (0.6834)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2266 (0.1697 -- 0.4288)  data: 0.0195 (0.0001 -- 0.2260)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5900 (0.7444)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2093 (0.1331 -- 0.4288)  data: 0.0191 (0.0001 -- 0.2260)  max mem: 16413
Val: Total time: 0:00:07 (0.2932 s / it)
* Acc@1 81.328 Acc@5 96.473 loss 0.702
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [171]  [  0/160]  eta: 0:26:20  lr: 0.000001  min_lr: 0.000000  loss: 1.8047 (1.8047)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5168 (8.5168)  time: 9.8783 (9.8783 -- 9.8783)  data: 9.3328 (9.3328 -- 9.3328)  max mem: 16413
Epoch: [171]  [ 20/160]  eta: 0:02:50  lr: 0.000001  min_lr: 0.000000  loss: 1.7872 (1.7534)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2367 (8.4930)  time: 0.7821 (0.5243 -- 4.6084)  data: 0.2351 (0.0005 -- 4.0869)  max mem: 16413
[2023-08-31 04:40:26,206] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:40:26,206] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:40:26,211] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:40:26,212] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [171]  [ 40/160]  eta: 0:01:57  lr: 0.000001  min_lr: 0.000000  loss: 1.9791 (1.8136)  loss_scale: 16384.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0062 (8.5356)  time: 0.7234 (0.5332 -- 2.0422)  data: 0.1457 (0.0007 -- 1.5343)  max mem: 16413
Epoch: [171]  [ 60/160]  eta: 0:01:35  lr: 0.000001  min_lr: 0.000000  loss: 1.6405 (1.7740)  loss_scale: 16384.0000 (12623.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2013 (8.7244)  time: 0.9069 (0.5340 -- 3.3679)  data: 0.1754 (0.0003 -- 2.8518)  max mem: 16413
Epoch: [171]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6320 (1.7662)  loss_scale: 16384.0000 (13552.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7086 (8.7784)  time: 0.9255 (0.5231 -- 2.8975)  data: 0.3746 (0.0004 -- 2.3529)  max mem: 16413
Epoch: [171]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7017 (1.7783)  loss_scale: 16384.0000 (14112.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4950 (8.7959)  time: 0.8861 (0.5195 -- 3.0091)  data: 0.3417 (0.0004 -- 2.4553)  max mem: 16413
Epoch: [171]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8367 (1.7871)  loss_scale: 16384.0000 (14488.3306)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0623 (8.7893)  time: 0.8416 (0.5301 -- 3.0932)  data: 0.2893 (0.0003 -- 2.5720)  max mem: 16413
Epoch: [171]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7012 (1.7795)  loss_scale: 16384.0000 (14757.2199)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0807 (8.7692)  time: 0.8880 (0.5266 -- 3.5694)  data: 0.3438 (0.0003 -- 3.0583)  max mem: 16413
[2023-08-31 04:42:15,277] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:42:15,277] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:42:15,278] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:42:15,278] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [171]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7621 (1.7803)  loss_scale: 16384.0000 (15360.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4635 (8.8759)  time: 0.6614 (0.4960 -- 3.3423)  data: 0.1426 (0.0001 -- 2.8355)  max mem: 16413
Epoch: [171] Total time: 0:02:21 (0.8854 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7621 (1.8024)  loss_scale: 16384.0000 (15360.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4635 (8.8759)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3386 (0.3386)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5881 (2.5881 -- 2.5881)  data: 2.3791 (2.3791 -- 2.3791)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4571 (0.7566)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4443 (0.2008 -- 2.5881)  data: 0.2370 (0.0004 -- 2.3791)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5325 (0.6833)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2190 (0.1688 -- 0.4157)  data: 0.0202 (0.0001 -- 0.2192)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5905 (0.7444)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2044 (0.1325 -- 0.4157)  data: 0.0199 (0.0001 -- 0.2192)  max mem: 16413
Val: Total time: 0:00:07 (0.2943 s / it)
* Acc@1 81.120 Acc@5 96.473 loss 0.702
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 81.74%
Epoch: [172]  [  0/160]  eta: 0:22:24  lr: 0.000001  min_lr: 0.000000  loss: 2.1694 (2.1694)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8515 (11.8515)  time: 8.4028 (8.4028 -- 8.4028)  data: 7.7519 (7.7519 -- 7.7519)  max mem: 16413
Epoch: [172]  [ 20/160]  eta: 0:02:53  lr: 0.000001  min_lr: 0.000000  loss: 1.8883 (1.8854)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7135 (10.1182)  time: 0.8775 (0.5229 -- 4.7444)  data: 0.1474 (0.0004 -- 1.6253)  max mem: 16413
[2023-08-31 04:42:59,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27548
[2023-08-31 04:42:59,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:42:59,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27548
[2023-08-31 04:42:59,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:42:59,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [172]  [ 40/160]  eta: 0:02:13  lr: 0.000001  min_lr: 0.000000  loss: 1.7997 (1.8195)  loss_scale: 16384.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7205 (9.1970)  time: 0.9782 (0.5238 -- 5.0778)  data: 0.0014 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [172]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.8458 (1.8268)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5718 (9.4325)  time: 0.7737 (0.5308 -- 3.1000)  data: 0.0015 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [172]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.9855 (1.8430)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7635 (9.5411)  time: 0.7558 (0.5161 -- 2.9939)  data: 0.1239 (0.0004 -- 2.4510)  max mem: 16413
Epoch: [172]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7889 (1.8414)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5601 (9.2948)  time: 0.9742 (0.5177 -- 3.4087)  data: 0.3229 (0.0002 -- 2.8974)  max mem: 16413
Epoch: [172]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 2.0491 (1.8729)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1403 (9.3573)  time: 0.8998 (0.5160 -- 4.5985)  data: 0.3498 (0.0002 -- 4.0656)  max mem: 16413
[2023-08-31 04:44:31,740] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27656
[2023-08-31 04:44:31,741] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:44:31,741] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27656
[2023-08-31 04:44:31,741] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:44:31,741] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [172]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.4984 (1.8450)  loss_scale: 16384.0000 (19347.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5860 (9.3543)  time: 0.9250 (0.5234 -- 3.3153)  data: 0.3716 (0.0002 -- 2.7461)  max mem: 16413
Epoch: [172]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7945 (1.8382)  loss_scale: 8192.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0632 (9.3574)  time: 0.6377 (0.4945 -- 1.7590)  data: 0.1187 (0.0003 -- 1.2461)  max mem: 16413
Epoch: [172] Total time: 0:02:24 (0.9018 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7945 (1.8075)  loss_scale: 8192.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0632 (9.3574)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3399 (0.3399)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3419 (2.3419 -- 2.3419)  data: 2.1160 (2.1160 -- 2.1160)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4571 (0.7560)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4351 (0.1991 -- 2.3419)  data: 0.2199 (0.0008 -- 2.1160)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5323 (0.6824)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2246 (0.1691 -- 0.4803)  data: 0.0212 (0.0001 -- 0.2919)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5895 (0.7435)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2083 (0.1328 -- 0.4803)  data: 0.0208 (0.0001 -- 0.2919)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 81.328 Acc@5 96.473 loss 0.702
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [173]  [  0/160]  eta: 0:21:14  lr: 0.000001  min_lr: 0.000000  loss: 2.0106 (2.0106)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8935 (6.8935)  time: 7.9662 (7.9662 -- 7.9662)  data: 5.0835 (5.0835 -- 5.0835)  max mem: 16413
Epoch: [173]  [ 20/160]  eta: 0:02:54  lr: 0.000001  min_lr: 0.000000  loss: 1.9760 (1.9285)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4848 (8.2258)  time: 0.9082 (0.5239 -- 2.7995)  data: 0.0016 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [173]  [ 40/160]  eta: 0:02:08  lr: 0.000001  min_lr: 0.000000  loss: 1.8342 (1.8755)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2461 (8.5972)  time: 0.8879 (0.5192 -- 3.4978)  data: 0.0013 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [173]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.5016 (1.8163)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3538 (8.7617)  time: 0.8258 (0.5221 -- 3.6523)  data: 0.0013 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [173]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.6964 (1.7943)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1232 (8.4193)  time: 0.8818 (0.5211 -- 3.8234)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [173]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.7061 (1.7799)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3871 (8.5051)  time: 0.9605 (0.5036 -- 4.8916)  data: 0.0012 (0.0003 -- 0.0035)  max mem: 16413
[2023-08-31 04:46:38,175] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:46:38,175] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:46:38,176] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:46:38,176] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [173]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8750 (1.7828)  loss_scale: 16384.0000 (9275.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0128 (8.5852)  time: 0.7606 (0.5348 -- 3.2958)  data: 0.0015 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [173]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.9747 (1.8040)  loss_scale: 16384.0000 (10283.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3426 (8.4888)  time: 0.8566 (0.5313 -- 4.1940)  data: 0.0017 (0.0004 -- 0.0041)  max mem: 16413
Epoch: [173]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8013 (1.8027)  loss_scale: 16384.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3457 (8.5085)  time: 0.6535 (0.4971 -- 2.9142)  data: 0.0011 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [173] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8013 (1.8051)  loss_scale: 16384.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3457 (8.5085)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3366 (0.3366)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4334 (2.4334 -- 2.4334)  data: 2.1943 (2.1943 -- 2.1943)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4499 (0.7539)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4496 (0.2063 -- 2.4334)  data: 0.2356 (0.0007 -- 2.1943)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5375 (0.6808)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2241 (0.1703 -- 0.5870)  data: 0.0200 (0.0001 -- 0.3815)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5904 (0.7415)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2092 (0.1327 -- 0.5870)  data: 0.0195 (0.0001 -- 0.3815)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 81.328 Acc@5 96.888 loss 0.700
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [174]  [  0/160]  eta: 0:17:16  lr: 0.000001  min_lr: 0.000000  loss: 1.6192 (1.6192)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1236 (6.1236)  time: 6.4810 (6.4810 -- 6.4810)  data: 5.9475 (5.9475 -- 5.9475)  max mem: 16413
Epoch: [174]  [ 20/160]  eta: 0:02:33  lr: 0.000001  min_lr: 0.000000  loss: 1.7134 (1.7209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5582 (8.2873)  time: 0.8262 (0.5305 -- 2.2843)  data: 0.0017 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [174]  [ 40/160]  eta: 0:01:59  lr: 0.000001  min_lr: 0.000000  loss: 1.9769 (1.8017)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3759 (8.3989)  time: 0.8935 (0.5284 -- 2.4656)  data: 0.1704 (0.0004 -- 1.7038)  max mem: 16413
Epoch: [174]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.8448 (1.8040)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2946 (8.4776)  time: 0.9497 (0.5333 -- 3.8702)  data: 0.2981 (0.0002 -- 3.3583)  max mem: 16413
[2023-08-31 04:48:37,316] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:48:37,317] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:48:37,318] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:48:37,319] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [174]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.6847 (1.7745)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9915 (8.6010)  time: 0.8663 (0.5185 -- 4.0649)  data: 0.1235 (0.0004 -- 2.4432)  max mem: 16413
[2023-08-31 04:48:57,370] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27934
[2023-08-31 04:48:57,370] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:48:57,370] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27934
[2023-08-31 04:48:57,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 04:48:57,370] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [174]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6463 (1.7630)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6496 (8.7262)  time: 0.9202 (0.5156 -- 3.6964)  data: 0.0018 (0.0002 -- 0.0113)  max mem: 16413
Epoch: [174]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8482 (1.7745)  loss_scale: 16384.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1314 (8.7729)  time: 0.8023 (0.5203 -- 3.3208)  data: 0.0970 (0.0002 -- 1.2085)  max mem: 16413
Epoch: [174]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7404 (1.7889)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5455 (8.7204)  time: 0.9126 (0.5228 -- 4.5051)  data: 0.0977 (0.0004 -- 1.9288)  max mem: 16413
[2023-08-31 04:49:48,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=164, lr=[2.3358190854927335e-08, 2.3358190854927335e-08, 3.1144254473236446e-08, 3.1144254473236446e-08, 4.1525672630981926e-08, 4.1525672630981926e-08, 5.53675635079759e-08, 5.53675635079759e-08, 7.382341801063453e-08, 7.382341801063453e-08, 9.843122401417938e-08, 9.843122401417938e-08, 1.3124163201890585e-07, 1.3124163201890585e-07, 1.7498884269187445e-07, 1.7498884269187445e-07, 2.3331845692249928e-07, 2.3331845692249928e-07, 3.110912758966657e-07, 3.110912758966657e-07, 4.147883678622209e-07, 4.147883678622209e-07, 5.53051157149628e-07, 5.53051157149628e-07, 7.374015428661706e-07, 7.374015428661706e-07, 9.83202057154894e-07, 9.83202057154894e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 04:49:48,407] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=17.2504317331834, CurrSamplesPerSec=24.662076426904058, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [174]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7581 (1.7892)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3140 (8.7330)  time: 0.6079 (0.4974 -- 1.8397)  data: 0.0008 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [174] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7581 (1.7921)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3140 (8.7330)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3338 (0.3338)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6178 (2.6178 -- 2.6178)  data: 2.3767 (2.3767 -- 2.3767)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4541 (0.7547)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4295 (0.2003 -- 2.6178)  data: 0.2172 (0.0006 -- 2.3767)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5367 (0.6812)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2121 (0.1703 -- 0.3373)  data: 0.0077 (0.0001 -- 0.1373)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5894 (0.7420)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.1976 (0.1328 -- 0.3373)  data: 0.0073 (0.0001 -- 0.1373)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 81.743 Acc@5 96.680 loss 0.700
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 81.74%
Epoch: [175]  [  0/160]  eta: 0:17:52  lr: 0.000001  min_lr: 0.000000  loss: 1.8257 (1.8257)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8498 (6.8498)  time: 6.7043 (6.7043 -- 6.7043)  data: 5.5372 (5.5372 -- 5.5372)  max mem: 16413
Epoch: [175]  [ 20/160]  eta: 0:02:37  lr: 0.000001  min_lr: 0.000000  loss: 1.6369 (1.7182)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4965 (8.8966)  time: 0.8464 (0.5348 -- 2.7407)  data: 0.1556 (0.0003 -- 2.2025)  max mem: 16413
Epoch: [175]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.7175 (1.7067)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0397 (8.8353)  time: 0.9072 (0.5278 -- 3.3752)  data: 0.2264 (0.0009 -- 2.0621)  max mem: 16413
Epoch: [175]  [ 60/160]  eta: 0:01:35  lr: 0.000001  min_lr: 0.000000  loss: 1.6730 (1.7556)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6899 (9.0326)  time: 0.8161 (0.5287 -- 2.6225)  data: 0.0146 (0.0002 -- 0.2585)  max mem: 16413
[2023-08-31 04:51:00,223] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:51:00,223] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:51:00,224] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:51:00,224] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 04:51:06,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28069
[2023-08-31 04:51:06,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28069
[2023-08-31 04:51:06,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:51:06,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 04:51:06,700] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [175]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.8436 (1.7740)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5097 (8.7436)  time: 0.9938 (0.5198 -- 4.7512)  data: 0.0012 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [175]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7384 (1.7688)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0151 (8.6723)  time: 0.8832 (0.5209 -- 4.1918)  data: 0.0017 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [175]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8050 (1.7608)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2735 (8.6887)  time: 0.9383 (0.5126 -- 4.3698)  data: 0.0010 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [175]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5625 (1.7315)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7328 (8.6590)  time: 0.7185 (0.5255 -- 3.8163)  data: 0.0015 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [175]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7853 (1.7433)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3480 (8.6287)  time: 0.7574 (0.4963 -- 4.2409)  data: 0.0006 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [175] Total time: 0:02:23 (0.8964 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7853 (1.7830)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3480 (8.6287)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3343 (0.3343)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3893 (2.3893 -- 2.3893)  data: 2.1374 (2.1374 -- 2.1374)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4551 (0.7548)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4428 (0.2088 -- 2.3893)  data: 0.2187 (0.0006 -- 2.1374)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5337 (0.6811)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2239 (0.1704 -- 0.4705)  data: 0.0136 (0.0001 -- 0.2575)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5893 (0.7420)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2058 (0.1333 -- 0.4705)  data: 0.0133 (0.0001 -- 0.2575)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 81.743 Acc@5 96.680 loss 0.700
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 81.74%
Epoch: [176]  [  0/160]  eta: 0:15:39  lr: 0.000001  min_lr: 0.000000  loss: 2.2396 (2.2396)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8717 (6.8717)  time: 5.8718 (5.8718 -- 5.8718)  data: 4.8992 (4.8992 -- 4.8992)  max mem: 16413
Epoch: [176]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.9092 (1.9234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8549 (8.1768)  time: 0.9442 (0.5289 -- 3.3923)  data: 0.3969 (0.0005 -- 2.8722)  max mem: 16413
[2023-08-31 04:52:59,212] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28187
[2023-08-31 04:52:59,212] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28187
[2023-08-31 04:52:59,212] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:52:59,212] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:52:59,212] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [176]  [ 40/160]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8031 (1.8314)  loss_scale: 8192.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2020 (8.6407)  time: 0.8125 (0.4957 -- 3.7078)  data: 0.2666 (0.0004 -- 3.1860)  max mem: 16413
Epoch: [176]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8206 (1.8156)  loss_scale: 8192.0000 (11817.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3512 (8.7217)  time: 0.9121 (0.5317 -- 2.5908)  data: 0.3508 (0.0004 -- 2.0404)  max mem: 16413
Epoch: [176]  [ 80/160]  eta: 0:01:13  lr: 0.000001  min_lr: 0.000000  loss: 1.9567 (1.8471)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9183 (8.7507)  time: 0.7427 (0.5291 -- 2.9445)  data: 0.0414 (0.0009 -- 0.7939)  max mem: 16413
Epoch: [176]  [100/160]  eta: 0:00:54  lr: 0.000001  min_lr: 0.000000  loss: 1.8384 (1.8564)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4211 (8.7432)  time: 0.9215 (0.5323 -- 3.4648)  data: 0.0228 (0.0002 -- 0.4303)  max mem: 16413
Epoch: [176]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7814 (1.8531)  loss_scale: 8192.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7972 (8.8372)  time: 1.0136 (0.5308 -- 4.4474)  data: 0.0015 (0.0002 -- 0.0056)  max mem: 16413
Epoch: [176]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6939 (1.8381)  loss_scale: 8192.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0432 (8.9117)  time: 0.8239 (0.5163 -- 4.2378)  data: 0.0012 (0.0004 -- 0.0046)  max mem: 16413
[2023-08-31 04:54:47,973] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:54:47,973] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:54:47,973] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:54:47,973] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [176]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.9116 (1.8349)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3104 (8.8400)  time: 0.6596 (0.4967 -- 2.7374)  data: 0.0013 (0.0003 -- 0.0162)  max mem: 16413
Epoch: [176] Total time: 0:02:21 (0.8871 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.9116 (1.8050)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3104 (8.8400)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3355 (0.3355)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3600 (2.3600 -- 2.3600)  data: 2.1172 (2.1172 -- 2.1172)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4535 (0.7534)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4261 (0.1976 -- 2.3600)  data: 0.2055 (0.0006 -- 2.1172)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5340 (0.6801)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2282 (0.1696 -- 0.3968)  data: 0.0169 (0.0001 -- 0.1916)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5896 (0.7413)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2108 (0.1323 -- 0.3968)  data: 0.0166 (0.0001 -- 0.1916)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 81.328 Acc@5 96.680 loss 0.699
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [177]  [  0/160]  eta: 0:23:42  lr: 0.000001  min_lr: 0.000000  loss: 1.9548 (1.9548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7955 (6.7955)  time: 8.8930 (8.8930 -- 8.8930)  data: 6.2415 (6.2415 -- 6.2415)  max mem: 16413
Epoch: [177]  [ 20/160]  eta: 0:02:44  lr: 0.000001  min_lr: 0.000000  loss: 1.8342 (1.7806)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2991 (9.3496)  time: 0.7921 (0.5336 -- 2.9725)  data: 0.2405 (0.0003 -- 2.4350)  max mem: 16413
Epoch: [177]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.8546 (1.8011)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1336 (9.2739)  time: 0.9443 (0.5174 -- 3.9760)  data: 0.2748 (0.0004 -- 2.5068)  max mem: 16413
[2023-08-31 04:55:43,798] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28365
[2023-08-31 04:55:43,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28365
[2023-08-31 04:55:43,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:55:43,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:55:43,800] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [177]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.8697 (1.8450)  loss_scale: 8192.0000 (14235.2787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1193 (9.1506)  time: 0.8199 (0.5262 -- 3.9077)  data: 0.0094 (0.0004 -- 0.1596)  max mem: 16413
Epoch: [177]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.4757 (1.7858)  loss_scale: 8192.0000 (12743.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1043 (9.3569)  time: 0.9620 (0.5137 -- 3.9391)  data: 0.0014 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [177]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.9162 (1.8145)  loss_scale: 8192.0000 (11841.9010)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8248 (9.4412)  time: 0.7894 (0.5379 -- 3.6966)  data: 0.0016 (0.0006 -- 0.0038)  max mem: 16413
Epoch: [177]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7058 (1.8033)  loss_scale: 8192.0000 (11238.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9254 (9.3937)  time: 0.9503 (0.5227 -- 3.2731)  data: 0.0020 (0.0004 -- 0.0151)  max mem: 16413
Epoch: [177]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7373 (1.7998)  loss_scale: 8192.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8502 (9.1960)  time: 0.7207 (0.5169 -- 2.4321)  data: 0.0018 (0.0004 -- 0.0119)  max mem: 16413
Epoch: [177]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8720 (1.8031)  loss_scale: 8192.0000 (10496.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6987 (9.1935)  time: 0.6823 (0.4982 -- 3.2882)  data: 0.0009 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [177] Total time: 0:02:21 (0.8829 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8720 (1.8101)  loss_scale: 8192.0000 (10496.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6987 (9.1935)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3336 (0.3336)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4223 (2.4223 -- 2.4223)  data: 2.1531 (2.1531 -- 2.1531)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4572 (0.7534)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4244 (0.2066 -- 2.4223)  data: 0.1996 (0.0005 -- 2.1531)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5302 (0.6797)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2151 (0.1704 -- 0.2428)  data: 0.0038 (0.0001 -- 0.0329)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5899 (0.7410)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.1971 (0.1328 -- 0.2428)  data: 0.0036 (0.0001 -- 0.0329)  max mem: 16413
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.699
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [178]  [  0/160]  eta: 0:18:09  lr: 0.000001  min_lr: 0.000000  loss: 2.3154 (2.3154)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7105 (8.7105)  time: 6.8086 (6.8086 -- 6.8086)  data: 6.2600 (6.2600 -- 6.2600)  max mem: 16413
[2023-08-31 04:57:43,639] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:57:43,639] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:57:43,639] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:57:43,639] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [178]  [ 20/160]  eta: 0:02:43  lr: 0.000001  min_lr: 0.000000  loss: 1.8843 (1.8223)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2612 (8.6560)  time: 0.8841 (0.5283 -- 2.5168)  data: 0.1612 (0.0005 -- 1.9433)  max mem: 16413
[2023-08-31 04:57:59,123] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28510
[2023-08-31 04:57:59,123] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28510
[2023-08-31 04:57:59,124] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:57:59,124] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 04:57:59,124] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [178]  [ 40/160]  eta: 0:02:04  lr: 0.000001  min_lr: 0.000000  loss: 1.9156 (1.8212)  loss_scale: 8192.0000 (11388.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0106 (8.4767)  time: 0.9036 (0.5213 -- 3.1466)  data: 0.2282 (0.0004 -- 2.5895)  max mem: 16413
Epoch: [178]  [ 60/160]  eta: 0:01:32  lr: 0.000001  min_lr: 0.000000  loss: 1.6858 (1.7868)  loss_scale: 8192.0000 (10340.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5359 (8.3812)  time: 0.7009 (0.5267 -- 1.5308)  data: 0.0527 (0.0004 -- 1.0151)  max mem: 16413
Epoch: [178]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.8145 (1.7931)  loss_scale: 8192.0000 (9810.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8123 (8.4824)  time: 0.9300 (0.5214 -- 2.5039)  data: 0.1039 (0.0005 -- 0.9557)  max mem: 16413
Epoch: [178]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.7184 (1.7930)  loss_scale: 8192.0000 (9489.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2524 (8.4615)  time: 0.9214 (0.5299 -- 3.8939)  data: 0.0794 (0.0006 -- 0.7605)  max mem: 16413
Epoch: [178]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.9688 (1.8171)  loss_scale: 8192.0000 (9275.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9793 (8.6763)  time: 0.8795 (0.5342 -- 2.4202)  data: 0.0865 (0.0003 -- 1.4552)  max mem: 16413
Epoch: [178]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.9455 (1.8211)  loss_scale: 8192.0000 (9121.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4569 (8.7316)  time: 0.8939 (0.5196 -- 4.5661)  data: 0.3186 (0.0001 -- 4.0407)  max mem: 16413
[2023-08-31 04:59:47,714] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:59:47,714] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 04:59:47,715] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 04:59:47,715] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [178]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8461 (1.8301)  loss_scale: 8192.0000 (9062.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8325 (8.7046)  time: 0.6368 (0.4938 -- 2.3944)  data: 0.1170 (0.0002 -- 1.8279)  max mem: 16413
Epoch: [178] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8461 (1.8151)  loss_scale: 8192.0000 (9062.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8325 (8.7046)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3362 (0.3362)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4064 (2.4064 -- 2.4064)  data: 2.1607 (2.1607 -- 2.1607)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4594 (0.7535)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4189 (0.1979 -- 2.4064)  data: 0.2052 (0.0007 -- 2.1607)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5298 (0.6797)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2201 (0.1702 -- 0.4630)  data: 0.0185 (0.0001 -- 0.2704)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5888 (0.7408)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2058 (0.1327 -- 0.4630)  data: 0.0182 (0.0001 -- 0.2704)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.699
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [179]  [  0/160]  eta: 0:18:21  lr: 0.000001  min_lr: 0.000000  loss: 1.3373 (1.3373)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7978 (3.7978)  time: 6.8869 (6.8869 -- 6.8869)  data: 6.3755 (6.3755 -- 6.3755)  max mem: 16413
Epoch: [179]  [ 20/160]  eta: 0:02:51  lr: 0.000001  min_lr: 0.000000  loss: 1.8477 (1.8129)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1368 (8.6098)  time: 0.9437 (0.5328 -- 3.0729)  data: 0.2527 (0.0005 -- 2.5313)  max mem: 16413
Epoch: [179]  [ 40/160]  eta: 0:02:08  lr: 0.000001  min_lr: 0.000000  loss: 1.6436 (1.7889)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7487 (8.4362)  time: 0.9079 (0.5149 -- 4.1558)  data: 0.3668 (0.0002 -- 3.5860)  max mem: 16413
[2023-08-31 05:00:43,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28687
[2023-08-31 05:00:43,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:00:43,605] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 05:00:43,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28687
[2023-08-31 05:00:43,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [179]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8053 (1.8004)  loss_scale: 8192.0000 (14503.8689)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9920 (8.4208)  time: 0.7357 (0.5373 -- 2.1365)  data: 0.1561 (0.0004 -- 1.6123)  max mem: 16413
Epoch: [179]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.8077 (1.8028)  loss_scale: 8192.0000 (12945.3827)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9597 (8.4502)  time: 0.9274 (0.5342 -- 3.2300)  data: 0.3602 (0.0009 -- 2.6814)  max mem: 16413
Epoch: [179]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.8810 (1.8118)  loss_scale: 8192.0000 (12004.1188)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5633 (8.7925)  time: 0.9782 (0.5156 -- 4.2928)  data: 0.4391 (0.0003 -- 3.7554)  max mem: 16413
Epoch: [179]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.9411 (1.8281)  loss_scale: 8192.0000 (11374.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3824 (8.8629)  time: 0.8542 (0.5316 -- 3.5627)  data: 0.3045 (0.0003 -- 3.0450)  max mem: 16413
Epoch: [179]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6189 (1.8056)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0420 (8.8421)  time: 0.9768 (0.5247 -- 4.5182)  data: 0.4334 (0.0004 -- 4.0048)  max mem: 16413
Epoch: [179]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8169 (1.7957)  loss_scale: 8192.0000 (10598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9006 (8.8832)  time: 0.6140 (0.4949 -- 1.8187)  data: 0.0995 (0.0002 -- 1.3274)  max mem: 16413
Epoch: [179] Total time: 0:02:25 (0.9072 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8169 (1.8072)  loss_scale: 8192.0000 (10598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9006 (8.8832)
[2023-08-31 05:02:20,692] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-08-31 05:02:20,694] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-08-31 05:02:20,694] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt
[2023-08-31 05:02:20,694] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt...
[2023-08-31 05:02:21,645] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt.
[2023-08-31 05:02:21,646] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3348 (0.3348)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6281 (2.6281 -- 2.6281)  data: 2.3983 (2.3983 -- 2.3983)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4592 (0.7528)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4497 (0.2015 -- 2.6281)  data: 0.2347 (0.0010 -- 2.3983)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5288 (0.6791)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2168 (0.1715 -- 0.3985)  data: 0.0093 (0.0001 -- 0.1712)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5873 (0.7402)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2018 (0.1332 -- 0.3985)  data: 0.0089 (0.0001 -- 0.1712)  max mem: 16413
Val: Total time: 0:00:07 (0.2945 s / it)
* Acc@1 81.743 Acc@5 96.473 loss 0.699
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 81.74%
Epoch: [180]  [  0/160]  eta: 0:21:35  lr: 0.000001  min_lr: 0.000000  loss: 1.8523 (1.8523)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3893 (8.3893)  time: 8.0966 (8.0966 -- 8.0966)  data: 5.5627 (5.5627 -- 5.5627)  max mem: 16413
[2023-08-31 05:02:50,417] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:02:50,418] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 05:02:50,419] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:02:50,419] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [180]  [ 20/160]  eta: 0:02:38  lr: 0.000001  min_lr: 0.000000  loss: 1.9149 (1.9159)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1791 (8.7799)  time: 0.7874 (0.5340 -- 2.9787)  data: 0.2406 (0.0004 -- 2.4397)  max mem: 16413
Epoch: [180]  [ 40/160]  eta: 0:02:11  lr: 0.000001  min_lr: 0.000000  loss: 1.9042 (1.8513)  loss_scale: 16384.0000 (13187.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8145 (8.9007)  time: 1.0602 (0.5254 -- 5.1311)  data: 0.5133 (0.0002 -- 4.5854)  max mem: 16413
[2023-08-31 05:03:17,818] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28846
[2023-08-31 05:03:17,818] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28846
[2023-08-31 05:03:17,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:03:17,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:03:17,819] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [180]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.8858 (1.8414)  loss_scale: 8192.0000 (12220.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3522 (9.0675)  time: 0.7552 (0.5110 -- 4.0982)  data: 0.2137 (0.0004 -- 3.5715)  max mem: 16413
Epoch: [180]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7905 (1.8367)  loss_scale: 8192.0000 (11226.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0413 (9.1429)  time: 0.9859 (0.5247 -- 3.8509)  data: 0.4349 (0.0004 -- 3.3220)  max mem: 16413
Epoch: [180]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6888 (1.8016)  loss_scale: 8192.0000 (10625.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8876 (9.0989)  time: 0.7403 (0.5103 -- 2.8487)  data: 0.1961 (0.0001 -- 2.2795)  max mem: 16413
Epoch: [180]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6441 (1.7845)  loss_scale: 8192.0000 (10223.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8984 (9.1053)  time: 0.8209 (0.5429 -- 3.1584)  data: 0.0607 (0.0003 -- 1.1832)  max mem: 16413
Epoch: [180]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7373 (1.7811)  loss_scale: 8192.0000 (9934.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4936 (9.0891)  time: 0.8244 (0.5311 -- 1.9316)  data: 0.1510 (0.0004 -- 1.2504)  max mem: 16413
Epoch: [180]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7275 (1.7819)  loss_scale: 8192.0000 (9728.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8432 (8.9183)  time: 0.7626 (0.4956 -- 2.5751)  data: 0.2435 (0.0002 -- 2.0330)  max mem: 16413
Epoch: [180] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7275 (1.7974)  loss_scale: 8192.0000 (9728.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8432 (8.9183)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3334 (0.3334)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3325 (2.3325 -- 2.3325)  data: 2.1248 (2.1248 -- 2.1248)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4612 (0.7539)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4581 (0.1950 -- 2.3325)  data: 0.2510 (0.0004 -- 2.1248)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5306 (0.6800)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2291 (0.1695 -- 0.8279)  data: 0.0319 (0.0001 -- 0.6231)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5870 (0.7415)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2157 (0.1326 -- 0.8279)  data: 0.0314 (0.0001 -- 0.6231)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.700
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [181]  [  0/160]  eta: 0:20:47  lr: 0.000001  min_lr: 0.000000  loss: 2.0943 (2.0943)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6505 (7.6505)  time: 7.7990 (7.7990 -- 7.7990)  data: 6.6146 (6.6146 -- 6.6146)  max mem: 16413
[2023-08-31 05:05:18,194] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:05:18,194] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:05:18,194] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 05:05:18,194] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [181]  [ 20/160]  eta: 0:02:48  lr: 0.000001  min_lr: 0.000000  loss: 1.8088 (1.8281)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4333 (7.8643)  time: 0.8754 (0.5260 -- 4.7370)  data: 0.2912 (0.0005 -- 4.2099)  max mem: 16413
[2023-08-31 05:05:39,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=170, lr=[1.3702691449281511e-08, 1.3702691449281511e-08, 1.8270255265708683e-08, 1.8270255265708683e-08, 2.4360340354278244e-08, 2.4360340354278244e-08, 3.248045380570432e-08, 3.248045380570432e-08, 4.33072717409391e-08, 4.33072717409391e-08, 5.77430289879188e-08, 5.77430289879188e-08, 7.699070531722507e-08, 7.699070531722507e-08, 1.0265427375630009e-07, 1.0265427375630009e-07, 1.3687236500840012e-07, 1.3687236500840012e-07, 1.8249648667786682e-07, 1.8249648667786682e-07, 2.4332864890382244e-07, 2.4332864890382244e-07, 3.244381985384299e-07, 3.244381985384299e-07, 4.3258426471790657e-07, 4.3258426471790657e-07, 5.767790196238754e-07, 5.767790196238754e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 05:05:39,965] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=17.265259541368145, CurrSamplesPerSec=23.005482649095374, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [181]  [ 40/160]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7674 (1.7862)  loss_scale: 16384.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5426 (8.0733)  time: 0.7999 (0.5362 -- 2.3201)  data: 0.2485 (0.0002 -- 1.7733)  max mem: 16413
Epoch: [181]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.9161 (1.8374)  loss_scale: 16384.0000 (14369.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2802 (8.2489)  time: 0.9525 (0.5186 -- 3.5442)  data: 0.4108 (0.0004 -- 3.0048)  max mem: 16413
Epoch: [181]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.8374 (1.8557)  loss_scale: 16384.0000 (14866.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2848 (8.3323)  time: 0.7355 (0.5243 -- 3.4466)  data: 0.1865 (0.0003 -- 2.9139)  max mem: 16413
Epoch: [181]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.7939 (1.8513)  loss_scale: 16384.0000 (15167.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8342 (8.4860)  time: 0.9543 (0.5397 -- 3.4596)  data: 0.3958 (0.0004 -- 2.9421)  max mem: 16413
Epoch: [181]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8580 (1.8473)  loss_scale: 16384.0000 (15368.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8787 (8.5123)  time: 0.8214 (0.5248 -- 3.9012)  data: 0.2727 (0.0002 -- 3.3785)  max mem: 16413
Epoch: [181]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6878 (1.8291)  loss_scale: 16384.0000 (15512.5106)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2323 (8.4053)  time: 0.9553 (0.5343 -- 4.9502)  data: 0.4031 (0.0003 -- 4.4287)  max mem: 16413
[2023-08-31 05:07:11,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:07:11,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:07:11,194] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:07:11,194] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [181]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7325 (1.8211)  loss_scale: 32768.0000 (17356.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2683 (8.4604)  time: 0.6082 (0.4968 -- 2.3517)  data: 0.0934 (0.0001 -- 1.8565)  max mem: 16413
Epoch: [181] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7325 (1.8185)  loss_scale: 32768.0000 (17356.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2683 (8.4604)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3332 (0.3332)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3193 (2.3193 -- 2.3193)  data: 2.0737 (2.0737 -- 2.0737)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4619 (0.7538)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4173 (0.1953 -- 2.3193)  data: 0.1971 (0.0008 -- 2.0737)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5308 (0.6799)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2221 (0.1690 -- 0.3856)  data: 0.0140 (0.0001 -- 0.1833)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5876 (0.7414)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2075 (0.1327 -- 0.3856)  data: 0.0137 (0.0001 -- 0.1833)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.700
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [182]  [  0/160]  eta: 0:20:49  lr: 0.000001  min_lr: 0.000000  loss: 1.7711 (1.7711)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3966 (12.3966)  time: 7.8104 (7.8104 -- 7.8104)  data: 7.2604 (7.2604 -- 7.2604)  max mem: 16413
[2023-08-31 05:07:49,296] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29134
[2023-08-31 05:07:49,296] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29134
[2023-08-31 05:07:49,296] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:07:49,296] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:07:49,296] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [182]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.8493 (1.8733)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0716 (8.8889)  time: 0.8514 (0.5289 -- 3.7336)  data: 0.2981 (0.0009 -- 3.2024)  max mem: 16413
Epoch: [182]  [ 40/160]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8389 (1.8572)  loss_scale: 16384.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4859 (8.9080)  time: 0.8175 (0.5427 -- 1.7235)  data: 0.2673 (0.0004 -- 1.1802)  max mem: 16413
Epoch: [182]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6207 (1.7954)  loss_scale: 16384.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1805 (9.3075)  time: 0.8919 (0.5251 -- 3.2206)  data: 0.3469 (0.0006 -- 2.7006)  max mem: 16413
Epoch: [182]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.8937 (1.8110)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2423 (9.2462)  time: 0.8809 (0.5311 -- 3.8441)  data: 0.2381 (0.0005 -- 3.2923)  max mem: 16413
Epoch: [182]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.8014 (1.7946)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5182 (9.1665)  time: 0.9972 (0.5275 -- 4.1362)  data: 0.4502 (0.0005 -- 3.6039)  max mem: 16413
Epoch: [182]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8683 (1.8076)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7991 (9.1755)  time: 0.7826 (0.5248 -- 3.7854)  data: 0.1372 (0.0004 -- 1.3990)  max mem: 16413
Epoch: [182]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7299 (1.7930)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8732 (9.1164)  time: 0.8547 (0.5439 -- 3.8703)  data: 0.0349 (0.0002 -- 0.3643)  max mem: 16413
[2023-08-31 05:09:39,896] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:09:39,897] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:09:39,897] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:09:39,897] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [182]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8486 (1.8178)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7325 (9.1242)  time: 0.6249 (0.4979 -- 2.0159)  data: 0.0009 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [182] Total time: 0:02:21 (0.8832 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8486 (1.8165)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7325 (9.1242)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3331 (0.3331)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4214 (2.4214 -- 2.4214)  data: 2.1903 (2.1903 -- 2.1903)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4619 (0.7541)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4543 (0.1963 -- 2.4214)  data: 0.2407 (0.0005 -- 2.1903)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5324 (0.6800)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2258 (0.1704 -- 0.6740)  data: 0.0231 (0.0001 -- 0.4243)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5880 (0.7418)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2110 (0.1325 -- 0.6740)  data: 0.0219 (0.0001 -- 0.4243)  max mem: 16413
Val: Total time: 0:00:07 (0.2955 s / it)
* Acc@1 81.743 Acc@5 96.473 loss 0.701
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 81.74%
Epoch: [183]  [  0/160]  eta: 0:19:21  lr: 0.000000  min_lr: 0.000000  loss: 1.7572 (1.7572)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8393 (7.8393)  time: 7.2580 (7.2580 -- 7.2580)  data: 6.7211 (6.7211 -- 6.7211)  max mem: 16413
[2023-08-31 05:10:09,154] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29286
[2023-08-31 05:10:09,154] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29286
[2023-08-31 05:10:09,155] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:10:09,155] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:10:09,155] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [183]  [ 20/160]  eta: 0:02:52  lr: 0.000000  min_lr: 0.000000  loss: 1.8258 (1.8123)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2195 (9.1801)  time: 0.9327 (0.5165 -- 3.7237)  data: 0.3804 (0.0005 -- 3.1857)  max mem: 16413
Epoch: [183]  [ 40/160]  eta: 0:01:59  lr: 0.000000  min_lr: 0.000000  loss: 1.7501 (1.7900)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0375 (8.9914)  time: 0.7480 (0.5302 -- 2.0925)  data: 0.0929 (0.0003 -- 1.5477)  max mem: 16413
[2023-08-31 05:10:55,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29338
[2023-08-31 05:10:55,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29338
[2023-08-31 05:10:55,244] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:10:55,244] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:10:55,244] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [183]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8753 (1.8217)  loss_scale: 16384.0000 (17592.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3764 (9.0807)  time: 0.9212 (0.5336 -- 3.1799)  data: 0.3325 (0.0005 -- 2.6661)  max mem: 16413
Epoch: [183]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8735 (1.8215)  loss_scale: 8192.0000 (15271.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1324 (9.2196)  time: 0.8373 (0.5339 -- 3.9031)  data: 0.2603 (0.0003 -- 3.3316)  max mem: 16413
Epoch: [183]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8193 (1.8323)  loss_scale: 8192.0000 (13869.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3026 (9.2093)  time: 0.9301 (0.5288 -- 2.9084)  data: 0.1758 (0.0004 -- 1.8994)  max mem: 16413
Epoch: [183]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7181 (1.8103)  loss_scale: 8192.0000 (12931.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0860 (9.0068)  time: 0.7677 (0.5181 -- 1.8485)  data: 0.1414 (0.0005 -- 1.3361)  max mem: 16413
Epoch: [183]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8173 (1.8059)  loss_scale: 8192.0000 (12258.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8588 (9.1310)  time: 0.8959 (0.5248 -- 3.3206)  data: 0.2825 (0.0005 -- 2.7968)  max mem: 16413
Epoch: [183]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8114 (1.8092)  loss_scale: 8192.0000 (11776.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4976 (9.2082)  time: 0.7105 (0.4946 -- 2.5313)  data: 0.0205 (0.0002 -- 0.3963)  max mem: 16413
Epoch: [183] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8114 (1.8018)  loss_scale: 8192.0000 (11776.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4976 (9.2082)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3325 (0.3325)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5956 (2.5956 -- 2.5956)  data: 2.3786 (2.3786 -- 2.3786)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4601 (0.7528)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4535 (0.2002 -- 2.5956)  data: 0.2416 (0.0005 -- 2.3786)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5352 (0.6796)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2177 (0.1716 -- 0.4840)  data: 0.0141 (0.0001 -- 0.2700)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5874 (0.7410)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2032 (0.1337 -- 0.4840)  data: 0.0138 (0.0001 -- 0.2700)  max mem: 16413
Val: Total time: 0:00:07 (0.2941 s / it)
* Acc@1 81.743 Acc@5 96.680 loss 0.700
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 81.74%
Epoch: [184]  [  0/160]  eta: 0:21:49  lr: 0.000000  min_lr: 0.000000  loss: 1.5461 (1.5461)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1911 (11.1911)  time: 8.1834 (8.1834 -- 8.1834)  data: 7.6452 (7.6452 -- 7.6452)  max mem: 16413
Epoch: [184]  [ 20/160]  eta: 0:02:40  lr: 0.000000  min_lr: 0.000000  loss: 1.6535 (1.7100)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0764 (10.2931)  time: 0.7927 (0.5340 -- 2.8621)  data: 0.0878 (0.0005 -- 0.9871)  max mem: 16413
[2023-08-31 05:12:58,176] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:12:58,176] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 05:12:58,177] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:12:58,177] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [184]  [ 40/160]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8582 (1.7595)  loss_scale: 16384.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7497 (9.0461)  time: 0.8505 (0.5286 -- 3.8987)  data: 0.0014 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [184]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.8554 (1.7879)  loss_scale: 16384.0000 (12758.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0248 (8.7179)  time: 0.9931 (0.5279 -- 4.1394)  data: 0.0020 (0.0004 -- 0.0094)  max mem: 16413
Epoch: [184]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8121 (1.7778)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3130 (8.7263)  time: 0.7935 (0.5299 -- 3.0581)  data: 0.0025 (0.0001 -- 0.0133)  max mem: 16413
Epoch: [184]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.7204 (1.7632)  loss_scale: 16384.0000 (14194.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2235 (8.6085)  time: 0.9811 (0.5236 -- 4.0191)  data: 0.0018 (0.0005 -- 0.0042)  max mem: 16413
Epoch: [184]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7943 (1.7531)  loss_scale: 16384.0000 (14556.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9261 (8.6124)  time: 0.8193 (0.5209 -- 4.0386)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [184]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 2.0083 (1.7785)  loss_scale: 16384.0000 (14815.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6938 (8.7104)  time: 0.8851 (0.5261 -- 3.3936)  data: 0.0021 (0.0004 -- 0.0077)  max mem: 16413
[2023-08-31 05:14:46,823] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:14:46,823] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:14:46,823] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:14:46,823] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:14:49,275] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29599
[2023-08-31 05:14:49,275] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29599
[2023-08-31 05:14:49,275] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:14:49,275] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:14:49,275] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [184]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6054 (1.7660)  loss_scale: 16384.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9931 (8.7283)  time: 0.7190 (0.4885 -- 3.3936)  data: 0.0009 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [184] Total time: 0:02:21 (0.8844 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6054 (1.7951)  loss_scale: 16384.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9931 (8.7283)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.3310 (0.3310)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6399 (2.6399 -- 2.6399)  data: 2.4250 (2.4250 -- 2.4250)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4597 (0.7532)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4574 (0.1867 -- 2.6399)  data: 0.2428 (0.0006 -- 2.4250)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5316 (0.6798)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2164 (0.1695 -- 0.4333)  data: 0.0125 (0.0001 -- 0.2211)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5879 (0.7414)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2014 (0.1329 -- 0.4333)  data: 0.0121 (0.0001 -- 0.2211)  max mem: 16413
Val: Total time: 0:00:07 (0.2946 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.700
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [185]  [  0/160]  eta: 0:21:12  lr: 0.000000  min_lr: 0.000000  loss: 2.3108 (2.3108)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2218 (6.2218)  time: 7.9560 (7.9560 -- 7.9560)  data: 7.4099 (7.4099 -- 7.4099)  max mem: 16413
Epoch: [185]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.6176 (1.6933)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4482 (8.2097)  time: 0.8113 (0.5213 -- 3.0957)  data: 0.2576 (0.0005 -- 2.5628)  max mem: 16413
Epoch: [185]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.8236 (1.7281)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2584 (8.4328)  time: 0.8860 (0.5365 -- 2.8830)  data: 0.3163 (0.0004 -- 2.2972)  max mem: 16413
[2023-08-31 05:15:52,969] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29657
[2023-08-31 05:15:52,969] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29657
[2023-08-31 05:15:52,969] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:15:52,969] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:15:52,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [185]  [ 60/160]  eta: 0:01:34  lr: 0.000000  min_lr: 0.000000  loss: 1.7582 (1.7149)  loss_scale: 16384.0000 (15846.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6904 (8.5584)  time: 0.7727 (0.5173 -- 3.6153)  data: 0.2217 (0.0010 -- 3.0836)  max mem: 16413
Epoch: [185]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.9148 (1.7616)  loss_scale: 8192.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3046 (8.5704)  time: 0.8995 (0.5200 -- 3.1672)  data: 0.2373 (0.0003 -- 2.6289)  max mem: 16413
Epoch: [185]  [100/160]  eta: 0:00:54  lr: 0.000000  min_lr: 0.000000  loss: 1.8224 (1.7683)  loss_scale: 8192.0000 (12815.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5055 (8.5114)  time: 0.8485 (0.5372 -- 3.1714)  data: 0.2934 (0.0006 -- 2.6470)  max mem: 16413
Epoch: [185]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8657 (1.7751)  loss_scale: 8192.0000 (12051.0413)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0651 (8.5305)  time: 0.8808 (0.5223 -- 2.2258)  data: 0.3388 (0.0002 -- 1.7022)  max mem: 16413
Epoch: [185]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8634 (1.7763)  loss_scale: 8192.0000 (11503.6596)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1424 (8.5406)  time: 0.8901 (0.5325 -- 3.5444)  data: 0.3428 (0.0002 -- 2.9998)  max mem: 16413
Epoch: [185]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8320 (1.7753)  loss_scale: 8192.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9973 (8.7185)  time: 0.6398 (0.4982 -- 2.1839)  data: 0.1077 (0.0002 -- 1.6233)  max mem: 16413
Epoch: [185] Total time: 0:02:20 (0.8756 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8320 (1.7970)  loss_scale: 8192.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9973 (8.7185)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3308 (0.3308)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4134 (2.4134 -- 2.4134)  data: 2.1637 (2.1637 -- 2.1637)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4603 (0.7527)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4488 (0.2096 -- 2.4134)  data: 0.2258 (0.0006 -- 2.1637)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5295 (0.6796)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2209 (0.1702 -- 0.5195)  data: 0.0162 (0.0001 -- 0.3098)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5874 (0.7410)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2036 (0.1326 -- 0.5195)  data: 0.0158 (0.0001 -- 0.3098)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.700
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [186]  [  0/160]  eta: 0:20:09  lr: 0.000000  min_lr: 0.000000  loss: 1.4834 (1.4834)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4699 (7.4699)  time: 7.5580 (7.5580 -- 7.5580)  data: 7.0105 (7.0105 -- 7.0105)  max mem: 16413
Epoch: [186]  [ 20/160]  eta: 0:02:55  lr: 0.000000  min_lr: 0.000000  loss: 1.8111 (1.7322)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8489 (9.2880)  time: 0.9372 (0.5268 -- 3.0438)  data: 0.1226 (0.0004 -- 2.1335)  max mem: 16413
[2023-08-31 05:17:56,779] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:17:56,779] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 05:17:56,780] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:17:56,780] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [186]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 2.0433 (1.8806)  loss_scale: 16384.0000 (11189.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6333 (8.8483)  time: 0.7734 (0.5164 -- 2.4619)  data: 0.0425 (0.0006 -- 0.7209)  max mem: 16413
Epoch: [186]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.9201 (1.8765)  loss_scale: 16384.0000 (12892.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2431 (8.9000)  time: 0.8782 (0.5378 -- 3.5229)  data: 0.2399 (0.0002 -- 2.9847)  max mem: 16413
Epoch: [186]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.8846 (1.8675)  loss_scale: 16384.0000 (13754.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3350 (8.8132)  time: 0.9797 (0.5325 -- 3.7345)  data: 0.4275 (0.0004 -- 3.1787)  max mem: 16413
[2023-08-31 05:18:59,386] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29859
[2023-08-31 05:18:59,386] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29859
[2023-08-31 05:18:59,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:18:59,387] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:18:59,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [186]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8185 (1.8504)  loss_scale: 16384.0000 (14112.9505)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2227 (9.0733)  time: 0.7904 (0.5117 -- 3.2161)  data: 0.2394 (0.0004 -- 2.6962)  max mem: 16413
Epoch: [186]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6769 (1.8341)  loss_scale: 8192.0000 (13134.2810)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1401 (9.0141)  time: 0.8398 (0.5362 -- 3.0563)  data: 0.2829 (0.0004 -- 2.5112)  max mem: 16413
Epoch: [186]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7997 (1.8221)  loss_scale: 8192.0000 (12433.2482)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7612 (9.0502)  time: 0.8813 (0.5197 -- 3.2473)  data: 0.1155 (0.0007 -- 2.2862)  max mem: 16413
Epoch: [186]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8294 (1.8246)  loss_scale: 8192.0000 (11929.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5886 (9.0849)  time: 0.7244 (0.4957 -- 2.1173)  data: 0.1164 (0.0002 -- 1.5787)  max mem: 16413
Epoch: [186] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8294 (1.8374)  loss_scale: 8192.0000 (11929.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5886 (9.0849)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3304 (0.3304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4601 (2.4601 -- 2.4601)  data: 2.1669 (2.1669 -- 2.1669)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4606 (0.7535)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4565 (0.1897 -- 2.4601)  data: 0.2377 (0.0006 -- 2.1669)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5296 (0.6801)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2219 (0.1704 -- 0.6794)  data: 0.0225 (0.0001 -- 0.4361)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5875 (0.7416)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2084 (0.1328 -- 0.6794)  data: 0.0221 (0.0001 -- 0.4361)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.700
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [187]  [  0/160]  eta: 0:22:37  lr: 0.000000  min_lr: 0.000000  loss: 2.3364 (2.3364)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3759 (10.3759)  time: 8.4852 (8.4852 -- 8.4852)  data: 7.1859 (7.1859 -- 7.1859)  max mem: 16413
[2023-08-31 05:20:06,662] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29926
[2023-08-31 05:20:06,662] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29926
[2023-08-31 05:20:06,663] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 05:20:06,663] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 05:20:06,663] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [187]  [ 20/160]  eta: 0:02:50  lr: 0.000000  min_lr: 0.000000  loss: 1.6854 (1.8080)  loss_scale: 4096.0000 (5266.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4145 (9.5119)  time: 0.8544 (0.5248 -- 3.4216)  data: 0.3062 (0.0002 -- 2.8986)  max mem: 16413
Epoch: [187]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 2.0110 (1.8678)  loss_scale: 4096.0000 (4695.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7567 (8.9541)  time: 0.9230 (0.5195 -- 3.1263)  data: 0.3727 (0.0004 -- 2.5921)  max mem: 16413
Epoch: [187]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6507 (1.8293)  loss_scale: 4096.0000 (4498.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8585 (9.1001)  time: 0.7618 (0.5241 -- 2.0806)  data: 0.2054 (0.0003 -- 1.4980)  max mem: 16413
[2023-08-31 05:21:08,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=177, lr=[6.73435885986037e-09, 6.73435885986037e-09, 8.979145146480493e-09, 8.979145146480493e-09, 1.1972193528640657e-08, 1.1972193528640657e-08, 1.596292470485421e-08, 1.596292470485421e-08, 2.128389960647228e-08, 2.128389960647228e-08, 2.8378532808629706e-08, 2.8378532808629706e-08, 3.783804374483961e-08, 3.783804374483961e-08, 5.045072499311948e-08, 5.045072499311948e-08, 6.72676333241593e-08, 6.72676333241593e-08, 8.969017776554574e-08, 8.969017776554574e-08, 1.1958690368739433e-07, 1.1958690368739433e-07, 1.5944920491652577e-07, 1.5944920491652577e-07, 2.12598939888701e-07, 2.12598939888701e-07, 2.834652531849347e-07, 2.834652531849347e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 05:21:08,761] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=17.32543448851096, CurrSamplesPerSec=22.18246051977449, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [187]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.8147 (1.8558)  loss_scale: 4096.0000 (4399.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9887 (9.2302)  time: 0.8325 (0.5315 -- 2.3912)  data: 0.2585 (0.0005 -- 1.8264)  max mem: 16413
Epoch: [187]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.6773 (1.8425)  loss_scale: 4096.0000 (4339.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3667 (9.1084)  time: 0.8997 (0.5170 -- 3.1917)  data: 0.3093 (0.0004 -- 2.6125)  max mem: 16413
Epoch: [187]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7713 (1.8266)  loss_scale: 4096.0000 (4299.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2985 (8.9779)  time: 0.8752 (0.5142 -- 3.2046)  data: 0.2977 (0.0007 -- 2.6609)  max mem: 16413
[2023-08-31 05:22:00,066] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:22:00,066] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-31 05:22:00,067] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:22:00,067] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [187]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8777 (1.8300)  loss_scale: 4096.0000 (4444.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4425 (9.0258)  time: 0.9263 (0.5367 -- 3.6952)  data: 0.1143 (0.0006 -- 2.2548)  max mem: 16413
Epoch: [187]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6979 (1.8187)  loss_scale: 8192.0000 (4889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6095 (8.9856)  time: 0.7286 (0.4965 -- 3.7468)  data: 0.0007 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [187] Total time: 0:02:24 (0.9000 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6979 (1.8088)  loss_scale: 8192.0000 (4889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6095 (8.9856)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.3310 (0.3310)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6322 (2.6322 -- 2.6322)  data: 2.3444 (2.3444 -- 2.3444)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4602 (0.7532)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4567 (0.2042 -- 2.6322)  data: 0.2345 (0.0004 -- 2.3444)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5295 (0.6801)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2165 (0.1695 -- 0.4416)  data: 0.0119 (0.0001 -- 0.2245)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5877 (0.7415)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2003 (0.1323 -- 0.4416)  data: 0.0116 (0.0001 -- 0.2245)  max mem: 16413
Val: Total time: 0:00:07 (0.2958 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.700
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [188]  [  0/160]  eta: 0:22:06  lr: 0.000000  min_lr: 0.000000  loss: 1.3847 (1.3847)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6433 (8.6433)  time: 8.2895 (8.2895 -- 8.2895)  data: 7.6865 (7.6865 -- 7.6865)  max mem: 16413
Epoch: [188]  [ 20/160]  eta: 0:02:39  lr: 0.000000  min_lr: 0.000000  loss: 1.9866 (1.8386)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9356 (8.6064)  time: 0.7838 (0.5336 -- 3.6647)  data: 0.2229 (0.0003 -- 3.0862)  max mem: 16413
Epoch: [188]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 2.0413 (1.8833)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9200 (8.6092)  time: 0.9809 (0.5234 -- 4.2593)  data: 0.4314 (0.0001 -- 3.7219)  max mem: 16413
Epoch: [188]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.9741 (1.8975)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7005 (8.7027)  time: 0.8393 (0.5182 -- 3.9763)  data: 0.2967 (0.0002 -- 3.4332)  max mem: 16413
Epoch: [188]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7436 (1.8668)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2925 (8.6668)  time: 0.8222 (0.5130 -- 2.5931)  data: 0.2247 (0.0003 -- 2.0753)  max mem: 16413
Epoch: [188]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.9098 (1.8605)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1479 (8.7822)  time: 0.8909 (0.5223 -- 3.4255)  data: 0.0011 (0.0003 -- 0.0029)  max mem: 16413
[2023-08-31 05:24:03,205] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:24:03,205] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 05:24:03,208] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:24:03,208] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [188]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7788 (1.8403)  loss_scale: 16384.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4896 (8.7557)  time: 0.7644 (0.5356 -- 2.8630)  data: 0.0019 (0.0001 -- 0.0053)  max mem: 16413
Epoch: [188]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7987 (1.8476)  loss_scale: 16384.0000 (10399.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6253 (8.9607)  time: 0.8838 (0.5134 -- 3.4353)  data: 0.0017 (0.0005 -- 0.0048)  max mem: 16413
Epoch: [188]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5305 (1.8169)  loss_scale: 16384.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1799 (8.9124)  time: 0.7077 (0.4942 -- 2.8812)  data: 0.0013 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [188] Total time: 0:02:21 (0.8827 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5305 (1.8278)  loss_scale: 16384.0000 (11110.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1799 (8.9124)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3305 (0.3305)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5058 (2.5058 -- 2.5058)  data: 2.2463 (2.2463 -- 2.2463)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4607 (0.7538)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4651 (0.1937 -- 2.5058)  data: 0.2512 (0.0006 -- 2.2463)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5320 (0.6807)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2236 (0.1695 -- 0.7253)  data: 0.0260 (0.0001 -- 0.4970)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5872 (0.7422)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2083 (0.1330 -- 0.7253)  data: 0.0252 (0.0001 -- 0.4970)  max mem: 16413
Val: Total time: 0:00:07 (0.2950 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.701
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [189]  [  0/160]  eta: 0:24:30  lr: 0.000000  min_lr: 0.000000  loss: 1.1646 (1.1646)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.7988 (15.7988)  time: 9.1890 (9.1890 -- 9.1890)  data: 8.6501 (8.6501 -- 8.6501)  max mem: 16413
Epoch: [189]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.8658 (1.9013)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6021 (9.8187)  time: 0.7525 (0.5276 -- 2.7487)  data: 0.2000 (0.0005 -- 2.2265)  max mem: 16413
Epoch: [189]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.7299 (1.8255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6942 (8.7488)  time: 0.9631 (0.5155 -- 5.7411)  data: 0.4201 (0.0002 -- 5.2057)  max mem: 16413
Epoch: [189]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.8054 (1.8291)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8239 (8.8437)  time: 0.8353 (0.5301 -- 3.1731)  data: 0.1513 (0.0005 -- 1.5369)  max mem: 16413
[2023-08-31 05:26:03,300] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:26:03,300] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:26:03,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:26:03,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:26:09,002] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30318
[2023-08-31 05:26:09,002] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30318
[2023-08-31 05:26:09,003] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:26:09,003] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:26:09,003] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [189]  [ 80/160]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 1.7727 (1.8336)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2450 (8.8579)  time: 0.7090 (0.5269 -- 2.0333)  data: 0.0375 (0.0004 -- 0.3735)  max mem: 16413
Epoch: [189]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.8467 (1.8405)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1417 (8.6833)  time: 0.9535 (0.5220 -- 2.6716)  data: 0.1951 (0.0004 -- 1.2501)  max mem: 16413
Epoch: [189]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7436 (1.8313)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7460 (8.7368)  time: 0.8601 (0.5283 -- 1.9769)  data: 0.2378 (0.0003 -- 1.4451)  max mem: 16413
Epoch: [189]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8350 (1.8222)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4596 (8.7661)  time: 0.8985 (0.5136 -- 2.9254)  data: 0.1892 (0.0005 -- 1.8851)  max mem: 16413
Epoch: [189]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6888 (1.8055)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6936 (8.7221)  time: 0.6539 (0.4974 -- 1.3984)  data: 0.1253 (0.0002 -- 0.8836)  max mem: 16413
Epoch: [189] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6888 (1.8128)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6936 (8.7221)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3297 (0.3297)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3813 (2.3813 -- 2.3813)  data: 2.1682 (2.1682 -- 2.1682)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4594 (0.7536)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4209 (0.2025 -- 2.3813)  data: 0.1981 (0.0006 -- 2.1682)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5310 (0.6803)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1685 -- 0.3839)  data: 0.0107 (0.0001 -- 0.1995)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5875 (0.7420)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2021 (0.1329 -- 0.3839)  data: 0.0103 (0.0001 -- 0.1995)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.701
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [190]  [  0/160]  eta: 0:19:34  lr: 0.000000  min_lr: 0.000000  loss: 1.1007 (1.1007)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7250 (8.7250)  time: 7.3405 (7.3405 -- 7.3405)  data: 6.8130 (6.8130 -- 6.8130)  max mem: 16413
Epoch: [190]  [ 20/160]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.6804 (1.6648)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4120 (9.4051)  time: 0.8825 (0.5212 -- 2.8384)  data: 0.2974 (0.0003 -- 1.8380)  max mem: 16413
Epoch: [190]  [ 40/160]  eta: 0:02:09  lr: 0.000000  min_lr: 0.000000  loss: 1.9308 (1.7888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7801 (9.7437)  time: 0.9603 (0.5236 -- 3.6660)  data: 0.1469 (0.0004 -- 1.9971)  max mem: 16413
[2023-08-31 05:28:13,116] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:28:13,116] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:28:13,116] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:28:13,116] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:28:23,374] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30460
[2023-08-31 05:28:23,374] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30460
[2023-08-31 05:28:23,374] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:28:23,374] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:28:23,374] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [190]  [ 60/160]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 1.8339 (1.7781)  loss_scale: 32768.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3996 (9.4211)  time: 0.7064 (0.5190 -- 2.6875)  data: 0.0513 (0.0004 -- 0.9569)  max mem: 16413
Epoch: [190]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8503 (1.8026)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0545 (9.3239)  time: 1.0804 (0.5159 -- 4.5513)  data: 0.5314 (0.0004 -- 4.0020)  max mem: 16413
Epoch: [190]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.9103 (1.8099)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3661 (9.1263)  time: 0.7588 (0.5110 -- 3.2397)  data: 0.2103 (0.0004 -- 2.7334)  max mem: 16413
Epoch: [190]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8085 (1.8065)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7597 (9.0946)  time: 0.9491 (0.5242 -- 3.5124)  data: 0.3964 (0.0003 -- 2.9138)  max mem: 16413
Epoch: [190]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8333 (1.7954)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5035 (9.0823)  time: 0.7375 (0.5197 -- 4.2593)  data: 0.1877 (0.0002 -- 3.7253)  max mem: 16413
Epoch: [190]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8400 (1.8070)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4571 (9.1094)  time: 0.7474 (0.4959 -- 3.1519)  data: 0.2277 (0.0002 -- 2.6231)  max mem: 16413
Epoch: [190] Total time: 0:02:23 (0.8955 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8400 (1.7993)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4571 (9.1094)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3301 (0.3301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5047 (2.5047 -- 2.5047)  data: 2.2876 (2.2876 -- 2.2876)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4597 (0.7539)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4625 (0.2070 -- 2.5047)  data: 0.2435 (0.0006 -- 2.2876)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5326 (0.6807)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2260 (0.1691 -- 0.5864)  data: 0.0226 (0.0001 -- 0.3780)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5881 (0.7423)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2080 (0.1325 -- 0.5864)  data: 0.0223 (0.0001 -- 0.3780)  max mem: 16413
Val: Total time: 0:00:08 (0.2966 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.701
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [191]  [  0/160]  eta: 0:22:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5924 (1.5924)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9108 (9.9108)  time: 8.3673 (8.3673 -- 8.3673)  data: 7.2902 (7.2902 -- 7.2902)  max mem: 16413
Epoch: [191]  [ 20/160]  eta: 0:02:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7246 (1.8169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7910 (9.0625)  time: 0.7659 (0.5281 -- 2.7532)  data: 0.2059 (0.0005 -- 2.2118)  max mem: 16413
[2023-08-31 05:30:27,847] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:30:27,847] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:30:27,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:30:27,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:30:28,395] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30590
[2023-08-31 05:30:28,395] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30590
[2023-08-31 05:30:28,395] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:30:28,395] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:30:28,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [191]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 2.0052 (1.8598)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9429 (8.6018)  time: 0.9445 (0.5226 -- 2.6781)  data: 0.3925 (0.0004 -- 2.1402)  max mem: 16413
Epoch: [191]  [ 60/160]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.7777 (1.8209)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2101 (8.6711)  time: 0.9465 (0.5458 -- 4.0334)  data: 0.3917 (0.0007 -- 3.4735)  max mem: 16413
Epoch: [191]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7156 (1.7993)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8554 (8.7087)  time: 0.7573 (0.5297 -- 3.1422)  data: 0.1326 (0.0001 -- 2.6268)  max mem: 16413
Epoch: [191]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8768 (1.8257)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9107 (8.6515)  time: 0.9313 (0.5277 -- 3.9634)  data: 0.3822 (0.0004 -- 3.4210)  max mem: 16413
Epoch: [191]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6412 (1.8169)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3202 (8.9066)  time: 0.8840 (0.5212 -- 4.5312)  data: 0.3348 (0.0002 -- 4.0062)  max mem: 16413
Epoch: [191]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7680 (1.7962)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6883 (8.9827)  time: 0.9209 (0.5151 -- 3.9584)  data: 0.3719 (0.0002 -- 3.4437)  max mem: 16413
[2023-08-31 05:32:19,751] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:32:19,751] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:32:19,751] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:32:19,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [191]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7861 (1.7941)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9195 (8.8577)  time: 0.6283 (0.4951 -- 2.2016)  data: 0.1137 (0.0002 -- 1.6846)  max mem: 16413
Epoch: [191] Total time: 0:02:23 (0.8964 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7861 (1.7895)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9195 (8.8577)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3304 (0.3304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4741 (2.4741 -- 2.4741)  data: 2.2146 (2.2146 -- 2.2146)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4603 (0.7537)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4386 (0.1981 -- 2.4741)  data: 0.2152 (0.0004 -- 2.2146)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5320 (0.6805)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2208 (0.1693 -- 0.3395)  data: 0.0143 (0.0001 -- 0.1293)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5880 (0.7421)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2006 (0.1331 -- 0.3395)  data: 0.0134 (0.0001 -- 0.1293)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.701
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [192]  [  0/160]  eta: 0:19:40  lr: 0.000000  min_lr: 0.000000  loss: 1.1039 (1.1039)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0123 (3.0123)  time: 7.3771 (7.3771 -- 7.3771)  data: 6.8589 (6.8589 -- 6.8589)  max mem: 16413
[2023-08-31 05:32:37,764] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30725
[2023-08-31 05:32:37,764] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30725
[2023-08-31 05:32:37,764] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:32:37,764] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:32:37,764] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [192]  [ 20/160]  eta: 0:02:48  lr: 0.000000  min_lr: 0.000000  loss: 1.7676 (1.7841)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4234 (8.3869)  time: 0.8924 (0.5141 -- 4.5551)  data: 0.3273 (0.0005 -- 4.0030)  max mem: 16413
Epoch: [192]  [ 40/160]  eta: 0:02:12  lr: 0.000000  min_lr: 0.000000  loss: 1.9127 (1.8067)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6356 (9.0450)  time: 1.0021 (0.5169 -- 4.1255)  data: 0.4564 (0.0003 -- 3.5909)  max mem: 16413
Epoch: [192]  [ 60/160]  eta: 0:01:34  lr: 0.000000  min_lr: 0.000000  loss: 2.0164 (1.8197)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5757 (8.8619)  time: 0.6243 (0.5219 -- 1.3790)  data: 0.0791 (0.0001 -- 0.8372)  max mem: 16413
Epoch: [192]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.9474 (1.8401)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5347 (8.7091)  time: 0.9073 (0.5279 -- 2.9728)  data: 0.2132 (0.0004 -- 2.3888)  max mem: 16413
Epoch: [192]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8446 (1.8515)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9655 (8.6915)  time: 0.9186 (0.5293 -- 2.3476)  data: 0.3647 (0.0003 -- 1.8072)  max mem: 16413
Epoch: [192]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8312 (1.8459)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1836 (8.7788)  time: 0.7973 (0.5262 -- 2.5652)  data: 0.1953 (0.0005 -- 2.0401)  max mem: 16413
[2023-08-31 05:34:30,612] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:34:30,612] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:34:30,612] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:34:30,613] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:34:31,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30855
[2023-08-31 05:34:31,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:34:31,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30855
[2023-08-31 05:34:31,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:34:31,157] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [192]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8942 (1.8568)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5421 (8.6284)  time: 0.8972 (0.5204 -- 2.4266)  data: 0.2259 (0.0004 -- 1.8761)  max mem: 16413
Epoch: [192]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8021 (1.8497)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5910 (8.6828)  time: 0.7126 (0.4963 -- 2.6956)  data: 0.0714 (0.0002 -- 1.4137)  max mem: 16413
Epoch: [192] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8021 (1.8469)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5910 (8.6828)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3299 (0.3299)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5728 (2.5728 -- 2.5728)  data: 2.3340 (2.3340 -- 2.3340)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4604 (0.7537)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4682 (0.2057 -- 2.5728)  data: 0.2442 (0.0004 -- 2.3340)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5318 (0.6806)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2227 (0.1691 -- 0.5975)  data: 0.0178 (0.0001 -- 0.3442)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5884 (0.7422)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2063 (0.1370 -- 0.5975)  data: 0.0175 (0.0001 -- 0.3442)  max mem: 16413
Val: Total time: 0:00:08 (0.2972 s / it)
* Acc@1 81.328 Acc@5 96.473 loss 0.701
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [193]  [  0/160]  eta: 0:21:37  lr: 0.000000  min_lr: 0.000000  loss: 1.9706 (1.9706)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4769 (7.4769)  time: 8.1084 (8.1084 -- 8.1084)  data: 5.3885 (5.3885 -- 5.3885)  max mem: 16413
Epoch: [193]  [ 20/160]  eta: 0:02:44  lr: 0.000000  min_lr: 0.000000  loss: 1.7770 (1.8079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9437 (9.2363)  time: 0.8270 (0.5266 -- 3.0116)  data: 0.1637 (0.0006 -- 2.4871)  max mem: 16413
Epoch: [193]  [ 40/160]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8868 (1.8329)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4476 (9.4146)  time: 0.8228 (0.5310 -- 2.3490)  data: 0.2664 (0.0006 -- 1.8251)  max mem: 16413
Epoch: [193]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.8317 (1.8153)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7879 (9.0488)  time: 0.9549 (0.5160 -- 3.5806)  data: 0.4112 (0.0003 -- 3.0376)  max mem: 16413
Epoch: [193]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.7599 (1.7947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1032 (8.8659)  time: 0.8513 (0.5082 -- 4.9803)  data: 0.3132 (0.0003 -- 4.4618)  max mem: 16413
Epoch: [193]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.8026 (1.8049)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5670 (8.8952)  time: 0.9427 (0.5298 -- 4.4729)  data: 0.3881 (0.0005 -- 3.9367)  max mem: 16413
[2023-08-31 05:36:35,869] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:36:35,869] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:36:35,871] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:36:35,871] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:36:48,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=182, lr=[2.523784594471536e-09, 2.523784594471536e-09, 3.3650461259620475e-09, 3.3650461259620475e-09, 4.486728167949397e-09, 4.486728167949397e-09, 5.982304223932529e-09, 5.982304223932529e-09, 7.976405631910039e-09, 7.976405631910039e-09, 1.0635207509213385e-08, 1.0635207509213385e-08, 1.4180276678951179e-08, 1.4180276678951179e-08, 1.8907035571934907e-08, 1.8907035571934907e-08, 2.5209380762579874e-08, 2.5209380762579874e-08, 3.361250768343983e-08, 3.361250768343983e-08, 4.481667691125311e-08, 4.481667691125311e-08, 5.975556921500415e-08, 5.975556921500415e-08, 7.96740922866722e-08, 7.96740922866722e-08, 1.0623212304889626e-07, 1.0623212304889626e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 05:36:48,961] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=17.396854348311365, CurrSamplesPerSec=20.966723861916588, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [193]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7710 (1.8094)  loss_scale: 32768.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4984 (8.7618)  time: 0.7907 (0.5185 -- 3.9750)  data: 0.2353 (0.0004 -- 3.4190)  max mem: 16413
Epoch: [193]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8032 (1.8049)  loss_scale: 32768.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9380 (8.6425)  time: 0.8719 (0.5289 -- 3.2489)  data: 0.3217 (0.0003 -- 2.7335)  max mem: 16413
Epoch: [193]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8547 (1.8175)  loss_scale: 32768.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2830 (8.8156)  time: 0.6598 (0.4971 -- 2.6360)  data: 0.1071 (0.0002 -- 2.1278)  max mem: 16413
Epoch: [193] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8547 (1.8124)  loss_scale: 32768.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2830 (8.8156)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3301 (0.3301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3807 (2.3807 -- 2.3807)  data: 2.1672 (2.1672 -- 2.1672)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4598 (0.7538)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4208 (0.1965 -- 2.3807)  data: 0.2044 (0.0008 -- 2.1672)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5319 (0.6805)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2238 (0.1689 -- 0.4661)  data: 0.0184 (0.0001 -- 0.2819)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5885 (0.7421)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2069 (0.1329 -- 0.4661)  data: 0.0175 (0.0001 -- 0.2819)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.701
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [194]  [  0/160]  eta: 0:24:50  lr: 0.000000  min_lr: 0.000000  loss: 2.2459 (2.2459)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0080 (12.0080)  time: 9.3175 (9.3175 -- 9.3175)  data: 6.4765 (6.4765 -- 6.4765)  max mem: 16413
Epoch: [194]  [ 20/160]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 1.8867 (1.8623)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6984 (9.2552)  time: 0.7543 (0.5301 -- 3.4483)  data: 0.0015 (0.0004 -- 0.0031)  max mem: 16413
[2023-08-31 05:37:56,856] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31064
[2023-08-31 05:37:56,856] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31064
[2023-08-31 05:37:56,857] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:37:56,857] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:37:56,857] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [194]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 1.8104 (1.8321)  loss_scale: 16384.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3375 (8.7387)  time: 0.9830 (0.5255 -- 3.7788)  data: 0.0013 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [194]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.9472 (1.8388)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9298 (8.8784)  time: 0.8290 (0.5205 -- 3.5219)  data: 0.0017 (0.0002 -- 0.0112)  max mem: 16413
[2023-08-31 05:38:35,288] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31111
[2023-08-31 05:38:35,288] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31111
[2023-08-31 05:38:35,289] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:38:35,289] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:38:35,289] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [194]  [ 80/160]  eta: 0:01:13  lr: 0.000000  min_lr: 0.000000  loss: 1.8688 (1.8461)  loss_scale: 8192.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6529 (8.7994)  time: 0.7049 (0.5332 -- 2.4472)  data: 0.0732 (0.0001 -- 1.2810)  max mem: 16413
Epoch: [194]  [100/160]  eta: 0:00:54  lr: 0.000000  min_lr: 0.000000  loss: 1.7849 (1.8558)  loss_scale: 8192.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2833 (8.9675)  time: 0.8823 (0.5431 -- 3.0903)  data: 0.2469 (0.0004 -- 1.7115)  max mem: 16413
Epoch: [194]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8001 (1.8653)  loss_scale: 8192.0000 (16248.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0360 (8.9610)  time: 0.8969 (0.5294 -- 3.9734)  data: 0.2810 (0.0003 -- 3.4308)  max mem: 16413
Epoch: [194]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7523 (1.8483)  loss_scale: 8192.0000 (15105.8156)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1217 (8.9509)  time: 0.9809 (0.5339 -- 4.2013)  data: 0.4271 (0.0005 -- 3.6550)  max mem: 16413
Epoch: [194]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8208 (1.8438)  loss_scale: 8192.0000 (14284.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1948 (9.0165)  time: 0.6604 (0.4984 -- 2.0388)  data: 0.1366 (0.0001 -- 1.5334)  max mem: 16413
Epoch: [194] Total time: 0:02:22 (0.8912 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8208 (1.8452)  loss_scale: 8192.0000 (14284.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1948 (9.0165)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3304 (0.3304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5928 (2.5928 -- 2.5928)  data: 2.3574 (2.3574 -- 2.3574)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4598 (0.7540)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4355 (0.2020 -- 2.5928)  data: 0.2210 (0.0007 -- 2.3574)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5315 (0.6805)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2095 (0.1696 -- 0.2610)  data: 0.0046 (0.0001 -- 0.0481)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5885 (0.7421)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.1927 (0.1326 -- 0.2610)  data: 0.0035 (0.0001 -- 0.0481)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 81.328 Acc@5 96.680 loss 0.701
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [195]  [  0/160]  eta: 0:26:10  lr: 0.000000  min_lr: 0.000000  loss: 1.4264 (1.4264)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6527 (7.6527)  time: 9.8158 (9.8158 -- 9.8158)  data: 5.4690 (5.4690 -- 5.4690)  max mem: 16413
Epoch: [195]  [ 20/160]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 1.6850 (1.7098)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6009 (9.0552)  time: 0.7339 (0.5311 -- 3.5334)  data: 0.0056 (0.0002 -- 0.0804)  max mem: 16413
[2023-08-31 05:40:41,928] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:40:41,928] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:40:41,929] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 05:40:41,929] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [195]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 1.9382 (1.8126)  loss_scale: 8192.0000 (8391.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2717 (9.0494)  time: 0.9786 (0.5250 -- 3.8588)  data: 0.3594 (0.0006 -- 3.0440)  max mem: 16413
Epoch: [195]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.9156 (1.8261)  loss_scale: 16384.0000 (11012.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2651 (8.5307)  time: 0.7696 (0.5140 -- 3.1857)  data: 0.0334 (0.0004 -- 0.5830)  max mem: 16413
Epoch: [195]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8313 (1.8375)  loss_scale: 16384.0000 (12338.5679)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6604 (8.5007)  time: 0.8559 (0.5344 -- 2.5566)  data: 0.0019 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [195]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.7573 (1.8290)  loss_scale: 16384.0000 (13139.6436)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9407 (8.6103)  time: 0.8503 (0.5275 -- 4.3600)  data: 0.0078 (0.0002 -- 0.1249)  max mem: 16413
Epoch: [195]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8184 (1.8283)  loss_scale: 16384.0000 (13675.9008)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7662 (8.5898)  time: 0.9541 (0.5314 -- 3.2289)  data: 0.0016 (0.0002 -- 0.0085)  max mem: 16413
Epoch: [195]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 2.0536 (1.8511)  loss_scale: 16384.0000 (14060.0284)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6321 (8.6728)  time: 0.8198 (0.5249 -- 4.0581)  data: 0.0020 (0.0006 -- 0.0059)  max mem: 16413
Epoch: [195]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7212 (1.8330)  loss_scale: 16384.0000 (14336.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1266 (8.6921)  time: 0.6504 (0.4966 -- 3.0531)  data: 0.0009 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [195] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7212 (1.8228)  loss_scale: 16384.0000 (14336.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1266 (8.6921)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3304 (0.3304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4294 (2.4294 -- 2.4294)  data: 2.2060 (2.2060 -- 2.2060)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4603 (0.7541)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4623 (0.2045 -- 2.4294)  data: 0.2421 (0.0004 -- 2.2060)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5312 (0.6806)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2266 (0.1690 -- 0.6589)  data: 0.0230 (0.0001 -- 0.4501)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5885 (0.7423)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2109 (0.1330 -- 0.6589)  data: 0.0228 (0.0001 -- 0.4501)  max mem: 16413
Val: Total time: 0:00:07 (0.2945 s / it)
* Acc@1 81.328 Acc@5 96.680 loss 0.701
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [196]  [  0/160]  eta: 0:15:10  lr: 0.000000  min_lr: 0.000000  loss: 1.7029 (1.7029)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2287 (8.2287)  time: 5.6897 (5.6897 -- 5.6897)  data: 5.1661 (5.1661 -- 5.1661)  max mem: 16413
[2023-08-31 05:42:41,652] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:42:41,652] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:42:41,653] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:42:41,653] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [196]  [ 20/160]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.7648 (1.7697)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9480 (8.8643)  time: 0.9665 (0.5192 -- 3.3037)  data: 0.3880 (0.0006 -- 2.3705)  max mem: 16413
[2023-08-31 05:42:57,465] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31386
[2023-08-31 05:42:57,465] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:42:57,465] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31386
[2023-08-31 05:42:57,465] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:42:57,465] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [196]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 1.8138 (1.7763)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0691 (8.7001)  time: 0.9112 (0.5229 -- 3.7852)  data: 0.3589 (0.0004 -- 3.2491)  max mem: 16413
Epoch: [196]  [ 60/160]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.9078 (1.8179)  loss_scale: 16384.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2504 (8.5121)  time: 0.9070 (0.5348 -- 5.1996)  data: 0.2591 (0.0003 -- 4.6828)  max mem: 16413
[2023-08-31 05:43:41,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31435
[2023-08-31 05:43:41,686] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31435
[2023-08-31 05:43:41,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:43:41,686] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 05:43:41,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [196]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9191 (1.8428)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5186 (8.7497)  time: 0.8916 (0.5134 -- 4.8126)  data: 0.3517 (0.0003 -- 4.2901)  max mem: 16413
Epoch: [196]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7581 (1.8361)  loss_scale: 8192.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7046 (8.7918)  time: 0.7625 (0.5296 -- 3.1006)  data: 0.2118 (0.0003 -- 2.5590)  max mem: 16413
Epoch: [196]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7816 (1.8353)  loss_scale: 8192.0000 (15706.9752)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8104 (8.8093)  time: 0.8837 (0.5131 -- 2.8579)  data: 0.1742 (0.0002 -- 2.1304)  max mem: 16413
Epoch: [196]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8722 (1.8417)  loss_scale: 8192.0000 (14641.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2449 (8.7259)  time: 0.9508 (0.5191 -- 3.3618)  data: 0.2115 (0.0003 -- 2.8089)  max mem: 16413
Epoch: [196]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7250 (1.8328)  loss_scale: 8192.0000 (13875.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3316 (8.8095)  time: 0.6682 (0.4949 -- 3.0476)  data: 0.1536 (0.0002 -- 2.5208)  max mem: 16413
Epoch: [196] Total time: 0:02:24 (0.9001 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7250 (1.8267)  loss_scale: 8192.0000 (13875.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3316 (8.8095)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3303 (0.3303)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2978 (2.2978 -- 2.2978)  data: 2.0725 (2.0725 -- 2.0725)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4604 (0.7540)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4137 (0.2019 -- 2.2978)  data: 0.1938 (0.0004 -- 2.0725)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5317 (0.6805)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2238 (0.1711 -- 0.4494)  data: 0.0158 (0.0001 -- 0.2535)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5884 (0.7423)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2062 (0.1330 -- 0.4494)  data: 0.0155 (0.0001 -- 0.2535)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.701
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [197]  [  0/160]  eta: 0:23:18  lr: 0.000000  min_lr: 0.000000  loss: 2.0031 (2.0031)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8855 (8.8855)  time: 8.7393 (8.7393 -- 8.7393)  data: 8.1896 (8.1896 -- 8.1896)  max mem: 16413
Epoch: [197]  [ 20/160]  eta: 0:02:48  lr: 0.000000  min_lr: 0.000000  loss: 1.8459 (1.8195)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7523 (8.9284)  time: 0.8245 (0.5285 -- 4.6047)  data: 0.0362 (0.0004 -- 0.6482)  max mem: 16413
Epoch: [197]  [ 40/160]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 1.7700 (1.8142)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0925 (9.1153)  time: 0.8814 (0.5299 -- 3.7352)  data: 0.0016 (0.0005 -- 0.0040)  max mem: 16413
[2023-08-31 05:45:44,313] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:45:44,313] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:45:44,313] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 05:45:44,313] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [197]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.9310 (1.8629)  loss_scale: 16384.0000 (10475.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1451 (8.9484)  time: 0.8908 (0.5215 -- 2.7050)  data: 0.0018 (0.0004 -- 0.0091)  max mem: 16413
Epoch: [197]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6749 (1.8196)  loss_scale: 16384.0000 (11934.0247)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8364 (8.9236)  time: 0.7513 (0.5316 -- 2.4660)  data: 0.0017 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [197]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.5969 (1.7895)  loss_scale: 16384.0000 (12815.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1747 (8.9317)  time: 0.9459 (0.5394 -- 4.1130)  data: 0.0018 (0.0006 -- 0.0055)  max mem: 16413
Epoch: [197]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8949 (1.8137)  loss_scale: 16384.0000 (13405.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5036 (8.7975)  time: 0.8323 (0.5318 -- 2.4749)  data: 0.0014 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [197]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7371 (1.8176)  loss_scale: 16384.0000 (13827.6312)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8390 (8.7993)  time: 0.9364 (0.5132 -- 5.9308)  data: 0.0013 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [197]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8830 (1.8223)  loss_scale: 16384.0000 (14131.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0841 (8.8031)  time: 0.6245 (0.4965 -- 1.9413)  data: 0.0008 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [197] Total time: 0:02:21 (0.8874 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8830 (1.7934)  loss_scale: 16384.0000 (14131.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0841 (8.8031)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3299 (0.3299)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2145 (2.2145 -- 2.2145)  data: 2.0075 (2.0075 -- 2.0075)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4601 (0.7540)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4264 (0.1953 -- 2.2145)  data: 0.2100 (0.0005 -- 2.0075)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5310 (0.6805)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2320 (0.1704 -- 0.5102)  data: 0.0261 (0.0001 -- 0.2938)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5885 (0.7423)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2159 (0.1328 -- 0.5102)  data: 0.0258 (0.0001 -- 0.2938)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 81.328 Acc@5 96.473 loss 0.701
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Epoch: [198]  [  0/160]  eta: 0:19:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7020 (1.7020)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3887 (8.3887)  time: 7.2392 (7.2392 -- 7.2392)  data: 6.0882 (6.0882 -- 6.0882)  max mem: 16413
[2023-08-31 05:47:47,475] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:47:47,476] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:47:47,476] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:47:47,476] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:47:52,435] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31696
[2023-08-31 05:47:52,435] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31696
[2023-08-31 05:47:52,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:47:52,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:47:52,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [198]  [ 20/160]  eta: 0:02:50  lr: 0.000000  min_lr: 0.000000  loss: 1.8744 (1.8306)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0252 (8.6766)  time: 0.9158 (0.5230 -- 4.5327)  data: 0.0159 (0.0003 -- 0.2906)  max mem: 16413
Epoch: [198]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.8814 (1.9091)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8314 (8.8350)  time: 0.8350 (0.5231 -- 3.0071)  data: 0.0082 (0.0002 -- 0.1413)  max mem: 16413
Epoch: [198]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7131 (1.8317)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7731 (8.8725)  time: 0.8403 (0.5229 -- 2.3045)  data: 0.0984 (0.0005 -- 1.2169)  max mem: 16413
Epoch: [198]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.9885 (1.8370)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3478 (8.9174)  time: 0.9006 (0.5263 -- 2.8882)  data: 0.0014 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [198]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.9294 (1.8412)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1889 (8.7814)  time: 0.8016 (0.5265 -- 2.9668)  data: 0.0016 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [198]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.9935 (1.8589)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4999 (8.8547)  time: 0.9358 (0.5336 -- 2.2911)  data: 0.0430 (0.0003 -- 0.8277)  max mem: 16413
Epoch: [198]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7790 (1.8438)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0093 (8.7779)  time: 0.9099 (0.5213 -- 2.7263)  data: 0.1420 (0.0004 -- 1.6620)  max mem: 16413
[2023-08-31 05:49:42,087] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:49:42,088] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:49:42,089] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:49:42,089] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:49:47,744] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31831
[2023-08-31 05:49:47,744] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31831
[2023-08-31 05:49:47,744] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:49:47,744] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:49:47,744] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [198]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7840 (1.8411)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3702 (8.8896)  time: 0.6546 (0.4945 -- 2.6934)  data: 0.1107 (0.0002 -- 2.1968)  max mem: 16413
Epoch: [198] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7840 (1.8302)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3702 (8.8896)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3301 (0.3301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3878 (2.3878 -- 2.3878)  data: 2.1151 (2.1151 -- 2.1151)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4604 (0.7542)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4155 (0.1936 -- 2.3878)  data: 0.1934 (0.0006 -- 2.1151)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5310 (0.6806)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2203 (0.1710 -- 0.4058)  data: 0.0113 (0.0001 -- 0.2099)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5885 (0.7424)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2013 (0.1358 -- 0.4058)  data: 0.0109 (0.0001 -- 0.2099)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.701
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 81.74%
Epoch: [199]  [  0/160]  eta: 0:17:42  lr: 0.000000  min_lr: 0.000000  loss: 1.4752 (1.4752)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4993 (7.4993)  time: 6.6397 (6.6397 -- 6.6397)  data: 5.2736 (5.2736 -- 5.2736)  max mem: 16413
Epoch: [199]  [ 20/160]  eta: 0:02:45  lr: 0.000000  min_lr: 0.000000  loss: 1.7248 (1.7768)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0017 (7.9798)  time: 0.9065 (0.5167 -- 3.0801)  data: 0.2289 (0.0004 -- 2.4995)  max mem: 16413
Epoch: [199]  [ 40/160]  eta: 0:01:57  lr: 0.000000  min_lr: 0.000000  loss: 1.8010 (1.8148)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9745 (8.5593)  time: 0.7722 (0.5222 -- 3.2401)  data: 0.2073 (0.0004 -- 2.6927)  max mem: 16413
Epoch: [199]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.7895 (1.8004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2678 (8.5270)  time: 0.9807 (0.5258 -- 2.9658)  data: 0.1625 (0.0003 -- 1.9065)  max mem: 16413
Epoch: [199]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.9530 (1.8447)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9805 (8.8352)  time: 0.7897 (0.5196 -- 3.4515)  data: 0.0555 (0.0005 -- 0.8266)  max mem: 16413
Epoch: [199]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.7885 (1.8299)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2138 (8.7266)  time: 0.8999 (0.5285 -- 2.4650)  data: 0.3084 (0.0003 -- 1.9460)  max mem: 16413
[2023-08-31 05:51:52,188] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:51:52,188] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 05:51:52,189] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 05:51:52,190] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [199]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8188 (1.8394)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6814 (8.6804)  time: 0.9503 (0.5168 -- 2.9459)  data: 0.4049 (0.0004 -- 2.4148)  max mem: 16413
Epoch: [199]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6105 (1.8139)  loss_scale: 32768.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6205 (8.6631)  time: 0.8836 (0.5282 -- 3.0485)  data: 0.3394 (0.0004 -- 2.5279)  max mem: 16413
[2023-08-31 05:52:10,370] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31981
[2023-08-31 05:52:10,371] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31981
[2023-08-31 05:52:10,371] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:52:10,371] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 05:52:10,371] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 05:52:22,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=189, lr=[1.1136231593853239e-09, 1.1136231593853239e-09, 1.4848308791804318e-09, 1.4848308791804318e-09, 1.979774505573909e-09, 1.979774505573909e-09, 2.639699340765212e-09, 2.639699340765212e-09, 3.5195991210202825e-09, 3.5195991210202825e-09, 4.692798828027044e-09, 4.692798828027044e-09, 6.257065104036058e-09, 6.257065104036058e-09, 8.342753472048078e-09, 8.342753472048078e-09, 1.1123671296064103e-08, 1.1123671296064103e-08, 1.4831561728085471e-08, 1.4831561728085471e-08, 1.9775415637447294e-08, 1.9775415637447294e-08, 2.6367220849929725e-08, 2.6367220849929725e-08, 3.5156294466572967e-08, 3.5156294466572967e-08, 4.687505928876396e-08, 4.687505928876396e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 05:52:22,571] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=17.382580613451715, CurrSamplesPerSec=24.574308116482214, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [199]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6143 (1.8025)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3552 (8.6456)  time: 0.6617 (0.4930 -- 2.8976)  data: 0.1524 (0.0002 -- 2.3882)  max mem: 16413
Epoch: [199] Total time: 0:02:23 (0.8940 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6143 (1.8048)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3552 (8.6456)
[2023-08-31 05:52:22,575] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-08-31 05:52:22,577] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-08-31 05:52:22,578] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt
[2023-08-31 05:52:22,578] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt...
[2023-08-31 05:52:23,597] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt.
[2023-08-31 05:52:23,597] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3303 (0.3303)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6067 (2.6067 -- 2.6067)  data: 2.3962 (2.3962 -- 2.3962)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4604 (0.7542)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4666 (0.2041 -- 2.6067)  data: 0.2534 (0.0009 -- 2.3962)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5313 (0.6807)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2201 (0.1694 -- 0.6115)  data: 0.0198 (0.0001 -- 0.3784)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5884 (0.7423)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2058 (0.1327 -- 0.6115)  data: 0.0195 (0.0001 -- 0.3784)  max mem: 16413
Val: Total time: 0:00:07 (0.2961 s / it)
* Acc@1 81.328 Acc@5 96.680 loss 0.701
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 81.74%
Test:  [  0/603]  eta: 1:03:49  loss: 0.4165 (0.4165)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.3502 (6.3502 -- 6.3502)  data: 6.1196 (6.1196 -- 6.1196)  max mem: 16413
Test:  [ 10/603]  eta: 0:09:15  loss: 0.5671 (0.6968)  acc1: 66.6667 (74.2424)  acc5: 100.0000 (98.4848)  time: 0.9364 (0.1245 -- 6.3502)  data: 0.7899 (0.0004 -- 6.1196)  max mem: 16413
Test:  [ 20/603]  eta: 0:08:51  loss: 0.7827 (0.9967)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.2381)  time: 0.6401 (0.1245 -- 4.3499)  data: 0.5016 (0.0002 -- 4.2324)  max mem: 16413
Test:  [ 30/603]  eta: 0:06:17  loss: 0.6937 (0.8540)  acc1: 66.6667 (73.1183)  acc5: 100.0000 (96.2366)  time: 0.5049 (0.1185 -- 4.3499)  data: 0.3734 (0.0002 -- 4.2324)  max mem: 16413
Test:  [ 40/603]  eta: 0:05:51  loss: 0.4826 (0.8341)  acc1: 83.3333 (73.1707)  acc5: 100.0000 (96.7480)  time: 0.3233 (0.1185 -- 3.9432)  data: 0.1916 (0.0002 -- 3.8135)  max mem: 16413
Test:  [ 50/603]  eta: 0:05:16  loss: 0.7020 (0.8131)  acc1: 66.6667 (72.8758)  acc5: 100.0000 (97.0588)  time: 0.4373 (0.1288 -- 3.9432)  data: 0.2968 (0.0002 -- 3.8135)  max mem: 16413
Test:  [ 60/603]  eta: 0:06:36  loss: 0.7797 (0.8952)  acc1: 66.6667 (70.4918)  acc5: 100.0000 (96.1749)  time: 0.9449 (0.1233 -- 9.6140)  data: 0.8093 (0.0004 -- 9.4886)  max mem: 16413
Test:  [ 70/603]  eta: 0:05:43  loss: 0.6749 (0.8469)  acc1: 83.3333 (73.0047)  acc5: 100.0000 (96.4789)  time: 0.8294 (0.1162 -- 9.6140)  data: 0.7041 (0.0001 -- 9.4886)  max mem: 16413
Test:  [ 80/603]  eta: 0:05:30  loss: 0.4821 (0.8395)  acc1: 83.3333 (72.8395)  acc5: 100.0000 (96.7078)  time: 0.3299 (0.1162 -- 4.2400)  data: 0.2055 (0.0001 -- 4.1026)  max mem: 16413
Test:  [ 90/603]  eta: 0:05:02  loss: 0.5484 (0.8271)  acc1: 66.6667 (72.7106)  acc5: 100.0000 (96.8864)  time: 0.3974 (0.1208 -- 4.2400)  data: 0.2580 (0.0002 -- 4.1026)  max mem: 16413
Test:  [100/603]  eta: 0:05:40  loss: 0.7842 (0.8738)  acc1: 66.6667 (71.4521)  acc5: 100.0000 (96.3696)  time: 0.8625 (0.1254 -- 8.8352)  data: 0.7149 (0.0002 -- 8.7112)  max mem: 16413
Test:  [110/603]  eta: 0:05:11  loss: 0.6772 (0.8463)  acc1: 83.3333 (72.8228)  acc5: 100.0000 (96.5465)  time: 0.8257 (0.1314 -- 8.8352)  data: 0.6767 (0.0003 -- 8.7112)  max mem: 16413
Test:  [120/603]  eta: 0:05:10  loss: 0.5746 (0.8414)  acc1: 83.3333 (72.7273)  acc5: 100.0000 (96.6942)  time: 0.4729 (0.1175 -- 6.0025)  data: 0.3094 (0.0001 -- 5.8871)  max mem: 16413
Test:  [130/603]  eta: 0:04:56  loss: 0.5913 (0.8329)  acc1: 66.6667 (72.7735)  acc5: 100.0000 (96.8193)  time: 0.5947 (0.1175 -- 6.0025)  data: 0.4395 (0.0001 -- 5.8871)  max mem: 16413
Test:  [140/603]  eta: 0:05:23  loss: 0.5913 (0.8258)  acc1: 83.3333 (73.5225)  acc5: 100.0000 (96.9267)  time: 1.0350 (0.1199 -- 7.9830)  data: 0.8758 (0.0001 -- 7.8592)  max mem: 16413
Test:  [150/603]  eta: 0:05:00  loss: 0.5056 (0.7997)  acc1: 83.3333 (74.7241)  acc5: 100.0000 (97.0199)  time: 0.8990 (0.1288 -- 7.9830)  data: 0.7388 (0.0003 -- 7.8592)  max mem: 16413
Test:  [160/603]  eta: 0:04:49  loss: 0.5056 (0.8005)  acc1: 83.3333 (74.8447)  acc5: 100.0000 (96.8944)  time: 0.3334 (0.1261 -- 3.9339)  data: 0.1983 (0.0003 -- 3.8081)  max mem: 16413
Test:  [170/603]  eta: 0:04:35  loss: 0.6833 (0.7960)  acc1: 66.6667 (74.7563)  acc5: 100.0000 (96.9786)  time: 0.4327 (0.1261 -- 3.9339)  data: 0.2670 (0.0004 -- 3.8081)  max mem: 16413
Test:  [180/603]  eta: 0:04:50  loss: 0.5724 (0.7928)  acc1: 83.3333 (75.2302)  acc5: 100.0000 (97.0534)  time: 0.9569 (0.1306 -- 8.4028)  data: 0.7402 (0.0004 -- 8.1651)  max mem: 16413
Test:  [190/603]  eta: 0:04:31  loss: 0.4624 (0.7740)  acc1: 83.3333 (76.0908)  acc5: 100.0000 (97.1204)  time: 0.8525 (0.1336 -- 8.4028)  data: 0.6642 (0.0002 -- 8.1651)  max mem: 16413
Test:  [200/603]  eta: 0:04:32  loss: 0.5629 (0.7748)  acc1: 83.3333 (76.1194)  acc5: 100.0000 (97.0149)  time: 0.5886 (0.1303 -- 3.7063)  data: 0.4085 (0.0002 -- 3.5602)  max mem: 16413
Test:  [210/603]  eta: 0:04:28  loss: 0.7006 (0.7729)  acc1: 66.6667 (76.0664)  acc5: 100.0000 (97.0774)  time: 0.9096 (0.1237 -- 3.7063)  data: 0.6832 (0.0003 -- 3.5602)  max mem: 16413
Test:  [220/603]  eta: 0:04:26  loss: 0.6253 (0.7692)  acc1: 83.3333 (76.3952)  acc5: 100.0000 (97.2097)  time: 0.8888 (0.1235 -- 6.4378)  data: 0.6631 (0.0005 -- 5.9254)  max mem: 16413
Test:  [230/603]  eta: 0:04:17  loss: 0.4791 (0.7572)  acc1: 83.3333 (76.9841)  acc5: 100.0000 (97.1861)  time: 0.7763 (0.1235 -- 6.4378)  data: 0.5978 (0.0002 -- 5.9254)  max mem: 16413
Test:  [240/603]  eta: 0:04:13  loss: 0.4812 (0.7585)  acc1: 83.3333 (76.9710)  acc5: 100.0000 (97.0954)  time: 0.7152 (0.1257 -- 4.2631)  data: 0.5649 (0.0002 -- 4.1283)  max mem: 16413
Test:  [250/603]  eta: 0:04:00  loss: 0.5566 (0.7588)  acc1: 83.3333 (76.8924)  acc5: 100.0000 (97.0784)  time: 0.5683 (0.1257 -- 3.9298)  data: 0.3911 (0.0002 -- 3.6562)  max mem: 16413
Test:  [260/603]  eta: 0:03:53  loss: 0.5824 (0.7628)  acc1: 83.3333 (76.8838)  acc5: 100.0000 (97.0626)  time: 0.4927 (0.1279 -- 5.1218)  data: 0.2867 (0.0002 -- 4.9899)  max mem: 16413
Test:  [270/603]  eta: 0:03:59  loss: 0.4264 (0.7536)  acc1: 83.3333 (77.4293)  acc5: 100.0000 (97.0480)  time: 1.1995 (0.1138 -- 15.6721)  data: 1.0275 (0.0001 -- 15.5312)  max mem: 16413
Test:  [280/603]  eta: 0:03:50  loss: 0.5228 (0.7619)  acc1: 83.3333 (77.1649)  acc5: 100.0000 (96.9158)  time: 1.1202 (0.1138 -- 15.6721)  data: 0.9894 (0.0001 -- 15.5312)  max mem: 16413
Test:  [290/603]  eta: 0:03:40  loss: 0.8369 (0.7620)  acc1: 66.6667 (77.0905)  acc5: 100.0000 (96.9072)  time: 0.4886 (0.1211 -- 4.3684)  data: 0.3557 (0.0002 -- 4.2477)  max mem: 16413
Test:  [300/603]  eta: 0:03:32  loss: 0.5942 (0.7633)  acc1: 83.3333 (77.1318)  acc5: 100.0000 (96.9546)  time: 0.5335 (0.1211 -- 5.2281)  data: 0.3958 (0.0003 -- 5.0502)  max mem: 16413
Test:  [310/603]  eta: 0:03:25  loss: 0.4688 (0.7573)  acc1: 83.3333 (77.5456)  acc5: 100.0000 (96.8917)  time: 0.6880 (0.1148 -- 6.0528)  data: 0.5497 (0.0002 -- 5.9328)  max mem: 16413
Test:  [320/603]  eta: 0:03:18  loss: 0.5449 (0.7627)  acc1: 83.3333 (77.3624)  acc5: 100.0000 (96.8328)  time: 0.7132 (0.1137 -- 6.0528)  data: 0.5829 (0.0002 -- 5.9328)  max mem: 16413
Test:  [330/603]  eta: 0:03:08  loss: 0.8233 (0.7639)  acc1: 66.6667 (77.2910)  acc5: 100.0000 (96.7774)  time: 0.5349 (0.1137 -- 3.6681)  data: 0.4028 (0.0002 -- 3.5247)  max mem: 16413
Test:  [340/603]  eta: 0:03:03  loss: 0.6957 (0.7652)  acc1: 83.3333 (77.2727)  acc5: 100.0000 (96.8231)  time: 0.6023 (0.1189 -- 4.5977)  data: 0.4569 (0.0002 -- 4.4517)  max mem: 16413
Test:  [350/603]  eta: 0:02:54  loss: 0.4667 (0.7601)  acc1: 83.3333 (77.6353)  acc5: 100.0000 (96.7711)  time: 0.6469 (0.1168 -- 4.5977)  data: 0.5025 (0.0002 -- 4.4517)  max mem: 16413
Test:  [360/603]  eta: 0:02:54  loss: 0.6108 (0.7647)  acc1: 83.3333 (77.4700)  acc5: 100.0000 (96.7221)  time: 1.1186 (0.1168 -- 11.7408)  data: 0.9151 (0.0002 -- 11.6121)  max mem: 16413
Test:  [370/603]  eta: 0:02:46  loss: 0.9274 (0.7677)  acc1: 66.6667 (77.4483)  acc5: 100.0000 (96.6757)  time: 1.1161 (0.1285 -- 11.7408)  data: 0.8349 (0.0002 -- 11.6121)  max mem: 16413
Test:  [380/603]  eta: 0:02:38  loss: 0.6005 (0.7700)  acc1: 83.3333 (77.4278)  acc5: 100.0000 (96.6754)  time: 0.5847 (0.1353 -- 5.3674)  data: 0.3418 (0.0003 -- 5.1755)  max mem: 16413
Test:  [390/603]  eta: 0:02:34  loss: 0.5139 (0.7645)  acc1: 83.3333 (77.7494)  acc5: 100.0000 (96.6326)  time: 0.9508 (0.1295 -- 7.3778)  data: 0.7820 (0.0002 -- 7.2184)  max mem: 16413
Test:  [400/603]  eta: 0:02:27  loss: 0.5045 (0.7655)  acc1: 83.3333 (77.6808)  acc5: 100.0000 (96.5503)  time: 0.9809 (0.1168 -- 7.3778)  data: 0.8456 (0.0002 -- 7.2184)  max mem: 16413
Test:  [410/603]  eta: 0:02:18  loss: 0.7279 (0.7662)  acc1: 83.3333 (77.7372)  acc5: 100.0000 (96.5531)  time: 0.6032 (0.1168 -- 6.5782)  data: 0.4643 (0.0002 -- 6.4568)  max mem: 16413
Test:  [420/603]  eta: 0:02:13  loss: 0.6212 (0.7682)  acc1: 83.3333 (77.7118)  acc5: 100.0000 (96.5558)  time: 0.8219 (0.1245 -- 9.7254)  data: 0.6762 (0.0004 -- 9.5952)  max mem: 16413
Test:  [430/603]  eta: 0:02:05  loss: 0.5005 (0.7638)  acc1: 83.3333 (77.9582)  acc5: 100.0000 (96.5197)  time: 0.8472 (0.1248 -- 9.7254)  data: 0.7047 (0.0004 -- 9.5952)  max mem: 16413
Test:  [440/603]  eta: 0:01:56  loss: 0.5005 (0.7646)  acc1: 83.3333 (77.8912)  acc5: 100.0000 (96.4475)  time: 0.4186 (0.1230 -- 3.5203)  data: 0.2773 (0.0003 -- 3.3826)  max mem: 16413
Test:  [450/603]  eta: 0:01:49  loss: 0.7115 (0.7654)  acc1: 83.3333 (77.9379)  acc5: 100.0000 (96.4523)  time: 0.4608 (0.1230 -- 2.8102)  data: 0.3237 (0.0003 -- 2.6809)  max mem: 16413
Test:  [460/603]  eta: 0:01:43  loss: 0.6320 (0.7673)  acc1: 83.3333 (77.9103)  acc5: 100.0000 (96.4570)  time: 0.8318 (0.1278 -- 7.8181)  data: 0.6026 (0.0003 -- 7.6928)  max mem: 16413
Test:  [470/603]  eta: 0:01:36  loss: 0.4723 (0.7631)  acc1: 83.3333 (78.1316)  acc5: 100.0000 (96.4260)  time: 1.0227 (0.1213 -- 7.8181)  data: 0.7777 (0.0003 -- 7.6928)  max mem: 16413
Test:  [480/603]  eta: 0:01:28  loss: 0.4160 (0.7637)  acc1: 83.3333 (78.1012)  acc5: 100.0000 (96.3617)  time: 0.6284 (0.1213 -- 5.3236)  data: 0.4789 (0.0002 -- 5.1774)  max mem: 16413
Test:  [490/603]  eta: 0:01:21  loss: 0.8378 (0.7644)  acc1: 66.6667 (78.0380)  acc5: 100.0000 (96.3340)  time: 0.5530 (0.1197 -- 5.6905)  data: 0.4225 (0.0002 -- 5.5664)  max mem: 16413
Test:  [500/603]  eta: 0:01:15  loss: 0.8643 (0.7709)  acc1: 66.6667 (77.8776)  acc5: 100.0000 (96.1743)  time: 1.1269 (0.1197 -- 8.0767)  data: 0.9813 (0.0002 -- 7.8316)  max mem: 16413
Test:  [510/603]  eta: 0:01:07  loss: 0.5289 (0.7684)  acc1: 83.3333 (78.0170)  acc5: 100.0000 (96.1513)  time: 0.9068 (0.1242 -- 8.0767)  data: 0.7592 (0.0003 -- 7.8316)  max mem: 16413
Test:  [520/603]  eta: 0:01:00  loss: 0.6881 (0.7709)  acc1: 83.3333 (77.7671)  acc5: 100.0000 (96.2252)  time: 0.5399 (0.1242 -- 3.9246)  data: 0.3029 (0.0003 -- 3.7931)  max mem: 16413
Test:  [530/603]  eta: 0:00:52  loss: 0.8542 (0.7721)  acc1: 66.6667 (77.7150)  acc5: 100.0000 (96.1707)  time: 0.4757 (0.1249 -- 3.9246)  data: 0.2305 (0.0002 -- 3.7931)  max mem: 16413
Test:  [540/603]  eta: 0:00:44  loss: 0.7442 (0.7781)  acc1: 83.3333 (77.5724)  acc5: 100.0000 (96.0259)  time: 0.3334 (0.1324 -- 2.1264)  data: 0.1881 (0.0002 -- 1.9772)  max mem: 16413
Test:  [550/603]  eta: 0:00:38  loss: 0.4774 (0.7757)  acc1: 83.3333 (77.7072)  acc5: 100.0000 (96.0073)  time: 0.9046 (0.1262 -- 11.6793)  data: 0.7661 (0.0004 -- 11.5386)  max mem: 16413
Test:  [560/603]  eta: 0:00:30  loss: 0.4589 (0.7776)  acc1: 83.3333 (77.4807)  acc5: 100.0000 (96.0784)  time: 0.8345 (0.1212 -- 11.6793)  data: 0.7037 (0.0002 -- 11.5386)  max mem: 16413
Test:  [570/603]  eta: 0:00:23  loss: 0.8558 (0.7788)  acc1: 66.6667 (77.4372)  acc5: 100.0000 (96.0304)  time: 0.2664 (0.1205 -- 1.8337)  data: 0.1337 (0.0002 -- 1.7072)  max mem: 16413
Test:  [580/603]  eta: 0:00:16  loss: 0.7571 (0.7843)  acc1: 83.3333 (77.3092)  acc5: 100.0000 (95.8979)  time: 0.7367 (0.1205 -- 5.9811)  data: 0.5513 (0.0003 -- 5.8534)  max mem: 16413
Test:  [590/603]  eta: 0:00:09  loss: 0.4973 (0.7819)  acc1: 83.3333 (77.4394)  acc5: 100.0000 (95.8827)  time: 1.0494 (0.1276 -- 5.9811)  data: 0.8620 (0.0002 -- 5.8534)  max mem: 16413
Test:  [600/603]  eta: 0:00:02  loss: 0.6471 (0.7836)  acc1: 66.6667 (77.2324)  acc5: 100.0000 (95.9512)  time: 0.6218 (0.1177 -- 4.3484)  data: 0.4831 (0.0001 -- 4.2218)  max mem: 16413
Test:  [602/603]  eta: 0:00:00  loss: 0.6471 (0.7838)  acc1: 66.6667 (77.2614)  acc5: 100.0000 (95.9336)  time: 0.4092 (0.1161 -- 3.0830)  data: 0.2720 (0.0001 -- 2.9513)  max mem: 16413
Test: Total time: 0:07:09 (0.7122 s / it)
* Acc@1 78.755 Acc@5 95.892 loss 0.763
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 7230 test videos: Top-1: 86.10%, Top-5: 98.96%
Training time 8:27:19
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
