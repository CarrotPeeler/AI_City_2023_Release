[2023-09-04 11:23:05,834] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-04 11:23:05,899] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.9, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.1, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f7ceb0c80a0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.1,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_1_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_2_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_3_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_4_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_5_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_6_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441
  },
  "layer_7_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_8_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561
  },
  "layer_9_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_10_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.81
  },
  "layer_11_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.81
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.9
  },
  "layer_12_decay": {
    "weight_decay": 0.1,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.9
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.1,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-09-04 11:23:11,043] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-04 11:23:11,043] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-04 11:23:11,233] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-04 11:23:11,234] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-04 11:23:11,337] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.5058200359344482 seconds
[2023-09-04 11:23:12,449] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-04 11:23:12,459] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-09-04 11:23:12,459] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-09-04 11:23:12,486] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-04 11:23:12,486] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-04 11:23:12,487] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-04 11:23:12,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 11:23:12,487] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-04 11:23:12,488] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-04 11:23:12,488] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-04 11:23:12,488] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-04 11:23:12,488] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7c89099400>
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-04 11:23:12,489] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-04 11:23:12,490] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.1, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   world_size ................... 2
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-04 11:23:12,491] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-04 11:23:12,492] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.1, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.1000000, Min WD = 0.1000000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:32:40  lr: 0.000000  min_lr: 0.000000  loss: 2.7734 (2.7734)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  time: 12.2527 (12.2527 -- 12.2527)  data: 6.3145 (6.3145 -- 6.3145)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:42  lr: 0.000001  min_lr: 0.000000  loss: 2.7730 (2.7729)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.5933 (1.6021)  time: 0.6098 (0.4815 -- 2.0403)  data: 0.0018 (0.0004 -- 0.0070)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:11  lr: 0.000002  min_lr: 0.000001  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.4587 (1.5416)  time: 1.0283 (0.5063 -- 4.3661)  data: 0.0012 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000001  loss: 2.7724 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.4176 (1.5245)  time: 0.7951 (0.5107 -- 3.3756)  data: 0.0025 (0.0004 -- 0.0144)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:19  lr: 0.000005  min_lr: 0.000001  loss: 2.7722 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.4500 (1.5163)  time: 0.9843 (0.5203 -- 5.0315)  data: 0.0020 (0.0004 -- 0.0055)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000001  loss: 2.7720 (2.7724)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.4748 (1.5100)  time: 0.6964 (0.5063 -- 3.1237)  data: 0.0016 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000002  loss: 2.7713 (2.7722)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.5045 (1.5100)  time: 0.9384 (0.5182 -- 4.4780)  data: 0.0014 (0.0004 -- 0.0036)  max mem: 16413
[2023-09-04 11:25:12,067] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:25:12,067] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:25:12,067] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-09-04 11:25:12,068] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 2.7704 (2.7719)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.4517 (1.4996)  time: 0.7866 (0.5223 -- 3.5722)  data: 0.0025 (0.0004 -- 0.0161)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 2.7694 (2.7716)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.3417 (1.4860)  time: 0.7434 (0.4914 -- 3.4111)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [0] Total time: 0:02:23 (0.8963 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 2.7694 (2.7716)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.3417 (1.4860)
Val:  [ 0/27]  eta: 0:01:20  loss: 2.7630 (2.7630)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.9948 (2.9948 -- 2.9948)  data: 2.6447 (2.6447 -- 2.6447)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7632 (2.7633)  acc1: 33.3333 (34.3434)  acc5: 88.8889 (86.8687)  time: 0.4615 (0.1952 -- 2.9948)  data: 0.2412 (0.0004 -- 2.6447)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7632 (2.7628)  acc1: 33.3333 (37.0370)  acc5: 88.8889 (87.8307)  time: 0.2102 (0.1691 -- 0.3348)  data: 0.0083 (0.0001 -- 0.1544)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7632 (2.7632)  acc1: 33.3333 (36.5145)  acc5: 85.7143 (85.4772)  time: 0.1998 (0.1666 -- 0.3348)  data: 0.0082 (0.0001 -- 0.1544)  max mem: 16413
Val: Total time: 0:00:08 (0.3043 s / it)
* Acc@1 37.344 Acc@5 84.855 loss 2.763
Accuracy of the network on the 482 val images: 37.34%
[2023-09-04 11:25:44,242] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-09-04 11:25:44,245] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:25:44,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:25:44,245] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:25:45,135] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:25:45,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 37.34%
Epoch: [1]  [  0/160]  eta: 0:20:02  lr: 0.000009  min_lr: 0.000002  loss: 2.7688 (2.7688)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6530 (1.6530)  time: 7.5133 (7.5133 -- 7.5133)  data: 6.9883 (6.9883 -- 6.9883)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:34  lr: 0.000011  min_lr: 0.000003  loss: 2.7666 (2.7664)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.5039 (1.5058)  time: 0.7849 (0.5199 -- 3.2546)  data: 0.2452 (0.0004 -- 2.7273)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:04  lr: 0.000012  min_lr: 0.000003  loss: 2.7646 (2.7654)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.5720 (1.5282)  time: 0.9643 (0.5228 -- 3.3959)  data: 0.2806 (0.0003 -- 2.8313)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000003  loss: 2.7588 (2.7630)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.5767 (1.5423)  time: 0.8623 (0.5037 -- 2.6809)  data: 0.1479 (0.0005 -- 1.6698)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:17  lr: 0.000014  min_lr: 0.000004  loss: 2.7522 (2.7603)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.5493 (1.5350)  time: 0.9300 (0.5195 -- 3.6572)  data: 0.3315 (0.0004 -- 3.0965)  max mem: 16413
[2023-09-04 11:27:14,593] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:27:14,593] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-09-04 11:27:14,593] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:27:14,594] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000004  loss: 2.7459 (2.7569)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.5926 (1.5488)  time: 0.8178 (0.5228 -- 3.0668)  data: 0.2485 (0.0004 -- 2.5255)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000004  loss: 2.7297 (2.7524)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.5893 (1.5576)  time: 0.8146 (0.5268 -- 3.2915)  data: 0.2485 (0.0003 -- 2.7618)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000004  loss: 2.7184 (2.7472)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7228 (1.5773)  time: 0.8892 (0.5172 -- 3.5658)  data: 0.2809 (0.0002 -- 2.9992)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 2.7124 (2.7432)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7090 (1.5890)  time: 0.6655 (0.4949 -- 2.5566)  data: 0.1504 (0.0002 -- 2.0255)  max mem: 16413
Epoch: [1] Total time: 0:02:21 (0.8849 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 2.7124 (2.7437)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7090 (1.5890)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.6276 (2.6276)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3402 (2.3402 -- 2.3402)  data: 2.1183 (2.1183 -- 2.1183)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.6536 (2.6528)  acc1: 33.3333 (31.3131)  acc5: 100.0000 (87.8788)  time: 0.4123 (0.1959 -- 2.3402)  data: 0.1981 (0.0005 -- 2.1183)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6424 (2.6472)  acc1: 33.3333 (35.9788)  acc5: 100.0000 (89.9471)  time: 0.2180 (0.1692 -- 0.3946)  data: 0.0133 (0.0001 -- 0.1998)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6476 (2.6522)  acc1: 33.3333 (34.8548)  acc5: 85.7143 (85.8921)  time: 0.2023 (0.1327 -- 0.3946)  data: 0.0129 (0.0001 -- 0.1998)  max mem: 16413
Val: Total time: 0:00:07 (0.2847 s / it)
* Acc@1 36.307 Acc@5 85.892 loss 2.651
Accuracy of the network on the 482 val images: 36.31%
Max accuracy: 37.34%
Epoch: [2]  [  0/160]  eta: 0:22:04  lr: 0.000019  min_lr: 0.000005  loss: 2.7363 (2.7363)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7051 (1.7051)  time: 8.2810 (8.2810 -- 8.2810)  data: 7.7664 (7.7664 -- 7.7664)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:37  lr: 0.000020  min_lr: 0.000005  loss: 2.6989 (2.7034)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6754 (1.6326)  time: 0.7640 (0.5216 -- 4.2826)  data: 0.2206 (0.0003 -- 3.7584)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:06  lr: 0.000021  min_lr: 0.000005  loss: 2.6861 (2.6961)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6831 (1.6562)  time: 0.9847 (0.5248 -- 4.5892)  data: 0.4400 (0.0002 -- 4.0502)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000006  loss: 2.6693 (2.6866)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7239 (1.6968)  time: 0.7669 (0.5273 -- 3.2671)  data: 0.2243 (0.0004 -- 2.7591)  max mem: 16413
[2023-09-04 11:29:16,780] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:29:16,781] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:29:16,781] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-09-04 11:29:16,781] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000006  loss: 2.6630 (2.6811)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.6977 (1.7016)  time: 0.8725 (0.5090 -- 3.5285)  data: 0.2740 (0.0003 -- 3.0102)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000006  loss: 2.6414 (2.6760)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7634 (1.7246)  time: 0.9245 (0.5220 -- 3.0946)  data: 0.0695 (0.0002 -- 0.7889)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000007  loss: 2.6389 (2.6707)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7691 (1.7470)  time: 0.8623 (0.5320 -- 2.2385)  data: 0.2171 (0.0009 -- 1.7121)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 2.6157 (2.6619)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.7708 (1.7638)  time: 0.8056 (0.5298 -- 2.5486)  data: 0.1670 (0.0004 -- 2.0249)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 2.5954 (2.6534)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8755 (1.7752)  time: 0.6835 (0.4939 -- 2.1717)  data: 0.1138 (0.0002 -- 1.0971)  max mem: 16413
Epoch: [2] Total time: 0:02:21 (0.8815 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 2.5954 (2.6520)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.8755 (1.7752)
Val:  [ 0/27]  eta: 0:01:02  loss: 2.4019 (2.4019)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3228 (2.3228 -- 2.3228)  data: 2.1153 (2.1153 -- 2.1153)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.4657 (2.4622)  acc1: 33.3333 (35.3535)  acc5: 100.0000 (91.9192)  time: 0.4387 (0.1990 -- 2.3228)  data: 0.2264 (0.0003 -- 2.1153)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.4288 (2.4457)  acc1: 33.3333 (35.4497)  acc5: 100.0000 (92.0635)  time: 0.2275 (0.1700 -- 0.5816)  data: 0.0260 (0.0001 -- 0.3590)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.4849 (2.4579)  acc1: 33.3333 (33.6100)  acc5: 88.8889 (89.6266)  time: 0.2129 (0.1328 -- 0.5816)  data: 0.0257 (0.0001 -- 0.3590)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 38.382 Acc@5 88.797 loss 2.462
Accuracy of the network on the 482 val images: 38.38%
[2023-09-04 11:30:43,321] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:30:43,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:30:43,323] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:30:43,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:30:44,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:30:44,494] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 38.38%
Epoch: [3]  [  0/160]  eta: 0:15:54  lr: 0.000028  min_lr: 0.000007  loss: 2.5754 (2.5754)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1602 (2.1602)  time: 5.9660 (5.9660 -- 5.9660)  data: 5.4197 (5.4197 -- 5.4197)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:37  lr: 0.000029  min_lr: 0.000007  loss: 2.6071 (2.6104)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0114 (2.0642)  time: 0.8839 (0.5193 -- 2.9933)  data: 0.2595 (0.0007 -- 2.1757)  max mem: 16413
[2023-09-04 11:31:20,168] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:31:20,168] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-09-04 11:31:20,170] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:31:20,171] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:01  lr: 0.000031  min_lr: 0.000008  loss: 2.5678 (2.5880)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.1000 (0.1000)  grad_norm: 1.9676 (2.0170)  time: 0.9015 (0.5247 -- 3.7579)  data: 0.3260 (0.0006 -- 3.2063)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:36  lr: 0.000032  min_lr: 0.000008  loss: 2.5648 (2.5738)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0874 (2.0655)  time: 0.8671 (0.5193 -- 3.1976)  data: 0.2717 (0.0003 -- 2.6856)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:15  lr: 0.000033  min_lr: 0.000008  loss: 2.5478 (2.5678)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.2343 (2.1181)  time: 0.8848 (0.5291 -- 3.1848)  data: 0.3215 (0.0006 -- 2.6194)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000009  loss: 2.5565 (2.5652)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.0603 (2.1297)  time: 0.8724 (0.5273 -- 3.4493)  data: 0.0325 (0.0003 -- 0.6236)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000009  loss: 2.4790 (2.5535)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3113 (2.1940)  time: 0.8026 (0.5275 -- 2.7595)  data: 0.1311 (0.0002 -- 2.2304)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 2.5193 (2.5477)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3679 (2.2217)  time: 0.9174 (0.5215 -- 4.0607)  data: 0.3697 (0.0002 -- 3.5424)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 2.5127 (2.5417)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4909 (2.2832)  time: 0.7119 (0.4939 -- 3.0093)  data: 0.1866 (0.0002 -- 2.5133)  max mem: 16413
Epoch: [3] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 2.5127 (2.5410)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4909 (2.2832)
Val:  [ 0/27]  eta: 0:01:02  loss: 2.1897 (2.1897)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3012 (2.3012 -- 2.3012)  data: 2.0647 (2.0647 -- 2.0647)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.2920 (2.2775)  acc1: 33.3333 (38.3838)  acc5: 88.8889 (88.8889)  time: 0.4152 (0.2039 -- 2.3012)  data: 0.2004 (0.0005 -- 2.0647)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.2358 (2.2499)  acc1: 33.3333 (38.0952)  acc5: 88.8889 (88.3598)  time: 0.2223 (0.1691 -- 0.4345)  data: 0.0194 (0.0001 -- 0.2455)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.2613 (2.2708)  acc1: 33.3333 (36.0996)  acc5: 85.7143 (86.3071)  time: 0.2074 (0.1329 -- 0.4345)  data: 0.0191 (0.0001 -- 0.2455)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 38.589 Acc@5 86.722 loss 2.272
Accuracy of the network on the 482 val images: 38.59%
[2023-09-04 11:33:14,500] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:33:14,502] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:33:14,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:33:14,502] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:33:15,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:33:15,882] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 38.59%
[2023-09-04 11:33:21,595] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:33:21,595] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-09-04 11:33:21,598] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:33:21,598] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:15:15  lr: 0.000038  min_lr: 0.000010  loss: 2.4300 (2.4300)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1316 (2.1316)  time: 5.7245 (5.7245 -- 5.7245)  data: 3.9115 (3.9115 -- 3.9115)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:48  lr: 0.000039  min_lr: 0.000010  loss: 2.4652 (2.4386)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.4284 (2.5132)  time: 0.9784 (0.5240 -- 3.4138)  data: 0.3321 (0.0009 -- 2.8952)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:02:01  lr: 0.000040  min_lr: 0.000010  loss: 2.4401 (2.4572)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.6618 (2.5817)  time: 0.8114 (0.5395 -- 3.7965)  data: 0.2512 (0.0005 -- 3.2388)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:39  lr: 0.000041  min_lr: 0.000010  loss: 2.3874 (2.4401)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.3461 (2.5851)  time: 0.9636 (0.5181 -- 3.6145)  data: 0.4275 (0.0003 -- 3.0943)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000011  loss: 2.4311 (2.4304)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.9227 (2.7470)  time: 0.7857 (0.5324 -- 3.7079)  data: 0.2364 (0.0006 -- 3.1740)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 2.4532 (2.4273)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.8628 (2.8537)  time: 0.9355 (0.5266 -- 3.9364)  data: 0.3847 (0.0009 -- 3.3883)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 2.3903 (2.4190)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.9193 (2.9155)  time: 0.7762 (0.5278 -- 4.3990)  data: 0.2245 (0.0002 -- 3.8726)  max mem: 16413
[2023-09-04 11:35:15,053] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:35:15,053] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-09-04 11:35:15,054] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:35:15,054] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.4497 (2.4224)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2599 (2.9858)  time: 1.0097 (0.5142 -- 4.5876)  data: 0.4737 (0.0004 -- 4.0391)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.4092 (2.4231)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.0875 (3.0514)  time: 0.6219 (0.4952 -- 2.2578)  data: 0.1064 (0.0002 -- 1.7366)  max mem: 16413
Epoch: [4] Total time: 0:02:22 (0.8929 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.4092 (2.4365)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.0875 (3.0514)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.9704 (1.9704)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.5765 (2.5765 -- 2.5765)  data: 2.3656 (2.3656 -- 2.3656)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.9939 (2.0181)  acc1: 44.4444 (45.4545)  acc5: 100.0000 (93.9394)  time: 0.4339 (0.2063 -- 2.5765)  data: 0.2162 (0.0005 -- 2.3656)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.9752 (1.9892)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (94.1799)  time: 0.2079 (0.1697 -- 0.2669)  data: 0.0008 (0.0001 -- 0.0025)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0059 (2.0194)  acc1: 44.4444 (41.9087)  acc5: 88.8889 (90.8714)  time: 0.1928 (0.1324 -- 0.2669)  data: 0.0005 (0.0001 -- 0.0025)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 44.398 Acc@5 91.286 loss 2.007
Accuracy of the network on the 482 val images: 44.40%
[2023-09-04 11:35:46,478] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:35:46,480] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:35:46,480] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:35:46,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:35:47,841] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:35:47,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.40%
Epoch: [5]  [  0/160]  eta: 0:19:24  lr: 0.000047  min_lr: 0.000012  loss: 2.2945 (2.2945)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.3312 (3.3312)  time: 7.2752 (7.2752 -- 7.2752)  data: 5.9444 (5.9444 -- 5.9444)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000012  loss: 2.3173 (2.3590)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.1432 (3.5680)  time: 0.8763 (0.5257 -- 4.4018)  data: 0.1400 (0.0007 -- 2.0145)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:02:04  lr: 0.000047  min_lr: 0.000012  loss: 2.3318 (2.3389)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.2955 (3.5799)  time: 0.8868 (0.5269 -- 4.5113)  data: 0.0018 (0.0005 -- 0.0047)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000012  loss: 2.3864 (2.3494)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.1061 (3.5706)  time: 0.8836 (0.5177 -- 3.5688)  data: 0.0637 (0.0001 -- 0.7070)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 2.2985 (2.3374)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1190 (3.7739)  time: 0.8964 (0.5244 -- 3.6088)  data: 0.0138 (0.0004 -- 0.2275)  max mem: 16413
[2023-09-04 11:37:17,806] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:37:17,806] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:37:17,806] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-09-04 11:37:17,806] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 2.3839 (2.3402)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9541 (3.8443)  time: 0.7414 (0.5364 -- 2.6366)  data: 0.0441 (0.0006 -- 0.8478)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.3222 (2.3428)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8040 (3.9603)  time: 0.8259 (0.5147 -- 2.9383)  data: 0.0174 (0.0006 -- 0.3044)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.3466 (2.3445)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8571 (4.0190)  time: 0.8743 (0.5261 -- 2.3562)  data: 0.0791 (0.0009 -- 1.1322)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1967 (2.3333)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6956 (4.0511)  time: 0.6894 (0.4970 -- 1.8793)  data: 0.0024 (0.0002 -- 0.0224)  max mem: 16413
Epoch: [5] Total time: 0:02:20 (0.8766 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1967 (2.3354)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.6956 (4.0511)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.7806 (1.7806)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.2418 (2.2418 -- 2.2418)  data: 2.0205 (2.0205 -- 2.0205)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.8243 (1.8170)  acc1: 55.5556 (48.4848)  acc5: 100.0000 (94.9495)  time: 0.4115 (0.2026 -- 2.2418)  data: 0.1946 (0.0006 -- 2.0205)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.7921 (1.7749)  acc1: 44.4444 (47.6190)  acc5: 100.0000 (95.7672)  time: 0.2274 (0.1686 -- 0.5393)  data: 0.0241 (0.0001 -- 0.3584)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.8243 (1.8140)  acc1: 44.4444 (47.3029)  acc5: 88.8889 (94.1909)  time: 0.2115 (0.1328 -- 0.5393)  data: 0.0238 (0.0001 -- 0.3584)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 50.000 Acc@5 92.946 loss 1.803
Accuracy of the network on the 482 val images: 50.00%
[2023-09-04 11:38:15,882] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:38:15,884] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:38:15,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:38:15,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:38:17,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:38:17,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.00%
Epoch: [6]  [  0/160]  eta: 0:15:51  lr: 0.000047  min_lr: 0.000012  loss: 2.4403 (2.4403)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3159 (5.3159)  time: 5.9462 (5.9462 -- 5.9462)  data: 5.4133 (5.4133 -- 5.4133)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:46  lr: 0.000047  min_lr: 0.000012  loss: 2.3348 (2.4064)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.5060 (3.6840)  time: 0.9482 (0.5233 -- 3.6778)  data: 0.3710 (0.0005 -- 3.1520)  max mem: 16413
[2023-09-04 11:38:59,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.1913801307802163e-05, 1.1913801307802163e-05, 1.3237557008669071e-05, 1.3237557008669071e-05, 1.4708396676298964e-05, 1.4708396676298964e-05, 1.634266297366552e-05, 1.634266297366552e-05, 1.8158514415183906e-05, 1.8158514415183906e-05, 2.017612712798212e-05, 2.017612712798212e-05, 2.2417919031091242e-05, 2.2417919031091242e-05, 2.490879892343471e-05, 2.490879892343471e-05, 2.7676443248260792e-05, 2.7676443248260792e-05, 3.0751603609178656e-05, 3.0751603609178656e-05, 3.416844845464296e-05, 3.416844845464296e-05, 3.7964942727381054e-05, 3.7964942727381054e-05, 4.218326969709006e-05, 4.218326969709006e-05, 4.68702996634334e-05, 4.68702996634334e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 11:38:59,618] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=18.40653539520505, CurrSamplesPerSec=21.89440569053208, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000012  loss: 2.2472 (2.3358)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0866 (3.9219)  time: 0.9029 (0.5227 -- 4.8763)  data: 0.3653 (0.0003 -- 4.3603)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000012  loss: 2.2506 (2.3113)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.4836 (3.9080)  time: 0.9854 (0.5118 -- 3.5753)  data: 0.4421 (0.0004 -- 3.0522)  max mem: 16413
[2023-09-04 11:39:22,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:39:22,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:39:22,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-09-04 11:39:22,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.2376 (2.3027)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9448 (3.9857)  time: 0.7527 (0.5294 -- 4.0871)  data: 0.2010 (0.0002 -- 3.5463)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.2969 (2.3014)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2154 (4.0847)  time: 0.9007 (0.5176 -- 3.5095)  data: 0.3534 (0.0005 -- 2.9740)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1999 (2.2914)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4553 (4.2768)  time: 0.7947 (0.5112 -- 3.3659)  data: 0.2462 (0.0001 -- 2.8168)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.1760 (2.2817)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2460 (4.4044)  time: 0.8750 (0.5202 -- 3.2793)  data: 0.3272 (0.0004 -- 2.7271)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1667 (2.2736)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9568 (4.3821)  time: 0.7336 (0.4933 -- 3.7223)  data: 0.2200 (0.0002 -- 3.1992)  max mem: 16413
Epoch: [6] Total time: 0:02:23 (0.8955 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1667 (2.2759)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9568 (4.3821)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.6802 (1.6802)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3637 (2.3637 -- 2.3637)  data: 2.1574 (2.1574 -- 2.1574)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.6802 (1.6712)  acc1: 55.5556 (52.5253)  acc5: 100.0000 (94.9495)  time: 0.4212 (0.2037 -- 2.3637)  data: 0.2061 (0.0007 -- 2.1574)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6485 (1.6739)  acc1: 55.5556 (50.2646)  acc5: 100.0000 (93.1217)  time: 0.2225 (0.1685 -- 0.4400)  data: 0.0186 (0.0001 -- 0.2600)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6842 (1.7143)  acc1: 44.4444 (48.9627)  acc5: 88.8889 (92.5311)  time: 0.2069 (0.1325 -- 0.4400)  data: 0.0183 (0.0001 -- 0.2600)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 53.112 Acc@5 91.494 loss 1.685
Accuracy of the network on the 482 val images: 53.11%
[2023-09-04 11:40:48,254] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:40:48,256] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:40:48,256] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:40:48,256] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:40:49,644] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:40:49,644] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 53.11%
Epoch: [7]  [  0/160]  eta: 0:18:54  lr: 0.000047  min_lr: 0.000012  loss: 1.8359 (1.8359)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.8629 (2.8629)  time: 7.0882 (7.0882 -- 7.0882)  data: 6.5494 (6.5494 -- 6.5494)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:58  lr: 0.000047  min_lr: 0.000012  loss: 2.3007 (2.2763)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1219 (4.6215)  time: 0.9828 (0.5265 -- 4.3748)  data: 0.2704 (0.0004 -- 3.8158)  max mem: 16413
[2023-09-04 11:41:25,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:41:25,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-09-04 11:41:25,110] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:41:25,110] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:02:02  lr: 0.000047  min_lr: 0.000012  loss: 2.1888 (2.2373)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1684 (4.5197)  time: 0.7469 (0.5263 -- 2.6213)  data: 0.0016 (0.0005 -- 0.0029)  max mem: 16413
[2023-09-04 11:41:42,469] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1172
[2023-09-04 11:41:42,470] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-09-04 11:41:42,470] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1172
[2023-09-04 11:41:42,470] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
[2023-09-04 11:41:42,470] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
Epoch: [7]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000012  loss: 2.3499 (2.2624)  loss_scale: 65536.0000 (43511.6066)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7170 (4.6023)  time: 0.8621 (0.5194 -- 3.1064)  data: 0.0016 (0.0007 -- 0.0039)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:13  lr: 0.000047  min_lr: 0.000012  loss: 2.3329 (2.2792)  loss_scale: 32768.0000 (40858.8642)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4548 (4.5052)  time: 0.7920 (0.5331 -- 3.3687)  data: 0.0332 (0.0006 -- 0.4322)  max mem: 16413
Epoch: [7]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 2.0995 (2.2563)  loss_scale: 32768.0000 (39256.7129)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2981 (4.5617)  time: 0.9046 (0.5154 -- 3.8297)  data: 0.0439 (0.0004 -- 0.4850)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.2188 (2.2502)  loss_scale: 32768.0000 (38184.1983)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1188 (4.7338)  time: 0.8646 (0.5295 -- 2.4280)  data: 0.0014 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000012  loss: 2.3199 (2.2461)  loss_scale: 32768.0000 (37415.9433)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9585 (4.7783)  time: 0.8372 (0.5326 -- 2.9517)  data: 0.1538 (0.0005 -- 2.4180)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.3069 (2.2498)  loss_scale: 32768.0000 (36864.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8695 (4.7816)  time: 0.7070 (0.4945 -- 3.5362)  data: 0.1848 (0.0002 -- 2.9949)  max mem: 16413
Epoch: [7] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.3069 (2.2464)  loss_scale: 32768.0000 (36864.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8695 (4.7816)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.5526 (1.5526)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4248 (2.4248 -- 2.4248)  data: 2.2158 (2.2158 -- 2.2158)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5526 (1.6151)  acc1: 44.4444 (47.4747)  acc5: 100.0000 (94.9495)  time: 0.4159 (0.2015 -- 2.4248)  data: 0.2025 (0.0007 -- 2.2158)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5489 (1.5808)  acc1: 44.4444 (50.7937)  acc5: 100.0000 (95.2381)  time: 0.2127 (0.1683 -- 0.3368)  data: 0.0086 (0.0001 -- 0.1559)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5900 (1.6099)  acc1: 44.4444 (51.0373)  acc5: 100.0000 (94.1909)  time: 0.1969 (0.1330 -- 0.3368)  data: 0.0082 (0.0001 -- 0.1559)  max mem: 16413
Val: Total time: 0:00:07 (0.2838 s / it)
* Acc@1 55.394 Acc@5 93.154 loss 1.585
Accuracy of the network on the 482 val images: 55.39%
[2023-09-04 11:43:17,900] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:43:17,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:43:17,905] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:43:17,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:43:19,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:43:19,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 55.39%
Epoch: [8]  [  0/160]  eta: 0:19:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0099 (2.0099)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9720 (5.9720)  time: 7.2411 (7.2411 -- 7.2411)  data: 6.4530 (6.4530 -- 6.4530)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:46  lr: 0.000047  min_lr: 0.000012  loss: 2.1793 (2.1709)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2319 (4.6166)  time: 0.8840 (0.5236 -- 2.6111)  data: 0.1565 (0.0005 -- 1.6614)  max mem: 16413
[2023-09-04 11:43:44,648] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:43:44,648] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:43:44,689] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 11:43:44,689] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 11:43:59,100] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1313
[2023-09-04 11:43:59,100] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1313
[2023-09-04 11:43:59,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 11:43:59,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 11:43:59,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [8]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000012  loss: 2.0868 (2.1367)  loss_scale: 65536.0000 (42358.6341)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6054 (5.1153)  time: 0.9349 (0.5089 -- 4.9403)  data: 0.0267 (0.0004 -- 0.5102)  max mem: 16413
Epoch: [8]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000012  loss: 2.1624 (2.1648)  loss_scale: 32768.0000 (39214.1639)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5552 (5.0605)  time: 0.8943 (0.5131 -- 4.2743)  data: 0.0014 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [8]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.1820 (2.1709)  loss_scale: 32768.0000 (37622.5185)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4900 (4.9718)  time: 0.8195 (0.5313 -- 4.2835)  data: 0.0015 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.3123 (2.1934)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7399 (4.9288)  time: 0.8549 (0.5259 -- 2.8731)  data: 0.0020 (0.0006 -- 0.0099)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1431 (2.1986)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.2364 (4.8395)  time: 0.7581 (0.5356 -- 2.8159)  data: 0.0019 (0.0004 -- 0.0031)  max mem: 16413
[2023-09-04 11:45:19,650] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1411
[2023-09-04 11:45:19,650] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1411
[2023-09-04 11:45:19,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 11:45:19,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 11:45:19,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [8]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.3785 (2.2147)  loss_scale: 16384.0000 (34394.7801)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1162 (4.9085)  time: 0.8999 (0.5283 -- 1.9956)  data: 0.0020 (0.0005 -- 0.0054)  max mem: 16413
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.2416 (2.2172)  loss_scale: 16384.0000 (32256.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4361 (4.9268)  time: 0.7067 (0.4950 -- 2.1036)  data: 0.0059 (0.0001 -- 0.0974)  max mem: 16413
Epoch: [8] Total time: 0:02:21 (0.8857 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.2416 (2.2109)  loss_scale: 16384.0000 (32256.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4361 (4.9268)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.4322 (1.4322)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4027 (2.4027 -- 2.4027)  data: 2.1572 (2.1572 -- 2.1572)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.4322 (1.4950)  acc1: 44.4444 (53.5354)  acc5: 100.0000 (95.9596)  time: 0.4172 (0.2046 -- 2.4027)  data: 0.2000 (0.0008 -- 2.1572)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4103 (1.4704)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (95.2381)  time: 0.2124 (0.1701 -- 0.2879)  data: 0.0074 (0.0001 -- 0.1021)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5223 (1.5061)  acc1: 55.5556 (56.8465)  acc5: 100.0000 (95.0207)  time: 0.1973 (0.1333 -- 0.2879)  data: 0.0071 (0.0001 -- 0.1021)  max mem: 16413
Val: Total time: 0:00:07 (0.2830 s / it)
* Acc@1 59.751 Acc@5 93.983 loss 1.481
Accuracy of the network on the 482 val images: 59.75%
[2023-09-04 11:45:48,650] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:45:48,652] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:45:48,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:45:48,652] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:45:50,050] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:45:50,050] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.75%
Epoch: [9]  [  0/160]  eta: 0:16:33  lr: 0.000047  min_lr: 0.000012  loss: 2.0406 (2.0406)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9142 (3.9142)  time: 6.2118 (6.2118 -- 6.2118)  data: 5.5324 (5.5324 -- 5.5324)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:03:03  lr: 0.000047  min_lr: 0.000012  loss: 2.0913 (2.1618)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6505 (5.1924)  time: 1.0668 (0.5221 -- 4.2512)  data: 0.0017 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [9]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000012  loss: 2.1588 (2.1822)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1638 (5.5194)  time: 0.7710 (0.5303 -- 2.8488)  data: 0.0016 (0.0001 -- 0.0048)  max mem: 16413
Epoch: [9]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000012  loss: 2.1716 (2.1661)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9030 (5.4140)  time: 0.9793 (0.5195 -- 4.8108)  data: 0.0015 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.2510 (2.1678)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8147 (5.4710)  time: 0.7329 (0.5236 -- 2.2298)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
[2023-09-04 11:47:26,525] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:47:26,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 11:47:26,526] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:47:26,527] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [9]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 2.2768 (2.1692)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9328 (5.4309)  time: 0.9627 (0.5240 -- 3.0862)  data: 0.0016 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [9]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1700 (2.1675)  loss_scale: 32768.0000 (19227.5041)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5736 (5.4421)  time: 0.7710 (0.5225 -- 2.0633)  data: 0.0015 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0324 (2.1497)  loss_scale: 32768.0000 (21148.1418)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0289 (5.4716)  time: 0.9524 (0.5280 -- 3.0172)  data: 0.0023 (0.0002 -- 0.0133)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1251 (2.1508)  loss_scale: 32768.0000 (22528.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3664 (5.4910)  time: 0.6390 (0.4948 -- 1.8892)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [9] Total time: 0:02:23 (0.8948 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1251 (2.1568)  loss_scale: 32768.0000 (22528.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3664 (5.4910)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.3980 (1.3980)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2340 (2.2340 -- 2.2340)  data: 2.0291 (2.0291 -- 2.0291)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.3980 (1.4243)  acc1: 66.6667 (57.5758)  acc5: 100.0000 (96.9697)  time: 0.4032 (0.2061 -- 2.2340)  data: 0.1859 (0.0007 -- 2.0291)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3039 (1.3917)  acc1: 66.6667 (57.6720)  acc5: 100.0000 (96.8254)  time: 0.2248 (0.1697 -- 0.3908)  data: 0.0165 (0.0001 -- 0.1699)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3733 (1.4305)  acc1: 66.6667 (59.3361)  acc5: 100.0000 (96.2656)  time: 0.2060 (0.1324 -- 0.3908)  data: 0.0162 (0.0001 -- 0.1699)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 59.751 Acc@5 95.021 loss 1.409
Accuracy of the network on the 482 val images: 59.75%
Max accuracy: 59.75%
Epoch: [10]  [  0/160]  eta: 0:24:16  lr: 0.000047  min_lr: 0.000012  loss: 2.4210 (2.4210)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3361 (5.3361)  time: 9.1001 (9.1001 -- 9.1001)  data: 8.5824 (8.5824 -- 8.5824)  max mem: 16413
Epoch: [10]  [ 20/160]  eta: 0:02:51  lr: 0.000047  min_lr: 0.000012  loss: 2.0743 (2.1361)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8090 (5.3444)  time: 0.8336 (0.5144 -- 4.4204)  data: 0.2856 (0.0003 -- 3.9083)  max mem: 16413
Epoch: [10]  [ 40/160]  eta: 0:02:08  lr: 0.000047  min_lr: 0.000012  loss: 2.1385 (2.1677)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0427 (5.5379)  time: 0.9142 (0.5153 -- 3.8816)  data: 0.2007 (0.0003 -- 2.1668)  max mem: 16413
Epoch: [10]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000012  loss: 1.9412 (2.1020)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9781 (5.6799)  time: 0.8201 (0.5191 -- 3.2337)  data: 0.0179 (0.0006 -- 0.3318)  max mem: 16413
[2023-09-04 11:49:27,928] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:49:27,928] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:49:27,928] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 11:49:27,928] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 11:49:29,545] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1671
[2023-09-04 11:49:29,545] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1671
[2023-09-04 11:49:29,545] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 11:49:29,546] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 11:49:29,546] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [10]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 2.0074 (2.0764)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.8217 (5.4560)  time: 0.8932 (0.5178 -- 3.5510)  data: 0.0015 (0.0003 -- 0.0067)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.2754 (2.1066)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8277 (5.5655)  time: 0.8192 (0.5264 -- 3.0553)  data: 0.0016 (0.0004 -- 0.0041)  max mem: 16413
Epoch: [10]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1236 (2.1040)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5997 (5.5090)  time: 0.7942 (0.5280 -- 3.5495)  data: 0.1075 (0.0003 -- 2.1153)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.2240 (2.1218)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3333 (5.5935)  time: 0.9819 (0.5283 -- 4.2419)  data: 0.4135 (0.0003 -- 3.7278)  max mem: 16413
[2023-09-04 11:50:32,317] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1742
[2023-09-04 11:50:32,317] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 11:50:32,317] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1742
[2023-09-04 11:50:32,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 11:50:32,318] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1847 (2.1356)  loss_scale: 16384.0000 (31539.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1027 (5.6421)  time: 0.6603 (0.4939 -- 3.1285)  data: 0.1437 (0.0002 -- 2.6108)  max mem: 16413
Epoch: [10] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1847 (2.1444)  loss_scale: 16384.0000 (31539.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1027 (5.6421)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.3645 (1.3645)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3346 (2.3346 -- 2.3346)  data: 2.1006 (2.1006 -- 2.1006)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2886 (1.3341)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (97.9798)  time: 0.4281 (0.2119 -- 2.3346)  data: 0.1987 (0.0009 -- 2.1006)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2692 (1.3197)  acc1: 55.5556 (60.3175)  acc5: 100.0000 (96.8254)  time: 0.2170 (0.1697 -- 0.3385)  data: 0.0051 (0.0001 -- 0.0742)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2886 (1.3707)  acc1: 55.5556 (60.1660)  acc5: 100.0000 (95.0207)  time: 0.1997 (0.1327 -- 0.3385)  data: 0.0047 (0.0001 -- 0.0742)  max mem: 16413
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 61.411 Acc@5 94.398 loss 1.356
Accuracy of the network on the 482 val images: 61.41%
[2023-09-04 11:50:51,561] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:50:51,563] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:50:51,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:50:51,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:50:52,921] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:50:52,921] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 61.41%
Epoch: [11]  [  0/160]  eta: 0:18:17  lr: 0.000047  min_lr: 0.000012  loss: 2.4140 (2.4140)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1983 (4.1983)  time: 6.8624 (6.8624 -- 6.8624)  data: 5.5224 (5.5224 -- 5.5224)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000012  loss: 2.1859 (2.2279)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9733 (5.4632)  time: 0.8465 (0.5319 -- 2.4069)  data: 0.1839 (0.0009 -- 1.8548)  max mem: 16413
Epoch: [11]  [ 40/160]  eta: 0:02:02  lr: 0.000047  min_lr: 0.000012  loss: 2.1112 (2.1703)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4847 (5.6069)  time: 0.9009 (0.5294 -- 3.5039)  data: 0.0443 (0.0004 -- 0.8395)  max mem: 16413
Epoch: [11]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000012  loss: 2.2167 (2.1785)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8243 (5.7114)  time: 0.9814 (0.5133 -- 4.0858)  data: 0.0496 (0.0003 -- 0.7007)  max mem: 16413
Epoch: [11]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.2097 (2.1781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6426 (5.8170)  time: 0.8116 (0.5148 -- 3.2341)  data: 0.0010 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 2.0790 (2.1628)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9582 (5.9318)  time: 0.9148 (0.5320 -- 4.1994)  data: 0.0028 (0.0004 -- 0.0178)  max mem: 16413
[2023-09-04 11:52:37,434] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:52:37,435] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 11:52:37,436] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:52:37,436] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.1314 (2.1590)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4306 (5.8895)  time: 0.8414 (0.5259 -- 3.9345)  data: 0.0019 (0.0004 -- 0.0090)  max mem: 16413
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0142 (2.1339)  loss_scale: 32768.0000 (19869.9574)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3399 (5.8787)  time: 0.8916 (0.5162 -- 4.2826)  data: 0.0013 (0.0004 -- 0.0041)  max mem: 16413
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9894 (2.1221)  loss_scale: 32768.0000 (21401.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9423 (5.8401)  time: 0.7345 (0.4945 -- 2.9672)  data: 0.0195 (0.0002 -- 0.3779)  max mem: 16413
Epoch: [11] Total time: 0:02:24 (0.9052 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9894 (2.1055)  loss_scale: 32768.0000 (21401.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9423 (5.8401)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.2260 (1.2260)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4409 (2.4409 -- 2.4409)  data: 2.1602 (2.1602 -- 2.1602)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2260 (1.3049)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (97.9798)  time: 0.4174 (0.2036 -- 2.4409)  data: 0.1976 (0.0009 -- 2.1602)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2114 (1.2614)  acc1: 66.6667 (58.7302)  acc5: 100.0000 (96.8254)  time: 0.2101 (0.1704 -- 0.2800)  data: 0.0036 (0.0001 -- 0.0559)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2653 (1.3188)  acc1: 55.5556 (58.5062)  acc5: 100.0000 (95.0207)  time: 0.1937 (0.1326 -- 0.2800)  data: 0.0032 (0.0001 -- 0.0559)  max mem: 16413
Val: Total time: 0:00:07 (0.2824 s / it)
* Acc@1 61.203 Acc@5 94.398 loss 1.293
Accuracy of the network on the 482 val images: 61.20%
Max accuracy: 61.41%
Epoch: [12]  [  0/160]  eta: 0:17:59  lr: 0.000047  min_lr: 0.000012  loss: 1.4067 (1.4067)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.6333 (8.6333)  time: 6.7444 (6.7444 -- 6.7444)  data: 5.3289 (5.3289 -- 5.3289)  max mem: 16413
Epoch: [12]  [ 20/160]  eta: 0:02:36  lr: 0.000047  min_lr: 0.000012  loss: 1.9792 (1.9520)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1728 (5.7393)  time: 0.8354 (0.5183 -- 2.2395)  data: 0.2857 (0.0005 -- 1.6771)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:01:58  lr: 0.000047  min_lr: 0.000012  loss: 1.9756 (2.0181)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5477 (5.8770)  time: 0.8580 (0.5190 -- 2.2620)  data: 0.1114 (0.0005 -- 1.2002)  max mem: 16413
Epoch: [12]  [ 60/160]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000012  loss: 2.0827 (2.0201)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9174 (5.8420)  time: 1.0650 (0.5295 -- 3.0017)  data: 0.2757 (0.0006 -- 2.4425)  max mem: 16413
[2023-09-04 11:54:41,423] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:54:41,423] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 11:54:41,424] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:54:41,424] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 11:54:41,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[1.1871674843346339e-05, 1.1871674843346339e-05, 1.3190749825940376e-05, 1.3190749825940376e-05, 1.4656388695489305e-05, 1.4656388695489305e-05, 1.628487632832145e-05, 1.628487632832145e-05, 1.8094307031468277e-05, 1.8094307031468277e-05, 2.010478559052031e-05, 2.010478559052031e-05, 2.2338650656133676e-05, 2.2338650656133676e-05, 2.4820722951259638e-05, 2.4820722951259638e-05, 2.7578581056955155e-05, 2.7578581056955155e-05, 3.064286784106128e-05, 3.064286784106128e-05, 3.4047630934512536e-05, 3.4047630934512536e-05, 3.783070103834726e-05, 3.783070103834726e-05, 4.203411226483028e-05, 4.203411226483028e-05, 4.670456918314476e-05, 4.670456918314476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 11:54:41,436] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=17.226199851101352, CurrSamplesPerSec=22.728571131821397, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000012  loss: 2.1003 (2.0279)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1823 (5.8306)  time: 0.7315 (0.5110 -- 1.7146)  data: 0.0544 (0.0002 -- 0.8802)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.3505 (2.0757)  loss_scale: 65536.0000 (39905.5842)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0377 (5.8458)  time: 0.9532 (0.5280 -- 3.0256)  data: 0.0365 (0.0004 -- 0.6361)  max mem: 16413
[2023-09-04 11:55:02,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2022
[2023-09-04 11:55:02,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 11:55:02,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2022
[2023-09-04 11:55:02,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 11:55:02,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [12]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.2504 (2.0935)  loss_scale: 32768.0000 (38996.6281)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6607 (5.9064)  time: 0.8231 (0.5177 -- 2.5164)  data: 0.0309 (0.0001 -- 0.5838)  max mem: 16413
Epoch: [12]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.8297 (2.0655)  loss_scale: 32768.0000 (38113.1348)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5964 (5.7814)  time: 0.8830 (0.5228 -- 3.1485)  data: 0.0444 (0.0003 -- 0.8477)  max mem: 16413
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1517 (2.0750)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7350 (5.7168)  time: 0.7061 (0.4938 -- 4.0435)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [12] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1517 (2.0808)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7350 (5.7168)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.2150 (1.2150)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4222 (2.4222 -- 2.4222)  data: 2.2050 (2.2050 -- 2.2050)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2150 (1.2702)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4152 (0.1970 -- 2.4222)  data: 0.2029 (0.0004 -- 2.2050)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1847 (1.2332)  acc1: 66.6667 (61.9048)  acc5: 100.0000 (94.7090)  time: 0.2117 (0.1703 -- 0.3294)  data: 0.0082 (0.0001 -- 0.1331)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2125 (1.2722)  acc1: 55.5556 (61.8257)  acc5: 100.0000 (94.6058)  time: 0.1967 (0.1335 -- 0.3294)  data: 0.0074 (0.0001 -- 0.1331)  max mem: 16413
Val: Total time: 0:00:07 (0.2833 s / it)
* Acc@1 64.315 Acc@5 94.191 loss 1.244
Accuracy of the network on the 482 val images: 64.32%
[2023-09-04 11:55:56,422] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:55:56,423] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:55:56,423] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:55:56,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:55:57,945] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:55:57,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.32%
Epoch: [13]  [  0/160]  eta: 0:19:59  lr: 0.000047  min_lr: 0.000012  loss: 1.7257 (1.7257)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8594 (4.8594)  time: 7.4990 (7.4990 -- 7.4990)  data: 6.2296 (6.2296 -- 6.2296)  max mem: 16413
Epoch: [13]  [ 20/160]  eta: 0:02:40  lr: 0.000047  min_lr: 0.000012  loss: 2.2058 (2.1440)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1867 (6.5463)  time: 0.8281 (0.5186 -- 3.4191)  data: 0.0414 (0.0007 -- 0.5478)  max mem: 16413
Epoch: [13]  [ 40/160]  eta: 0:02:02  lr: 0.000047  min_lr: 0.000012  loss: 2.1232 (2.1544)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7660 (6.3310)  time: 0.8882 (0.5277 -- 3.5084)  data: 0.0037 (0.0002 -- 0.0185)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000012  loss: 2.0963 (2.1371)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2246 (6.3213)  time: 0.9184 (0.5198 -- 3.5606)  data: 0.2821 (0.0010 -- 3.0502)  max mem: 16413
[2023-09-04 11:57:06,221] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:57:06,221] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 11:57:06,222] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 11:57:06,223] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 2.0948 (2.1373)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7475 (6.1597)  time: 0.9328 (0.5251 -- 3.8702)  data: 0.0971 (0.0002 -- 1.1036)  max mem: 16413
[2023-09-04 11:57:24,689] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2170
[2023-09-04 11:57:24,689] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 11:57:24,689] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2170
[2023-09-04 11:57:24,689] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 11:57:24,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.9360 (2.1155)  loss_scale: 32768.0000 (38932.2772)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5539 (6.1432)  time: 0.8357 (0.5291 -- 2.9894)  data: 0.0966 (0.0004 -- 1.8628)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.9984 (2.0982)  loss_scale: 32768.0000 (37913.3884)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4496 (6.3402)  time: 0.8010 (0.5278 -- 3.2544)  data: 0.2468 (0.0003 -- 2.7222)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.1202 (2.0755)  loss_scale: 32768.0000 (37183.5461)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5161 (6.3590)  time: 0.9741 (0.5291 -- 2.7619)  data: 0.1398 (0.0004 -- 1.2582)  max mem: 16413
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9440 (2.0578)  loss_scale: 32768.0000 (36659.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4108 (6.4345)  time: 0.7469 (0.4954 -- 2.7619)  data: 0.0006 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [13] Total time: 0:02:23 (0.8954 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9440 (2.0670)  loss_scale: 32768.0000 (36659.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4108 (6.4345)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.1731 (1.1731)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3316 (2.3316 -- 2.3316)  data: 2.1264 (2.1264 -- 2.1264)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1407 (1.2196)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (97.9798)  time: 0.4182 (0.2061 -- 2.3316)  data: 0.2046 (0.0005 -- 2.1264)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1407 (1.1977)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (95.7672)  time: 0.2200 (0.1714 -- 0.3907)  data: 0.0168 (0.0001 -- 0.2097)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1666 (1.2270)  acc1: 66.6667 (63.4855)  acc5: 88.8889 (94.6058)  time: 0.2055 (0.1327 -- 0.3907)  data: 0.0163 (0.0001 -- 0.2097)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 65.768 Acc@5 94.398 loss 1.196
Accuracy of the network on the 482 val images: 65.77%
[2023-09-04 11:58:28,937] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 11:58:28,939] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 11:58:28,939] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 11:58:28,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 11:58:30,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 11:58:30,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.77%
Epoch: [14]  [  0/160]  eta: 0:21:09  lr: 0.000047  min_lr: 0.000012  loss: 2.0652 (2.0652)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6302 (5.6302)  time: 7.9329 (7.9329 -- 7.9329)  data: 7.2481 (7.2481 -- 7.2481)  max mem: 16413
Epoch: [14]  [ 20/160]  eta: 0:02:39  lr: 0.000047  min_lr: 0.000012  loss: 1.9828 (2.0114)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0760 (7.4818)  time: 0.7977 (0.5163 -- 3.1117)  data: 0.1769 (0.0003 -- 2.5836)  max mem: 16413
[2023-09-04 11:59:13,439] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2280
[2023-09-04 11:59:13,439] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2280
[2023-09-04 11:59:13,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 11:59:13,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 11:59:13,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [ 40/160]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000012  loss: 2.0158 (2.0007)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4411 (6.7557)  time: 0.9603 (0.5322 -- 4.6268)  data: 0.0245 (0.0005 -- 0.4618)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0629 (2.0202)  loss_scale: 16384.0000 (27127.6066)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4942 (6.5858)  time: 0.8100 (0.5194 -- 3.8406)  data: 0.1931 (0.0004 -- 2.4015)  max mem: 16413
Epoch: [14]  [ 80/160]  eta: 0:01:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0409 (2.0327)  loss_scale: 16384.0000 (24474.8642)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3669 (6.5862)  time: 0.9863 (0.5183 -- 3.7683)  data: 0.0707 (0.0003 -- 1.1288)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 1.9987 (2.0343)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3619 (6.6765)  time: 0.7421 (0.5233 -- 2.5794)  data: 0.0026 (0.0004 -- 0.0139)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.9283 (2.0199)  loss_scale: 16384.0000 (21800.1983)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0271 (6.7214)  time: 0.9304 (0.5167 -- 2.2624)  data: 0.0583 (0.0003 -- 1.1353)  max mem: 16413
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.1117 (2.0206)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7740 (6.7050)  time: 0.8466 (0.5353 -- 4.1064)  data: 0.0023 (0.0003 -- 0.0096)  max mem: 16413
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9722 (2.0137)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0458 (6.7075)  time: 0.6100 (0.4939 -- 1.6270)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [14] Total time: 0:02:21 (0.8818 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9722 (2.0487)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0458 (6.7075)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.0951 (1.0951)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3705 (2.3705 -- 2.3705)  data: 2.1534 (2.1534 -- 2.1534)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.0951 (1.1968)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4061 (0.1962 -- 2.3705)  data: 0.1968 (0.0005 -- 2.1534)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0511 (1.1543)  acc1: 66.6667 (61.9048)  acc5: 100.0000 (96.2963)  time: 0.2095 (0.1717 -- 0.3007)  data: 0.0059 (0.0001 -- 0.1028)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1574 (1.2157)  acc1: 55.5556 (60.5809)  acc5: 100.0000 (95.0207)  time: 0.1953 (0.1330 -- 0.3007)  data: 0.0057 (0.0001 -- 0.1028)  max mem: 16413
Val: Total time: 0:00:07 (0.2797 s / it)
* Acc@1 63.278 Acc@5 94.398 loss 1.191
Accuracy of the network on the 482 val images: 63.28%
Max accuracy: 65.77%
Epoch: [15]  [  0/160]  eta: 0:17:43  lr: 0.000047  min_lr: 0.000012  loss: 1.9465 (1.9465)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.0219 (4.0219)  time: 6.6481 (6.6481 -- 6.6481)  data: 5.5817 (5.5817 -- 5.5817)  max mem: 16413
[2023-09-04 12:01:14,603] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:01:14,604] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:01:14,607] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:01:14,607] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [ 20/160]  eta: 0:02:55  lr: 0.000047  min_lr: 0.000012  loss: 2.1458 (2.0519)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3427 (6.1883)  time: 0.9803 (0.5209 -- 3.1181)  data: 0.2263 (0.0005 -- 2.2976)  max mem: 16413
Epoch: [15]  [ 40/160]  eta: 0:02:02  lr: 0.000047  min_lr: 0.000012  loss: 2.0887 (2.0440)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0007 (6.3739)  time: 0.7875 (0.5184 -- 3.2777)  data: 0.2529 (0.0001 -- 2.7230)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1335 (2.0599)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0009 (6.3154)  time: 0.8431 (0.5299 -- 3.5127)  data: 0.2990 (0.0004 -- 2.9850)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 1.9366 (2.0472)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9894 (6.3850)  time: 0.9079 (0.5213 -- 2.6135)  data: 0.3185 (0.0006 -- 2.1023)  max mem: 16413
Epoch: [15]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 2.0456 (2.0495)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5880 (6.3435)  time: 0.9486 (0.5196 -- 3.8950)  data: 0.4056 (0.0004 -- 3.3396)  max mem: 16413
Epoch: [15]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.9347 (2.0382)  loss_scale: 32768.0000 (31549.3554)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4834 (6.4524)  time: 0.8300 (0.5236 -- 3.4817)  data: 0.2083 (0.0004 -- 1.4046)  max mem: 16413
[2023-09-04 12:03:08,726] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:03:08,727] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:03:08,727] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:03:08,727] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.1982 (2.0556)  loss_scale: 32768.0000 (32651.8014)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7919 (6.3538)  time: 0.9273 (0.5257 -- 4.1514)  data: 0.3281 (0.0003 -- 3.6105)  max mem: 16413
[2023-09-04 12:03:22,104] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2559
[2023-09-04 12:03:22,104] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:03:22,104] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2559
[2023-09-04 12:03:22,104] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:03:22,104] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.1754 (2.0710)  loss_scale: 65536.0000 (36352.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7919 (6.2992)  time: 0.6136 (0.4896 -- 1.6054)  data: 0.0892 (0.0002 -- 1.0630)  max mem: 16413
Epoch: [15] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.1754 (2.0309)  loss_scale: 65536.0000 (36352.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7919 (6.2992)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.0677 (1.0677)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3424 (2.3424 -- 2.3424)  data: 2.1228 (2.1228 -- 2.1228)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0677 (1.1308)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4277 (0.2081 -- 2.3424)  data: 0.2105 (0.0008 -- 2.1228)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9408 (1.0950)  acc1: 66.6667 (62.4339)  acc5: 100.0000 (95.2381)  time: 0.2235 (0.1685 -- 0.3870)  data: 0.0171 (0.0001 -- 0.1801)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0733 (1.1499)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (93.7759)  time: 0.2059 (0.1321 -- 0.3870)  data: 0.0167 (0.0001 -- 0.1801)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 65.768 Acc@5 94.191 loss 1.124
Accuracy of the network on the 482 val images: 65.77%
Max accuracy: 65.77%
Epoch: [16]  [  0/160]  eta: 0:22:16  lr: 0.000047  min_lr: 0.000012  loss: 2.1652 (2.1652)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.7535 (7.7535)  time: 8.3555 (8.3555 -- 8.3555)  data: 7.8188 (7.8188 -- 7.8188)  max mem: 16413
Epoch: [16]  [ 20/160]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000012  loss: 1.9800 (2.0374)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1118 (6.6477)  time: 0.8519 (0.5219 -- 3.7066)  data: 0.3014 (0.0003 -- 3.1632)  max mem: 16413
Epoch: [16]  [ 40/160]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000012  loss: 2.0556 (2.0282)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9190 (6.2536)  time: 0.9509 (0.5197 -- 3.4737)  data: 0.4048 (0.0002 -- 2.9302)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000012  loss: 1.9720 (2.0179)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6575 (6.3762)  time: 0.7760 (0.5221 -- 2.3937)  data: 0.1298 (0.0002 -- 1.8478)  max mem: 16413
Epoch: [16]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.9325 (2.0174)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7123 (6.4942)  time: 0.8202 (0.5245 -- 2.1884)  data: 0.0021 (0.0006 -- 0.0069)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 2.0980 (2.0164)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4310 (6.5657)  time: 0.8804 (0.5197 -- 3.0543)  data: 0.0262 (0.0003 -- 0.4908)  max mem: 16413
[2023-09-04 12:05:06,101] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2664
[2023-09-04 12:05:06,101] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2664
[2023-09-04 12:05:06,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:05:06,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:05:06,102] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.1120 (2.0377)  loss_scale: 16384.0000 (30466.1157)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7280 (6.5996)  time: 0.9069 (0.5258 -- 3.6408)  data: 0.0681 (0.0005 -- 1.0108)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.0909 (2.0394)  loss_scale: 16384.0000 (28468.6525)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5535 (6.6454)  time: 1.0208 (0.5181 -- 4.9731)  data: 0.0017 (0.0005 -- 0.0046)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0858 (2.0466)  loss_scale: 16384.0000 (27033.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2384 (6.6004)  time: 0.6188 (0.4936 -- 2.7203)  data: 0.0004 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [16] Total time: 0:02:24 (0.9023 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0858 (2.0441)  loss_scale: 16384.0000 (27033.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2384 (6.6004)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.0523 (1.0523)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3693 (2.3693 -- 2.3693)  data: 2.1620 (2.1620 -- 2.1620)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9479 (1.1094)  acc1: 66.6667 (60.6061)  acc5: 100.0000 (94.9495)  time: 0.4388 (0.2023 -- 2.3693)  data: 0.2275 (0.0008 -- 2.1620)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9479 (1.0844)  acc1: 66.6667 (62.4339)  acc5: 100.0000 (95.7672)  time: 0.2179 (0.1697 -- 0.5646)  data: 0.0172 (0.0001 -- 0.3304)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0997 (1.1264)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (94.6058)  time: 0.2039 (0.1325 -- 0.5646)  data: 0.0169 (0.0001 -- 0.3304)  max mem: 16413
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 66.805 Acc@5 94.813 loss 1.096
Accuracy of the network on the 482 val images: 66.80%
[2023-09-04 12:06:01,988] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:06:01,990] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:06:01,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:06:01,990] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:06:03,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:06:03,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.80%
Epoch: [17]  [  0/160]  eta: 0:20:15  lr: 0.000046  min_lr: 0.000012  loss: 2.4386 (2.4386)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8735 (4.8735)  time: 7.5945 (7.5945 -- 7.5945)  data: 4.5172 (4.5172 -- 4.5172)  max mem: 16413
Epoch: [17]  [ 20/160]  eta: 0:02:53  lr: 0.000046  min_lr: 0.000012  loss: 1.9210 (1.9382)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3453 (6.5750)  time: 0.9217 (0.5368 -- 3.2314)  data: 0.2407 (0.0008 -- 2.7015)  max mem: 16413
Epoch: [17]  [ 40/160]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000012  loss: 1.9070 (1.9471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1924 (6.0895)  time: 0.8098 (0.5155 -- 3.8878)  data: 0.2660 (0.0004 -- 3.3373)  max mem: 16413
Epoch: [17]  [ 60/160]  eta: 0:01:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8851 (1.9637)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0125 (6.1514)  time: 0.8750 (0.5097 -- 4.0410)  data: 0.3341 (0.0003 -- 3.4996)  max mem: 16413
[2023-09-04 12:07:15,416] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:07:15,415] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:07:15,416] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:07:15,416] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 80/160]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000012  loss: 1.9687 (1.9604)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3335 (6.3421)  time: 0.8080 (0.5194 -- 3.3961)  data: 0.2590 (0.0005 -- 2.8706)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 2.0270 (1.9738)  loss_scale: 32768.0000 (20926.0990)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6912 (6.3774)  time: 0.9145 (0.5137 -- 4.8370)  data: 0.3602 (0.0003 -- 4.3226)  max mem: 16413
Epoch: [17]  [120/160]  eta: 0:00:38  lr: 0.000046  min_lr: 0.000012  loss: 1.7969 (1.9605)  loss_scale: 32768.0000 (22883.4380)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5317 (6.3055)  time: 1.0533 (0.5054 -- 5.4627)  data: 0.0766 (0.0004 -- 1.5132)  max mem: 16413
[2023-09-04 12:08:13,245] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2859
[2023-09-04 12:08:13,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:08:13,245] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2859
[2023-09-04 12:08:13,246] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:08:13,247] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.8196 (1.9383)  loss_scale: 32768.0000 (24053.1064)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3906 (6.2104)  time: 0.8167 (0.5262 -- 2.6558)  data: 0.0020 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7426 (1.9340)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2433 (6.2758)  time: 0.6504 (0.4951 -- 1.8732)  data: 0.0010 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [17] Total time: 0:02:22 (0.8929 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7426 (1.9692)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2433 (6.2758)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.9486 (0.9486)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3727 (2.3727 -- 2.3727)  data: 2.1243 (2.1243 -- 2.1243)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9486 (1.1030)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.4081 (0.2014 -- 2.3727)  data: 0.1953 (0.0006 -- 2.1243)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9390 (1.0401)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (96.2963)  time: 0.2164 (0.1687 -- 0.3542)  data: 0.0126 (0.0001 -- 0.1608)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0288 (1.0933)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (94.6058)  time: 0.2031 (0.1333 -- 0.3542)  data: 0.0123 (0.0001 -- 0.1608)  max mem: 16413
Val: Total time: 0:00:07 (0.2848 s / it)
* Acc@1 67.427 Acc@5 94.606 loss 1.067
Accuracy of the network on the 482 val images: 67.43%
[2023-09-04 12:08:33,955] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:08:33,956] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:08:33,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:08:33,956] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:08:35,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:08:35,341] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.43%
Epoch: [18]  [  0/160]  eta: 0:19:57  lr: 0.000046  min_lr: 0.000012  loss: 1.8607 (1.8607)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3313 (6.3313)  time: 7.4839 (7.4839 -- 7.4839)  data: 6.9346 (6.9346 -- 6.9346)  max mem: 16413
Epoch: [18]  [ 20/160]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000012  loss: 1.9095 (1.9817)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8476 (7.1558)  time: 0.8616 (0.5172 -- 4.0534)  data: 0.3079 (0.0004 -- 3.5069)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000012  loss: 1.9902 (1.9910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4130 (6.8220)  time: 0.9000 (0.5203 -- 3.6434)  data: 0.1893 (0.0003 -- 2.2721)  max mem: 16413
Epoch: [18]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 2.1753 (2.0419)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6163 (6.4505)  time: 0.9110 (0.5301 -- 3.7780)  data: 0.1826 (0.0004 -- 3.1785)  max mem: 16413
Epoch: [18]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.9272 (2.0119)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2477 (6.4631)  time: 0.7753 (0.5099 -- 3.8639)  data: 0.1782 (0.0003 -- 2.5745)  max mem: 16413
Epoch: [18]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 2.1102 (2.0199)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6189 (6.5054)  time: 0.9375 (0.5161 -- 4.4668)  data: 0.2408 (0.0003 -- 3.9056)  max mem: 16413
[2023-09-04 12:10:19,157] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:10:19,157] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:10:19,157] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:10:19,157] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:10:26,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=11, lr=[1.1769696168306417e-05, 1.1769696168306417e-05, 1.307744018700713e-05, 1.307744018700713e-05, 1.4530489096674585e-05, 1.4530489096674585e-05, 1.6144987885193987e-05, 1.6144987885193987e-05, 1.7938875427993317e-05, 1.7938875427993317e-05, 1.9932083808881464e-05, 1.9932083808881464e-05, 2.214675978764607e-05, 2.214675978764607e-05, 2.4607510875162297e-05, 2.4607510875162297e-05, 2.734167875018033e-05, 2.734167875018033e-05, 3.037964305575592e-05, 3.037964305575592e-05, 3.3755158950839915e-05, 3.3755158950839915e-05, 3.75057321675999e-05, 3.75057321675999e-05, 4.167303574177767e-05, 4.167303574177767e-05, 4.630337304641963e-05, 4.630337304641963e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 12:10:26,995] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=17.22885978205, CurrSamplesPerSec=23.224328221502912, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.1553 (2.0308)  loss_scale: 32768.0000 (18144.2645)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6948 (6.5814)  time: 0.8484 (0.5120 -- 4.7798)  data: 0.0496 (0.0002 -- 0.9634)  max mem: 16413
Epoch: [18]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9962 (2.0351)  loss_scale: 32768.0000 (20218.5532)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8107 (6.5786)  time: 0.8123 (0.5327 -- 2.6999)  data: 0.0749 (0.0004 -- 1.0721)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9422 (2.0293)  loss_scale: 32768.0000 (21708.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0078 (6.4896)  time: 0.7199 (0.4960 -- 2.7324)  data: 0.1922 (0.0002 -- 2.2298)  max mem: 16413
Epoch: [18] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9422 (2.0006)  loss_scale: 32768.0000 (21708.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0078 (6.4896)
Val:  [ 0/27]  eta: 0:01:08  loss: 1.0405 (1.0405)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5440 (2.5440 -- 2.5440)  data: 2.3337 (2.3337 -- 2.3337)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0350 (1.0831)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (94.9495)  time: 0.4331 (0.1861 -- 2.5440)  data: 0.2278 (0.0008 -- 2.3337)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8522 (1.0310)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.7672)  time: 0.2079 (0.1690 -- 0.3677)  data: 0.0088 (0.0001 -- 0.1618)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0882 (1.0964)  acc1: 66.6667 (63.4855)  acc5: 100.0000 (94.6058)  time: 0.1955 (0.1318 -- 0.3677)  data: 0.0085 (0.0001 -- 0.1618)  max mem: 16413
Val: Total time: 0:00:07 (0.2847 s / it)
* Acc@1 67.427 Acc@5 94.398 loss 1.063
Accuracy of the network on the 482 val images: 67.43%
Max accuracy: 67.43%
Epoch: [19]  [  0/160]  eta: 0:18:04  lr: 0.000046  min_lr: 0.000012  loss: 1.6916 (1.6916)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3006 (6.3006)  time: 6.7790 (6.7790 -- 6.7790)  data: 6.2085 (6.2085 -- 6.2085)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000012  loss: 1.9377 (1.9113)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0190 (6.0416)  time: 0.9190 (0.5240 -- 4.5569)  data: 0.1114 (0.0008 -- 1.7539)  max mem: 16413
Epoch: [19]  [ 40/160]  eta: 0:01:59  lr: 0.000046  min_lr: 0.000012  loss: 1.9343 (1.9377)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6821 (6.4956)  time: 0.7775 (0.5277 -- 4.1766)  data: 0.0019 (0.0003 -- 0.0063)  max mem: 16413
Epoch: [19]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000012  loss: 2.1501 (2.0007)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3876 (6.4785)  time: 0.8946 (0.5331 -- 3.0396)  data: 0.2929 (0.0007 -- 2.4771)  max mem: 16413
[2023-09-04 12:12:19,221] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:12:19,221] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:12:19,221] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:12:19,221] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:12:19,776] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3117
[2023-09-04 12:12:19,776] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3117
[2023-09-04 12:12:19,777] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:12:19,777] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:12:19,777] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [19]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.8825 (1.9688)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7359 (6.4865)  time: 0.8734 (0.5208 -- 3.6543)  data: 0.0770 (0.0004 -- 1.1156)  max mem: 16413
Epoch: [19]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.8800 (1.9652)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3070 (6.5592)  time: 0.8678 (0.5344 -- 2.6997)  data: 0.1861 (0.0001 -- 2.1562)  max mem: 16413
Epoch: [19]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.0343 (1.9753)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5529 (6.5970)  time: 0.9353 (0.5161 -- 3.0594)  data: 0.3955 (0.0007 -- 2.5209)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9918 (1.9784)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6603 (6.5787)  time: 0.9842 (0.5364 -- 3.6941)  data: 0.1992 (0.0009 -- 1.7612)  max mem: 16413
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0377 (1.9743)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2372 (6.4592)  time: 0.7051 (0.4965 -- 3.6941)  data: 0.0190 (0.0002 -- 0.3690)  max mem: 16413
Epoch: [19] Total time: 0:02:22 (0.8891 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0377 (1.9863)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2372 (6.4592)
[2023-09-04 12:13:27,634] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-09-04 12:13:27,636] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-09-04 12:13:27,636] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-09-04 12:13:27,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-09-04 12:13:28,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-09-04 12:13:28,666] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 1.0566 (1.0566)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3907 (2.3907 -- 2.3907)  data: 2.1676 (2.1676 -- 2.1676)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.0566 (1.0863)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4090 (0.1987 -- 2.3907)  data: 0.1980 (0.0006 -- 2.1676)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9577 (1.0272)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2129 (0.1694 -- 0.3548)  data: 0.0089 (0.0001 -- 0.1402)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0558 (1.0973)  acc1: 55.5556 (61.8257)  acc5: 100.0000 (94.1909)  time: 0.1980 (0.1331 -- 0.3548)  data: 0.0086 (0.0001 -- 0.1402)  max mem: 16413
Val: Total time: 0:00:07 (0.2829 s / it)
* Acc@1 65.353 Acc@5 93.568 loss 1.071
Accuracy of the network on the 482 val images: 65.35%
Max accuracy: 67.43%
Epoch: [20]  [  0/160]  eta: 0:17:28  lr: 0.000046  min_lr: 0.000012  loss: 2.2342 (2.2342)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2004 (5.2004)  time: 6.5511 (6.5511 -- 6.5511)  data: 4.8668 (4.8668 -- 4.8668)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:41  lr: 0.000046  min_lr: 0.000012  loss: 1.9191 (1.9639)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3083 (6.2090)  time: 0.8827 (0.5148 -- 3.8549)  data: 0.0356 (0.0011 -- 0.4889)  max mem: 16413
Epoch: [20]  [ 40/160]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000012  loss: 2.0309 (2.0255)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2180 (6.2694)  time: 0.8896 (0.5192 -- 4.4039)  data: 0.0345 (0.0003 -- 0.6606)  max mem: 16413
[2023-09-04 12:14:25,155] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:14:25,155] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:14:25,155] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:14:25,160] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [20]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000012  loss: 1.9814 (2.0160)  loss_scale: 65536.0000 (40825.7049)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1437 (6.0307)  time: 0.9085 (0.5354 -- 3.9247)  data: 0.2385 (0.0005 -- 3.3985)  max mem: 16413
[2023-09-04 12:14:38,281] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3263
[2023-09-04 12:14:38,281] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3263
[2023-09-04 12:14:38,281] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:14:38,281] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:14:38,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 2.0205 (1.9974)  loss_scale: 32768.0000 (39645.2346)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9622 (5.8398)  time: 0.8640 (0.5129 -- 2.9107)  data: 0.2041 (0.0004 -- 2.3708)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.9750 (1.9958)  loss_scale: 32768.0000 (38283.4059)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3416 (5.8507)  time: 0.9539 (0.5231 -- 2.9013)  data: 0.1576 (0.0004 -- 2.0946)  max mem: 16413
Epoch: [20]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.9846 (1.9923)  loss_scale: 32768.0000 (37371.7686)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9454 (5.8913)  time: 0.8514 (0.5185 -- 4.9807)  data: 0.0013 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9647 (1.9836)  loss_scale: 32768.0000 (36718.7518)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7420 (6.0890)  time: 0.8333 (0.5208 -- 3.3490)  data: 0.0789 (0.0003 -- 1.5169)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0465 (1.9916)  loss_scale: 32768.0000 (36249.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3331 (6.1435)  time: 0.6572 (0.4937 -- 2.0975)  data: 0.1352 (0.0002 -- 1.5638)  max mem: 16413
Epoch: [20] Total time: 0:02:22 (0.8927 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0465 (1.9827)  loss_scale: 32768.0000 (36249.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3331 (6.1435)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.9339 (0.9339)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1240 (2.1240 -- 2.1240)  data: 1.9120 (1.9120 -- 1.9120)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9529 (1.0728)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (96.9697)  time: 0.4108 (0.1976 -- 2.1240)  data: 0.2015 (0.0006 -- 1.9120)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9529 (1.0008)  acc1: 66.6667 (68.2540)  acc5: 100.0000 (97.3545)  time: 0.2272 (0.1695 -- 0.5076)  data: 0.0262 (0.0001 -- 0.2961)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9791 (1.0599)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (95.4357)  time: 0.2130 (0.1325 -- 0.5076)  data: 0.0259 (0.0001 -- 0.2961)  max mem: 16413
Val: Total time: 0:00:07 (0.2834 s / it)
* Acc@1 69.502 Acc@5 95.436 loss 1.032
Accuracy of the network on the 482 val images: 69.50%
[2023-09-04 12:16:07,118] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:16:07,120] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:16:07,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:16:07,121] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:16:08,431] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:16:08,431] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.50%
Epoch: [21]  [  0/160]  eta: 0:25:06  lr: 0.000046  min_lr: 0.000012  loss: 2.0859 (2.0859)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.4642 (8.4642)  time: 9.4170 (9.4170 -- 9.4170)  data: 8.9064 (8.9064 -- 8.9064)  max mem: 16413
Epoch: [21]  [ 20/160]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000012  loss: 2.0226 (2.0110)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9724 (6.2856)  time: 0.8110 (0.5150 -- 3.6884)  data: 0.2697 (0.0003 -- 3.1426)  max mem: 16413
[2023-09-04 12:16:46,358] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:16:46,358] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:16:46,359] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:16:46,359] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:16:49,140] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3397
[2023-09-04 12:16:49,140] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:16:49,140] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3397
[2023-09-04 12:16:49,141] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:16:49,141] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 40/160]  eta: 0:02:07  lr: 0.000046  min_lr: 0.000012  loss: 1.8559 (1.9690)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6067 (5.9510)  time: 0.8886 (0.5174 -- 3.9526)  data: 0.3484 (0.0004 -- 3.4362)  max mem: 16413
Epoch: [21]  [ 60/160]  eta: 0:01:35  lr: 0.000046  min_lr: 0.000012  loss: 2.1493 (1.9995)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3872 (6.2641)  time: 0.7507 (0.5199 -- 3.0496)  data: 0.1984 (0.0002 -- 2.5036)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 1.9306 (1.9717)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0875 (6.3013)  time: 1.0191 (0.5242 -- 4.8430)  data: 0.4225 (0.0003 -- 4.3130)  max mem: 16413
Epoch: [21]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 2.0302 (1.9773)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7058 (6.4283)  time: 0.7204 (0.5286 -- 2.2488)  data: 0.1703 (0.0005 -- 1.7001)  max mem: 16413
Epoch: [21]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.9412 (1.9718)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8219 (6.3626)  time: 0.9655 (0.5180 -- 4.2286)  data: 0.4165 (0.0002 -- 3.6881)  max mem: 16413
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.7976 (1.9581)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6237 (6.4550)  time: 0.7574 (0.5169 -- 2.2101)  data: 0.0967 (0.0004 -- 1.0381)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9543 (1.9615)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1623 (6.5304)  time: 0.7515 (0.4958 -- 4.1355)  data: 0.2077 (0.0002 -- 3.5663)  max mem: 16413
Epoch: [21] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9543 (1.9608)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1623 (6.5304)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7837 (0.7837)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3748 (2.3748 -- 2.3748)  data: 2.1770 (2.1770 -- 2.1770)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9258 (1.0594)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (94.9495)  time: 0.4427 (0.2101 -- 2.3748)  data: 0.2264 (0.0008 -- 2.1770)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9258 (0.9981)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (96.2963)  time: 0.2237 (0.1688 -- 0.5140)  data: 0.0189 (0.0001 -- 0.2895)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9908 (1.0442)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (95.4357)  time: 0.2075 (0.1326 -- 0.5140)  data: 0.0185 (0.0001 -- 0.2895)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 68.880 Acc@5 94.606 loss 1.016
Accuracy of the network on the 482 val images: 68.88%
Max accuracy: 69.50%
Epoch: [22]  [  0/160]  eta: 0:21:48  lr: 0.000046  min_lr: 0.000012  loss: 1.8497 (1.8497)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.5655 (4.5655)  time: 8.1800 (8.1800 -- 8.1800)  data: 7.6596 (7.6596 -- 7.6596)  max mem: 16413
[2023-09-04 12:18:49,606] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:18:49,606] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:18:49,606] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:18:49,607] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:18:50,149] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3527
[2023-09-04 12:18:50,149] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3527
[2023-09-04 12:18:50,149] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:18:50,149] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:18:50,149] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [ 20/160]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000012  loss: 1.9288 (1.9075)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9926 (5.6572)  time: 0.8691 (0.5255 -- 3.4717)  data: 0.2418 (0.0004 -- 2.9477)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000012  loss: 2.0277 (1.9822)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4821 (6.1655)  time: 0.8304 (0.5139 -- 3.7817)  data: 0.0479 (0.0003 -- 0.9304)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 2.1138 (2.0311)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7395 (6.5470)  time: 0.9411 (0.5201 -- 3.1739)  data: 0.1148 (0.0002 -- 1.6081)  max mem: 16413
Epoch: [22]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.9234 (2.0085)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6766 (6.5724)  time: 0.7611 (0.5176 -- 2.3312)  data: 0.0278 (0.0004 -- 0.3966)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 2.0066 (2.0011)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4505 (6.5589)  time: 0.8704 (0.5267 -- 2.3828)  data: 0.1396 (0.0003 -- 0.9101)  max mem: 16413
Epoch: [22]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.0578 (1.9996)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7347 (6.5540)  time: 1.0096 (0.5258 -- 2.8704)  data: 0.1317 (0.0004 -- 2.1982)  max mem: 16413
[2023-09-04 12:20:45,406] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:20:45,406] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:20:45,407] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:20:45,407] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.0166 (2.0018)  loss_scale: 32768.0000 (34162.3830)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8190 (6.4860)  time: 0.7867 (0.5325 -- 2.5924)  data: 0.0532 (0.0004 -- 0.8814)  max mem: 16413
[2023-09-04 12:20:58,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3674
[2023-09-04 12:20:58,083] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:20:58,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3674
[2023-09-04 12:20:58,083] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:20:58,083] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0510 (1.9993)  loss_scale: 65536.0000 (36659.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0859 (6.3869)  time: 0.6876 (0.4883 -- 2.0879)  data: 0.0706 (0.0001 -- 1.1189)  max mem: 16413
Epoch: [22] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0510 (1.9734)  loss_scale: 65536.0000 (36659.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0859 (6.3869)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.8527 (0.8527)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1783 (2.1783 -- 2.1783)  data: 1.9613 (1.9613 -- 1.9613)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.8641 (1.0638)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4014 (0.1968 -- 2.1783)  data: 0.1955 (0.0006 -- 1.9613)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8898 (0.9965)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (96.2963)  time: 0.2254 (0.1697 -- 0.3956)  data: 0.0260 (0.0001 -- 0.1808)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9825 (1.0440)  acc1: 66.6667 (65.5602)  acc5: 100.0000 (95.4357)  time: 0.2132 (0.1329 -- 0.3956)  data: 0.0257 (0.0001 -- 0.1808)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 69.502 Acc@5 95.228 loss 1.007
Accuracy of the network on the 482 val images: 69.50%
Max accuracy: 69.50%
Epoch: [23]  [  0/160]  eta: 0:23:08  lr: 0.000046  min_lr: 0.000012  loss: 2.1595 (2.1595)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5151 (6.5151)  time: 8.6774 (8.6774 -- 8.6774)  data: 8.1400 (8.1400 -- 8.1400)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000012  loss: 1.9248 (1.9360)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0440 (6.7096)  time: 0.7985 (0.5181 -- 5.3420)  data: 0.2625 (0.0003 -- 4.8312)  max mem: 16413
Epoch: [23]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000012  loss: 1.9534 (1.9396)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9244 (6.8262)  time: 0.9029 (0.5220 -- 2.9006)  data: 0.3566 (0.0003 -- 2.3869)  max mem: 16413
Epoch: [23]  [ 60/160]  eta: 0:01:42  lr: 0.000046  min_lr: 0.000012  loss: 2.1213 (2.0077)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9368 (6.6942)  time: 0.9992 (0.5231 -- 4.8042)  data: 0.4576 (0.0003 -- 4.2918)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 2.1127 (2.0168)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4966 (6.5603)  time: 0.7883 (0.5163 -- 3.7075)  data: 0.2451 (0.0002 -- 3.1706)  max mem: 16413
Epoch: [23]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.9780 (2.0059)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5366 (6.4691)  time: 0.9106 (0.5214 -- 3.0415)  data: 0.3685 (0.0004 -- 2.5137)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.9430 (2.0072)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0178 (6.4600)  time: 0.7483 (0.5260 -- 1.9528)  data: 0.1232 (0.0004 -- 1.3140)  max mem: 16413
[2023-09-04 12:23:03,096] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:23:03,096] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:23:03,096] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:23:03,096] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:23:03,668] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3804
[2023-09-04 12:23:03,668] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3804
[2023-09-04 12:23:03,668] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:23:03,668] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:23:03,668] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [23]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.8620 (1.9976)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9908 (6.4026)  time: 0.8577 (0.5323 -- 3.3305)  data: 0.0893 (0.0005 -- 1.0505)  max mem: 16413
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9204 (1.9949)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4794 (6.3985)  time: 0.7184 (0.4955 -- 2.5029)  data: 0.0093 (0.0002 -- 0.1503)  max mem: 16413
Epoch: [23] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9204 (1.9910)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4794 (6.3985)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.9563 (0.9563)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2107 (2.2107 -- 2.2107)  data: 1.9910 (1.9910 -- 1.9910)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9563 (1.0382)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4131 (0.1937 -- 2.2107)  data: 0.2030 (0.0006 -- 1.9910)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8768 (0.9588)  acc1: 66.6667 (69.3122)  acc5: 100.0000 (95.7672)  time: 0.2281 (0.1698 -- 0.4241)  data: 0.0240 (0.0001 -- 0.2165)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9329 (1.0194)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (94.6058)  time: 0.2136 (0.1334 -- 0.4241)  data: 0.0230 (0.0001 -- 0.2165)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 69.295 Acc@5 94.813 loss 0.991
Accuracy of the network on the 482 val images: 69.29%
Max accuracy: 69.50%
Epoch: [24]  [  0/160]  eta: 0:19:45  lr: 0.000046  min_lr: 0.000012  loss: 1.7638 (1.7638)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4429 (4.4429)  time: 7.4091 (7.4091 -- 7.4091)  data: 5.9444 (5.9444 -- 5.9444)  max mem: 16413
[2023-09-04 12:23:48,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3843
[2023-09-04 12:23:48,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3843
[2023-09-04 12:23:48,102] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:23:48,102] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:23:48,102] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [ 20/160]  eta: 0:02:43  lr: 0.000046  min_lr: 0.000012  loss: 1.8843 (1.8932)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1165 (6.4812)  time: 0.8582 (0.5296 -- 3.7120)  data: 0.0724 (0.0004 -- 0.7666)  max mem: 16413
Epoch: [24]  [ 40/160]  eta: 0:02:01  lr: 0.000046  min_lr: 0.000012  loss: 1.7739 (1.8493)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5438 (6.3274)  time: 0.8428 (0.5334 -- 2.4059)  data: 0.1989 (0.0003 -- 1.4609)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:42  lr: 0.000046  min_lr: 0.000012  loss: 1.7650 (1.8649)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9950 (6.2622)  time: 1.0420 (0.5238 -- 4.1683)  data: 0.3738 (0.0004 -- 3.6416)  max mem: 16413
Epoch: [24]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9703 (1.8948)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3009 (6.4760)  time: 0.8693 (0.5206 -- 4.1725)  data: 0.3352 (0.0003 -- 3.6398)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.6226 (1.8520)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8127 (6.4589)  time: 0.8912 (0.5290 -- 4.2694)  data: 0.3437 (0.0002 -- 3.7381)  max mem: 16413
Epoch: [24]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8474 (1.8570)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8473 (6.5760)  time: 0.7756 (0.5213 -- 3.3724)  data: 0.2346 (0.0002 -- 2.8396)  max mem: 16413
[2023-09-04 12:25:43,404] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:25:43,404] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:25:43,406] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:25:43,407] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 2.0068 (1.8797)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2921 (6.5333)  time: 0.8956 (0.5267 -- 3.1372)  data: 0.3471 (0.0007 -- 2.6149)  max mem: 16413
[2023-09-04 12:26:01,812] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[1.1608898360300739e-05, 1.1608898360300739e-05, 1.2898775955889711e-05, 1.2898775955889711e-05, 1.4331973284321898e-05, 1.4331973284321898e-05, 1.5924414760357666e-05, 1.5924414760357666e-05, 1.7693794178175183e-05, 1.7693794178175183e-05, 1.9659771309083537e-05, 1.9659771309083537e-05, 2.1844190343426154e-05, 2.1844190343426154e-05, 2.4271322603806833e-05, 2.4271322603806833e-05, 2.6968136226452038e-05, 2.6968136226452038e-05, 2.9964595807168928e-05, 2.9964595807168928e-05, 3.329399534129881e-05, 3.329399534129881e-05, 3.6993328156998675e-05, 3.6993328156998675e-05, 4.1103697952220754e-05, 4.1103697952220754e-05, 4.56707755024675e-05, 4.56707755024675e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 12:26:01,816] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=17.548783809369983, CurrSamplesPerSec=24.71260714133155, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9945 (1.8894)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3567 (6.5522)  time: 0.7269 (0.4948 -- 2.6984)  data: 0.2096 (0.0002 -- 2.1726)  max mem: 16413
Epoch: [24] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9945 (1.9054)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3567 (6.5522)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7139 (0.7139)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3769 (2.3769 -- 2.3769)  data: 2.1459 (2.1459 -- 2.1459)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8098 (0.9405)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (96.9697)  time: 0.4119 (0.1959 -- 2.3769)  data: 0.1979 (0.0005 -- 2.1459)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8563 (0.9293)  acc1: 66.6667 (69.3122)  acc5: 100.0000 (95.7672)  time: 0.2131 (0.1691 -- 0.3455)  data: 0.0092 (0.0001 -- 0.1500)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9747 (0.9862)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (95.0207)  time: 0.1981 (0.1323 -- 0.3455)  data: 0.0090 (0.0001 -- 0.1500)  max mem: 16413
Val: Total time: 0:00:07 (0.2823 s / it)
* Acc@1 71.577 Acc@5 95.021 loss 0.941
Accuracy of the network on the 482 val images: 71.58%
[2023-09-04 12:26:09,556] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:26:09,558] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:26:09,558] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:26:09,558] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:26:10,957] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:26:10,957] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.58%
Epoch: [25]  [  0/160]  eta: 0:19:50  lr: 0.000046  min_lr: 0.000012  loss: 2.1894 (2.1894)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3643 (4.3643)  time: 7.4390 (7.4390 -- 7.4390)  data: 6.8789 (6.8789 -- 6.8789)  max mem: 16413
Epoch: [25]  [ 20/160]  eta: 0:03:05  lr: 0.000046  min_lr: 0.000012  loss: 1.9996 (2.0401)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8777 (7.0309)  time: 1.0189 (0.5067 -- 3.8313)  data: 0.2211 (0.0004 -- 2.1492)  max mem: 16413
Epoch: [25]  [ 40/160]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000012  loss: 1.9499 (1.9793)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8144 (6.5224)  time: 0.8034 (0.5176 -- 3.2592)  data: 0.0016 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000012  loss: 2.1179 (2.0184)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2148 (6.4053)  time: 0.8836 (0.5176 -- 4.0064)  data: 0.0018 (0.0001 -- 0.0091)  max mem: 16413
Epoch: [25]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 1.8388 (1.9934)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5738 (6.3129)  time: 0.7754 (0.5312 -- 3.0680)  data: 0.0021 (0.0005 -- 0.0074)  max mem: 16413
[2023-09-04 12:27:45,557] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:27:45,557] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:27:45,557] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:27:45,557] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [25]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.8737 (1.9756)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1979 (6.1850)  time: 0.8761 (0.5299 -- 3.5539)  data: 0.0018 (0.0004 -- 0.0029)  max mem: 16413
[2023-09-04 12:27:57,970] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4116
[2023-09-04 12:27:57,970] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:27:57,971] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 12:27:57,971] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4116
[2023-09-04 12:27:57,971] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.9271 (1.9564)  loss_scale: 65536.0000 (37100.9587)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5501 (6.0865)  time: 0.7345 (0.5280 -- 2.9943)  data: 0.0195 (0.0002 -- 0.3513)  max mem: 16413
Epoch: [25]  [140/160]  eta: 0:00:17  lr: 0.000046  min_lr: 0.000012  loss: 2.0134 (1.9553)  loss_scale: 32768.0000 (36486.3546)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5519 (6.2265)  time: 0.8620 (0.5207 -- 2.1445)  data: 0.0841 (0.0007 -- 0.7272)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0134 (1.9507)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1111 (6.4045)  time: 0.7726 (0.4960 -- 2.1860)  data: 0.1293 (0.0002 -- 1.4545)  max mem: 16413
Epoch: [25] Total time: 0:02:21 (0.8825 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0134 (1.9564)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1111 (6.4045)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.8394 (0.8394)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2493 (2.2493 -- 2.2493)  data: 2.0233 (2.0233 -- 2.0233)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8691 (1.0259)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.9596)  time: 0.4181 (0.1991 -- 2.2493)  data: 0.2067 (0.0008 -- 2.0233)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8477 (0.9418)  acc1: 66.6667 (67.1958)  acc5: 100.0000 (95.7672)  time: 0.2208 (0.1698 -- 0.4653)  data: 0.0187 (0.0001 -- 0.2416)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0057 (1.0234)  acc1: 66.6667 (65.5602)  acc5: 100.0000 (94.6058)  time: 0.2080 (0.1328 -- 0.4653)  data: 0.0184 (0.0001 -- 0.2416)  max mem: 16413
Val: Total time: 0:00:07 (0.2835 s / it)
* Acc@1 69.917 Acc@5 94.606 loss 0.976
Accuracy of the network on the 482 val images: 69.92%
Max accuracy: 71.58%
Epoch: [26]  [  0/160]  eta: 0:22:27  lr: 0.000046  min_lr: 0.000012  loss: 1.0042 (1.0042)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0720 (5.0720)  time: 8.4247 (8.4247 -- 8.4247)  data: 5.6511 (5.6511 -- 5.6511)  max mem: 16413
Epoch: [26]  [ 20/160]  eta: 0:02:38  lr: 0.000046  min_lr: 0.000012  loss: 2.0966 (2.0392)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1789 (6.0793)  time: 0.7704 (0.5239 -- 2.2121)  data: 0.0015 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:10  lr: 0.000046  min_lr: 0.000012  loss: 1.9204 (2.0015)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3432 (5.9181)  time: 1.0375 (0.5143 -- 3.4619)  data: 0.0013 (0.0005 -- 0.0029)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000012  loss: 2.0426 (1.9987)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3066 (5.9120)  time: 0.8413 (0.5239 -- 3.7184)  data: 0.0018 (0.0002 -- 0.0107)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000012  loss: 1.8924 (1.9791)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8635 (5.9542)  time: 0.8828 (0.5223 -- 3.2411)  data: 0.0017 (0.0002 -- 0.0057)  max mem: 16413
[2023-09-04 12:30:01,727] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:30:01,728] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:30:01,728] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:30:01,729] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:30:08,886] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4252
[2023-09-04 12:30:08,886] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4252
[2023-09-04 12:30:08,886] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:30:08,886] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:30:08,886] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000012  loss: 2.0495 (1.9797)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2130 (6.0494)  time: 0.8230 (0.5444 -- 3.7997)  data: 0.0026 (0.0006 -- 0.0168)  max mem: 16413
Epoch: [26]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000012  loss: 2.0643 (1.9779)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4985 (6.0580)  time: 0.9712 (0.5222 -- 3.7630)  data: 0.0019 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.9795 (1.9712)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2394 (6.0804)  time: 0.6345 (0.5256 -- 1.6029)  data: 0.0018 (0.0005 -- 0.0075)  max mem: 16413
[2023-09-04 12:30:56,537] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4311
[2023-09-04 12:30:56,537] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4311
[2023-09-04 12:30:56,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:30:56,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:30:56,537] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 2.0155 (1.9735)  loss_scale: 32768.0000 (33280.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9083 (6.0870)  time: 0.7641 (0.4955 -- 3.1991)  data: 0.0379 (0.0002 -- 0.7304)  max mem: 16413
Epoch: [26] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 2.0155 (1.9558)  loss_scale: 32768.0000 (33280.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9083 (6.0870)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7888 (0.7888)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3711 (2.3711 -- 2.3711)  data: 2.1631 (2.1631 -- 2.1631)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8341 (0.9788)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (97.9798)  time: 0.4194 (0.2040 -- 2.3711)  data: 0.2034 (0.0006 -- 2.1631)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8331 (0.9072)  acc1: 77.7778 (71.4286)  acc5: 100.0000 (97.3545)  time: 0.2138 (0.1692 -- 0.2857)  data: 0.0040 (0.0001 -- 0.0579)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8377 (0.9648)  acc1: 66.6667 (69.7095)  acc5: 100.0000 (95.8506)  time: 0.1985 (0.1333 -- 0.2857)  data: 0.0037 (0.0001 -- 0.0579)  max mem: 16413
Val: Total time: 0:00:07 (0.2827 s / it)
* Acc@1 73.651 Acc@5 95.436 loss 0.917
Accuracy of the network on the 482 val images: 73.65%
[2023-09-04 12:31:09,877] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:31:09,879] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:31:09,879] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:31:09,879] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:31:11,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:31:11,296] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.65%
Epoch: [27]  [  0/160]  eta: 0:20:56  lr: 0.000045  min_lr: 0.000012  loss: 1.9755 (1.9755)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9774 (5.9774)  time: 7.8542 (7.8542 -- 7.8542)  data: 5.1495 (5.1495 -- 5.1495)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:46  lr: 0.000045  min_lr: 0.000012  loss: 1.8116 (1.8548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2769 (6.4585)  time: 0.8528 (0.5235 -- 4.0992)  data: 0.1301 (0.0007 -- 1.4736)  max mem: 16413
Epoch: [27]  [ 40/160]  eta: 0:02:04  lr: 0.000045  min_lr: 0.000012  loss: 1.7919 (1.8462)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4971 (5.9661)  time: 0.8785 (0.5221 -- 3.9957)  data: 0.3282 (0.0003 -- 3.4376)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:38  lr: 0.000045  min_lr: 0.000012  loss: 1.9421 (1.8654)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6356 (6.3363)  time: 0.8695 (0.5224 -- 3.5385)  data: 0.3240 (0.0004 -- 2.9938)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000012  loss: 2.0043 (1.8901)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7549 (6.6030)  time: 0.8905 (0.5190 -- 2.6220)  data: 0.3298 (0.0001 -- 2.0910)  max mem: 16413
Epoch: [27]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000012  loss: 1.9968 (1.9187)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9666 (6.5458)  time: 0.8709 (0.5219 -- 3.6424)  data: 0.3033 (0.0003 -- 3.1214)  max mem: 16413
[2023-09-04 12:33:03,276] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:33:03,276] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:33:03,277] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:33:03,277] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [27]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000012  loss: 1.9198 (1.9193)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3472 (6.6065)  time: 0.8435 (0.5122 -- 3.3671)  data: 0.3006 (0.0001 -- 2.8097)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.9094 (1.9169)  loss_scale: 32768.0000 (18824.1702)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4115 (6.4749)  time: 0.8844 (0.5212 -- 2.7059)  data: 0.2146 (0.0003 -- 2.1712)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.9939 (1.9255)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2874 (6.5769)  time: 0.6844 (0.4972 -- 2.8218)  data: 0.1157 (0.0001 -- 2.2999)  max mem: 16413
Epoch: [27] Total time: 0:02:22 (0.8928 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.9939 (1.9338)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2874 (6.5769)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.6714 (0.6714)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4191 (2.4191 -- 2.4191)  data: 2.1739 (2.1739 -- 2.1739)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8167 (1.0226)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4325 (0.1938 -- 2.4191)  data: 0.2223 (0.0004 -- 2.1739)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7921 (0.9304)  acc1: 77.7778 (70.8995)  acc5: 100.0000 (96.2963)  time: 0.2202 (0.1689 -- 0.4642)  data: 0.0215 (0.0001 -- 0.2498)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8939 (1.0099)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (95.4357)  time: 0.2065 (0.1331 -- 0.4642)  data: 0.0208 (0.0001 -- 0.2498)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 71.369 Acc@5 95.021 loss 0.971
Accuracy of the network on the 482 val images: 71.37%
Max accuracy: 73.65%
Epoch: [28]  [  0/160]  eta: 0:17:17  lr: 0.000045  min_lr: 0.000012  loss: 1.9611 (1.9611)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 9.3199 (9.3199)  time: 6.4858 (6.4858 -- 6.4858)  data: 5.9650 (5.9650 -- 5.9650)  max mem: 16413
Epoch: [28]  [ 20/160]  eta: 0:02:46  lr: 0.000045  min_lr: 0.000012  loss: 1.8497 (1.7933)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1370 (6.1724)  time: 0.9256 (0.5328 -- 3.3333)  data: 0.3115 (0.0003 -- 2.8144)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:02:04  lr: 0.000045  min_lr: 0.000012  loss: 2.0391 (1.8763)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8699 (6.8223)  time: 0.8750 (0.5266 -- 2.4946)  data: 0.1327 (0.0004 -- 1.3179)  max mem: 16413
Epoch: [28]  [ 60/160]  eta: 0:01:34  lr: 0.000045  min_lr: 0.000011  loss: 1.7781 (1.8713)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5583 (6.5842)  time: 0.7708 (0.5260 -- 2.8959)  data: 0.1080 (0.0004 -- 1.5890)  max mem: 16413
Epoch: [28]  [ 80/160]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000011  loss: 1.8952 (1.8795)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0936 (6.5597)  time: 0.9070 (0.5159 -- 3.7787)  data: 0.3586 (0.0004 -- 3.2211)  max mem: 16413
[2023-09-04 12:35:04,913] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:35:04,913] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:35:04,913] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:35:04,913] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:35:12,548] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4577
[2023-09-04 12:35:12,548] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:35:12,548] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4577
[2023-09-04 12:35:12,548] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 12:35:12,548] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [28]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 2.0522 (1.9106)  loss_scale: 32768.0000 (35687.9208)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5739 (6.6038)  time: 0.9362 (0.5232 -- 3.1128)  data: 0.3112 (0.0003 -- 2.5732)  max mem: 16413
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.9114 (1.9070)  loss_scale: 32768.0000 (35205.2893)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6881 (6.6718)  time: 0.7783 (0.5420 -- 2.9960)  data: 0.2179 (0.0003 -- 2.4635)  max mem: 16413
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8094 (1.9021)  loss_scale: 32768.0000 (34859.5745)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8684 (6.7704)  time: 0.9145 (0.5286 -- 3.4970)  data: 0.3415 (0.0008 -- 2.9687)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8094 (1.9004)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9380 (6.8254)  time: 0.6691 (0.4954 -- 1.6638)  data: 0.0590 (0.0002 -- 1.1531)  max mem: 16413
Epoch: [28] Total time: 0:02:21 (0.8843 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8094 (1.9208)  loss_scale: 32768.0000 (34611.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9380 (6.8254)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5825 (0.5825)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3964 (2.3964 -- 2.3964)  data: 2.1537 (2.1537 -- 2.1537)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6677 (0.9820)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4353 (0.2084 -- 2.3964)  data: 0.2129 (0.0007 -- 2.1537)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7324 (0.9011)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.2963)  time: 0.2436 (0.1679 -- 0.7796)  data: 0.0389 (0.0001 -- 0.5880)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9394 (0.9621)  acc1: 66.6667 (70.9544)  acc5: 100.0000 (95.0207)  time: 0.2242 (0.1323 -- 0.7796)  data: 0.0381 (0.0001 -- 0.5880)  max mem: 16413
Val: Total time: 0:00:08 (0.3056 s / it)
* Acc@1 73.859 Acc@5 95.851 loss 0.918
Accuracy of the network on the 482 val images: 73.86%
[2023-09-04 12:36:11,707] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:36:11,708] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:36:11,708] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:36:11,708] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:36:12,820] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:36:12,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.86%
Epoch: [29]  [  0/160]  eta: 0:20:59  lr: 0.000045  min_lr: 0.000011  loss: 2.5186 (2.5186)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 3.9650 (3.9650)  time: 7.8738 (7.8738 -- 7.8738)  data: 7.3085 (7.3085 -- 7.3085)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:38  lr: 0.000045  min_lr: 0.000011  loss: 2.0853 (2.0828)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6215 (6.5900)  time: 0.7938 (0.5212 -- 2.7377)  data: 0.2444 (0.0009 -- 2.1921)  max mem: 16413
Epoch: [29]  [ 40/160]  eta: 0:02:08  lr: 0.000045  min_lr: 0.000011  loss: 1.8016 (1.9685)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1690 (6.4603)  time: 1.0036 (0.5305 -- 3.3827)  data: 0.4513 (0.0009 -- 2.8190)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000011  loss: 1.7900 (1.9788)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7056 (6.5371)  time: 0.7929 (0.5248 -- 4.0900)  data: 0.2423 (0.0001 -- 3.5529)  max mem: 16413
[2023-09-04 12:37:17,387] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:37:17,388] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:37:17,389] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:37:17,389] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:37:27,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4719
[2023-09-04 12:37:27,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:37:27,369] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4719
[2023-09-04 12:37:27,369] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:37:27,370] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [29]  [ 80/160]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000011  loss: 1.9852 (1.9647)  loss_scale: 65536.0000 (38027.0617)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1791 (6.5782)  time: 0.8428 (0.5382 -- 3.3184)  data: 0.2901 (0.0003 -- 2.7592)  max mem: 16413
Epoch: [29]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000011  loss: 1.8181 (1.9409)  loss_scale: 32768.0000 (36985.6634)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1311 (6.5047)  time: 0.8838 (0.5261 -- 4.1942)  data: 0.3290 (0.0004 -- 3.6398)  max mem: 16413
Epoch: [29]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.8714 (1.9290)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2988 (6.5162)  time: 0.8327 (0.5184 -- 3.0419)  data: 0.2907 (0.0004 -- 2.4752)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.9046 (1.9152)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0063 (6.5053)  time: 0.8283 (0.5238 -- 2.5402)  data: 0.1776 (0.0005 -- 2.0248)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.9159 (1.9225)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1542 (6.4747)  time: 0.6763 (0.4995 -- 1.1371)  data: 0.0522 (0.0002 -- 0.4855)  max mem: 16413
Epoch: [29] Total time: 0:02:20 (0.8778 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.9159 (1.9012)  loss_scale: 32768.0000 (35430.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1542 (6.4747)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.7409 (0.7409)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3722 (2.3722 -- 2.3722)  data: 2.1371 (2.1371 -- 2.1371)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8168 (0.9327)  acc1: 66.6667 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4202 (0.2021 -- 2.3722)  data: 0.2016 (0.0007 -- 2.1371)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8168 (0.8712)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.8254)  time: 0.2174 (0.1694 -- 0.3636)  data: 0.0128 (0.0001 -- 0.1719)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9501 (0.9414)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (95.0207)  time: 0.2018 (0.1352 -- 0.3636)  data: 0.0121 (0.0001 -- 0.1719)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 75.311 Acc@5 96.058 loss 0.900
Accuracy of the network on the 482 val images: 75.31%
[2023-09-04 12:38:40,977] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:38:40,979] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:38:40,979] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:38:40,979] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:38:42,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:38:42,530] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.31%
Epoch: [30]  [  0/160]  eta: 0:15:56  lr: 0.000045  min_lr: 0.000011  loss: 1.7374 (1.7374)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5898 (6.5898)  time: 5.9766 (5.9766 -- 5.9766)  data: 5.4259 (5.4259 -- 5.4259)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:03:08  lr: 0.000045  min_lr: 0.000011  loss: 1.7929 (1.8552)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3742 (7.3073)  time: 1.1168 (0.5051 -- 4.6946)  data: 0.4758 (0.0001 -- 4.1997)  max mem: 16413
Epoch: [30]  [ 40/160]  eta: 0:02:06  lr: 0.000045  min_lr: 0.000011  loss: 1.8718 (1.8641)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7433 (6.6820)  time: 0.7382 (0.5110 -- 3.9687)  data: 0.2002 (0.0002 -- 3.4487)  max mem: 16413
[2023-09-04 12:39:35,046] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:39:35,046] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:39:35,047] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:39:35,047] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:39:37,569] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4850
[2023-09-04 12:39:37,569] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4850
[2023-09-04 12:39:37,570] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:39:37,570] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:39:37,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [30]  [ 60/160]  eta: 0:01:46  lr: 0.000045  min_lr: 0.000011  loss: 1.8546 (1.8416)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5173 (6.6570)  time: 1.0911 (0.5023 -- 5.7207)  data: 0.1232 (0.0002 -- 1.3100)  max mem: 16413
Epoch: [30]  [ 80/160]  eta: 0:01:19  lr: 0.000045  min_lr: 0.000011  loss: 1.6749 (1.8091)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0982 (6.5632)  time: 0.7581 (0.5131 -- 3.5687)  data: 0.0020 (0.0003 -- 0.0139)  max mem: 16413
[2023-09-04 12:40:04,486] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4882
[2023-09-04 12:40:04,486] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4882
[2023-09-04 12:40:04,486] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:40:04,486] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:40:04,486] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [100/160]  eta: 0:00:58  lr: 0.000045  min_lr: 0.000011  loss: 1.8584 (1.8337)  loss_scale: 16384.0000 (30334.7327)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1507 (6.6947)  time: 0.8851 (0.5350 -- 4.5826)  data: 0.0019 (0.0008 -- 0.0050)  max mem: 16413
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.8426 (1.8422)  loss_scale: 16384.0000 (28028.8264)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.6064 (6.8701)  time: 0.8111 (0.5235 -- 2.8685)  data: 0.0314 (0.0002 -- 0.5954)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 2.0230 (1.8629)  loss_scale: 16384.0000 (26377.0780)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6326 (6.9106)  time: 0.8924 (0.5290 -- 3.4610)  data: 0.2054 (0.0003 -- 2.9297)  max mem: 16413
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8826 (1.8745)  loss_scale: 16384.0000 (25190.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6263 (6.9200)  time: 0.7268 (0.4944 -- 1.7433)  data: 0.0595 (0.0002 -- 1.0756)  max mem: 16413
Epoch: [30] Total time: 0:02:24 (0.9043 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8826 (1.8789)  loss_scale: 16384.0000 (25190.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6263 (6.9200)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5319 (0.5319)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4078 (2.4078 -- 2.4078)  data: 2.1998 (2.1998 -- 2.1998)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7909 (0.9220)  acc1: 88.8889 (73.7374)  acc5: 100.0000 (97.9798)  time: 0.4422 (0.2025 -- 2.4078)  data: 0.2312 (0.0006 -- 2.1998)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7242 (0.8547)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2194 (0.1687 -- 0.5512)  data: 0.0173 (0.0001 -- 0.3345)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7990 (0.9306)  acc1: 77.7778 (73.4440)  acc5: 100.0000 (95.4357)  time: 0.2042 (0.1329 -- 0.5512)  data: 0.0171 (0.0001 -- 0.3345)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 76.349 Acc@5 95.643 loss 0.867
Accuracy of the network on the 482 val images: 76.35%
[2023-09-04 12:41:15,053] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:41:15,055] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:41:15,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:41:15,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:41:16,472] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:41:16,472] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.35%
Epoch: [31]  [  0/160]  eta: 0:19:48  lr: 0.000045  min_lr: 0.000011  loss: 1.2117 (1.2117)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2016 (6.2016)  time: 7.4268 (7.4268 -- 7.4268)  data: 6.8627 (6.8627 -- 6.8627)  max mem: 16413
Epoch: [31]  [ 20/160]  eta: 0:02:47  lr: 0.000045  min_lr: 0.000011  loss: 1.9599 (1.8441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9760 (7.4768)  time: 0.8820 (0.5228 -- 2.7246)  data: 0.0521 (0.0002 -- 1.0119)  max mem: 16413
[2023-09-04 12:41:57,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=25, lr=[1.1390910354167532e-05, 1.1390910354167532e-05, 1.2656567060186148e-05, 1.2656567060186148e-05, 1.4062852289095717e-05, 1.4062852289095717e-05, 1.5625391432328577e-05, 1.5625391432328577e-05, 1.736154603592064e-05, 1.736154603592064e-05, 1.9290606706578487e-05, 1.9290606706578487e-05, 2.1434007451753873e-05, 2.1434007451753873e-05, 2.381556383528208e-05, 2.381556383528208e-05, 2.6461737594757868e-05, 2.6461737594757868e-05, 2.9401930660842074e-05, 2.9401930660842074e-05, 3.2668811845380086e-05, 3.2668811845380086e-05, 3.629867982820009e-05, 3.629867982820009e-05, 4.033186647577788e-05, 4.033186647577788e-05, 4.481318497308653e-05, 4.481318497308653e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 12:41:57,175] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=17.541522017209324, CurrSamplesPerSec=21.84457484115135, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:02:01  lr: 0.000045  min_lr: 0.000011  loss: 2.0040 (1.9162)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4596 (6.7599)  time: 0.8142 (0.5162 -- 2.0981)  data: 0.0023 (0.0003 -- 0.0114)  max mem: 16413
[2023-09-04 12:42:08,814] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:42:08,814] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:42:08,814] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:42:08,815] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 60/160]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000011  loss: 1.9869 (1.9060)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0078 (6.7103)  time: 1.0049 (0.5160 -- 4.7456)  data: 0.0018 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [31]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000011  loss: 1.9932 (1.9091)  loss_scale: 32768.0000 (22452.1481)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7219 (6.6268)  time: 0.7753 (0.5117 -- 2.9567)  data: 0.0013 (0.0001 -- 0.0045)  max mem: 16413
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000011  loss: 2.0689 (1.9358)  loss_scale: 32768.0000 (24494.8911)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1075 (6.6261)  time: 0.7871 (0.5180 -- 2.3951)  data: 0.0019 (0.0002 -- 0.0096)  max mem: 16413
[2023-09-04 12:42:58,922] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5071
[2023-09-04 12:42:58,922] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5071
[2023-09-04 12:42:58,922] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:42:58,922] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:42:58,922] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.8331 (1.9354)  loss_scale: 16384.0000 (24508.2975)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4576 (6.5799)  time: 0.9105 (0.5276 -- 3.0482)  data: 0.0016 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8218 (1.9191)  loss_scale: 16384.0000 (23355.9149)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3231 (6.5684)  time: 0.8510 (0.5220 -- 3.8001)  data: 0.0697 (0.0002 -- 1.0798)  max mem: 16413
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 2.0177 (1.9163)  loss_scale: 16384.0000 (22528.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7457 (6.6652)  time: 0.6954 (0.4960 -- 1.3052)  data: 0.0373 (0.0002 -- 0.7309)  max mem: 16413
Epoch: [31] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 2.0177 (1.9149)  loss_scale: 16384.0000 (22528.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7457 (6.6652)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.5549 (0.5549)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3102 (2.3102 -- 2.3102)  data: 2.0907 (2.0907 -- 2.0907)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7784 (0.9532)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (96.9697)  time: 0.4241 (0.2002 -- 2.3102)  data: 0.2145 (0.0007 -- 2.0907)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7308 (0.8903)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.8254)  time: 0.2268 (0.1710 -- 0.4605)  data: 0.0218 (0.0001 -- 0.2504)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9234 (0.9555)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (95.8506)  time: 0.2129 (0.1334 -- 0.4605)  data: 0.0212 (0.0001 -- 0.2504)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 75.519 Acc@5 96.058 loss 0.896
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 76.35%
Epoch: [32]  [  0/160]  eta: 0:21:54  lr: 0.000045  min_lr: 0.000011  loss: 2.3560 (2.3560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.0279 (5.0279)  time: 8.2171 (8.2171 -- 8.2171)  data: 7.6490 (7.6490 -- 7.6490)  max mem: 16413
Epoch: [32]  [ 20/160]  eta: 0:02:57  lr: 0.000045  min_lr: 0.000011  loss: 1.8858 (1.9097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8763 (6.6598)  time: 0.9176 (0.5279 -- 4.8502)  data: 0.2831 (0.0005 -- 3.2065)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:07  lr: 0.000045  min_lr: 0.000011  loss: 1.9689 (1.9537)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3586 (6.5245)  time: 0.8460 (0.5210 -- 3.2977)  data: 0.1549 (0.0004 -- 1.2648)  max mem: 16413
Epoch: [32]  [ 60/160]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000011  loss: 1.8074 (1.9255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4983 (6.3042)  time: 0.9031 (0.5200 -- 5.1863)  data: 0.1459 (0.0005 -- 2.4269)  max mem: 16413
[2023-09-04 12:45:03,274] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:45:03,275] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:45:03,275] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:45:03,276] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000011  loss: 1.8960 (1.9139)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4971 (6.1650)  time: 0.8032 (0.5234 -- 2.2824)  data: 0.0907 (0.0008 -- 1.7597)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 2.0974 (1.9355)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4953 (6.2363)  time: 0.8982 (0.5268 -- 2.4828)  data: 0.1966 (0.0005 -- 1.4149)  max mem: 16413
[2023-09-04 12:45:30,293] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5230
[2023-09-04 12:45:30,293] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:45:30,293] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5230
[2023-09-04 12:45:30,293] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:45:30,294] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 2.0519 (1.9515)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3278 (6.2575)  time: 0.8565 (0.5246 -- 3.9861)  data: 0.3095 (0.0002 -- 3.4417)  max mem: 16413
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8242 (1.9503)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4394 (6.3085)  time: 0.8639 (0.5270 -- 3.5582)  data: 0.3240 (0.0002 -- 3.0426)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.9669 (1.9447)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5356 (6.2892)  time: 0.6820 (0.4947 -- 2.1731)  data: 0.1593 (0.0002 -- 1.6821)  max mem: 16413
Epoch: [32] Total time: 0:02:23 (0.8944 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.9669 (1.9020)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5356 (6.2892)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6981 (0.6981)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3770 (2.3770 -- 2.3770)  data: 2.1770 (2.1770 -- 2.1770)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6981 (0.8956)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (96.9697)  time: 0.4093 (0.1965 -- 2.3770)  data: 0.2002 (0.0007 -- 2.1770)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6704 (0.8477)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (96.8254)  time: 0.2154 (0.1699 -- 0.3509)  data: 0.0132 (0.0001 -- 0.1779)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8719 (0.9099)  acc1: 77.7778 (72.6141)  acc5: 100.0000 (96.2656)  time: 0.2032 (0.1332 -- 0.3509)  data: 0.0123 (0.0001 -- 0.1779)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 75.104 Acc@5 96.058 loss 0.862
Accuracy of the network on the 482 val images: 75.10%
Max accuracy: 76.35%
Epoch: [33]  [  0/160]  eta: 0:22:25  lr: 0.000045  min_lr: 0.000011  loss: 2.2167 (2.2167)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.9206 (4.9206)  time: 8.4122 (8.4122 -- 8.4122)  data: 5.3403 (5.3403 -- 5.3403)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:40  lr: 0.000045  min_lr: 0.000011  loss: 1.9973 (1.9351)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9188 (6.6059)  time: 0.7832 (0.5256 -- 3.1619)  data: 0.0027 (0.0003 -- 0.0112)  max mem: 16413
Epoch: [33]  [ 40/160]  eta: 0:02:01  lr: 0.000044  min_lr: 0.000011  loss: 1.8982 (1.8928)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4413 (6.9860)  time: 0.8788 (0.5219 -- 2.0405)  data: 0.0618 (0.0002 -- 0.8716)  max mem: 16413
Epoch: [33]  [ 60/160]  eta: 0:01:36  lr: 0.000044  min_lr: 0.000011  loss: 2.0353 (1.9231)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3528 (6.8144)  time: 0.8706 (0.5329 -- 2.7061)  data: 0.2165 (0.0004 -- 1.6001)  max mem: 16413
[2023-09-04 12:47:31,205] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:47:31,205] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:47:31,205] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:47:31,205] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000011  loss: 1.9387 (1.9212)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5005 (6.6467)  time: 0.8865 (0.5237 -- 2.6324)  data: 0.0332 (0.0004 -- 0.6325)  max mem: 16413
Epoch: [33]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 1.7602 (1.9002)  loss_scale: 32768.0000 (19952.7921)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1033 (6.5836)  time: 0.8799 (0.5122 -- 3.9681)  data: 0.2618 (0.0004 -- 3.4573)  max mem: 16413
[2023-09-04 12:48:07,333] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5399
[2023-09-04 12:48:07,333] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5399
[2023-09-04 12:48:07,333] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:48:07,333] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:48:07,333] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.8521 (1.8936)  loss_scale: 32768.0000 (21800.1983)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3534 (6.6649)  time: 0.9257 (0.5140 -- 4.7125)  data: 0.0019 (0.0003 -- 0.0087)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:17  lr: 0.000044  min_lr: 0.000011  loss: 1.7709 (1.8778)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0728 (6.6312)  time: 0.6809 (0.5335 -- 2.4583)  data: 0.0019 (0.0004 -- 0.0125)  max mem: 16413
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8884 (1.8650)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0859 (6.7239)  time: 0.7402 (0.4948 -- 1.9833)  data: 0.0268 (0.0002 -- 0.5169)  max mem: 16413
Epoch: [33] Total time: 0:02:20 (0.8801 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8884 (1.8612)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0859 (6.7239)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.5274 (0.5274)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2781 (2.2781 -- 2.2781)  data: 2.0501 (2.0501 -- 2.0501)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6384 (0.9313)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4181 (0.1894 -- 2.2781)  data: 0.2082 (0.0009 -- 2.0501)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7600 (0.8460)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2239 (0.1707 -- 0.4219)  data: 0.0252 (0.0001 -- 0.2288)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8137 (0.9060)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (95.4357)  time: 0.2071 (0.1331 -- 0.4219)  data: 0.0235 (0.0001 -- 0.2288)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 76.763 Acc@5 95.021 loss 0.859
Accuracy of the network on the 482 val images: 76.76%
[2023-09-04 12:48:45,132] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:48:45,134] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:48:45,134] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:48:45,134] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:48:46,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:48:46,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.76%
Epoch: [34]  [  0/160]  eta: 0:16:00  lr: 0.000044  min_lr: 0.000011  loss: 1.7771 (1.7771)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.3337 (8.3337)  time: 6.0047 (6.0047 -- 6.0047)  data: 5.2874 (5.2874 -- 5.2874)  max mem: 16413
Epoch: [34]  [ 20/160]  eta: 0:02:53  lr: 0.000044  min_lr: 0.000011  loss: 1.8603 (1.8835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6437 (7.0069)  time: 0.9991 (0.5164 -- 3.9282)  data: 0.1014 (0.0002 -- 0.7001)  max mem: 16413
Epoch: [34]  [ 40/160]  eta: 0:02:08  lr: 0.000044  min_lr: 0.000011  loss: 1.9090 (1.8835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4081 (6.7059)  time: 0.8916 (0.5226 -- 3.7968)  data: 0.0016 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [34]  [ 60/160]  eta: 0:01:37  lr: 0.000044  min_lr: 0.000011  loss: 1.8709 (1.8671)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0734 (6.5567)  time: 0.7786 (0.5212 -- 1.8297)  data: 0.0025 (0.0007 -- 0.0142)  max mem: 16413
Epoch: [34]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.9725 (1.8828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8195 (6.6810)  time: 0.8961 (0.5224 -- 2.4567)  data: 0.0985 (0.0005 -- 1.9308)  max mem: 16413
[2023-09-04 12:50:09,841] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:50:09,842] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:50:09,842] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:50:09,843] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 1.9175 (1.8839)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3700 (6.6200)  time: 0.9297 (0.5139 -- 3.7752)  data: 0.3817 (0.0004 -- 3.2539)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000011  loss: 1.7600 (1.8525)  loss_scale: 32768.0000 (20852.3636)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2159 (6.6316)  time: 0.7337 (0.5177 -- 2.5077)  data: 0.1880 (0.0002 -- 1.9718)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.8684 (1.8472)  loss_scale: 32768.0000 (22542.5248)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8142 (6.6537)  time: 0.9051 (0.5268 -- 3.5536)  data: 0.2179 (0.0005 -- 3.0280)  max mem: 16413
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.9224 (1.8538)  loss_scale: 32768.0000 (23756.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1294 (6.7618)  time: 0.6820 (0.4945 -- 3.4418)  data: 0.0154 (0.0002 -- 0.2918)  max mem: 16413
Epoch: [34] Total time: 0:02:21 (0.8861 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.9224 (1.8782)  loss_scale: 32768.0000 (23756.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1294 (6.7618)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4988 (0.4988)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4464 (2.4464 -- 2.4464)  data: 2.2325 (2.2325 -- 2.2325)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6106 (0.8801)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (96.9697)  time: 0.4233 (0.2019 -- 2.4464)  data: 0.2101 (0.0005 -- 2.2325)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6374 (0.7989)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (96.8254)  time: 0.2098 (0.1720 -- 0.3019)  data: 0.0056 (0.0001 -- 0.0633)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7539 (0.8541)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.4357)  time: 0.1956 (0.1334 -- 0.3019)  data: 0.0050 (0.0001 -- 0.0633)  max mem: 16413
Val: Total time: 0:00:07 (0.2827 s / it)
* Acc@1 78.423 Acc@5 95.851 loss 0.810
Accuracy of the network on the 482 val images: 78.42%
[2023-09-04 12:51:16,012] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 12:51:16,013] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 12:51:16,013] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 12:51:16,014] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 12:51:17,528] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 12:51:17,528] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.42%
Epoch: [35]  [  0/160]  eta: 0:22:19  lr: 0.000044  min_lr: 0.000011  loss: 1.3321 (1.3321)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9939 (5.9939)  time: 8.3716 (8.3716 -- 8.3716)  data: 7.8285 (7.8285 -- 7.8285)  max mem: 16413
Epoch: [35]  [ 20/160]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000011  loss: 1.8060 (1.7384)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0384 (6.1782)  time: 0.7745 (0.5244 -- 3.9216)  data: 0.1692 (0.0003 -- 2.3130)  max mem: 16413
Epoch: [35]  [ 40/160]  eta: 0:02:07  lr: 0.000044  min_lr: 0.000011  loss: 1.9512 (1.8592)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4311 (6.4508)  time: 0.9766 (0.5140 -- 3.2889)  data: 0.3329 (0.0003 -- 2.7379)  max mem: 16413
[2023-09-04 12:52:14,338] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:52:14,339] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:52:14,341] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:52:14,380] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:52:14,928] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5657
[2023-09-04 12:52:14,928] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5657
[2023-09-04 12:52:14,929] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:52:14,929] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:52:14,929] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 60/160]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000011  loss: 1.9224 (1.8562)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5998 (6.2690)  time: 0.8716 (0.5279 -- 3.4959)  data: 0.1501 (0.0003 -- 2.9603)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000011  loss: 1.6790 (1.8435)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9762 (6.3238)  time: 0.8061 (0.5311 -- 2.9788)  data: 0.0016 (0.0002 -- 0.0033)  max mem: 16413
[2023-09-04 12:52:41,577] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5687
[2023-09-04 12:52:41,577] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5687
[2023-09-04 12:52:41,577] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:52:41,577] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 12:52:41,577] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 2.0288 (1.8615)  loss_scale: 16384.0000 (30821.3861)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6573 (6.3786)  time: 0.9652 (0.5275 -- 3.2864)  data: 0.0019 (0.0001 -- 0.0130)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.8824 (1.8509)  loss_scale: 16384.0000 (28435.0413)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7417 (6.3348)  time: 0.8476 (0.5298 -- 2.8109)  data: 0.0019 (0.0004 -- 0.0086)  max mem: 16413
Epoch: [35]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.8774 (1.8579)  loss_scale: 16384.0000 (26725.6738)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8584 (6.3339)  time: 0.9147 (0.5227 -- 2.3184)  data: 0.0014 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8223 (1.8683)  loss_scale: 16384.0000 (25497.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0035 (6.3747)  time: 0.6053 (0.4951 -- 1.2490)  data: 0.0008 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [35] Total time: 0:02:23 (0.8943 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8223 (1.8558)  loss_scale: 16384.0000 (25497.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0035 (6.3747)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3961 (0.3961)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2644 (2.2644 -- 2.2644)  data: 2.0615 (2.0615 -- 2.0615)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6952 (0.8783)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4026 (0.2022 -- 2.2644)  data: 0.1891 (0.0007 -- 2.0615)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6644 (0.7984)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (96.8254)  time: 0.2262 (0.1699 -- 0.6141)  data: 0.0217 (0.0001 -- 0.4128)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8318 (0.8677)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.0207)  time: 0.2103 (0.1327 -- 0.6141)  data: 0.0212 (0.0001 -- 0.4128)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 76.763 Acc@5 95.436 loss 0.840
Accuracy of the network on the 482 val images: 76.76%
Max accuracy: 78.42%
Epoch: [36]  [  0/160]  eta: 0:21:09  lr: 0.000044  min_lr: 0.000011  loss: 1.9511 (1.9511)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4060 (6.4060)  time: 7.9353 (7.9353 -- 7.9353)  data: 7.4251 (7.4251 -- 7.4251)  max mem: 16413
Epoch: [36]  [ 20/160]  eta: 0:02:42  lr: 0.000044  min_lr: 0.000011  loss: 1.6400 (1.6943)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5306 (5.8970)  time: 0.8248 (0.5311 -- 2.6775)  data: 0.1931 (0.0002 -- 2.1216)  max mem: 16413
Epoch: [36]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000011  loss: 1.9274 (1.7871)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1837 (6.1983)  time: 0.8403 (0.5192 -- 2.9310)  data: 0.2441 (0.0005 -- 2.4066)  max mem: 16413
[2023-09-04 12:54:43,317] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:54:43,317] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 12:54:43,317] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:54:43,317] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000011  loss: 1.7536 (1.7892)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8870 (6.2759)  time: 0.9482 (0.5203 -- 3.6920)  data: 0.3683 (0.0005 -- 3.1657)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.9684 (1.8334)  loss_scale: 32768.0000 (21440.7901)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6900 (6.1774)  time: 0.8619 (0.5226 -- 4.0842)  data: 0.3167 (0.0003 -- 3.5585)  max mem: 16413
Epoch: [36]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 2.0417 (1.8669)  loss_scale: 32768.0000 (23683.8020)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9330 (6.2622)  time: 0.9719 (0.5177 -- 4.1794)  data: 0.0708 (0.0004 -- 0.8851)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 2.0712 (1.9092)  loss_scale: 32768.0000 (25185.3223)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5263 (6.3594)  time: 0.7648 (0.5193 -- 3.1289)  data: 0.1283 (0.0003 -- 2.4836)  max mem: 16413
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.6409 (1.8936)  loss_scale: 32768.0000 (26260.8794)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3191 (6.3996)  time: 0.9263 (0.5280 -- 4.2050)  data: 0.3764 (0.0004 -- 3.6918)  max mem: 16413
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 2.0145 (1.9028)  loss_scale: 32768.0000 (27033.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5069 (6.4338)  time: 0.6540 (0.4946 -- 3.2170)  data: 0.1014 (0.0002 -- 2.0141)  max mem: 16413
Epoch: [36] Total time: 0:02:23 (0.8955 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 2.0145 (1.8771)  loss_scale: 32768.0000 (27033.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5069 (6.4338)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3917 (0.3917)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3512 (2.3512 -- 2.3512)  data: 2.1411 (2.1411 -- 2.1411)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6443 (0.9178)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (96.9697)  time: 0.4094 (0.1935 -- 2.3512)  data: 0.1958 (0.0008 -- 2.1411)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5998 (0.7758)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.8254)  time: 0.2182 (0.1693 -- 0.3305)  data: 0.0152 (0.0001 -- 0.1557)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6290 (0.8523)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.4357)  time: 0.2028 (0.1331 -- 0.3305)  data: 0.0149 (0.0001 -- 0.1557)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 75.934 Acc@5 95.643 loss 0.825
Accuracy of the network on the 482 val images: 75.93%
Max accuracy: 78.42%
Epoch: [37]  [  0/160]  eta: 0:17:31  lr: 0.000044  min_lr: 0.000011  loss: 2.3092 (2.3092)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4962 (6.4962)  time: 6.5740 (6.5740 -- 6.5740)  data: 5.9815 (5.9815 -- 5.9815)  max mem: 16413
Epoch: [37]  [ 20/160]  eta: 0:02:56  lr: 0.000044  min_lr: 0.000011  loss: 1.9471 (1.9546)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7995 (5.9743)  time: 0.9922 (0.5297 -- 3.6939)  data: 0.3340 (0.0005 -- 3.1775)  max mem: 16413
[2023-09-04 12:56:47,995] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:56:47,995] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:56:47,998] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:56:47,999] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [ 40/160]  eta: 0:02:01  lr: 0.000044  min_lr: 0.000011  loss: 1.8637 (1.9020)  loss_scale: 65536.0000 (46354.7317)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5871 (6.2433)  time: 0.7491 (0.5313 -- 2.9931)  data: 0.1994 (0.0001 -- 2.4844)  max mem: 16413
[2023-09-04 12:57:21,744] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5979
[2023-09-04 12:57:21,744] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:57:21,744] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5979
[2023-09-04 12:57:21,744] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 12:57:21,744] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [37]  [ 60/160]  eta: 0:01:43  lr: 0.000044  min_lr: 0.000011  loss: 1.9634 (1.9156)  loss_scale: 65536.0000 (51569.3115)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0051 (6.2047)  time: 1.0730 (0.5062 -- 4.8352)  data: 0.5311 (0.0005 -- 4.2824)  max mem: 16413
[2023-09-04 12:57:37,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=31, lr=[1.1117940440317657e-05, 1.1117940440317657e-05, 1.2353267155908508e-05, 1.2353267155908508e-05, 1.3725852395453895e-05, 1.3725852395453895e-05, 1.5250947106059886e-05, 1.5250947106059886e-05, 1.6945496784510984e-05, 1.6945496784510984e-05, 1.882832976056776e-05, 1.882832976056776e-05, 2.092036640063084e-05, 2.092036640063084e-05, 2.324485155625649e-05, 2.324485155625649e-05, 2.5827612840284987e-05, 2.5827612840284987e-05, 2.869734760031665e-05, 2.869734760031665e-05, 3.1885941778129613e-05, 3.1885941778129613e-05, 3.542882419792179e-05, 3.542882419792179e-05, 3.93653602199131e-05, 3.93653602199131e-05, 4.373928913323678e-05, 4.373928913323678e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 12:57:37,965] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=17.493832907480897, CurrSamplesPerSec=22.26983268748656, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:18  lr: 0.000044  min_lr: 0.000011  loss: 1.8220 (1.9029)  loss_scale: 32768.0000 (46927.0123)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3853 (6.1347)  time: 0.8123 (0.5229 -- 3.1817)  data: 0.2648 (0.0001 -- 2.6381)  max mem: 16413
Epoch: [37]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 1.9864 (1.9099)  loss_scale: 32768.0000 (44123.2475)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9368 (6.1057)  time: 0.9162 (0.5199 -- 3.5419)  data: 0.3718 (0.0004 -- 3.0087)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.8700 (1.8944)  loss_scale: 32768.0000 (42246.3471)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7500 (6.2675)  time: 0.8337 (0.5262 -- 4.0185)  data: 0.2845 (0.0004 -- 3.4924)  max mem: 16413
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.7909 (1.8745)  loss_scale: 32768.0000 (40901.9007)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1068 (6.2434)  time: 0.8402 (0.5191 -- 2.8453)  data: 0.2980 (0.0005 -- 2.3346)  max mem: 16413
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8932 (1.8762)  loss_scale: 32768.0000 (39936.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9755 (6.2436)  time: 0.6932 (0.4945 -- 2.8655)  data: 0.1719 (0.0001 -- 2.3494)  max mem: 16413
Epoch: [37] Total time: 0:02:24 (0.9016 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8932 (1.8806)  loss_scale: 32768.0000 (39936.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9755 (6.2436)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3985 (0.3985)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3158 (2.3158 -- 2.3158)  data: 2.0953 (2.0953 -- 2.0953)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5479 (0.8370)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4376 (0.2076 -- 2.3158)  data: 0.2155 (0.0005 -- 2.0953)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7157 (0.7819)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (96.8254)  time: 0.2220 (0.1694 -- 0.4912)  data: 0.0139 (0.0001 -- 0.2634)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7709 (0.8476)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (96.2656)  time: 0.2047 (0.1329 -- 0.4912)  data: 0.0136 (0.0001 -- 0.2634)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 76.349 Acc@5 96.680 loss 0.815
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 78.42%
Epoch: [38]  [  0/160]  eta: 0:15:11  lr: 0.000044  min_lr: 0.000011  loss: 1.9263 (1.9263)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8592 (6.8592)  time: 5.6966 (5.6966 -- 5.6966)  data: 5.1676 (5.1675 -- 5.1675)  max mem: 16413
Epoch: [38]  [ 20/160]  eta: 0:02:40  lr: 0.000044  min_lr: 0.000011  loss: 1.8673 (1.8801)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1968 (6.6195)  time: 0.9181 (0.5329 -- 2.9942)  data: 0.3174 (0.0006 -- 2.4232)  max mem: 16413
[2023-09-04 12:59:23,085] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:59:23,085] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 12:59:23,085] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:59:23,085] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 12:59:24,212] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6110
[2023-09-04 12:59:24,213] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:59:24,213] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6110
[2023-09-04 12:59:24,213] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 12:59:24,213] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [38]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000011  loss: 1.9691 (1.8794)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0539 (6.7625)  time: 0.8610 (0.5419 -- 2.7569)  data: 0.3126 (0.0003 -- 2.2482)  max mem: 16413
Epoch: [38]  [ 60/160]  eta: 0:01:35  lr: 0.000044  min_lr: 0.000011  loss: 1.8152 (1.8496)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3002 (6.7002)  time: 0.8518 (0.5310 -- 4.7895)  data: 0.3007 (0.0005 -- 4.2803)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:14  lr: 0.000044  min_lr: 0.000011  loss: 1.8453 (1.8595)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5510 (6.6353)  time: 0.8550 (0.5309 -- 2.9388)  data: 0.2806 (0.0005 -- 2.4222)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:55  lr: 0.000044  min_lr: 0.000011  loss: 1.9768 (1.8882)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5222 (6.6453)  time: 0.9322 (0.5405 -- 3.2648)  data: 0.3695 (0.0010 -- 2.7288)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000011  loss: 1.8955 (1.9040)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6711 (6.7725)  time: 0.9263 (0.5191 -- 4.6700)  data: 0.3808 (0.0003 -- 4.1657)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.8588 (1.8955)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1937 (6.9048)  time: 0.8991 (0.5199 -- 3.6125)  data: 0.3563 (0.0005 -- 3.1068)  max mem: 16413
[2023-09-04 13:01:13,929] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:01:13,929] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:01:13,929] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:01:13,929] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.9087 (1.8948)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7723 (6.7895)  time: 0.6269 (0.4955 -- 2.1961)  data: 0.1045 (0.0003 -- 1.6853)  max mem: 16413
Epoch: [38] Total time: 0:02:22 (0.8910 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.9087 (1.8768)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7723 (6.7895)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4456 (0.4456)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2808 (2.2808 -- 2.2808)  data: 2.0594 (2.0594 -- 2.0594)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6286 (0.8339)  acc1: 66.6667 (72.7273)  acc5: 100.0000 (97.9798)  time: 0.4069 (0.2083 -- 2.2808)  data: 0.1897 (0.0008 -- 2.0594)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6383 (0.7752)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (97.3545)  time: 0.2176 (0.1693 -- 0.4534)  data: 0.0132 (0.0001 -- 0.2349)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7374 (0.8390)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.6805)  time: 0.2019 (0.1330 -- 0.4534)  data: 0.0129 (0.0001 -- 0.2349)  max mem: 16413
Val: Total time: 0:00:07 (0.2822 s / it)
* Acc@1 78.838 Acc@5 96.888 loss 0.807
Accuracy of the network on the 482 val images: 78.84%
[2023-09-04 13:01:21,643] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 13:01:21,645] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 13:01:21,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 13:01:21,645] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 13:01:22,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 13:01:22,978] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.84%
Epoch: [39]  [  0/160]  eta: 0:17:55  lr: 0.000043  min_lr: 0.000011  loss: 2.0400 (2.0400)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 2.1795 (2.1795)  time: 6.7210 (6.7210 -- 6.7210)  data: 4.4246 (4.4246 -- 4.4246)  max mem: 16413
[2023-09-04 13:01:36,181] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6248
[2023-09-04 13:01:36,181] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:01:36,181] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6248
[2023-09-04 13:01:36,182] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:01:36,182] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [ 20/160]  eta: 0:02:38  lr: 0.000043  min_lr: 0.000011  loss: 1.9204 (1.8934)  loss_scale: 32768.0000 (45251.0476)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8814 (5.9634)  time: 0.8554 (0.5442 -- 3.0919)  data: 0.1353 (0.0002 -- 2.5315)  max mem: 16413
Epoch: [39]  [ 40/160]  eta: 0:02:00  lr: 0.000043  min_lr: 0.000011  loss: 1.7642 (1.8825)  loss_scale: 32768.0000 (39161.7561)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9858 (6.1205)  time: 0.8645 (0.5199 -- 4.2242)  data: 0.2534 (0.0006 -- 3.6822)  max mem: 16413
Epoch: [39]  [ 60/160]  eta: 0:01:36  lr: 0.000043  min_lr: 0.000011  loss: 1.9854 (1.9014)  loss_scale: 32768.0000 (37065.4426)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3931 (6.4814)  time: 0.8752 (0.5214 -- 3.0724)  data: 0.3219 (0.0004 -- 2.5437)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:13  lr: 0.000043  min_lr: 0.000011  loss: 2.0251 (1.9235)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0617 (6.4568)  time: 0.8087 (0.5247 -- 2.8841)  data: 0.1342 (0.0002 -- 2.0365)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000011  loss: 1.8411 (1.9113)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4564 (6.5172)  time: 0.9530 (0.5305 -- 3.0665)  data: 0.3066 (0.0004 -- 2.1948)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.8283 (1.8983)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9471 (6.4941)  time: 0.8995 (0.5129 -- 3.5679)  data: 0.1220 (0.0002 -- 1.5115)  max mem: 16413
[2023-09-04 13:03:30,184] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:03:30,185] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:03:30,185] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:03:30,185] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.7898 (1.8957)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8593 (6.5660)  time: 0.8890 (0.5412 -- 3.2773)  data: 0.2623 (0.0005 -- 2.7261)  max mem: 16413
[2023-09-04 13:03:36,081] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6385
[2023-09-04 13:03:36,081] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6385
[2023-09-04 13:03:36,081] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:03:36,081] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:03:36,081] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.8158 (1.8816)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0452 (6.5220)  time: 0.5854 (0.4955 -- 1.2937)  data: 0.0452 (0.0002 -- 0.7827)  max mem: 16413
Epoch: [39] Total time: 0:02:20 (0.8801 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.8158 (1.8732)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0452 (6.5220)
[2023-09-04 13:03:43,800] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-09-04 13:03:43,802] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-09-04 13:03:43,802] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-09-04 13:03:43,802] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-09-04 13:03:44,683] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-09-04 13:03:44,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3978 (0.3978)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5849 (2.5849 -- 2.5849)  data: 2.3615 (2.3615 -- 2.3615)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6562 (0.8913)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4450 (0.1913 -- 2.5849)  data: 0.2283 (0.0008 -- 2.3615)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6562 (0.8117)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2110 (0.1690 -- 0.4026)  data: 0.0077 (0.0001 -- 0.1244)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8747 (0.8735)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.8506)  time: 0.1961 (0.1329 -- 0.4026)  data: 0.0066 (0.0001 -- 0.1244)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 77.178 Acc@5 95.851 loss 0.818
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 78.84%
Epoch: [40]  [  0/160]  eta: 0:17:37  lr: 0.000043  min_lr: 0.000011  loss: 1.2597 (1.2597)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 10.5079 (10.5079)  time: 6.6112 (6.6112 -- 6.6112)  data: 4.5326 (4.5326 -- 4.5326)  max mem: 16413
Epoch: [40]  [ 20/160]  eta: 0:02:38  lr: 0.000043  min_lr: 0.000011  loss: 1.8878 (1.8091)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4804 (5.8338)  time: 0.8580 (0.5251 -- 4.3687)  data: 0.0440 (0.0005 -- 0.8336)  max mem: 16413
Epoch: [40]  [ 40/160]  eta: 0:02:01  lr: 0.000043  min_lr: 0.000011  loss: 1.8909 (1.8477)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7603 (6.3566)  time: 0.8824 (0.5188 -- 3.1370)  data: 0.0024 (0.0002 -- 0.0136)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.7638 (1.8260)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8411 (6.5299)  time: 0.9268 (0.5198 -- 3.4421)  data: 0.0014 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000011  loss: 1.7368 (1.8131)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1565 (6.5162)  time: 0.8199 (0.5285 -- 2.7047)  data: 0.0016 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.9168 (1.8242)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5150 (6.5328)  time: 0.9266 (0.5162 -- 2.7852)  data: 0.0018 (0.0003 -- 0.0065)  max mem: 16413
[2023-09-04 13:05:39,316] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:05:39,317] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:05:39,317] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:05:39,317] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.9480 (1.8388)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5115 (6.4798)  time: 0.8132 (0.5270 -- 2.8685)  data: 0.0189 (0.0001 -- 0.3485)  max mem: 16413
[2023-09-04 13:05:49,349] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6526
[2023-09-04 13:05:49,350] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:05:49,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6526
[2023-09-04 13:05:49,353] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:05:49,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [40]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.8798 (1.8500)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2623 (6.4310)  time: 0.8968 (0.5246 -- 2.5152)  data: 0.0021 (0.0002 -- 0.0088)  max mem: 16413
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.8190 (1.8359)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9876 (6.5228)  time: 0.7635 (0.4946 -- 2.5152)  data: 0.0820 (0.0002 -- 1.6250)  max mem: 16413
Epoch: [40] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.8190 (1.8431)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9876 (6.5228)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3824 (0.3824)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4991 (2.4991 -- 2.4991)  data: 2.2847 (2.2847 -- 2.2847)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6865 (0.8762)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4208 (0.1922 -- 2.4991)  data: 0.2091 (0.0006 -- 2.2847)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6248 (0.7792)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2079 (0.1706 -- 0.2505)  data: 0.0042 (0.0001 -- 0.0509)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6930 (0.8372)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.4357)  time: 0.1921 (0.1333 -- 0.2505)  data: 0.0037 (0.0001 -- 0.0509)  max mem: 16413
Val: Total time: 0:00:07 (0.2832 s / it)
* Acc@1 79.876 Acc@5 95.228 loss 0.788
Accuracy of the network on the 482 val images: 79.88%
[2023-09-04 13:06:22,034] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 13:06:22,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 13:06:22,036] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 13:06:22,036] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 13:06:23,453] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 13:06:23,454] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.88%
Epoch: [41]  [  0/160]  eta: 0:22:44  lr: 0.000043  min_lr: 0.000011  loss: 2.3268 (2.3268)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3315 (6.3315)  time: 8.5281 (8.5281 -- 8.5281)  data: 8.0052 (8.0052 -- 8.0052)  max mem: 16413
Epoch: [41]  [ 20/160]  eta: 0:02:38  lr: 0.000043  min_lr: 0.000011  loss: 1.9317 (1.9153)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9527 (6.9768)  time: 0.7651 (0.5263 -- 3.8902)  data: 0.2202 (0.0008 -- 3.3487)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:07  lr: 0.000043  min_lr: 0.000011  loss: 1.8266 (1.9100)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6134 (6.9504)  time: 0.9790 (0.5286 -- 4.0694)  data: 0.4252 (0.0005 -- 3.5454)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:36  lr: 0.000043  min_lr: 0.000011  loss: 1.8879 (1.9012)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0801 (6.7993)  time: 0.7671 (0.5318 -- 1.8740)  data: 0.1809 (0.0003 -- 1.3527)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000011  loss: 2.0116 (1.8959)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5108 (6.9105)  time: 0.8687 (0.5298 -- 2.7206)  data: 0.1674 (0.0001 -- 1.6676)  max mem: 16413
[2023-09-04 13:07:51,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6652
[2023-09-04 13:07:51,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6652
[2023-09-04 13:07:51,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:07:51,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:07:51,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.8728 (1.8771)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4399 (6.8421)  time: 0.9800 (0.5179 -- 3.4935)  data: 0.0015 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [41]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.7693 (1.8577)  loss_scale: 16384.0000 (28841.2562)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7016 (6.9046)  time: 0.7833 (0.5177 -- 2.7347)  data: 0.0018 (0.0003 -- 0.0061)  max mem: 16413
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.7513 (1.8481)  loss_scale: 16384.0000 (27074.2695)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8159 (6.9547)  time: 1.0028 (0.5165 -- 3.6412)  data: 0.0018 (0.0005 -- 0.0049)  max mem: 16413
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.8974 (1.8569)  loss_scale: 16384.0000 (25804.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7147 (6.9583)  time: 0.5807 (0.4975 -- 1.1234)  data: 0.0006 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [41] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.8974 (1.8507)  loss_scale: 16384.0000 (25804.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7147 (6.9583)
Val:  [ 0/27]  eta: 0:00:55  loss: 0.4285 (0.4285)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0529 (2.0529 -- 2.0529)  data: 1.8469 (1.8469 -- 1.8469)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4947 (0.7706)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4158 (0.1988 -- 2.0529)  data: 0.1955 (0.0008 -- 1.8469)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6133 (0.7211)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2318 (0.1692 -- 0.5197)  data: 0.0230 (0.0001 -- 0.2625)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7569 (0.7652)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2150 (0.1333 -- 0.5197)  data: 0.0216 (0.0001 -- 0.2625)  max mem: 16413
Val: Total time: 0:00:07 (0.2846 s / it)
* Acc@1 80.705 Acc@5 96.680 loss 0.728
Accuracy of the network on the 482 val images: 80.71%
[2023-09-04 13:08:53,821] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 13:08:53,822] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 13:08:53,822] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 13:08:53,823] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 13:08:55,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 13:08:55,382] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.71%
Epoch: [42]  [  0/160]  eta: 0:20:33  lr: 0.000043  min_lr: 0.000011  loss: 2.0690 (2.0690)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.5765 (8.5765)  time: 7.7092 (7.7092 -- 7.7092)  data: 6.6906 (6.6906 -- 6.6906)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:40  lr: 0.000043  min_lr: 0.000011  loss: 1.9300 (1.8835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9872 (6.4609)  time: 0.8204 (0.5192 -- 3.4490)  data: 0.1276 (0.0003 -- 1.8357)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:03  lr: 0.000043  min_lr: 0.000011  loss: 1.8129 (1.8127)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1326 (6.8665)  time: 0.9107 (0.5194 -- 4.0524)  data: 0.0455 (0.0004 -- 0.6256)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.8561 (1.8248)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5552 (7.0179)  time: 0.8742 (0.5140 -- 3.3376)  data: 0.0072 (0.0005 -- 0.0859)  max mem: 16413
[2023-09-04 13:09:55,741] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:09:55,741] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:09:55,745] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:09:55,746] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [42]  [ 80/160]  eta: 0:01:16  lr: 0.000043  min_lr: 0.000011  loss: 1.9488 (1.8514)  loss_scale: 32768.0000 (20429.4321)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6793 (6.9179)  time: 0.8632 (0.5219 -- 3.3607)  data: 0.0726 (0.0004 -- 1.4105)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:57  lr: 0.000043  min_lr: 0.000011  loss: 1.7686 (1.8371)  loss_scale: 32768.0000 (22872.7129)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0753 (6.7982)  time: 0.9471 (0.5254 -- 2.6910)  data: 0.2948 (0.0002 -- 2.1659)  max mem: 16413
Epoch: [42]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.7287 (1.8165)  loss_scale: 32768.0000 (24508.2975)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.4519 (6.6446)  time: 0.6976 (0.5227 -- 2.1683)  data: 0.0351 (0.0006 -- 0.6625)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.6443 (1.8023)  loss_scale: 32768.0000 (25679.8865)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9821 (6.6075)  time: 1.0275 (0.5355 -- 3.1198)  data: 0.1432 (0.0002 -- 2.4423)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.7637 (1.7967)  loss_scale: 32768.0000 (26521.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8935 (6.6314)  time: 0.7434 (0.4946 -- 2.8751)  data: 0.1793 (0.0002 -- 2.3701)  max mem: 16413
Epoch: [42] Total time: 0:02:24 (0.9053 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.7637 (1.8225)  loss_scale: 32768.0000 (26521.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8935 (6.6314)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3823 (0.3823)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2607 (2.2607 -- 2.2607)  data: 2.0233 (2.0233 -- 2.0233)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5469 (0.8177)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4069 (0.2043 -- 2.2607)  data: 0.1868 (0.0004 -- 2.0233)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6003 (0.7257)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2141 (0.1687 -- 0.2924)  data: 0.0076 (0.0001 -- 0.1176)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6270 (0.7812)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.8506)  time: 0.1945 (0.1327 -- 0.2924)  data: 0.0068 (0.0001 -- 0.1176)  max mem: 16413
Val: Total time: 0:00:07 (0.2789 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.750
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 80.71%
Epoch: [43]  [  0/160]  eta: 0:17:20  lr: 0.000043  min_lr: 0.000011  loss: 1.9530 (1.9530)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 10.1273 (10.1273)  time: 6.5021 (6.5021 -- 6.5021)  data: 4.7110 (4.7110 -- 4.7110)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:46  lr: 0.000043  min_lr: 0.000011  loss: 1.8832 (1.8516)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4738 (6.8273)  time: 0.9225 (0.5196 -- 2.9259)  data: 0.2946 (0.0005 -- 2.4006)  max mem: 16413
[2023-09-04 13:12:00,155] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:12:00,156] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:12:00,158] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:12:00,159] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:12:06,032] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6914
[2023-09-04 13:12:06,032] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:12:06,032] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6914
[2023-09-04 13:12:06,032] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:12:06,032] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [43]  [ 40/160]  eta: 0:02:01  lr: 0.000043  min_lr: 0.000011  loss: 1.7970 (1.7869)  loss_scale: 32768.0000 (36764.0976)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3102 (6.6165)  time: 0.8198 (0.5170 -- 3.6183)  data: 0.1065 (0.0004 -- 1.2856)  max mem: 16413
Epoch: [43]  [ 60/160]  eta: 0:01:39  lr: 0.000043  min_lr: 0.000011  loss: 1.7116 (1.8072)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4940 (6.8060)  time: 0.9798 (0.5163 -- 4.8497)  data: 0.0539 (0.0003 -- 1.0493)  max mem: 16413
Epoch: [43]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000011  loss: 1.8918 (1.7936)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3672 (6.7440)  time: 0.7811 (0.5222 -- 2.9297)  data: 0.0016 (0.0005 -- 0.0027)  max mem: 16413
[2023-09-04 13:12:48,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6965
[2023-09-04 13:12:48,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6965
[2023-09-04 13:12:48,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:12:48,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:12:48,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [43]  [100/160]  eta: 0:00:55  lr: 0.000042  min_lr: 0.000011  loss: 1.7804 (1.8125)  loss_scale: 16384.0000 (31794.6931)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8075 (6.7818)  time: 0.8802 (0.5256 -- 3.3771)  data: 0.0042 (0.0002 -- 0.0445)  max mem: 16413
[2023-09-04 13:13:17,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=38, lr=[1.079275389402428e-05, 1.079275389402428e-05, 1.199194877113809e-05, 1.199194877113809e-05, 1.3324387523486763e-05, 1.3324387523486763e-05, 1.4804875026096405e-05, 1.4804875026096405e-05, 1.6449861140107117e-05, 1.6449861140107117e-05, 1.8277623489007908e-05, 1.8277623489007908e-05, 2.0308470543342117e-05, 2.0308470543342117e-05, 2.2564967270380128e-05, 2.2564967270380128e-05, 2.5072185855977923e-05, 2.5072185855977923e-05, 2.785798428441991e-05, 2.785798428441991e-05, 3.095331587157768e-05, 3.095331587157768e-05, 3.439257319064187e-05, 3.439257319064187e-05, 3.8213970211824294e-05, 3.8213970211824294e-05, 4.245996690202699e-05, 4.245996690202699e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 13:13:17,789] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=17.493252210923245, CurrSamplesPerSec=21.17453081418096, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.6451 (1.7915)  loss_scale: 16384.0000 (29247.4711)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0191 (6.7408)  time: 0.8096 (0.5308 -- 3.4880)  data: 0.1376 (0.0002 -- 1.6884)  max mem: 16413
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.8739 (1.7963)  loss_scale: 16384.0000 (27422.8652)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6911 (6.7582)  time: 1.0062 (0.5277 -- 3.4754)  data: 0.3783 (0.0006 -- 2.9509)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.8759 (1.8013)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2142 (6.7494)  time: 0.6467 (0.4930 -- 2.1841)  data: 0.1233 (0.0001 -- 1.5715)  max mem: 16413
Epoch: [43] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.8759 (1.8227)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2142 (6.7494)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3535 (0.3535)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4183 (2.4183 -- 2.4183)  data: 2.1918 (2.1918 -- 2.1918)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5141 (0.7908)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (94.9495)  time: 0.4228 (0.1902 -- 2.4183)  data: 0.2109 (0.0004 -- 2.1918)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5141 (0.6919)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2227 (0.1695 -- 0.4894)  data: 0.0220 (0.0001 -- 0.3093)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6132 (0.7689)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.0207)  time: 0.2079 (0.1325 -- 0.4894)  data: 0.0212 (0.0001 -- 0.3093)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 78.631 Acc@5 95.643 loss 0.756
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 80.71%
Epoch: [44]  [  0/160]  eta: 0:20:07  lr: 0.000042  min_lr: 0.000011  loss: 1.6781 (1.6781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 9.8913 (9.8913)  time: 7.5500 (7.5500 -- 7.5500)  data: 7.0392 (7.0392 -- 7.0392)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:02:46  lr: 0.000042  min_lr: 0.000011  loss: 1.8155 (1.7488)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6742 (6.0497)  time: 0.8681 (0.5198 -- 3.4713)  data: 0.3140 (0.0007 -- 2.9576)  max mem: 16413
Epoch: [44]  [ 40/160]  eta: 0:02:05  lr: 0.000042  min_lr: 0.000011  loss: 1.6560 (1.7172)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8834 (6.1534)  time: 0.8949 (0.5163 -- 3.3863)  data: 0.3522 (0.0002 -- 2.8333)  max mem: 16413
[2023-09-04 13:14:55,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:14:55,142] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:14:55,147] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:14:55,148] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000011  loss: 1.8601 (1.7698)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3359 (6.4117)  time: 0.9015 (0.5261 -- 3.5123)  data: 0.3212 (0.0002 -- 2.9899)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:17  lr: 0.000042  min_lr: 0.000011  loss: 1.8660 (1.7864)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8371 (6.5623)  time: 0.8595 (0.5189 -- 4.2196)  data: 0.3104 (0.0002 -- 3.7017)  max mem: 16413
Epoch: [44]  [100/160]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000011  loss: 1.7075 (1.7729)  loss_scale: 32768.0000 (24008.2376)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3035 (6.5530)  time: 0.9075 (0.5154 -- 3.6990)  data: 0.3427 (0.0003 -- 3.1685)  max mem: 16413
Epoch: [44]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000011  loss: 1.8178 (1.7835)  loss_scale: 32768.0000 (25456.1322)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5277 (6.6358)  time: 0.7925 (0.5324 -- 3.3062)  data: 0.0017 (0.0005 -- 0.0043)  max mem: 16413
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.7863 (1.7944)  loss_scale: 32768.0000 (26493.2766)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6427 (6.7010)  time: 0.9930 (0.5165 -- 5.7541)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.9105 (1.8068)  loss_scale: 32768.0000 (27238.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5344 (6.7408)  time: 0.5943 (0.4940 -- 1.4787)  data: 0.0008 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [44] Total time: 0:02:23 (0.8955 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.9105 (1.8288)  loss_scale: 32768.0000 (27238.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5344 (6.7408)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2724 (0.2724)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2348 (2.2348 -- 2.2348)  data: 2.0216 (2.0216 -- 2.0216)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4632 (0.7333)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4066 (0.2030 -- 2.2348)  data: 0.1941 (0.0007 -- 2.0216)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5016 (0.6705)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (97.3545)  time: 0.2243 (0.1700 -- 0.4755)  data: 0.0207 (0.0001 -- 0.2895)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6033 (0.7526)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2091 (0.1327 -- 0.4755)  data: 0.0197 (0.0001 -- 0.2895)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 80.083 Acc@5 96.266 loss 0.722
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.71%
Epoch: [45]  [  0/160]  eta: 0:19:11  lr: 0.000042  min_lr: 0.000011  loss: 1.9351 (1.9351)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 9.2577 (9.2577)  time: 7.1949 (7.1949 -- 7.1949)  data: 5.1570 (5.1570 -- 5.1570)  max mem: 16413
Epoch: [45]  [ 20/160]  eta: 0:02:38  lr: 0.000042  min_lr: 0.000011  loss: 1.9637 (1.8348)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5018 (7.1071)  time: 0.8320 (0.5322 -- 3.0201)  data: 0.0685 (0.0005 -- 1.1528)  max mem: 16413
[2023-09-04 13:16:54,619] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:16:54,620] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:16:54,621] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:16:54,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:17:08,940] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7237
[2023-09-04 13:17:08,940] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7237
[2023-09-04 13:17:08,940] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:17:08,940] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:17:08,940] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [45]  [ 40/160]  eta: 0:01:59  lr: 0.000042  min_lr: 0.000011  loss: 1.9513 (1.8598)  loss_scale: 65536.0000 (44756.2927)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3299 (6.9194)  time: 0.8514 (0.5149 -- 2.9883)  data: 0.1723 (0.0004 -- 1.6830)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:36  lr: 0.000042  min_lr: 0.000011  loss: 1.8447 (1.8425)  loss_scale: 32768.0000 (40825.7049)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0700 (6.7660)  time: 0.8974 (0.5282 -- 3.4999)  data: 0.0957 (0.0007 -- 1.5316)  max mem: 16413
Epoch: [45]  [ 80/160]  eta: 0:01:13  lr: 0.000042  min_lr: 0.000011  loss: 1.8567 (1.8293)  loss_scale: 32768.0000 (38836.1481)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7032 (7.0372)  time: 0.8042 (0.5275 -- 3.3740)  data: 0.0942 (0.0003 -- 1.5784)  max mem: 16413
Epoch: [45]  [100/160]  eta: 0:00:58  lr: 0.000042  min_lr: 0.000011  loss: 1.9282 (1.8494)  loss_scale: 32768.0000 (37634.5347)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7742 (6.9202)  time: 1.1417 (0.5187 -- 5.2014)  data: 0.0830 (0.0005 -- 1.4539)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000011  loss: 1.7683 (1.8472)  loss_scale: 32768.0000 (36830.1488)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6453 (6.8779)  time: 0.8384 (0.5074 -- 3.8752)  data: 0.0011 (0.0003 -- 0.0019)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.8707 (1.8442)  loss_scale: 32768.0000 (36253.9574)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2829 (6.8905)  time: 0.7807 (0.5128 -- 3.1113)  data: 0.0028 (0.0006 -- 0.0128)  max mem: 16413
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.9109 (1.8404)  loss_scale: 32768.0000 (35840.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1654 (6.8632)  time: 0.6633 (0.4947 -- 2.3521)  data: 0.0010 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [45] Total time: 0:02:22 (0.8929 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.9109 (1.8013)  loss_scale: 32768.0000 (35840.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1654 (6.8632)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2826 (0.2826)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5123 (2.5123 -- 2.5123)  data: 2.2857 (2.2857 -- 2.2857)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4835 (0.7253)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4362 (0.1970 -- 2.5123)  data: 0.2224 (0.0005 -- 2.2857)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5908 (0.7052)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2125 (0.1695 -- 0.3579)  data: 0.0090 (0.0001 -- 0.1499)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7554 (0.7647)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.6805)  time: 0.1989 (0.1323 -- 0.3579)  data: 0.0087 (0.0001 -- 0.1499)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 79.253 Acc@5 96.473 loss 0.733
Accuracy of the network on the 482 val images: 79.25%
Max accuracy: 80.71%
Epoch: [46]  [  0/160]  eta: 0:17:12  lr: 0.000042  min_lr: 0.000011  loss: 2.1740 (2.1740)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0505 (6.0505)  time: 6.4508 (6.4508 -- 6.4508)  data: 5.9397 (5.9397 -- 5.9397)  max mem: 16413
[2023-09-04 13:19:11,039] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:19:11,039] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:19:11,040] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:19:11,040] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:19:15,183] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7369
[2023-09-04 13:19:15,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:19:15,184] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7369
[2023-09-04 13:19:15,184] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:19:15,184] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [46]  [ 20/160]  eta: 0:02:54  lr: 0.000042  min_lr: 0.000011  loss: 1.7908 (1.8319)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7362 (6.1280)  time: 0.9831 (0.5238 -- 3.4760)  data: 0.4153 (0.0004 -- 2.9410)  max mem: 16413
Epoch: [46]  [ 40/160]  eta: 0:02:05  lr: 0.000042  min_lr: 0.000011  loss: 1.6892 (1.8220)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.3311 (6.2558)  time: 0.8321 (0.5235 -- 4.2202)  data: 0.2853 (0.0003 -- 3.6785)  max mem: 16413
Epoch: [46]  [ 60/160]  eta: 0:01:40  lr: 0.000042  min_lr: 0.000011  loss: 1.9165 (1.8361)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9583 (6.2699)  time: 0.9148 (0.5345 -- 3.2112)  data: 0.2710 (0.0007 -- 2.6770)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000011  loss: 1.7396 (1.8070)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9256 (6.4619)  time: 0.7583 (0.5271 -- 2.4085)  data: 0.1478 (0.0007 -- 1.8816)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:55  lr: 0.000042  min_lr: 0.000011  loss: 1.7059 (1.7911)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4172 (6.4709)  time: 0.9000 (0.5283 -- 2.7540)  data: 0.2894 (0.0005 -- 2.2085)  max mem: 16413
[2023-09-04 13:20:38,188] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7465
[2023-09-04 13:20:38,188] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7465
[2023-09-04 13:20:38,188] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:20:38,188] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:20:38,188] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [46]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 2.0303 (1.8214)  loss_scale: 16384.0000 (31413.9504)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1137 (6.4329)  time: 0.8441 (0.5154 -- 3.2516)  data: 0.2469 (0.0003 -- 2.7095)  max mem: 16413
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.6816 (1.8178)  loss_scale: 16384.0000 (29282.0426)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5082 (6.4866)  time: 0.9552 (0.5130 -- 2.1924)  data: 0.1874 (0.0002 -- 1.4699)  max mem: 16413
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.9554 (1.8184)  loss_scale: 16384.0000 (27750.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7064 (6.5005)  time: 0.6335 (0.4929 -- 2.7842)  data: 0.1140 (0.0002 -- 2.2616)  max mem: 16413
Epoch: [46] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.9554 (1.8276)  loss_scale: 16384.0000 (27750.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7064 (6.5005)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3186 (0.3186)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4019 (2.4019 -- 2.4019)  data: 2.1479 (2.1479 -- 2.1479)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4513 (0.7066)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4112 (0.1956 -- 2.4019)  data: 0.1977 (0.0005 -- 2.1479)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5605 (0.6652)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.3545)  time: 0.2174 (0.1715 -- 0.4496)  data: 0.0138 (0.0001 -- 0.2476)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6590 (0.7226)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.6805)  time: 0.2051 (0.1326 -- 0.4496)  data: 0.0136 (0.0001 -- 0.2476)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 78.838 Acc@5 97.303 loss 0.714
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 80.71%
Epoch: [47]  [  0/160]  eta: 0:20:44  lr: 0.000042  min_lr: 0.000011  loss: 1.7142 (1.7142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.4639 (8.4639)  time: 7.7757 (7.7757 -- 7.7757)  data: 6.5040 (6.5040 -- 6.5040)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:46  lr: 0.000042  min_lr: 0.000011  loss: 1.7930 (1.8011)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6283 (6.8116)  time: 0.8586 (0.5171 -- 4.1958)  data: 0.3057 (0.0003 -- 3.6646)  max mem: 16413
Epoch: [47]  [ 40/160]  eta: 0:02:08  lr: 0.000042  min_lr: 0.000011  loss: 1.8441 (1.7940)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4971 (7.0916)  time: 0.9402 (0.5142 -- 4.0660)  data: 0.3973 (0.0003 -- 3.5343)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:41  lr: 0.000042  min_lr: 0.000011  loss: 1.6379 (1.7747)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2882 (6.9415)  time: 0.9177 (0.5208 -- 2.9530)  data: 0.2123 (0.0002 -- 2.4233)  max mem: 16413
[2023-09-04 13:22:43,108] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:22:43,108] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:22:43,108] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:22:43,108] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [47]  [ 80/160]  eta: 0:01:17  lr: 0.000042  min_lr: 0.000011  loss: 1.6802 (1.7651)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0352 (6.7799)  time: 0.8199 (0.5284 -- 3.0758)  data: 0.1799 (0.0002 -- 2.5220)  max mem: 16413
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.7771 (1.7666)  loss_scale: 32768.0000 (20763.8812)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3517 (6.7807)  time: 0.8687 (0.5282 -- 4.2537)  data: 0.3212 (0.0004 -- 3.7276)  max mem: 16413
Epoch: [47]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000011  loss: 1.9068 (1.7841)  loss_scale: 32768.0000 (22748.0331)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9058 (6.8364)  time: 0.9039 (0.5183 -- 4.7113)  data: 0.3676 (0.0003 -- 4.1811)  max mem: 16413
[2023-09-04 13:23:38,366] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7659
[2023-09-04 13:23:38,366] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7659
[2023-09-04 13:23:38,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:23:38,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:23:38,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.8446 (1.7924)  loss_scale: 32768.0000 (23936.9078)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6185 (6.8439)  time: 0.7274 (0.5311 -- 2.7221)  data: 0.1743 (0.0005 -- 2.1952)  max mem: 16413
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000011  loss: 1.8786 (1.7832)  loss_scale: 16384.0000 (23040.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5658 (6.9626)  time: 0.7264 (0.4943 -- 3.5258)  data: 0.2052 (0.0002 -- 2.9992)  max mem: 16413
Epoch: [47] Total time: 0:02:22 (0.8907 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000011  loss: 1.8786 (1.7996)  loss_scale: 16384.0000 (23040.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5658 (6.9626)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2851 (0.2851)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3411 (2.3411 -- 2.3411)  data: 2.1388 (2.1388 -- 2.1388)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6365 (0.7955)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (97.9798)  time: 0.4136 (0.1954 -- 2.3411)  data: 0.1957 (0.0008 -- 2.1388)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5626 (0.7145)  acc1: 77.7778 (78.3069)  acc5: 100.0000 (97.3545)  time: 0.2199 (0.1697 -- 0.3507)  data: 0.0090 (0.0001 -- 0.1618)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6372 (0.7676)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.6805)  time: 0.2021 (0.1327 -- 0.3507)  data: 0.0085 (0.0001 -- 0.1618)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 78.631 Acc@5 96.680 loss 0.730
Accuracy of the network on the 482 val images: 78.63%
Max accuracy: 80.71%
Epoch: [48]  [  0/160]  eta: 0:17:59  lr: 0.000041  min_lr: 0.000011  loss: 1.5840 (1.5840)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9085 (5.9085)  time: 6.7485 (6.7485 -- 6.7485)  data: 6.2264 (6.2264 -- 6.2264)  max mem: 16413
Epoch: [48]  [ 20/160]  eta: 0:02:42  lr: 0.000041  min_lr: 0.000011  loss: 1.8565 (1.8694)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4114 (6.5431)  time: 0.8828 (0.5244 -- 3.3829)  data: 0.3323 (0.0003 -- 2.8533)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:01  lr: 0.000041  min_lr: 0.000011  loss: 1.6827 (1.7757)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5395 (6.4458)  time: 0.8470 (0.5291 -- 2.9299)  data: 0.2042 (0.0002 -- 2.4074)  max mem: 16413
Epoch: [48]  [ 60/160]  eta: 0:01:41  lr: 0.000041  min_lr: 0.000011  loss: 1.8422 (1.7907)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3127 (6.5736)  time: 1.0189 (0.5255 -- 4.1908)  data: 0.0599 (0.0005 -- 1.1609)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000011  loss: 1.7375 (1.7980)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5315 (6.6615)  time: 0.8217 (0.5256 -- 3.0640)  data: 0.0030 (0.0003 -- 0.0248)  max mem: 16413
Epoch: [48]  [100/160]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000011  loss: 1.7767 (1.7873)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3115 (6.6759)  time: 0.8271 (0.5296 -- 2.3911)  data: 0.1232 (0.0003 -- 1.2735)  max mem: 16413
[2023-09-04 13:25:42,316] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:25:42,316] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:25:42,317] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:25:42,317] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [48]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.9397 (1.8022)  loss_scale: 32768.0000 (18144.2645)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8988 (6.6745)  time: 0.8550 (0.5262 -- 3.1645)  data: 0.0023 (0.0005 -- 0.0137)  max mem: 16413
Epoch: [48]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.8136 (1.8121)  loss_scale: 32768.0000 (20218.5532)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2394 (6.6619)  time: 0.8558 (0.5352 -- 3.0503)  data: 0.0019 (0.0003 -- 0.0081)  max mem: 16413
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.7740 (1.8002)  loss_scale: 32768.0000 (21708.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1676 (6.7159)  time: 0.8625 (0.4945 -- 3.1116)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [48] Total time: 0:02:23 (0.8947 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.7740 (1.7849)  loss_scale: 32768.0000 (21708.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1676 (6.7159)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2444 (0.2444)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4554 (2.4554 -- 2.4554)  data: 2.2070 (2.2070 -- 2.2070)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5547 (0.7462)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (97.9798)  time: 0.4210 (0.1972 -- 2.4554)  data: 0.2041 (0.0009 -- 2.2070)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5547 (0.6889)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (97.3545)  time: 0.2128 (0.1713 -- 0.2883)  data: 0.0077 (0.0001 -- 0.0916)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6346 (0.7582)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.6805)  time: 0.1981 (0.1330 -- 0.2883)  data: 0.0072 (0.0001 -- 0.0916)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 78.423 Acc@5 96.680 loss 0.731
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 80.71%
Epoch: [49]  [  0/160]  eta: 0:17:38  lr: 0.000041  min_lr: 0.000010  loss: 2.2351 (2.2351)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.0767 (8.0767)  time: 6.6165 (6.6165 -- 6.6165)  data: 5.7831 (5.7831 -- 5.7831)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:45  lr: 0.000041  min_lr: 0.000010  loss: 1.8007 (1.8283)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6523 (6.7210)  time: 0.9076 (0.5245 -- 2.4904)  data: 0.3339 (0.0003 -- 1.9599)  max mem: 16413
Epoch: [49]  [ 40/160]  eta: 0:02:07  lr: 0.000041  min_lr: 0.000010  loss: 1.8931 (1.8554)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3284 (6.5936)  time: 0.9353 (0.5229 -- 3.3659)  data: 0.3901 (0.0002 -- 2.8201)  max mem: 16413
Epoch: [49]  [ 60/160]  eta: 0:01:39  lr: 0.000041  min_lr: 0.000010  loss: 1.8010 (1.8418)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3616 (6.5490)  time: 0.8478 (0.5174 -- 4.6464)  data: 0.2957 (0.0002 -- 4.0849)  max mem: 16413
[2023-09-04 13:27:45,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:27:45,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:27:45,554] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:27:45,554] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [49]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000010  loss: 1.9191 (1.8477)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2393 (6.6856)  time: 0.9253 (0.5199 -- 2.8689)  data: 0.1423 (0.0004 -- 1.9958)  max mem: 16413
[2023-09-04 13:28:07,745] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7939
[2023-09-04 13:28:07,745] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7939
[2023-09-04 13:28:07,745] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:28:07,745] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:28:07,745] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [49]  [100/160]  eta: 0:00:57  lr: 0.000041  min_lr: 0.000010  loss: 1.8621 (1.8401)  loss_scale: 65536.0000 (40230.0198)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9062 (6.6574)  time: 0.8925 (0.5188 -- 2.7295)  data: 0.1526 (0.0001 -- 1.9952)  max mem: 16413
Epoch: [49]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000010  loss: 1.9152 (1.8350)  loss_scale: 32768.0000 (38996.6281)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4034 (6.6673)  time: 0.8391 (0.5137 -- 3.1256)  data: 0.1969 (0.0002 -- 2.6113)  max mem: 16413
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.6786 (1.8200)  loss_scale: 32768.0000 (38113.1348)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4044 (6.6907)  time: 0.7701 (0.5224 -- 3.4345)  data: 0.2041 (0.0002 -- 2.9221)  max mem: 16413
[2023-09-04 13:28:54,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[1.0418644962272174e-05, 1.0418644962272174e-05, 1.1576272180302416e-05, 1.1576272180302416e-05, 1.286252464478046e-05, 1.286252464478046e-05, 1.4291694049756068e-05, 1.4291694049756068e-05, 1.587966005528452e-05, 1.587966005528452e-05, 1.764406672809391e-05, 1.764406672809391e-05, 1.960451858677101e-05, 1.960451858677101e-05, 2.1782798429745566e-05, 2.1782798429745566e-05, 2.4203109366383962e-05, 2.4203109366383962e-05, 2.689234374042662e-05, 2.689234374042662e-05, 2.988038193380736e-05, 2.988038193380736e-05, 3.3200424370897066e-05, 3.3200424370897066e-05, 3.688936041210785e-05, 3.688936041210785e-05, 4.0988178235675386e-05, 4.0988178235675386e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 13:28:54,452] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=17.598717983370204, CurrSamplesPerSec=24.82912860159803, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.6971 (1.8067)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3513 (6.7115)  time: 0.7261 (0.4960 -- 3.3911)  data: 0.1525 (0.0002 -- 2.8304)  max mem: 16413
Epoch: [49] Total time: 0:02:22 (0.8936 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.6971 (1.8184)  loss_scale: 32768.0000 (37478.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3513 (6.7115)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3369 (0.3369)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4673 (2.4673 -- 2.4673)  data: 2.2222 (2.2222 -- 2.2222)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5194 (0.7441)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4127 (0.1947 -- 2.4673)  data: 0.2029 (0.0005 -- 2.2222)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5939 (0.6996)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2067 (0.1688 -- 0.3043)  data: 0.0055 (0.0001 -- 0.0978)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6423 (0.7423)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.1929 (0.1333 -- 0.3043)  data: 0.0053 (0.0001 -- 0.0978)  max mem: 16413
Val: Total time: 0:00:07 (0.2810 s / it)
* Acc@1 80.705 Acc@5 96.266 loss 0.747
Accuracy of the network on the 482 val images: 80.71%
[2023-09-04 13:29:02,317] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 13:29:02,319] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 13:29:02,319] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 13:29:02,319] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 13:29:03,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 13:29:03,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.71%
Epoch: [50]  [  0/160]  eta: 0:18:39  lr: 0.000041  min_lr: 0.000010  loss: 1.8975 (1.8975)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3521 (6.3521)  time: 6.9950 (6.9950 -- 6.9950)  data: 6.1720 (6.1720 -- 6.1720)  max mem: 16413
Epoch: [50]  [ 20/160]  eta: 0:02:49  lr: 0.000041  min_lr: 0.000010  loss: 1.6301 (1.6493)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8196 (6.6973)  time: 0.9215 (0.5317 -- 3.2703)  data: 0.3069 (0.0002 -- 2.7348)  max mem: 16413
Epoch: [50]  [ 40/160]  eta: 0:02:02  lr: 0.000041  min_lr: 0.000010  loss: 1.6420 (1.6883)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3145 (6.6735)  time: 0.8296 (0.5158 -- 3.2389)  data: 0.2817 (0.0005 -- 2.6890)  max mem: 16413
Epoch: [50]  [ 60/160]  eta: 0:01:38  lr: 0.000041  min_lr: 0.000010  loss: 1.7785 (1.7004)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0348 (6.5481)  time: 0.9130 (0.5259 -- 3.2285)  data: 0.3138 (0.0002 -- 2.7117)  max mem: 16413
[2023-09-04 13:30:09,800] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:30:09,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:30:09,802] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:30:09,802] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:30:17,413] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8076
[2023-09-04 13:30:17,413] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8076
[2023-09-04 13:30:17,413] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:30:17,413] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:30:17,414] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [50]  [ 80/160]  eta: 0:01:15  lr: 0.000041  min_lr: 0.000010  loss: 2.0009 (1.7279)  loss_scale: 32768.0000 (36004.3457)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8439 (6.8012)  time: 0.7971 (0.5295 -- 1.8234)  data: 0.0704 (0.0003 -- 0.9975)  max mem: 16413
Epoch: [50]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000010  loss: 1.7243 (1.7361)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3150 (6.8967)  time: 0.8810 (0.5232 -- 2.8271)  data: 0.1175 (0.0006 -- 2.3050)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.7173 (1.7407)  loss_scale: 32768.0000 (34934.4793)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6459 (6.9581)  time: 0.8260 (0.5278 -- 3.2335)  data: 0.2388 (0.0004 -- 2.7179)  max mem: 16413
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.7667 (1.7541)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.9956 (7.0952)  time: 0.9481 (0.5287 -- 3.4269)  data: 0.2590 (0.0001 -- 2.9142)  max mem: 16413
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.8024 (1.7556)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4445 (7.1363)  time: 0.6746 (0.4949 -- 2.8086)  data: 0.0037 (0.0002 -- 0.0585)  max mem: 16413
Epoch: [50] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.8024 (1.7813)  loss_scale: 32768.0000 (34406.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4445 (7.1363)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3002 (0.3002)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2986 (2.2986 -- 2.2986)  data: 2.0848 (2.0848 -- 2.0848)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5431 (0.7403)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4142 (0.2001 -- 2.2986)  data: 0.1940 (0.0011 -- 2.0848)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5431 (0.6759)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2164 (0.1698 -- 0.3191)  data: 0.0090 (0.0001 -- 0.1263)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6233 (0.7335)  acc1: 85.7143 (78.4232)  acc5: 100.0000 (96.2656)  time: 0.1980 (0.1325 -- 0.3191)  data: 0.0085 (0.0001 -- 0.1263)  max mem: 16413
Val: Total time: 0:00:07 (0.2819 s / it)
* Acc@1 79.876 Acc@5 96.473 loss 0.712
Accuracy of the network on the 482 val images: 79.88%
Max accuracy: 80.71%
Epoch: [51]  [  0/160]  eta: 0:18:55  lr: 0.000041  min_lr: 0.000010  loss: 1.9755 (1.9755)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2558 (6.2558)  time: 7.0946 (7.0946 -- 7.0946)  data: 6.4447 (6.4447 -- 6.4447)  max mem: 16413
Epoch: [51]  [ 20/160]  eta: 0:02:46  lr: 0.000041  min_lr: 0.000010  loss: 1.7402 (1.7907)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9046 (6.4609)  time: 0.8950 (0.5358 -- 2.4940)  data: 0.0965 (0.0006 -- 1.8957)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:02:00  lr: 0.000041  min_lr: 0.000010  loss: 1.7579 (1.7699)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4328 (7.0704)  time: 0.8076 (0.5340 -- 3.5731)  data: 0.0345 (0.0008 -- 0.4447)  max mem: 16413
[2023-09-04 13:32:16,920] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8202
[2023-09-04 13:32:16,920] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8202
[2023-09-04 13:32:16,920] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:32:16,920] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:32:16,920] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [51]  [ 60/160]  eta: 0:01:34  lr: 0.000041  min_lr: 0.000010  loss: 1.5924 (1.7626)  loss_scale: 16384.0000 (27664.7869)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0818 (7.1878)  time: 0.8313 (0.5092 -- 2.7945)  data: 0.0624 (0.0008 -- 1.2107)  max mem: 16413
Epoch: [51]  [ 80/160]  eta: 0:01:14  lr: 0.000041  min_lr: 0.000010  loss: 1.7942 (1.7633)  loss_scale: 16384.0000 (24879.4074)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3050 (7.0355)  time: 0.8652 (0.5264 -- 2.3941)  data: 0.1039 (0.0004 -- 0.9476)  max mem: 16413
Epoch: [51]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000010  loss: 1.7285 (1.7686)  loss_scale: 16384.0000 (23197.1485)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9691 (6.8412)  time: 0.9467 (0.5371 -- 2.7541)  data: 0.0281 (0.0003 -- 0.5081)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.9100 (1.7871)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5011 (6.7445)  time: 0.8203 (0.5330 -- 3.1113)  data: 0.0401 (0.0007 -- 0.3842)  max mem: 16413
Epoch: [51]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.8047 (1.7940)  loss_scale: 16384.0000 (21264.3404)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2826 (6.7272)  time: 0.9316 (0.5242 -- 3.3796)  data: 0.0022 (0.0008 -- 0.0050)  max mem: 16413
[2023-09-04 13:33:43,877] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8301
[2023-09-04 13:33:43,877] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 13:33:43,878] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8301
[2023-09-04 13:33:43,878] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 13:33:43,878] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.8769 (1.8000)  loss_scale: 8192.0000 (19712.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5596 (6.8095)  time: 0.7124 (0.4933 -- 1.6973)  data: 0.0699 (0.0001 -- 0.9621)  max mem: 16413
Epoch: [51] Total time: 0:02:22 (0.8925 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.8769 (1.8019)  loss_scale: 8192.0000 (19712.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5596 (6.8095)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2680 (0.2680)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4180 (2.4180 -- 2.4180)  data: 2.1874 (2.1874 -- 2.1874)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5389 (0.6976)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4116 (0.1844 -- 2.4180)  data: 0.1998 (0.0005 -- 2.1874)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5168 (0.6346)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2118 (0.1688 -- 0.3650)  data: 0.0091 (0.0001 -- 0.1359)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6601 (0.7113)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.1979 (0.1329 -- 0.3650)  data: 0.0089 (0.0001 -- 0.1359)  max mem: 16413
Val: Total time: 0:00:07 (0.2830 s / it)
* Acc@1 80.083 Acc@5 96.473 loss 0.695
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.71%
Epoch: [52]  [  0/160]  eta: 0:19:33  lr: 0.000040  min_lr: 0.000010  loss: 1.7542 (1.7542)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4299 (6.4299)  time: 7.3353 (7.3353 -- 7.3353)  data: 6.7780 (6.7780 -- 6.7780)  max mem: 16413
Epoch: [52]  [ 20/160]  eta: 0:02:33  lr: 0.000040  min_lr: 0.000010  loss: 1.5814 (1.6300)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9411 (6.4826)  time: 0.7811 (0.5256 -- 2.4173)  data: 0.0955 (0.0005 -- 1.2437)  max mem: 16413
Epoch: [52]  [ 40/160]  eta: 0:02:00  lr: 0.000040  min_lr: 0.000010  loss: 1.7351 (1.6906)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2309 (6.4122)  time: 0.9149 (0.5210 -- 2.4009)  data: 0.0287 (0.0002 -- 0.3791)  max mem: 16413
Epoch: [52]  [ 60/160]  eta: 0:01:37  lr: 0.000040  min_lr: 0.000010  loss: 1.8807 (1.7711)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1011 (6.6294)  time: 0.8989 (0.5336 -- 2.5796)  data: 0.0252 (0.0008 -- 0.4513)  max mem: 16413
Epoch: [52]  [ 80/160]  eta: 0:01:15  lr: 0.000040  min_lr: 0.000010  loss: 1.8122 (1.7783)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3332 (6.5795)  time: 0.8737 (0.5260 -- 2.3443)  data: 0.0650 (0.0004 -- 1.2674)  max mem: 16413
Epoch: [52]  [100/160]  eta: 0:00:57  lr: 0.000040  min_lr: 0.000010  loss: 1.8621 (1.7904)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1031 (6.4981)  time: 0.9653 (0.5249 -- 3.7820)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16413
[2023-09-04 13:35:47,829] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:35:47,829] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:35:47,829] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 13:35:47,829] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [52]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000010  loss: 1.7402 (1.7916)  loss_scale: 16384.0000 (8936.7273)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7709 (6.4586)  time: 0.8526 (0.5057 -- 3.9384)  data: 0.0012 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.7969 (1.7942)  loss_scale: 16384.0000 (9993.0780)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1275 (6.4692)  time: 0.9563 (0.5300 -- 4.0331)  data: 0.0014 (0.0003 -- 0.0053)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.7857 (1.7915)  loss_scale: 16384.0000 (10752.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4372 (6.4791)  time: 0.6379 (0.4924 -- 2.9006)  data: 0.0007 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [52] Total time: 0:02:24 (0.9024 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.7857 (1.8071)  loss_scale: 16384.0000 (10752.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4372 (6.4791)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2366 (0.2366)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4855 (2.4855 -- 2.4855)  data: 2.2401 (2.2401 -- 2.2401)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4356 (0.6789)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4272 (0.2027 -- 2.4855)  data: 0.2097 (0.0005 -- 2.2401)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5180 (0.6310)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2111 (0.1692 -- 0.2815)  data: 0.0067 (0.0001 -- 0.0582)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6317 (0.6990)  acc1: 85.7143 (81.3278)  acc5: 100.0000 (96.2656)  time: 0.1955 (0.1327 -- 0.2815)  data: 0.0062 (0.0001 -- 0.0582)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 82.780 Acc@5 96.888 loss 0.688
Accuracy of the network on the 482 val images: 82.78%
[2023-09-04 13:36:36,784] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 13:36:36,786] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 13:36:36,786] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 13:36:36,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 13:36:38,271] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 13:36:38,271] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.78%
Epoch: [53]  [  0/160]  eta: 0:20:50  lr: 0.000040  min_lr: 0.000010  loss: 2.0022 (2.0022)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.8175 (4.8175)  time: 7.8169 (7.8169 -- 7.8169)  data: 7.2451 (7.2451 -- 7.2451)  max mem: 16413
Epoch: [53]  [ 20/160]  eta: 0:02:40  lr: 0.000040  min_lr: 0.000010  loss: 1.7192 (1.7867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8739 (6.9458)  time: 0.8141 (0.5262 -- 3.3289)  data: 0.2699 (0.0004 -- 2.8119)  max mem: 16413
Epoch: [53]  [ 40/160]  eta: 0:02:11  lr: 0.000040  min_lr: 0.000010  loss: 1.6768 (1.7384)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5049 (6.9799)  time: 1.0405 (0.5133 -- 4.6341)  data: 0.4990 (0.0003 -- 4.1089)  max mem: 16413
Epoch: [53]  [ 60/160]  eta: 0:01:37  lr: 0.000040  min_lr: 0.000010  loss: 1.9638 (1.8208)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3295 (6.8818)  time: 0.7378 (0.5150 -- 2.8425)  data: 0.1830 (0.0003 -- 2.3156)  max mem: 16413
[2023-09-04 13:37:51,345] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:37:51,346] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:37:51,387] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:37:51,387] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [53]  [ 80/160]  eta: 0:01:13  lr: 0.000040  min_lr: 0.000010  loss: 1.6386 (1.7935)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2331 (6.9431)  time: 0.7258 (0.5287 -- 2.4026)  data: 0.0950 (0.0004 -- 1.8634)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:57  lr: 0.000040  min_lr: 0.000010  loss: 1.9348 (1.8192)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6573 (6.9330)  time: 1.0896 (0.5400 -- 4.3554)  data: 0.1167 (0.0005 -- 1.2992)  max mem: 16413
[2023-09-04 13:38:14,808] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8581
[2023-09-04 13:38:14,808] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8581
[2023-09-04 13:38:14,808] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:38:14,808] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:38:14,808] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [53]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000010  loss: 1.8429 (1.8155)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3242 (6.8461)  time: 0.8322 (0.5265 -- 3.9035)  data: 0.0012 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.6615 (1.8018)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1057 (6.9492)  time: 0.8792 (0.5206 -- 4.5303)  data: 0.0022 (0.0004 -- 0.0172)  max mem: 16413
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.9342 (1.7998)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0947 (7.0412)  time: 0.7139 (0.4964 -- 3.8384)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [53] Total time: 0:02:23 (0.8999 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.9342 (1.7800)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0947 (7.0412)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3725 (0.3725)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5983 (2.5983 -- 2.5983)  data: 2.3612 (2.3612 -- 2.3612)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5499 (0.7374)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4335 (0.2008 -- 2.5983)  data: 0.2164 (0.0005 -- 2.3612)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4946 (0.6546)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2080 (0.1687 -- 0.2407)  data: 0.0018 (0.0001 -- 0.0139)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5673 (0.7268)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.8506)  time: 0.1929 (0.1328 -- 0.2407)  data: 0.0015 (0.0001 -- 0.0139)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 79.668 Acc@5 96.058 loss 0.713
Accuracy of the network on the 482 val images: 79.67%
Max accuracy: 82.78%
Epoch: [54]  [  0/160]  eta: 0:18:57  lr: 0.000040  min_lr: 0.000010  loss: 2.3445 (2.3445)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1767 (6.1767)  time: 7.1077 (7.1077 -- 7.1077)  data: 5.6143 (5.6143 -- 5.6143)  max mem: 16413
Epoch: [54]  [ 20/160]  eta: 0:02:46  lr: 0.000040  min_lr: 0.000010  loss: 1.6939 (1.7008)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3745 (6.9377)  time: 0.8910 (0.5321 -- 2.8987)  data: 0.1907 (0.0004 -- 1.6698)  max mem: 16413
Epoch: [54]  [ 40/160]  eta: 0:01:59  lr: 0.000040  min_lr: 0.000010  loss: 1.6851 (1.7298)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4716 (6.9039)  time: 0.7917 (0.5251 -- 1.7812)  data: 0.1891 (0.0005 -- 1.2515)  max mem: 16413
Epoch: [54]  [ 60/160]  eta: 0:01:37  lr: 0.000040  min_lr: 0.000010  loss: 1.6980 (1.7179)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5992 (7.0027)  time: 0.9308 (0.5133 -- 5.9944)  data: 0.3822 (0.0003 -- 5.4684)  max mem: 16413
[2023-09-04 13:40:18,230] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:40:18,230] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:40:18,231] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:40:18,231] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [54]  [ 80/160]  eta: 0:01:15  lr: 0.000040  min_lr: 0.000010  loss: 1.8691 (1.7589)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1557 (6.8005)  time: 0.8734 (0.5226 -- 3.8061)  data: 0.3209 (0.0004 -- 3.2493)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000010  loss: 1.8618 (1.7662)  loss_scale: 32768.0000 (21412.7525)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5113 (6.8265)  time: 0.9507 (0.5214 -- 3.2366)  data: 0.4097 (0.0006 -- 2.7057)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000010  loss: 1.9023 (1.7744)  loss_scale: 32768.0000 (23289.6529)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3963 (6.8040)  time: 0.8297 (0.5365 -- 2.4596)  data: 0.2823 (0.0004 -- 1.9323)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.8420 (1.7782)  loss_scale: 32768.0000 (24634.0993)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9042 (6.8569)  time: 0.7954 (0.5293 -- 2.7850)  data: 0.2498 (0.0005 -- 2.2623)  max mem: 16413
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.8068 (1.7744)  loss_scale: 32768.0000 (25600.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0045 (6.8031)  time: 0.6938 (0.4953 -- 3.2531)  data: 0.1724 (0.0002 -- 2.6937)  max mem: 16413
Epoch: [54] Total time: 0:02:21 (0.8857 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.8068 (1.7733)  loss_scale: 32768.0000 (25600.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0045 (6.8031)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2442 (0.2442)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1651 (2.1651 -- 2.1651)  data: 1.9636 (1.9636 -- 1.9636)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4332 (0.6479)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.3977 (0.2055 -- 2.1651)  data: 0.1836 (0.0005 -- 1.9636)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5068 (0.6200)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2243 (0.1688 -- 0.3713)  data: 0.0162 (0.0001 -- 0.1949)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5943 (0.6943)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2164 (0.1327 -- 0.3713)  data: 0.0219 (0.0001 -- 0.1949)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 80.705 Acc@5 96.680 loss 0.689
Accuracy of the network on the 482 val images: 80.71%
Max accuracy: 82.78%
Epoch: [55]  [  0/160]  eta: 0:21:57  lr: 0.000040  min_lr: 0.000010  loss: 1.3558 (1.3558)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.6204 (7.6204)  time: 8.2359 (8.2359 -- 8.2359)  data: 6.4412 (6.4412 -- 6.4412)  max mem: 16413
Epoch: [55]  [ 20/160]  eta: 0:02:47  lr: 0.000040  min_lr: 0.000010  loss: 1.7956 (1.8615)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3653 (7.4662)  time: 0.8423 (0.5169 -- 3.7349)  data: 0.1742 (0.0002 -- 2.0823)  max mem: 16413
[2023-09-04 13:42:18,645] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:42:18,646] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:42:18,647] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:42:18,648] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [55]  [ 40/160]  eta: 0:02:02  lr: 0.000040  min_lr: 0.000010  loss: 1.9371 (1.8895)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0605 (6.9267)  time: 0.8412 (0.5179 -- 2.9962)  data: 0.2573 (0.0003 -- 2.4438)  max mem: 16413
[2023-09-04 13:42:24,933] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8845
[2023-09-04 13:42:24,933] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8845
[2023-09-04 13:42:24,933] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:42:24,933] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:42:24,933] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [55]  [ 60/160]  eta: 0:01:36  lr: 0.000040  min_lr: 0.000010  loss: 1.7716 (1.8550)  loss_scale: 32768.0000 (36528.2623)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7027 (6.6705)  time: 0.8631 (0.5233 -- 3.1464)  data: 0.3193 (0.0004 -- 2.6113)  max mem: 16413
Epoch: [55]  [ 80/160]  eta: 0:01:15  lr: 0.000040  min_lr: 0.000010  loss: 1.7643 (1.8124)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2679 (6.7778)  time: 0.8721 (0.5167 -- 2.2689)  data: 0.3329 (0.0003 -- 1.7519)  max mem: 16413
[2023-09-04 13:43:09,745] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8896
[2023-09-04 13:43:09,746] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8896
[2023-09-04 13:43:09,746] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:43:09,746] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:43:09,746] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [55]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 1.8317 (1.8023)  loss_scale: 32768.0000 (34227.9604)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7066 (6.7925)  time: 0.8415 (0.5177 -- 1.8283)  data: 0.2960 (0.0003 -- 1.3062)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000010  loss: 1.8137 (1.8042)  loss_scale: 16384.0000 (31278.5455)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6681 (6.8368)  time: 0.8585 (0.5286 -- 2.7108)  data: 0.1899 (0.0005 -- 2.1745)  max mem: 16413
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 2.0378 (1.8174)  loss_scale: 16384.0000 (29165.8440)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4972 (6.8741)  time: 0.9100 (0.5296 -- 3.0151)  data: 0.1067 (0.0005 -- 2.0971)  max mem: 16413
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8326 (1.8299)  loss_scale: 16384.0000 (27648.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3552 (6.8434)  time: 0.6948 (0.4958 -- 2.5859)  data: 0.0163 (0.0003 -- 0.3107)  max mem: 16413
Epoch: [55] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8326 (1.8032)  loss_scale: 16384.0000 (27648.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3552 (6.8434)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3948 (0.3948)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4236 (2.4236 -- 2.4236)  data: 2.2106 (2.2106 -- 2.2106)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4373 (0.6821)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4105 (0.1927 -- 2.4236)  data: 0.2020 (0.0005 -- 2.2106)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4875 (0.6323)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2197 (0.1696 -- 0.5735)  data: 0.0205 (0.0001 -- 0.3958)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5945 (0.7040)  acc1: 85.7143 (80.4979)  acc5: 100.0000 (95.4357)  time: 0.2053 (0.1322 -- 0.5735)  data: 0.0202 (0.0001 -- 0.3958)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 82.158 Acc@5 96.058 loss 0.686
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.78%
Epoch: [56]  [  0/160]  eta: 0:17:44  lr: 0.000039  min_lr: 0.000010  loss: 2.1944 (2.1944)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 12.6658 (12.6658)  time: 6.6560 (6.6560 -- 6.6560)  data: 6.1159 (6.1159 -- 6.1159)  max mem: 16413
Epoch: [56]  [ 20/160]  eta: 0:02:45  lr: 0.000039  min_lr: 0.000010  loss: 1.7913 (1.8441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4646 (8.2344)  time: 0.9077 (0.5246 -- 2.6112)  data: 0.2499 (0.0006 -- 2.0789)  max mem: 16413
[2023-09-04 13:44:49,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[9.999403491949061e-06, 9.999403491949061e-06, 1.1110448324387846e-05, 1.1110448324387846e-05, 1.234494258265316e-05, 1.234494258265316e-05, 1.3716602869614624e-05, 1.3716602869614624e-05, 1.524066985512736e-05, 1.524066985512736e-05, 1.6934077616808177e-05, 1.6934077616808177e-05, 1.881564179645353e-05, 1.881564179645353e-05, 2.0906268662726143e-05, 2.0906268662726143e-05, 2.3229187403029046e-05, 2.3229187403029046e-05, 2.5810208225587826e-05, 2.5810208225587826e-05, 2.8678009139542034e-05, 2.8678009139542034e-05, 3.186445459949115e-05, 3.186445459949115e-05, 3.540494955499016e-05, 3.540494955499016e-05, 3.9338832838877956e-05, 3.9338832838877956e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 13:44:49,983] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=17.589077635419667, CurrSamplesPerSec=22.404472735366124, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:00  lr: 0.000039  min_lr: 0.000010  loss: 1.7526 (1.7623)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2483 (7.9977)  time: 0.8109 (0.5255 -- 1.8279)  data: 0.1933 (0.0004 -- 1.1597)  max mem: 16413
Epoch: [56]  [ 60/160]  eta: 0:01:38  lr: 0.000039  min_lr: 0.000010  loss: 1.7741 (1.7543)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5499 (7.6037)  time: 0.9500 (0.5152 -- 4.0804)  data: 0.3211 (0.0004 -- 3.5296)  max mem: 16413
[2023-09-04 13:45:13,492] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:45:13,492] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:45:13,495] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:45:13,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [56]  [ 80/160]  eta: 0:01:14  lr: 0.000039  min_lr: 0.000010  loss: 1.8613 (1.7778)  loss_scale: 32768.0000 (19620.3457)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2599 (7.6047)  time: 0.7724 (0.5236 -- 1.8989)  data: 0.1622 (0.0002 -- 1.3484)  max mem: 16413
[2023-09-04 13:45:30,899] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9045
[2023-09-04 13:45:30,900] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9045
[2023-09-04 13:45:30,900] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:45:30,900] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 13:45:30,900] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [56]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000010  loss: 1.6412 (1.7355)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5103 (7.4959)  time: 0.9399 (0.5350 -- 3.2277)  data: 0.0468 (0.0004 -- 0.9018)  max mem: 16413
Epoch: [56]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000010  loss: 1.8864 (1.7501)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4420 (7.4937)  time: 0.7723 (0.5255 -- 3.3139)  data: 0.1093 (0.0002 -- 1.6416)  max mem: 16413
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.7523 (1.7492)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8177 (7.4022)  time: 0.9157 (0.5288 -- 3.4125)  data: 0.2342 (0.0005 -- 2.8774)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8996 (1.7625)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6731 (7.3216)  time: 0.7537 (0.4942 -- 3.2859)  data: 0.0010 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [56] Total time: 0:02:22 (0.8910 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8996 (1.7598)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6731 (7.3216)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2681 (0.2681)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2867 (2.2867 -- 2.2867)  data: 2.0696 (2.0696 -- 2.0696)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5872 (0.7256)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4195 (0.1996 -- 2.2867)  data: 0.2076 (0.0006 -- 2.0696)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5519 (0.6689)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2245 (0.1696 -- 0.4581)  data: 0.0243 (0.0001 -- 0.2682)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5830 (0.7426)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2102 (0.1321 -- 0.4581)  data: 0.0237 (0.0001 -- 0.2682)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 80.290 Acc@5 96.473 loss 0.719
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 82.78%
Epoch: [57]  [  0/160]  eta: 0:21:13  lr: 0.000039  min_lr: 0.000010  loss: 2.1864 (2.1864)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4735 (6.4735)  time: 7.9599 (7.9599 -- 7.9599)  data: 7.4163 (7.4163 -- 7.4163)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:41  lr: 0.000039  min_lr: 0.000010  loss: 1.9818 (1.8756)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1084 (7.1971)  time: 0.8146 (0.5309 -- 2.5056)  data: 0.2667 (0.0006 -- 1.9555)  max mem: 16413
Epoch: [57]  [ 40/160]  eta: 0:02:15  lr: 0.000039  min_lr: 0.000010  loss: 1.5255 (1.7439)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5712 (6.7993)  time: 1.1038 (0.5272 -- 4.9244)  data: 0.5553 (0.0001 -- 4.4053)  max mem: 16413
[2023-09-04 13:47:34,718] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:47:34,718] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:47:34,722] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:47:34,723] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [ 60/160]  eta: 0:01:36  lr: 0.000039  min_lr: 0.000010  loss: 1.7363 (1.7418)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9755 (6.5382)  time: 0.6404 (0.5208 -- 1.4559)  data: 0.0922 (0.0006 -- 0.9401)  max mem: 16413
Epoch: [57]  [ 80/160]  eta: 0:01:17  lr: 0.000039  min_lr: 0.000010  loss: 1.6671 (1.7323)  loss_scale: 32768.0000 (21845.3333)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9122 (6.6340)  time: 0.9778 (0.5094 -- 3.2506)  data: 0.4350 (0.0007 -- 2.7488)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:57  lr: 0.000039  min_lr: 0.000010  loss: 1.8391 (1.7482)  loss_scale: 32768.0000 (24008.2376)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8277 (6.7692)  time: 0.8964 (0.5219 -- 2.3338)  data: 0.3528 (0.0002 -- 1.7968)  max mem: 16413
Epoch: [57]  [120/160]  eta: 0:00:38  lr: 0.000039  min_lr: 0.000010  loss: 1.7923 (1.7448)  loss_scale: 32768.0000 (25456.1322)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0569 (6.7664)  time: 0.9611 (0.5147 -- 4.0932)  data: 0.4228 (0.0003 -- 3.5376)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.9329 (1.7592)  loss_scale: 32768.0000 (26493.2766)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2885 (6.8687)  time: 0.7412 (0.5180 -- 2.1248)  data: 0.1075 (0.0005 -- 1.5719)  max mem: 16413
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8373 (1.7653)  loss_scale: 32768.0000 (27238.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4882 (6.8690)  time: 0.6807 (0.4970 -- 2.7676)  data: 0.0007 (0.0002 -- 0.0014)  max mem: 16413
Epoch: [57] Total time: 0:02:23 (0.8987 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8373 (1.7997)  loss_scale: 32768.0000 (27238.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4882 (6.8690)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3026 (0.3026)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4026 (2.4026 -- 2.4026)  data: 2.1951 (2.1951 -- 2.1951)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6036 (0.7717)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4139 (0.1966 -- 2.4026)  data: 0.2004 (0.0005 -- 2.1951)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5465 (0.6794)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2183 (0.1698 -- 0.4801)  data: 0.0151 (0.0001 -- 0.2905)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6087 (0.7383)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.0207)  time: 0.2052 (0.1328 -- 0.4801)  data: 0.0149 (0.0001 -- 0.2905)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 79.668 Acc@5 95.643 loss 0.720
Accuracy of the network on the 482 val images: 79.67%
Max accuracy: 82.78%
Epoch: [58]  [  0/160]  eta: 0:24:49  lr: 0.000039  min_lr: 0.000010  loss: 1.7070 (1.7070)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.5661 (8.5661)  time: 9.3117 (9.3117 -- 9.3117)  data: 6.9985 (6.9985 -- 6.9985)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:48  lr: 0.000039  min_lr: 0.000010  loss: 1.5469 (1.5992)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3320 (6.4781)  time: 0.7992 (0.5269 -- 3.4342)  data: 0.0015 (0.0004 -- 0.0048)  max mem: 16413
[2023-09-04 13:49:37,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:49:37,754] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:49:37,754] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:49:37,755] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:49:45,124] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9309
[2023-09-04 13:49:45,124] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:49:45,124] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9309
[2023-09-04 13:49:45,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 13:49:45,125] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [58]  [ 40/160]  eta: 0:02:08  lr: 0.000039  min_lr: 0.000010  loss: 1.7515 (1.6740)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3274 (6.5324)  time: 0.9248 (0.5281 -- 4.1328)  data: 0.3096 (0.0003 -- 2.2068)  max mem: 16413
[2023-09-04 13:50:04,900] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9333
[2023-09-04 13:50:04,900] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9333
[2023-09-04 13:50:04,900] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:50:04,900] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:50:04,901] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [ 60/160]  eta: 0:01:40  lr: 0.000039  min_lr: 0.000010  loss: 1.8242 (1.6873)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2784 (6.5025)  time: 0.8676 (0.5273 -- 4.3412)  data: 0.1733 (0.0008 -- 1.7238)  max mem: 16413
Epoch: [58]  [ 80/160]  eta: 0:01:18  lr: 0.000039  min_lr: 0.000010  loss: 1.7385 (1.6901)  loss_scale: 16384.0000 (29936.1975)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1276 (6.7705)  time: 0.9328 (0.5203 -- 3.5377)  data: 0.0775 (0.0003 -- 0.9844)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:57  lr: 0.000039  min_lr: 0.000010  loss: 2.0777 (1.7401)  loss_scale: 16384.0000 (27252.5941)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.8809 (7.0385)  time: 0.8349 (0.5170 -- 4.4390)  data: 0.0149 (0.0004 -- 0.2783)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.8545 (1.7736)  loss_scale: 16384.0000 (25456.1322)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9430 (6.9159)  time: 0.7796 (0.5230 -- 2.2205)  data: 0.0020 (0.0002 -- 0.0069)  max mem: 16413
Epoch: [58]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.9590 (1.7864)  loss_scale: 16384.0000 (24169.3050)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4443 (6.9061)  time: 0.8003 (0.5266 -- 2.6374)  data: 0.0346 (0.0001 -- 0.3371)  max mem: 16413
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8085 (1.7878)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0750 (6.8083)  time: 0.7711 (0.4953 -- 1.8976)  data: 0.0458 (0.0002 -- 0.5682)  max mem: 16413
Epoch: [58] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8085 (1.7898)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0750 (6.8083)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2481 (0.2481)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3882 (2.3882 -- 2.3882)  data: 2.1585 (2.1585 -- 2.1585)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5370 (0.7256)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4274 (0.1990 -- 2.3882)  data: 0.2103 (0.0004 -- 2.1585)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5370 (0.6613)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (95.7672)  time: 0.2175 (0.1693 -- 0.3615)  data: 0.0139 (0.0001 -- 0.1286)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6296 (0.7145)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.0207)  time: 0.2026 (0.1358 -- 0.3615)  data: 0.0135 (0.0001 -- 0.1286)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 80.498 Acc@5 95.851 loss 0.700
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 82.78%
Epoch: [59]  [  0/160]  eta: 0:19:45  lr: 0.000039  min_lr: 0.000010  loss: 1.4961 (1.4961)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0564 (6.0564)  time: 7.4099 (7.4099 -- 7.4099)  data: 6.4258 (6.4258 -- 6.4258)  max mem: 16413
Epoch: [59]  [ 20/160]  eta: 0:02:42  lr: 0.000039  min_lr: 0.000010  loss: 1.6430 (1.7438)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1634 (7.5527)  time: 0.8517 (0.5205 -- 3.2176)  data: 0.2252 (0.0006 -- 2.0719)  max mem: 16413
[2023-09-04 13:52:07,442] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:52:07,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:52:07,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:52:07,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [59]  [ 40/160]  eta: 0:02:05  lr: 0.000038  min_lr: 0.000010  loss: 1.8907 (1.8118)  loss_scale: 32768.0000 (23976.5854)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5694 (7.0077)  time: 0.9283 (0.5287 -- 3.2917)  data: 0.3543 (0.0004 -- 2.7584)  max mem: 16413
[2023-09-04 13:52:33,929] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9492
[2023-09-04 13:52:33,929] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9492
[2023-09-04 13:52:33,929] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:52:33,929] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:52:33,930] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [59]  [ 60/160]  eta: 0:01:36  lr: 0.000038  min_lr: 0.000010  loss: 1.8180 (1.8112)  loss_scale: 32768.0000 (24441.7049)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5948 (7.1135)  time: 0.7890 (0.5160 -- 2.3256)  data: 0.0648 (0.0003 -- 0.6681)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000010  loss: 1.6362 (1.7534)  loss_scale: 16384.0000 (22452.1481)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5739 (7.0883)  time: 0.8955 (0.5279 -- 4.7324)  data: 0.0243 (0.0005 -- 0.4623)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.9534 (1.7937)  loss_scale: 16384.0000 (21250.5347)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6507 (7.2396)  time: 0.9364 (0.5155 -- 4.0590)  data: 0.0551 (0.0002 -- 1.0636)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.8802 (1.7813)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9048 (7.3202)  time: 0.7717 (0.5294 -- 3.3541)  data: 0.1336 (0.0002 -- 2.1768)  max mem: 16413
Epoch: [59]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.6626 (1.7768)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4787 (7.2083)  time: 0.8985 (0.5252 -- 3.2411)  data: 0.2867 (0.0005 -- 2.7113)  max mem: 16413
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.5812 (1.7694)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.6824 (7.2407)  time: 0.7264 (0.4941 -- 2.1771)  data: 0.1669 (0.0002 -- 1.6829)  max mem: 16413
Epoch: [59] Total time: 0:02:22 (0.8928 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.5812 (1.7567)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.6824 (7.2407)
[2023-09-04 13:54:04,723] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-09-04 13:54:04,725] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-09-04 13:54:04,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-09-04 13:54:04,725] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-09-04 13:54:05,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-09-04 13:54:05,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2626 (0.2626)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3748 (2.3748 -- 2.3748)  data: 2.0937 (2.0937 -- 2.0937)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4033 (0.7332)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4291 (0.2026 -- 2.3748)  data: 0.2086 (0.0009 -- 2.0937)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4722 (0.6582)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2270 (0.1699 -- 0.4059)  data: 0.0205 (0.0001 -- 0.2060)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5809 (0.7198)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.2111 (0.1329 -- 0.4059)  data: 0.0201 (0.0001 -- 0.2060)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 81.328 Acc@5 95.851 loss 0.720
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 82.78%
Epoch: [60]  [  0/160]  eta: 0:23:46  lr: 0.000038  min_lr: 0.000010  loss: 1.9554 (1.9554)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 11.3638 (11.3638)  time: 8.9154 (8.9154 -- 8.9154)  data: 8.3696 (8.3696 -- 8.3696)  max mem: 16413
Epoch: [60]  [ 20/160]  eta: 0:02:54  lr: 0.000038  min_lr: 0.000010  loss: 1.6033 (1.6989)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1387 (6.8440)  time: 0.8632 (0.5220 -- 3.4634)  data: 0.3226 (0.0005 -- 2.9420)  max mem: 16413
[2023-09-04 13:54:40,276] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:54:40,277] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:54:40,280] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:54:40,281] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [60]  [ 40/160]  eta: 0:02:05  lr: 0.000038  min_lr: 0.000010  loss: 1.7281 (1.7248)  loss_scale: 32768.0000 (24376.1951)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0715 (6.7996)  time: 0.8347 (0.5232 -- 4.4297)  data: 0.2885 (0.0003 -- 3.9104)  max mem: 16413
Epoch: [60]  [ 60/160]  eta: 0:01:38  lr: 0.000038  min_lr: 0.000010  loss: 1.7882 (1.7457)  loss_scale: 32768.0000 (27127.6066)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3902 (6.8082)  time: 0.8458 (0.5340 -- 2.8484)  data: 0.2998 (0.0005 -- 2.3137)  max mem: 16413
Epoch: [60]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000010  loss: 1.8059 (1.7753)  loss_scale: 32768.0000 (28520.2963)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3345 (6.6738)  time: 0.8357 (0.5315 -- 4.1904)  data: 0.2817 (0.0004 -- 3.6472)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:55  lr: 0.000038  min_lr: 0.000010  loss: 1.7174 (1.7721)  loss_scale: 32768.0000 (29361.4257)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3817 (6.6277)  time: 0.8614 (0.5247 -- 4.1012)  data: 0.3129 (0.0004 -- 3.5980)  max mem: 16413
Epoch: [60]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.8095 (1.7769)  loss_scale: 32768.0000 (29924.4959)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9585 (6.7294)  time: 0.8952 (0.5373 -- 3.1494)  data: 0.3418 (0.0007 -- 2.6158)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.7956 (1.7839)  loss_scale: 32768.0000 (30327.8298)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9964 (6.8466)  time: 0.9106 (0.5228 -- 3.2873)  data: 0.2294 (0.0003 -- 2.1024)  max mem: 16413
[2023-09-04 13:56:31,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:56:31,569] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 13:56:31,572] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:56:31,572] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.6959 (1.7808)  loss_scale: 65536.0000 (32870.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3843 (6.7766)  time: 0.7341 (0.4964 -- 3.1099)  data: 0.0008 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [60] Total time: 0:02:23 (0.9000 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.6959 (1.7699)  loss_scale: 65536.0000 (32870.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3843 (6.7766)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1948 (0.1948)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3983 (2.3983 -- 2.3983)  data: 2.1554 (2.1554 -- 2.1554)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5346 (0.7495)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (95.9596)  time: 0.4333 (0.2079 -- 2.3983)  data: 0.2116 (0.0005 -- 2.1554)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5047 (0.6627)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2185 (0.1693 -- 0.3909)  data: 0.0150 (0.0001 -- 0.1544)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5346 (0.7227)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.4357)  time: 0.2027 (0.1328 -- 0.3909)  data: 0.0146 (0.0001 -- 0.1544)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 80.083 Acc@5 96.266 loss 0.702
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 82.78%
Epoch: [61]  [  0/160]  eta: 0:19:03  lr: 0.000038  min_lr: 0.000010  loss: 2.0953 (2.0953)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7196 (5.7196)  time: 7.1492 (7.1492 -- 7.1492)  data: 6.6072 (6.6072 -- 6.6072)  max mem: 16413
[2023-09-04 13:57:01,867] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9769
[2023-09-04 13:57:01,867] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:57:01,867] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9769
[2023-09-04 13:57:01,868] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 13:57:01,868] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [61]  [ 20/160]  eta: 0:02:54  lr: 0.000038  min_lr: 0.000010  loss: 1.6654 (1.7397)  loss_scale: 32768.0000 (46811.4286)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2868 (6.7751)  time: 0.9493 (0.5230 -- 4.9456)  data: 0.0980 (0.0004 -- 1.9273)  max mem: 16413
[2023-09-04 13:57:19,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9791
[2023-09-04 13:57:19,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9791
[2023-09-04 13:57:19,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:57:19,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 13:57:19,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [61]  [ 40/160]  eta: 0:02:12  lr: 0.000038  min_lr: 0.000010  loss: 1.6947 (1.7167)  loss_scale: 16384.0000 (35964.8780)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6304 (6.3277)  time: 0.9561 (0.5207 -- 2.9794)  data: 0.0016 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [61]  [ 60/160]  eta: 0:01:41  lr: 0.000038  min_lr: 0.000010  loss: 1.6361 (1.7240)  loss_scale: 16384.0000 (29544.9180)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3091 (6.6314)  time: 0.8180 (0.5304 -- 3.7033)  data: 0.0016 (0.0004 -- 0.0060)  max mem: 16413
Epoch: [61]  [ 80/160]  eta: 0:01:19  lr: 0.000038  min_lr: 0.000010  loss: 1.6479 (1.7044)  loss_scale: 16384.0000 (26295.3086)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5144 (6.5467)  time: 0.9428 (0.5226 -- 3.7285)  data: 0.0019 (0.0003 -- 0.0141)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.8297 (1.7222)  loss_scale: 16384.0000 (24332.6733)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6550 (6.5980)  time: 0.7434 (0.5271 -- 3.0685)  data: 0.0019 (0.0002 -- 0.0082)  max mem: 16413
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000038  min_lr: 0.000010  loss: 1.6617 (1.7312)  loss_scale: 16384.0000 (23018.8430)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3359 (6.5870)  time: 0.9799 (0.5335 -- 4.1040)  data: 0.0016 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.6894 (1.7384)  loss_scale: 16384.0000 (22077.7305)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9150 (6.5438)  time: 0.8079 (0.5305 -- 3.4721)  data: 0.0014 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.8698 (1.7526)  loss_scale: 16384.0000 (21401.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3138 (6.5921)  time: 0.6563 (0.4936 -- 2.5688)  data: 0.0008 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [61] Total time: 0:02:23 (0.8983 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.8698 (1.7708)  loss_scale: 16384.0000 (21401.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3138 (6.5921)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3173 (0.3173)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4340 (2.4340 -- 2.4340)  data: 2.1923 (2.1923 -- 2.1923)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4054 (0.7137)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (96.9697)  time: 0.4277 (0.2023 -- 2.4340)  data: 0.2055 (0.0006 -- 2.1923)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5056 (0.6516)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2155 (0.1703 -- 0.2923)  data: 0.0083 (0.0001 -- 0.0943)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6035 (0.7152)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.2656)  time: 0.1979 (0.1332 -- 0.2923)  data: 0.0079 (0.0001 -- 0.0943)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 81.950 Acc@5 96.680 loss 0.709
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.78%
[2023-09-04 13:59:24,693] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:59:24,693] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 13:59:24,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 13:59:24,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [62]  [  0/160]  eta: 0:21:06  lr: 0.000038  min_lr: 0.000010  loss: 1.5372 (1.5372)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1812 (4.1812)  time: 7.9164 (7.9164 -- 7.9164)  data: 7.3777 (7.3777 -- 7.3777)  max mem: 16413
Epoch: [62]  [ 20/160]  eta: 0:02:36  lr: 0.000038  min_lr: 0.000010  loss: 1.7697 (1.8037)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8636 (6.9391)  time: 0.7805 (0.5238 -- 2.8046)  data: 0.2203 (0.0002 -- 2.2280)  max mem: 16413
Epoch: [62]  [ 40/160]  eta: 0:02:06  lr: 0.000038  min_lr: 0.000010  loss: 1.6535 (1.7441)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0257 (7.2030)  time: 0.9861 (0.5272 -- 3.5986)  data: 0.3739 (0.0008 -- 3.0742)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:39  lr: 0.000038  min_lr: 0.000010  loss: 1.8532 (1.7750)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0654 (7.2099)  time: 0.8789 (0.5168 -- 3.9845)  data: 0.3196 (0.0005 -- 3.4682)  max mem: 16413
[2023-09-04 14:00:31,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=55, lr=[9.539276537446474e-06, 9.539276537446474e-06, 1.0599196152718305e-05, 1.0599196152718305e-05, 1.1776884614131447e-05, 1.1776884614131447e-05, 1.3085427349034943e-05, 1.3085427349034943e-05, 1.4539363721149935e-05, 1.4539363721149935e-05, 1.6154848579055485e-05, 1.6154848579055485e-05, 1.794983175450609e-05, 1.794983175450609e-05, 1.9944257505006766e-05, 1.9944257505006766e-05, 2.2160286116674188e-05, 2.2160286116674188e-05, 2.4622540129637982e-05, 2.4622540129637982e-05, 2.7358377921819982e-05, 2.7358377921819982e-05, 3.0398197690911092e-05, 3.0398197690911092e-05, 3.377577521212343e-05, 3.377577521212343e-05, 3.752863912458159e-05, 3.752863912458159e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 14:00:31,757] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=17.61663380455308, CurrSamplesPerSec=21.932014289113134, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000010  loss: 1.6456 (1.7503)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2262 (7.0807)  time: 0.9535 (0.5278 -- 4.9423)  data: 0.2554 (0.0003 -- 1.9039)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000010  loss: 1.6791 (1.7467)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8448 (7.0544)  time: 0.7725 (0.5288 -- 4.0278)  data: 0.0140 (0.0003 -- 0.2546)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000010  loss: 1.9237 (1.7676)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2276 (7.1396)  time: 0.8703 (0.5296 -- 4.1734)  data: 0.0021 (0.0006 -- 0.0075)  max mem: 16413
[2023-09-04 14:01:17,223] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:01:17,223] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 14:01:17,225] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:01:17,225] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000010  loss: 1.8255 (1.7686)  loss_scale: 65536.0000 (35789.1631)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0091 (7.0401)  time: 0.7680 (0.5130 -- 3.7606)  data: 0.0013 (0.0003 -- 0.0027)  max mem: 16413
[2023-09-04 14:01:36,728] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10077
[2023-09-04 14:01:36,728] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10077
[2023-09-04 14:01:36,728] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 14:01:36,728] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 14:01:36,728] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 1.8003 (1.7683)  loss_scale: 65536.0000 (38707.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5343 (6.9920)  time: 0.6681 (0.4876 -- 1.9681)  data: 0.0011 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [62] Total time: 0:02:20 (0.8809 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 1.8003 (1.7405)  loss_scale: 65536.0000 (38707.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5343 (6.9920)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2057 (0.2057)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4671 (2.4671 -- 2.4671)  data: 2.2451 (2.2451 -- 2.2451)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4905 (0.7115)  acc1: 77.7778 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4211 (0.2018 -- 2.4671)  data: 0.2103 (0.0007 -- 2.2451)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4954 (0.6503)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (97.3545)  time: 0.2143 (0.1688 -- 0.3397)  data: 0.0108 (0.0001 -- 0.1457)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5704 (0.7007)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (97.0954)  time: 0.1999 (0.1328 -- 0.3397)  data: 0.0105 (0.0001 -- 0.1457)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.684
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.78%
Epoch: [63]  [  0/160]  eta: 0:20:18  lr: 0.000037  min_lr: 0.000010  loss: 1.8668 (1.8668)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9053 (5.9053)  time: 7.6127 (7.6127 -- 7.6127)  data: 6.3321 (6.3321 -- 6.3321)  max mem: 16413
Epoch: [63]  [ 20/160]  eta: 0:03:05  lr: 0.000037  min_lr: 0.000009  loss: 1.8150 (1.7306)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9764 (7.3306)  time: 1.0075 (0.5164 -- 4.7413)  data: 0.0498 (0.0005 -- 0.6972)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:02:06  lr: 0.000037  min_lr: 0.000009  loss: 1.7984 (1.7615)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5320 (7.2141)  time: 0.7681 (0.5256 -- 2.9601)  data: 0.0021 (0.0003 -- 0.0140)  max mem: 16413
Epoch: [63]  [ 60/160]  eta: 0:01:40  lr: 0.000037  min_lr: 0.000009  loss: 1.7619 (1.7471)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5159 (7.0646)  time: 0.8964 (0.5157 -- 3.9183)  data: 0.0012 (0.0003 -- 0.0031)  max mem: 16413
[2023-09-04 14:02:51,734] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10146
[2023-09-04 14:02:51,734] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10146
[2023-09-04 14:02:51,734] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:02:51,734] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:02:51,735] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [63]  [ 80/160]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000009  loss: 1.6722 (1.7485)  loss_scale: 16384.0000 (29733.9259)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5279 (7.2676)  time: 0.8086 (0.5092 -- 3.8178)  data: 0.0018 (0.0006 -- 0.0035)  max mem: 16413
Epoch: [63]  [100/160]  eta: 0:00:57  lr: 0.000037  min_lr: 0.000009  loss: 1.7523 (1.7650)  loss_scale: 16384.0000 (27090.3762)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5451 (7.2052)  time: 0.9431 (0.5233 -- 3.3679)  data: 0.1058 (0.0003 -- 1.6471)  max mem: 16413
Epoch: [63]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000009  loss: 1.8255 (1.7737)  loss_scale: 16384.0000 (25320.7273)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5105 (6.9925)  time: 0.8969 (0.5136 -- 4.5728)  data: 0.0227 (0.0004 -- 0.4316)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.8285 (1.7749)  loss_scale: 16384.0000 (24053.1064)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1339 (7.0549)  time: 0.8183 (0.5213 -- 2.9796)  data: 0.0021 (0.0005 -- 0.0145)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.7136 (1.7681)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5620 (7.0521)  time: 0.6963 (0.4960 -- 2.8145)  data: 0.0014 (0.0002 -- 0.0145)  max mem: 16413
Epoch: [63] Total time: 0:02:23 (0.8986 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.7136 (1.7740)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5620 (7.0521)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2418 (0.2418)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3049 (2.3049 -- 2.3049)  data: 2.0711 (2.0711 -- 2.0711)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5869 (0.7946)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4117 (0.1966 -- 2.3049)  data: 0.2008 (0.0008 -- 2.0711)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5200 (0.6730)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2232 (0.1699 -- 0.4389)  data: 0.0242 (0.0001 -- 0.2620)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5690 (0.7355)  acc1: 85.7143 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2088 (0.1331 -- 0.4389)  data: 0.0237 (0.0001 -- 0.2620)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 80.083 Acc@5 96.473 loss 0.705
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 82.78%
Epoch: [64]  [  0/160]  eta: 0:18:58  lr: 0.000037  min_lr: 0.000009  loss: 1.5210 (1.5210)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 10.2577 (10.2577)  time: 7.1165 (7.1165 -- 7.1165)  data: 6.5695 (6.5695 -- 6.5695)  max mem: 16413
Epoch: [64]  [ 20/160]  eta: 0:02:43  lr: 0.000037  min_lr: 0.000009  loss: 1.7957 (1.8409)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7035 (6.9997)  time: 0.8693 (0.5298 -- 3.5051)  data: 0.3223 (0.0009 -- 2.9771)  max mem: 16413
[2023-09-04 14:04:54,451] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:04:54,451] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:04:54,452] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:04:54,452] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:04:58,299] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10279
[2023-09-04 14:04:58,299] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10279
[2023-09-04 14:04:58,299] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:04:58,299] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:04:58,299] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [64]  [ 40/160]  eta: 0:02:02  lr: 0.000037  min_lr: 0.000009  loss: 1.7407 (1.8164)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0290 (6.8245)  time: 0.8598 (0.5130 -- 2.2597)  data: 0.1847 (0.0002 -- 1.7218)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000009  loss: 1.6570 (1.7554)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4581 (6.7551)  time: 0.9430 (0.5374 -- 2.1037)  data: 0.1794 (0.0009 -- 1.5749)  max mem: 16413
Epoch: [64]  [ 80/160]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000009  loss: 1.7089 (1.7481)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.0865 (7.0798)  time: 0.8401 (0.5217 -- 4.0032)  data: 0.0378 (0.0002 -- 0.7273)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.8908 (1.7736)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3324 (7.1912)  time: 0.9119 (0.5195 -- 3.4896)  data: 0.2397 (0.0004 -- 2.9728)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000009  loss: 1.8939 (1.7934)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0815 (7.1197)  time: 0.7968 (0.5198 -- 3.7826)  data: 0.2393 (0.0006 -- 3.2458)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.6375 (1.7745)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5871 (7.0219)  time: 0.8650 (0.5307 -- 3.6523)  data: 0.2122 (0.0006 -- 3.1362)  max mem: 16413
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.7918 (1.7679)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.6002 (7.0960)  time: 0.6774 (0.4963 -- 3.4201)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [64] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.7918 (1.7713)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.6002 (7.0960)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2731 (0.2731)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2822 (2.2822 -- 2.2822)  data: 2.0718 (2.0718 -- 2.0718)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5655 (0.7276)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4064 (0.1921 -- 2.2822)  data: 0.2004 (0.0009 -- 2.0718)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5123 (0.6411)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2290 (0.1697 -- 0.6153)  data: 0.0276 (0.0001 -- 0.4161)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5599 (0.7027)  acc1: 85.7143 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.2145 (0.1326 -- 0.6153)  data: 0.0265 (0.0001 -- 0.4161)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 81.328 Acc@5 96.266 loss 0.712
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 82.78%
Epoch: [65]  [  0/160]  eta: 0:18:29  lr: 0.000037  min_lr: 0.000009  loss: 1.4580 (1.4580)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4073 (6.4073)  time: 6.9373 (6.9373 -- 6.9373)  data: 5.0740 (5.0740 -- 5.0740)  max mem: 16413
[2023-09-04 14:07:01,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:07:01,924] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:07:01,929] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:07:01,930] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [65]  [ 20/160]  eta: 0:02:45  lr: 0.000037  min_lr: 0.000009  loss: 1.6779 (1.7180)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5130 (6.8689)  time: 0.8913 (0.5235 -- 3.7214)  data: 0.0047 (0.0006 -- 0.0551)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:05  lr: 0.000037  min_lr: 0.000009  loss: 1.4302 (1.6185)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0848 (6.6088)  time: 0.8992 (0.5222 -- 2.6414)  data: 0.0631 (0.0002 -- 1.2237)  max mem: 16413
Epoch: [65]  [ 60/160]  eta: 0:01:39  lr: 0.000037  min_lr: 0.000009  loss: 1.7245 (1.6318)  loss_scale: 32768.0000 (30619.2787)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3696 (6.5265)  time: 0.9067 (0.5210 -- 2.6386)  data: 0.0226 (0.0003 -- 0.2244)  max mem: 16413
[2023-09-04 14:07:55,328] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10471
[2023-09-04 14:07:55,328] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:07:55,328] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10471
[2023-09-04 14:07:55,328] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:07:55,328] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [65]  [ 80/160]  eta: 0:01:14  lr: 0.000037  min_lr: 0.000009  loss: 1.8577 (1.6674)  loss_scale: 16384.0000 (29127.1111)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2582 (6.6473)  time: 0.7376 (0.5304 -- 2.7606)  data: 0.1853 (0.0001 -- 2.2014)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.6974 (1.6716)  loss_scale: 16384.0000 (26603.7228)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1456 (6.5906)  time: 0.9971 (0.5167 -- 4.4291)  data: 0.2711 (0.0004 -- 3.9177)  max mem: 16413
Epoch: [65]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000009  loss: 1.7564 (1.6906)  loss_scale: 16384.0000 (24914.5124)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6854 (6.6358)  time: 0.7548 (0.5212 -- 3.0953)  data: 0.2130 (0.0004 -- 2.5700)  max mem: 16413
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.6335 (1.6860)  loss_scale: 16384.0000 (23704.5106)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4485 (6.8136)  time: 0.9842 (0.5170 -- 5.2530)  data: 0.4478 (0.0002 -- 4.7409)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.8107 (1.6986)  loss_scale: 16384.0000 (22835.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3488 (6.7897)  time: 0.6910 (0.4934 -- 2.1588)  data: 0.1667 (0.0002 -- 1.6654)  max mem: 16413
Epoch: [65] Total time: 0:02:23 (0.8978 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.8107 (1.7426)  loss_scale: 16384.0000 (22835.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3488 (6.7897)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2957 (0.2957)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3119 (2.3119 -- 2.3119)  data: 2.0994 (2.0994 -- 2.0994)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5118 (0.7113)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4358 (0.2081 -- 2.3119)  data: 0.2185 (0.0006 -- 2.0994)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3961 (0.6028)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (97.3545)  time: 0.2199 (0.1683 -- 0.5020)  data: 0.0153 (0.0001 -- 0.2802)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5118 (0.6653)  acc1: 88.8889 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2018 (0.1329 -- 0.5020)  data: 0.0151 (0.0001 -- 0.2802)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 81.328 Acc@5 97.095 loss 0.679
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 82.78%
Epoch: [66]  [  0/160]  eta: 0:17:09  lr: 0.000036  min_lr: 0.000009  loss: 1.0587 (1.0587)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.1845 (8.1845)  time: 6.4320 (6.4320 -- 6.4320)  data: 5.4107 (5.4107 -- 5.4107)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:47  lr: 0.000036  min_lr: 0.000009  loss: 1.7629 (1.7273)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8189 (5.9891)  time: 0.9315 (0.5305 -- 3.1453)  data: 0.3879 (0.0005 -- 2.6232)  max mem: 16413
[2023-09-04 14:10:00,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:10:00,077] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:10:00,078] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:10:00,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [66]  [ 40/160]  eta: 0:02:02  lr: 0.000036  min_lr: 0.000009  loss: 1.6509 (1.7272)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5310 (5.8687)  time: 0.8339 (0.5157 -- 3.5279)  data: 0.2917 (0.0003 -- 2.9835)  max mem: 16413
Epoch: [66]  [ 60/160]  eta: 0:01:42  lr: 0.000036  min_lr: 0.000009  loss: 1.5718 (1.7125)  loss_scale: 32768.0000 (22024.3934)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6883 (6.1388)  time: 1.0435 (0.5237 -- 4.3635)  data: 0.4906 (0.0003 -- 3.8359)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000009  loss: 1.9482 (1.7600)  loss_scale: 32768.0000 (24677.1358)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6172 (6.3290)  time: 0.7458 (0.5291 -- 2.6644)  data: 0.2047 (0.0002 -- 2.1538)  max mem: 16413
Epoch: [66]  [100/160]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000009  loss: 1.6621 (1.7526)  loss_scale: 32768.0000 (26279.2871)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9448 (6.2784)  time: 0.9598 (0.5172 -- 4.7004)  data: 0.4185 (0.0003 -- 4.1920)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.8321 (1.7672)  loss_scale: 32768.0000 (27351.8017)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3663 (6.2905)  time: 0.8589 (0.5299 -- 3.6336)  data: 0.3127 (0.0002 -- 3.1298)  max mem: 16413
[2023-09-04 14:11:25,754] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10695
[2023-09-04 14:11:25,754] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10695
[2023-09-04 14:11:25,755] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:11:25,755] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:11:25,755] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.7487 (1.7599)  loss_scale: 32768.0000 (27422.8652)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5371 (6.3845)  time: 0.9695 (0.5070 -- 3.7528)  data: 0.4315 (0.0004 -- 3.2409)  max mem: 16413
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.6130 (1.7430)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6035 (6.4604)  time: 0.5874 (0.4943 -- 1.9734)  data: 0.0732 (0.0002 -- 1.4531)  max mem: 16413
Epoch: [66] Total time: 0:02:24 (0.9034 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.6130 (1.7715)  loss_scale: 16384.0000 (26112.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6035 (6.4604)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2226 (0.2226)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3247 (2.3247 -- 2.3247)  data: 2.0479 (2.0479 -- 2.0479)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6215 (0.6890)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4431 (0.1979 -- 2.3247)  data: 0.2321 (0.0006 -- 2.0479)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4995 (0.6021)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2269 (0.1688 -- 0.6922)  data: 0.0306 (0.0001 -- 0.4899)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5011 (0.6635)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.2656)  time: 0.2150 (0.1326 -- 0.6922)  data: 0.0304 (0.0001 -- 0.4899)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 82.158 Acc@5 96.888 loss 0.674
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.78%
Epoch: [67]  [  0/160]  eta: 0:18:31  lr: 0.000036  min_lr: 0.000009  loss: 1.7683 (1.7683)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2269 (5.2269)  time: 6.9445 (6.9445 -- 6.9445)  data: 5.7404 (5.7404 -- 5.7404)  max mem: 16413
Epoch: [67]  [ 20/160]  eta: 0:02:41  lr: 0.000036  min_lr: 0.000009  loss: 1.6554 (1.7730)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4767 (7.0553)  time: 0.8652 (0.5390 -- 3.3240)  data: 0.3074 (0.0005 -- 2.7895)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:02:08  lr: 0.000036  min_lr: 0.000009  loss: 1.6230 (1.7275)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2633 (6.8058)  time: 0.9881 (0.5175 -- 4.2387)  data: 0.4420 (0.0004 -- 3.6830)  max mem: 16413
Epoch: [67]  [ 60/160]  eta: 0:01:36  lr: 0.000036  min_lr: 0.000009  loss: 1.8386 (1.7332)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9936 (6.9006)  time: 0.7383 (0.5275 -- 3.0082)  data: 0.1923 (0.0004 -- 2.4729)  max mem: 16413
Epoch: [67]  [ 80/160]  eta: 0:01:18  lr: 0.000036  min_lr: 0.000009  loss: 1.8141 (1.7423)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8489 (7.0368)  time: 1.0245 (0.5145 -- 4.7693)  data: 0.4881 (0.0003 -- 4.2341)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000009  loss: 1.7360 (1.7524)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1063 (7.1677)  time: 0.8576 (0.5058 -- 3.7099)  data: 0.3173 (0.0003 -- 3.1819)  max mem: 16413
[2023-09-04 14:13:31,270] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:13:31,270] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:13:31,271] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:13:31,271] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [67]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.6348 (1.7362)  loss_scale: 32768.0000 (18685.8843)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8547 (7.0682)  time: 0.8748 (0.5298 -- 2.8815)  data: 0.3238 (0.0002 -- 2.3548)  max mem: 16413
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.8084 (1.7353)  loss_scale: 32768.0000 (20683.3475)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3159 (7.0264)  time: 0.7788 (0.5122 -- 3.2970)  data: 0.2306 (0.0003 -- 2.7493)  max mem: 16413
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.8867 (1.7403)  loss_scale: 32768.0000 (22118.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3732 (7.0036)  time: 0.6655 (0.4950 -- 3.2454)  data: 0.0613 (0.0002 -- 1.1983)  max mem: 16413
Epoch: [67] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.8867 (1.7441)  loss_scale: 32768.0000 (22118.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3732 (7.0036)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2302 (0.2302)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4542 (2.4542 -- 2.4542)  data: 2.2112 (2.2112 -- 2.2112)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4815 (0.6711)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4148 (0.1938 -- 2.4542)  data: 0.2018 (0.0005 -- 2.2112)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4815 (0.6148)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2088 (0.1709 -- 0.2994)  data: 0.0055 (0.0001 -- 0.0973)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5063 (0.6738)  acc1: 88.8889 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.1935 (0.1333 -- 0.2994)  data: 0.0052 (0.0001 -- 0.0973)  max mem: 16413
Val: Total time: 0:00:07 (0.2822 s / it)
* Acc@1 82.158 Acc@5 96.680 loss 0.661
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.78%
Epoch: [68]  [  0/160]  eta: 0:20:11  lr: 0.000036  min_lr: 0.000009  loss: 2.0826 (2.0826)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.1628 (4.1628)  time: 7.5706 (7.5706 -- 7.5706)  data: 7.0217 (7.0217 -- 7.0217)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:44  lr: 0.000036  min_lr: 0.000009  loss: 1.5730 (1.6583)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5898 (8.1064)  time: 0.8588 (0.5334 -- 4.6319)  data: 0.2736 (0.0005 -- 3.3611)  max mem: 16413
Epoch: [68]  [ 40/160]  eta: 0:02:11  lr: 0.000036  min_lr: 0.000009  loss: 1.7183 (1.6614)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9157 (7.6468)  time: 1.0062 (0.5221 -- 5.1050)  data: 0.0152 (0.0003 -- 0.2592)  max mem: 16413
[2023-09-04 14:15:06,620] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10922
[2023-09-04 14:15:06,620] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10922
[2023-09-04 14:15:06,620] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:15:06,621] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 14:15:06,621] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [68]  [ 60/160]  eta: 0:01:40  lr: 0.000036  min_lr: 0.000009  loss: 1.6886 (1.6626)  loss_scale: 16384.0000 (27664.7869)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0794 (7.3275)  time: 0.8224 (0.5121 -- 4.7552)  data: 0.0014 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:17  lr: 0.000036  min_lr: 0.000009  loss: 1.9239 (1.7215)  loss_scale: 16384.0000 (24879.4074)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6598 (7.2442)  time: 0.8589 (0.5237 -- 3.3103)  data: 0.0016 (0.0005 -- 0.0051)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:56  lr: 0.000036  min_lr: 0.000009  loss: 1.7495 (1.7334)  loss_scale: 16384.0000 (23197.1485)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4799 (7.1710)  time: 0.8154 (0.5192 -- 2.5974)  data: 0.0021 (0.0008 -- 0.0106)  max mem: 16413
[2023-09-04 14:16:09,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=61, lr=[9.042925336597765e-06, 9.042925336597765e-06, 1.0047694818441962e-05, 1.0047694818441962e-05, 1.1164105353824401e-05, 1.1164105353824401e-05, 1.2404561504249335e-05, 1.2404561504249335e-05, 1.3782846115832595e-05, 1.3782846115832595e-05, 1.5314273462036215e-05, 1.5314273462036215e-05, 1.701585940226246e-05, 1.701585940226246e-05, 1.890651044695829e-05, 1.890651044695829e-05, 2.1007233829953654e-05, 2.1007233829953654e-05, 2.3341370922170724e-05, 2.3341370922170724e-05, 2.59348565801897e-05, 2.59348565801897e-05, 2.8816507311321884e-05, 2.8816507311321884e-05, 3.2018341457024315e-05, 3.2018341457024315e-05, 3.557593495224924e-05, 3.557593495224924e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 14:16:09,935] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=17.6326470384498, CurrSamplesPerSec=22.65352538164967, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:36  lr: 0.000036  min_lr: 0.000009  loss: 1.7618 (1.7393)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8661 (7.1113)  time: 0.7806 (0.5328 -- 2.2983)  data: 0.0400 (0.0005 -- 0.7695)  max mem: 16413
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.9992 (1.7697)  loss_scale: 16384.0000 (21264.3404)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7184 (7.0708)  time: 0.8960 (0.5242 -- 2.1207)  data: 0.1999 (0.0003 -- 1.4122)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.7772 (1.7651)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9645 (6.9403)  time: 0.7056 (0.4951 -- 1.6973)  data: 0.0399 (0.0002 -- 0.7679)  max mem: 16413
Epoch: [68] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.7772 (1.7579)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9645 (6.9403)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2266 (0.2266)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3823 (2.3823 -- 2.3823)  data: 2.1529 (2.1529 -- 2.1529)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4197 (0.6663)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (95.9596)  time: 0.4092 (0.1911 -- 2.3823)  data: 0.1966 (0.0006 -- 2.1529)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4521 (0.6006)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2170 (0.1709 -- 0.3488)  data: 0.0120 (0.0001 -- 0.1651)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4677 (0.6582)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.2656)  time: 0.2040 (0.1332 -- 0.3488)  data: 0.0117 (0.0001 -- 0.1651)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.664
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.78%
Epoch: [69]  [  0/160]  eta: 0:18:52  lr: 0.000035  min_lr: 0.000009  loss: 2.3056 (2.3056)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.4279 (4.4279)  time: 7.0761 (7.0761 -- 7.0761)  data: 5.1256 (5.1256 -- 5.1256)  max mem: 16413
[2023-09-04 14:17:07,476] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:17:07,476] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:17:07,517] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:17:07,517] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [69]  [ 20/160]  eta: 0:03:01  lr: 0.000035  min_lr: 0.000009  loss: 1.7727 (1.8405)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8408 (6.3573)  time: 1.0106 (0.5236 -- 5.1037)  data: 0.4380 (0.0004 -- 4.5851)  max mem: 16413
[2023-09-04 14:17:31,305] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11074
[2023-09-04 14:17:31,305] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11074
[2023-09-04 14:17:31,305] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:17:31,305] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:17:31,305] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [69]  [ 40/160]  eta: 0:02:11  lr: 0.000035  min_lr: 0.000009  loss: 1.8188 (1.8067)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1712 (6.4117)  time: 0.8751 (0.5075 -- 5.7187)  data: 0.3258 (0.0003 -- 5.1968)  max mem: 16413
Epoch: [69]  [ 60/160]  eta: 0:01:39  lr: 0.000035  min_lr: 0.000009  loss: 1.9253 (1.8437)  loss_scale: 16384.0000 (22561.5738)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4984 (6.5972)  time: 0.8052 (0.5227 -- 3.3335)  data: 0.2336 (0.0004 -- 2.7890)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:16  lr: 0.000035  min_lr: 0.000009  loss: 1.8263 (1.8311)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8611 (6.7322)  time: 0.8386 (0.5236 -- 2.7348)  data: 0.2057 (0.0004 -- 2.1746)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:57  lr: 0.000035  min_lr: 0.000009  loss: 1.8256 (1.8141)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4636 (6.7364)  time: 0.9402 (0.5267 -- 4.2771)  data: 0.3801 (0.0006 -- 3.7133)  max mem: 16413
Epoch: [69]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000009  loss: 1.7813 (1.8184)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8408 (6.6603)  time: 0.7528 (0.5255 -- 3.2755)  data: 0.2054 (0.0003 -- 2.7345)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.7068 (1.8095)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4543 (6.6367)  time: 0.9079 (0.5193 -- 3.7658)  data: 0.2505 (0.0007 -- 3.2403)  max mem: 16413
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.5789 (1.7905)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6688 (6.6983)  time: 0.6959 (0.4957 -- 2.4861)  data: 0.0361 (0.0002 -- 0.7096)  max mem: 16413
Epoch: [69] Total time: 0:02:23 (0.8942 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.5789 (1.7639)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6688 (6.6983)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1951 (0.1951)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5748 (2.5748 -- 2.5748)  data: 2.3740 (2.3740 -- 2.3740)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3942 (0.6458)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4229 (0.1971 -- 2.5748)  data: 0.2165 (0.0004 -- 2.3740)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4633 (0.5855)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2143 (0.1693 -- 0.4569)  data: 0.0141 (0.0001 -- 0.2725)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5420 (0.6499)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.2656)  time: 0.2009 (0.1326 -- 0.4569)  data: 0.0139 (0.0001 -- 0.2725)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.647
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.78%
Epoch: [70]  [  0/160]  eta: 0:17:50  lr: 0.000035  min_lr: 0.000009  loss: 1.6366 (1.6366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.1266 (5.1266)  time: 6.6925 (6.6925 -- 6.6925)  data: 6.1465 (6.1465 -- 6.1465)  max mem: 16413
[2023-09-04 14:19:29,488] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:19:29,488] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:19:29,492] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:19:29,492] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:19:37,182] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11209
[2023-09-04 14:19:37,182] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11209
[2023-09-04 14:19:37,182] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:19:37,182] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:19:37,182] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [70]  [ 20/160]  eta: 0:02:48  lr: 0.000035  min_lr: 0.000009  loss: 1.7401 (1.7295)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0829 (6.2829)  time: 0.9291 (0.5288 -- 4.0061)  data: 0.3167 (0.0002 -- 3.4789)  max mem: 16413
Epoch: [70]  [ 40/160]  eta: 0:02:00  lr: 0.000035  min_lr: 0.000009  loss: 1.6794 (1.6962)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2574 (6.6341)  time: 0.7908 (0.5267 -- 2.9597)  data: 0.2273 (0.0011 -- 2.3806)  max mem: 16413
Epoch: [70]  [ 60/160]  eta: 0:01:38  lr: 0.000035  min_lr: 0.000009  loss: 1.6384 (1.6802)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0638 (6.8645)  time: 0.9572 (0.5225 -- 3.5975)  data: 0.4111 (0.0004 -- 3.0660)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:16  lr: 0.000035  min_lr: 0.000009  loss: 1.8017 (1.7212)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6906 (6.9068)  time: 0.8628 (0.5231 -- 2.0720)  data: 0.0808 (0.0004 -- 0.8891)  max mem: 16413
Epoch: [70]  [100/160]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000009  loss: 1.9776 (1.7537)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5300 (6.8727)  time: 0.8849 (0.5407 -- 2.6377)  data: 0.0473 (0.0003 -- 0.6560)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 1.8072 (1.7555)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1358 (6.7948)  time: 0.9256 (0.5217 -- 3.3243)  data: 0.0300 (0.0002 -- 0.5721)  max mem: 16413
[2023-09-04 14:21:27,461] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:21:27,461] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:21:27,461] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:21:27,462] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [70]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.7122 (1.7591)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1292 (6.7340)  time: 0.7080 (0.5221 -- 2.3202)  data: 0.0014 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.8982 (1.7675)  loss_scale: 32768.0000 (19251.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6359 (6.7673)  time: 0.7691 (0.4948 -- 3.5333)  data: 0.0047 (0.0003 -- 0.0782)  max mem: 16413
Epoch: [70] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.8982 (1.7441)  loss_scale: 32768.0000 (19251.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6359 (6.7673)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.1997 (0.1997)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0830 (2.0830 -- 2.0830)  data: 1.8823 (1.8823 -- 1.8823)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4869 (0.6892)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4232 (0.1993 -- 2.0830)  data: 0.2132 (0.0005 -- 1.8823)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4869 (0.6215)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.8836)  time: 0.2342 (0.1724 -- 0.4164)  data: 0.0304 (0.0001 -- 0.1804)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4952 (0.6819)  acc1: 88.8889 (80.9129)  acc5: 100.0000 (97.5104)  time: 0.2145 (0.1324 -- 0.4164)  data: 0.0252 (0.0001 -- 0.1804)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 81.535 Acc@5 97.718 loss 0.672
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.78%
Epoch: [71]  [  0/160]  eta: 0:19:53  lr: 0.000035  min_lr: 0.000009  loss: 1.2691 (1.2691)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 9.6728 (9.6728)  time: 7.4596 (7.4596 -- 7.4596)  data: 6.0721 (6.0721 -- 6.0721)  max mem: 16413
Epoch: [71]  [ 20/160]  eta: 0:02:42  lr: 0.000035  min_lr: 0.000009  loss: 1.7001 (1.6828)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2754 (7.6373)  time: 0.8484 (0.5236 -- 3.4942)  data: 0.2785 (0.0008 -- 2.9428)  max mem: 16413
Epoch: [71]  [ 40/160]  eta: 0:02:03  lr: 0.000035  min_lr: 0.000009  loss: 1.6522 (1.6689)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0477 (7.0661)  time: 0.8878 (0.5150 -- 4.0081)  data: 0.0638 (0.0004 -- 1.2397)  max mem: 16413
Epoch: [71]  [ 60/160]  eta: 0:01:39  lr: 0.000035  min_lr: 0.000009  loss: 1.8955 (1.6989)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7948 (7.0459)  time: 0.9205 (0.5278 -- 4.7534)  data: 0.0945 (0.0008 -- 1.3402)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:14  lr: 0.000035  min_lr: 0.000009  loss: 1.4614 (1.6695)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7668 (6.8673)  time: 0.7553 (0.5347 -- 2.7036)  data: 0.2021 (0.0003 -- 2.1733)  max mem: 16413
[2023-09-04 14:23:17,165] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11449
[2023-09-04 14:23:17,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11449
[2023-09-04 14:23:17,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:23:17,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:23:17,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [71]  [100/160]  eta: 0:00:55  lr: 0.000035  min_lr: 0.000009  loss: 1.8316 (1.6802)  loss_scale: 16384.0000 (30821.3861)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0789 (6.8733)  time: 0.9019 (0.5164 -- 3.8460)  data: 0.3532 (0.0008 -- 3.3056)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000009  loss: 1.8145 (1.7060)  loss_scale: 16384.0000 (28435.0413)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0222 (6.8305)  time: 0.8790 (0.5208 -- 2.5177)  data: 0.2361 (0.0003 -- 1.6008)  max mem: 16413
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.9666 (1.7314)  loss_scale: 16384.0000 (26725.6738)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9386 (6.8778)  time: 0.8226 (0.5208 -- 3.2870)  data: 0.1384 (0.0004 -- 1.5401)  max mem: 16413
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.8445 (1.7475)  loss_scale: 16384.0000 (25497.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4130 (6.8229)  time: 0.7760 (0.4942 -- 3.2469)  data: 0.2531 (0.0002 -- 2.7178)  max mem: 16413
Epoch: [71] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.8445 (1.7188)  loss_scale: 16384.0000 (25497.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4130 (6.8229)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2296 (0.2296)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2382 (2.2382 -- 2.2382)  data: 2.0146 (2.0146 -- 2.0146)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5368 (0.6893)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4298 (0.2035 -- 2.2382)  data: 0.2125 (0.0006 -- 2.0146)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4408 (0.6096)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (97.3545)  time: 0.2262 (0.1722 -- 0.5581)  data: 0.0187 (0.0001 -- 0.3139)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4485 (0.6630)  acc1: 88.8889 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2099 (0.1332 -- 0.5581)  data: 0.0184 (0.0001 -- 0.3139)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 81.120 Acc@5 97.095 loss 0.674
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 82.78%
Epoch: [72]  [  0/160]  eta: 0:18:13  lr: 0.000035  min_lr: 0.000009  loss: 1.3347 (1.3347)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.3067 (4.3067)  time: 6.8346 (6.8346 -- 6.8346)  data: 5.4288 (5.4288 -- 5.4288)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:02:38  lr: 0.000034  min_lr: 0.000009  loss: 1.6260 (1.6790)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8919 (6.6351)  time: 0.8466 (0.5355 -- 2.7596)  data: 0.2668 (0.0005 -- 2.2410)  max mem: 16413
Epoch: [72]  [ 40/160]  eta: 0:02:00  lr: 0.000034  min_lr: 0.000009  loss: 1.7765 (1.7052)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0671 (6.8996)  time: 0.8662 (0.5302 -- 3.3977)  data: 0.2539 (0.0002 -- 2.8820)  max mem: 16413
[2023-09-04 14:25:19,733] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:25:19,733] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:25:19,735] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:25:19,735] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [72]  [ 60/160]  eta: 0:01:36  lr: 0.000034  min_lr: 0.000009  loss: 1.5483 (1.6726)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9026 (6.9772)  time: 0.9003 (0.5316 -- 2.7915)  data: 0.3192 (0.0011 -- 2.2329)  max mem: 16413
Epoch: [72]  [ 80/160]  eta: 0:01:16  lr: 0.000034  min_lr: 0.000009  loss: 1.6410 (1.6889)  loss_scale: 32768.0000 (21036.2469)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6425 (6.8704)  time: 0.9262 (0.5216 -- 2.4381)  data: 0.1896 (0.0004 -- 1.9075)  max mem: 16413
Epoch: [72]  [100/160]  eta: 0:00:56  lr: 0.000034  min_lr: 0.000009  loss: 1.6002 (1.6936)  loss_scale: 32768.0000 (23359.3663)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8322 (6.6779)  time: 0.8421 (0.5293 -- 2.9902)  data: 0.2019 (0.0004 -- 2.4514)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000009  loss: 1.7048 (1.6893)  loss_scale: 32768.0000 (24914.5124)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5750 (6.6924)  time: 0.8482 (0.5216 -- 2.9322)  data: 0.1702 (0.0001 -- 2.3893)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.8164 (1.7007)  loss_scale: 32768.0000 (26028.4823)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3290 (6.8138)  time: 0.9490 (0.5282 -- 4.5492)  data: 0.3953 (0.0006 -- 4.0147)  max mem: 16413
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.6307 (1.6995)  loss_scale: 32768.0000 (26828.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1613 (6.9561)  time: 0.6560 (0.4945 -- 2.2022)  data: 0.1361 (0.0001 -- 1.6984)  max mem: 16413
Epoch: [72] Total time: 0:02:22 (0.8936 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.6307 (1.7169)  loss_scale: 32768.0000 (26828.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1613 (6.9561)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2202 (0.2202)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4362 (2.4362 -- 2.4362)  data: 2.2028 (2.2028 -- 2.2028)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5778 (0.6634)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4198 (0.2003 -- 2.4362)  data: 0.2033 (0.0008 -- 2.2028)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5359 (0.5978)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (97.3545)  time: 0.2106 (0.1694 -- 0.2396)  data: 0.0049 (0.0001 -- 0.0503)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5403 (0.6615)  acc1: 88.8889 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.1942 (0.1328 -- 0.2396)  data: 0.0037 (0.0001 -- 0.0503)  max mem: 16413
Val: Total time: 0:00:07 (0.2828 s / it)
* Acc@1 81.328 Acc@5 97.510 loss 0.672
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 82.78%
Epoch: [73]  [  0/160]  eta: 0:18:31  lr: 0.000034  min_lr: 0.000009  loss: 2.2704 (2.2704)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.3785 (8.3785)  time: 6.9446 (6.9446 -- 6.9446)  data: 6.1840 (6.1840 -- 6.1840)  max mem: 16413
Epoch: [73]  [ 20/160]  eta: 0:02:44  lr: 0.000034  min_lr: 0.000009  loss: 1.6942 (1.7584)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7321 (7.2677)  time: 0.8836 (0.5359 -- 2.5255)  data: 0.0510 (0.0004 -- 0.9869)  max mem: 16413
[2023-09-04 14:27:18,698] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11703
[2023-09-04 14:27:18,698] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11703
[2023-09-04 14:27:18,698] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:27:18,699] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 14:27:18,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [73]  [ 40/160]  eta: 0:02:08  lr: 0.000034  min_lr: 0.000009  loss: 1.5705 (1.6810)  loss_scale: 16384.0000 (25575.0244)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3421 (7.1513)  time: 0.9639 (0.5150 -- 3.8984)  data: 0.0014 (0.0004 -- 0.0041)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:40  lr: 0.000034  min_lr: 0.000009  loss: 1.7300 (1.7037)  loss_scale: 16384.0000 (22561.5738)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2292 (7.0642)  time: 0.8599 (0.5294 -- 3.8714)  data: 0.0033 (0.0006 -- 0.0159)  max mem: 16413
Epoch: [73]  [ 80/160]  eta: 0:01:18  lr: 0.000034  min_lr: 0.000009  loss: 1.6774 (1.6961)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0007 (7.0744)  time: 0.9118 (0.5030 -- 5.5489)  data: 0.0016 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:56  lr: 0.000034  min_lr: 0.000009  loss: 1.8172 (1.7030)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4104 (6.9537)  time: 0.8161 (0.5262 -- 3.1954)  data: 0.0017 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000009  loss: 1.6350 (1.6975)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6933 (6.8519)  time: 0.9103 (0.5313 -- 3.7780)  data: 0.0017 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.7177 (1.7000)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4240 (6.9227)  time: 0.9556 (0.5159 -- 4.6731)  data: 0.0014 (0.0003 -- 0.0042)  max mem: 16413
[2023-09-04 14:29:13,179] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:29:13,179] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:29:13,179] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:29:13,179] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.5906 (1.7006)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3805 (6.8587)  time: 0.5876 (0.4943 -- 1.8933)  data: 0.0007 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [73] Total time: 0:02:24 (0.9013 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.5906 (1.7352)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3805 (6.8587)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2353 (0.2353)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3513 (2.3513 -- 2.3513)  data: 2.1258 (2.1258 -- 2.1258)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4014 (0.6622)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4283 (0.1967 -- 2.3513)  data: 0.2156 (0.0007 -- 2.1258)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5173 (0.6104)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2197 (0.1713 -- 0.4620)  data: 0.0125 (0.0001 -- 0.2339)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6063 (0.6608)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2060 (0.1329 -- 0.4620)  data: 0.0122 (0.0001 -- 0.2339)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 82.158 Acc@5 97.095 loss 0.640
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.78%
Epoch: [74]  [  0/160]  eta: 0:20:30  lr: 0.000034  min_lr: 0.000009  loss: 1.5818 (1.5818)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8788 (5.8788)  time: 7.6926 (7.6926 -- 7.6926)  data: 7.1158 (7.1158 -- 7.1158)  max mem: 16413
[2023-09-04 14:29:44,189] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11855
[2023-09-04 14:29:44,189] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11855
[2023-09-04 14:29:44,190] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:29:44,190] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:29:44,190] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [74]  [ 20/160]  eta: 0:02:44  lr: 0.000034  min_lr: 0.000009  loss: 1.6278 (1.6865)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4601 (7.2408)  time: 0.8497 (0.5243 -- 2.7892)  data: 0.2987 (0.0006 -- 2.2441)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:02:01  lr: 0.000034  min_lr: 0.000009  loss: 1.5817 (1.6460)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9209 (7.9759)  time: 0.8437 (0.5320 -- 3.7087)  data: 0.2544 (0.0007 -- 3.1857)  max mem: 16413
Epoch: [74]  [ 60/160]  eta: 0:01:37  lr: 0.000034  min_lr: 0.000009  loss: 1.8069 (1.6945)  loss_scale: 16384.0000 (20412.8525)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4966 (7.9959)  time: 0.9045 (0.5253 -- 4.3278)  data: 0.1261 (0.0003 -- 2.2930)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:15  lr: 0.000034  min_lr: 0.000009  loss: 1.8294 (1.7173)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4358 (7.6648)  time: 0.8328 (0.5256 -- 2.4346)  data: 0.0383 (0.0007 -- 0.7111)  max mem: 16413
Epoch: [74]  [100/160]  eta: 0:00:56  lr: 0.000034  min_lr: 0.000009  loss: 1.7491 (1.7259)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9877 (7.5797)  time: 0.8993 (0.5257 -- 1.9597)  data: 0.1478 (0.0004 -- 1.1393)  max mem: 16413
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000009  loss: 1.8809 (1.7374)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9432 (7.4074)  time: 0.8305 (0.5357 -- 3.0630)  data: 0.2566 (0.0004 -- 2.5483)  max mem: 16413
Epoch: [74]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.7389 (1.7496)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9275 (7.3623)  time: 0.8867 (0.5231 -- 3.7968)  data: 0.3022 (0.0003 -- 3.1531)  max mem: 16413
[2023-09-04 14:31:35,873] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:31:35,873] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:31:35,873] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:31:35,873] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:31:46,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=66, lr=[8.515378090801247e-06, 8.515378090801247e-06, 9.461531212001385e-06, 9.461531212001385e-06, 1.0512812457779315e-05, 1.0512812457779315e-05, 1.1680902730865906e-05, 1.1680902730865906e-05, 1.297878081207323e-05, 1.297878081207323e-05, 1.4420867568970255e-05, 1.4420867568970255e-05, 1.6023186187744725e-05, 1.6023186187744725e-05, 1.780354020860525e-05, 1.780354020860525e-05, 1.9781711342894723e-05, 1.9781711342894723e-05, 2.1979679269883025e-05, 2.1979679269883025e-05, 2.4421865855425585e-05, 2.4421865855425585e-05, 2.7135406506028427e-05, 2.7135406506028427e-05, 3.0150451673364916e-05, 3.0150451673364916e-05, 3.350050185929435e-05, 3.350050185929435e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 14:31:46,682] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=17.653102374693308, CurrSamplesPerSec=24.812775424867276, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.5804 (1.7391)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5637 (7.3021)  time: 0.7003 (0.4943 -- 3.5803)  data: 0.0266 (0.0002 -- 0.4363)  max mem: 16413
Epoch: [74] Total time: 0:02:22 (0.8884 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.5804 (1.7481)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5637 (7.3021)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2406 (0.2406)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2654 (2.2654 -- 2.2654)  data: 2.0407 (2.0407 -- 2.0407)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5545 (0.6871)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4123 (0.1935 -- 2.2654)  data: 0.1979 (0.0005 -- 2.0407)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4721 (0.6143)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (97.3545)  time: 0.2192 (0.1692 -- 0.3749)  data: 0.0152 (0.0001 -- 0.1282)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5291 (0.6742)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.6805)  time: 0.2041 (0.1325 -- 0.3749)  data: 0.0150 (0.0001 -- 0.1282)  max mem: 16413
Val: Total time: 0:00:07 (0.2827 s / it)
* Acc@1 82.158 Acc@5 97.510 loss 0.644
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.78%
Epoch: [75]  [  0/160]  eta: 0:21:57  lr: 0.000033  min_lr: 0.000009  loss: 1.7573 (1.7573)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.6587 (4.6587)  time: 8.2326 (8.2326 -- 8.2326)  data: 5.3932 (5.3932 -- 5.3932)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:02:53  lr: 0.000033  min_lr: 0.000009  loss: 1.6862 (1.6732)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1736 (7.5008)  time: 0.8890 (0.5154 -- 3.2585)  data: 0.0637 (0.0004 -- 1.2439)  max mem: 16413
Epoch: [75]  [ 40/160]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000008  loss: 1.8811 (1.7347)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3911 (7.0930)  time: 0.8455 (0.5052 -- 3.5712)  data: 0.2321 (0.0005 -- 2.3287)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:40  lr: 0.000033  min_lr: 0.000008  loss: 1.8563 (1.7355)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.9773 (7.2880)  time: 0.9221 (0.5244 -- 2.9335)  data: 0.1471 (0.0003 -- 1.7261)  max mem: 16413
Epoch: [75]  [ 80/160]  eta: 0:01:18  lr: 0.000033  min_lr: 0.000008  loss: 1.7487 (1.7279)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8796 (7.1640)  time: 0.8941 (0.5082 -- 3.2007)  data: 0.0640 (0.0002 -- 0.8318)  max mem: 16413
Epoch: [75]  [100/160]  eta: 0:00:57  lr: 0.000033  min_lr: 0.000008  loss: 1.6881 (1.7008)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6098 (7.1128)  time: 0.8747 (0.5180 -- 3.2096)  data: 0.1151 (0.0003 -- 2.2605)  max mem: 16413
[2023-09-04 14:33:40,595] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:33:40,595] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:33:40,636] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 14:33:40,636] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 14:33:46,190] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12119
[2023-09-04 14:33:46,190] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12119
[2023-09-04 14:33:46,190] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 14:33:46,190] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 14:33:46,190] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [75]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000008  loss: 1.6738 (1.6965)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7537 (7.0142)  time: 0.7888 (0.5211 -- 3.0766)  data: 0.0020 (0.0004 -- 0.0088)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.7093 (1.7009)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7533 (7.1247)  time: 0.8662 (0.5144 -- 2.9716)  data: 0.0147 (0.0005 -- 0.2663)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.6068 (1.6981)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2094 (7.0934)  time: 0.7103 (0.4943 -- 3.0493)  data: 0.0559 (0.0002 -- 0.8388)  max mem: 16413
Epoch: [75] Total time: 0:02:23 (0.8956 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.6068 (1.7021)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2094 (7.0934)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3115 (0.3115)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3839 (2.3839 -- 2.3839)  data: 2.1516 (2.1516 -- 2.1516)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3804 (0.6596)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (100.0000)  time: 0.4171 (0.1956 -- 2.3839)  data: 0.2008 (0.0008 -- 2.1516)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4617 (0.6009)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2129 (0.1690 -- 0.2766)  data: 0.0091 (0.0001 -- 0.1060)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5233 (0.6555)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (97.0954)  time: 0.1999 (0.1336 -- 0.2766)  data: 0.0088 (0.0001 -- 0.1060)  max mem: 16413
Val: Total time: 0:00:07 (0.2827 s / it)
* Acc@1 83.195 Acc@5 97.718 loss 0.634
Accuracy of the network on the 482 val images: 83.20%
[2023-09-04 14:34:25,540] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 14:34:25,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 14:34:25,542] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 14:34:25,542] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 14:34:26,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 14:34:26,968] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.20%
Epoch: [76]  [  0/160]  eta: 0:17:47  lr: 0.000033  min_lr: 0.000008  loss: 1.7841 (1.7841)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.0731 (8.0731)  time: 6.6734 (6.6734 -- 6.6734)  data: 4.2164 (4.2164 -- 4.2164)  max mem: 16413
Epoch: [76]  [ 20/160]  eta: 0:02:53  lr: 0.000033  min_lr: 0.000008  loss: 1.7625 (1.7663)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9788 (7.1690)  time: 0.9675 (0.5138 -- 3.9637)  data: 0.4136 (0.0002 -- 3.4282)  max mem: 16413
Epoch: [76]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000008  loss: 1.4340 (1.6344)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2269 (7.0005)  time: 0.8800 (0.5189 -- 3.5929)  data: 0.2097 (0.0003 -- 3.0604)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000008  loss: 1.6783 (1.6687)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4599 (6.8729)  time: 0.8365 (0.5086 -- 2.8440)  data: 0.2531 (0.0007 -- 2.3040)  max mem: 16413
Epoch: [76]  [ 80/160]  eta: 0:01:15  lr: 0.000033  min_lr: 0.000008  loss: 1.8317 (1.7094)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0294 (6.8541)  time: 0.7821 (0.5281 -- 2.2253)  data: 0.1500 (0.0003 -- 1.6875)  max mem: 16413
[2023-09-04 14:35:49,654] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:35:49,654] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:35:49,654] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 14:35:49,654] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 14:36:02,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12259
[2023-09-04 14:36:02,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12259
[2023-09-04 14:36:02,523] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 14:36:02,523] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 14:36:02,523] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [76]  [100/160]  eta: 0:00:57  lr: 0.000033  min_lr: 0.000008  loss: 1.6976 (1.7211)  loss_scale: 65536.0000 (36336.7921)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4359 (6.8497)  time: 1.0049 (0.5236 -- 2.6745)  data: 0.2503 (0.0009 -- 2.0915)  max mem: 16413
[2023-09-04 14:36:08,821] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12268
[2023-09-04 14:36:08,821] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12268
[2023-09-04 14:36:08,821] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:36:08,821] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:36:08,821] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [76]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000008  loss: 1.8095 (1.7337)  loss_scale: 16384.0000 (33986.6446)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1067 (6.9563)  time: 0.7564 (0.5185 -- 3.3985)  data: 0.0015 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [76]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.7201 (1.7372)  loss_scale: 16384.0000 (31489.8156)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6705 (7.0058)  time: 0.9622 (0.5275 -- 3.7821)  data: 0.0442 (0.0003 -- 0.8537)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.5532 (1.7200)  loss_scale: 16384.0000 (29696.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3270 (6.9482)  time: 0.6205 (0.4951 -- 1.8589)  data: 0.0309 (0.0002 -- 0.4659)  max mem: 16413
Epoch: [76] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.5532 (1.7256)  loss_scale: 16384.0000 (29696.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3270 (6.9482)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2804 (0.2804)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2473 (2.2473 -- 2.2473)  data: 2.0387 (2.0387 -- 2.0387)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4849 (0.6873)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (100.0000)  time: 0.4197 (0.2029 -- 2.2473)  data: 0.2050 (0.0003 -- 2.0387)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4849 (0.6135)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2224 (0.1691 -- 0.4114)  data: 0.0165 (0.0001 -- 0.1811)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5354 (0.6562)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.5104)  time: 0.2060 (0.1325 -- 0.4114)  data: 0.0154 (0.0001 -- 0.1811)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 83.610 Acc@5 98.133 loss 0.656
Accuracy of the network on the 482 val images: 83.61%
[2023-09-04 14:36:57,107] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 14:36:57,109] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 14:36:57,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 14:36:57,109] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 14:36:58,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 14:36:58,507] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.61%
Epoch: [77]  [  0/160]  eta: 0:22:10  lr: 0.000033  min_lr: 0.000008  loss: 2.0644 (2.0644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4559 (7.4559)  time: 8.3170 (8.3170 -- 8.3170)  data: 7.6985 (7.6985 -- 7.6985)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:50  lr: 0.000033  min_lr: 0.000008  loss: 1.7843 (1.7728)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5495 (6.7384)  time: 0.8652 (0.5312 -- 4.4269)  data: 0.3102 (0.0005 -- 3.9103)  max mem: 16413
Epoch: [77]  [ 40/160]  eta: 0:02:08  lr: 0.000033  min_lr: 0.000008  loss: 1.7144 (1.7270)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5360 (7.0834)  time: 0.9081 (0.5285 -- 3.0559)  data: 0.3573 (0.0003 -- 2.4998)  max mem: 16413
Epoch: [77]  [ 60/160]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000008  loss: 1.7140 (1.6728)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2247 (7.0970)  time: 0.8088 (0.5198 -- 3.3090)  data: 0.2526 (0.0003 -- 2.5434)  max mem: 16413
[2023-09-04 14:38:13,543] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:38:13,543] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:38:13,544] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:38:13,544] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000008  loss: 1.5951 (1.6712)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.0069 (7.2842)  time: 0.8905 (0.5156 -- 3.8215)  data: 0.2281 (0.0003 -- 2.6407)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000008  loss: 1.6965 (1.6729)  loss_scale: 32768.0000 (20277.2277)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6806 (7.2852)  time: 0.8231 (0.5232 -- 3.4682)  data: 0.0216 (0.0004 -- 0.3140)  max mem: 16413
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000008  loss: 1.5632 (1.6653)  loss_scale: 32768.0000 (22341.8182)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9760 (7.2646)  time: 0.7595 (0.5197 -- 2.3915)  data: 0.1718 (0.0007 -- 1.8549)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.6374 (1.6635)  loss_scale: 32768.0000 (23820.7092)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5573 (7.2609)  time: 0.9038 (0.5405 -- 3.4388)  data: 0.1692 (0.0009 -- 2.8984)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.7334 (1.6849)  loss_scale: 32768.0000 (24883.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9100 (7.1747)  time: 0.7438 (0.4963 -- 1.9148)  data: 0.0468 (0.0001 -- 0.9222)  max mem: 16413
Epoch: [77] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.7334 (1.7233)  loss_scale: 32768.0000 (24883.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9100 (7.1747)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2357 (0.2357)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3710 (2.3710 -- 2.3710)  data: 2.1593 (2.1593 -- 2.1593)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5214 (0.6569)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4066 (0.1912 -- 2.3710)  data: 0.1982 (0.0007 -- 2.1593)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5662 (0.6206)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2204 (0.1694 -- 0.5765)  data: 0.0206 (0.0001 -- 0.3887)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5723 (0.6557)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.2059 (0.1329 -- 0.5765)  data: 0.0199 (0.0001 -- 0.3887)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 83.817 Acc@5 97.510 loss 0.641
Accuracy of the network on the 482 val images: 83.82%
[2023-09-04 14:39:28,205] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 14:39:28,207] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 14:39:28,207] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 14:39:28,207] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 14:39:29,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 14:39:29,347] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.82%
Epoch: [78]  [  0/160]  eta: 0:21:00  lr: 0.000032  min_lr: 0.000008  loss: 1.3144 (1.3144)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 4.7151 (4.7151)  time: 7.8812 (7.8812 -- 7.8812)  data: 7.3361 (7.3361 -- 7.3361)  max mem: 16413
[2023-09-04 14:39:44,676] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12489
[2023-09-04 14:39:44,676] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12489
[2023-09-04 14:39:44,677] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:39:44,677] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 14:39:44,677] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [78]  [ 20/160]  eta: 0:02:40  lr: 0.000032  min_lr: 0.000008  loss: 1.6028 (1.6302)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4246 (7.1757)  time: 0.8099 (0.5205 -- 2.1288)  data: 0.2104 (0.0005 -- 1.5626)  max mem: 16413
Epoch: [78]  [ 40/160]  eta: 0:02:00  lr: 0.000032  min_lr: 0.000008  loss: 1.8497 (1.7324)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6076 (7.3198)  time: 0.8576 (0.5271 -- 3.0349)  data: 0.2651 (0.0001 -- 2.4903)  max mem: 16413
Epoch: [78]  [ 60/160]  eta: 0:01:36  lr: 0.000032  min_lr: 0.000008  loss: 1.7557 (1.7445)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0735 (7.4045)  time: 0.8770 (0.5182 -- 2.5748)  data: 0.2763 (0.0004 -- 2.0225)  max mem: 16413
Epoch: [78]  [ 80/160]  eta: 0:01:15  lr: 0.000032  min_lr: 0.000008  loss: 1.9023 (1.7589)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.1000 (0.1000)  grad_norm: 8.0879 (7.5542)  time: 0.8690 (0.5223 -- 3.7999)  data: 0.3216 (0.0006 -- 3.2835)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000008  loss: 1.8112 (1.7613)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7806 (7.4497)  time: 0.8993 (0.5270 -- 3.8727)  data: 0.3583 (0.0004 -- 3.3730)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.7431 (1.7629)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4155 (7.3156)  time: 0.9218 (0.5245 -- 4.5929)  data: 0.3772 (0.0004 -- 4.0641)  max mem: 16413
[2023-09-04 14:41:38,971] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:41:38,972] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:41:38,974] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:41:38,974] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.6925 (1.7449)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5568 (7.5051)  time: 0.9065 (0.5026 -- 5.3506)  data: 0.3706 (0.0003 -- 4.8336)  max mem: 16413
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.7368 (1.7377)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6326 (7.4362)  time: 0.7312 (0.4957 -- 4.3668)  data: 0.2148 (0.0001 -- 3.8385)  max mem: 16413
Epoch: [78] Total time: 0:02:24 (0.9051 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.7368 (1.7393)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6326 (7.4362)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2787 (0.2787)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4227 (2.4227 -- 2.4227)  data: 2.1821 (2.1821 -- 2.1821)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4873 (0.6469)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4205 (0.1900 -- 2.4227)  data: 0.2090 (0.0007 -- 2.1821)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4873 (0.6116)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2134 (0.1683 -- 0.3470)  data: 0.0128 (0.0001 -- 0.1359)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5481 (0.6487)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (96.6805)  time: 0.2009 (0.1323 -- 0.3470)  data: 0.0125 (0.0001 -- 0.1359)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.643
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.82%
Epoch: [79]  [  0/160]  eta: 0:26:58  lr: 0.000032  min_lr: 0.000008  loss: 2.0493 (2.0493)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.4897 (7.4897)  time: 10.1168 (10.1168 -- 10.1168)  data: 9.5848 (9.5848 -- 9.5848)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:46  lr: 0.000032  min_lr: 0.000008  loss: 1.6924 (1.7596)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6482 (7.1328)  time: 0.7464 (0.5089 -- 2.5057)  data: 0.1943 (0.0002 -- 1.9767)  max mem: 16413
Epoch: [79]  [ 40/160]  eta: 0:02:02  lr: 0.000032  min_lr: 0.000008  loss: 1.7992 (1.7923)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0312 (7.3556)  time: 0.8467 (0.5199 -- 3.1632)  data: 0.2123 (0.0004 -- 2.6083)  max mem: 16413
[2023-09-04 14:42:55,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12694
[2023-09-04 14:42:55,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12694
[2023-09-04 14:42:55,966] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:42:55,966] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:42:55,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [79]  [ 60/160]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000008  loss: 1.8956 (1.8044)  loss_scale: 32768.0000 (30887.8689)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0883 (7.1366)  time: 0.8820 (0.5286 -- 2.6727)  data: 0.0037 (0.0002 -- 0.0443)  max mem: 16413
Epoch: [79]  [ 80/160]  eta: 0:01:16  lr: 0.000032  min_lr: 0.000008  loss: 1.7209 (1.7841)  loss_scale: 16384.0000 (27306.6667)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4740 (7.0583)  time: 0.9140 (0.5248 -- 4.0825)  data: 0.0017 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:57  lr: 0.000032  min_lr: 0.000008  loss: 1.8134 (1.7831)  loss_scale: 16384.0000 (25143.7624)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2279 (7.0956)  time: 0.9287 (0.5304 -- 3.0767)  data: 0.0013 (0.0001 -- 0.0030)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.5599 (1.7400)  loss_scale: 16384.0000 (23695.8678)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6683 (7.0277)  time: 0.7890 (0.5219 -- 4.1980)  data: 0.0022 (0.0004 -- 0.0158)  max mem: 16413
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.6322 (1.7279)  loss_scale: 16384.0000 (22658.7234)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6613 (7.0101)  time: 0.8669 (0.5204 -- 3.9708)  data: 0.0014 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.5389 (1.7100)  loss_scale: 16384.0000 (21913.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5682 (6.9700)  time: 0.6973 (0.4941 -- 3.2564)  data: 0.0008 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [79] Total time: 0:02:23 (0.8938 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.5389 (1.7280)  loss_scale: 16384.0000 (21913.6000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5682 (6.9700)
[2023-09-04 14:44:24,841] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-09-04 14:44:24,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-09-04 14:44:24,845] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-09-04 14:44:24,845] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-09-04 14:44:26,065] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-09-04 14:44:26,065] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1658 (0.1658)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5475 (2.5475 -- 2.5475)  data: 2.2943 (2.2943 -- 2.2943)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5409 (0.6536)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4242 (0.1994 -- 2.5475)  data: 0.2107 (0.0006 -- 2.2943)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4806 (0.5878)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2081 (0.1689 -- 0.2838)  data: 0.0063 (0.0001 -- 0.0946)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4931 (0.6226)  acc1: 88.8889 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.1951 (0.1323 -- 0.2838)  data: 0.0060 (0.0001 -- 0.0946)  max mem: 16413
Val: Total time: 0:00:07 (0.2848 s / it)
* Acc@1 82.158 Acc@5 97.510 loss 0.643
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 83.82%
Epoch: [80]  [  0/160]  eta: 0:20:45  lr: 0.000032  min_lr: 0.000008  loss: 2.0138 (2.0138)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9615 (6.9615)  time: 7.7815 (7.7815 -- 7.7815)  data: 5.9674 (5.9674 -- 5.9674)  max mem: 16413
Epoch: [80]  [ 20/160]  eta: 0:02:45  lr: 0.000032  min_lr: 0.000008  loss: 1.7229 (1.7563)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4076 (7.7442)  time: 0.8504 (0.5205 -- 4.3458)  data: 0.0081 (0.0003 -- 0.1201)  max mem: 16413
[2023-09-04 14:45:00,203] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:45:00,204] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:45:00,204] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:45:00,204] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [80]  [ 40/160]  eta: 0:02:01  lr: 0.000032  min_lr: 0.000008  loss: 1.6316 (1.7189)  loss_scale: 32768.0000 (23576.9756)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5183 (7.6108)  time: 0.8380 (0.5266 -- 3.3155)  data: 0.0358 (0.0003 -- 0.6690)  max mem: 16413
Epoch: [80]  [ 60/160]  eta: 0:01:37  lr: 0.000032  min_lr: 0.000008  loss: 1.6467 (1.7271)  loss_scale: 32768.0000 (26590.4262)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6636 (7.7013)  time: 0.8905 (0.5235 -- 2.6052)  data: 0.0747 (0.0002 -- 1.4717)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:17  lr: 0.000032  min_lr: 0.000008  loss: 1.7728 (1.7482)  loss_scale: 32768.0000 (28115.7531)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3983 (7.3649)  time: 0.9506 (0.5375 -- 4.5435)  data: 0.2914 (0.0009 -- 4.0135)  max mem: 16413
[2023-09-04 14:46:04,808] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12896
[2023-09-04 14:46:04,808] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12896
[2023-09-04 14:46:04,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:46:04,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:46:04,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [80]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000008  loss: 1.8273 (1.7523)  loss_scale: 32768.0000 (28225.9010)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0653 (7.1934)  time: 0.7807 (0.5159 -- 2.8737)  data: 0.2393 (0.0003 -- 2.3545)  max mem: 16413
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.7269 (1.7636)  loss_scale: 16384.0000 (26268.5620)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6452 (7.2254)  time: 0.8035 (0.5273 -- 2.9895)  data: 0.2546 (0.0004 -- 2.4383)  max mem: 16413
Epoch: [80]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.4730 (1.7333)  loss_scale: 16384.0000 (24866.4965)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5444 (7.2389)  time: 0.9221 (0.5255 -- 2.8239)  data: 0.1823 (0.0004 -- 2.0072)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.7954 (1.7389)  loss_scale: 16384.0000 (23859.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1726 (7.2262)  time: 0.7333 (0.4934 -- 1.9296)  data: 0.1237 (0.0002 -- 1.1684)  max mem: 16413
Epoch: [80] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.7954 (1.7392)  loss_scale: 16384.0000 (23859.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1726 (7.2262)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.1753 (0.1753)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1682 (2.1682 -- 2.1682)  data: 1.9428 (1.9428 -- 1.9428)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5037 (0.6161)  acc1: 77.7778 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4005 (0.2046 -- 2.1682)  data: 0.1811 (0.0005 -- 1.9428)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3686 (0.5555)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2224 (0.1735 -- 0.4494)  data: 0.0156 (0.0001 -- 0.2583)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4615 (0.5977)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2389 (0.1329 -- 0.8388)  data: 0.0486 (0.0001 -- 0.6643)  max mem: 16413
Val: Total time: 0:00:08 (0.3066 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.616
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 83.82%
Epoch: [81]  [  0/160]  eta: 0:21:31  lr: 0.000031  min_lr: 0.000008  loss: 2.0692 (2.0692)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.5652 (5.5652)  time: 8.0714 (8.0714 -- 8.0714)  data: 7.5439 (7.5439 -- 7.5439)  max mem: 16413
Epoch: [81]  [ 20/160]  eta: 0:02:49  lr: 0.000031  min_lr: 0.000008  loss: 1.7822 (1.7702)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6166 (6.8386)  time: 0.8681 (0.5293 -- 3.2132)  data: 0.1329 (0.0005 -- 1.1818)  max mem: 16413
[2023-09-04 14:47:46,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=72, lr=[7.961979027681322e-06, 7.961979027681322e-06, 8.846643364090358e-06, 8.846643364090358e-06, 9.829603737878174e-06, 9.829603737878174e-06, 1.0921781930975751e-05, 1.0921781930975751e-05, 1.2135313256639722e-05, 1.2135313256639722e-05, 1.3483681396266359e-05, 1.3483681396266359e-05, 1.498186821807373e-05, 1.498186821807373e-05, 1.6646520242304144e-05, 1.6646520242304144e-05, 1.849613360256016e-05, 1.849613360256016e-05, 2.0551259558400174e-05, 2.0551259558400174e-05, 2.2834732842666863e-05, 2.2834732842666863e-05, 2.537192538074096e-05, 2.537192538074096e-05, 2.8191028200823287e-05, 2.8191028200823287e-05, 3.132336466758143e-05, 3.132336466758143e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 14:47:46,316] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=17.646531779460382, CurrSamplesPerSec=22.525296157891205, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:03  lr: 0.000031  min_lr: 0.000008  loss: 1.4199 (1.6753)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.2867 (6.8882)  time: 0.8332 (0.5215 -- 3.8021)  data: 0.0497 (0.0005 -- 0.8709)  max mem: 16413
Epoch: [81]  [ 60/160]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000008  loss: 1.9087 (1.7295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1809 (6.8510)  time: 0.9295 (0.5275 -- 2.8664)  data: 0.2790 (0.0005 -- 2.3154)  max mem: 16413
[2023-09-04 14:48:09,786] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:48:09,786] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:48:09,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:48:09,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [81]  [ 80/160]  eta: 0:01:18  lr: 0.000031  min_lr: 0.000008  loss: 1.7648 (1.7097)  loss_scale: 32768.0000 (19620.3457)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0293 (6.6386)  time: 0.9309 (0.5170 -- 4.3061)  data: 0.3631 (0.0003 -- 3.8014)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000008  loss: 1.7848 (1.7134)  loss_scale: 32768.0000 (22223.8416)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6430 (6.7678)  time: 0.8227 (0.5282 -- 2.2424)  data: 0.1903 (0.0003 -- 1.7034)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.5548 (1.6937)  loss_scale: 32768.0000 (23966.6777)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4820 (6.7728)  time: 0.7989 (0.5216 -- 3.2553)  data: 0.0018 (0.0007 -- 0.0052)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.7099 (1.6959)  loss_scale: 32768.0000 (25215.0922)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0296 (6.8738)  time: 0.9470 (0.5137 -- 4.6995)  data: 0.0416 (0.0003 -- 0.7928)  max mem: 16413
[2023-09-04 14:49:23,844] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13108
[2023-09-04 14:49:23,845] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:49:23,845] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 14:49:23,845] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13108
[2023-09-04 14:49:23,845] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.5997 (1.6836)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7549 (6.9188)  time: 0.7570 (0.4870 -- 4.6713)  data: 0.0008 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [81] Total time: 0:02:25 (0.9081 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.5997 (1.6845)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7549 (6.9188)
Val:  [ 0/27]  eta: 0:00:55  loss: 0.2975 (0.2975)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.0590 (2.0590 -- 2.0590)  data: 1.8344 (1.8344 -- 1.8344)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4537 (0.6573)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.3982 (0.1860 -- 2.0590)  data: 0.1851 (0.0005 -- 1.8344)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4447 (0.5918)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2295 (0.1691 -- 0.3844)  data: 0.0253 (0.0001 -- 0.1782)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4908 (0.6416)  acc1: 88.8889 (80.9129)  acc5: 100.0000 (97.0954)  time: 0.2147 (0.1327 -- 0.3844)  data: 0.0250 (0.0001 -- 0.1782)  max mem: 16413
Val: Total time: 0:00:07 (0.2827 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.641
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.82%
Epoch: [82]  [  0/160]  eta: 0:22:20  lr: 0.000031  min_lr: 0.000008  loss: 2.1374 (2.1374)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.7741 (7.7741)  time: 8.3777 (8.3777 -- 8.3777)  data: 5.1380 (5.1380 -- 5.1380)  max mem: 16413
Epoch: [82]  [ 20/160]  eta: 0:02:46  lr: 0.000031  min_lr: 0.000008  loss: 1.8030 (1.7841)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7968 (6.5204)  time: 0.8304 (0.5226 -- 2.3949)  data: 0.1975 (0.0004 -- 1.8797)  max mem: 16413
Epoch: [82]  [ 40/160]  eta: 0:02:09  lr: 0.000031  min_lr: 0.000008  loss: 1.7233 (1.7414)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8255 (7.0390)  time: 0.9671 (0.5258 -- 3.5223)  data: 0.4146 (0.0006 -- 2.9771)  max mem: 16413
Epoch: [82]  [ 60/160]  eta: 0:01:37  lr: 0.000031  min_lr: 0.000008  loss: 1.6410 (1.7155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.5993 (7.0986)  time: 0.7469 (0.5281 -- 2.8497)  data: 0.2008 (0.0002 -- 2.3174)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:14  lr: 0.000031  min_lr: 0.000008  loss: 1.7595 (1.7372)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.1254 (7.1719)  time: 0.7984 (0.5155 -- 2.0877)  data: 0.1424 (0.0003 -- 1.3626)  max mem: 16413
Epoch: [82]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000008  loss: 1.8761 (1.7555)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7754 (7.1387)  time: 0.9875 (0.5178 -- 4.1571)  data: 0.0016 (0.0002 -- 0.0036)  max mem: 16413
[2023-09-04 14:51:26,016] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:51:26,017] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:51:26,017] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:51:26,017] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.6891 (1.7351)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4695 (7.0853)  time: 0.7254 (0.5284 -- 2.4508)  data: 0.0018 (0.0003 -- 0.0077)  max mem: 16413
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.5542 (1.7162)  loss_scale: 32768.0000 (19172.7660)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6850 (7.0936)  time: 1.0415 (0.5274 -- 3.4811)  data: 0.0028 (0.0004 -- 0.0152)  max mem: 16413
[2023-09-04 14:51:55,339] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13269
[2023-09-04 14:51:55,339] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:51:55,339] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13269
[2023-09-04 14:51:55,339] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:51:55,339] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.5943 (1.7094)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8875 (6.9888)  time: 0.7644 (0.4891 -- 3.4811)  data: 0.0008 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [82] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.5943 (1.7199)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.8875 (6.9888)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2457 (0.2457)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2625 (2.2625 -- 2.2625)  data: 2.0645 (2.0645 -- 2.0645)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5011 (0.6556)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4127 (0.2032 -- 2.2625)  data: 0.1944 (0.0003 -- 2.0645)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5011 (0.5999)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (97.3545)  time: 0.2231 (0.1726 -- 0.3126)  data: 0.0123 (0.0001 -- 0.0858)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5106 (0.6324)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (96.6805)  time: 0.2056 (0.1323 -- 0.3126)  data: 0.0120 (0.0001 -- 0.0858)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 81.743 Acc@5 96.888 loss 0.646
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 83.82%
Epoch: [83]  [  0/160]  eta: 0:21:28  lr: 0.000031  min_lr: 0.000008  loss: 1.7152 (1.7152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9080 (5.9080)  time: 8.0520 (8.0520 -- 8.0520)  data: 7.5331 (7.5331 -- 7.5331)  max mem: 16413
Epoch: [83]  [ 20/160]  eta: 0:02:44  lr: 0.000031  min_lr: 0.000008  loss: 1.6332 (1.7217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7311 (7.3555)  time: 0.8336 (0.5238 -- 3.5760)  data: 0.2855 (0.0003 -- 3.0031)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:01:57  lr: 0.000031  min_lr: 0.000008  loss: 1.5836 (1.6462)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5404 (7.1244)  time: 0.7726 (0.5221 -- 1.8951)  data: 0.1226 (0.0005 -- 1.3210)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000008  loss: 1.8508 (1.6963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9000 (7.1090)  time: 1.0160 (0.5311 -- 3.8939)  data: 0.0514 (0.0005 -- 0.8814)  max mem: 16413
Epoch: [83]  [ 80/160]  eta: 0:01:16  lr: 0.000031  min_lr: 0.000008  loss: 1.8299 (1.7029)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6698 (6.8313)  time: 0.8548 (0.5199 -- 2.8063)  data: 0.1898 (0.0005 -- 2.2779)  max mem: 16413
Epoch: [83]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000008  loss: 1.9405 (1.7448)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.2499 (6.8256)  time: 0.9112 (0.5235 -- 3.4159)  data: 0.2691 (0.0004 -- 2.8733)  max mem: 16413
[2023-09-04 14:54:00,905] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:54:00,905] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:54:00,906] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:54:00,906] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [83]  [120/160]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000008  loss: 1.5499 (1.7209)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9808 (6.8477)  time: 0.8989 (0.5110 -- 5.0393)  data: 0.3481 (0.0003 -- 4.5223)  max mem: 16413
[2023-09-04 14:54:15,062] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13415
[2023-09-04 14:54:15,062] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13415
[2023-09-04 14:54:15,062] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:54:15,062] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:54:15,062] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.4350 (1.7001)  loss_scale: 32768.0000 (18359.3759)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5446 (6.8749)  time: 0.9459 (0.5145 -- 3.6962)  data: 0.4106 (0.0005 -- 3.1721)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.8449 (1.7134)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9327 (6.9244)  time: 0.6686 (0.4933 -- 3.3856)  data: 0.1556 (0.0002 -- 2.8803)  max mem: 16413
Epoch: [83] Total time: 0:02:25 (0.9098 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.8449 (1.7302)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9327 (6.9244)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2308 (0.2308)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3841 (2.3841 -- 2.3841)  data: 2.1620 (2.1620 -- 2.1620)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3601 (0.5661)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4136 (0.1974 -- 2.3841)  data: 0.1979 (0.0004 -- 2.1620)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5141 (0.5619)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2148 (0.1687 -- 0.3295)  data: 0.0112 (0.0001 -- 0.1364)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5787 (0.5948)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.1996 (0.1326 -- 0.3295)  data: 0.0108 (0.0001 -- 0.1364)  max mem: 16413
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 84.025 Acc@5 97.303 loss 0.613
Accuracy of the network on the 482 val images: 84.02%
[2023-09-04 14:54:41,386] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 14:54:41,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 14:54:41,388] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 14:54:41,388] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 14:54:42,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 14:54:42,875] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.02%
Epoch: [84]  [  0/160]  eta: 0:21:26  lr: 0.000030  min_lr: 0.000008  loss: 1.7795 (1.7795)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4418 (6.4418)  time: 8.0405 (8.0405 -- 8.0405)  data: 7.5177 (7.5177 -- 7.5177)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:54  lr: 0.000030  min_lr: 0.000008  loss: 1.5404 (1.5768)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4485 (6.6227)  time: 0.9031 (0.5336 -- 4.1606)  data: 0.3475 (0.0004 -- 3.6421)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:03  lr: 0.000030  min_lr: 0.000008  loss: 1.8393 (1.6611)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5055 (6.7799)  time: 0.8069 (0.5318 -- 2.5688)  data: 0.2547 (0.0002 -- 1.9999)  max mem: 16413
Epoch: [84]  [ 60/160]  eta: 0:01:38  lr: 0.000030  min_lr: 0.000008  loss: 1.8437 (1.7065)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7206 (7.0234)  time: 0.8913 (0.5243 -- 3.6289)  data: 0.3396 (0.0004 -- 3.0354)  max mem: 16413
Epoch: [84]  [ 80/160]  eta: 0:01:16  lr: 0.000030  min_lr: 0.000008  loss: 1.6262 (1.6861)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6351 (6.9343)  time: 0.8571 (0.5173 -- 3.0212)  data: 0.2612 (0.0003 -- 2.4863)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000008  loss: 1.6041 (1.6751)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.6855 (6.7000)  time: 0.8563 (0.5251 -- 2.5891)  data: 0.0560 (0.0005 -- 0.6496)  max mem: 16413
[2023-09-04 14:56:20,190] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:56:20,190] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:56:20,190] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 14:56:20,190] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [84]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000008  loss: 1.7588 (1.6902)  loss_scale: 32768.0000 (18685.8843)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.2924 (6.6189)  time: 0.8052 (0.5267 -- 2.2825)  data: 0.0677 (0.0003 -- 0.7560)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.7483 (1.6919)  loss_scale: 32768.0000 (20683.3475)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3237 (6.7413)  time: 0.9057 (0.5280 -- 3.1992)  data: 0.0266 (0.0001 -- 0.4871)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.6913 (1.6825)  loss_scale: 32768.0000 (22118.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7362 (6.6651)  time: 0.6730 (0.4939 -- 2.1145)  data: 0.0495 (0.0002 -- 0.9756)  max mem: 16413
Epoch: [84] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.6913 (1.7201)  loss_scale: 32768.0000 (22118.4000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7362 (6.6651)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2415 (0.2415)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2974 (2.2974 -- 2.2974)  data: 2.0652 (2.0652 -- 2.0652)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4768 (0.6248)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4170 (0.2099 -- 2.2974)  data: 0.1926 (0.0006 -- 2.0652)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4088 (0.5717)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (97.8836)  time: 0.2143 (0.1705 -- 0.2767)  data: 0.0029 (0.0001 -- 0.0430)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4768 (0.6025)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (97.0954)  time: 0.1959 (0.1333 -- 0.2767)  data: 0.0025 (0.0001 -- 0.0430)  max mem: 16413
Val: Total time: 0:00:07 (0.2804 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.623
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.02%
Epoch: [85]  [  0/160]  eta: 0:21:27  lr: 0.000030  min_lr: 0.000008  loss: 1.3481 (1.3481)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.7807 (5.7807)  time: 8.0442 (8.0442 -- 8.0442)  data: 6.9431 (6.9431 -- 6.9431)  max mem: 16413
Epoch: [85]  [ 20/160]  eta: 0:02:41  lr: 0.000030  min_lr: 0.000008  loss: 1.7774 (1.7032)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4059 (6.5507)  time: 0.8099 (0.5301 -- 2.4457)  data: 0.0219 (0.0004 -- 0.3142)  max mem: 16413
Epoch: [85]  [ 40/160]  eta: 0:02:00  lr: 0.000030  min_lr: 0.000008  loss: 1.7844 (1.6893)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.4510 (6.5548)  time: 0.8432 (0.5319 -- 3.0217)  data: 0.0568 (0.0005 -- 1.1061)  max mem: 16413
Epoch: [85]  [ 60/160]  eta: 0:01:36  lr: 0.000030  min_lr: 0.000008  loss: 1.4027 (1.6348)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.3344 (6.4331)  time: 0.8873 (0.5217 -- 2.9248)  data: 0.0345 (0.0005 -- 0.5357)  max mem: 16413
[2023-09-04 14:58:22,651] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:58:22,651] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 14:58:22,651] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 14:58:22,651] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [85]  [ 80/160]  eta: 0:01:16  lr: 0.000030  min_lr: 0.000008  loss: 1.8880 (1.6763)  loss_scale: 32768.0000 (36408.8889)  weight_decay: 0.1000 (0.1000)  grad_norm: 5.9764 (6.3374)  time: 0.9052 (0.5297 -- 4.0407)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
[2023-09-04 14:58:38,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13691
[2023-09-04 14:58:38,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13691
[2023-09-04 14:58:38,622] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 14:58:38,622] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 14:58:38,622] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [85]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000008  loss: 1.5581 (1.6435)  loss_scale: 32768.0000 (38932.2772)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1584 (6.3180)  time: 0.8034 (0.5349 -- 3.3552)  data: 0.0017 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [85]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000008  loss: 1.6840 (1.6577)  loss_scale: 32768.0000 (37913.3884)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.7919 (6.5573)  time: 0.8907 (0.5259 -- 3.1829)  data: 0.0599 (0.0004 -- 1.0813)  max mem: 16413
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.6765 (1.6650)  loss_scale: 32768.0000 (37183.5461)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.9000 (6.6809)  time: 0.8878 (0.5363 -- 2.3672)  data: 0.0874 (0.0005 -- 0.4577)  max mem: 16413
[2023-09-04 14:59:32,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13754
[2023-09-04 14:59:32,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:59:32,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13754
[2023-09-04 14:59:32,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 14:59:32,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.8132 (1.6844)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5634 (6.7910)  time: 0.7132 (0.4902 -- 2.4980)  data: 0.0008 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [85] Total time: 0:02:22 (0.8897 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.8132 (1.7204)  loss_scale: 32768.0000 (36044.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.5634 (6.7910)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2091 (0.2091)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1934 (2.1934 -- 2.1934)  data: 1.9676 (1.9676 -- 1.9676)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3468 (0.5716)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4032 (0.2014 -- 2.1934)  data: 0.1805 (0.0008 -- 1.9676)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3987 (0.5369)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2291 (0.1731 -- 0.4319)  data: 0.0155 (0.0001 -- 0.2257)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5237 (0.5909)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.5104)  time: 0.2102 (0.1351 -- 0.4319)  data: 0.0151 (0.0001 -- 0.2257)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 82.988 Acc@5 97.303 loss 0.618
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.02%
Epoch: [86]  [  0/160]  eta: 0:20:23  lr: 0.000030  min_lr: 0.000008  loss: 1.9226 (1.9226)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0787 (6.0787)  time: 7.6452 (7.6452 -- 7.6452)  data: 7.1205 (7.1205 -- 7.1205)  max mem: 16413
Epoch: [86]  [ 20/160]  eta: 0:03:01  lr: 0.000030  min_lr: 0.000008  loss: 1.7258 (1.7505)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.6560 (7.0685)  time: 0.9762 (0.5147 -- 3.3459)  data: 0.0509 (0.0003 -- 0.7641)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:10  lr: 0.000030  min_lr: 0.000008  loss: 1.6213 (1.6861)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7582 (7.0109)  time: 0.8790 (0.5162 -- 4.4828)  data: 0.0016 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:42  lr: 0.000029  min_lr: 0.000007  loss: 1.6236 (1.6902)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.3612 (7.2972)  time: 0.8883 (0.5244 -- 5.0727)  data: 0.0012 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [86]  [ 80/160]  eta: 0:01:16  lr: 0.000029  min_lr: 0.000007  loss: 1.7966 (1.7228)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.7137 (7.4599)  time: 0.7478 (0.5254 -- 2.8228)  data: 0.0020 (0.0010 -- 0.0043)  max mem: 16413
Epoch: [86]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000007  loss: 1.6177 (1.7080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.7759 (7.4072)  time: 0.8306 (0.5237 -- 3.8633)  data: 0.0350 (0.0003 -- 0.6493)  max mem: 16413
Epoch: [86]  [120/160]  eta: 0:00:38  lr: 0.000029  min_lr: 0.000007  loss: 1.8878 (1.7333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 7.0363 (7.3790)  time: 1.0544 (0.5320 -- 3.9736)  data: 0.5059 (0.0005 -- 3.4331)  max mem: 16413
[2023-09-04 15:01:40,664] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:01:40,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 15:01:40,667] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 15:01:40,667] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.6412 (1.7177)  loss_scale: 32768.0000 (18475.5745)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.8570 (7.3040)  time: 0.8739 (0.5302 -- 4.1469)  data: 0.3250 (0.0004 -- 3.6375)  max mem: 16413
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.6622 (1.7108)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0747 (7.1995)  time: 0.6584 (0.4935 -- 3.3347)  data: 0.1417 (0.0001 -- 2.8186)  max mem: 16413
Epoch: [86] Total time: 0:02:25 (0.9083 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.6622 (1.6829)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.0747 (7.1995)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1946 (0.1946)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3982 (2.3982 -- 2.3982)  data: 2.1799 (2.1799 -- 2.1799)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4759 (0.5806)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4293 (0.2001 -- 2.3982)  data: 0.2191 (0.0006 -- 2.1799)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4607 (0.5300)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2160 (0.1692 -- 0.4234)  data: 0.0145 (0.0001 -- 0.2176)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4863 (0.5971)  acc1: 85.7143 (82.5726)  acc5: 100.0000 (97.5104)  time: 0.2014 (0.1332 -- 0.4234)  data: 0.0141 (0.0001 -- 0.2176)  max mem: 16413
Val: Total time: 0:00:07 (0.2856 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.608
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.02%
Epoch: [87]  [  0/160]  eta: 0:21:16  lr: 0.000029  min_lr: 0.000007  loss: 1.9977 (1.9977)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.1000 (0.1000)  grad_norm: 6.1378 (6.1378)  time: 7.9811 (7.9811 -- 7.9811)  data: 7.4370 (7.4370 -- 7.4370)  max mem: 16413
configs/vit_b_k710.sh: line 44: 2530521 Killed                  OMP_NUM_THREADS=1 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node=${GPUS_PER_NODE} --master_port 12320 --nnodes=1 --node_rank=0 --master_addr=localhost run_class_finetuning.py --model vit_base_patch16_224 --data_set AI-City-Track-3 --nb_classes 16 --data_path ${DATA_PATH} --finetune ${MODEL_PATH} --log_dir ${OUTPUT_DIR} --output_dir ${OUTPUT_DIR} --batch_size 6 --input_size 224 --short_side_size 224 --save_ckpt_freq 20 --num_frames 16 --sampling_rate 4 --num_sample 2 --num_workers 8 --opt adamw --lr 1e-3 --drop_path 0.1 --head_drop_rate 0.0 --layer_decay 0.9 --weight_decay 0.1 --opt_betas 0.9 0.999 --warmup_epochs 5 --epochs 200 --test_num_segment 5 --test_num_crop 3 --dist_eval --enable_deepspeed
