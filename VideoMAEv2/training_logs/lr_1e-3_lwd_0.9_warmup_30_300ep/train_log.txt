[2023-09-04 21:31:25,294] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-04 21:31:25,451] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=300, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.9, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=30, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f9f925460a0>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.2541865828329001, 0.2824295364810001, 0.31381059609000006, 0.3486784401000001, 0.3874204890000001, 0.4304672100000001, 0.4782969000000001, 0.531441, 0.5904900000000001, 0.6561, 0.7290000000000001, 0.81, 0.9, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.2541865828329001
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.81
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.81
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.9
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.9
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-09-04 21:31:30,496] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-04 21:31:30,496] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-04 21:31:30,801] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-09-04 21:31:30,801] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-09-04 21:31:30,907] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.47597360610961914 seconds
[2023-09-04 21:31:32,001] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-09-04 21:31:32,011] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-09-04 21:31:32,011] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-09-04 21:31:32,039] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-09-04 21:31:32,040] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-04 21:31:32,040] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-04 21:31:32,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 21:31:32,041] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-04 21:31:32,041] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-04 21:31:32,042] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-04 21:31:32,042] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-04 21:31:32,042] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-04 21:31:32,042] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-04 21:31:32,042] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9f3ba18400>
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-04 21:31:32,043] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-09-04 21:31:32,044] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-09-04 21:31:32,045] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-04 21:31:32,046] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-04 21:31:32,046] [INFO] [config.py:964:print]   world_size ................... 2
[2023-09-04 21:31:32,046] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-09-04 21:31:32,046] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-09-04 21:31:32,046] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-09-04 21:31:32,046] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-04 21:31:32,046] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-09-04 21:31:32,046] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 4800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 300 epochs
Epoch: [0]  [  0/160]  eta: 0:31:31  lr: 0.000000  min_lr: 0.000000  loss: 2.7734 (2.7734)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 11.8224 (11.8224 -- 11.8224)  data: 6.2220 (6.2220 -- 6.2220)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:48  lr: 0.000000  min_lr: 0.000000  loss: 2.7731 (2.7732)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5932 (1.6020)  time: 0.6704 (0.5054 -- 3.0619)  data: 0.0018 (0.0005 -- 0.0045)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:16  lr: 0.000000  min_lr: 0.000000  loss: 2.7730 (2.7731)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4587 (1.5416)  time: 1.0650 (0.5109 -- 4.1506)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:41  lr: 0.000001  min_lr: 0.000000  loss: 2.7729 (2.7730)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4179 (1.5246)  time: 0.7668 (0.5093 -- 3.5617)  data: 0.0011 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:21  lr: 0.000001  min_lr: 0.000000  loss: 2.7727 (2.7729)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4506 (1.5166)  time: 1.0280 (0.5119 -- 5.5549)  data: 0.0024 (0.0004 -- 0.0164)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 2.7726 (2.7729)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4780 (1.5100)  time: 0.6398 (0.5067 -- 2.4622)  data: 0.0019 (0.0002 -- 0.0115)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 2.7725 (2.7728)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4889 (1.5103)  time: 0.9833 (0.5131 -- 4.1937)  data: 0.0149 (0.0004 -- 0.2733)  max mem: 16413
[2023-09-04 21:33:32,921] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:33:32,921] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-09-04 21:33:32,921] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:33:32,962] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 2.7723 (2.7727)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4382 (1.5000)  time: 0.7807 (0.5011 -- 2.7444)  data: 0.0248 (0.0003 -- 0.4618)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 2.7722 (2.7727)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3381 (1.4861)  time: 0.7037 (0.4858 -- 2.9852)  data: 0.0271 (0.0002 -- 0.5304)  max mem: 16413
Epoch: [0] Total time: 0:02:24 (0.9005 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 2.7722 (2.7727)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3381 (1.4861)
Val:  [ 0/27]  eta: 0:01:18  loss: 2.7717 (2.7717)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 2.9184 (2.9184 -- 2.9184)  data: 2.5484 (2.5484 -- 2.5484)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7717 (2.7716)  acc1: 22.2222 (28.2828)  acc5: 66.6667 (72.7273)  time: 0.4605 (0.1876 -- 2.9184)  data: 0.2342 (0.0006 -- 2.5484)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7715 (2.7716)  acc1: 22.2222 (29.6296)  acc5: 66.6667 (74.6032)  time: 0.2135 (0.1681 -- 0.3104)  data: 0.0115 (0.0001 -- 0.1158)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7715 (2.7716)  acc1: 22.2222 (29.4606)  acc5: 66.6667 (73.0290)  time: 0.2009 (0.1681 -- 0.3104)  data: 0.0105 (0.0001 -- 0.1158)  max mem: 16413
Val: Total time: 0:00:08 (0.3042 s / it)
* Acc@1 30.290 Acc@5 73.859 loss 2.772
Accuracy of the network on the 482 val images: 30.29%
[2023-09-04 21:34:04,563] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-09-04 21:34:04,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 21:34:04,565] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 21:34:04,566] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 21:34:05,580] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 21:34:05,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 30.29%
Epoch: [1]  [  0/160]  eta: 0:20:24  lr: 0.000002  min_lr: 0.000000  loss: 2.7721 (2.7721)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7695 (1.7695)  time: 7.6522 (7.6522 -- 7.6522)  data: 7.1249 (7.1249 -- 7.1249)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:36  lr: 0.000002  min_lr: 0.000000  loss: 2.7722 (2.7721)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4653 (1.5049)  time: 0.7918 (0.5194 -- 2.9461)  data: 0.2298 (0.0006 -- 2.4348)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 2.7721 (2.7721)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5278 (1.5104)  time: 0.9455 (0.5178 -- 3.6390)  data: 0.1867 (0.0007 -- 2.1058)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000001  loss: 2.7717 (2.7720)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4777 (1.5206)  time: 0.8950 (0.5221 -- 3.8434)  data: 0.0291 (0.0002 -- 0.2779)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000001  loss: 2.7716 (2.7719)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5185 (1.5080)  time: 0.8046 (0.5086 -- 3.2530)  data: 0.0206 (0.0005 -- 0.3775)  max mem: 16413
[2023-09-04 21:35:36,044] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:35:36,044] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-09-04 21:35:36,047] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:35:36,047] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000001  loss: 2.7711 (2.7717)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5011 (1.5102)  time: 0.9632 (0.5204 -- 2.9543)  data: 0.2459 (0.0003 -- 2.4373)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 2.7707 (2.7716)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4803 (1.4992)  time: 0.7518 (0.5139 -- 2.8005)  data: 0.2143 (0.0002 -- 2.2719)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 2.7697 (2.7713)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4865 (1.5007)  time: 0.9662 (0.5184 -- 2.9609)  data: 0.4264 (0.0002 -- 2.3939)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 2.7695 (2.7711)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5121 (1.5030)  time: 0.7606 (0.4919 -- 3.0794)  data: 0.2427 (0.0002 -- 2.5828)  max mem: 16413
Epoch: [1] Total time: 0:02:22 (0.8903 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 2.7695 (2.7711)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5121 (1.5030)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.7654 (2.7654)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 2.3369 (2.3369 -- 2.3369)  data: 2.1080 (2.1080 -- 2.1080)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7656 (2.7658)  acc1: 33.3333 (35.3535)  acc5: 88.8889 (84.8485)  time: 0.4175 (0.2065 -- 2.3369)  data: 0.1999 (0.0006 -- 2.1080)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7654 (2.7655)  acc1: 33.3333 (37.5661)  acc5: 88.8889 (85.7143)  time: 0.2209 (0.1698 -- 0.4094)  data: 0.0166 (0.0001 -- 0.2381)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7658 (2.7658)  acc1: 33.3333 (39.4191)  acc5: 88.8889 (84.6473)  time: 0.2046 (0.1335 -- 0.4094)  data: 0.0162 (0.0001 -- 0.2381)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 40.249 Acc@5 83.610 loss 2.766
Accuracy of the network on the 482 val images: 40.25%
[2023-09-04 21:36:35,777] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 21:36:35,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 21:36:35,779] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 21:36:35,779] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 21:36:37,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 21:36:37,284] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 40.25%
Epoch: [2]  [  0/160]  eta: 0:23:26  lr: 0.000003  min_lr: 0.000001  loss: 2.7716 (2.7716)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3599 (1.3599)  time: 8.7922 (8.7922 -- 8.7922)  data: 8.2628 (8.2628 -- 8.2628)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:42  lr: 0.000003  min_lr: 0.000001  loss: 2.7684 (2.7686)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5159 (1.5184)  time: 0.7776 (0.5306 -- 4.1009)  data: 0.2329 (0.0005 -- 3.5885)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:09  lr: 0.000004  min_lr: 0.000001  loss: 2.7669 (2.7681)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5501 (1.5346)  time: 0.9905 (0.5130 -- 4.9210)  data: 0.4504 (0.0006 -- 4.3356)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000001  loss: 2.7668 (2.7676)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4845 (1.5243)  time: 0.7445 (0.5118 -- 3.2249)  data: 0.2039 (0.0002 -- 2.6936)  max mem: 16413
[2023-09-04 21:37:40,567] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:37:40,568] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-09-04 21:37:40,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:37:40,568] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000001  loss: 2.7655 (2.7671)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4825 (1.5144)  time: 0.8727 (0.5225 -- 3.8754)  data: 0.3266 (0.0004 -- 3.3356)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000001  loss: 2.7644 (2.7667)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4940 (1.5198)  time: 0.9338 (0.5257 -- 2.7061)  data: 0.1417 (0.0003 -- 1.4370)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000001  loss: 2.7630 (2.7660)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5140 (1.5263)  time: 0.8046 (0.5233 -- 3.0623)  data: 0.2318 (0.0003 -- 2.5149)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:17  lr: 0.000004  min_lr: 0.000001  loss: 2.7585 (2.7651)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4780 (1.5239)  time: 0.7640 (0.5334 -- 2.7112)  data: 0.2107 (0.0003 -- 2.1845)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 2.7579 (2.7643)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5066 (1.5250)  time: 0.7232 (0.4937 -- 2.2260)  data: 0.1639 (0.0002 -- 1.6999)  max mem: 16413
Epoch: [2] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 2.7579 (2.7642)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5066 (1.5250)
Val:  [ 0/27]  eta: 0:01:09  loss: 2.7420 (2.7420)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.5626 (2.5626 -- 2.5626)  data: 2.2561 (2.2561 -- 2.2561)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7444 (2.7451)  acc1: 33.3333 (37.3737)  acc5: 100.0000 (89.8990)  time: 0.4477 (0.1955 -- 2.5626)  data: 0.2281 (0.0005 -- 2.2561)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7426 (2.7437)  acc1: 44.4444 (39.1534)  acc5: 88.8889 (90.4762)  time: 0.2143 (0.1701 -- 0.4579)  data: 0.0128 (0.0001 -- 0.2396)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7441 (2.7449)  acc1: 44.4444 (40.2490)  acc5: 88.8889 (88.7967)  time: 0.2006 (0.1358 -- 0.4579)  data: 0.0123 (0.0001 -- 0.2396)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 42.739 Acc@5 86.307 loss 2.744
Accuracy of the network on the 482 val images: 42.74%
[2023-09-04 21:39:05,631] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 21:39:05,632] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 21:39:05,632] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 21:39:05,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 21:39:07,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 21:39:07,040] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 42.74%
Epoch: [3]  [  0/160]  eta: 0:16:33  lr: 0.000005  min_lr: 0.000001  loss: 2.7548 (2.7548)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7464 (1.7464)  time: 6.2109 (6.2109 -- 6.2109)  data: 4.9811 (4.9811 -- 4.9811)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:38  lr: 0.000005  min_lr: 0.000001  loss: 2.7583 (2.7585)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6121 (1.6204)  time: 0.8762 (0.5219 -- 2.8233)  data: 0.2325 (0.0004 -- 1.6400)  max mem: 16413
[2023-09-04 21:39:43,212] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:39:43,212] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-09-04 21:39:43,214] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:39:43,215] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:03  lr: 0.000005  min_lr: 0.000001  loss: 2.7535 (2.7563)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4796 (1.5521)  time: 0.9283 (0.5389 -- 3.5354)  data: 0.3462 (0.0008 -- 2.9798)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:37  lr: 0.000005  min_lr: 0.000001  loss: 2.7483 (2.7537)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5383 (1.5588)  time: 0.8688 (0.5195 -- 3.7223)  data: 0.3275 (0.0004 -- 3.1798)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:17  lr: 0.000005  min_lr: 0.000001  loss: 2.7490 (2.7524)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6212 (1.5820)  time: 0.9407 (0.5273 -- 3.8017)  data: 0.2795 (0.0003 -- 2.2512)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000001  loss: 2.7503 (2.7513)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5317 (1.5768)  time: 0.8490 (0.5115 -- 4.0389)  data: 0.0014 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000001  loss: 2.7405 (2.7495)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5877 (1.5839)  time: 0.7795 (0.5247 -- 2.4887)  data: 0.0566 (0.0004 -- 1.0557)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000002  loss: 2.7391 (2.7480)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6274 (1.5898)  time: 1.0010 (0.5297 -- 4.3690)  data: 0.3234 (0.0003 -- 3.8284)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 2.7404 (2.7467)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5601 (1.5858)  time: 0.7727 (0.4920 -- 3.1971)  data: 0.2574 (0.0002 -- 2.7074)  max mem: 16413
Epoch: [3] Total time: 0:02:23 (0.8987 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 2.7404 (2.7466)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5601 (1.5858)
Val:  [ 0/27]  eta: 0:01:05  loss: 2.6912 (2.6912)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.4200 (2.4200 -- 2.4200)  data: 2.1824 (2.1824 -- 2.1824)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7040 (2.7038)  acc1: 33.3333 (37.3737)  acc5: 88.8889 (85.8586)  time: 0.4155 (0.1985 -- 2.4200)  data: 0.2003 (0.0008 -- 2.1824)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6988 (2.6998)  acc1: 44.4444 (38.6243)  acc5: 88.8889 (87.8307)  time: 0.2106 (0.1705 -- 0.2927)  data: 0.0062 (0.0001 -- 0.0995)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6988 (2.7024)  acc1: 44.4444 (39.0041)  acc5: 77.7778 (85.0622)  time: 0.1946 (0.1331 -- 0.2927)  data: 0.0054 (0.0001 -- 0.0995)  max mem: 16413
Val: Total time: 0:00:07 (0.2825 s / it)
* Acc@1 40.871 Acc@5 84.647 loss 2.702
Accuracy of the network on the 482 val images: 40.87%
Max accuracy: 42.74%
[2023-09-04 21:41:43,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:41:43,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:41:44,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-09-04 21:41:44,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:14:37  lr: 0.000006  min_lr: 0.000002  loss: 2.7232 (2.7232)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9036 (1.9036)  time: 5.4846 (5.4846 -- 5.4846)  data: 4.8833 (4.8833 -- 4.8833)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:43  lr: 0.000006  min_lr: 0.000002  loss: 2.7250 (2.7259)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6777 (1.6448)  time: 0.9499 (0.5304 -- 3.0470)  data: 0.2473 (0.0004 -- 2.5077)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:02:02  lr: 0.000007  min_lr: 0.000002  loss: 2.7231 (2.7246)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5855 (1.6539)  time: 0.8688 (0.5199 -- 3.1408)  data: 0.2205 (0.0005 -- 2.6026)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:37  lr: 0.000007  min_lr: 0.000002  loss: 2.7176 (2.7208)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6199 (1.6616)  time: 0.8845 (0.5327 -- 2.9637)  data: 0.2258 (0.0006 -- 2.4290)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:14  lr: 0.000007  min_lr: 0.000002  loss: 2.7132 (2.7186)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6476 (1.6709)  time: 0.7939 (0.5298 -- 1.8651)  data: 0.0634 (0.0006 -- 0.7842)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000002  loss: 2.7183 (2.7167)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5642 (1.6644)  time: 0.8864 (0.5212 -- 3.4958)  data: 0.2506 (0.0003 -- 2.9686)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000002  loss: 2.7024 (2.7134)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6754 (1.6681)  time: 0.9304 (0.5159 -- 3.7386)  data: 0.0332 (0.0003 -- 0.6351)  max mem: 16413
[2023-09-04 21:43:36,483] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:43:36,484] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-09-04 21:43:36,484] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:43:36,484] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 2.7131 (2.7131)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6315 (1.6637)  time: 1.0120 (0.5121 -- 3.9115)  data: 0.0951 (0.0005 -- 1.2842)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 2.7067 (2.7121)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5787 (1.6547)  time: 0.7082 (0.4925 -- 3.9115)  data: 0.0006 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [4] Total time: 0:02:22 (0.8893 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 2.7067 (2.7148)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5787 (1.6547)
Val:  [ 0/27]  eta: 0:01:08  loss: 2.6120 (2.6120)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.5452 (2.5452 -- 2.5452)  data: 2.3161 (2.3161 -- 2.3161)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.6432 (2.6374)  acc1: 44.4444 (41.4141)  acc5: 88.8889 (87.8788)  time: 0.4300 (0.1996 -- 2.5452)  data: 0.2142 (0.0006 -- 2.3161)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6296 (2.6309)  acc1: 44.4444 (41.2698)  acc5: 88.8889 (88.8889)  time: 0.2154 (0.1692 -- 0.3885)  data: 0.0126 (0.0001 -- 0.2082)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6296 (2.6361)  acc1: 44.4444 (41.9087)  acc5: 88.8889 (85.8921)  time: 0.2009 (0.1323 -- 0.3885)  data: 0.0120 (0.0001 -- 0.2082)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 43.154 Acc@5 86.929 loss 2.635
Accuracy of the network on the 482 val images: 43.15%
[2023-09-04 21:44:08,662] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 21:44:08,663] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 21:44:08,663] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 21:44:08,663] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 21:44:09,928] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 21:44:09,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.15%
Epoch: [5]  [  0/160]  eta: 0:17:37  lr: 0.000008  min_lr: 0.000002  loss: 2.6640 (2.6640)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6720 (1.6720)  time: 6.6086 (6.6086 -- 6.6086)  data: 6.0834 (6.0834 -- 6.0834)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:45  lr: 0.000008  min_lr: 0.000002  loss: 2.7000 (2.6910)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7560 (1.7793)  time: 0.9135 (0.5317 -- 4.4191)  data: 0.2217 (0.0005 -- 2.9006)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:02:03  lr: 0.000008  min_lr: 0.000002  loss: 2.6744 (2.6849)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7608 (1.7546)  time: 0.8676 (0.5203 -- 3.5325)  data: 0.0013 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000002  loss: 2.6904 (2.6848)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6784 (1.7410)  time: 0.9022 (0.5312 -- 2.5679)  data: 0.1299 (0.0004 -- 1.4063)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000002  loss: 2.6704 (2.6810)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6619 (1.7453)  time: 0.8611 (0.5229 -- 2.1571)  data: 0.0706 (0.0003 -- 0.7750)  max mem: 16413
[2023-09-04 21:45:39,874] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:45:39,874] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:45:39,874] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-09-04 21:45:39,874] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000002  loss: 2.6753 (2.6792)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7177 (1.7499)  time: 0.8548 (0.5447 -- 2.9740)  data: 0.2260 (0.0003 -- 2.4204)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000002  loss: 2.6647 (2.6763)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8214 (1.7582)  time: 0.7607 (0.5350 -- 2.3422)  data: 0.0590 (0.0007 -- 1.0847)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:17  lr: 0.000009  min_lr: 0.000002  loss: 2.6648 (2.6752)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7197 (1.7534)  time: 0.8399 (0.5349 -- 2.2949)  data: 0.1514 (0.0002 -- 1.7306)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 2.6430 (2.6726)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8250 (1.7617)  time: 0.7014 (0.4961 -- 2.2937)  data: 0.0065 (0.0003 -- 0.1174)  max mem: 16413
Epoch: [5] Total time: 0:02:20 (0.8757 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 2.6430 (2.6739)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8250 (1.7617)
Val:  [ 0/27]  eta: 0:01:02  loss: 2.5256 (2.5256)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3208 (2.3208 -- 2.3208)  data: 2.0259 (2.0259 -- 2.0259)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.5668 (2.5628)  acc1: 44.4444 (39.3939)  acc5: 100.0000 (91.9192)  time: 0.4097 (0.1994 -- 2.3208)  data: 0.1863 (0.0006 -- 2.0259)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.5454 (2.5533)  acc1: 44.4444 (39.6825)  acc5: 88.8889 (91.5344)  time: 0.2248 (0.1693 -- 0.5657)  data: 0.0201 (0.0001 -- 0.3755)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.5482 (2.5608)  acc1: 44.4444 (40.2490)  acc5: 88.8889 (89.2116)  time: 0.2096 (0.1326 -- 0.5657)  data: 0.0198 (0.0001 -- 0.3755)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 41.701 Acc@5 88.797 loss 2.561
Accuracy of the network on the 482 val images: 41.70%
Max accuracy: 43.15%
Epoch: [6]  [  0/160]  eta: 0:17:00  lr: 0.000009  min_lr: 0.000002  loss: 2.6899 (2.6899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1084 (2.1084)  time: 6.3779 (6.3779 -- 6.3779)  data: 5.1915 (5.1915 -- 5.1915)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:39  lr: 0.000010  min_lr: 0.000002  loss: 2.6704 (2.6736)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7479 (1.8105)  time: 0.8754 (0.5325 -- 2.5211)  data: 0.1520 (0.0003 -- 1.3897)  max mem: 16413
[2023-09-04 21:47:18,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[2.480325291565305e-06, 2.480325291565305e-06, 2.755916990628117e-06, 2.755916990628117e-06, 3.0621299895867958e-06, 3.0621299895867958e-06, 3.4023666550964403e-06, 3.4023666550964403e-06, 3.7804073945516e-06, 3.7804073945516e-06, 4.200452660612889e-06, 4.200452660612889e-06, 4.66716962290321e-06, 4.66716962290321e-06, 5.18574402544801e-06, 5.18574402544801e-06, 5.761937806053345e-06, 5.761937806053345e-06, 6.402153117837049e-06, 6.402153117837049e-06, 7.113503464263389e-06, 7.113503464263389e-06, 7.903892738070431e-06, 7.903892738070431e-06, 8.78210304230048e-06, 8.78210304230048e-06, 9.757892269222754e-06, 9.757892269222754e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 21:47:18,832] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=17.631559517618133, CurrSamplesPerSec=21.837769730796364, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:02:01  lr: 0.000010  min_lr: 0.000002  loss: 2.6420 (2.6546)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8742 (1.8775)  time: 0.8810 (0.5139 -- 4.3807)  data: 0.2770 (0.0004 -- 3.8560)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:38  lr: 0.000010  min_lr: 0.000003  loss: 2.6257 (2.6462)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8754 (1.8865)  time: 0.9323 (0.5158 -- 2.9285)  data: 0.3008 (0.0004 -- 2.3708)  max mem: 16413
[2023-09-04 21:47:40,371] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:47:40,371] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:47:40,371] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-09-04 21:47:40,371] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000003  loss: 2.6275 (2.6421)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8123 (1.8871)  time: 0.8664 (0.5198 -- 3.1962)  data: 0.2651 (0.0004 -- 2.6733)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000003  loss: 2.6484 (2.6413)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9171 (1.9033)  time: 0.7927 (0.5268 -- 3.0736)  data: 0.2373 (0.0003 -- 2.5443)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000003  loss: 2.6226 (2.6368)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8459 (1.8972)  time: 0.9707 (0.5359 -- 3.7565)  data: 0.4206 (0.0006 -- 3.2497)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 2.6072 (2.6313)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9662 (1.9178)  time: 0.8491 (0.5358 -- 2.3495)  data: 0.0922 (0.0002 -- 1.8143)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 2.6159 (2.6278)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8152 (1.9077)  time: 0.7401 (0.4954 -- 1.9999)  data: 0.0803 (0.0002 -- 1.2095)  max mem: 16413
Epoch: [6] Total time: 0:02:22 (0.8910 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 2.6159 (2.6282)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8152 (1.9077)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.4212 (2.4212)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3964 (2.3964 -- 2.3964)  data: 2.1706 (2.1706 -- 2.1706)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.4894 (2.4735)  acc1: 33.3333 (37.3737)  acc5: 100.0000 (89.8990)  time: 0.4138 (0.1960 -- 2.3964)  data: 0.1987 (0.0006 -- 2.1706)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.4559 (2.4632)  acc1: 33.3333 (36.5079)  acc5: 88.8889 (91.0053)  time: 0.2179 (0.1686 -- 0.4218)  data: 0.0128 (0.0001 -- 0.2399)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.4651 (2.4725)  acc1: 33.3333 (35.6846)  acc5: 88.8889 (87.5519)  time: 0.2017 (0.1330 -- 0.4218)  data: 0.0126 (0.0001 -- 0.2399)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 39.834 Acc@5 88.382 loss 2.471
Accuracy of the network on the 482 val images: 39.83%
Max accuracy: 43.15%
Epoch: [7]  [  0/160]  eta: 0:18:53  lr: 0.000011  min_lr: 0.000003  loss: 2.5178 (2.5178)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6376 (1.6376)  time: 7.0840 (7.0840 -- 7.0840)  data: 6.1781 (6.1781 -- 6.1781)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:49  lr: 0.000011  min_lr: 0.000003  loss: 2.6170 (2.6002)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9580 (1.9785)  time: 0.9173 (0.5335 -- 3.2176)  data: 0.1806 (0.0002 -- 2.4084)  max mem: 16413
[2023-09-04 21:49:42,687] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:49:42,687] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:49:42,687] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-09-04 21:49:42,687] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:02:04  lr: 0.000011  min_lr: 0.000003  loss: 2.5739 (2.5913)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9345 (1.9971)  time: 0.8556 (0.5356 -- 2.8309)  data: 0.0541 (0.0005 -- 0.7054)  max mem: 16413
Epoch: [7]  [ 60/160]  eta: 0:01:34  lr: 0.000012  min_lr: 0.000003  loss: 2.6296 (2.5999)  loss_scale: 65536.0000 (48346.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0875 (2.0312)  time: 0.7581 (0.5237 -- 3.0284)  data: 0.0543 (0.0006 -- 1.0566)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000003  loss: 2.5851 (2.6004)  loss_scale: 65536.0000 (52590.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9279 (2.0172)  time: 0.9711 (0.5326 -- 4.8312)  data: 0.1688 (0.0004 -- 1.7046)  max mem: 16413
Epoch: [7]  [100/160]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000003  loss: 2.5613 (2.5933)  loss_scale: 65536.0000 (55154.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9607 (2.0338)  time: 0.8350 (0.5200 -- 3.5587)  data: 0.1702 (0.0002 -- 2.2576)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000003  loss: 2.5694 (2.5901)  loss_scale: 65536.0000 (56870.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1341 (2.0566)  time: 0.8352 (0.5326 -- 3.0174)  data: 0.0700 (0.0004 -- 1.0401)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:17  lr: 0.000012  min_lr: 0.000003  loss: 2.5595 (2.5840)  loss_scale: 65536.0000 (58099.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0040 (2.0786)  time: 0.8061 (0.5337 -- 3.8338)  data: 0.2448 (0.0006 -- 3.2841)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 2.5867 (2.5841)  loss_scale: 65536.0000 (58982.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0738 (2.0972)  time: 0.7180 (0.4977 -- 3.8568)  data: 0.1602 (0.0002 -- 2.7515)  max mem: 16413
Epoch: [7] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 2.5867 (2.5861)  loss_scale: 65536.0000 (58982.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0738 (2.0972)
Val:  [ 0/27]  eta: 0:01:08  loss: 2.3253 (2.3253)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.5386 (2.5386 -- 2.5386)  data: 2.3375 (2.3375 -- 2.3375)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.3934 (2.3863)  acc1: 33.3333 (36.3636)  acc5: 100.0000 (90.9091)  time: 0.4289 (0.2029 -- 2.5386)  data: 0.2137 (0.0008 -- 2.3375)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.3700 (2.3738)  acc1: 33.3333 (35.4497)  acc5: 88.8889 (91.0053)  time: 0.2143 (0.1695 -- 0.3121)  data: 0.0071 (0.0001 -- 0.1251)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.3700 (2.3875)  acc1: 33.3333 (34.0249)  acc5: 88.8889 (88.7967)  time: 0.1991 (0.1330 -- 0.3121)  data: 0.0067 (0.0001 -- 0.1251)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 38.589 Acc@5 89.627 loss 2.383
Accuracy of the network on the 482 val images: 38.59%
Max accuracy: 43.15%
[2023-09-04 21:51:43,834] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:51:43,834] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536 to 131072
[2023-09-04 21:51:43,839] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:51:43,839] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536 to 131072
Epoch: [8]  [  0/160]  eta: 0:19:41  lr: 0.000013  min_lr: 0.000003  loss: 2.5232 (2.5232)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2644 (2.2644)  time: 7.3828 (7.3828 -- 7.3828)  data: 6.1997 (6.1997 -- 6.1997)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:45  lr: 0.000013  min_lr: 0.000003  loss: 2.5673 (2.5664)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0881 (2.1380)  time: 0.8722 (0.5278 -- 2.8871)  data: 0.1524 (0.0007 -- 1.7540)  max mem: 16413
[2023-09-04 21:52:09,223] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1309
[2023-09-04 21:52:09,223] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1309
[2023-09-04 21:52:09,223] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
[2023-09-04 21:52:09,223] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072 to 65536.0
[2023-09-04 21:52:09,223] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072, reducing to 65536.0
Epoch: [8]  [ 40/160]  eta: 0:02:08  lr: 0.000013  min_lr: 0.000003  loss: 2.5287 (2.5472)  loss_scale: 65536.0000 (111890.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5089 (2.2368)  time: 0.9550 (0.5133 -- 5.7071)  data: 0.0422 (0.0004 -- 0.8088)  max mem: 16413
Epoch: [8]  [ 60/160]  eta: 0:01:41  lr: 0.000013  min_lr: 0.000003  loss: 2.5173 (2.5393)  loss_scale: 65536.0000 (96692.4590)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0956 (2.2448)  time: 0.9078 (0.5278 -- 4.0125)  data: 0.0014 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [8]  [ 80/160]  eta: 0:01:16  lr: 0.000013  min_lr: 0.000003  loss: 2.5473 (2.5351)  loss_scale: 65536.0000 (88999.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1444 (2.2564)  time: 0.7688 (0.5301 -- 3.4486)  data: 0.0014 (0.0005 -- 0.0034)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000003  loss: 2.5511 (2.5384)  loss_scale: 65536.0000 (84353.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0914 (2.2414)  time: 0.9227 (0.5351 -- 3.1522)  data: 0.0017 (0.0007 -- 0.0040)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000003  loss: 2.5375 (2.5399)  loss_scale: 65536.0000 (81242.9752)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1061 (2.2485)  time: 0.7750 (0.5373 -- 3.0799)  data: 0.0017 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [8]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000004  loss: 2.5865 (2.5442)  loss_scale: 65536.0000 (79015.0355)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4470 (2.2903)  time: 0.8606 (0.5320 -- 3.6010)  data: 0.0015 (0.0004 -- 0.0038)  max mem: 16413
[2023-09-04 21:53:57,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:53:57,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-09-04 21:53:57,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:53:57,679] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 2.5570 (2.5446)  loss_scale: 65536.0000 (78233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0725 (2.2654)  time: 0.7182 (0.4985 -- 1.9501)  data: 0.0586 (0.0002 -- 0.6446)  max mem: 16413
Epoch: [8] Total time: 0:02:21 (0.8859 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 2.5570 (2.5434)  loss_scale: 65536.0000 (78233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0725 (2.2654)
Val:  [ 0/27]  eta: 0:01:02  loss: 2.2256 (2.2256)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3243 (2.3243 -- 2.3243)  data: 2.0794 (2.0794 -- 2.0794)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.3075 (2.3048)  acc1: 44.4444 (38.3838)  acc5: 100.0000 (93.9394)  time: 0.4129 (0.2052 -- 2.3243)  data: 0.1916 (0.0008 -- 2.0794)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.2831 (2.2870)  acc1: 33.3333 (38.0952)  acc5: 100.0000 (93.6508)  time: 0.2203 (0.1701 -- 0.3488)  data: 0.0141 (0.0001 -- 0.1566)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.2831 (2.3015)  acc1: 33.3333 (36.9295)  acc5: 88.8889 (91.2863)  time: 0.2038 (0.1331 -- 0.3488)  data: 0.0137 (0.0001 -- 0.1566)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 42.531 Acc@5 90.871 loss 2.299
Accuracy of the network on the 482 val images: 42.53%
Max accuracy: 43.15%
Epoch: [9]  [  0/160]  eta: 0:18:04  lr: 0.000014  min_lr: 0.000004  loss: 2.5193 (2.5193)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4794 (2.4794)  time: 6.7804 (6.7804 -- 6.7804)  data: 6.2553 (6.2553 -- 6.2553)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:03:01  lr: 0.000014  min_lr: 0.000004  loss: 2.5133 (2.5054)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3738 (2.5764)  time: 1.0227 (0.5231 -- 4.3438)  data: 0.0013 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [9]  [ 40/160]  eta: 0:02:08  lr: 0.000014  min_lr: 0.000004  loss: 2.5222 (2.5090)  loss_scale: 131072.0000 (131072.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3608 (2.5111)  time: 0.8309 (0.5246 -- 3.7213)  data: 0.0012 (0.0003 -- 0.0029)  max mem: 16413
[2023-09-04 21:55:00,320] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1492
[2023-09-04 21:55:00,320] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1492
[2023-09-04 21:55:00,320] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-09-04 21:55:00,320] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-09-04 21:55:00,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [9]  [ 60/160]  eta: 0:01:44  lr: 0.000015  min_lr: 0.000004  loss: 2.4952 (2.5049)  loss_scale: 131072.0000 (121402.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3899 (2.5053)  time: 0.9924 (0.5224 -- 5.5063)  data: 0.0013 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:17  lr: 0.000015  min_lr: 0.000004  loss: 2.5358 (2.5086)  loss_scale: 65536.0000 (107608.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4747 (2.5228)  time: 0.7315 (0.5111 -- 2.6730)  data: 0.0017 (0.0001 -- 0.0071)  max mem: 16413
Epoch: [9]  [100/160]  eta: 0:00:57  lr: 0.000015  min_lr: 0.000004  loss: 2.4908 (2.5025)  loss_scale: 65536.0000 (99277.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3643 (2.5268)  time: 0.9419 (0.5238 -- 3.7973)  data: 0.0015 (0.0007 -- 0.0043)  max mem: 16413
Epoch: [9]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000004  loss: 2.4439 (2.4961)  loss_scale: 65536.0000 (93700.2314)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3394 (2.5275)  time: 0.7833 (0.5203 -- 3.2314)  data: 0.0013 (0.0002 -- 0.0050)  max mem: 16413
[2023-09-04 21:56:13,122] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1577
[2023-09-04 21:56:13,122] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1577
[2023-09-04 21:56:13,122] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 21:56:13,122] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 21:56:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 2.4179 (2.4875)  loss_scale: 65536.0000 (88775.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3302 (2.5344)  time: 0.9658 (0.5266 -- 3.9110)  data: 0.0020 (0.0002 -- 0.0175)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 2.4271 (2.4841)  loss_scale: 32768.0000 (82124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6080 (2.5775)  time: 0.6730 (0.4956 -- 2.5416)  data: 0.0008 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [9] Total time: 0:02:25 (0.9068 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 2.4271 (2.4873)  loss_scale: 32768.0000 (82124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6080 (2.5775)
Val:  [ 0/27]  eta: 0:01:05  loss: 2.1350 (2.1350)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4277 (2.4277 -- 2.4277)  data: 2.1916 (2.1916 -- 2.1916)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.2055 (2.2056)  acc1: 44.4444 (43.4343)  acc5: 100.0000 (92.9293)  time: 0.4318 (0.1963 -- 2.4277)  data: 0.2126 (0.0005 -- 2.1916)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.1663 (2.1854)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (93.6508)  time: 0.2194 (0.1717 -- 0.4196)  data: 0.0119 (0.0001 -- 0.1386)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.1663 (2.2007)  acc1: 33.3333 (42.3237)  acc5: 100.0000 (92.1162)  time: 0.2064 (0.1325 -- 0.4196)  data: 0.0117 (0.0001 -- 0.1386)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 44.606 Acc@5 91.909 loss 2.197
Accuracy of the network on the 482 val images: 44.61%
[2023-09-04 21:56:38,830] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 21:56:38,832] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 21:56:38,832] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 21:56:38,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 21:56:40,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 21:56:40,106] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.61%
Epoch: [10]  [  0/160]  eta: 0:24:29  lr: 0.000016  min_lr: 0.000004  loss: 2.5754 (2.5754)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0607 (2.0607)  time: 9.1863 (9.1863 -- 9.1863)  data: 8.6786 (8.6786 -- 8.6786)  max mem: 16413
Epoch: [10]  [ 20/160]  eta: 0:02:50  lr: 0.000016  min_lr: 0.000004  loss: 2.4197 (2.4458)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4877 (2.5624)  time: 0.8168 (0.5229 -- 3.8613)  data: 0.2283 (0.0002 -- 3.3351)  max mem: 16413
Epoch: [10]  [ 40/160]  eta: 0:02:09  lr: 0.000016  min_lr: 0.000004  loss: 2.4872 (2.4717)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.5072 (2.6755)  time: 0.9297 (0.5227 -- 3.8242)  data: 0.1374 (0.0004 -- 1.4864)  max mem: 16413
Epoch: [10]  [ 60/160]  eta: 0:01:39  lr: 0.000016  min_lr: 0.000004  loss: 2.3893 (2.4437)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9399 (2.7178)  time: 0.8241 (0.5211 -- 3.2122)  data: 0.0018 (0.0004 -- 0.0049)  max mem: 16413
Epoch: [10]  [ 80/160]  eta: 0:01:18  lr: 0.000016  min_lr: 0.000004  loss: 2.4089 (2.4296)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7542 (2.7456)  time: 0.9302 (0.5178 -- 4.5022)  data: 0.0015 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:57  lr: 0.000017  min_lr: 0.000004  loss: 2.4853 (2.4391)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7098 (2.7822)  time: 0.8589 (0.5247 -- 4.4318)  data: 0.0016 (0.0005 -- 0.0050)  max mem: 16413
[2023-09-04 21:58:20,090] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:58:20,090] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 21:58:20,090] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 21:58:20,090] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [10]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000004  loss: 2.4480 (2.4391)  loss_scale: 65536.0000 (36830.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6667 (2.7685)  time: 0.8040 (0.5323 -- 3.5921)  data: 0.0023 (0.0004 -- 0.0079)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 2.4974 (2.4459)  loss_scale: 65536.0000 (40901.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.8874 (2.8177)  time: 0.8199 (0.5194 -- 2.5281)  data: 0.0727 (0.0004 -- 0.7203)  max mem: 16413
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 2.4233 (2.4479)  loss_scale: 65536.0000 (43827.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0944 (2.8743)  time: 0.6676 (0.4986 -- 1.7735)  data: 0.0794 (0.0002 -- 1.1523)  max mem: 16413
Epoch: [10] Total time: 0:02:21 (0.8859 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 2.4233 (2.4461)  loss_scale: 65536.0000 (43827.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0944 (2.8743)
Val:  [ 0/27]  eta: 0:01:06  loss: 2.0503 (2.0503)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4713 (2.4713 -- 2.4713)  data: 2.2241 (2.2241 -- 2.2241)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.1181 (2.1061)  acc1: 55.5556 (44.4444)  acc5: 100.0000 (93.9394)  time: 0.4252 (0.2033 -- 2.4713)  data: 0.2045 (0.0008 -- 2.2241)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.0791 (2.0863)  acc1: 44.4444 (41.7989)  acc5: 100.0000 (94.7090)  time: 0.2118 (0.1693 -- 0.2582)  data: 0.0022 (0.0001 -- 0.0148)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0968 (2.1036)  acc1: 44.4444 (41.4938)  acc5: 88.8889 (93.3610)  time: 0.1937 (0.1329 -- 0.2368)  data: 0.0012 (0.0001 -- 0.0148)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 46.058 Acc@5 92.324 loss 2.098
Accuracy of the network on the 482 val images: 46.06%
[2023-09-04 21:59:09,553] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 21:59:09,555] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 21:59:09,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 21:59:09,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 21:59:10,952] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 21:59:10,953] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.06%
Epoch: [11]  [  0/160]  eta: 0:19:50  lr: 0.000017  min_lr: 0.000004  loss: 2.5175 (2.5175)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7100 (2.7100)  time: 7.4412 (7.4412 -- 7.4412)  data: 5.9528 (5.9528 -- 5.9528)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:33  lr: 0.000017  min_lr: 0.000004  loss: 2.4472 (2.4688)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1285 (3.2749)  time: 0.7768 (0.5301 -- 2.8325)  data: 0.1588 (0.0004 -- 2.0543)  max mem: 16413
Epoch: [11]  [ 40/160]  eta: 0:01:59  lr: 0.000018  min_lr: 0.000004  loss: 2.3890 (2.4311)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9357 (3.3334)  time: 0.8971 (0.5231 -- 2.4496)  data: 0.0284 (0.0004 -- 0.5399)  max mem: 16413
Epoch: [11]  [ 60/160]  eta: 0:01:39  lr: 0.000018  min_lr: 0.000005  loss: 2.4368 (2.4343)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3979 (3.4008)  time: 1.0002 (0.5189 -- 4.6414)  data: 0.0038 (0.0003 -- 0.0494)  max mem: 16413
[2023-09-04 22:00:24,660] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:00:24,661] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-09-04 22:00:24,661] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:00:24,661] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-09-04 22:00:25,206] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1835
[2023-09-04 22:00:25,206] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1835
[2023-09-04 22:00:25,207] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-09-04 22:00:25,207] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-09-04 22:00:25,207] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [11]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000005  loss: 2.4064 (2.4299)  loss_scale: 65536.0000 (66345.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3827 (3.4368)  time: 0.8031 (0.5229 -- 3.5268)  data: 0.0020 (0.0002 -- 0.0101)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000005  loss: 2.3929 (2.4214)  loss_scale: 65536.0000 (66184.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3068 (3.4615)  time: 0.8908 (0.5246 -- 3.8457)  data: 0.0270 (0.0004 -- 0.5057)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000005  loss: 2.3650 (2.4183)  loss_scale: 65536.0000 (66077.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1537 (3.4469)  time: 0.8390 (0.5216 -- 3.4490)  data: 0.1407 (0.0003 -- 1.8574)  max mem: 16413
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 2.3604 (2.4073)  loss_scale: 65536.0000 (66000.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0681 (3.4346)  time: 0.8880 (0.5192 -- 4.0689)  data: 0.0012 (0.0005 -- 0.0029)  max mem: 16413
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 2.3006 (2.4005)  loss_scale: 65536.0000 (65945.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4694 (3.4542)  time: 0.7043 (0.4939 -- 2.5077)  data: 0.0006 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [11] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 2.3006 (2.3973)  loss_scale: 65536.0000 (65945.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4694 (3.4542)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.9524 (1.9524)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.2841 (2.2841 -- 2.2841)  data: 2.0574 (2.0574 -- 2.0574)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.0232 (2.0142)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (93.9394)  time: 0.4629 (0.2017 -- 2.2841)  data: 0.2479 (0.0005 -- 2.0574)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.9856 (1.9897)  acc1: 44.4444 (42.3280)  acc5: 100.0000 (93.6508)  time: 0.2334 (0.1695 -- 0.8911)  data: 0.0336 (0.0001 -- 0.6551)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0098 (2.0149)  acc1: 44.4444 (42.7386)  acc5: 88.8889 (92.1162)  time: 0.2182 (0.1324 -- 0.8911)  data: 0.0331 (0.0001 -- 0.6551)  max mem: 16413
Val: Total time: 0:00:07 (0.2940 s / it)
* Acc@1 45.851 Acc@5 91.286 loss 2.004
Accuracy of the network on the 482 val images: 45.85%
Max accuracy: 46.06%
Epoch: [12]  [  0/160]  eta: 0:19:42  lr: 0.000019  min_lr: 0.000005  loss: 1.9598 (1.9598)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2200 (4.2200)  time: 7.3908 (7.3908 -- 7.3908)  data: 5.2780 (5.2780 -- 5.2780)  max mem: 16413
Epoch: [12]  [ 20/160]  eta: 0:02:43  lr: 0.000019  min_lr: 0.000005  loss: 2.3297 (2.3140)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3753 (3.4190)  time: 0.8593 (0.5276 -- 2.4015)  data: 0.2118 (0.0003 -- 1.8577)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:02:00  lr: 0.000019  min_lr: 0.000005  loss: 2.3552 (2.3437)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3736 (3.4036)  time: 0.8212 (0.5257 -- 3.4352)  data: 0.0296 (0.0005 -- 0.3814)  max mem: 16413
[2023-09-04 22:02:27,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:02:27,298] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-09-04 22:02:27,300] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:02:27,300] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
[2023-09-04 22:02:28,338] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1966
[2023-09-04 22:02:28,338] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1966
[2023-09-04 22:02:28,380] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-09-04 22:02:28,380] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
[2023-09-04 22:02:28,380] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
Epoch: [12]  [ 60/160]  eta: 0:01:38  lr: 0.000019  min_lr: 0.000005  loss: 2.3503 (2.3442)  loss_scale: 65536.0000 (67684.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2610 (3.4342)  time: 0.9682 (0.5246 -- 2.9717)  data: 0.2378 (0.0005 -- 2.4278)  max mem: 16413
[2023-09-04 22:02:56,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[4.963133391230276e-06, 4.963133391230276e-06, 5.514592656922528e-06, 5.514592656922528e-06, 6.127325174358364e-06, 6.127325174358364e-06, 6.808139082620405e-06, 6.808139082620405e-06, 7.564598980689339e-06, 7.564598980689339e-06, 8.40510997854371e-06, 8.40510997854371e-06, 9.339011087270787e-06, 9.339011087270787e-06, 1.037667898585643e-05, 1.037667898585643e-05, 1.1529643317618256e-05, 1.1529643317618256e-05, 1.2810714797353617e-05, 1.2810714797353617e-05, 1.423412755261513e-05, 1.423412755261513e-05, 1.5815697280683476e-05, 1.5815697280683476e-05, 1.7572996978537196e-05, 1.7572996978537196e-05, 1.9525552198374662e-05, 1.9525552198374662e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 22:02:56,975] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=16.731129506433568, CurrSamplesPerSec=20.275398001933613, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000005  loss: 2.3216 (2.3349)  loss_scale: 65536.0000 (67154.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2302 (3.5019)  time: 0.7667 (0.5336 -- 2.3885)  data: 0.0338 (0.0004 -- 0.5994)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000005  loss: 2.4351 (2.3574)  loss_scale: 65536.0000 (66833.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5877 (3.6899)  time: 0.9798 (0.5286 -- 3.2007)  data: 0.1071 (0.0004 -- 1.5955)  max mem: 16413
Epoch: [12]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000005  loss: 2.4160 (2.3658)  loss_scale: 65536.0000 (66619.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3071 (3.6607)  time: 0.8167 (0.5275 -- 2.6000)  data: 0.1599 (0.0006 -- 2.0753)  max mem: 16413
Epoch: [12]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 2.2129 (2.3498)  loss_scale: 65536.0000 (66465.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3805 (3.6541)  time: 0.8282 (0.5290 -- 3.3861)  data: 0.2648 (0.0004 -- 2.8448)  max mem: 16413
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 2.3876 (2.3540)  loss_scale: 65536.0000 (66355.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3585 (3.6590)  time: 0.7105 (0.4956 -- 1.9816)  data: 0.1894 (0.0001 -- 1.4562)  max mem: 16413
Epoch: [12] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 2.3876 (2.3511)  loss_scale: 65536.0000 (66355.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3585 (3.6590)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.8581 (1.8581)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.2978 (2.2978 -- 2.2978)  data: 2.0424 (2.0424 -- 2.0424)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.8943 (1.9163)  acc1: 55.5556 (47.4747)  acc5: 100.0000 (93.9394)  time: 0.4035 (0.2031 -- 2.2978)  data: 0.1867 (0.0007 -- 2.0424)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8721 (1.8964)  acc1: 55.5556 (46.5608)  acc5: 100.0000 (94.1799)  time: 0.2220 (0.1689 -- 0.3908)  data: 0.0154 (0.0001 -- 0.1894)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.9030 (1.9270)  acc1: 44.4444 (46.0581)  acc5: 88.8889 (92.9461)  time: 0.2056 (0.1359 -- 0.3908)  data: 0.0151 (0.0001 -- 0.1894)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 50.622 Acc@5 92.531 loss 1.911
Accuracy of the network on the 482 val images: 50.62%
[2023-09-04 22:04:11,462] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:04:11,463] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:04:11,463] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:04:11,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:04:12,880] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:04:12,881] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.62%
Epoch: [13]  [  0/160]  eta: 0:22:30  lr: 0.000020  min_lr: 0.000005  loss: 2.2479 (2.2479)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1833 (3.1833)  time: 8.4404 (8.4404 -- 8.4404)  data: 6.2941 (6.2941 -- 6.2941)  max mem: 16413
[2023-09-04 22:04:30,878] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2092
[2023-09-04 22:04:30,878] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2092
[2023-09-04 22:04:30,879] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:04:30,879] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:04:30,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [13]  [ 20/160]  eta: 0:02:46  lr: 0.000021  min_lr: 0.000005  loss: 2.3723 (2.3870)  loss_scale: 65536.0000 (51492.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5613 (3.9341)  time: 0.8298 (0.5366 -- 3.4556)  data: 0.0018 (0.0006 -- 0.0044)  max mem: 16413
Epoch: [13]  [ 40/160]  eta: 0:02:04  lr: 0.000021  min_lr: 0.000005  loss: 2.3816 (2.3890)  loss_scale: 32768.0000 (42358.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6239 (4.0287)  time: 0.8677 (0.5231 -- 3.5672)  data: 0.0403 (0.0003 -- 0.4809)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000005  loss: 2.3276 (2.3761)  loss_scale: 32768.0000 (39214.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7776 (4.0221)  time: 0.8555 (0.5267 -- 3.9589)  data: 0.1621 (0.0006 -- 1.7540)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:19  lr: 0.000021  min_lr: 0.000005  loss: 2.3511 (2.3736)  loss_scale: 32768.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9516 (4.0807)  time: 1.0377 (0.5194 -- 4.8350)  data: 0.0013 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [13]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000005  loss: 2.2393 (2.3535)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3528 (4.1939)  time: 0.6862 (0.5202 -- 2.2526)  data: 0.0019 (0.0005 -- 0.0084)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000005  loss: 2.2714 (2.3395)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2556 (4.2760)  time: 0.9056 (0.5350 -- 3.1725)  data: 0.0026 (0.0002 -- 0.0158)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000006  loss: 2.3410 (2.3261)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0288 (4.2666)  time: 0.8828 (0.5140 -- 5.6076)  data: 0.0604 (0.0002 -- 1.1837)  max mem: 16413
[2023-09-04 22:06:23,219] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:06:23,219] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:06:23,220] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:06:23,221] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 2.2371 (2.3134)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3433 (4.2833)  time: 0.7021 (0.4977 -- 3.3015)  data: 0.0009 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [13] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 2.2371 (2.3163)  loss_scale: 65536.0000 (39116.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3433 (4.2833)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.7671 (1.7671)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4217 (2.4217 -- 2.4217)  data: 2.1775 (2.1775 -- 2.1775)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.7957 (1.8388)  acc1: 55.5556 (47.4747)  acc5: 100.0000 (93.9394)  time: 0.4153 (0.1999 -- 2.4217)  data: 0.2018 (0.0007 -- 2.1775)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.7957 (1.8125)  acc1: 55.5556 (46.0317)  acc5: 100.0000 (94.1799)  time: 0.2242 (0.1698 -- 0.5979)  data: 0.0227 (0.0001 -- 0.4091)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.8206 (1.8370)  acc1: 44.4444 (46.0581)  acc5: 100.0000 (94.1909)  time: 0.2085 (0.1322 -- 0.5979)  data: 0.0224 (0.0001 -- 0.4091)  max mem: 16413
Val: Total time: 0:00:07 (0.2923 s / it)
* Acc@1 49.585 Acc@5 93.361 loss 1.825
Accuracy of the network on the 482 val images: 49.59%
Max accuracy: 50.62%
Epoch: [14]  [  0/160]  eta: 0:22:17  lr: 0.000022  min_lr: 0.000006  loss: 2.1633 (2.1633)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5312 (3.5312)  time: 8.3600 (8.3600 -- 8.3600)  data: 7.8523 (7.8523 -- 7.8523)  max mem: 16413
[2023-09-04 22:07:02,087] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2253
[2023-09-04 22:07:02,087] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2253
[2023-09-04 22:07:02,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:07:02,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 22:07:02,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [14]  [ 20/160]  eta: 0:02:44  lr: 0.000022  min_lr: 0.000006  loss: 2.3179 (2.2769)  loss_scale: 65536.0000 (53052.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6398 (4.7163)  time: 0.8151 (0.5265 -- 3.3804)  data: 0.1692 (0.0005 -- 2.4726)  max mem: 16413
Epoch: [14]  [ 40/160]  eta: 0:02:07  lr: 0.000022  min_lr: 0.000006  loss: 2.2482 (2.2666)  loss_scale: 32768.0000 (43157.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5839 (4.8504)  time: 0.9417 (0.5208 -- 4.4068)  data: 0.0258 (0.0002 -- 0.4871)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:38  lr: 0.000022  min_lr: 0.000006  loss: 2.2827 (2.2761)  loss_scale: 32768.0000 (39751.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0184 (4.6724)  time: 0.8353 (0.5268 -- 5.0546)  data: 0.1788 (0.0004 -- 2.6955)  max mem: 16413
Epoch: [14]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000006  loss: 2.3364 (2.2869)  loss_scale: 32768.0000 (38027.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5837 (4.8132)  time: 0.9650 (0.5289 -- 3.6283)  data: 0.0012 (0.0005 -- 0.0021)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000006  loss: 2.3003 (2.2878)  loss_scale: 32768.0000 (36985.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8102 (4.8773)  time: 0.7569 (0.5257 -- 2.9074)  data: 0.0021 (0.0003 -- 0.0126)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000006  loss: 2.1152 (2.2694)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3647 (4.8342)  time: 0.9188 (0.5193 -- 3.0911)  data: 0.0013 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 2.2765 (2.2676)  loss_scale: 32768.0000 (35789.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6027 (4.8367)  time: 0.8382 (0.5283 -- 4.3973)  data: 0.0013 (0.0003 -- 0.0042)  max mem: 16413
[2023-09-04 22:08:55,033] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:08:55,033] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:08:55,033] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:08:55,033] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:08:59,378] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2387
[2023-09-04 22:08:59,378] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2387
[2023-09-04 22:08:59,378] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:08:59,378] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:08:59,378] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 2.1940 (2.2624)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9940 (4.7656)  time: 0.6221 (0.4977 -- 2.1365)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [14] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 2.1940 (2.2878)  loss_scale: 32768.0000 (36454.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9940 (4.7656)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.6901 (1.6901)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3482 (2.3482 -- 2.3482)  data: 2.1202 (2.1202 -- 2.1202)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.7335 (1.7572)  acc1: 55.5556 (49.4950)  acc5: 100.0000 (93.9394)  time: 0.4209 (0.2027 -- 2.3482)  data: 0.1997 (0.0006 -- 2.1202)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6705 (1.7336)  acc1: 55.5556 (50.2646)  acc5: 100.0000 (93.6508)  time: 0.2219 (0.1691 -- 0.3836)  data: 0.0154 (0.0001 -- 0.1896)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7636 (1.7727)  acc1: 44.4444 (48.5477)  acc5: 100.0000 (93.7759)  time: 0.2031 (0.1328 -- 0.3836)  data: 0.0147 (0.0001 -- 0.1896)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 52.075 Acc@5 92.946 loss 1.757
Accuracy of the network on the 482 val images: 52.07%
[2023-09-04 22:09:13,580] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:09:13,581] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:09:13,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:09:13,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:09:14,992] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:09:14,992] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 52.07%
Epoch: [15]  [  0/160]  eta: 0:17:13  lr: 0.000023  min_lr: 0.000006  loss: 2.1687 (2.1687)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0739 (6.0739)  time: 6.4575 (6.4575 -- 6.4575)  data: 5.9142 (5.9142 -- 5.9142)  max mem: 16413
Epoch: [15]  [ 20/160]  eta: 0:02:51  lr: 0.000024  min_lr: 0.000006  loss: 2.2605 (2.2532)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7206 (4.5701)  time: 0.9631 (0.5327 -- 2.8769)  data: 0.3103 (0.0009 -- 2.3615)  max mem: 16413
Epoch: [15]  [ 40/160]  eta: 0:02:00  lr: 0.000024  min_lr: 0.000006  loss: 2.2035 (2.2413)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5555 (4.6401)  time: 0.7726 (0.5172 -- 2.5387)  data: 0.2262 (0.0001 -- 2.0184)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:36  lr: 0.000024  min_lr: 0.000006  loss: 2.2500 (2.2472)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2952 (4.5105)  time: 0.8848 (0.5419 -- 3.2658)  data: 0.3289 (0.0008 -- 2.7422)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:16  lr: 0.000024  min_lr: 0.000006  loss: 2.1595 (2.2424)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0841 (4.5035)  time: 0.9360 (0.5116 -- 3.6115)  data: 0.3908 (0.0002 -- 3.0700)  max mem: 16413
Epoch: [15]  [100/160]  eta: 0:00:55  lr: 0.000024  min_lr: 0.000006  loss: 2.2159 (2.2476)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3763 (4.5433)  time: 0.8140 (0.5302 -- 3.5703)  data: 0.2659 (0.0003 -- 3.0251)  max mem: 16413
[2023-09-04 22:11:02,491] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:11:02,492] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:11:02,494] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:11:02,494] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [15]  [120/160]  eta: 0:00:36  lr: 0.000025  min_lr: 0.000006  loss: 2.1832 (2.2428)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4540 (4.5527)  time: 0.8783 (0.5291 -- 2.6208)  data: 0.2656 (0.0007 -- 2.0886)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 2.3551 (2.2542)  loss_scale: 65536.0000 (38577.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4587 (4.5991)  time: 0.9035 (0.5365 -- 5.1834)  data: 0.3463 (0.0003 -- 4.6666)  max mem: 16413
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 2.3313 (2.2662)  loss_scale: 65536.0000 (41779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6233 (4.6162)  time: 0.6612 (0.4969 -- 1.7272)  data: 0.1059 (0.0002 -- 1.1768)  max mem: 16413
Epoch: [15] Total time: 0:02:22 (0.8889 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 2.3313 (2.2527)  loss_scale: 65536.0000 (41779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6233 (4.6162)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.6442 (1.6442)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4760 (2.4760 -- 2.4760)  data: 2.2618 (2.2618 -- 2.2618)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5646 (1.6620)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (93.9394)  time: 0.4279 (0.2002 -- 2.4760)  data: 0.2200 (0.0009 -- 2.2618)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5617 (1.6429)  acc1: 55.5556 (51.3228)  acc5: 100.0000 (94.7090)  time: 0.2175 (0.1686 -- 0.3609)  data: 0.0169 (0.0001 -- 0.1768)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6570 (1.6826)  acc1: 44.4444 (49.7925)  acc5: 100.0000 (95.0207)  time: 0.2045 (0.1331 -- 0.3609)  data: 0.0165 (0.0001 -- 0.1768)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 53.527 Acc@5 93.568 loss 1.668
Accuracy of the network on the 482 val images: 53.53%
[2023-09-04 22:11:45,033] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:11:45,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:11:45,035] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:11:45,035] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:11:46,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:11:46,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 53.53%
Epoch: [16]  [  0/160]  eta: 0:21:46  lr: 0.000025  min_lr: 0.000006  loss: 2.1846 (2.1846)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2097 (7.2097)  time: 8.1642 (8.1642 -- 8.1642)  data: 7.5570 (7.5570 -- 7.5570)  max mem: 16413
Epoch: [16]  [ 20/160]  eta: 0:02:47  lr: 0.000025  min_lr: 0.000006  loss: 2.2089 (2.2665)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6383 (4.8589)  time: 0.8496 (0.5222 -- 3.6786)  data: 0.2800 (0.0005 -- 3.1218)  max mem: 16413
[2023-09-04 22:12:25,433] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2597
[2023-09-04 22:12:25,433] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2597
[2023-09-04 22:12:25,434] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:12:25,434] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:12:25,434] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [16]  [ 40/160]  eta: 0:02:06  lr: 0.000025  min_lr: 0.000006  loss: 2.1969 (2.2407)  loss_scale: 65536.0000 (62339.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1746 (5.0103)  time: 0.9081 (0.5289 -- 3.8032)  data: 0.3081 (0.0002 -- 2.7145)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:38  lr: 0.000026  min_lr: 0.000007  loss: 2.2835 (2.2466)  loss_scale: 32768.0000 (52643.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7723 (5.1143)  time: 0.8412 (0.5274 -- 3.0842)  data: 0.1137 (0.0007 -- 1.6991)  max mem: 16413
Epoch: [16]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000007  loss: 2.2231 (2.2428)  loss_scale: 32768.0000 (47736.0988)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8299 (5.1421)  time: 0.8364 (0.5304 -- 3.3061)  data: 0.0020 (0.0003 -- 0.0143)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000007  loss: 2.2087 (2.2349)  loss_scale: 32768.0000 (44772.1188)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9026 (5.1940)  time: 0.8765 (0.5312 -- 3.7754)  data: 0.0019 (0.0002 -- 0.0053)  max mem: 16413
Epoch: [16]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000007  loss: 2.2927 (2.2435)  loss_scale: 32768.0000 (42787.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5468 (5.0942)  time: 0.8234 (0.5239 -- 2.6527)  data: 0.0240 (0.0006 -- 0.4367)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 2.2560 (2.2459)  loss_scale: 32768.0000 (41366.6950)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7672 (5.1237)  time: 1.0140 (0.5284 -- 4.6713)  data: 0.0015 (0.0005 -- 0.0034)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 2.3261 (2.2524)  loss_scale: 32768.0000 (40345.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2167 (5.0787)  time: 0.6401 (0.4949 -- 2.9117)  data: 0.0005 (0.0002 -- 0.0014)  max mem: 16413
Epoch: [16] Total time: 0:02:23 (0.8964 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 2.3261 (2.2460)  loss_scale: 32768.0000 (40345.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2167 (5.0787)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.5867 (1.5867)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3720 (2.3720 -- 2.3720)  data: 2.1386 (2.1386 -- 2.1386)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5317 (1.6113)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (96.9697)  time: 0.4170 (0.2021 -- 2.3720)  data: 0.1955 (0.0004 -- 2.1386)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4972 (1.5769)  acc1: 55.5556 (57.1429)  acc5: 100.0000 (96.8254)  time: 0.2240 (0.1695 -- 0.5202)  data: 0.0180 (0.0001 -- 0.3456)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5502 (1.6078)  acc1: 55.5556 (56.4315)  acc5: 100.0000 (96.2656)  time: 0.2051 (0.1330 -- 0.5202)  data: 0.0177 (0.0001 -- 0.3456)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 58.299 Acc@5 94.813 loss 1.599
Accuracy of the network on the 482 val images: 58.30%
[2023-09-04 22:14:17,732] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:14:17,734] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:14:17,734] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:14:17,734] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:14:19,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:14:19,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 58.30%
Epoch: [17]  [  0/160]  eta: 0:21:42  lr: 0.000027  min_lr: 0.000007  loss: 2.4358 (2.4358)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5186 (4.5186)  time: 8.1396 (8.1396 -- 8.1396)  data: 4.7509 (4.7509 -- 4.7509)  max mem: 16413
[2023-09-04 22:14:32,320] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:14:32,320] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:14:32,320] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:14:32,361] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:14:39,070] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2733
[2023-09-04 22:14:39,070] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2733
[2023-09-04 22:14:39,070] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:14:39,070] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:14:39,070] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [17]  [ 20/160]  eta: 0:02:47  lr: 0.000027  min_lr: 0.000007  loss: 2.1154 (2.1493)  loss_scale: 32768.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9146 (4.5883)  time: 0.8482 (0.5288 -- 3.4295)  data: 0.1047 (0.0005 -- 1.6272)  max mem: 16413
Epoch: [17]  [ 40/160]  eta: 0:02:04  lr: 0.000027  min_lr: 0.000007  loss: 2.1258 (2.1462)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0786 (4.5875)  time: 0.8725 (0.5218 -- 2.5594)  data: 0.1755 (0.0002 -- 2.0076)  max mem: 16413
Epoch: [17]  [ 60/160]  eta: 0:01:38  lr: 0.000027  min_lr: 0.000007  loss: 2.0399 (2.1466)  loss_scale: 32768.0000 (36528.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5516 (4.6066)  time: 0.8740 (0.5190 -- 2.8651)  data: 0.3302 (0.0004 -- 2.3424)  max mem: 16413
Epoch: [17]  [ 80/160]  eta: 0:01:18  lr: 0.000027  min_lr: 0.000007  loss: 2.1833 (2.1526)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1028 (4.8592)  time: 0.9514 (0.5216 -- 4.1060)  data: 0.0919 (0.0008 -- 1.1360)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000007  loss: 2.1905 (2.1613)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6778 (4.8675)  time: 0.7136 (0.5246 -- 2.3886)  data: 0.0693 (0.0004 -- 0.7293)  max mem: 16413
[2023-09-04 22:16:02,650] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2828
[2023-09-04 22:16:02,650] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2828
[2023-09-04 22:16:02,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:16:02,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:16:02,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [120/160]  eta: 0:00:37  lr: 0.000028  min_lr: 0.000007  loss: 2.1109 (2.1580)  loss_scale: 16384.0000 (32903.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2974 (4.9351)  time: 1.0438 (0.5111 -- 4.8075)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 2.0716 (2.1432)  loss_scale: 16384.0000 (30560.2270)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6170 (4.9360)  time: 0.8272 (0.5221 -- 2.6542)  data: 0.0012 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 2.0529 (2.1388)  loss_scale: 16384.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4064 (4.9917)  time: 0.6690 (0.4956 -- 2.2588)  data: 0.0009 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [17] Total time: 0:02:21 (0.8871 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 2.0529 (2.1654)  loss_scale: 16384.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4064 (4.9917)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.4984 (1.4984)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2380 (2.2380 -- 2.2380)  data: 2.0003 (2.0003 -- 2.0003)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.4934 (1.5717)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (92.9293)  time: 0.4214 (0.1930 -- 2.2380)  data: 0.2103 (0.0005 -- 2.0003)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4692 (1.5259)  acc1: 55.5556 (52.9101)  acc5: 100.0000 (94.7090)  time: 0.2296 (0.1690 -- 0.5018)  data: 0.0277 (0.0001 -- 0.2964)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5601 (1.5733)  acc1: 55.5556 (51.4523)  acc5: 100.0000 (93.3610)  time: 0.2168 (0.1333 -- 0.5018)  data: 0.0271 (0.0001 -- 0.2964)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 56.846 Acc@5 92.324 loss 1.553
Accuracy of the network on the 482 val images: 56.85%
Max accuracy: 58.30%
Epoch: [18]  [  0/160]  eta: 0:18:09  lr: 0.000028  min_lr: 0.000007  loss: 2.2674 (2.2674)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7556 (5.7556)  time: 6.8084 (6.8084 -- 6.8084)  data: 6.1974 (6.1974 -- 6.1974)  max mem: 16413
Epoch: [18]  [ 20/160]  eta: 0:02:44  lr: 0.000028  min_lr: 0.000007  loss: 2.1314 (2.1626)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5544 (6.0580)  time: 0.8923 (0.5256 -- 4.7129)  data: 0.3416 (0.0003 -- 4.1771)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:05  lr: 0.000029  min_lr: 0.000007  loss: 2.2455 (2.1842)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7151 (6.1621)  time: 0.9087 (0.5288 -- 4.5827)  data: 0.3530 (0.0006 -- 4.0497)  max mem: 16413
Epoch: [18]  [ 60/160]  eta: 0:01:37  lr: 0.000029  min_lr: 0.000007  loss: 2.2607 (2.2197)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7509 (5.9014)  time: 0.8189 (0.5265 -- 3.3810)  data: 0.2412 (0.0004 -- 2.8465)  max mem: 16413
[2023-09-04 22:18:02,621] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:18:02,621] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:18:02,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:18:02,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [ 80/160]  eta: 0:01:14  lr: 0.000029  min_lr: 0.000007  loss: 2.1015 (2.1947)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9535 (5.7178)  time: 0.8308 (0.5165 -- 2.6844)  data: 0.2812 (0.0006 -- 2.1633)  max mem: 16413
Epoch: [18]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000007  loss: 2.1941 (2.1959)  loss_scale: 32768.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2549 (5.7037)  time: 0.8506 (0.5504 -- 2.4421)  data: 0.2771 (0.0008 -- 1.8907)  max mem: 16413
[2023-09-04 22:18:27,582] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2985
[2023-09-04 22:18:27,623] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:18:27,582] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2985
[2023-09-04 22:18:27,628] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:18:27,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 22:18:37,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[7.445941490895245e-06, 7.445941490895245e-06, 8.273268323216939e-06, 8.273268323216939e-06, 9.192520359129932e-06, 9.192520359129932e-06, 1.0213911510144368e-05, 1.0213911510144368e-05, 1.1348790566827076e-05, 1.1348790566827076e-05, 1.2609767296474529e-05, 1.2609767296474529e-05, 1.4010852551638364e-05, 1.4010852551638364e-05, 1.5567613946264848e-05, 1.5567613946264848e-05, 1.7297348829183167e-05, 1.7297348829183167e-05, 1.9219276476870183e-05, 1.9219276476870183e-05, 2.135475164096687e-05, 2.135475164096687e-05, 2.372750182329652e-05, 2.372750182329652e-05, 2.6363890914773912e-05, 2.6363890914773912e-05, 2.929321212752657e-05, 2.929321212752657e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 22:18:37,656] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=16.85599689069981, CurrSamplesPerSec=12.789556054562581, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000007  loss: 2.2715 (2.2080)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6311 (5.7688)  time: 0.9333 (0.5339 -- 2.8287)  data: 0.2607 (0.0002 -- 2.2849)  max mem: 16413
Epoch: [18]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 2.1479 (2.2104)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7702 (5.6917)  time: 0.8333 (0.5346 -- 2.4842)  data: 0.2330 (0.0003 -- 1.9548)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 2.0696 (2.1969)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3738 (5.6009)  time: 0.6595 (0.4949 -- 3.0111)  data: 0.1356 (0.0002 -- 2.4886)  max mem: 16413
Epoch: [18] Total time: 0:02:20 (0.8799 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 2.0696 (2.1798)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3738 (5.6009)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.4459 (1.4459)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5678 (2.5678 -- 2.5678)  data: 2.3185 (2.3185 -- 2.3185)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.4046 (1.4965)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (92.9293)  time: 0.4347 (0.2006 -- 2.5678)  data: 0.2182 (0.0005 -- 2.3185)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3886 (1.4663)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (94.1799)  time: 0.2087 (0.1695 -- 0.2880)  data: 0.0043 (0.0001 -- 0.0735)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4558 (1.5046)  acc1: 44.4444 (53.1120)  acc5: 100.0000 (93.7759)  time: 0.1924 (0.1329 -- 0.2880)  data: 0.0040 (0.0001 -- 0.0735)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 58.506 Acc@5 93.361 loss 1.473
Accuracy of the network on the 482 val images: 58.51%
[2023-09-04 22:19:17,475] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:19:17,477] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:19:17,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:19:17,477] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:19:18,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:19:18,875] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 58.51%
Epoch: [19]  [  0/160]  eta: 0:18:12  lr: 0.000030  min_lr: 0.000008  loss: 1.9560 (1.9560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4368 (3.4368)  time: 6.8279 (6.8279 -- 6.8279)  data: 6.2706 (6.2706 -- 6.2706)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:36  lr: 0.000030  min_lr: 0.000008  loss: 2.0017 (2.0445)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6941 (4.9649)  time: 0.8302 (0.5149 -- 2.4434)  data: 0.1037 (0.0005 -- 1.9093)  max mem: 16413
Epoch: [19]  [ 40/160]  eta: 0:01:56  lr: 0.000030  min_lr: 0.000008  loss: 2.1385 (2.1029)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7202 (5.5158)  time: 0.8132 (0.5301 -- 3.6319)  data: 0.0733 (0.0003 -- 0.9685)  max mem: 16413
Epoch: [19]  [ 60/160]  eta: 0:01:36  lr: 0.000030  min_lr: 0.000008  loss: 2.2269 (2.1545)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0862 (5.4496)  time: 0.9682 (0.5217 -- 2.3306)  data: 0.3178 (0.0004 -- 1.7985)  max mem: 16413
[2023-09-04 22:20:29,676] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:20:29,676] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:20:29,676] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:20:29,676] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [ 80/160]  eta: 0:01:14  lr: 0.000030  min_lr: 0.000008  loss: 2.0070 (2.1291)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2575 (5.6067)  time: 0.8402 (0.5328 -- 2.2605)  data: 0.2821 (0.0003 -- 1.7088)  max mem: 16413
Epoch: [19]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000008  loss: 2.0525 (2.1237)  loss_scale: 32768.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1651 (5.7275)  time: 0.9341 (0.5370 -- 2.5237)  data: 0.3312 (0.0002 -- 1.9902)  max mem: 16413
Epoch: [19]  [120/160]  eta: 0:00:37  lr: 0.000031  min_lr: 0.000008  loss: 2.1586 (2.1317)  loss_scale: 32768.0000 (22748.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4964 (5.8171)  time: 0.9561 (0.5223 -- 4.2862)  data: 0.4080 (0.0004 -- 3.7576)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 2.1519 (2.1401)  loss_scale: 32768.0000 (24169.3050)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8220 (5.7954)  time: 0.8193 (0.5343 -- 3.1589)  data: 0.2631 (0.0002 -- 2.6300)  max mem: 16413
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 2.1519 (2.1386)  loss_scale: 32768.0000 (25190.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6833 (5.7889)  time: 0.7221 (0.4943 -- 2.1115)  data: 0.2001 (0.0002 -- 1.5826)  max mem: 16413
Epoch: [19] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 2.1519 (2.1508)  loss_scale: 32768.0000 (25190.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6833 (5.7889)
[2023-09-04 22:21:41,348] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-09-04 22:21:41,350] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-09-04 22:21:41,352] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-09-04 22:21:41,352] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-09-04 22:21:42,387] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-09-04 22:21:42,387] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:00:58  loss: 1.4219 (1.4219)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.1763 (2.1763 -- 2.1763)  data: 1.9507 (1.9507 -- 1.9507)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.4471 (1.5095)  acc1: 44.4444 (51.5152)  acc5: 100.0000 (94.9495)  time: 0.4030 (0.1955 -- 2.1763)  data: 0.1816 (0.0005 -- 1.9507)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4284 (1.4616)  acc1: 55.5556 (52.9101)  acc5: 100.0000 (95.7672)  time: 0.2306 (0.1690 -- 0.6128)  data: 0.0240 (0.0001 -- 0.4303)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4700 (1.5122)  acc1: 55.5556 (53.1120)  acc5: 100.0000 (94.6058)  time: 0.2120 (0.1327 -- 0.6128)  data: 0.0237 (0.0001 -- 0.4303)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 57.261 Acc@5 93.568 loss 1.475
Accuracy of the network on the 482 val images: 57.26%
Max accuracy: 58.51%
Epoch: [20]  [  0/160]  eta: 0:16:12  lr: 0.000031  min_lr: 0.000008  loss: 2.3842 (2.3842)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5084 (4.5084)  time: 6.0805 (6.0805 -- 6.0805)  data: 4.7967 (4.7967 -- 4.7967)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:40  lr: 0.000031  min_lr: 0.000008  loss: 2.0387 (2.1036)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1525 (5.4072)  time: 0.8972 (0.5391 -- 3.2204)  data: 0.0824 (0.0002 -- 0.8098)  max mem: 16413
Epoch: [20]  [ 40/160]  eta: 0:02:03  lr: 0.000032  min_lr: 0.000008  loss: 2.2536 (2.1551)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3844 (5.6597)  time: 0.9140 (0.5213 -- 4.4949)  data: 0.0495 (0.0003 -- 0.8068)  max mem: 16413
[2023-09-04 22:22:33,549] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:22:33,550] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:22:33,555] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:22:33,555] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:22:39,201] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3247
[2023-09-04 22:22:39,201] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3247
[2023-09-04 22:22:39,201] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:22:39,201] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:22:39,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [20]  [ 60/160]  eta: 0:01:39  lr: 0.000032  min_lr: 0.000008  loss: 2.2026 (2.1509)  loss_scale: 32768.0000 (35453.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4990 (5.6150)  time: 0.9181 (0.5386 -- 3.3740)  data: 0.3624 (0.0003 -- 2.8541)  max mem: 16413
Epoch: [20]  [ 80/160]  eta: 0:01:16  lr: 0.000032  min_lr: 0.000008  loss: 2.1270 (2.1486)  loss_scale: 32768.0000 (34790.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9196 (5.5356)  time: 0.8337 (0.5284 -- 3.0399)  data: 0.2052 (0.0003 -- 2.5110)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:00:57  lr: 0.000032  min_lr: 0.000008  loss: 2.0914 (2.1399)  loss_scale: 32768.0000 (34390.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6629 (5.4329)  time: 0.9541 (0.5253 -- 3.2723)  data: 0.1326 (0.0003 -- 1.6764)  max mem: 16413
Epoch: [20]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000008  loss: 2.1391 (2.1387)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2662 (5.4886)  time: 0.7176 (0.5265 -- 2.2163)  data: 0.0712 (0.0004 -- 0.8729)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 2.0952 (2.1363)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7329 (5.6008)  time: 0.8910 (0.5223 -- 2.6468)  data: 0.1554 (0.0005 -- 1.8510)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 2.2247 (2.1452)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2795 (5.6318)  time: 0.7371 (0.4979 -- 2.6410)  data: 0.1250 (0.0002 -- 2.1237)  max mem: 16413
Epoch: [20] Total time: 0:02:22 (0.8907 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 2.2247 (2.1423)  loss_scale: 32768.0000 (33792.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2795 (5.6318)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.4192 (1.4192)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4032 (2.4032 -- 2.4032)  data: 2.1565 (2.1565 -- 2.1565)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3177 (1.4186)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (94.9495)  time: 0.4320 (0.2037 -- 2.4032)  data: 0.2028 (0.0008 -- 2.1565)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3085 (1.4010)  acc1: 55.5556 (53.9683)  acc5: 100.0000 (95.2381)  time: 0.2199 (0.1717 -- 0.2871)  data: 0.0082 (0.0001 -- 0.0863)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3880 (1.4355)  acc1: 55.5556 (53.5270)  acc5: 100.0000 (94.1909)  time: 0.2011 (0.1324 -- 0.2871)  data: 0.0079 (0.0001 -- 0.0863)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 60.166 Acc@5 93.776 loss 1.393
Accuracy of the network on the 482 val images: 60.17%
[2023-09-04 22:24:20,454] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:24:20,455] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:24:20,455] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:24:20,456] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:24:21,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:24:21,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 60.17%
Epoch: [21]  [  0/160]  eta: 0:24:51  lr: 0.000033  min_lr: 0.000008  loss: 2.0707 (2.0707)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5500 (10.5500)  time: 9.3217 (9.3217 -- 9.3217)  data: 8.7999 (8.7999 -- 8.7999)  max mem: 16413
[2023-09-04 22:24:44,570] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:24:44,570] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:24:44,570] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:24:44,570] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:24:46,201] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3379
[2023-09-04 22:24:46,201] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3379
[2023-09-04 22:24:46,201] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:24:46,201] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:24:46,201] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [ 20/160]  eta: 0:02:48  lr: 0.000033  min_lr: 0.000008  loss: 2.1297 (2.1503)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0169 (5.3673)  time: 0.7949 (0.5138 -- 3.2876)  data: 0.2476 (0.0002 -- 2.7381)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000008  loss: 2.1109 (2.1083)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7805 (5.3408)  time: 0.9188 (0.5198 -- 3.3002)  data: 0.3424 (0.0004 -- 2.7687)  max mem: 16413
Epoch: [21]  [ 60/160]  eta: 0:01:35  lr: 0.000033  min_lr: 0.000008  loss: 2.2508 (2.1282)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7902 (5.4725)  time: 0.7336 (0.5252 -- 2.8332)  data: 0.1553 (0.0003 -- 2.2939)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:17  lr: 0.000034  min_lr: 0.000009  loss: 2.0173 (2.1078)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8003 (5.5985)  time: 1.0115 (0.5254 -- 4.6292)  data: 0.3261 (0.0002 -- 4.0880)  max mem: 16413
Epoch: [21]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000009  loss: 2.1362 (2.1096)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9720 (5.6560)  time: 0.7422 (0.5239 -- 2.8301)  data: 0.1861 (0.0004 -- 2.2720)  max mem: 16413
Epoch: [21]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000009  loss: 2.1008 (2.1050)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4252 (5.6986)  time: 0.9699 (0.5189 -- 3.3707)  data: 0.4216 (0.0003 -- 2.8511)  max mem: 16413
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.9816 (2.0969)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0385 (5.9434)  time: 0.7959 (0.5320 -- 2.6735)  data: 0.2455 (0.0003 -- 2.1228)  max mem: 16413
[2023-09-04 22:26:37,553] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:26:37,554] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:26:37,554] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:26:37,554] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:26:39,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3512
[2023-09-04 22:26:39,567] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3512
[2023-09-04 22:26:39,567] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:26:39,567] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:26:39,567] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 2.0689 (2.1005)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1606 (5.9400)  time: 0.7763 (0.4870 -- 3.5090)  data: 0.2554 (0.0001 -- 2.9746)  max mem: 16413
Epoch: [21] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 2.0689 (2.1029)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1606 (5.9400)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.3429 (1.3429)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3137 (2.3137 -- 2.3137)  data: 2.1020 (2.1020 -- 2.1020)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2325 (1.3420)  acc1: 66.6667 (57.5758)  acc5: 100.0000 (94.9495)  time: 0.4524 (0.2005 -- 2.3137)  data: 0.2355 (0.0007 -- 2.1020)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2211 (1.3126)  acc1: 66.6667 (58.7302)  acc5: 100.0000 (94.1799)  time: 0.2278 (0.1686 -- 0.6913)  data: 0.0246 (0.0001 -- 0.4798)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3155 (1.3499)  acc1: 55.5556 (58.5062)  acc5: 100.0000 (94.6058)  time: 0.2108 (0.1328 -- 0.6913)  data: 0.0243 (0.0001 -- 0.4798)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 62.863 Acc@5 94.606 loss 1.322
Accuracy of the network on the 482 val images: 62.86%
[2023-09-04 22:26:50,955] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:26:50,957] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:26:50,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:26:50,957] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:26:52,330] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:26:52,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.86%
Epoch: [22]  [  0/160]  eta: 0:21:39  lr: 0.000034  min_lr: 0.000009  loss: 1.9689 (1.9689)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6033 (3.6033)  time: 8.1189 (8.1189 -- 8.1189)  data: 7.5965 (7.5965 -- 7.5965)  max mem: 16413
[2023-09-04 22:27:17,249] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3537
[2023-09-04 22:27:17,249] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3537
[2023-09-04 22:27:17,249] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:27:17,249] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:27:17,249] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [ 20/160]  eta: 0:02:56  lr: 0.000035  min_lr: 0.000009  loss: 2.1305 (2.0692)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3425 (5.8915)  time: 0.9194 (0.5230 -- 3.1147)  data: 0.1637 (0.0003 -- 2.2011)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:04  lr: 0.000035  min_lr: 0.000009  loss: 2.2426 (2.1415)  loss_scale: 16384.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9005 (6.4506)  time: 0.7949 (0.5329 -- 2.7681)  data: 0.0016 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:38  lr: 0.000035  min_lr: 0.000009  loss: 2.3547 (2.1842)  loss_scale: 16384.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6524 (6.5548)  time: 0.8888 (0.5177 -- 2.7051)  data: 0.1513 (0.0004 -- 2.1708)  max mem: 16413
Epoch: [22]  [ 80/160]  eta: 0:01:15  lr: 0.000035  min_lr: 0.000009  loss: 2.1384 (2.1600)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4913 (6.5143)  time: 0.8371 (0.5311 -- 1.7646)  data: 0.0932 (0.0004 -- 1.0728)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:55  lr: 0.000035  min_lr: 0.000009  loss: 2.1253 (2.1448)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2854 (6.5828)  time: 0.8339 (0.5250 -- 1.8482)  data: 0.1321 (0.0001 -- 1.1651)  max mem: 16413
Epoch: [22]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 2.0795 (2.1392)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2789 (6.5453)  time: 1.0656 (0.5273 -- 4.2225)  data: 0.1805 (0.0006 -- 2.0730)  max mem: 16413
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 2.1526 (2.1402)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2609 (6.5229)  time: 0.8019 (0.5117 -- 3.2455)  data: 0.0023 (0.0004 -- 0.0168)  max mem: 16413
[2023-09-04 22:29:08,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:29:08,474] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:29:08,474] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:29:08,474] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 2.1336 (2.1401)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (6.4349)  time: 0.6473 (0.4957 -- 2.4509)  data: 0.0008 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [22] Total time: 0:02:23 (0.8961 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 2.1336 (2.1162)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2138 (6.4349)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.3092 (1.3092)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4096 (2.4096 -- 2.4096)  data: 2.1929 (2.1929 -- 2.1929)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1902 (1.3234)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (95.9596)  time: 0.4240 (0.2032 -- 2.4096)  data: 0.2101 (0.0006 -- 2.1929)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1902 (1.2730)  acc1: 55.5556 (60.3175)  acc5: 100.0000 (96.2963)  time: 0.2206 (0.1689 -- 0.4284)  data: 0.0176 (0.0001 -- 0.2302)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2738 (1.3108)  acc1: 55.5556 (60.9959)  acc5: 100.0000 (95.8506)  time: 0.2033 (0.1332 -- 0.4284)  data: 0.0166 (0.0001 -- 0.2302)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 62.448 Acc@5 95.021 loss 1.299
Accuracy of the network on the 482 val images: 62.45%
Max accuracy: 62.86%
Epoch: [23]  [  0/160]  eta: 0:22:17  lr: 0.000036  min_lr: 0.000009  loss: 2.3393 (2.3393)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1006 (7.1006)  time: 8.3614 (8.3614 -- 8.3614)  data: 7.8100 (7.8100 -- 7.8100)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:45  lr: 0.000036  min_lr: 0.000009  loss: 2.1022 (2.0840)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4839 (5.7074)  time: 0.8211 (0.5210 -- 4.3526)  data: 0.2756 (0.0004 -- 3.8084)  max mem: 16413
[2023-09-04 22:30:04,896] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3719
[2023-09-04 22:30:04,896] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:30:04,897] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3719
[2023-09-04 22:30:04,897] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:30:04,897] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [ 40/160]  eta: 0:02:02  lr: 0.000036  min_lr: 0.000009  loss: 2.0695 (2.0657)  loss_scale: 32768.0000 (31968.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4175 (5.9414)  time: 0.8575 (0.5118 -- 2.9702)  data: 0.2988 (0.0003 -- 2.4378)  max mem: 16413
Epoch: [23]  [ 60/160]  eta: 0:01:43  lr: 0.000037  min_lr: 0.000009  loss: 2.2798 (2.1284)  loss_scale: 16384.0000 (26859.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0246 (6.1973)  time: 1.0611 (0.5143 -- 4.1807)  data: 0.5230 (0.0002 -- 3.6133)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:17  lr: 0.000037  min_lr: 0.000009  loss: 2.1684 (2.1326)  loss_scale: 16384.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9623 (6.0434)  time: 0.7713 (0.5059 -- 3.7339)  data: 0.2374 (0.0002 -- 3.2039)  max mem: 16413
Epoch: [23]  [100/160]  eta: 0:00:57  lr: 0.000037  min_lr: 0.000009  loss: 2.1681 (2.1286)  loss_scale: 16384.0000 (22710.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7890 (6.3150)  time: 0.9035 (0.5274 -- 3.1108)  data: 0.3557 (0.0003 -- 2.5672)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000009  loss: 2.1115 (2.1317)  loss_scale: 16384.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9374 (6.3498)  time: 0.7086 (0.5403 -- 1.7299)  data: 0.1294 (0.0002 -- 1.2119)  max mem: 16413
Epoch: [23]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 2.0429 (2.1277)  loss_scale: 16384.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0005 (6.4074)  time: 0.8789 (0.5252 -- 3.0905)  data: 0.1626 (0.0002 -- 1.5956)  max mem: 16413
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 2.0461 (2.1234)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8254 (6.4038)  time: 0.7441 (0.5007 -- 2.4433)  data: 0.0364 (0.0002 -- 0.2378)  max mem: 16413
Epoch: [23] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 2.0461 (2.1212)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8254 (6.4038)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.2998 (1.2998)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4562 (2.4562 -- 2.4562)  data: 2.2287 (2.2287 -- 2.2287)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2224 (1.3078)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (96.9697)  time: 0.4375 (0.2047 -- 2.4562)  data: 0.2207 (0.0004 -- 2.2287)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2091 (1.2660)  acc1: 55.5556 (58.7302)  acc5: 100.0000 (96.8254)  time: 0.2189 (0.1697 -- 0.3506)  data: 0.0142 (0.0001 -- 0.1304)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3002 (1.3084)  acc1: 55.5556 (57.6764)  acc5: 100.0000 (95.8506)  time: 0.2035 (0.1334 -- 0.3506)  data: 0.0139 (0.0001 -- 0.1304)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 61.826 Acc@5 94.813 loss 1.290
Accuracy of the network on the 482 val images: 61.83%
Max accuracy: 62.86%
Epoch: [24]  [  0/160]  eta: 0:19:33  lr: 0.000038  min_lr: 0.000010  loss: 1.8105 (1.8105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4075 (4.4075)  time: 7.3363 (7.3363 -- 7.3363)  data: 6.3229 (6.3229 -- 6.3229)  max mem: 16413
[2023-09-04 22:32:09,442] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:32:09,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:32:09,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:32:09,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 20/160]  eta: 0:02:50  lr: 0.000038  min_lr: 0.000010  loss: 2.0732 (2.0428)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4975 (6.8827)  time: 0.9145 (0.5296 -- 4.3618)  data: 0.1145 (0.0003 -- 1.4065)  max mem: 16413
Epoch: [24]  [ 40/160]  eta: 0:02:04  lr: 0.000038  min_lr: 0.000010  loss: 1.9647 (2.0202)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4017 (6.6601)  time: 0.8515 (0.5233 -- 3.1773)  data: 0.1351 (0.0004 -- 1.3162)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:43  lr: 0.000038  min_lr: 0.000010  loss: 1.9552 (2.0165)  loss_scale: 32768.0000 (30619.2787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2007 (6.6270)  time: 1.0189 (0.5261 -- 3.6810)  data: 0.3211 (0.0004 -- 3.1381)  max mem: 16413
Epoch: [24]  [ 80/160]  eta: 0:01:18  lr: 0.000038  min_lr: 0.000010  loss: 2.0782 (2.0421)  loss_scale: 32768.0000 (31149.8272)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7602 (6.5895)  time: 0.8432 (0.5059 -- 3.6439)  data: 0.3106 (0.0002 -- 3.1253)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:58  lr: 0.000038  min_lr: 0.000010  loss: 1.8364 (2.0046)  loss_scale: 32768.0000 (31470.2574)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6868 (6.5341)  time: 0.9208 (0.5270 -- 4.3192)  data: 0.3750 (0.0002 -- 3.7737)  max mem: 16413
Epoch: [24]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.9566 (2.0024)  loss_scale: 32768.0000 (31684.7603)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3922 (6.5705)  time: 0.7968 (0.5318 -- 3.8702)  data: 0.2483 (0.0002 -- 3.3375)  max mem: 16413
[2023-09-04 22:34:01,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:34:01,793] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:34:01,794] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:34:01,794] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:34:02,856] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3978
[2023-09-04 22:34:02,856] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3978
[2023-09-04 22:34:02,857] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:34:02,857] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:34:02,857] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 2.0784 (2.0223)  loss_scale: 32768.0000 (32303.2057)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9662 (6.5183)  time: 0.9001 (0.5194 -- 3.0359)  data: 0.3502 (0.0003 -- 2.5018)  max mem: 16413
[2023-09-04 22:34:17,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[9.928749590560217e-06, 9.928749590560217e-06, 1.1031943989511353e-05, 1.1031943989511353e-05, 1.22577155439015e-05, 1.22577155439015e-05, 1.3619683937668335e-05, 1.3619683937668335e-05, 1.5132982152964816e-05, 1.5132982152964816e-05, 1.6814424614405352e-05, 1.6814424614405352e-05, 1.8682694016005945e-05, 1.8682694016005945e-05, 2.0758548906673268e-05, 2.0758548906673268e-05, 2.3065054340748078e-05, 2.3065054340748078e-05, 2.562783815638675e-05, 2.562783815638675e-05, 2.8475375729318614e-05, 2.8475375729318614e-05, 3.163930636590957e-05, 3.163930636590957e-05, 3.5154784851010635e-05, 3.5154784851010635e-05, 3.906087205667848e-05, 3.906087205667848e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 22:34:17,948] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=17.368242597609115, CurrSamplesPerSec=24.65017922586377, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 2.1128 (2.0258)  loss_scale: 32768.0000 (32358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0785 (6.4541)  time: 0.7240 (0.4947 -- 2.7090)  data: 0.2034 (0.0001 -- 2.1718)  max mem: 16413
Epoch: [24] Total time: 0:02:24 (0.9003 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 2.1128 (2.0452)  loss_scale: 32768.0000 (32358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0785 (6.4541)
Val:  [ 0/27]  eta: 0:00:58  loss: 1.2334 (1.2334)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.1487 (2.1487 -- 2.1487)  data: 1.9175 (1.9175 -- 1.9175)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1553 (1.2341)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (96.9697)  time: 0.4280 (0.2083 -- 2.1487)  data: 0.2002 (0.0009 -- 1.9175)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1110 (1.2136)  acc1: 55.5556 (59.2593)  acc5: 100.0000 (96.2963)  time: 0.2329 (0.1776 -- 0.5312)  data: 0.0183 (0.0001 -- 0.2731)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2062 (1.2536)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (95.4357)  time: 0.2139 (0.1328 -- 0.5312)  data: 0.0179 (0.0001 -- 0.2731)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 63.693 Acc@5 94.813 loss 1.217
Accuracy of the network on the 482 val images: 63.69%
[2023-09-04 22:34:25,764] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:34:25,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:34:25,766] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:34:25,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:34:27,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:34:27,196] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.69%
Epoch: [25]  [  0/160]  eta: 0:18:35  lr: 0.000039  min_lr: 0.000010  loss: 2.2237 (2.2237)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0298 (4.0298)  time: 6.9689 (6.9689 -- 6.9689)  data: 6.4029 (6.4029 -- 6.4029)  max mem: 16413
[2023-09-04 22:34:42,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4009
[2023-09-04 22:34:42,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4009
[2023-09-04 22:34:42,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:34:42,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:34:42,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [ 20/160]  eta: 0:02:58  lr: 0.000039  min_lr: 0.000010  loss: 2.0802 (2.1488)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7698 (5.9848)  time: 0.9928 (0.5211 -- 2.9528)  data: 0.2983 (0.0009 -- 2.4278)  max mem: 16413
Epoch: [25]  [ 40/160]  eta: 0:02:05  lr: 0.000039  min_lr: 0.000010  loss: 1.9954 (2.0933)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8028 (6.0323)  time: 0.8044 (0.5315 -- 3.0935)  data: 0.0015 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:39  lr: 0.000040  min_lr: 0.000010  loss: 2.2534 (2.1281)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5778 (6.1071)  time: 0.8860 (0.5185 -- 3.6360)  data: 0.0016 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [25]  [ 80/160]  eta: 0:01:15  lr: 0.000040  min_lr: 0.000010  loss: 1.9949 (2.1087)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9696 (6.0969)  time: 0.7996 (0.5306 -- 3.0369)  data: 0.0723 (0.0004 -- 0.9423)  max mem: 16413
Epoch: [25]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 2.0051 (2.0911)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5892 (6.0305)  time: 0.8698 (0.5418 -- 3.2372)  data: 0.0041 (0.0007 -- 0.0528)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000010  loss: 1.9815 (2.0701)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4912 (6.0775)  time: 0.7909 (0.5373 -- 3.0462)  data: 0.1996 (0.0004 -- 2.5083)  max mem: 16413
[2023-09-04 22:36:32,025] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:36:32,025] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:36:32,025] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:36:32,025] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 2.1034 (2.0705)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6332 (6.1928)  time: 0.8969 (0.5250 -- 2.4868)  data: 0.2162 (0.0010 -- 1.9491)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 2.0951 (2.0644)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6546 (6.3121)  time: 0.8088 (0.4930 -- 2.4945)  data: 0.2851 (0.0002 -- 1.9653)  max mem: 16413
Epoch: [25] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 2.0951 (2.0706)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6546 (6.3121)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.2381 (1.2381)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4746 (2.4746 -- 2.4746)  data: 2.2064 (2.2064 -- 2.2064)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0331 (1.2207)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (96.9697)  time: 0.4280 (0.2091 -- 2.4746)  data: 0.2027 (0.0011 -- 2.2064)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0331 (1.1772)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (96.2963)  time: 0.2130 (0.1697 -- 0.2796)  data: 0.0014 (0.0001 -- 0.0074)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2251 (1.2306)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (95.0207)  time: 0.1949 (0.1328 -- 0.2391)  data: 0.0006 (0.0001 -- 0.0027)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 63.900 Acc@5 94.606 loss 1.208
Accuracy of the network on the 482 val images: 63.90%
[2023-09-04 22:36:56,489] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:36:56,491] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:36:56,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:36:56,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:36:57,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:36:57,893] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.90%
Epoch: [26]  [  0/160]  eta: 0:21:20  lr: 0.000041  min_lr: 0.000010  loss: 1.4633 (1.4633)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6176 (4.6176)  time: 8.0043 (8.0043 -- 8.0043)  data: 5.2574 (5.2574 -- 5.2574)  max mem: 16413
Epoch: [26]  [ 20/160]  eta: 0:02:42  lr: 0.000041  min_lr: 0.000010  loss: 2.2332 (2.1822)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0852 (6.4625)  time: 0.8174 (0.5355 -- 3.2404)  data: 0.0015 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:12  lr: 0.000041  min_lr: 0.000010  loss: 2.0786 (2.1448)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4792 (6.3927)  time: 1.0452 (0.5132 -- 4.5477)  data: 0.0024 (0.0004 -- 0.0118)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:41  lr: 0.000041  min_lr: 0.000010  loss: 2.1667 (2.1421)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4373 (6.2837)  time: 0.8325 (0.5225 -- 4.0264)  data: 0.0014 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:19  lr: 0.000041  min_lr: 0.000011  loss: 2.1014 (2.1125)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6296 (6.3497)  time: 0.9231 (0.5279 -- 3.3075)  data: 0.0013 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [26]  [100/160]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000011  loss: 2.2092 (2.1100)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9313 (6.3538)  time: 0.8063 (0.5211 -- 3.1653)  data: 0.0021 (0.0002 -- 0.0112)  max mem: 16413
[2023-09-04 22:38:40,425] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:38:40,425] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:38:40,425] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:38:40,425] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:38:49,956] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4278
[2023-09-04 22:38:49,956] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4278
[2023-09-04 22:38:49,956] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:38:49,956] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:38:49,956] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [26]  [120/160]  eta: 0:00:38  lr: 0.000042  min_lr: 0.000011  loss: 2.1828 (2.1080)  loss_scale: 65536.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5112 (6.3821)  time: 0.9514 (0.5290 -- 3.3014)  data: 0.0014 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 2.1246 (2.0994)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8745 (6.4226)  time: 0.6689 (0.5197 -- 2.4364)  data: 0.0016 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 2.1002 (2.0995)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7259 (6.3999)  time: 0.7406 (0.4954 -- 3.1502)  data: 0.0008 (0.0003 -- 0.0020)  max mem: 16413
Epoch: [26] Total time: 0:02:23 (0.8949 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 2.1002 (2.0804)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7259 (6.3999)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.2050 (1.2050)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2683 (2.2683 -- 2.2683)  data: 2.0192 (2.0192 -- 2.0192)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1142 (1.1630)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4221 (0.1944 -- 2.2683)  data: 0.2024 (0.0007 -- 2.0192)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0314 (1.1295)  acc1: 55.5556 (60.8466)  acc5: 100.0000 (95.7672)  time: 0.2270 (0.1716 -- 0.3901)  data: 0.0192 (0.0001 -- 0.1670)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0772 (1.1746)  acc1: 66.6667 (60.9959)  acc5: 100.0000 (94.6058)  time: 0.2122 (0.1330 -- 0.3901)  data: 0.0190 (0.0001 -- 0.1670)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 63.900 Acc@5 94.398 loss 1.137
Accuracy of the network on the 482 val images: 63.90%
[2023-09-04 22:39:28,873] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:39:28,874] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:39:28,874] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:39:28,874] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:39:30,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:39:30,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.90%
Epoch: [27]  [  0/160]  eta: 0:19:12  lr: 0.000042  min_lr: 0.000011  loss: 2.1257 (2.1257)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4839 (5.4839)  time: 7.2010 (7.2010 -- 7.2010)  data: 4.8753 (4.8753 -- 4.8753)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:46  lr: 0.000042  min_lr: 0.000011  loss: 2.0045 (1.9948)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0345 (6.1454)  time: 0.8853 (0.5238 -- 4.4528)  data: 0.0875 (0.0002 -- 1.1328)  max mem: 16413
[2023-09-04 22:40:09,851] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4358
[2023-09-04 22:40:09,851] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:40:09,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 22:40:09,851] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4358
[2023-09-04 22:40:09,852] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [27]  [ 40/160]  eta: 0:02:07  lr: 0.000043  min_lr: 0.000011  loss: 1.9521 (1.9927)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7917 (5.7558)  time: 0.9254 (0.5120 -- 3.4753)  data: 0.1861 (0.0006 -- 1.9523)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 2.0892 (1.9990)  loss_scale: 16384.0000 (26590.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7893 (6.2174)  time: 0.8448 (0.5306 -- 3.6713)  data: 0.1039 (0.0003 -- 1.5154)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:17  lr: 0.000043  min_lr: 0.000011  loss: 2.0223 (2.0086)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6098 (6.2910)  time: 0.8893 (0.5199 -- 3.7073)  data: 0.0270 (0.0005 -- 0.4998)  max mem: 16413
[2023-09-04 22:41:01,458] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4416
[2023-09-04 22:41:01,459] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 22:41:01,459] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4416
[2023-09-04 22:41:01,459] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-04 22:41:01,459] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [27]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 2.1423 (2.0313)  loss_scale: 16384.0000 (22142.7327)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1958 (6.3723)  time: 0.8444 (0.5253 -- 2.1836)  data: 0.1224 (0.0005 -- 1.4298)  max mem: 16413
Epoch: [27]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 2.0215 (2.0340)  loss_scale: 8192.0000 (19836.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1633 (6.3980)  time: 0.8099 (0.5269 -- 2.6790)  data: 0.0685 (0.0003 -- 0.7258)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 2.0564 (2.0307)  loss_scale: 8192.0000 (18185.0780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2098 (6.4184)  time: 0.9241 (0.5182 -- 3.8581)  data: 0.1654 (0.0004 -- 1.4667)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 2.1158 (2.0380)  loss_scale: 8192.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3276 (6.4664)  time: 0.6086 (0.4950 -- 1.6981)  data: 0.0680 (0.0002 -- 1.1530)  max mem: 16413
Epoch: [27] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 2.1158 (2.0503)  loss_scale: 8192.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3276 (6.4664)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.1441 (1.1441)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2718 (2.2718 -- 2.2718)  data: 2.0680 (2.0680 -- 2.0680)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.1076 (1.2099)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (96.9697)  time: 0.4041 (0.1930 -- 2.2718)  data: 0.1904 (0.0011 -- 2.0680)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0836 (1.1634)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (96.8254)  time: 0.2222 (0.1696 -- 0.3762)  data: 0.0159 (0.0001 -- 0.1590)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2087 (1.2059)  acc1: 66.6667 (61.8257)  acc5: 100.0000 (95.0207)  time: 0.2094 (0.1331 -- 0.3762)  data: 0.0155 (0.0001 -- 0.1590)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 65.353 Acc@5 94.813 loss 1.167
Accuracy of the network on the 482 val images: 65.35%
[2023-09-04 22:41:59,299] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:41:59,301] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:41:59,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:41:59,301] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:42:00,826] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:42:00,827] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.35%
Epoch: [28]  [  0/160]  eta: 0:17:44  lr: 0.000044  min_lr: 0.000011  loss: 2.0779 (2.0779)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7758 (8.7758)  time: 6.6517 (6.6517 -- 6.6517)  data: 5.4141 (5.4141 -- 5.4141)  max mem: 16413
Epoch: [28]  [ 20/160]  eta: 0:02:41  lr: 0.000044  min_lr: 0.000011  loss: 1.8837 (1.9112)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1811 (6.6597)  time: 0.8789 (0.5340 -- 2.7385)  data: 0.1364 (0.0004 -- 1.4111)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:02:10  lr: 0.000044  min_lr: 0.000011  loss: 2.1437 (1.9797)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5984 (6.6029)  time: 1.0182 (0.5236 -- 4.0859)  data: 0.0017 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [28]  [ 60/160]  eta: 0:01:35  lr: 0.000044  min_lr: 0.000011  loss: 1.8899 (1.9703)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4154 (6.5392)  time: 0.6933 (0.5186 -- 3.1376)  data: 0.0016 (0.0005 -- 0.0030)  max mem: 16413
[2023-09-04 22:43:05,379] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:43:05,380] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:43:05,380] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-04 22:43:05,381] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [28]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000011  loss: 2.0157 (1.9856)  loss_scale: 16384.0000 (9810.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7915 (6.4805)  time: 1.0288 (0.5140 -- 3.8517)  data: 0.1554 (0.0004 -- 1.8952)  max mem: 16413
Epoch: [28]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 2.1355 (2.0127)  loss_scale: 16384.0000 (11111.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2988 (6.5467)  time: 0.7903 (0.5225 -- 4.2731)  data: 0.0013 (0.0004 -- 0.0022)  max mem: 16413
Epoch: [28]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.9430 (2.0052)  loss_scale: 16384.0000 (11983.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0784 (6.5386)  time: 0.8859 (0.5288 -- 3.4019)  data: 0.0014 (0.0006 -- 0.0040)  max mem: 16413
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.9063 (2.0034)  loss_scale: 16384.0000 (12607.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1742 (6.5620)  time: 0.8626 (0.5219 -- 3.3233)  data: 0.0025 (0.0004 -- 0.0138)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.9941 (1.9998)  loss_scale: 16384.0000 (13056.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0922 (6.5193)  time: 0.7191 (0.4973 -- 2.6102)  data: 0.0007 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [28] Total time: 0:02:23 (0.8981 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.9941 (2.0246)  loss_scale: 16384.0000 (13056.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0922 (6.5193)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.0058 (1.0058)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4382 (2.4382 -- 2.4382)  data: 2.2157 (2.2157 -- 2.2157)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1060 (1.1674)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (96.9697)  time: 0.4283 (0.1997 -- 2.4382)  data: 0.2128 (0.0007 -- 2.2157)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9721 (1.1121)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (95.7672)  time: 0.2277 (0.1692 -- 0.5053)  data: 0.0226 (0.0001 -- 0.3246)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0755 (1.1604)  acc1: 66.6667 (61.8257)  acc5: 100.0000 (95.0207)  time: 0.2133 (0.1323 -- 0.5053)  data: 0.0223 (0.0001 -- 0.3246)  max mem: 16413
Val: Total time: 0:00:07 (0.2953 s / it)
* Acc@1 65.145 Acc@5 95.228 loss 1.131
Accuracy of the network on the 482 val images: 65.15%
Max accuracy: 65.35%
Epoch: [29]  [  0/160]  eta: 0:19:55  lr: 0.000045  min_lr: 0.000012  loss: 2.5829 (2.5829)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6001 (3.6001)  time: 7.4726 (7.4726 -- 7.4726)  data: 6.9424 (6.9424 -- 6.9424)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:41  lr: 0.000046  min_lr: 0.000012  loss: 2.1580 (2.1687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0504 (6.8525)  time: 0.8344 (0.5201 -- 2.6790)  data: 0.2050 (0.0004 -- 2.1232)  max mem: 16413
[2023-09-04 22:45:09,593] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:45:09,594] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:45:09,595] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:45:09,595] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [ 40/160]  eta: 0:02:07  lr: 0.000046  min_lr: 0.000012  loss: 1.9496 (2.0581)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2674 (6.1519)  time: 0.9689 (0.5229 -- 2.8354)  data: 0.3322 (0.0004 -- 2.2842)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 2.0364 (2.0850)  loss_scale: 32768.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6417 (6.3144)  time: 0.8441 (0.5304 -- 4.1320)  data: 0.2892 (0.0006 -- 3.5932)  max mem: 16413
[2023-09-04 22:45:38,138] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4706
[2023-09-04 22:45:38,138] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4706
[2023-09-04 22:45:38,139] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:45:38,139] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:45:38,139] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 2.0388 (2.0710)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6581 (6.5004)  time: 0.9006 (0.5438 -- 3.6732)  data: 0.3434 (0.0002 -- 3.1050)  max mem: 16413
Epoch: [29]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.8885 (2.0455)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3322 (6.3832)  time: 0.8501 (0.5294 -- 4.0273)  data: 0.3038 (0.0001 -- 3.5096)  max mem: 16413
Epoch: [29]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.0170 (2.0342)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4177 (6.4467)  time: 0.8751 (0.5168 -- 2.9182)  data: 0.3268 (0.0004 -- 2.4040)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9737 (2.0224)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1674 (6.5095)  time: 0.7600 (0.5245 -- 2.6524)  data: 0.2050 (0.0003 -- 2.1310)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0068 (2.0248)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9038 (6.4641)  time: 0.6932 (0.4950 -- 2.4253)  data: 0.1625 (0.0002 -- 1.8856)  max mem: 16413
Epoch: [29] Total time: 0:02:21 (0.8843 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0068 (2.0090)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9038 (6.4641)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.1694 (1.1694)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3325 (2.3325 -- 2.3325)  data: 2.1011 (2.1011 -- 2.1011)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0247 (1.0834)  acc1: 55.5556 (64.6465)  acc5: 100.0000 (96.9697)  time: 0.4189 (0.2054 -- 2.3325)  data: 0.2027 (0.0008 -- 2.1011)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0247 (1.0421)  acc1: 66.6667 (65.6085)  acc5: 100.0000 (95.7672)  time: 0.2198 (0.1731 -- 0.3397)  data: 0.0145 (0.0001 -- 0.1183)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0791 (1.0903)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (94.6058)  time: 0.2054 (0.1334 -- 0.3397)  data: 0.0141 (0.0001 -- 0.1183)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 67.220 Acc@5 94.813 loss 1.065
Accuracy of the network on the 482 val images: 67.22%
[2023-09-04 22:47:01,725] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:47:01,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:47:01,727] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:47:01,727] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:47:03,208] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:47:03,208] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.22%
Epoch: [30]  [  0/160]  eta: 0:17:49  lr: 0.000047  min_lr: 0.000012  loss: 1.8318 (1.8318)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1742 (5.1742)  time: 6.6823 (6.6823 -- 6.6823)  data: 6.1429 (6.1429 -- 6.1429)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:03:06  lr: 0.000047  min_lr: 0.000012  loss: 1.8520 (1.9748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2722 (6.8338)  time: 1.0620 (0.5047 -- 4.7276)  data: 0.4252 (0.0003 -- 4.2220)  max mem: 16413
[2023-09-04 22:47:44,041] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:47:44,041] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:47:44,041] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:47:44,041] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000012  loss: 2.0470 (1.9729)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1988 (6.5893)  time: 0.7850 (0.5043 -- 4.5387)  data: 0.2511 (0.0003 -- 4.0006)  max mem: 16413
Epoch: [30]  [ 60/160]  eta: 0:01:44  lr: 0.000047  min_lr: 0.000012  loss: 1.9494 (1.9557)  loss_scale: 32768.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1249 (6.7023)  time: 0.9988 (0.5173 -- 4.2341)  data: 0.1723 (0.0003 -- 1.9870)  max mem: 16413
Epoch: [30]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 1.7912 (1.9239)  loss_scale: 32768.0000 (25688.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0830 (6.7266)  time: 0.7513 (0.5170 -- 3.1595)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 1.9513 (1.9443)  loss_scale: 32768.0000 (27090.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5765 (6.8351)  time: 0.8902 (0.5181 -- 4.1103)  data: 0.0636 (0.0001 -- 0.9105)  max mem: 16413
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0302 (1.9567)  loss_scale: 32768.0000 (28028.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6172 (6.8631)  time: 0.8322 (0.5262 -- 2.3854)  data: 0.0282 (0.0003 -- 0.5391)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0888 (1.9723)  loss_scale: 32768.0000 (28701.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0495 (6.8062)  time: 0.8540 (0.5273 -- 2.7157)  data: 0.2527 (0.0001 -- 2.1213)  max mem: 16413
[2023-09-04 22:49:22,131] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4951
[2023-09-04 22:49:22,131] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4951
[2023-09-04 22:49:22,131] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:49:22,131] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:49:22,131] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0000 (1.9812)  loss_scale: 32768.0000 (28262.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6398 (6.8415)  time: 0.7263 (0.4969 -- 2.2619)  data: 0.0987 (0.0002 -- 1.7324)  max mem: 16413
Epoch: [30] Total time: 0:02:23 (0.8948 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0000 (1.9672)  loss_scale: 32768.0000 (28262.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6398 (6.8415)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.9434 (0.9434)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4259 (2.4259 -- 2.4259)  data: 2.2056 (2.2056 -- 2.2056)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8870 (1.0485)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (96.9697)  time: 0.4212 (0.1916 -- 2.4259)  data: 0.2014 (0.0005 -- 2.2056)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8870 (1.0336)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (95.7672)  time: 0.2133 (0.1697 -- 0.2797)  data: 0.0049 (0.0001 -- 0.0850)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0125 (1.0847)  acc1: 66.6667 (64.3154)  acc5: 100.0000 (95.0207)  time: 0.1945 (0.1341 -- 0.2797)  data: 0.0047 (0.0001 -- 0.0850)  max mem: 16413
Val: Total time: 0:00:07 (0.2843 s / it)
* Acc@1 67.635 Acc@5 94.813 loss 1.037
Accuracy of the network on the 482 val images: 67.63%
[2023-09-04 22:49:34,105] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:49:34,107] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:49:34,107] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:49:34,107] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:49:35,634] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:49:35,635] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.63%
Epoch: [31]  [  0/160]  eta: 0:21:49  lr: 0.000047  min_lr: 0.000012  loss: 1.1833 (1.1833)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9253 (6.9253)  time: 8.1853 (8.1853 -- 8.1853)  data: 7.6536 (7.6536 -- 7.6536)  max mem: 16413
Epoch: [31]  [ 20/160]  eta: 0:02:48  lr: 0.000047  min_lr: 0.000012  loss: 2.1014 (1.9776)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5130 (7.3357)  time: 0.8511 (0.5272 -- 2.8555)  data: 0.1153 (0.0003 -- 2.1089)  max mem: 16413
[2023-09-04 22:50:16,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[1.1914372866301914e-05, 1.1914372866301914e-05, 1.3238192073668794e-05, 1.3238192073668794e-05, 1.4709102304076434e-05, 1.4709102304076434e-05, 1.6343447004529372e-05, 1.6343447004529372e-05, 1.815938556058819e-05, 1.815938556058819e-05, 2.0177095067320214e-05, 2.0177095067320214e-05, 2.2418994519244678e-05, 2.2418994519244678e-05, 2.4909993910271866e-05, 2.4909993910271866e-05, 2.7677771011413185e-05, 2.7677771011413185e-05, 3.07530789015702e-05, 3.07530789015702e-05, 3.4170087668411335e-05, 3.4170087668411335e-05, 3.796676407601259e-05, 3.796676407601259e-05, 4.218529341779177e-05, 4.218529341779177e-05, 4.6872548241990855e-05, 4.6872548241990855e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 22:50:16,611] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=17.195170863253807, CurrSamplesPerSec=21.613754709737595, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:02:01  lr: 0.000047  min_lr: 0.000012  loss: 2.0977 (2.0201)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3655 (6.9736)  time: 0.8141 (0.5220 -- 2.4289)  data: 0.0016 (0.0003 -- 0.0052)  max mem: 16413
Epoch: [31]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000012  loss: 1.9026 (1.9968)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0125 (7.0145)  time: 1.0047 (0.5341 -- 4.5395)  data: 0.0014 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [31]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.0609 (1.9983)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4173 (6.9291)  time: 0.7758 (0.5280 -- 2.9729)  data: 0.0014 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 2.1717 (2.0185)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8745 (6.7888)  time: 0.8012 (0.5288 -- 2.4240)  data: 0.0016 (0.0002 -- 0.0043)  max mem: 16413
[2023-09-04 22:51:26,367] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:51:26,367] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:51:26,369] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:51:26,369] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.9867 (2.0242)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1599 (6.8300)  time: 0.8779 (0.5141 -- 2.6409)  data: 0.0237 (0.0002 -- 0.4434)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000012  loss: 1.8125 (2.0065)  loss_scale: 32768.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8534 (6.7908)  time: 0.8045 (0.5265 -- 2.3566)  data: 0.1028 (0.0004 -- 1.3528)  max mem: 16413
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0521 (2.0015)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3853 (6.7584)  time: 0.7497 (0.4971 -- 3.0308)  data: 0.1305 (0.0001 -- 2.4738)  max mem: 16413
Epoch: [31] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0521 (2.0006)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3853 (6.7584)
Val:  [ 0/27]  eta: 0:00:59  loss: 1.0057 (1.0057)  acc1: 55.5556 (55.5556)  acc5: 88.8889 (88.8889)  time: 2.2160 (2.2160 -- 2.2160)  data: 1.9967 (1.9967 -- 1.9967)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0057 (1.0843)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (93.9394)  time: 0.4294 (0.2020 -- 2.2160)  data: 0.2039 (0.0005 -- 1.9967)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8911 (1.0390)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (95.2381)  time: 0.2280 (0.1694 -- 0.4033)  data: 0.0125 (0.0001 -- 0.1405)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0312 (1.0843)  acc1: 55.5556 (62.2407)  acc5: 100.0000 (94.6058)  time: 0.2129 (0.1327 -- 0.4033)  data: 0.0121 (0.0001 -- 0.1405)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 68.257 Acc@5 95.228 loss 1.031
Accuracy of the network on the 482 val images: 68.26%
[2023-09-04 22:52:04,725] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:52:04,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:52:04,727] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:52:04,727] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:52:06,074] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:52:06,074] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.26%
Epoch: [32]  [  0/160]  eta: 0:23:14  lr: 0.000047  min_lr: 0.000012  loss: 2.4655 (2.4655)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5760 (5.5760)  time: 8.7131 (8.7131 -- 8.7131)  data: 8.1754 (8.1754 -- 8.1754)  max mem: 16413
Epoch: [32]  [ 20/160]  eta: 0:03:02  lr: 0.000047  min_lr: 0.000012  loss: 1.9612 (1.9941)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2531 (6.6028)  time: 0.9348 (0.5098 -- 5.0833)  data: 0.2500 (0.0004 -- 2.8727)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000012  loss: 1.9756 (2.0022)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3096 (6.7098)  time: 0.8066 (0.5156 -- 4.1481)  data: 0.0017 (0.0002 -- 0.0064)  max mem: 16413
Epoch: [32]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000012  loss: 1.8919 (1.9854)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7396 (6.5692)  time: 0.9630 (0.5245 -- 4.9931)  data: 0.0206 (0.0003 -- 0.3854)  max mem: 16413
Epoch: [32]  [ 80/160]  eta: 0:01:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9932 (1.9774)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5671 (6.3979)  time: 0.8539 (0.5271 -- 3.6733)  data: 0.0015 (0.0004 -- 0.0035)  max mem: 16413
[2023-09-04 22:53:32,281] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:53:32,281] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:53:32,282] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:53:32,282] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:53:32,868] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5209
[2023-09-04 22:53:32,868] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5209
[2023-09-04 22:53:32,869] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:53:32,869] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:53:32,869] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [32]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.1104 (1.9994)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1699 (6.3307)  time: 0.7606 (0.5249 -- 2.8229)  data: 0.0020 (0.0004 -- 0.0148)  max mem: 16413
Epoch: [32]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.0998 (2.0248)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1150 (6.3537)  time: 0.8404 (0.5303 -- 2.8953)  data: 0.2295 (0.0005 -- 2.3514)  max mem: 16413
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9678 (2.0208)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5642 (6.3149)  time: 0.9481 (0.5309 -- 4.0783)  data: 0.3916 (0.0003 -- 3.5278)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0128 (2.0155)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1574 (6.3338)  time: 0.6216 (0.4937 -- 1.4027)  data: 0.0959 (0.0002 -- 0.8746)  max mem: 16413
Epoch: [32] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0128 (1.9838)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1574 (6.3338)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.8822 (0.8822)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5004 (2.5004 -- 2.5004)  data: 2.2978 (2.2978 -- 2.2978)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8822 (1.0466)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (94.9495)  time: 0.4342 (0.1978 -- 2.5004)  data: 0.2209 (0.0006 -- 2.2978)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8370 (1.0115)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (95.7672)  time: 0.2122 (0.1693 -- 0.3365)  data: 0.0068 (0.0001 -- 0.1084)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0464 (1.0688)  acc1: 66.6667 (64.3154)  acc5: 100.0000 (95.4357)  time: 0.1967 (0.1325 -- 0.3365)  data: 0.0059 (0.0001 -- 0.1084)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 69.502 Acc@5 95.228 loss 1.014
Accuracy of the network on the 482 val images: 69.50%
[2023-09-04 22:54:36,631] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:54:36,633] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:54:36,633] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:54:36,633] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:54:38,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:54:38,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.50%
Epoch: [33]  [  0/160]  eta: 0:22:58  lr: 0.000047  min_lr: 0.000012  loss: 2.1871 (2.1871)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0851 (6.0851)  time: 8.6161 (8.6161 -- 8.6161)  data: 5.3162 (5.3162 -- 5.3162)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:43  lr: 0.000047  min_lr: 0.000012  loss: 2.0353 (2.0162)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4013 (6.4715)  time: 0.7971 (0.5261 -- 3.4270)  data: 0.0019 (0.0008 -- 0.0048)  max mem: 16413
Epoch: [33]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000012  loss: 1.9744 (1.9765)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9173 (6.8448)  time: 0.9090 (0.5301 -- 2.3754)  data: 0.0016 (0.0004 -- 0.0069)  max mem: 16413
[2023-09-04 22:55:35,653] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:55:35,653] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:55:35,655] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:55:35,655] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [33]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1366 (2.0038)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0154 (7.0875)  time: 0.8001 (0.5454 -- 2.6334)  data: 0.0025 (0.0007 -- 0.0187)  max mem: 16413
[2023-09-04 22:55:43,399] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5346
[2023-09-04 22:55:43,399] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5346
[2023-09-04 22:55:43,399] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:55:43,399] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:55:43,399] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 22:55:43,934] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5347
[2023-09-04 22:55:43,934] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5347
[2023-09-04 22:55:43,934] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:55:43,934] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 22:55:43,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 1.9729 (1.9981)  loss_scale: 16384.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4275 (6.7244)  time: 0.9711 (0.5343 -- 3.8117)  data: 0.0014 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [33]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.8805 (1.9904)  loss_scale: 16384.0000 (29848.0792)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4429 (6.7082)  time: 0.8491 (0.5258 -- 4.1944)  data: 0.0016 (0.0005 -- 0.0031)  max mem: 16413
Epoch: [33]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.9591 (1.9807)  loss_scale: 16384.0000 (27622.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1672 (6.7188)  time: 0.9239 (0.5096 -- 5.1151)  data: 0.0012 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9425 (1.9664)  loss_scale: 16384.0000 (26028.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3031 (6.7739)  time: 0.6810 (0.5278 -- 2.4507)  data: 0.0019 (0.0007 -- 0.0043)  max mem: 16413
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0160 (1.9587)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7556 (7.0239)  time: 0.7329 (0.4978 -- 1.8344)  data: 0.1258 (0.0002 -- 1.2956)  max mem: 16413
Epoch: [33] Total time: 0:02:21 (0.8838 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0160 (1.9501)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7556 (7.0239)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.9137 (0.9137)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3178 (2.3178 -- 2.3178)  data: 2.0792 (2.0792 -- 2.0792)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9314 (1.0951)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (94.9495)  time: 0.4095 (0.2035 -- 2.3178)  data: 0.1959 (0.0007 -- 2.0792)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9299 (1.0335)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.2381)  time: 0.2234 (0.1705 -- 0.4325)  data: 0.0192 (0.0001 -- 0.2405)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9971 (1.0721)  acc1: 66.6667 (64.7303)  acc5: 100.0000 (95.4357)  time: 0.2090 (0.1330 -- 0.4325)  data: 0.0190 (0.0001 -- 0.2405)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 67.842 Acc@5 95.436 loss 1.025
Accuracy of the network on the 482 val images: 67.84%
Max accuracy: 69.50%
Epoch: [34]  [  0/160]  eta: 0:17:57  lr: 0.000047  min_lr: 0.000012  loss: 1.8088 (1.8088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0250 (7.0250)  time: 6.7321 (6.7321 -- 6.7321)  data: 5.6421 (5.6421 -- 5.6421)  max mem: 16413
Epoch: [34]  [ 20/160]  eta: 0:02:51  lr: 0.000047  min_lr: 0.000012  loss: 2.0322 (1.9974)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5122 (6.8021)  time: 0.9526 (0.5119 -- 3.2724)  data: 0.1662 (0.0003 -- 2.4078)  max mem: 16413
[2023-09-04 22:57:47,478] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:57:47,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 22:57:47,479] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:57:47,479] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [ 40/160]  eta: 0:02:08  lr: 0.000047  min_lr: 0.000012  loss: 2.0352 (1.9921)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5929 (6.7024)  time: 0.9033 (0.5223 -- 4.0540)  data: 0.0013 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [34]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000012  loss: 2.0462 (1.9744)  loss_scale: 32768.0000 (23098.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5601 (6.6493)  time: 0.7553 (0.5319 -- 2.1782)  data: 0.0015 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [34]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.0141 (1.9856)  loss_scale: 32768.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9599 (6.5991)  time: 0.9141 (0.5285 -- 3.8765)  data: 0.0410 (0.0003 -- 0.7868)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 2.0002 (1.9776)  loss_scale: 32768.0000 (26928.1584)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5479 (6.6151)  time: 0.9735 (0.5220 -- 3.7942)  data: 0.4278 (0.0004 -- 3.2567)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.9145 (1.9480)  loss_scale: 32768.0000 (27893.4215)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5101 (6.6449)  time: 0.8183 (0.5167 -- 3.3243)  data: 0.2727 (0.0002 -- 2.7901)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9708 (1.9418)  loss_scale: 32768.0000 (28584.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4603 (6.6465)  time: 0.8990 (0.5418 -- 4.3624)  data: 0.3363 (0.0004 -- 3.7799)  max mem: 16413
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0792 (1.9485)  loss_scale: 32768.0000 (29081.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8548 (6.7224)  time: 0.7610 (0.4984 -- 4.3624)  data: 0.2387 (0.0002 -- 3.7799)  max mem: 16413
Epoch: [34] Total time: 0:02:21 (0.8871 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0792 (1.9700)  loss_scale: 32768.0000 (29081.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8548 (6.7224)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.8657 (0.8657)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4661 (2.4661 -- 2.4661)  data: 2.2429 (2.2429 -- 2.2429)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8657 (1.0352)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (96.9697)  time: 0.4227 (0.2052 -- 2.4661)  data: 0.2049 (0.0008 -- 2.2429)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8007 (0.9917)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (96.8254)  time: 0.2161 (0.1688 -- 0.3969)  data: 0.0106 (0.0001 -- 0.1989)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9061 (1.0372)  acc1: 55.5556 (65.5602)  acc5: 100.0000 (95.4357)  time: 0.1980 (0.1329 -- 0.3969)  data: 0.0103 (0.0001 -- 0.1989)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 70.124 Acc@5 94.606 loss 0.997
Accuracy of the network on the 482 val images: 70.12%
[2023-09-04 22:59:36,927] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 22:59:36,929] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 22:59:36,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 22:59:36,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 22:59:38,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 22:59:38,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.12%
Epoch: [35]  [  0/160]  eta: 0:20:55  lr: 0.000047  min_lr: 0.000012  loss: 1.7476 (1.7476)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9947 (7.9947)  time: 7.8457 (7.8457 -- 7.8457)  data: 7.3003 (7.3003 -- 7.3003)  max mem: 16413
[2023-09-04 22:59:48,425] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:59:48,426] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:59:48,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 22:59:48,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 22:59:57,278] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5614
[2023-09-04 22:59:57,278] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5614
[2023-09-04 22:59:57,278] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:59:57,278] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 22:59:57,278] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [ 20/160]  eta: 0:02:40  lr: 0.000047  min_lr: 0.000012  loss: 1.8837 (1.8595)  loss_scale: 32768.0000 (48371.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5262 (6.1990)  time: 0.8113 (0.5394 -- 3.8759)  data: 0.2344 (0.0004 -- 2.8610)  max mem: 16413
Epoch: [35]  [ 40/160]  eta: 0:02:09  lr: 0.000047  min_lr: 0.000012  loss: 2.0509 (1.9505)  loss_scale: 32768.0000 (40760.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0461 (6.4138)  time: 1.0060 (0.5330 -- 3.6649)  data: 0.3309 (0.0004 -- 3.1317)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:42  lr: 0.000047  min_lr: 0.000012  loss: 1.9912 (1.9417)  loss_scale: 32768.0000 (38139.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1133 (6.5040)  time: 0.9225 (0.5218 -- 3.4595)  data: 0.1798 (0.0002 -- 2.9269)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 1.7462 (1.9207)  loss_scale: 32768.0000 (36813.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6906 (6.6133)  time: 0.7874 (0.5127 -- 3.7516)  data: 0.0012 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [35]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 2.0037 (1.9367)  loss_scale: 32768.0000 (36012.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1037 (6.5208)  time: 0.9285 (0.5176 -- 3.4216)  data: 0.0014 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.9684 (1.9311)  loss_scale: 32768.0000 (35476.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0302 (6.4801)  time: 0.8230 (0.5319 -- 3.6892)  data: 0.0013 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [35]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9492 (1.9385)  loss_scale: 32768.0000 (35091.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2350 (6.4752)  time: 0.8760 (0.5235 -- 3.9961)  data: 0.0013 (0.0003 -- 0.0035)  max mem: 16413
[2023-09-04 23:01:51,023] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:01:51,023] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:01:51,024] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:01:51,024] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:01:52,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5745
[2023-09-04 23:01:52,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5745
[2023-09-04 23:01:52,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:01:52,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:01:52,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9937 (1.9463)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9779 (6.4681)  time: 0.5632 (0.4948 -- 1.3077)  data: 0.0008 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [35] Total time: 0:02:21 (0.8856 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9937 (1.9376)  loss_scale: 32768.0000 (35225.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9779 (6.4681)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.8369 (0.8369)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4282 (2.4282 -- 2.4282)  data: 2.1918 (2.1918 -- 2.1918)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8367 (1.0433)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (96.9697)  time: 0.4231 (0.1922 -- 2.4282)  data: 0.2033 (0.0006 -- 2.1918)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7787 (0.9804)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (96.8254)  time: 0.2162 (0.1699 -- 0.2456)  data: 0.0053 (0.0001 -- 0.0571)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0189 (1.0336)  acc1: 66.6667 (65.1452)  acc5: 100.0000 (95.4357)  time: 0.2003 (0.1331 -- 0.2456)  data: 0.0050 (0.0001 -- 0.0571)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 69.710 Acc@5 95.851 loss 0.978
Accuracy of the network on the 482 val images: 69.71%
Max accuracy: 70.12%
Epoch: [36]  [  0/160]  eta: 0:19:17  lr: 0.000047  min_lr: 0.000012  loss: 2.0478 (2.0478)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6526 (5.6526)  time: 7.2343 (7.2343 -- 7.2343)  data: 6.6973 (6.6973 -- 6.6973)  max mem: 16413
Epoch: [36]  [ 20/160]  eta: 0:02:39  lr: 0.000047  min_lr: 0.000012  loss: 1.7232 (1.7484)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3157 (6.1771)  time: 0.8368 (0.5197 -- 2.7797)  data: 0.1771 (0.0004 -- 2.2263)  max mem: 16413
[2023-09-04 23:02:45,449] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5794
[2023-09-04 23:02:45,449] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5794
[2023-09-04 23:02:45,449] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:02:45,449] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:02:45,449] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [36]  [ 40/160]  eta: 0:01:59  lr: 0.000047  min_lr: 0.000012  loss: 2.0567 (1.8605)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8364 (6.6868)  time: 0.8512 (0.5351 -- 3.1147)  data: 0.2540 (0.0004 -- 2.5874)  max mem: 16413
Epoch: [36]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000012  loss: 1.8355 (1.8528)  loss_scale: 16384.0000 (25516.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0788 (6.7389)  time: 0.8987 (0.5301 -- 3.8593)  data: 0.2207 (0.0005 -- 2.5257)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000012  loss: 2.1009 (1.8985)  loss_scale: 16384.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1639 (6.7193)  time: 0.8484 (0.5368 -- 2.9253)  data: 0.2844 (0.0003 -- 2.3855)  max mem: 16413
Epoch: [36]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.1124 (1.9337)  loss_scale: 16384.0000 (21899.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3827 (6.7932)  time: 0.9610 (0.5306 -- 4.0578)  data: 0.1795 (0.0006 -- 1.0317)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 2.1926 (1.9764)  loss_scale: 16384.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5512 (6.8277)  time: 0.7516 (0.5215 -- 3.8964)  data: 0.1956 (0.0003 -- 3.3750)  max mem: 16413
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.7995 (1.9650)  loss_scale: 16384.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0451 (6.7785)  time: 0.9724 (0.5249 -- 4.1768)  data: 0.3689 (0.0003 -- 3.6304)  max mem: 16413
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0728 (1.9736)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1181 (6.7419)  time: 0.6649 (0.4950 -- 3.3689)  data: 0.0374 (0.0002 -- 0.7338)  max mem: 16413
Epoch: [36] Total time: 0:02:22 (0.8902 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0728 (1.9537)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1181 (6.7419)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7812 (0.7812)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2685 (2.2685 -- 2.2685)  data: 2.0209 (2.0209 -- 2.0209)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7921 (1.0354)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (94.9495)  time: 0.4343 (0.2037 -- 2.2685)  data: 0.2133 (0.0005 -- 2.0209)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8531 (0.9636)  acc1: 77.7778 (67.1958)  acc5: 100.0000 (95.7672)  time: 0.2335 (0.1698 -- 0.5420)  data: 0.0286 (0.0001 -- 0.3043)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9664 (1.0221)  acc1: 66.6667 (64.7303)  acc5: 100.0000 (94.6058)  time: 0.2174 (0.1325 -- 0.5420)  data: 0.0283 (0.0001 -- 0.3043)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 68.672 Acc@5 94.813 loss 0.980
Accuracy of the network on the 482 val images: 68.67%
Max accuracy: 70.12%
Epoch: [37]  [  0/160]  eta: 0:19:09  lr: 0.000047  min_lr: 0.000012  loss: 2.3988 (2.3988)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5843 (6.5843)  time: 7.1865 (7.1865 -- 7.1865)  data: 6.0222 (6.0222 -- 6.0222)  max mem: 16413
[2023-09-04 23:04:47,495] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:04:47,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:04:47,496] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:04:47,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [ 20/160]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000012  loss: 1.9348 (2.0162)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1910 (5.5771)  time: 0.8843 (0.5214 -- 3.4106)  data: 0.1566 (0.0004 -- 1.3452)  max mem: 16413
Epoch: [37]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000012  loss: 1.9208 (1.9846)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9052 (5.8897)  time: 0.8692 (0.5228 -- 2.8099)  data: 0.2431 (0.0003 -- 1.9102)  max mem: 16413
Epoch: [37]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000012  loss: 2.0511 (1.9997)  loss_scale: 32768.0000 (31962.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9577 (6.0374)  time: 0.9129 (0.5311 -- 3.1592)  data: 0.3206 (0.0005 -- 2.6123)  max mem: 16413
[2023-09-04 23:05:54,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=30, lr=[1.1892386367775517e-05, 1.1892386367775517e-05, 1.3213762630861687e-05, 1.3213762630861687e-05, 1.4681958478735205e-05, 1.4681958478735205e-05, 1.6313287198594674e-05, 1.6313287198594674e-05, 1.8125874665105192e-05, 1.8125874665105192e-05, 2.0139860739005768e-05, 2.0139860739005768e-05, 2.2377623043339742e-05, 2.2377623043339742e-05, 2.4864025603710823e-05, 2.4864025603710823e-05, 2.762669511523425e-05, 2.762669511523425e-05, 3.069632790581583e-05, 3.069632790581583e-05, 3.410703100646204e-05, 3.410703100646204e-05, 3.789670111829115e-05, 3.789670111829115e-05, 4.2107445686990164e-05, 4.2107445686990164e-05, 4.67860507633224e-05, 4.67860507633224e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 23:05:54,729] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=17.138732154345256, CurrSamplesPerSec=21.309723063114173, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 1.9542 (1.9829)  loss_scale: 32768.0000 (32161.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6515 (6.0329)  time: 0.8874 (0.5051 -- 4.1528)  data: 0.3501 (0.0004 -- 3.6361)  max mem: 16413
Epoch: [37]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000012  loss: 2.0502 (1.9913)  loss_scale: 32768.0000 (32281.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4953 (6.1393)  time: 0.9571 (0.5084 -- 5.4932)  data: 0.4143 (0.0004 -- 4.9796)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0460 (1.9780)  loss_scale: 32768.0000 (32361.7851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7921 (6.2618)  time: 0.7573 (0.5316 -- 2.4754)  data: 0.2083 (0.0005 -- 1.9606)  max mem: 16413
[2023-09-04 23:06:41,544] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:06:41,544] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:06:41,544] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:06:41,544] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9563 (1.9607)  loss_scale: 32768.0000 (34743.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2431 (6.2099)  time: 0.8616 (0.5215 -- 5.3330)  data: 0.3115 (0.0003 -- 4.8074)  max mem: 16413
[2023-09-04 23:06:54,639] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6068
[2023-09-04 23:06:54,640] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:06:54,640] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 23:06:54,639] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6068
[2023-09-04 23:06:54,640] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9327 (1.9604)  loss_scale: 32768.0000 (35942.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7689 (6.2313)  time: 0.7201 (0.4962 -- 2.9210)  data: 0.1969 (0.0002 -- 2.3814)  max mem: 16413
Epoch: [37] Total time: 0:02:23 (0.8980 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9327 (1.9642)  loss_scale: 32768.0000 (35942.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7689 (6.2313)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.8308 (0.8308)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5298 (2.5298 -- 2.5298)  data: 2.3008 (2.3008 -- 2.3008)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8308 (0.9692)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (96.9697)  time: 0.4318 (0.2043 -- 2.5298)  data: 0.2100 (0.0005 -- 2.3008)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8442 (0.9382)  acc1: 66.6667 (70.3704)  acc5: 100.0000 (96.8254)  time: 0.2084 (0.1694 -- 0.2464)  data: 0.0014 (0.0001 -- 0.0122)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9032 (0.9867)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (95.4357)  time: 0.1908 (0.1329 -- 0.2339)  data: 0.0011 (0.0001 -- 0.0122)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 70.332 Acc@5 95.643 loss 0.946
Accuracy of the network on the 482 val images: 70.33%
[2023-09-04 23:07:09,542] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:07:09,543] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:07:09,543] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:07:09,543] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:07:10,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:07:10,954] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.33%
Epoch: [38]  [  0/160]  eta: 0:15:33  lr: 0.000047  min_lr: 0.000012  loss: 2.0789 (2.0789)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0085 (8.0085)  time: 5.8358 (5.8358 -- 5.8358)  data: 5.3074 (5.3074 -- 5.3074)  max mem: 16413
Epoch: [38]  [ 20/160]  eta: 0:02:38  lr: 0.000047  min_lr: 0.000012  loss: 2.0094 (1.9629)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3090 (6.4759)  time: 0.8954 (0.5237 -- 2.7725)  data: 0.0837 (0.0006 -- 0.7091)  max mem: 16413
Epoch: [38]  [ 40/160]  eta: 0:02:01  lr: 0.000047  min_lr: 0.000012  loss: 1.8398 (1.9393)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1759 (6.8709)  time: 0.8832 (0.5204 -- 3.6087)  data: 0.3308 (0.0001 -- 3.0458)  max mem: 16413
Epoch: [38]  [ 60/160]  eta: 0:01:37  lr: 0.000047  min_lr: 0.000012  loss: 1.8795 (1.9223)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9345 (6.7918)  time: 0.8916 (0.5142 -- 4.7483)  data: 0.3183 (0.0002 -- 4.2190)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000012  loss: 1.9596 (1.9355)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7308 (6.7553)  time: 0.8636 (0.5338 -- 3.2656)  data: 0.1789 (0.0007 -- 1.7283)  max mem: 16413
[2023-09-04 23:08:33,327] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6166
[2023-09-04 23:08:33,327] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6166
[2023-09-04 23:08:33,328] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:08:33,328] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:08:33,328] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 2.0747 (1.9720)  loss_scale: 16384.0000 (30334.7327)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6797 (6.6991)  time: 0.9088 (0.5282 -- 3.1223)  data: 0.2379 (0.0004 -- 2.1967)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0452 (1.9837)  loss_scale: 16384.0000 (28028.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1986 (6.7359)  time: 0.8817 (0.5157 -- 3.8641)  data: 0.3322 (0.0004 -- 3.3557)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9030 (1.9790)  loss_scale: 16384.0000 (26377.0780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2072 (6.8313)  time: 0.8677 (0.5184 -- 3.2474)  data: 0.3114 (0.0003 -- 2.6959)  max mem: 16413
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9877 (1.9750)  loss_scale: 16384.0000 (25190.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5704 (6.7022)  time: 0.6361 (0.4960 -- 2.6089)  data: 0.1156 (0.0002 -- 2.0970)  max mem: 16413
Epoch: [38] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9877 (1.9529)  loss_scale: 16384.0000 (25190.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5704 (6.7022)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.8080 (0.8080)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.2746 (2.2746 -- 2.2746)  data: 2.0313 (2.0313 -- 2.0313)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8080 (0.9783)  acc1: 66.6667 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4307 (0.1999 -- 2.2746)  data: 0.2076 (0.0006 -- 2.0313)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7382 (0.9303)  acc1: 66.6667 (71.4286)  acc5: 100.0000 (96.2963)  time: 0.2247 (0.1721 -- 0.4270)  data: 0.0146 (0.0001 -- 0.1748)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9435 (0.9752)  acc1: 66.6667 (69.7095)  acc5: 100.0000 (95.0207)  time: 0.2108 (0.1330 -- 0.4270)  data: 0.0142 (0.0001 -- 0.1748)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 73.029 Acc@5 95.643 loss 0.928
Accuracy of the network on the 482 val images: 73.03%
[2023-09-04 23:09:40,669] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:09:40,671] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:09:40,671] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:09:40,671] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:09:42,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:09:42,170] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.03%
Epoch: [39]  [  0/160]  eta: 0:20:24  lr: 0.000047  min_lr: 0.000012  loss: 1.9929 (1.9929)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4761 (4.4761)  time: 7.6531 (7.6531 -- 7.6531)  data: 4.1904 (4.1904 -- 4.1904)  max mem: 16413
Epoch: [39]  [ 20/160]  eta: 0:02:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0016 (1.9477)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2099 (6.2179)  time: 0.7984 (0.5362 -- 2.4516)  data: 0.0229 (0.0004 -- 0.4240)  max mem: 16413
Epoch: [39]  [ 40/160]  eta: 0:02:02  lr: 0.000047  min_lr: 0.000012  loss: 1.9273 (1.9560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2072 (6.2880)  time: 0.9114 (0.5222 -- 3.5817)  data: 0.0401 (0.0002 -- 0.5878)  max mem: 16413
[2023-09-04 23:10:35,093] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:10:35,093] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:10:35,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:10:35,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [39]  [ 60/160]  eta: 0:01:34  lr: 0.000047  min_lr: 0.000012  loss: 2.0991 (1.9757)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2789 (6.4979)  time: 0.7996 (0.5356 -- 2.7066)  data: 0.0470 (0.0005 -- 0.9120)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000012  loss: 2.0770 (1.9983)  loss_scale: 32768.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8791 (6.4056)  time: 0.8766 (0.5235 -- 4.2175)  data: 0.0039 (0.0004 -- 0.0474)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 1.8536 (1.9870)  loss_scale: 32768.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5551 (6.5725)  time: 0.9435 (0.5225 -- 4.0930)  data: 0.0352 (0.0004 -- 0.4685)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.9389 (1.9755)  loss_scale: 32768.0000 (25320.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0025 (6.5534)  time: 0.7829 (0.5204 -- 2.0727)  data: 0.0285 (0.0006 -- 0.5350)  max mem: 16413
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.8807 (1.9741)  loss_scale: 32768.0000 (26377.0780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1134 (6.5078)  time: 0.9879 (0.5210 -- 4.4992)  data: 0.4335 (0.0004 -- 3.9453)  max mem: 16413
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9333 (1.9605)  loss_scale: 32768.0000 (27136.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7719 (6.4885)  time: 0.6966 (0.4945 -- 2.2435)  data: 0.1702 (0.0002 -- 1.7334)  max mem: 16413
Epoch: [39] Total time: 0:02:22 (0.8908 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9333 (1.9489)  loss_scale: 32768.0000 (27136.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7719 (6.4885)
[2023-09-04 23:12:04,705] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-09-04 23:12:04,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-09-04 23:12:04,707] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-09-04 23:12:04,707] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-09-04 23:12:05,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-09-04 23:12:05,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.7291 (0.7291)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5154 (2.5154 -- 2.5154)  data: 2.2831 (2.2831 -- 2.2831)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7291 (1.0180)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (94.9495)  time: 0.4363 (0.2103 -- 2.5154)  data: 0.2094 (0.0007 -- 2.2831)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8328 (0.9667)  acc1: 77.7778 (71.9577)  acc5: 100.0000 (96.2963)  time: 0.2169 (0.1691 -- 0.2692)  data: 0.0027 (0.0001 -- 0.0297)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0090 (1.0150)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (95.4357)  time: 0.1967 (0.1331 -- 0.2499)  data: 0.0022 (0.0001 -- 0.0297)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 72.199 Acc@5 95.228 loss 0.968
Accuracy of the network on the 482 val images: 72.20%
Max accuracy: 73.03%
Epoch: [40]  [  0/160]  eta: 0:17:49  lr: 0.000047  min_lr: 0.000012  loss: 1.5546 (1.5546)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1110 (9.1110)  time: 6.6846 (6.6846 -- 6.6846)  data: 4.9843 (4.9843 -- 4.9843)  max mem: 16413
Epoch: [40]  [ 20/160]  eta: 0:02:41  lr: 0.000047  min_lr: 0.000012  loss: 1.9480 (1.9147)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5291 (6.2124)  time: 0.8764 (0.5242 -- 4.8270)  data: 0.0748 (0.0004 -- 1.0716)  max mem: 16413
[2023-09-04 23:12:39,429] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:12:39,429] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:12:39,429] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:12:39,429] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:12:45,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6427
[2023-09-04 23:12:45,506] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6427
[2023-09-04 23:12:45,506] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:12:45,506] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:12:45,506] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [40]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000012  loss: 1.9964 (1.9402)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2089 (6.4821)  time: 0.8935 (0.5157 -- 3.5385)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000012  loss: 1.8059 (1.9034)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6614 (6.5889)  time: 0.8928 (0.5220 -- 4.0154)  data: 0.0015 (0.0003 -- 0.0040)  max mem: 16413
[2023-09-04 23:13:31,951] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6480
[2023-09-04 23:13:31,951] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6480
[2023-09-04 23:13:31,952] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:13:31,952] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:13:31,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [40]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 1.9723 (1.9018)  loss_scale: 32768.0000 (34183.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3589 (6.5892)  time: 0.9203 (0.5377 -- 2.7359)  data: 0.0017 (0.0001 -- 0.0046)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000012  loss: 2.0212 (1.9077)  loss_scale: 16384.0000 (30659.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0838 (6.7741)  time: 0.7912 (0.5345 -- 2.6467)  data: 0.0016 (0.0007 -- 0.0044)  max mem: 16413
Epoch: [40]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 2.0452 (1.9207)  loss_scale: 16384.0000 (28299.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5132 (6.8403)  time: 0.9210 (0.5288 -- 4.2417)  data: 0.0025 (0.0002 -- 0.0158)  max mem: 16413
Epoch: [40]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 2.0315 (1.9382)  loss_scale: 16384.0000 (26609.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6205 (6.8025)  time: 0.7847 (0.5296 -- 2.7006)  data: 0.0022 (0.0004 -- 0.0103)  max mem: 16413
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.8669 (1.9235)  loss_scale: 16384.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3907 (6.9324)  time: 0.7736 (0.4970 -- 4.2520)  data: 0.0007 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [40] Total time: 0:02:23 (0.8953 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.8669 (1.9233)  loss_scale: 16384.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3907 (6.9324)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7467 (0.7467)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2688 (2.2688 -- 2.2688)  data: 2.0291 (2.0291 -- 2.0291)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7756 (0.9998)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (94.9495)  time: 0.4160 (0.1957 -- 2.2688)  data: 0.1995 (0.0005 -- 2.0291)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7756 (0.9148)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (95.7672)  time: 0.2258 (0.1690 -- 0.4680)  data: 0.0216 (0.0001 -- 0.2637)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9220 (0.9745)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (95.0207)  time: 0.2123 (0.1330 -- 0.4680)  data: 0.0213 (0.0001 -- 0.2637)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 76.763 Acc@5 95.436 loss 0.937
Accuracy of the network on the 482 val images: 76.76%
[2023-09-04 23:14:44,574] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:14:44,576] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:14:44,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:14:44,576] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:14:46,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:14:46,053] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.76%
Epoch: [41]  [  0/160]  eta: 0:21:27  lr: 0.000047  min_lr: 0.000012  loss: 2.4243 (2.4243)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9131 (6.9131)  time: 8.0451 (8.0451 -- 8.0451)  data: 7.4933 (7.4933 -- 7.4933)  max mem: 16413
Epoch: [41]  [ 20/160]  eta: 0:02:40  lr: 0.000047  min_lr: 0.000012  loss: 1.9214 (1.9612)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4273 (6.6177)  time: 0.8051 (0.5354 -- 3.3596)  data: 0.0692 (0.0003 -- 0.8901)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:01  lr: 0.000047  min_lr: 0.000012  loss: 1.9200 (1.9646)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3854 (6.6351)  time: 0.8718 (0.5253 -- 2.7653)  data: 0.3224 (0.0005 -- 2.2173)  max mem: 16413
[2023-09-04 23:15:34,952] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:15:34,952] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:15:34,952] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:15:34,952] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [41]  [ 60/160]  eta: 0:01:35  lr: 0.000047  min_lr: 0.000012  loss: 1.8901 (1.9610)  loss_scale: 32768.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0524 (6.4942)  time: 0.8471 (0.5155 -- 3.0354)  data: 0.1096 (0.0001 -- 1.0684)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.0492 (1.9582)  loss_scale: 32768.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0208 (6.3678)  time: 0.9576 (0.5439 -- 4.2865)  data: 0.0016 (0.0005 -- 0.0044)  max mem: 16413
Epoch: [41]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.9837 (1.9388)  loss_scale: 32768.0000 (24819.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7105 (6.3443)  time: 0.8488 (0.5136 -- 3.8958)  data: 0.0017 (0.0008 -- 0.0047)  max mem: 16413
Epoch: [41]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000012  loss: 1.8776 (1.9296)  loss_scale: 32768.0000 (26133.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4924 (6.4501)  time: 0.9302 (0.5292 -- 3.2624)  data: 0.0022 (0.0003 -- 0.0158)  max mem: 16413
[2023-09-04 23:16:51,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6696
[2023-09-04 23:16:51,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6696
[2023-09-04 23:16:51,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:16:51,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:16:51,524] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.8802 (1.9202)  loss_scale: 32768.0000 (26493.2766)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6176 (6.4850)  time: 0.7843 (0.5302 -- 2.2598)  data: 0.0026 (0.0002 -- 0.0162)  max mem: 16413
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9377 (1.9292)  loss_scale: 16384.0000 (25292.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9748 (6.4878)  time: 0.7097 (0.5007 -- 2.7035)  data: 0.0011 (0.0002 -- 0.0053)  max mem: 16413
Epoch: [41] Total time: 0:02:21 (0.8871 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.9377 (1.9298)  loss_scale: 16384.0000 (25292.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9748 (6.4878)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.7542 (0.7542)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3288 (2.3288 -- 2.3288)  data: 2.1159 (2.1159 -- 2.1159)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7542 (0.9233)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (96.9697)  time: 0.4157 (0.2044 -- 2.3288)  data: 0.1998 (0.0003 -- 2.1159)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6683 (0.8996)  acc1: 77.7778 (76.1905)  acc5: 100.0000 (95.7672)  time: 0.2256 (0.1691 -- 0.5331)  data: 0.0213 (0.0001 -- 0.3412)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8377 (0.9290)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.4357)  time: 0.2099 (0.1326 -- 0.5331)  data: 0.0210 (0.0001 -- 0.3412)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 76.763 Acc@5 95.643 loss 0.882
Accuracy of the network on the 482 val images: 76.76%
[2023-09-04 23:17:15,815] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:17:15,817] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:17:15,819] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:17:15,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:17:17,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:17:17,384] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.76%
Epoch: [42]  [  0/160]  eta: 0:20:40  lr: 0.000047  min_lr: 0.000012  loss: 2.1233 (2.1233)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8958 (4.8958)  time: 7.7526 (7.7526 -- 7.7526)  data: 6.9908 (6.9908 -- 6.9908)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:39  lr: 0.000047  min_lr: 0.000012  loss: 1.8919 (1.9647)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7446 (5.8207)  time: 0.8085 (0.5222 -- 3.1790)  data: 0.1634 (0.0005 -- 2.0591)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000012  loss: 1.9034 (1.9031)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3079 (6.1603)  time: 0.9170 (0.5219 -- 5.0196)  data: 0.1603 (0.0004 -- 2.1091)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:37  lr: 0.000047  min_lr: 0.000012  loss: 1.8905 (1.9071)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6407 (6.1411)  time: 0.8715 (0.5251 -- 2.5049)  data: 0.2384 (0.0002 -- 1.9067)  max mem: 16413
Epoch: [42]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000012  loss: 2.0757 (1.9273)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5166 (6.1544)  time: 0.8974 (0.5250 -- 4.3346)  data: 0.2749 (0.0002 -- 3.8117)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.8166 (1.9151)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1125 (6.2052)  time: 0.8962 (0.5284 -- 2.9777)  data: 0.3506 (0.0003 -- 2.4608)  max mem: 16413
[2023-09-04 23:18:55,818] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:18:55,818] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:18:55,821] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:18:55,821] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:19:00,124] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6831
[2023-09-04 23:19:00,124] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6831
[2023-09-04 23:19:00,124] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:19:00,124] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:19:00,125] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [42]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.8378 (1.9030)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4547 (6.0908)  time: 0.7624 (0.5350 -- 2.0559)  data: 0.2089 (0.0008 -- 1.4814)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.8068 (1.8907)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3730 (6.1656)  time: 0.9428 (0.5234 -- 3.3585)  data: 0.3875 (0.0003 -- 2.8145)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.8127 (1.8857)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2443 (6.2356)  time: 0.7124 (0.4969 -- 2.1185)  data: 0.1917 (0.0002 -- 1.6056)  max mem: 16413
Epoch: [42] Total time: 0:02:23 (0.8962 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.8127 (1.9014)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2443 (6.2356)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6889 (0.6889)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3960 (2.3960 -- 2.3960)  data: 2.1620 (2.1620 -- 2.1620)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6889 (0.9800)  acc1: 88.8889 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4181 (0.2053 -- 2.3960)  data: 0.1976 (0.0007 -- 2.1620)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6508 (0.9051)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2149 (0.1705 -- 0.2762)  data: 0.0041 (0.0001 -- 0.0657)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8488 (0.9385)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.8506)  time: 0.1989 (0.1334 -- 0.2762)  data: 0.0037 (0.0001 -- 0.0657)  max mem: 16413
Val: Total time: 0:00:07 (0.2848 s / it)
* Acc@1 75.519 Acc@5 96.266 loss 0.899
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 76.76%
Epoch: [43]  [  0/160]  eta: 0:17:32  lr: 0.000047  min_lr: 0.000012  loss: 2.1489 (2.1489)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7635 (3.7635)  time: 6.5776 (6.5776 -- 6.5776)  data: 5.4377 (5.4377 -- 5.4377)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:48  lr: 0.000047  min_lr: 0.000012  loss: 1.9425 (1.9359)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3399 (6.4070)  time: 0.9316 (0.5095 -- 3.1462)  data: 0.2758 (0.0002 -- 2.6121)  max mem: 16413
Epoch: [43]  [ 40/160]  eta: 0:01:58  lr: 0.000047  min_lr: 0.000012  loss: 1.8433 (1.8651)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6177 (6.5758)  time: 0.7683 (0.5259 -- 3.2123)  data: 0.0173 (0.0003 -- 0.3092)  max mem: 16413
Epoch: [43]  [ 60/160]  eta: 0:01:37  lr: 0.000047  min_lr: 0.000012  loss: 1.8303 (1.8725)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6686 (6.6053)  time: 0.9496 (0.5289 -- 4.5129)  data: 0.0688 (0.0008 -- 1.3295)  max mem: 16413
[2023-09-04 23:21:04,367] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:21:04,367] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:21:04,367] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:21:04,368] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [43]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000012  loss: 1.8291 (1.8580)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9352 (6.7433)  time: 0.8159 (0.5365 -- 2.5983)  data: 0.1901 (0.0003 -- 2.0586)  max mem: 16413
Epoch: [43]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.9159 (1.8738)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4757 (6.6894)  time: 0.9754 (0.5305 -- 3.3009)  data: 0.4163 (0.0003 -- 2.7843)  max mem: 16413
[2023-09-04 23:21:38,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=36, lr=[1.1839058553104364e-05, 1.1839058553104364e-05, 1.3154509503449296e-05, 1.3154509503449296e-05, 1.4616121670499214e-05, 1.4616121670499214e-05, 1.6240135189443574e-05, 1.6240135189443574e-05, 1.8044594654937303e-05, 1.8044594654937303e-05, 2.0049549616597003e-05, 2.0049549616597003e-05, 2.2277277351774445e-05, 2.2277277351774445e-05, 2.4752530390860493e-05, 2.4752530390860493e-05, 2.7502811545400548e-05, 2.7502811545400548e-05, 3.0558679494889494e-05, 3.0558679494889494e-05, 3.3954088327655e-05, 3.3954088327655e-05, 3.772676480850555e-05, 3.772676480850555e-05, 4.191862756500617e-05, 4.191862756500617e-05, 4.657625285000685e-05, 4.657625285000685e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 23:21:38,386] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=17.172430384583826, CurrSamplesPerSec=22.58703915287689, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.7594 (1.8560)  loss_scale: 32768.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4447 (6.7397)  time: 0.7581 (0.5297 -- 3.0593)  data: 0.2105 (0.0003 -- 2.4942)  max mem: 16413
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.8750 (1.8659)  loss_scale: 32768.0000 (23472.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2477 (6.7384)  time: 0.9957 (0.5404 -- 4.1337)  data: 0.4249 (0.0004 -- 3.5865)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 1.8727 (1.8675)  loss_scale: 32768.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5009 (6.7223)  time: 0.6339 (0.4978 -- 1.6104)  data: 0.1099 (0.0002 -- 1.0949)  max mem: 16413
Epoch: [43] Total time: 0:02:22 (0.8913 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 1.8727 (1.8932)  loss_scale: 32768.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5009 (6.7223)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.6578 (0.6578)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4993 (2.4993 -- 2.4993)  data: 2.2361 (2.2361 -- 2.2361)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6754 (0.9331)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4280 (0.2013 -- 2.4993)  data: 0.2057 (0.0008 -- 2.2361)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7141 (0.8691)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2151 (0.1689 -- 0.2996)  data: 0.0069 (0.0001 -- 0.1098)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8299 (0.9135)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.0207)  time: 0.1965 (0.1324 -- 0.2996)  data: 0.0066 (0.0001 -- 0.1098)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 75.311 Acc@5 95.436 loss 0.893
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 76.76%
Epoch: [44]  [  0/160]  eta: 0:22:26  lr: 0.000047  min_lr: 0.000012  loss: 1.7200 (1.7200)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0541 (5.0541)  time: 8.4167 (8.4167 -- 8.4167)  data: 7.8508 (7.8508 -- 7.8508)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:02:49  lr: 0.000047  min_lr: 0.000012  loss: 1.8614 (1.8199)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1361 (6.4214)  time: 0.8481 (0.5248 -- 3.3861)  data: 0.2871 (0.0005 -- 2.8681)  max mem: 16413
Epoch: [44]  [ 40/160]  eta: 0:02:05  lr: 0.000047  min_lr: 0.000012  loss: 1.7387 (1.7932)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5607 (6.2690)  time: 0.8812 (0.5201 -- 2.8640)  data: 0.3322 (0.0002 -- 2.3354)  max mem: 16413
[2023-09-04 23:23:08,989] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:23:08,989] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:23:08,990] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:23:08,990] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [44]  [ 60/160]  eta: 0:01:38  lr: 0.000047  min_lr: 0.000012  loss: 1.9147 (1.8455)  loss_scale: 65536.0000 (39751.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9103 (6.2423)  time: 0.8560 (0.5391 -- 2.7669)  data: 0.2663 (0.0003 -- 2.2537)  max mem: 16413
[2023-09-04 23:23:19,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7101
[2023-09-04 23:23:19,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7101
[2023-09-04 23:23:19,567] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:23:19,567] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:23:19,567] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [44]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000012  loss: 1.9758 (1.8707)  loss_scale: 32768.0000 (38027.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6479 (6.4536)  time: 0.9394 (0.5157 -- 3.6997)  data: 0.2650 (0.0003 -- 3.1908)  max mem: 16413
Epoch: [44]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000012  loss: 1.8697 (1.8564)  loss_scale: 32768.0000 (36985.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5159 (6.3793)  time: 0.8298 (0.5258 -- 2.6429)  data: 0.2388 (0.0002 -- 2.1166)  max mem: 16413
Epoch: [44]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000012  loss: 1.9341 (1.8720)  loss_scale: 32768.0000 (36288.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8862 (6.5627)  time: 0.7681 (0.5365 -- 1.7992)  data: 0.1109 (0.0001 -- 1.2412)  max mem: 16413
[2023-09-04 23:24:16,176] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7164
[2023-09-04 23:24:16,176] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7164
[2023-09-04 23:24:16,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:24:16,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:24:16,176] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000012  loss: 1.9333 (1.8813)  loss_scale: 16384.0000 (33813.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4531 (6.6598)  time: 1.0470 (0.5144 -- 4.8115)  data: 0.0015 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000012  loss: 2.0411 (1.8928)  loss_scale: 16384.0000 (31744.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4844 (6.6788)  time: 0.5754 (0.4958 -- 1.2454)  data: 0.0009 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [44] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000012  loss: 2.0411 (1.9075)  loss_scale: 16384.0000 (31744.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4844 (6.6788)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4933 (0.4933)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3806 (2.3806 -- 2.3806)  data: 2.1364 (2.1364 -- 2.1364)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7152 (0.8994)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4175 (0.2078 -- 2.3806)  data: 0.1971 (0.0008 -- 2.1364)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7172 (0.8527)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.8254)  time: 0.2225 (0.1694 -- 0.3606)  data: 0.0133 (0.0001 -- 0.1521)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8969 (0.9085)  acc1: 71.4286 (72.6141)  acc5: 100.0000 (95.4357)  time: 0.2046 (0.1331 -- 0.3606)  data: 0.0129 (0.0001 -- 0.1521)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 75.311 Acc@5 95.851 loss 0.866
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 76.76%
Epoch: [45]  [  0/160]  eta: 0:18:55  lr: 0.000047  min_lr: 0.000012  loss: 2.0751 (2.0751)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6088 (7.6088)  time: 7.0943 (7.0943 -- 7.0943)  data: 5.1689 (5.1689 -- 5.1689)  max mem: 16413
Epoch: [45]  [ 20/160]  eta: 0:02:32  lr: 0.000047  min_lr: 0.000012  loss: 1.9996 (1.9215)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6266 (7.1174)  time: 0.7902 (0.5419 -- 2.5634)  data: 0.0511 (0.0006 -- 0.9673)  max mem: 16413
Epoch: [45]  [ 40/160]  eta: 0:02:00  lr: 0.000047  min_lr: 0.000012  loss: 1.9581 (1.9340)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3683 (7.3800)  time: 0.9061 (0.5409 -- 3.0121)  data: 0.2008 (0.0004 -- 2.4862)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000012  loss: 1.8677 (1.9115)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1014 (7.1806)  time: 0.8866 (0.5320 -- 2.8545)  data: 0.2057 (0.0003 -- 2.3071)  max mem: 16413
Epoch: [45]  [ 80/160]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000012  loss: 1.9609 (1.8955)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7210 (7.1295)  time: 0.8170 (0.5208 -- 1.8779)  data: 0.1095 (0.0002 -- 1.1873)  max mem: 16413
[2023-09-04 23:26:17,586] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:26:17,586] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:26:17,588] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:26:17,588] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [45]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 2.0056 (1.9171)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1133 (7.1122)  time: 1.0346 (0.5273 -- 4.3917)  data: 0.0898 (0.0003 -- 1.2847)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8509 (1.9161)  loss_scale: 32768.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0028 (7.1035)  time: 0.8492 (0.5182 -- 3.9426)  data: 0.0016 (0.0001 -- 0.0067)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9605 (1.9091)  loss_scale: 32768.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1821 (6.9783)  time: 0.7690 (0.5126 -- 3.1832)  data: 0.0015 (0.0001 -- 0.0046)  max mem: 16413
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8901 (1.9027)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9753 (6.8880)  time: 0.6826 (0.4993 -- 2.2943)  data: 0.0012 (0.0002 -- 0.0042)  max mem: 16413
Epoch: [45] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8901 (1.8702)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9753 (6.8880)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.5063 (0.5063)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2759 (2.2759 -- 2.2759)  data: 2.0213 (2.0213 -- 2.0213)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6362 (0.8686)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (96.9697)  time: 0.4040 (0.1958 -- 2.2759)  data: 0.1848 (0.0007 -- 2.0213)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6362 (0.8207)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.8254)  time: 0.2219 (0.1700 -- 0.3488)  data: 0.0145 (0.0001 -- 0.1450)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8474 (0.8607)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (96.6805)  time: 0.2046 (0.1333 -- 0.3488)  data: 0.0142 (0.0001 -- 0.1450)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 76.556 Acc@5 96.680 loss 0.825
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 76.76%
Epoch: [46]  [  0/160]  eta: 0:19:01  lr: 0.000046  min_lr: 0.000012  loss: 2.2253 (2.2253)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6716 (4.6716)  time: 7.1321 (7.1321 -- 7.1321)  data: 6.5383 (6.5383 -- 6.5383)  max mem: 16413
Epoch: [46]  [ 20/160]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000012  loss: 1.8192 (1.8986)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5792 (6.6802)  time: 0.9191 (0.5211 -- 3.7879)  data: 0.3630 (0.0004 -- 3.2601)  max mem: 16413
Epoch: [46]  [ 40/160]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000012  loss: 1.7755 (1.8861)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8572 (6.6713)  time: 0.8293 (0.5237 -- 2.9577)  data: 0.2763 (0.0003 -- 2.3796)  max mem: 16413
Epoch: [46]  [ 60/160]  eta: 0:01:37  lr: 0.000046  min_lr: 0.000012  loss: 2.0033 (1.9053)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2343 (6.5890)  time: 0.8662 (0.5250 -- 2.5953)  data: 0.3121 (0.0008 -- 2.0693)  max mem: 16413
[2023-09-04 23:28:18,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:28:18,473] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:28:18,474] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:28:18,474] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:28:20,981] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7424
[2023-09-04 23:28:20,981] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7424
[2023-09-04 23:28:20,981] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:28:20,981] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:28:20,981] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 23:28:25,152] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7427
[2023-09-04 23:28:25,152] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:28:25,152] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-04 23:28:25,152] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7427
[2023-09-04 23:28:25,152] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [46]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.8249 (1.8721)  loss_scale: 16384.0000 (31149.8272)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9705 (6.5281)  time: 0.8256 (0.5251 -- 3.0784)  data: 0.2558 (0.0009 -- 2.1406)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.7520 (1.8522)  loss_scale: 16384.0000 (28225.9010)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2525 (6.4270)  time: 0.8651 (0.5331 -- 2.8305)  data: 0.2792 (0.0007 -- 2.2733)  max mem: 16413
Epoch: [46]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 2.1053 (1.8776)  loss_scale: 16384.0000 (26268.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2763 (6.4297)  time: 0.8862 (0.5416 -- 2.7351)  data: 0.3237 (0.0005 -- 2.1785)  max mem: 16413
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.7854 (1.8780)  loss_scale: 16384.0000 (24866.4965)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5127 (6.4602)  time: 0.8851 (0.5384 -- 3.7017)  data: 0.3241 (0.0005 -- 3.1872)  max mem: 16413
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 2.0156 (1.8811)  loss_scale: 16384.0000 (23859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2355 (6.4878)  time: 0.6489 (0.4977 -- 2.8797)  data: 0.1241 (0.0002 -- 2.3578)  max mem: 16413
Epoch: [46] Total time: 0:02:21 (0.8820 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 2.0156 (1.8903)  loss_scale: 16384.0000 (23859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2355 (6.4878)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6264 (0.6264)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3305 (2.3305 -- 2.3305)  data: 2.1182 (2.1182 -- 2.1182)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6264 (0.8619)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4234 (0.2020 -- 2.3305)  data: 0.2030 (0.0008 -- 2.1182)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7083 (0.8149)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.8254)  time: 0.2239 (0.1702 -- 0.3506)  data: 0.0131 (0.0001 -- 0.1245)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8208 (0.8572)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.8506)  time: 0.2073 (0.1326 -- 0.3506)  data: 0.0128 (0.0001 -- 0.1245)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 79.046 Acc@5 96.266 loss 0.829
Accuracy of the network on the 482 val images: 79.05%
[2023-09-04 23:29:47,407] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:29:47,409] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:29:47,409] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:29:47,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:29:48,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:29:48,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.05%
Epoch: [47]  [  0/160]  eta: 0:18:19  lr: 0.000046  min_lr: 0.000012  loss: 1.8572 (1.8572)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0538 (8.0538)  time: 6.8745 (6.8745 -- 6.8745)  data: 6.2351 (6.2351 -- 6.2351)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000012  loss: 1.9010 (1.8790)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7335 (7.2202)  time: 0.9155 (0.5205 -- 5.0502)  data: 0.3639 (0.0005 -- 4.5017)  max mem: 16413
[2023-09-04 23:30:28,367] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:30:28,368] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:30:28,368] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:30:28,368] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [47]  [ 40/160]  eta: 0:02:06  lr: 0.000046  min_lr: 0.000012  loss: 1.8527 (1.8438)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2369 (7.0289)  time: 0.9073 (0.5262 -- 3.6515)  data: 0.3581 (0.0005 -- 3.1012)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000012  loss: 1.8106 (1.8437)  loss_scale: 32768.0000 (23098.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5234 (6.7902)  time: 0.9164 (0.5154 -- 2.9976)  data: 0.2139 (0.0005 -- 2.4552)  max mem: 16413
Epoch: [47]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000012  loss: 1.7222 (1.8377)  loss_scale: 32768.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7047 (6.6048)  time: 0.8735 (0.5318 -- 3.9416)  data: 0.2234 (0.0005 -- 3.4026)  max mem: 16413
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.7982 (1.8358)  loss_scale: 32768.0000 (26928.1584)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1186 (6.5729)  time: 0.8175 (0.5289 -- 4.0911)  data: 0.2680 (0.0003 -- 3.5693)  max mem: 16413
Epoch: [47]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8570 (1.8510)  loss_scale: 32768.0000 (27893.4215)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7995 (6.6682)  time: 0.9007 (0.5215 -- 4.9418)  data: 0.3557 (0.0002 -- 4.4244)  max mem: 16413
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.8849 (1.8608)  loss_scale: 32768.0000 (28584.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1579 (6.6666)  time: 0.7547 (0.5425 -- 2.7178)  data: 0.2042 (0.0005 -- 2.1969)  max mem: 16413
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8816 (1.8502)  loss_scale: 32768.0000 (29081.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7706 (6.8198)  time: 0.7245 (0.4931 -- 3.3956)  data: 0.1994 (0.0002 -- 2.8821)  max mem: 16413
Epoch: [47] Total time: 0:02:22 (0.8910 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8816 (1.8607)  loss_scale: 32768.0000 (29081.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7706 (6.8198)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.5246 (0.5246)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4920 (2.4920 -- 2.4920)  data: 2.2136 (2.2136 -- 2.2136)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7981 (0.9213)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4414 (0.2000 -- 2.4920)  data: 0.2165 (0.0007 -- 2.2136)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7981 (0.8396)  acc1: 77.7778 (74.0741)  acc5: 100.0000 (96.8254)  time: 0.2177 (0.1697 -- 0.3937)  data: 0.0086 (0.0001 -- 0.1473)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8096 (0.8882)  acc1: 77.7778 (71.3693)  acc5: 100.0000 (95.4357)  time: 0.2008 (0.1330 -- 0.3937)  data: 0.0082 (0.0001 -- 0.1473)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 75.311 Acc@5 95.851 loss 0.839
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 79.05%
Epoch: [48]  [  0/160]  eta: 0:17:36  lr: 0.000046  min_lr: 0.000012  loss: 1.5759 (1.5759)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5023 (5.5023)  time: 6.6027 (6.6027 -- 6.6027)  data: 5.9955 (5.9955 -- 5.9955)  max mem: 16413
[2023-09-04 23:32:29,265] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:32:29,265] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:32:29,265] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:32:29,265] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:32:30,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7687
[2023-09-04 23:32:30,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7687
[2023-09-04 23:32:30,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:32:30,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:32:30,866] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [48]  [ 20/160]  eta: 0:02:43  lr: 0.000046  min_lr: 0.000012  loss: 1.8804 (1.9280)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6372 (6.1087)  time: 0.8998 (0.5262 -- 3.0072)  data: 0.3544 (0.0008 -- 2.4779)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000012  loss: 1.7886 (1.8411)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1266 (6.3131)  time: 0.8582 (0.5239 -- 3.0706)  data: 0.1682 (0.0005 -- 2.5390)  max mem: 16413
Epoch: [48]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000012  loss: 1.9046 (1.8578)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0018 (6.6095)  time: 1.0009 (0.5311 -- 4.5816)  data: 0.0014 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000012  loss: 1.8305 (1.8678)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0082 (6.7244)  time: 0.7777 (0.5264 -- 2.5331)  data: 0.0017 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [48]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.8609 (1.8562)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2948 (6.7464)  time: 0.8174 (0.5362 -- 3.0908)  data: 0.0016 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [48]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8877 (1.8577)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5587 (6.8847)  time: 1.0015 (0.5192 -- 4.3279)  data: 0.1496 (0.0003 -- 2.9603)  max mem: 16413
[2023-09-04 23:34:24,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:34:24,143] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:34:24,144] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:34:24,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [48]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9156 (1.8687)  loss_scale: 32768.0000 (34627.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3418 (6.7945)  time: 0.6678 (0.5329 -- 1.9415)  data: 0.1158 (0.0005 -- 1.3920)  max mem: 16413
[2023-09-04 23:34:38,121] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7834
[2023-09-04 23:34:38,122] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:34:38,122] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7834
[2023-09-04 23:34:38,122] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 23:34:38,122] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9648 (1.8589)  loss_scale: 65536.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4912 (6.7843)  time: 0.7764 (0.4822 -- 1.8278)  data: 0.0556 (0.0001 -- 0.6775)  max mem: 16413
Epoch: [48] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9648 (1.8494)  loss_scale: 65536.0000 (37068.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4912 (6.7843)
Val:  [ 0/27]  eta: 0:00:54  loss: 0.3946 (0.3946)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0250 (2.0250 -- 2.0250)  data: 1.7925 (1.7925 -- 1.7925)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6337 (0.8702)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4127 (0.2072 -- 2.0250)  data: 0.1940 (0.0010 -- 1.7925)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7285 (0.8142)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.8254)  time: 0.2345 (0.1714 -- 0.3631)  data: 0.0267 (0.0002 -- 0.1731)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8149 (0.8492)  acc1: 77.7778 (73.8589)  acc5: 100.0000 (95.8506)  time: 0.2174 (0.1334 -- 0.3631)  data: 0.0253 (0.0001 -- 0.1731)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 76.141 Acc@5 96.058 loss 0.820
Accuracy of the network on the 482 val images: 76.14%
Max accuracy: 79.05%
Epoch: [49]  [  0/160]  eta: 0:18:21  lr: 0.000046  min_lr: 0.000012  loss: 2.3931 (2.3931)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1955 (7.1955)  time: 6.8835 (6.8835 -- 6.8835)  data: 6.3539 (6.3539 -- 6.3539)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:46  lr: 0.000046  min_lr: 0.000012  loss: 1.9542 (1.9145)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4127 (7.7609)  time: 0.9051 (0.5162 -- 3.2975)  data: 0.3599 (0.0002 -- 2.7331)  max mem: 16413
Epoch: [49]  [ 40/160]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000012  loss: 1.9803 (1.9149)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7480 (6.8786)  time: 0.9633 (0.5265 -- 3.4065)  data: 0.4090 (0.0003 -- 2.8337)  max mem: 16413
Epoch: [49]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 1.8277 (1.9075)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1695 (6.7748)  time: 0.8217 (0.5212 -- 4.3398)  data: 0.2689 (0.0007 -- 3.8241)  max mem: 16413
[2023-09-04 23:36:02,674] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7915
[2023-09-04 23:36:02,674] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7915
[2023-09-04 23:36:02,674] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:36:02,674] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:36:02,674] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [49]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9699 (1.9119)  loss_scale: 32768.0000 (31554.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0709 (6.7203)  time: 0.9226 (0.5180 -- 3.5460)  data: 0.3768 (0.0002 -- 3.0189)  max mem: 16413
Epoch: [49]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.9915 (1.9118)  loss_scale: 16384.0000 (28550.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5021 (6.6826)  time: 0.8948 (0.5176 -- 4.1143)  data: 0.3559 (0.0003 -- 3.5838)  max mem: 16413
Epoch: [49]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 2.0479 (1.9077)  loss_scale: 16384.0000 (26539.3719)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5176 (6.5610)  time: 0.8451 (0.5143 -- 4.2331)  data: 0.3020 (0.0003 -- 3.6904)  max mem: 16413
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.8226 (1.8936)  loss_scale: 16384.0000 (25098.8936)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0254 (6.6168)  time: 0.7910 (0.5274 -- 3.8120)  data: 0.2398 (0.0004 -- 3.2628)  max mem: 16413
[2023-09-04 23:37:13,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[1.1754671322001162e-05, 1.1754671322001162e-05, 1.3060745913334626e-05, 1.3060745913334626e-05, 1.4511939903705136e-05, 1.4511939903705136e-05, 1.6124377670783485e-05, 1.6124377670783485e-05, 1.7915975189759427e-05, 1.7915975189759427e-05, 1.99066390997327e-05, 1.99066390997327e-05, 2.2118487888591886e-05, 2.2118487888591886e-05, 2.4576097653990983e-05, 2.4576097653990983e-05, 2.730677517110109e-05, 2.730677517110109e-05, 3.034086130122343e-05, 3.034086130122343e-05, 3.371206811247048e-05, 3.371206811247048e-05, 3.7457853458300536e-05, 3.7457853458300536e-05, 4.161983717588948e-05, 4.161983717588948e-05, 4.624426352876609e-05, 4.624426352876609e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 23:37:13,292] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=17.402191255814728, CurrSamplesPerSec=24.52483076081983, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7830 (1.8788)  loss_scale: 16384.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2749 (6.6291)  time: 0.7361 (0.4944 -- 4.9284)  data: 0.2213 (0.0002 -- 4.4159)  max mem: 16413
Epoch: [49] Total time: 0:02:24 (0.9003 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7830 (1.8840)  loss_scale: 16384.0000 (24064.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2749 (6.6291)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.5015 (0.5015)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1959 (2.1959 -- 2.1959)  data: 1.9565 (1.9565 -- 1.9565)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6146 (0.8170)  acc1: 88.8889 (74.7475)  acc5: 100.0000 (97.9798)  time: 0.4147 (0.2035 -- 2.1959)  data: 0.1969 (0.0009 -- 1.9565)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6527 (0.7589)  acc1: 88.8889 (78.3069)  acc5: 100.0000 (97.3545)  time: 0.2282 (0.1710 -- 0.4323)  data: 0.0235 (0.0001 -- 0.2153)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7019 (0.7982)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.6805)  time: 0.2142 (0.1369 -- 0.4323)  data: 0.0232 (0.0001 -- 0.2153)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 79.046 Acc@5 96.473 loss 0.771
Accuracy of the network on the 482 val images: 79.05%
[2023-09-04 23:37:21,050] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:37:21,051] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:37:21,052] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:37:21,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:37:22,272] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:37:22,272] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.05%
Epoch: [50]  [  0/160]  eta: 0:21:06  lr: 0.000046  min_lr: 0.000012  loss: 2.0482 (2.0482)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5345 (6.5345)  time: 7.9179 (7.9179 -- 7.9179)  data: 5.9353 (5.9353 -- 5.9353)  max mem: 16413
Epoch: [50]  [ 20/160]  eta: 0:02:45  lr: 0.000046  min_lr: 0.000012  loss: 1.6331 (1.6980)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9021 (6.5639)  time: 0.8427 (0.5330 -- 3.6667)  data: 0.1468 (0.0003 -- 1.2286)  max mem: 16413
Epoch: [50]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000012  loss: 1.7277 (1.7411)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2899 (6.6551)  time: 0.8963 (0.5200 -- 2.4802)  data: 0.1327 (0.0002 -- 1.2556)  max mem: 16413
[2023-09-04 23:38:09,205] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:38:09,205] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:38:09,208] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:38:09,208] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [50]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000012  loss: 1.8038 (1.7589)  loss_scale: 32768.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5150 (6.6749)  time: 0.8833 (0.5173 -- 2.5508)  data: 0.2747 (0.0003 -- 2.0338)  max mem: 16413
Epoch: [50]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 2.0321 (1.7837)  loss_scale: 32768.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4908 (6.6352)  time: 0.8909 (0.5277 -- 2.9633)  data: 0.0301 (0.0001 -- 0.3676)  max mem: 16413
Epoch: [50]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.8730 (1.7925)  loss_scale: 32768.0000 (25630.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9976 (6.7817)  time: 0.8320 (0.5284 -- 3.6821)  data: 0.0963 (0.0004 -- 1.9016)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.7042 (1.7951)  loss_scale: 32768.0000 (26810.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1620 (6.8502)  time: 0.8674 (0.5292 -- 4.0958)  data: 0.2956 (0.0006 -- 3.5686)  max mem: 16413
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.8150 (1.8111)  loss_scale: 32768.0000 (27655.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3813 (6.9639)  time: 0.8294 (0.5310 -- 3.4521)  data: 0.2765 (0.0006 -- 2.8795)  max mem: 16413
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7919 (1.8106)  loss_scale: 32768.0000 (28262.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9331 (7.0265)  time: 0.6877 (0.4976 -- 3.8600)  data: 0.1677 (0.0002 -- 3.3415)  max mem: 16413
Epoch: [50] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7919 (1.8427)  loss_scale: 32768.0000 (28262.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9331 (7.0265)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3878 (0.3878)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3884 (2.3884 -- 2.3884)  data: 2.1712 (2.1712 -- 2.1712)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7268 (0.8053)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4164 (0.1946 -- 2.3884)  data: 0.2026 (0.0006 -- 2.1712)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6785 (0.7855)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (95.7672)  time: 0.2194 (0.1698 -- 0.3398)  data: 0.0146 (0.0001 -- 0.1648)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8847 (0.8351)  acc1: 71.4286 (75.1037)  acc5: 100.0000 (95.8506)  time: 0.2057 (0.1337 -- 0.3398)  data: 0.0143 (0.0001 -- 0.1648)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 76.556 Acc@5 96.680 loss 0.818
Accuracy of the network on the 482 val images: 76.56%
Max accuracy: 79.05%
Epoch: [51]  [  0/160]  eta: 0:19:34  lr: 0.000046  min_lr: 0.000012  loss: 2.0662 (2.0662)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3727 (9.3727)  time: 7.3387 (7.3387 -- 7.3387)  data: 6.8027 (6.8027 -- 6.8027)  max mem: 16413
[2023-09-04 23:40:10,399] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:40:10,400] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:40:10,402] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:40:10,402] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:40:12,113] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8175
[2023-09-04 23:40:12,114] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:40:12,117] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8175
[2023-09-04 23:40:12,118] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:40:12,118] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [51]  [ 20/160]  eta: 0:02:41  lr: 0.000046  min_lr: 0.000012  loss: 1.7946 (1.8371)  loss_scale: 32768.0000 (37449.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4745 (6.9034)  time: 0.8454 (0.5361 -- 3.8548)  data: 0.2213 (0.0005 -- 3.3175)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000012  loss: 1.7877 (1.8194)  loss_scale: 32768.0000 (35165.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6936 (7.0218)  time: 0.9075 (0.5476 -- 3.4516)  data: 0.3402 (0.0004 -- 2.9045)  max mem: 16413
Epoch: [51]  [ 60/160]  eta: 0:01:35  lr: 0.000046  min_lr: 0.000012  loss: 1.7138 (1.8175)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7192 (7.1777)  time: 0.7808 (0.5352 -- 2.4060)  data: 0.2022 (0.0004 -- 1.8611)  max mem: 16413
Epoch: [51]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.8352 (1.8186)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3820 (7.0928)  time: 0.9386 (0.5195 -- 3.2693)  data: 0.3983 (0.0003 -- 2.7249)  max mem: 16413
[2023-09-04 23:41:09,964] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8242
[2023-09-04 23:41:09,964] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8242
[2023-09-04 23:41:09,964] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:41:09,964] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:41:09,964] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [51]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000012  loss: 1.8549 (1.8271)  loss_scale: 16384.0000 (30659.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9263 (6.9465)  time: 0.8199 (0.5355 -- 4.3059)  data: 0.2684 (0.0002 -- 3.7934)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.9735 (1.8467)  loss_scale: 16384.0000 (28299.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0313 (6.9450)  time: 0.8461 (0.5344 -- 3.5928)  data: 0.2956 (0.0004 -- 3.0431)  max mem: 16413
Epoch: [51]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.8103 (1.8524)  loss_scale: 16384.0000 (26609.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9209 (6.9690)  time: 0.8790 (0.5343 -- 2.7952)  data: 0.1517 (0.0005 -- 2.1372)  max mem: 16413
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9476 (1.8569)  loss_scale: 16384.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6100 (6.9906)  time: 0.7170 (0.4957 -- 2.6559)  data: 0.1482 (0.0002 -- 2.1597)  max mem: 16413
Epoch: [51] Total time: 0:02:21 (0.8844 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9476 (1.8642)  loss_scale: 16384.0000 (25395.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6100 (6.9906)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3776 (0.3776)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5052 (2.5052 -- 2.5052)  data: 2.2588 (2.2588 -- 2.2588)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7507 (0.8096)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (96.9697)  time: 0.4280 (0.1995 -- 2.5052)  data: 0.2069 (0.0006 -- 2.2588)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6868 (0.7625)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (95.7672)  time: 0.2136 (0.1706 -- 0.2504)  data: 0.0047 (0.0001 -- 0.0465)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7793 (0.8174)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.0207)  time: 0.1947 (0.1327 -- 0.2383)  data: 0.0041 (0.0001 -- 0.0465)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 78.838 Acc@5 95.851 loss 0.787
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 79.05%
Epoch: [52]  [  0/160]  eta: 0:19:15  lr: 0.000046  min_lr: 0.000012  loss: 1.8326 (1.8326)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5949 (8.5949)  time: 7.2206 (7.2206 -- 7.2206)  data: 6.6440 (6.6440 -- 6.6440)  max mem: 16413
Epoch: [52]  [ 20/160]  eta: 0:02:29  lr: 0.000046  min_lr: 0.000012  loss: 1.5680 (1.6820)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9756 (6.2894)  time: 0.7588 (0.5449 -- 2.7421)  data: 0.0494 (0.0006 -- 0.9087)  max mem: 16413
Epoch: [52]  [ 40/160]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000012  loss: 1.7948 (1.7309)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1590 (6.3434)  time: 0.9816 (0.5254 -- 3.4848)  data: 0.0193 (0.0005 -- 0.3421)  max mem: 16413
[2023-09-04 23:43:12,605] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:43:12,605] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:43:12,608] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:43:12,608] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [52]  [ 60/160]  eta: 0:01:34  lr: 0.000046  min_lr: 0.000012  loss: 1.9642 (1.8143)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2447 (6.4936)  time: 0.7930 (0.5339 -- 3.7009)  data: 0.0031 (0.0006 -- 0.0129)  max mem: 16413
[2023-09-04 23:43:39,358] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8400
[2023-09-04 23:43:39,358] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8400
[2023-09-04 23:43:39,358] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:43:39,358] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:43:39,358] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [52]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 1.8856 (1.8275)  loss_scale: 32768.0000 (22249.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4595 (6.5359)  time: 1.0058 (0.5224 -- 2.9329)  data: 0.0200 (0.0004 -- 0.3739)  max mem: 16413
Epoch: [52]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.9251 (1.8395)  loss_scale: 16384.0000 (21088.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5087 (6.5961)  time: 0.8869 (0.5265 -- 3.8571)  data: 0.0018 (0.0004 -- 0.0056)  max mem: 16413
Epoch: [52]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8361 (1.8458)  loss_scale: 16384.0000 (20310.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2182 (6.6215)  time: 0.8634 (0.5218 -- 3.5983)  data: 0.0014 (0.0001 -- 0.0053)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9292 (1.8510)  loss_scale: 16384.0000 (19753.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1411 (6.6404)  time: 0.8060 (0.5220 -- 3.3523)  data: 0.0016 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8338 (1.8492)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1887 (6.6351)  time: 0.7096 (0.4950 -- 3.9903)  data: 0.0008 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [52] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8338 (1.8644)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1887 (6.6351)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.3442 (0.3442)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1089 (2.1089 -- 2.1089)  data: 1.8950 (1.8950 -- 1.8950)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6131 (0.7690)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4028 (0.1966 -- 2.1089)  data: 0.1839 (0.0008 -- 1.8950)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6140 (0.7317)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (95.7672)  time: 0.2271 (0.1714 -- 0.4528)  data: 0.0174 (0.0001 -- 0.2171)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7679 (0.7901)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.4357)  time: 0.2075 (0.1333 -- 0.4528)  data: 0.0160 (0.0001 -- 0.2171)  max mem: 16413
Val: Total time: 0:00:07 (0.2829 s / it)
* Acc@1 79.253 Acc@5 96.266 loss 0.765
Accuracy of the network on the 482 val images: 79.25%
[2023-09-04 23:44:51,898] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:44:51,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:44:51,900] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:44:51,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:44:53,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:44:53,334] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.25%
Epoch: [53]  [  0/160]  eta: 0:19:19  lr: 0.000046  min_lr: 0.000012  loss: 2.2059 (2.2059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7700 (6.7700)  time: 7.2489 (7.2489 -- 7.2489)  data: 6.6497 (6.6497 -- 6.6497)  max mem: 16413
Epoch: [53]  [ 20/160]  eta: 0:02:40  lr: 0.000046  min_lr: 0.000012  loss: 1.8739 (1.8856)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9674 (7.3199)  time: 0.8397 (0.5355 -- 3.1256)  data: 0.2353 (0.0006 -- 2.5983)  max mem: 16413
Epoch: [53]  [ 40/160]  eta: 0:02:13  lr: 0.000046  min_lr: 0.000012  loss: 1.7437 (1.8190)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8760 (6.9652)  time: 1.0785 (0.5135 -- 5.0074)  data: 0.5347 (0.0003 -- 4.4784)  max mem: 16413
[2023-09-04 23:45:45,960] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:45:45,960] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:45:45,965] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:45:45,965] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [53]  [ 60/160]  eta: 0:01:38  lr: 0.000046  min_lr: 0.000012  loss: 2.0411 (1.8956)  loss_scale: 32768.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0701 (6.7336)  time: 0.7319 (0.5101 -- 2.6457)  data: 0.1811 (0.0004 -- 2.0990)  max mem: 16413
Epoch: [53]  [ 80/160]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000012  loss: 1.7571 (1.8609)  loss_scale: 32768.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6291 (6.8008)  time: 0.7341 (0.5303 -- 2.5298)  data: 0.1354 (0.0008 -- 1.9702)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 2.0095 (1.8882)  loss_scale: 32768.0000 (24819.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7713 (6.9750)  time: 1.0586 (0.5322 -- 5.0651)  data: 0.0347 (0.0002 -- 0.5111)  max mem: 16413
Epoch: [53]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.9333 (1.8792)  loss_scale: 32768.0000 (26133.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6498 (6.9407)  time: 0.8341 (0.5395 -- 3.6925)  data: 0.0015 (0.0006 -- 0.0029)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.7080 (1.8650)  loss_scale: 32768.0000 (27074.2695)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6779 (6.9903)  time: 0.8456 (0.5308 -- 4.8279)  data: 0.0017 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9115 (1.8601)  loss_scale: 32768.0000 (27750.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1298 (7.1031)  time: 0.7069 (0.4958 -- 3.2888)  data: 0.0017 (0.0002 -- 0.0162)  max mem: 16413
Epoch: [53] Total time: 0:02:23 (0.8957 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9115 (1.8380)  loss_scale: 32768.0000 (27750.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1298 (7.1031)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.4020 (0.4020)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5661 (2.5661 -- 2.5661)  data: 2.3241 (2.3241 -- 2.3241)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6611 (0.8328)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4306 (0.2035 -- 2.5661)  data: 0.2124 (0.0007 -- 2.3241)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5983 (0.7564)  acc1: 88.8889 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2124 (0.1715 -- 0.2469)  data: 0.0029 (0.0001 -- 0.0434)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6350 (0.7910)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.1961 (0.1364 -- 0.2469)  data: 0.0026 (0.0001 -- 0.0434)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 79.046 Acc@5 96.473 loss 0.780
Accuracy of the network on the 482 val images: 79.05%
Max accuracy: 79.25%
Epoch: [54]  [  0/160]  eta: 0:19:01  lr: 0.000046  min_lr: 0.000012  loss: 2.4433 (2.4433)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2785 (6.2785)  time: 7.1319 (7.1319 -- 7.1319)  data: 6.5794 (6.5794 -- 6.5794)  max mem: 16413
[2023-09-04 23:47:47,730] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:47:47,734] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:47:47,772] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:47:47,772] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:47:48,317] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8658
[2023-09-04 23:47:48,317] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:47:48,317] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8658
[2023-09-04 23:47:48,318] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:47:48,318] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [54]  [ 20/160]  eta: 0:02:46  lr: 0.000046  min_lr: 0.000012  loss: 1.7348 (1.7702)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7557 (6.3671)  time: 0.8916 (0.5174 -- 3.4527)  data: 0.3022 (0.0004 -- 2.0714)  max mem: 16413
Epoch: [54]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000012  loss: 1.7901 (1.7904)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5114 (6.5396)  time: 0.8814 (0.5270 -- 3.2359)  data: 0.2262 (0.0005 -- 2.6788)  max mem: 16413
Epoch: [54]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000012  loss: 1.8090 (1.7857)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5842 (6.5590)  time: 0.9772 (0.5006 -- 4.7199)  data: 0.4276 (0.0003 -- 4.2037)  max mem: 16413
Epoch: [54]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 1.9306 (1.8247)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2938 (6.6034)  time: 0.8266 (0.5157 -- 2.8912)  data: 0.2811 (0.0002 -- 2.3819)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.8988 (1.8298)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5781 (6.5917)  time: 0.9066 (0.5051 -- 4.1759)  data: 0.3663 (0.0003 -- 3.6517)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8245 (1.8353)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1921 (6.6679)  time: 0.8456 (0.5199 -- 3.2005)  data: 0.2920 (0.0003 -- 2.6755)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.8566 (1.8362)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0611 (6.6556)  time: 0.7598 (0.5322 -- 2.5819)  data: 0.2038 (0.0001 -- 2.0448)  max mem: 16413
[2023-09-04 23:49:40,496] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:49:40,496] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:49:40,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:49:40,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.7944 (1.8286)  loss_scale: 65536.0000 (35635.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3276 (6.6327)  time: 0.7007 (0.4961 -- 3.8757)  data: 0.1824 (0.0002 -- 3.3101)  max mem: 16413
Epoch: [54] Total time: 0:02:22 (0.8899 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.7944 (1.8279)  loss_scale: 65536.0000 (35635.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3276 (6.6327)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2786 (0.2786)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3820 (2.3820 -- 2.3820)  data: 2.1511 (2.1511 -- 2.1511)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6174 (0.7421)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4182 (0.1988 -- 2.3820)  data: 0.1967 (0.0005 -- 2.1511)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6174 (0.7045)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2188 (0.1694 -- 0.3776)  data: 0.0100 (0.0001 -- 0.1845)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6284 (0.7600)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2010 (0.1325 -- 0.3776)  data: 0.0097 (0.0001 -- 0.1845)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 79.253 Acc@5 96.058 loss 0.743
Accuracy of the network on the 482 val images: 79.25%
Max accuracy: 79.25%
Epoch: [55]  [  0/160]  eta: 0:22:51  lr: 0.000046  min_lr: 0.000012  loss: 1.4529 (1.4529)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5676 (10.5676)  time: 8.5723 (8.5722 -- 8.5722)  data: 7.2640 (7.2640 -- 7.2640)  max mem: 16413
[2023-09-04 23:50:04,258] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8802
[2023-09-04 23:50:04,258] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:50:04,258] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8802
[2023-09-04 23:50:04,258] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 23:50:04,258] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [55]  [ 20/160]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000012  loss: 1.8610 (1.9065)  loss_scale: 32768.0000 (35888.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4776 (6.4498)  time: 0.8282 (0.5255 -- 3.5205)  data: 0.1951 (0.0001 -- 2.2908)  max mem: 16413
Epoch: [55]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000012  loss: 1.9735 (1.9383)  loss_scale: 32768.0000 (34366.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0435 (6.2735)  time: 0.8675 (0.5262 -- 3.3915)  data: 0.2344 (0.0004 -- 2.8236)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000012  loss: 1.8751 (1.9018)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4562 (6.3977)  time: 0.9291 (0.5343 -- 3.4384)  data: 0.3801 (0.0004 -- 2.9194)  max mem: 16413
Epoch: [55]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.8524 (1.8672)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8330 (6.6283)  time: 0.7537 (0.5235 -- 2.3448)  data: 0.2077 (0.0002 -- 1.8184)  max mem: 16413
Epoch: [55]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000012  loss: 1.8791 (1.8595)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9853 (6.8540)  time: 0.9330 (0.5222 -- 4.4662)  data: 0.3831 (0.0004 -- 3.9307)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.8430 (1.8555)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3420 (6.8386)  time: 0.8328 (0.5322 -- 3.0966)  data: 0.2538 (0.0003 -- 2.5708)  max mem: 16413
[2023-09-04 23:51:54,141] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:51:54,141] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:51:54,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:51:54,142] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:51:57,593] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8933
[2023-09-04 23:51:57,593] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8933
[2023-09-04 23:51:57,594] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:51:57,594] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-04 23:51:57,594] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9371 (1.8692)  loss_scale: 32768.0000 (33697.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4319 (6.8120)  time: 0.8670 (0.5226 -- 2.9162)  data: 0.3056 (0.0004 -- 2.3876)  max mem: 16413
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9349 (1.8816)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7995 (6.8167)  time: 0.7997 (0.4959 -- 3.4221)  data: 0.2730 (0.0002 -- 2.9068)  max mem: 16413
Epoch: [55] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9349 (1.8543)  loss_scale: 32768.0000 (33587.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7995 (6.8167)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.3978 (0.3978)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1778 (2.1778 -- 2.1778)  data: 1.9463 (1.9463 -- 1.9463)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6520 (0.7678)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4046 (0.2017 -- 2.1778)  data: 0.1885 (0.0009 -- 1.9463)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6808 (0.7345)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2316 (0.1706 -- 0.4048)  data: 0.0256 (0.0001 -- 0.2169)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7473 (0.7756)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.8506)  time: 0.2178 (0.1322 -- 0.4048)  data: 0.0245 (0.0001 -- 0.2169)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 79.876 Acc@5 96.473 loss 0.757
Accuracy of the network on the 482 val images: 79.88%
[2023-09-04 23:52:24,841] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:52:24,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:52:24,843] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:52:24,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:52:26,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:52:26,244] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.88%
Epoch: [56]  [  0/160]  eta: 0:18:13  lr: 0.000046  min_lr: 0.000012  loss: 1.9846 (1.9846)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7984 (5.7984)  time: 6.8361 (6.8361 -- 6.8361)  data: 6.2875 (6.2875 -- 6.2875)  max mem: 16413
Epoch: [56]  [ 20/160]  eta: 0:02:37  lr: 0.000046  min_lr: 0.000012  loss: 1.8796 (1.9029)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4710 (7.6237)  time: 0.8415 (0.5335 -- 4.1667)  data: 0.2717 (0.0008 -- 3.5974)  max mem: 16413
[2023-09-04 23:53:06,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[1.1639670759428334e-05, 1.1639670759428334e-05, 1.2932967510475928e-05, 1.2932967510475928e-05, 1.4369963900528806e-05, 1.4369963900528806e-05, 1.596662655614312e-05, 1.596662655614312e-05, 1.7740696173492355e-05, 1.7740696173492355e-05, 1.9711884637213724e-05, 1.9711884637213724e-05, 2.1902094041348583e-05, 2.1902094041348583e-05, 2.433566004594287e-05, 2.433566004594287e-05, 2.7039622273269855e-05, 2.7039622273269855e-05, 3.0044024748077613e-05, 3.0044024748077613e-05, 3.3382249720086243e-05, 3.3382249720086243e-05, 3.70913885778736e-05, 3.70913885778736e-05, 4.1212653975415106e-05, 4.1212653975415106e-05, 4.579183775046123e-05, 4.579183775046123e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-04 23:53:06,541] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=17.53525913989682, CurrSamplesPerSec=21.845627263214045, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000012  loss: 1.7050 (1.8088)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7378 (7.3367)  time: 0.9175 (0.5197 -- 3.9988)  data: 0.3565 (0.0002 -- 3.4483)  max mem: 16413
Epoch: [56]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000012  loss: 1.8280 (1.8027)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6374 (7.2113)  time: 0.9353 (0.5218 -- 3.6291)  data: 0.0035 (0.0002 -- 0.0153)  max mem: 16413
Epoch: [56]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.8159 (1.8278)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7135 (7.5229)  time: 0.7783 (0.5389 -- 2.3413)  data: 0.0217 (0.0004 -- 0.3851)  max mem: 16413
Epoch: [56]  [100/160]  eta: 0:00:54  lr: 0.000046  min_lr: 0.000012  loss: 1.6444 (1.7830)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5325 (7.3368)  time: 0.8111 (0.5376 -- 1.8170)  data: 0.0395 (0.0004 -- 0.4256)  max mem: 16413
[2023-09-04 23:54:00,590] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:54:00,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:54:00,591] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:54:00,591] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-04 23:54:01,139] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9063
[2023-09-04 23:54:01,139] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9063
[2023-09-04 23:54:01,139] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:54:01,139] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-04 23:54:01,139] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [56]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000012  loss: 1.9049 (1.7967)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9912 (7.1845)  time: 0.8885 (0.5364 -- 2.2744)  data: 0.1749 (0.0005 -- 1.6960)  max mem: 16413
[2023-09-04 23:54:32,623] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9098
[2023-09-04 23:54:32,623] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9098
[2023-09-04 23:54:32,624] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:54:32,624] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:54:32,624] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.7686 (1.7921)  loss_scale: 32768.0000 (32651.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5606 (7.1194)  time: 0.8603 (0.5238 -- 2.2866)  data: 0.0310 (0.0006 -- 0.3652)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.9727 (1.8033)  loss_scale: 16384.0000 (30720.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6505 (7.0334)  time: 0.7201 (0.4932 -- 2.3269)  data: 0.0039 (0.0002 -- 0.0592)  max mem: 16413
Epoch: [56] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.9727 (1.8067)  loss_scale: 16384.0000 (30720.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6505 (7.0334)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2713 (0.2713)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3208 (2.3208 -- 2.3208)  data: 2.0924 (2.0924 -- 2.0924)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6031 (0.7472)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4315 (0.2104 -- 2.3208)  data: 0.2056 (0.0006 -- 2.0924)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5896 (0.6964)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2242 (0.1699 -- 0.4072)  data: 0.0093 (0.0001 -- 0.1554)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6234 (0.7663)  acc1: 85.7143 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2049 (0.1335 -- 0.4072)  data: 0.0088 (0.0001 -- 0.1554)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 80.705 Acc@5 96.266 loss 0.744
Accuracy of the network on the 482 val images: 80.71%
[2023-09-04 23:54:55,415] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-04 23:54:55,417] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-04 23:54:55,417] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-04 23:54:55,417] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-04 23:54:56,724] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-04 23:54:56,725] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.71%
Epoch: [57]  [  0/160]  eta: 0:19:37  lr: 0.000046  min_lr: 0.000012  loss: 2.2684 (2.2684)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6873 (5.6873)  time: 7.3585 (7.3585 -- 7.3585)  data: 6.8202 (6.8202 -- 6.8202)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:45  lr: 0.000046  min_lr: 0.000012  loss: 1.9991 (1.9080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2984 (7.4510)  time: 0.8747 (0.5233 -- 2.4143)  data: 0.3204 (0.0004 -- 1.8264)  max mem: 16413
Epoch: [57]  [ 40/160]  eta: 0:02:16  lr: 0.000046  min_lr: 0.000012  loss: 1.5688 (1.7762)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3348 (6.9431)  time: 1.0855 (0.5251 -- 4.8798)  data: 0.5419 (0.0009 -- 4.3780)  max mem: 16413
Epoch: [57]  [ 60/160]  eta: 0:01:37  lr: 0.000046  min_lr: 0.000012  loss: 1.7096 (1.7783)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8245 (7.0162)  time: 0.6445 (0.5118 -- 1.7500)  data: 0.0994 (0.0002 -- 1.2086)  max mem: 16413
Epoch: [57]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000012  loss: 1.6867 (1.7738)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0287 (7.1901)  time: 0.9734 (0.5314 -- 4.0286)  data: 0.4170 (0.0002 -- 3.5097)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000012  loss: 1.8961 (1.7886)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0057 (7.2103)  time: 0.8895 (0.5273 -- 4.2131)  data: 0.3382 (0.0003 -- 3.6878)  max mem: 16413
[2023-09-04 23:56:41,284] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:56:41,284] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-04 23:56:41,285] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-04 23:56:41,285] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [120/160]  eta: 0:00:38  lr: 0.000046  min_lr: 0.000012  loss: 1.9155 (1.7896)  loss_scale: 32768.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1809 (7.0700)  time: 1.0275 (0.5213 -- 4.5981)  data: 0.4858 (0.0003 -- 4.0722)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9536 (1.8065)  loss_scale: 32768.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0352 (7.1689)  time: 0.7637 (0.5225 -- 3.5816)  data: 0.2126 (0.0003 -- 3.0637)  max mem: 16413
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8984 (1.8148)  loss_scale: 32768.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7916 (7.1365)  time: 0.6525 (0.4955 -- 2.7262)  data: 0.1342 (0.0002 -- 2.1831)  max mem: 16413
Epoch: [57] Total time: 0:02:25 (0.9066 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8984 (1.8467)  loss_scale: 32768.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7916 (7.1365)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3068 (0.3068)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3563 (2.3563 -- 2.3563)  data: 2.1000 (2.1000 -- 2.1000)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7258 (0.8013)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4478 (0.2016 -- 2.3563)  data: 0.2271 (0.0009 -- 2.1000)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5686 (0.7130)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2301 (0.1693 -- 0.6320)  data: 0.0268 (0.0001 -- 0.3879)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6191 (0.7798)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.0207)  time: 0.2129 (0.1330 -- 0.6320)  data: 0.0265 (0.0001 -- 0.3879)  max mem: 16413
Val: Total time: 0:00:07 (0.2941 s / it)
* Acc@1 78.423 Acc@5 95.436 loss 0.747
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 80.71%
Epoch: [58]  [  0/160]  eta: 0:26:21  lr: 0.000046  min_lr: 0.000012  loss: 1.7154 (1.7154)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3678 (7.3678)  time: 9.8840 (9.8840 -- 9.8840)  data: 7.0388 (7.0388 -- 7.0388)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:51  lr: 0.000046  min_lr: 0.000012  loss: 1.5907 (1.6579)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5540 (6.7758)  time: 0.7887 (0.5250 -- 3.7772)  data: 0.0018 (0.0003 -- 0.0068)  max mem: 16413
Epoch: [58]  [ 40/160]  eta: 0:02:10  lr: 0.000046  min_lr: 0.000012  loss: 1.8831 (1.7455)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2499 (7.1669)  time: 0.9512 (0.5257 -- 4.7144)  data: 0.2253 (0.0004 -- 1.9052)  max mem: 16413
Epoch: [58]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000012  loss: 1.8635 (1.7527)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3098 (6.8678)  time: 0.8682 (0.5366 -- 5.2680)  data: 0.1478 (0.0004 -- 1.9027)  max mem: 16413
[2023-09-04 23:58:40,108] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9351
[2023-09-04 23:58:40,108] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9351
[2023-09-04 23:58:40,108] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:58:40,108] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-04 23:58:40,108] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [ 80/160]  eta: 0:01:20  lr: 0.000046  min_lr: 0.000012  loss: 1.7491 (1.7483)  loss_scale: 16384.0000 (30745.2840)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1420 (7.0091)  time: 0.9485 (0.5227 -- 4.5866)  data: 0.0184 (0.0002 -- 0.3414)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:58  lr: 0.000046  min_lr: 0.000012  loss: 2.0433 (1.7957)  loss_scale: 16384.0000 (27901.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9588 (7.1825)  time: 0.8318 (0.5137 -- 4.1099)  data: 0.0011 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000012  loss: 1.9020 (1.8319)  loss_scale: 16384.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3538 (7.1105)  time: 0.8327 (0.5169 -- 2.9137)  data: 0.0014 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [58]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000012  loss: 1.9514 (1.8417)  loss_scale: 16384.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4691 (7.1251)  time: 0.7875 (0.5317 -- 2.9838)  data: 0.0021 (0.0007 -- 0.0138)  max mem: 16413
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000012  loss: 1.8473 (1.8417)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7435 (6.9986)  time: 0.6900 (0.4959 -- 2.8840)  data: 0.0008 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [58] Total time: 0:02:23 (0.8956 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000012  loss: 1.8473 (1.8407)  loss_scale: 16384.0000 (23654.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7435 (6.9986)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.3213 (0.3213)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0778 (2.0778 -- 2.0778)  data: 1.8336 (1.8336 -- 1.8336)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5573 (0.7119)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4074 (0.2033 -- 2.0778)  data: 0.1910 (0.0009 -- 1.8336)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4975 (0.6505)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.2963)  time: 0.2335 (0.1705 -- 0.3745)  data: 0.0290 (0.0001 -- 0.2005)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6860 (0.7227)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.2189 (0.1330 -- 0.3745)  data: 0.0286 (0.0001 -- 0.2005)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 81.120 Acc@5 97.095 loss 0.700
Accuracy of the network on the 482 val images: 81.12%
[2023-09-05 00:00:00,915] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 00:00:00,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 00:00:00,917] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 00:00:00,917] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 00:00:02,310] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 00:00:02,311] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.12%
Epoch: [59]  [  0/160]  eta: 0:20:23  lr: 0.000046  min_lr: 0.000012  loss: 1.6620 (1.6620)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7661 (6.7661)  time: 7.6450 (7.6450 -- 7.6450)  data: 6.7127 (6.7127 -- 6.7127)  max mem: 16413
Epoch: [59]  [ 20/160]  eta: 0:02:41  lr: 0.000046  min_lr: 0.000012  loss: 1.7004 (1.8031)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5012 (7.0680)  time: 0.8268 (0.5338 -- 4.2278)  data: 0.0779 (0.0003 -- 1.5178)  max mem: 16413
[2023-09-05 00:00:46,340] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:00:46,340] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:00:46,343] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:00:46,343] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [59]  [ 40/160]  eta: 0:02:08  lr: 0.000046  min_lr: 0.000012  loss: 1.9047 (1.8636)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7770 (7.1399)  time: 0.9924 (0.5223 -- 4.2444)  data: 0.3909 (0.0002 -- 3.6767)  max mem: 16413
Epoch: [59]  [ 60/160]  eta: 0:01:35  lr: 0.000046  min_lr: 0.000012  loss: 1.8393 (1.8551)  loss_scale: 32768.0000 (22024.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6176 (6.9791)  time: 0.7241 (0.5275 -- 2.9735)  data: 0.1499 (0.0004 -- 2.4018)  max mem: 16413
[2023-09-05 00:01:15,793] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9516
[2023-09-05 00:01:15,793] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:01:15,793] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9516
[2023-09-05 00:01:15,794] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:01:15,794] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [59]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000012  loss: 1.5588 (1.7999)  loss_scale: 32768.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8587 (7.0814)  time: 0.8792 (0.5215 -- 3.7619)  data: 0.1145 (0.0001 -- 1.4381)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000012  loss: 1.9493 (1.8352)  loss_scale: 16384.0000 (22223.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5302 (7.0572)  time: 0.9298 (0.5190 -- 3.8967)  data: 0.0503 (0.0003 -- 0.8198)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000012  loss: 1.8289 (1.8230)  loss_scale: 16384.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3388 (7.0670)  time: 0.7977 (0.5312 -- 2.4263)  data: 0.1129 (0.0002 -- 1.0947)  max mem: 16413
Epoch: [59]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.7971 (1.8204)  loss_scale: 16384.0000 (20567.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3021 (7.0861)  time: 0.9042 (0.5279 -- 3.9523)  data: 0.2878 (0.0004 -- 3.4028)  max mem: 16413
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.6128 (1.8121)  loss_scale: 16384.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7120 (7.1003)  time: 0.6640 (0.4963 -- 2.0774)  data: 0.1390 (0.0001 -- 1.5330)  max mem: 16413
Epoch: [59] Total time: 0:02:21 (0.8843 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.6128 (1.7966)  loss_scale: 16384.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7120 (7.1003)
[2023-09-05 00:02:23,799] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-09-05 00:02:23,801] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-09-05 00:02:23,801] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-09-05 00:02:23,801] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-09-05 00:02:24,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-09-05 00:02:24,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2474 (0.2474)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3997 (2.3997 -- 2.3997)  data: 2.1901 (2.1901 -- 2.1901)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5256 (0.7399)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (95.9596)  time: 0.4236 (0.1987 -- 2.3997)  data: 0.2086 (0.0006 -- 2.1901)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5267 (0.6696)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (95.7672)  time: 0.2230 (0.1698 -- 0.3406)  data: 0.0144 (0.0001 -- 0.0973)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5946 (0.7426)  acc1: 85.7143 (81.3278)  acc5: 100.0000 (95.0207)  time: 0.2095 (0.1335 -- 0.3406)  data: 0.0142 (0.0001 -- 0.0973)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 81.743 Acc@5 96.058 loss 0.721
Accuracy of the network on the 482 val images: 81.74%
[2023-09-05 00:02:32,697] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 00:02:32,698] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 00:02:32,699] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 00:02:32,699] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 00:02:34,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 00:02:34,169] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.74%
Epoch: [60]  [  0/160]  eta: 0:23:26  lr: 0.000045  min_lr: 0.000012  loss: 2.0201 (2.0201)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9981 (9.9981)  time: 8.7910 (8.7910 -- 8.7910)  data: 8.2358 (8.2358 -- 8.2358)  max mem: 16413
Epoch: [60]  [ 20/160]  eta: 0:02:40  lr: 0.000045  min_lr: 0.000012  loss: 1.6175 (1.7282)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3850 (6.5863)  time: 0.7672 (0.5211 -- 2.3626)  data: 0.2130 (0.0006 -- 1.8323)  max mem: 16413
Epoch: [60]  [ 40/160]  eta: 0:02:04  lr: 0.000045  min_lr: 0.000012  loss: 1.7490 (1.7698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2116 (6.7608)  time: 0.9249 (0.5222 -- 3.1530)  data: 0.2305 (0.0003 -- 2.0110)  max mem: 16413
[2023-09-05 00:03:20,351] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:03:20,351] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:03:20,352] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:03:20,352] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:03:30,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9656
[2023-09-05 00:03:30,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9656
[2023-09-05 00:03:30,832] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:03:30,832] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:03:30,832] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [60]  [ 60/160]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000012  loss: 1.8397 (1.7819)  loss_scale: 32768.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7272 (6.8197)  time: 0.8105 (0.5155 -- 3.7528)  data: 0.1831 (0.0006 -- 2.7951)  max mem: 16413
Epoch: [60]  [ 80/160]  eta: 0:01:17  lr: 0.000045  min_lr: 0.000012  loss: 1.8180 (1.8127)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1958 (6.9090)  time: 1.0002 (0.5246 -- 3.9438)  data: 0.0013 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000012  loss: 1.7324 (1.8112)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0091 (6.9824)  time: 0.7872 (0.5264 -- 3.2664)  data: 0.0014 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [60]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000012  loss: 1.9302 (1.8224)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8653 (7.1394)  time: 0.8790 (0.5142 -- 4.5618)  data: 0.3104 (0.0003 -- 4.0518)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.8624 (1.8332)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1239 (7.3299)  time: 0.8850 (0.5237 -- 3.6982)  data: 0.3191 (0.0004 -- 3.1539)  max mem: 16413
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.7740 (1.8324)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0884 (7.2676)  time: 0.6364 (0.4970 -- 1.6711)  data: 0.0584 (0.0002 -- 1.1524)  max mem: 16413
Epoch: [60] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.7740 (1.8180)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0884 (7.2676)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2524 (0.2524)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5084 (2.5084 -- 2.5084)  data: 2.2410 (2.2410 -- 2.2410)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6154 (0.8003)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (96.9697)  time: 0.4270 (0.1982 -- 2.5084)  data: 0.2046 (0.0007 -- 2.2410)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5792 (0.7017)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2181 (0.1721 -- 0.4194)  data: 0.0118 (0.0001 -- 0.2235)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5882 (0.7637)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.8506)  time: 0.1995 (0.1330 -- 0.4194)  data: 0.0116 (0.0001 -- 0.2235)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 79.876 Acc@5 95.851 loss 0.739
Accuracy of the network on the 482 val images: 79.88%
Max accuracy: 81.74%
Epoch: [61]  [  0/160]  eta: 0:19:42  lr: 0.000045  min_lr: 0.000012  loss: 2.1213 (2.1213)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5004 (5.5004)  time: 7.3934 (7.3934 -- 7.3934)  data: 6.4515 (6.4515 -- 6.4515)  max mem: 16413
Epoch: [61]  [ 20/160]  eta: 0:02:54  lr: 0.000045  min_lr: 0.000012  loss: 1.8405 (1.7521)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4431 (7.0827)  time: 0.9413 (0.5233 -- 5.2829)  data: 0.0613 (0.0004 -- 1.1957)  max mem: 16413
[2023-09-05 00:05:35,723] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:05:35,723] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:05:35,723] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:05:35,723] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [61]  [ 40/160]  eta: 0:02:14  lr: 0.000045  min_lr: 0.000012  loss: 1.7325 (1.7666)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4844 (6.8116)  time: 0.9804 (0.5356 -- 4.1592)  data: 0.0014 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [61]  [ 60/160]  eta: 0:01:42  lr: 0.000045  min_lr: 0.000012  loss: 1.7385 (1.7748)  loss_scale: 32768.0000 (26053.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9973 (6.9640)  time: 0.8252 (0.5163 -- 4.3371)  data: 0.0015 (0.0004 -- 0.0061)  max mem: 16413
Epoch: [61]  [ 80/160]  eta: 0:01:19  lr: 0.000045  min_lr: 0.000012  loss: 1.6623 (1.7520)  loss_scale: 32768.0000 (27711.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6149 (6.7497)  time: 0.9162 (0.5202 -- 3.7472)  data: 0.0009 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000012  loss: 1.7998 (1.7699)  loss_scale: 32768.0000 (28712.5545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3150 (6.9007)  time: 0.7699 (0.5303 -- 2.9880)  data: 0.0016 (0.0007 -- 0.0041)  max mem: 16413
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000012  loss: 1.7246 (1.7770)  loss_scale: 32768.0000 (29382.8760)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7286 (6.8395)  time: 0.9360 (0.5267 -- 4.0799)  data: 0.0018 (0.0001 -- 0.0055)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000012  loss: 1.7838 (1.7831)  loss_scale: 32768.0000 (29863.0355)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0230 (6.7614)  time: 0.8369 (0.5152 -- 4.3757)  data: 0.0013 (0.0002 -- 0.0037)  max mem: 16413
[2023-09-05 00:07:25,290] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:07:25,291] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 00:07:25,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:07:25,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000012  loss: 1.8995 (1.7993)  loss_scale: 32768.0000 (31641.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3210 (6.7677)  time: 0.6603 (0.4955 -- 2.7354)  data: 0.0009 (0.0002 -- 0.0070)  max mem: 16413
Epoch: [61] Total time: 0:02:24 (0.9011 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000012  loss: 1.8995 (1.8140)  loss_scale: 32768.0000 (31641.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3210 (6.7677)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2831 (0.2831)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4274 (2.4274 -- 2.4274)  data: 2.1906 (2.1906 -- 2.1906)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6985 (0.7529)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4111 (0.1925 -- 2.4274)  data: 0.2002 (0.0005 -- 2.1906)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6399 (0.6912)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2195 (0.1708 -- 0.3767)  data: 0.0139 (0.0001 -- 0.1547)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7600 (0.7567)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2057 (0.1377 -- 0.3767)  data: 0.0135 (0.0001 -- 0.1547)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 82.365 Acc@5 96.473 loss 0.732
Accuracy of the network on the 482 val images: 82.37%
[2023-09-05 00:07:36,119] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 00:07:36,120] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 00:07:36,120] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 00:07:36,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 00:07:37,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 00:07:37,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [62]  [  0/160]  eta: 0:19:27  lr: 0.000045  min_lr: 0.000012  loss: 1.6178 (1.6178)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3699 (3.3699)  time: 7.2977 (7.2977 -- 7.2977)  data: 6.5975 (6.5975 -- 6.5975)  max mem: 16413
[2023-09-05 00:07:48,789] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9927
[2023-09-05 00:07:48,789] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9927
[2023-09-05 00:07:48,790] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 00:07:48,790] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-05 00:07:48,790] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [62]  [ 20/160]  eta: 0:02:38  lr: 0.000045  min_lr: 0.000012  loss: 1.9241 (1.8568)  loss_scale: 32768.0000 (43690.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4904 (6.5151)  time: 0.8231 (0.5264 -- 3.3748)  data: 0.2463 (0.0007 -- 2.5545)  max mem: 16413
Epoch: [62]  [ 40/160]  eta: 0:02:09  lr: 0.000045  min_lr: 0.000012  loss: 1.7144 (1.7942)  loss_scale: 32768.0000 (38362.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8609 (6.7524)  time: 1.0253 (0.5386 -- 4.2665)  data: 0.4720 (0.0003 -- 3.7592)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000011  loss: 1.8589 (1.8230)  loss_scale: 32768.0000 (36528.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5735 (7.1704)  time: 0.8609 (0.5030 -- 3.9674)  data: 0.3215 (0.0002 -- 3.4426)  max mem: 16413
[2023-09-05 00:08:53,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=55, lr=[1.1494664777518603e-05, 1.1494664777518603e-05, 1.277184975279845e-05, 1.277184975279845e-05, 1.419094416977605e-05, 1.419094416977605e-05, 1.5767715744195615e-05, 1.5767715744195615e-05, 1.7519684160217346e-05, 1.7519684160217346e-05, 1.946631573357483e-05, 1.946631573357483e-05, 2.1629239703972033e-05, 2.1629239703972033e-05, 2.4032488559968922e-05, 2.4032488559968922e-05, 2.6702765066632137e-05, 2.6702765066632137e-05, 2.9669738962924594e-05, 2.9669738962924594e-05, 3.296637662547177e-05, 3.296637662547177e-05, 3.662930736163531e-05, 3.662930736163531e-05, 4.0699230401817e-05, 4.0699230401817e-05, 4.522136711313e-05, 4.522136711313e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 00:08:53,382] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=17.51084804284835, CurrSamplesPerSec=21.134005277239847, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000011  loss: 1.7345 (1.8061)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3675 (7.2765)  time: 0.9056 (0.5266 -- 3.9139)  data: 0.2725 (0.0001 -- 1.8755)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000011  loss: 1.7473 (1.8022)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5254 (7.3516)  time: 0.8317 (0.5197 -- 4.4532)  data: 0.0305 (0.0003 -- 0.5836)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.9405 (1.8221)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9208 (7.3489)  time: 0.8677 (0.5166 -- 4.2652)  data: 0.0013 (0.0003 -- 0.0030)  max mem: 16413
[2023-09-05 00:09:41,430] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10052
[2023-09-05 00:09:41,431] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:09:41,431] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10052
[2023-09-05 00:09:41,431] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 00:09:41,431] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8131 (1.8167)  loss_scale: 32768.0000 (33348.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8400 (7.2036)  time: 0.7741 (0.5234 -- 4.0813)  data: 0.0015 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8180 (1.8157)  loss_scale: 16384.0000 (31334.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2431 (7.0940)  time: 0.6823 (0.4969 -- 2.2959)  data: 0.0011 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [62] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8180 (1.7896)  loss_scale: 16384.0000 (31334.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2431 (7.0940)
Val:  [ 0/27]  eta: 0:01:13  loss: 0.2224 (0.2224)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7077 (2.7077 -- 2.7077)  data: 2.4804 (2.4804 -- 2.4804)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5779 (0.7032)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4399 (0.1986 -- 2.7077)  data: 0.2265 (0.0006 -- 2.4804)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5813 (0.6633)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2052 (0.1710 -- 0.2367)  data: 0.0016 (0.0001 -- 0.0162)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7359 (0.7273)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.1897 (0.1324 -- 0.2296)  data: 0.0013 (0.0001 -- 0.0162)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.699
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.37%
Epoch: [63]  [  0/160]  eta: 0:20:16  lr: 0.000045  min_lr: 0.000011  loss: 2.0746 (2.0746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8451 (4.8451)  time: 7.6021 (7.6021 -- 7.6021)  data: 6.2575 (6.2575 -- 6.2575)  max mem: 16413
Epoch: [63]  [ 20/160]  eta: 0:02:56  lr: 0.000045  min_lr: 0.000011  loss: 1.8804 (1.7607)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0553 (7.1251)  time: 0.9464 (0.5291 -- 4.3918)  data: 0.0327 (0.0006 -- 0.4259)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:02:02  lr: 0.000045  min_lr: 0.000011  loss: 1.8373 (1.7958)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3366 (7.1250)  time: 0.7681 (0.5214 -- 3.2673)  data: 0.0021 (0.0001 -- 0.0070)  max mem: 16413
Epoch: [63]  [ 60/160]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000011  loss: 1.7636 (1.7872)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9865 (7.1248)  time: 0.8692 (0.5232 -- 4.2001)  data: 0.0016 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [63]  [ 80/160]  eta: 0:01:14  lr: 0.000045  min_lr: 0.000011  loss: 1.7747 (1.7864)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3160 (7.2844)  time: 0.8114 (0.5256 -- 3.0364)  data: 0.1340 (0.0002 -- 0.8656)  max mem: 16413
Epoch: [63]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 1.8117 (1.7992)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9191 (7.2781)  time: 1.0013 (0.5345 -- 3.5021)  data: 0.3158 (0.0003 -- 2.9775)  max mem: 16413
[2023-09-05 00:11:43,820] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:11:43,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:11:43,821] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:11:43,821] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [63]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.8944 (1.8118)  loss_scale: 32768.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9609 (7.1261)  time: 0.8517 (0.5250 -- 2.9954)  data: 0.1495 (0.0003 -- 2.4545)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8634 (1.8162)  loss_scale: 32768.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1774 (7.1312)  time: 0.9055 (0.5130 -- 2.6128)  data: 0.1116 (0.0004 -- 1.2853)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.7778 (1.8119)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6945 (7.1426)  time: 0.6258 (0.4965 -- 1.5636)  data: 0.0007 (0.0001 -- 0.0019)  max mem: 16413
Epoch: [63] Total time: 0:02:22 (0.8916 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.7778 (1.8201)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6945 (7.1426)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2272 (0.2272)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2422 (2.2422 -- 2.2422)  data: 2.0038 (2.0038 -- 2.0038)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5185 (0.7258)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4163 (0.1957 -- 2.2422)  data: 0.1999 (0.0006 -- 2.0038)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5471 (0.6664)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2272 (0.1705 -- 0.4170)  data: 0.0218 (0.0001 -- 0.1851)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5709 (0.7314)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.6805)  time: 0.2125 (0.1332 -- 0.4170)  data: 0.0215 (0.0001 -- 0.1851)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 80.913 Acc@5 96.680 loss 0.700
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 82.37%
Epoch: [64]  [  0/160]  eta: 0:20:24  lr: 0.000045  min_lr: 0.000011  loss: 1.5840 (1.5840)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2398 (9.2398)  time: 7.6538 (7.6538 -- 7.6538)  data: 7.1104 (7.1104 -- 7.1104)  max mem: 16413
Epoch: [64]  [ 20/160]  eta: 0:02:48  lr: 0.000045  min_lr: 0.000011  loss: 1.8944 (1.8787)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4535 (7.0024)  time: 0.8847 (0.5303 -- 4.0147)  data: 0.3221 (0.0004 -- 3.4886)  max mem: 16413
Epoch: [64]  [ 40/160]  eta: 0:02:03  lr: 0.000045  min_lr: 0.000011  loss: 1.7713 (1.8574)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4693 (6.7626)  time: 0.8474 (0.5204 -- 2.1581)  data: 0.2345 (0.0003 -- 1.6160)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:39  lr: 0.000045  min_lr: 0.000011  loss: 1.7314 (1.7981)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5112 (6.9436)  time: 0.9056 (0.5269 -- 3.1236)  data: 0.3008 (0.0007 -- 2.5720)  max mem: 16413
[2023-09-05 00:13:45,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:13:45,092] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 00:13:45,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:13:45,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 00:13:48,262] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10312
[2023-09-05 00:13:48,262] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10312
[2023-09-05 00:13:48,263] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 00:13:48,263] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 00:13:48,263] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [64]  [ 80/160]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000011  loss: 1.7066 (1.7898)  loss_scale: 32768.0000 (33981.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8830 (7.0716)  time: 0.8056 (0.5180 -- 2.5118)  data: 0.1391 (0.0003 -- 1.5267)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000011  loss: 1.9309 (1.8129)  loss_scale: 32768.0000 (33741.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5074 (7.1602)  time: 0.8834 (0.5218 -- 2.5501)  data: 0.1622 (0.0004 -- 2.0266)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.9168 (1.8326)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3820 (7.1982)  time: 0.8257 (0.5356 -- 3.2548)  data: 0.2134 (0.0002 -- 2.6699)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.7274 (1.8168)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5739 (7.1483)  time: 0.8831 (0.5254 -- 3.1874)  data: 0.2697 (0.0004 -- 2.6519)  max mem: 16413
[2023-09-05 00:14:50,335] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10384
[2023-09-05 00:14:50,335] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10384
[2023-09-05 00:14:50,335] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:14:50,335] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:14:50,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8694 (1.8132)  loss_scale: 16384.0000 (31744.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5570 (7.1641)  time: 0.6739 (0.4978 -- 2.0844)  data: 0.0751 (0.0002 -- 1.3488)  max mem: 16413
Epoch: [64] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8694 (1.8119)  loss_scale: 16384.0000 (31744.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5570 (7.1641)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3519 (0.3519)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4149 (2.4149 -- 2.4149)  data: 2.1873 (2.1873 -- 2.1873)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5271 (0.7109)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4239 (0.2069 -- 2.4149)  data: 0.2056 (0.0004 -- 2.1873)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5916 (0.6469)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2197 (0.1695 -- 0.3661)  data: 0.0127 (0.0001 -- 0.1756)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6164 (0.6908)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (95.8506)  time: 0.2030 (0.1326 -- 0.3661)  data: 0.0116 (0.0001 -- 0.1756)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 82.365 Acc@5 96.680 loss 0.698
Accuracy of the network on the 482 val images: 82.37%
[2023-09-05 00:15:07,304] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 00:15:07,305] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 00:15:07,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 00:15:07,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 00:15:08,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 00:15:08,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [65]  [  0/160]  eta: 0:17:34  lr: 0.000045  min_lr: 0.000011  loss: 1.5692 (1.5692)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1831 (6.1831)  time: 6.5898 (6.5898 -- 6.5898)  data: 4.8478 (4.8478 -- 4.8478)  max mem: 16413
Epoch: [65]  [ 20/160]  eta: 0:02:47  lr: 0.000045  min_lr: 0.000011  loss: 1.6681 (1.7555)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0613 (7.1320)  time: 0.9277 (0.5326 -- 3.7727)  data: 0.0312 (0.0006 -- 0.5358)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:07  lr: 0.000045  min_lr: 0.000011  loss: 1.4546 (1.6504)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8463 (6.6637)  time: 0.9265 (0.5210 -- 4.1329)  data: 0.1571 (0.0003 -- 2.0624)  max mem: 16413
Epoch: [65]  [ 60/160]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000011  loss: 1.7538 (1.6635)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8607 (6.6364)  time: 0.8773 (0.5203 -- 3.5636)  data: 0.0012 (0.0003 -- 0.0022)  max mem: 16413
Epoch: [65]  [ 80/160]  eta: 0:01:14  lr: 0.000045  min_lr: 0.000011  loss: 1.9156 (1.6975)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1451 (6.6021)  time: 0.7238 (0.5338 -- 2.0618)  data: 0.0717 (0.0002 -- 1.1690)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 1.7163 (1.7007)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2911 (6.6640)  time: 0.9983 (0.5258 -- 4.0387)  data: 0.1338 (0.0007 -- 1.9467)  max mem: 16413
[2023-09-05 00:16:52,995] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:16:52,995] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:16:52,999] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:16:52,999] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [65]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.8360 (1.7243)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6635 (6.6469)  time: 0.8196 (0.5306 -- 2.7827)  data: 0.0022 (0.0005 -- 0.0137)  max mem: 16413
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.6804 (1.7190)  loss_scale: 32768.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8757 (6.7231)  time: 0.8368 (0.5351 -- 3.2726)  data: 0.0025 (0.0003 -- 0.0211)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8659 (1.7332)  loss_scale: 32768.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9445 (6.6632)  time: 0.7609 (0.4951 -- 3.7511)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [65] Total time: 0:02:23 (0.8969 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8659 (1.7741)  loss_scale: 32768.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9445 (6.6632)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2639 (0.2639)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4714 (2.4714 -- 2.4714)  data: 2.2245 (2.2245 -- 2.2245)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4845 (0.6872)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4447 (0.2059 -- 2.4714)  data: 0.2255 (0.0005 -- 2.2245)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4845 (0.6112)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2216 (0.1719 -- 0.4400)  data: 0.0130 (0.0001 -- 0.2250)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5242 (0.6772)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (95.8506)  time: 0.2060 (0.1333 -- 0.4400)  data: 0.0127 (0.0001 -- 0.2250)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 83.402 Acc@5 96.473 loss 0.662
Accuracy of the network on the 482 val images: 83.40%
[2023-09-05 00:17:39,887] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 00:17:39,889] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 00:17:39,889] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 00:17:39,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 00:17:41,442] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 00:17:41,442] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.40%
Epoch: [66]  [  0/160]  eta: 0:16:50  lr: 0.000045  min_lr: 0.000011  loss: 1.0056 (1.0056)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0619 (7.0619)  time: 6.3149 (6.3149 -- 6.3149)  data: 5.2460 (5.2460 -- 5.2460)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:43  lr: 0.000045  min_lr: 0.000011  loss: 1.8416 (1.7752)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7954 (6.2780)  time: 0.9090 (0.5316 -- 2.8704)  data: 0.1820 (0.0004 -- 2.3367)  max mem: 16413
Epoch: [66]  [ 40/160]  eta: 0:01:59  lr: 0.000045  min_lr: 0.000011  loss: 1.7419 (1.7640)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4262 (6.4643)  time: 0.8180 (0.5137 -- 2.8131)  data: 0.0177 (0.0004 -- 0.3156)  max mem: 16413
Epoch: [66]  [ 60/160]  eta: 0:01:37  lr: 0.000045  min_lr: 0.000011  loss: 1.6160 (1.7467)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3538 (6.5637)  time: 0.9309 (0.5240 -- 3.9550)  data: 0.1601 (0.0005 -- 2.2236)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:14  lr: 0.000045  min_lr: 0.000011  loss: 1.9298 (1.7931)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4948 (6.6080)  time: 0.8088 (0.5399 -- 2.8065)  data: 0.2024 (0.0008 -- 2.2498)  max mem: 16413
[2023-09-05 00:19:00,470] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:19:00,471] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 00:19:00,475] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:19:00,476] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 00:19:01,626] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10643
[2023-09-05 00:19:01,626] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 00:19:01,626] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10643
[2023-09-05 00:19:01,626] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 00:19:01,626] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [66]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000011  loss: 1.7250 (1.7866)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8718 (6.4690)  time: 0.9319 (0.5249 -- 3.7481)  data: 0.3728 (0.0005 -- 3.1872)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.9209 (1.8043)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6431 (6.5340)  time: 0.9081 (0.5183 -- 4.5462)  data: 0.3632 (0.0003 -- 3.9831)  max mem: 16413
[2023-09-05 00:19:46,742] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10693
[2023-09-05 00:19:46,742] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10693
[2023-09-05 00:19:46,742] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:19:46,742] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:19:46,742] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8059 (1.7967)  loss_scale: 32768.0000 (32303.2057)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9764 (6.6055)  time: 1.0113 (0.5062 -- 4.2705)  data: 0.4751 (0.0004 -- 3.7665)  max mem: 16413
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.6769 (1.7805)  loss_scale: 16384.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0356 (6.6547)  time: 0.5545 (0.4938 -- 1.2219)  data: 0.0344 (0.0001 -- 0.6772)  max mem: 16413
Epoch: [66] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.6769 (1.8060)  loss_scale: 16384.0000 (30412.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0356 (6.6547)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2046 (0.2046)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4515 (2.4515 -- 2.4515)  data: 2.1972 (2.1972 -- 2.1972)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5678 (0.6973)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4403 (0.2023 -- 2.4515)  data: 0.2196 (0.0006 -- 2.1972)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5553 (0.6426)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2211 (0.1689 -- 0.4361)  data: 0.0171 (0.0001 -- 0.2075)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5678 (0.6943)  acc1: 85.7143 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2045 (0.1356 -- 0.4361)  data: 0.0167 (0.0001 -- 0.2075)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 81.743 Acc@5 96.473 loss 0.670
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 83.40%
Epoch: [67]  [  0/160]  eta: 0:18:57  lr: 0.000045  min_lr: 0.000011  loss: 1.8373 (1.8373)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1740 (5.1740)  time: 7.1118 (7.1118 -- 7.1118)  data: 5.6403 (5.6403 -- 5.6403)  max mem: 16413
Epoch: [67]  [ 20/160]  eta: 0:02:44  lr: 0.000045  min_lr: 0.000011  loss: 1.6925 (1.8098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7017 (6.8385)  time: 0.8750 (0.5290 -- 3.3607)  data: 0.2960 (0.0003 -- 2.7955)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:02:08  lr: 0.000045  min_lr: 0.000011  loss: 1.7283 (1.7718)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0621 (7.2442)  time: 0.9716 (0.5151 -- 3.9419)  data: 0.3875 (0.0003 -- 2.8158)  max mem: 16413
Epoch: [67]  [ 60/160]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000011  loss: 1.8700 (1.7750)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5061 (7.1108)  time: 0.7481 (0.5319 -- 3.1407)  data: 0.1933 (0.0003 -- 2.5880)  max mem: 16413
Epoch: [67]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000011  loss: 1.7912 (1.7763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1370 (7.2269)  time: 1.0294 (0.5053 -- 4.9714)  data: 0.4904 (0.0003 -- 4.4686)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000011  loss: 1.8587 (1.7878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5860 (7.3773)  time: 0.8198 (0.5098 -- 3.5672)  data: 0.2751 (0.0003 -- 3.0556)  max mem: 16413
[2023-09-05 00:21:49,777] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:21:49,777] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:21:49,777] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:21:49,777] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [67]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000011  loss: 1.6448 (1.7689)  loss_scale: 32768.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3983 (7.2932)  time: 0.8989 (0.5250 -- 3.3514)  data: 0.3473 (0.0003 -- 2.7932)  max mem: 16413
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.8493 (1.7701)  loss_scale: 32768.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5895 (7.2545)  time: 0.7590 (0.5163 -- 3.7968)  data: 0.1481 (0.0003 -- 2.5990)  max mem: 16413
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8852 (1.7762)  loss_scale: 32768.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8972 (7.1462)  time: 0.7194 (0.4944 -- 4.5726)  data: 0.0404 (0.0002 -- 0.7932)  max mem: 16413
Epoch: [67] Total time: 0:02:22 (0.8936 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8852 (1.7800)  loss_scale: 32768.0000 (22323.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8972 (7.1462)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2179 (0.2179)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2764 (2.2764 -- 2.2764)  data: 2.0677 (2.0677 -- 2.0677)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4230 (0.6720)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4068 (0.1925 -- 2.2764)  data: 0.1930 (0.0005 -- 2.0677)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5911 (0.6538)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2231 (0.1696 -- 0.4481)  data: 0.0174 (0.0001 -- 0.2420)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6010 (0.7011)  acc1: 85.7143 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2057 (0.1330 -- 0.4481)  data: 0.0171 (0.0001 -- 0.2420)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 81.743 Acc@5 96.680 loss 0.669
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 83.40%
Epoch: [68]  [  0/160]  eta: 0:22:11  lr: 0.000045  min_lr: 0.000011  loss: 2.0642 (2.0642)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1635 (6.1635)  time: 8.3214 (8.3214 -- 8.3214)  data: 7.8010 (7.8010 -- 7.8010)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:50  lr: 0.000045  min_lr: 0.000011  loss: 1.6038 (1.6682)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9492 (6.9838)  time: 0.8592 (0.5139 -- 4.2021)  data: 0.2705 (0.0004 -- 3.1749)  max mem: 16413
Epoch: [68]  [ 40/160]  eta: 0:02:14  lr: 0.000045  min_lr: 0.000011  loss: 1.7310 (1.6831)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3281 (7.2081)  time: 1.0218 (0.5214 -- 4.4763)  data: 0.0304 (0.0001 -- 0.5847)  max mem: 16413
Epoch: [68]  [ 60/160]  eta: 0:01:41  lr: 0.000045  min_lr: 0.000011  loss: 1.6897 (1.6936)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3581 (7.1014)  time: 0.7940 (0.5105 -- 4.2041)  data: 0.0014 (0.0003 -- 0.0041)  max mem: 16413
[2023-09-05 00:23:53,546] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:23:53,546] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 00:23:53,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:23:53,547] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 00:23:57,018] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10952
[2023-09-05 00:23:57,018] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10952
[2023-09-05 00:23:57,020] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 00:23:57,020] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 00:23:57,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [68]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000011  loss: 1.9491 (1.7506)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7825 (7.1398)  time: 0.8657 (0.5284 -- 3.3006)  data: 0.0018 (0.0003 -- 0.0064)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000011  loss: 1.8582 (1.7604)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7793 (7.0825)  time: 0.8497 (0.5311 -- 3.6720)  data: 0.0024 (0.0002 -- 0.0127)  max mem: 16413
[2023-09-05 00:24:34,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=61, lr=[1.1320419902050174e-05, 1.1320419902050174e-05, 1.2578244335611306e-05, 1.2578244335611306e-05, 1.3975827039568116e-05, 1.3975827039568116e-05, 1.5528696710631242e-05, 1.5528696710631242e-05, 1.7254107456256934e-05, 1.7254107456256934e-05, 1.917123050695215e-05, 1.917123050695215e-05, 2.130136722994683e-05, 2.130136722994683e-05, 2.366818581105203e-05, 2.366818581105203e-05, 2.6297984234502258e-05, 2.6297984234502258e-05, 2.9219982482780285e-05, 2.9219982482780285e-05, 3.246664720308921e-05, 3.246664720308921e-05, 3.607405244787689e-05, 3.607405244787689e-05, 4.008228049764099e-05, 4.008228049764099e-05, 4.45358672196011e-05, 4.45358672196011e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 00:24:34,447] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=17.486479307108233, CurrSamplesPerSec=21.320681697262163, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000011  loss: 1.7812 (1.7680)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6992 (7.0618)  time: 0.7746 (0.5310 -- 2.6324)  data: 0.0022 (0.0001 -- 0.0172)  max mem: 16413
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000011  loss: 1.9802 (1.8031)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2833 (7.1162)  time: 0.9628 (0.5332 -- 3.7346)  data: 0.0092 (0.0007 -- 0.1478)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000011  loss: 1.8082 (1.8009)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3821 (7.0467)  time: 0.6386 (0.4966 -- 1.5262)  data: 0.0024 (0.0002 -- 0.0297)  max mem: 16413
Epoch: [68] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000011  loss: 1.8082 (1.7939)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3821 (7.0467)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2725 (0.2725)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4653 (2.4653 -- 2.4653)  data: 2.2496 (2.2496 -- 2.2496)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4214 (0.6681)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (100.0000)  time: 0.4222 (0.1877 -- 2.4653)  data: 0.2056 (0.0009 -- 2.2496)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5513 (0.6453)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2196 (0.1693 -- 0.5299)  data: 0.0181 (0.0001 -- 0.3464)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5815 (0.7010)  acc1: 85.7143 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2035 (0.1324 -- 0.5299)  data: 0.0178 (0.0001 -- 0.3464)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 83.195 Acc@5 97.303 loss 0.697
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 83.40%
Epoch: [69]  [  0/160]  eta: 0:19:30  lr: 0.000045  min_lr: 0.000011  loss: 2.4114 (2.4114)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0248 (6.0248)  time: 7.3173 (7.3173 -- 7.3173)  data: 5.3323 (5.3323 -- 5.3323)  max mem: 16413
Epoch: [69]  [ 20/160]  eta: 0:03:01  lr: 0.000044  min_lr: 0.000011  loss: 1.7936 (1.8569)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7715 (6.7983)  time: 0.9987 (0.5305 -- 4.9854)  data: 0.3774 (0.0004 -- 4.4398)  max mem: 16413
[2023-09-05 00:25:54,132] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11073
[2023-09-05 00:25:54,132] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11073
[2023-09-05 00:25:54,174] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:25:54,174] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:25:54,174] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [69]  [ 40/160]  eta: 0:02:10  lr: 0.000044  min_lr: 0.000011  loss: 1.7355 (1.8286)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3698 (7.0283)  time: 0.8610 (0.5208 -- 3.3233)  data: 0.2015 (0.0001 -- 2.7497)  max mem: 16413
Epoch: [69]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000011  loss: 1.9518 (1.8746)  loss_scale: 16384.0000 (25247.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2411 (7.0689)  time: 0.7654 (0.5375 -- 2.8564)  data: 0.0886 (0.0004 -- 1.2671)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:19  lr: 0.000044  min_lr: 0.000011  loss: 1.8792 (1.8627)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3971 (7.1669)  time: 1.0256 (0.5397 -- 4.9330)  data: 0.0745 (0.0003 -- 1.4343)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 1.9332 (1.8493)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8398 (7.1696)  time: 0.7627 (0.5161 -- 3.4741)  data: 0.0014 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [69]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.8560 (1.8550)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2367 (7.0802)  time: 0.9311 (0.5280 -- 3.6076)  data: 0.0016 (0.0003 -- 0.0064)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.8169 (1.8493)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1940 (7.0325)  time: 0.8615 (0.5329 -- 3.9805)  data: 0.0013 (0.0005 -- 0.0026)  max mem: 16413
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6693 (1.8289)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8362 (7.0417)  time: 0.5953 (0.4937 -- 1.3667)  data: 0.0010 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [69] Total time: 0:02:22 (0.8927 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6693 (1.8001)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8362 (7.0417)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1898 (0.1898)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2934 (2.2934 -- 2.2934)  data: 2.0462 (2.0462 -- 2.0462)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4601 (0.6921)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4128 (0.1925 -- 2.2934)  data: 0.1942 (0.0009 -- 2.0462)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5078 (0.6146)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2192 (0.1699 -- 0.3038)  data: 0.0082 (0.0001 -- 0.0697)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5219 (0.6749)  acc1: 85.7143 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.2032 (0.1329 -- 0.3038)  data: 0.0072 (0.0001 -- 0.0697)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 83.817 Acc@5 97.718 loss 0.666
Accuracy of the network on the 482 val images: 83.82%
[2023-09-05 00:27:44,910] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 00:27:44,911] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 00:27:44,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 00:27:44,911] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 00:27:46,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 00:27:46,213] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.82%
Epoch: [70]  [  0/160]  eta: 0:18:43  lr: 0.000044  min_lr: 0.000011  loss: 1.7099 (1.7099)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4805 (5.4805)  time: 7.0194 (7.0194 -- 7.0194)  data: 6.4557 (6.4557 -- 6.4557)  max mem: 16413
[2023-09-05 00:27:55,538] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:27:55,538] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:27:55,538] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:27:55,538] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [70]  [ 20/160]  eta: 0:02:49  lr: 0.000044  min_lr: 0.000011  loss: 1.7417 (1.7728)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3492 (6.6409)  time: 0.9187 (0.5315 -- 3.4664)  data: 0.2723 (0.0004 -- 2.8971)  max mem: 16413
[2023-09-05 00:28:19,066] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11230
[2023-09-05 00:28:19,066] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11230
[2023-09-05 00:28:19,066] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:28:19,066] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:28:19,066] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [70]  [ 40/160]  eta: 0:01:59  lr: 0.000044  min_lr: 0.000011  loss: 1.6353 (1.7307)  loss_scale: 16384.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2065 (7.1249)  time: 0.7642 (0.5275 -- 2.7322)  data: 0.2021 (0.0003 -- 2.1625)  max mem: 16413
[2023-09-05 00:28:44,366] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11258
[2023-09-05 00:28:44,366] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11258
[2023-09-05 00:28:44,367] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 00:28:44,367] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 00:28:44,367] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [70]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000011  loss: 1.6127 (1.7145)  loss_scale: 16384.0000 (23501.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1946 (7.1204)  time: 0.9775 (0.5277 -- 2.8538)  data: 0.3009 (0.0006 -- 2.2704)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:14  lr: 0.000044  min_lr: 0.000011  loss: 1.8926 (1.7656)  loss_scale: 8192.0000 (19721.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1742 (7.3428)  time: 0.7832 (0.5285 -- 2.1315)  data: 0.0451 (0.0004 -- 0.4305)  max mem: 16413
Epoch: [70]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 2.0024 (1.8030)  loss_scale: 8192.0000 (17438.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8465 (7.1909)  time: 0.9743 (0.5298 -- 3.4407)  data: 0.0834 (0.0003 -- 1.2988)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.7854 (1.8022)  loss_scale: 8192.0000 (15910.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9455 (7.1753)  time: 0.8786 (0.5261 -- 3.7962)  data: 0.2972 (0.0002 -- 3.2315)  max mem: 16413
Epoch: [70]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.8802 (1.8080)  loss_scale: 8192.0000 (14815.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9787 (7.1719)  time: 0.8411 (0.5213 -- 4.2081)  data: 0.2964 (0.0001 -- 3.6892)  max mem: 16413
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8661 (1.8138)  loss_scale: 8192.0000 (14028.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0464 (7.2063)  time: 0.6688 (0.4955 -- 2.0961)  data: 0.1449 (0.0002 -- 1.6030)  max mem: 16413
Epoch: [70] Total time: 0:02:22 (0.8912 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8661 (1.7861)  loss_scale: 8192.0000 (14028.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0464 (7.2063)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2132 (0.2132)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2846 (2.2846 -- 2.2846)  data: 2.0628 (2.0628 -- 2.0628)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5422 (0.7381)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (100.0000)  time: 0.4077 (0.2002 -- 2.2846)  data: 0.1952 (0.0006 -- 2.0628)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5532 (0.6785)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.8836)  time: 0.2279 (0.1685 -- 0.5777)  data: 0.0239 (0.0001 -- 0.3898)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5769 (0.7299)  acc1: 85.7143 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.2098 (0.1329 -- 0.5777)  data: 0.0215 (0.0001 -- 0.3898)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 81.535 Acc@5 97.303 loss 0.706
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 83.82%
Epoch: [71]  [  0/160]  eta: 0:19:57  lr: 0.000044  min_lr: 0.000011  loss: 1.3245 (1.3245)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1880 (8.1880)  time: 7.4833 (7.4833 -- 7.4833)  data: 5.7991 (5.7991 -- 5.7991)  max mem: 16413
Epoch: [71]  [ 20/160]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000011  loss: 1.7805 (1.7241)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4533 (7.6376)  time: 0.8195 (0.5376 -- 3.0650)  data: 0.1895 (0.0004 -- 1.4362)  max mem: 16413
[2023-09-05 00:30:49,065] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:30:49,065] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 00:30:49,068] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:30:49,069] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [71]  [ 40/160]  eta: 0:02:05  lr: 0.000044  min_lr: 0.000011  loss: 1.6638 (1.7069)  loss_scale: 16384.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9200 (7.2080)  time: 0.9559 (0.5206 -- 5.2178)  data: 0.0547 (0.0003 -- 1.0721)  max mem: 16413
Epoch: [71]  [ 60/160]  eta: 0:01:40  lr: 0.000044  min_lr: 0.000011  loss: 1.9085 (1.7335)  loss_scale: 16384.0000 (12758.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9149 (7.2715)  time: 0.9243 (0.5202 -- 4.5494)  data: 0.0014 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.4983 (1.7026)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9975 (7.0291)  time: 0.8010 (0.5299 -- 3.1458)  data: 0.2363 (0.0001 -- 2.5864)  max mem: 16413
Epoch: [71]  [100/160]  eta: 0:00:55  lr: 0.000044  min_lr: 0.000011  loss: 1.8498 (1.7116)  loss_scale: 16384.0000 (14194.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6601 (6.9810)  time: 0.8196 (0.5326 -- 2.7945)  data: 0.2618 (0.0003 -- 2.2589)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.8805 (1.7353)  loss_scale: 16384.0000 (14556.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6658 (6.9743)  time: 1.0234 (0.5319 -- 3.1821)  data: 0.4159 (0.0005 -- 2.6495)  max mem: 16413
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 2.0511 (1.7635)  loss_scale: 16384.0000 (14815.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3636 (6.9391)  time: 0.7662 (0.5198 -- 4.1886)  data: 0.2162 (0.0003 -- 3.6792)  max mem: 16413
[2023-09-05 00:32:37,292] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:32:37,292] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:32:37,292] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:32:37,292] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.8779 (1.7788)  loss_scale: 16384.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4608 (6.9103)  time: 0.6741 (0.4961 -- 2.9388)  data: 0.1482 (0.0002 -- 2.4031)  max mem: 16413
Epoch: [71] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.8779 (1.7547)  loss_scale: 16384.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4608 (6.9103)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2939 (0.2939)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5009 (2.5009 -- 2.5009)  data: 2.2652 (2.2652 -- 2.2652)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5855 (0.7187)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (100.0000)  time: 0.4217 (0.2001 -- 2.5009)  data: 0.2069 (0.0006 -- 2.2652)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5365 (0.6451)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (97.8836)  time: 0.2132 (0.1702 -- 0.3130)  data: 0.0069 (0.0001 -- 0.1235)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5365 (0.6932)  acc1: 88.8889 (80.4979)  acc5: 100.0000 (97.5104)  time: 0.1991 (0.1328 -- 0.3130)  data: 0.0066 (0.0001 -- 0.1235)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 81.535 Acc@5 97.718 loss 0.691
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 83.82%
Epoch: [72]  [  0/160]  eta: 0:18:21  lr: 0.000044  min_lr: 0.000011  loss: 1.3458 (1.3458)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4102 (4.4102)  time: 6.8854 (6.8854 -- 6.8854)  data: 5.1181 (5.1181 -- 5.1181)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:02:43  lr: 0.000044  min_lr: 0.000011  loss: 1.6250 (1.7047)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9623 (6.5783)  time: 0.8831 (0.5402 -- 2.9823)  data: 0.3227 (0.0004 -- 2.4411)  max mem: 16413
[2023-09-05 00:33:26,629] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11559
[2023-09-05 00:33:26,629] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11559
[2023-09-05 00:33:26,629] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:33:26,629] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:33:26,629] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [72]  [ 40/160]  eta: 0:01:57  lr: 0.000044  min_lr: 0.000011  loss: 1.8178 (1.7315)  loss_scale: 32768.0000 (31968.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2233 (6.8237)  time: 0.7791 (0.5319 -- 2.4978)  data: 0.2313 (0.0002 -- 1.9602)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000011  loss: 1.6795 (1.7068)  loss_scale: 16384.0000 (26859.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6774 (7.0210)  time: 1.0020 (0.5157 -- 2.8875)  data: 0.3598 (0.0002 -- 2.3124)  max mem: 16413
Epoch: [72]  [ 80/160]  eta: 0:01:17  lr: 0.000044  min_lr: 0.000011  loss: 1.6854 (1.7219)  loss_scale: 16384.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2297 (7.0995)  time: 0.9244 (0.5331 -- 3.1530)  data: 0.0441 (0.0001 -- 0.7446)  max mem: 16413
Epoch: [72]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 1.6284 (1.7225)  loss_scale: 16384.0000 (22710.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3543 (7.0089)  time: 0.8388 (0.5235 -- 3.6037)  data: 0.2955 (0.0004 -- 3.0572)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.7492 (1.7157)  loss_scale: 16384.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5167 (6.9977)  time: 0.8510 (0.5307 -- 3.8111)  data: 0.2938 (0.0004 -- 3.2679)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.8511 (1.7276)  loss_scale: 16384.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0162 (7.0472)  time: 0.9459 (0.5319 -- 4.0125)  data: 0.3816 (0.0006 -- 3.4897)  max mem: 16413
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.7025 (1.7270)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6491 (7.0669)  time: 0.6241 (0.4941 -- 1.8683)  data: 0.1053 (0.0002 -- 1.3730)  max mem: 16413
Epoch: [72] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.7025 (1.7461)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6491 (7.0669)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2049 (0.2049)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3498 (2.3498 -- 2.3498)  data: 2.1150 (2.1150 -- 2.1150)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4954 (0.6824)  acc1: 88.8889 (75.7576)  acc5: 100.0000 (97.9798)  time: 0.4221 (0.1958 -- 2.3498)  data: 0.2101 (0.0007 -- 2.1150)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5286 (0.6248)  acc1: 88.8889 (79.3651)  acc5: 100.0000 (96.8254)  time: 0.2232 (0.1696 -- 0.4066)  data: 0.0208 (0.0001 -- 0.2163)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5521 (0.6813)  acc1: 85.7143 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2090 (0.1323 -- 0.4066)  data: 0.0200 (0.0001 -- 0.2163)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 79.668 Acc@5 96.680 loss 0.691
Accuracy of the network on the 482 val images: 79.67%
Max accuracy: 83.82%
Epoch: [73]  [  0/160]  eta: 0:21:03  lr: 0.000044  min_lr: 0.000011  loss: 2.2395 (2.2395)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7401 (7.7401)  time: 7.8961 (7.8961 -- 7.8961)  data: 6.5980 (6.5980 -- 6.5980)  max mem: 16413
[2023-09-05 00:35:33,450] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:35:33,450] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:35:33,450] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:35:33,451] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [73]  [ 20/160]  eta: 0:02:44  lr: 0.000044  min_lr: 0.000011  loss: 1.7290 (1.7920)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4412 (6.6461)  time: 0.8410 (0.5193 -- 3.4156)  data: 0.0408 (0.0003 -- 0.7704)  max mem: 16413
[2023-09-05 00:35:43,467] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11701
[2023-09-05 00:35:43,467] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11701
[2023-09-05 00:35:43,468] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:35:43,468] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:35:43,468] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [73]  [ 40/160]  eta: 0:02:11  lr: 0.000044  min_lr: 0.000011  loss: 1.6036 (1.7140)  loss_scale: 16384.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0410 (6.6246)  time: 1.0065 (0.5245 -- 4.3553)  data: 0.0020 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:41  lr: 0.000044  min_lr: 0.000011  loss: 1.7423 (1.7388)  loss_scale: 16384.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9003 (6.7377)  time: 0.8666 (0.5079 -- 4.7525)  data: 0.0024 (0.0003 -- 0.0152)  max mem: 16413
Epoch: [73]  [ 80/160]  eta: 0:01:18  lr: 0.000044  min_lr: 0.000011  loss: 1.6773 (1.7324)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7006 (6.8240)  time: 0.8553 (0.5131 -- 3.5734)  data: 0.0011 (0.0001 -- 0.0034)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000011  loss: 1.8183 (1.7382)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9062 (6.7395)  time: 0.7828 (0.5187 -- 2.9206)  data: 0.0015 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000011  loss: 1.6518 (1.7293)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2337 (6.7236)  time: 0.9273 (0.5348 -- 3.7337)  data: 0.0025 (0.0004 -- 0.0164)  max mem: 16413
[2023-09-05 00:37:20,569] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11809
[2023-09-05 00:37:20,569] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11809
[2023-09-05 00:37:20,569] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 00:37:20,569] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 00:37:20,569] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.6698 (1.7321)  loss_scale: 8192.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8714 (6.8311)  time: 0.8892 (0.5030 -- 3.5374)  data: 0.0011 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6940 (1.7315)  loss_scale: 8192.0000 (16128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8612 (6.8042)  time: 0.6647 (0.4946 -- 3.4239)  data: 0.0007 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [73] Total time: 0:02:24 (0.9004 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6940 (1.7658)  loss_scale: 8192.0000 (16128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8612 (6.8042)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1978 (0.1978)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3620 (2.3620 -- 2.3620)  data: 2.1107 (2.1107 -- 2.1107)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5716 (0.6857)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4197 (0.2086 -- 2.3620)  data: 0.2009 (0.0008 -- 2.1107)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5312 (0.6346)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (97.3545)  time: 0.2210 (0.1693 -- 0.3780)  data: 0.0142 (0.0001 -- 0.1815)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5863 (0.6877)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (97.5104)  time: 0.2046 (0.1331 -- 0.3780)  data: 0.0138 (0.0001 -- 0.1815)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 82.780 Acc@5 97.510 loss 0.656
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 83.82%
Epoch: [74]  [  0/160]  eta: 0:20:31  lr: 0.000044  min_lr: 0.000011  loss: 1.6106 (1.6106)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3026 (5.3026)  time: 7.6946 (7.6946 -- 7.6946)  data: 7.1384 (7.1384 -- 7.1384)  max mem: 16413
Epoch: [74]  [ 20/160]  eta: 0:02:42  lr: 0.000044  min_lr: 0.000011  loss: 1.6742 (1.7060)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6445 (6.7445)  time: 0.8347 (0.5316 -- 2.0494)  data: 0.2756 (0.0004 -- 1.5198)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6528 (1.6818)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6031 (6.8630)  time: 0.8382 (0.5385 -- 2.4315)  data: 0.2482 (0.0004 -- 1.8471)  max mem: 16413
Epoch: [74]  [ 60/160]  eta: 0:01:37  lr: 0.000044  min_lr: 0.000011  loss: 1.7975 (1.7238)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6929 (6.9057)  time: 0.9303 (0.5282 -- 4.4926)  data: 0.1426 (0.0003 -- 2.8272)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000011  loss: 1.8649 (1.7451)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0314 (6.9147)  time: 0.8796 (0.5230 -- 2.8440)  data: 0.0719 (0.0005 -- 1.4088)  max mem: 16413
[2023-09-05 00:39:22,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:39:22,194] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 00:39:22,196] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:39:22,197] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [74]  [100/160]  eta: 0:00:55  lr: 0.000044  min_lr: 0.000011  loss: 1.8102 (1.7544)  loss_scale: 8192.0000 (8435.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5657 (7.0652)  time: 0.7905 (0.5223 -- 2.7993)  data: 0.0392 (0.0005 -- 0.5862)  max mem: 16413
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000011  loss: 1.9515 (1.7647)  loss_scale: 16384.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5121 (7.1307)  time: 0.9187 (0.5341 -- 2.7824)  data: 0.0930 (0.0002 -- 1.2670)  max mem: 16413
Epoch: [74]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.7541 (1.7734)  loss_scale: 16384.0000 (10690.2695)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7574 (7.0718)  time: 0.8052 (0.5230 -- 4.0490)  data: 0.1973 (0.0006 -- 3.5134)  max mem: 16413
[2023-09-05 00:40:12,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=67, lr=[1.1117857220463774e-05, 1.1117857220463774e-05, 1.2353174689404193e-05, 1.2353174689404193e-05, 1.3725749654893546e-05, 1.3725749654893546e-05, 1.525083294988172e-05, 1.525083294988172e-05, 1.694536994431302e-05, 1.694536994431302e-05, 1.8828188827014466e-05, 1.8828188827014466e-05, 2.092020980779385e-05, 2.092020980779385e-05, 2.3244677564215387e-05, 2.3244677564215387e-05, 2.5827419515794877e-05, 2.5827419515794877e-05, 2.8697132795327638e-05, 2.8697132795327638e-05, 3.18857031059196e-05, 3.18857031059196e-05, 3.542855900657733e-05, 3.542855900657733e-05, 3.93650655628637e-05, 3.93650655628637e-05, 4.373896173651522e-05, 4.373896173651522e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 00:40:12,014] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=17.469605700512112, CurrSamplesPerSec=24.79749441668182, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6783 (1.7675)  loss_scale: 16384.0000 (11366.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6914 (7.1168)  time: 0.7352 (0.4957 -- 4.0713)  data: 0.2142 (0.0002 -- 3.5359)  max mem: 16413
Epoch: [74] Total time: 0:02:21 (0.8865 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6783 (1.7793)  loss_scale: 16384.0000 (11366.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6914 (7.1168)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1833 (0.1833)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4829 (2.4829 -- 2.4829)  data: 2.2628 (2.2628 -- 2.2628)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5543 (0.6998)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4214 (0.1878 -- 2.4829)  data: 0.2068 (0.0006 -- 2.2628)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5284 (0.6258)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2130 (0.1687 -- 0.2956)  data: 0.0084 (0.0001 -- 0.0977)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5284 (0.6934)  acc1: 88.8889 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.1972 (0.1324 -- 0.2956)  data: 0.0081 (0.0001 -- 0.0977)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 82.573 Acc@5 96.473 loss 0.664
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.82%
Epoch: [75]  [  0/160]  eta: 0:20:21  lr: 0.000044  min_lr: 0.000011  loss: 1.8060 (1.8060)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3523 (5.3523)  time: 7.6313 (7.6313 -- 7.6313)  data: 5.4545 (5.4545 -- 5.4545)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:03:02  lr: 0.000044  min_lr: 0.000011  loss: 1.8140 (1.7126)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4491 (7.5023)  time: 0.9893 (0.5100 -- 5.5837)  data: 0.1830 (0.0005 -- 2.5619)  max mem: 16413
Epoch: [75]  [ 40/160]  eta: 0:02:07  lr: 0.000044  min_lr: 0.000011  loss: 1.9057 (1.7834)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1475 (7.1400)  time: 0.8020 (0.5248 -- 4.0547)  data: 0.0678 (0.0002 -- 1.2711)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000011  loss: 1.8297 (1.7705)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3649 (7.0757)  time: 0.8731 (0.5144 -- 2.2380)  data: 0.0013 (0.0005 -- 0.0031)  max mem: 16413
[2023-09-05 00:41:26,638] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:41:26,638] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:41:26,639] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:41:26,639] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [75]  [ 80/160]  eta: 0:01:19  lr: 0.000044  min_lr: 0.000011  loss: 1.7091 (1.7626)  loss_scale: 32768.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7593 (6.9025)  time: 0.9782 (0.5122 -- 4.5050)  data: 0.0016 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [75]  [100/160]  eta: 0:00:58  lr: 0.000044  min_lr: 0.000011  loss: 1.6271 (1.7282)  loss_scale: 32768.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8429 (6.7586)  time: 0.8734 (0.5139 -- 5.2355)  data: 0.0016 (0.0003 -- 0.0078)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:38  lr: 0.000044  min_lr: 0.000011  loss: 1.6790 (1.7202)  loss_scale: 32768.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9078 (6.7670)  time: 0.8625 (0.5334 -- 3.2678)  data: 0.0022 (0.0006 -- 0.0070)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000011  loss: 1.7312 (1.7248)  loss_scale: 32768.0000 (25098.8936)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4596 (6.8935)  time: 0.8563 (0.5281 -- 4.6437)  data: 0.0016 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000011  loss: 1.6954 (1.7206)  loss_scale: 32768.0000 (26009.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1392 (6.9340)  time: 0.6777 (0.4964 -- 2.5104)  data: 0.0014 (0.0002 -- 0.0161)  max mem: 16413
Epoch: [75] Total time: 0:02:25 (0.9086 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000011  loss: 1.6954 (1.7346)  loss_scale: 32768.0000 (26009.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1392 (6.9340)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.2204 (0.2204)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6743 (2.6743 -- 2.6743)  data: 2.4352 (2.4352 -- 2.4352)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5127 (0.6819)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (98.9899)  time: 0.4426 (0.2013 -- 2.6743)  data: 0.2224 (0.0004 -- 2.4352)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5127 (0.6441)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2096 (0.1700 -- 0.2413)  data: 0.0008 (0.0001 -- 0.0021)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5799 (0.7085)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (97.0954)  time: 0.1963 (0.1333 -- 0.2413)  data: 0.0005 (0.0001 -- 0.0020)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.664
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 83.82%
Epoch: [76]  [  0/160]  eta: 0:16:44  lr: 0.000044  min_lr: 0.000011  loss: 1.9161 (1.9161)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1459 (8.1459)  time: 6.2784 (6.2784 -- 6.2784)  data: 5.0681 (5.0681 -- 5.0681)  max mem: 16413
[2023-09-05 00:43:03,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12164
[2023-09-05 00:43:03,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12164
[2023-09-05 00:43:03,800] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:43:03,800] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:43:03,800] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [76]  [ 20/160]  eta: 0:02:50  lr: 0.000044  min_lr: 0.000011  loss: 1.8086 (1.8005)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2330 (6.8805)  time: 0.9648 (0.5173 -- 2.8282)  data: 0.2669 (0.0009 -- 2.3039)  max mem: 16413
Epoch: [76]  [ 40/160]  eta: 0:02:08  lr: 0.000044  min_lr: 0.000011  loss: 1.4681 (1.6631)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5608 (7.1909)  time: 0.9197 (0.5277 -- 4.4356)  data: 0.0317 (0.0007 -- 0.6065)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000011  loss: 1.7234 (1.6991)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8636 (7.1660)  time: 0.8336 (0.5315 -- 3.1572)  data: 0.0020 (0.0004 -- 0.0067)  max mem: 16413
Epoch: [76]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000011  loss: 1.8499 (1.7367)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7110 (7.1687)  time: 0.8143 (0.5385 -- 3.2731)  data: 0.0040 (0.0009 -- 0.0475)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000011  loss: 1.8499 (1.7500)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4676 (7.2637)  time: 0.9985 (0.5150 -- 4.2300)  data: 0.0016 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000011  loss: 1.8743 (1.7647)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2089 (7.3928)  time: 0.8398 (0.5354 -- 2.7233)  data: 0.0023 (0.0002 -- 0.0128)  max mem: 16413
[2023-09-05 00:44:56,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:44:56,600] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:44:56,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:44:56,600] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [76]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.8173 (1.7678)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1378 (7.3466)  time: 0.8548 (0.5220 -- 3.0140)  data: 0.0011 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.6864 (1.7521)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4589 (7.3262)  time: 0.5982 (0.4960 -- 1.9711)  data: 0.0009 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [76] Total time: 0:02:22 (0.8890 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.6864 (1.7515)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4589 (7.3262)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2531 (0.2531)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4109 (2.4109 -- 2.4109)  data: 2.1899 (2.1899 -- 2.1899)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5808 (0.6806)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (100.0000)  time: 0.4170 (0.1893 -- 2.4109)  data: 0.2030 (0.0006 -- 2.1899)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5808 (0.6384)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (97.8836)  time: 0.2215 (0.1686 -- 0.4183)  data: 0.0181 (0.0001 -- 0.2216)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6165 (0.6793)  acc1: 85.7143 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2069 (0.1333 -- 0.4183)  data: 0.0177 (0.0001 -- 0.2216)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 82.573 Acc@5 97.510 loss 0.669
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 83.82%
Epoch: [77]  [  0/160]  eta: 0:21:44  lr: 0.000043  min_lr: 0.000011  loss: 2.0078 (2.0078)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4572 (6.4572)  time: 8.1556 (8.1556 -- 8.1556)  data: 6.9498 (6.9498 -- 6.9498)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:48  lr: 0.000043  min_lr: 0.000011  loss: 1.7610 (1.8081)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4453 (7.6657)  time: 0.8594 (0.5298 -- 4.4280)  data: 0.3088 (0.0005 -- 3.9135)  max mem: 16413
Epoch: [77]  [ 40/160]  eta: 0:02:08  lr: 0.000043  min_lr: 0.000011  loss: 1.7219 (1.7616)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2924 (7.0473)  time: 0.9212 (0.5184 -- 3.1838)  data: 0.3689 (0.0005 -- 2.6356)  max mem: 16413
Epoch: [77]  [ 60/160]  eta: 0:01:39  lr: 0.000043  min_lr: 0.000011  loss: 1.7272 (1.7080)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0120 (6.8888)  time: 0.8338 (0.5239 -- 3.3029)  data: 0.2457 (0.0002 -- 2.7695)  max mem: 16413
[2023-09-05 00:46:29,356] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12387
[2023-09-05 00:46:29,356] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12387
[2023-09-05 00:46:29,356] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:46:29,356] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:46:29,356] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [77]  [ 80/160]  eta: 0:01:17  lr: 0.000043  min_lr: 0.000011  loss: 1.5716 (1.7062)  loss_scale: 16384.0000 (29936.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3194 (7.0702)  time: 0.9056 (0.5061 -- 5.0436)  data: 0.1787 (0.0003 -- 1.8563)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.7086 (1.7056)  loss_scale: 16384.0000 (27252.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8600 (7.1640)  time: 0.8164 (0.5252 -- 3.7129)  data: 0.0015 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.6203 (1.6936)  loss_scale: 16384.0000 (25456.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7518 (7.1856)  time: 0.7994 (0.5402 -- 2.3359)  data: 0.0893 (0.0008 -- 1.3866)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:17  lr: 0.000043  min_lr: 0.000011  loss: 1.8037 (1.6972)  loss_scale: 16384.0000 (24169.3050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8655 (7.2025)  time: 0.7636 (0.5366 -- 2.8741)  data: 0.0344 (0.0005 -- 0.6537)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.8270 (1.7159)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8139 (7.1673)  time: 0.7842 (0.4973 -- 3.1390)  data: 0.2212 (0.0002 -- 2.5921)  max mem: 16413
Epoch: [77] Total time: 0:02:21 (0.8832 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.8270 (1.7582)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8139 (7.1673)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2013 (0.2013)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5461 (2.5461 -- 2.5461)  data: 2.3281 (2.3281 -- 2.3281)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4618 (0.7012)  acc1: 100.0000 (82.8283)  acc5: 100.0000 (100.0000)  time: 0.4308 (0.2103 -- 2.5461)  data: 0.2125 (0.0006 -- 2.3281)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5609 (0.6622)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (97.8836)  time: 0.2215 (0.1704 -- 0.4862)  data: 0.0155 (0.0001 -- 0.2972)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5911 (0.6931)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2043 (0.1324 -- 0.4862)  data: 0.0152 (0.0001 -- 0.2972)  max mem: 16413
Val: Total time: 0:00:07 (0.2947 s / it)
* Acc@1 83.195 Acc@5 97.510 loss 0.669
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 83.82%
Epoch: [78]  [  0/160]  eta: 0:21:11  lr: 0.000043  min_lr: 0.000011  loss: 1.5582 (1.5582)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6634 (8.6634)  time: 7.9480 (7.9480 -- 7.9480)  data: 7.4115 (7.4115 -- 7.4115)  max mem: 16413
Epoch: [78]  [ 20/160]  eta: 0:02:37  lr: 0.000043  min_lr: 0.000011  loss: 1.5450 (1.6781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0629 (7.5676)  time: 0.7833 (0.5325 -- 3.0598)  data: 0.1354 (0.0003 -- 1.7984)  max mem: 16413
[2023-09-05 00:48:29,833] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:48:29,834] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:48:29,838] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:48:29,838] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [78]  [ 40/160]  eta: 0:02:04  lr: 0.000043  min_lr: 0.000011  loss: 1.9036 (1.7767)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9515 (7.4525)  time: 0.9517 (0.5223 -- 3.5432)  data: 0.1778 (0.0005 -- 1.3110)  max mem: 16413
[2023-09-05 00:48:37,620] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12524
[2023-09-05 00:48:37,620] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12524
[2023-09-05 00:48:37,620] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:48:37,620] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:48:37,620] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [78]  [ 60/160]  eta: 0:01:36  lr: 0.000043  min_lr: 0.000011  loss: 1.8628 (1.7857)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1046 (7.7914)  time: 0.7973 (0.5193 -- 3.2513)  data: 0.0018 (0.0001 -- 0.0063)  max mem: 16413
[2023-09-05 00:49:02,615] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12552
[2023-09-05 00:49:02,615] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12552
[2023-09-05 00:49:02,615] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 00:49:02,615] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 00:49:02,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [78]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000011  loss: 1.8782 (1.7969)  loss_scale: 16384.0000 (17091.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6488 (7.5976)  time: 0.8951 (0.5265 -- 3.4998)  data: 0.0206 (0.0002 -- 0.3799)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000011  loss: 1.8216 (1.7905)  loss_scale: 8192.0000 (15329.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6515 (7.5078)  time: 0.8064 (0.5217 -- 2.5940)  data: 0.1906 (0.0003 -- 2.0868)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000011  loss: 1.7746 (1.7863)  loss_scale: 8192.0000 (14149.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1363 (7.3433)  time: 0.9799 (0.5274 -- 4.0513)  data: 0.1396 (0.0004 -- 1.8494)  max mem: 16413
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.6956 (1.7677)  loss_scale: 8192.0000 (13304.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8705 (7.4291)  time: 0.8330 (0.5142 -- 2.5192)  data: 0.1988 (0.0004 -- 2.0086)  max mem: 16413
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.6901 (1.7615)  loss_scale: 8192.0000 (12697.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1839 (7.4278)  time: 0.7164 (0.4941 -- 2.6360)  data: 0.1374 (0.0002 -- 2.1061)  max mem: 16413
Epoch: [78] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.6901 (1.7705)  loss_scale: 8192.0000 (12697.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1839 (7.4278)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2106 (0.2106)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5153 (2.5153 -- 2.5153)  data: 2.2604 (2.2604 -- 2.2604)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5397 (0.6656)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4300 (0.2030 -- 2.5153)  data: 0.2066 (0.0006 -- 2.2604)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5397 (0.6326)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2126 (0.1721 -- 0.2719)  data: 0.0031 (0.0001 -- 0.0461)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5397 (0.6608)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (95.8506)  time: 0.1939 (0.1332 -- 0.2719)  data: 0.0027 (0.0001 -- 0.0461)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 84.232 Acc@5 96.888 loss 0.641
Accuracy of the network on the 482 val images: 84.23%
[2023-09-05 00:50:23,036] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 00:50:23,038] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 00:50:23,038] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 00:50:23,038] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 00:50:24,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 00:50:24,461] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.23%
Epoch: [79]  [  0/160]  eta: 0:27:09  lr: 0.000043  min_lr: 0.000011  loss: 1.9865 (1.9865)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7481 (6.7481)  time: 10.1830 (10.1830 -- 10.1830)  data: 9.6300 (9.6300 -- 9.6300)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:46  lr: 0.000043  min_lr: 0.000011  loss: 1.7350 (1.7816)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2440 (7.5706)  time: 0.7386 (0.5350 -- 2.5071)  data: 0.1832 (0.0003 -- 1.9650)  max mem: 16413
Epoch: [79]  [ 40/160]  eta: 0:02:08  lr: 0.000043  min_lr: 0.000011  loss: 1.8291 (1.8256)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3265 (7.6697)  time: 0.9401 (0.5204 -- 3.8649)  data: 0.2714 (0.0005 -- 2.2768)  max mem: 16413
[2023-09-05 00:51:09,018] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:51:09,019] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 00:51:09,021] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:51:09,022] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [79]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.9528 (1.8414)  loss_scale: 16384.0000 (10877.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5483 (7.3310)  time: 0.8031 (0.5281 -- 3.0205)  data: 0.0058 (0.0003 -- 0.0875)  max mem: 16413
Epoch: [79]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000011  loss: 1.7409 (1.8184)  loss_scale: 16384.0000 (12237.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1292 (7.4915)  time: 0.7963 (0.5366 -- 3.0437)  data: 0.0113 (0.0002 -- 0.1913)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.7878 (1.8216)  loss_scale: 16384.0000 (13058.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3042 (7.5300)  time: 0.9828 (0.5310 -- 4.0444)  data: 0.0016 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.4827 (1.7722)  loss_scale: 16384.0000 (13608.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5963 (7.4841)  time: 0.8237 (0.5344 -- 3.5192)  data: 0.0016 (0.0001 -- 0.0042)  max mem: 16413
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.6909 (1.7624)  loss_scale: 16384.0000 (14001.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8323 (7.4987)  time: 0.9101 (0.5129 -- 3.5974)  data: 0.0856 (0.0002 -- 1.6828)  max mem: 16413
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.5098 (1.7409)  loss_scale: 16384.0000 (14284.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4990 (7.3778)  time: 0.6688 (0.4952 -- 2.7224)  data: 0.0317 (0.0001 -- 0.4102)  max mem: 16413
Epoch: [79] Total time: 0:02:22 (0.8933 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.5098 (1.7584)  loss_scale: 16384.0000 (14284.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4990 (7.3778)
[2023-09-05 00:52:47,401] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-09-05 00:52:47,403] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-09-05 00:52:47,405] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-09-05 00:52:47,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-09-05 00:52:48,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-09-05 00:52:48,497] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1573 (0.1573)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3050 (2.3050 -- 2.3050)  data: 2.0777 (2.0777 -- 2.0777)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4054 (0.6659)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4311 (0.2040 -- 2.3050)  data: 0.2139 (0.0007 -- 2.0777)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4684 (0.6211)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2313 (0.1695 -- 0.4824)  data: 0.0246 (0.0001 -- 0.2652)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4989 (0.6478)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2152 (0.1333 -- 0.4824)  data: 0.0243 (0.0001 -- 0.2652)  max mem: 16413
Val: Total time: 0:00:07 (0.2933 s / it)
* Acc@1 82.158 Acc@5 97.510 loss 0.652
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 84.23%
Epoch: [80]  [  0/160]  eta: 0:21:25  lr: 0.000043  min_lr: 0.000011  loss: 1.9859 (1.9859)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0054 (8.0054)  time: 8.0349 (8.0349 -- 8.0349)  data: 6.6206 (6.6206 -- 6.6206)  max mem: 16413
[2023-09-05 00:53:11,858] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:53:11,858] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:53:11,858] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:53:11,858] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [80]  [ 20/160]  eta: 0:02:48  lr: 0.000043  min_lr: 0.000011  loss: 1.7344 (1.7901)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3304 (7.3677)  time: 0.8619 (0.5235 -- 4.2886)  data: 0.1395 (0.0003 -- 1.8461)  max mem: 16413
Epoch: [80]  [ 40/160]  eta: 0:02:02  lr: 0.000043  min_lr: 0.000011  loss: 1.6575 (1.7507)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1027 (7.7321)  time: 0.8276 (0.5188 -- 2.4059)  data: 0.2528 (0.0002 -- 1.7378)  max mem: 16413
[2023-09-05 00:53:46,924] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12849
[2023-09-05 00:53:46,924] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12849
[2023-09-05 00:53:46,924] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:53:46,924] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:53:46,924] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [80]  [ 60/160]  eta: 0:01:36  lr: 0.000043  min_lr: 0.000011  loss: 1.6578 (1.7593)  loss_scale: 16384.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2498 (7.6628)  time: 0.8375 (0.5223 -- 2.3609)  data: 0.2106 (0.0001 -- 1.8248)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:19  lr: 0.000043  min_lr: 0.000011  loss: 1.8211 (1.7801)  loss_scale: 16384.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5032 (7.6456)  time: 1.0933 (0.5155 -- 5.2194)  data: 0.5164 (0.0003 -- 4.6699)  max mem: 16413
Epoch: [80]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000011  loss: 1.8421 (1.7821)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7278 (7.5553)  time: 0.6890 (0.5143 -- 2.1440)  data: 0.1447 (0.0002 -- 1.6036)  max mem: 16413
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.8516 (1.7960)  loss_scale: 16384.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6546 (7.5570)  time: 0.8176 (0.5202 -- 3.2780)  data: 0.2532 (0.0005 -- 2.7304)  max mem: 16413
Epoch: [80]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.4861 (1.7685)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7934 (7.5122)  time: 0.8896 (0.5272 -- 2.6486)  data: 0.1700 (0.0008 -- 1.4644)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.8060 (1.7785)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7519 (7.4724)  time: 0.6695 (0.4982 -- 1.4822)  data: 0.0501 (0.0002 -- 0.5516)  max mem: 16413
Epoch: [80] Total time: 0:02:21 (0.8827 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.8060 (1.7716)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7519 (7.4724)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1595 (0.1595)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4529 (2.4529 -- 2.4529)  data: 2.1711 (2.1711 -- 2.1711)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4756 (0.6320)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4312 (0.2011 -- 2.4529)  data: 0.2117 (0.0007 -- 2.1711)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4756 (0.5791)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2197 (0.1707 -- 0.3876)  data: 0.0155 (0.0001 -- 0.1497)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5144 (0.6134)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2061 (0.1333 -- 0.3876)  data: 0.0152 (0.0001 -- 0.1497)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 82.780 Acc@5 96.888 loss 0.633
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.23%
Epoch: [81]  [  0/160]  eta: 0:20:45  lr: 0.000043  min_lr: 0.000011  loss: 2.2128 (2.2128)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6143 (5.6143)  time: 7.7828 (7.7828 -- 7.7828)  data: 6.4164 (6.4164 -- 6.4164)  max mem: 16413
[2023-09-05 00:55:49,839] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:55:49,839] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:55:49,839] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:55:49,840] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [81]  [ 20/160]  eta: 0:02:49  lr: 0.000043  min_lr: 0.000011  loss: 1.8576 (1.7994)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5214 (6.5102)  time: 0.8841 (0.5308 -- 2.5664)  data: 0.0367 (0.0007 -- 0.6940)  max mem: 16413
[2023-09-05 00:55:54,818] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12984
[2023-09-05 00:55:54,818] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12984
[2023-09-05 00:55:54,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:55:54,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:55:54,819] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 00:56:08,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=73, lr=[1.0888047512840977e-05, 1.0888047512840977e-05, 1.2097830569823309e-05, 1.2097830569823309e-05, 1.344203396647034e-05, 1.344203396647034e-05, 1.4935593296078158e-05, 1.4935593296078158e-05, 1.6595103662309063e-05, 1.6595103662309063e-05, 1.8439004069232294e-05, 1.8439004069232294e-05, 2.048778229914699e-05, 2.048778229914699e-05, 2.2764202554607765e-05, 2.2764202554607765e-05, 2.5293558394008627e-05, 2.5293558394008627e-05, 2.8103953771120694e-05, 2.8103953771120694e-05, 3.122661530124522e-05, 3.122661530124522e-05, 3.46962392236058e-05, 3.46962392236058e-05, 3.855137691511755e-05, 3.855137691511755e-05, 4.28348632390195e-05, 4.28348632390195e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 00:56:08,447] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=17.384598217820628, CurrSamplesPerSec=20.890733791533656, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:07  lr: 0.000043  min_lr: 0.000011  loss: 1.4367 (1.7089)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0038 (7.1809)  time: 0.9031 (0.5259 -- 3.6223)  data: 0.0014 (0.0006 -- 0.0041)  max mem: 16413
Epoch: [81]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.8988 (1.7640)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2712 (7.0567)  time: 0.8358 (0.5288 -- 2.9443)  data: 0.1824 (0.0003 -- 2.3808)  max mem: 16413
Epoch: [81]  [ 80/160]  eta: 0:01:19  lr: 0.000043  min_lr: 0.000011  loss: 1.7939 (1.7496)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5011 (6.9619)  time: 0.9966 (0.5137 -- 4.7151)  data: 0.4048 (0.0003 -- 4.1838)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000011  loss: 1.7377 (1.7510)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0430 (6.8043)  time: 0.7820 (0.5299 -- 2.6802)  data: 0.2103 (0.0003 -- 2.1591)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.5776 (1.7343)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3541 (6.8205)  time: 0.8034 (0.5236 -- 2.5694)  data: 0.0730 (0.0005 -- 0.6168)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.7018 (1.7369)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0770 (6.8465)  time: 0.8998 (0.5285 -- 4.1886)  data: 0.0334 (0.0003 -- 0.6437)  max mem: 16413
[2023-09-05 00:57:47,551] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:57:47,551] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 00:57:47,551] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 00:57:47,551] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.6481 (1.7203)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3020 (6.8144)  time: 0.7850 (0.4948 -- 4.7234)  data: 0.0008 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [81] Total time: 0:02:25 (0.9067 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.6481 (1.7194)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3020 (6.8144)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1923 (0.1923)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3363 (2.3363 -- 2.3363)  data: 2.0796 (2.0796 -- 2.0796)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3984 (0.6608)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (100.0000)  time: 0.4113 (0.1970 -- 2.3363)  data: 0.1901 (0.0006 -- 2.0796)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4233 (0.6150)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (97.8836)  time: 0.2205 (0.1690 -- 0.3473)  data: 0.0136 (0.0001 -- 0.1435)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5213 (0.6589)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (97.0954)  time: 0.2017 (0.1328 -- 0.3473)  data: 0.0133 (0.0001 -- 0.1435)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 81.743 Acc@5 97.303 loss 0.652
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 84.23%
Epoch: [82]  [  0/160]  eta: 0:22:56  lr: 0.000043  min_lr: 0.000011  loss: 2.0628 (2.0628)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3791 (11.3791)  time: 8.6031 (8.6031 -- 8.6031)  data: 6.0892 (6.0892 -- 6.0892)  max mem: 16413
Epoch: [82]  [ 20/160]  eta: 0:02:47  lr: 0.000043  min_lr: 0.000011  loss: 1.8803 (1.7964)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7130 (7.4591)  time: 0.8247 (0.5299 -- 3.4357)  data: 0.0517 (0.0005 -- 0.9718)  max mem: 16413
[2023-09-05 00:58:37,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13154
[2023-09-05 00:58:37,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13154
[2023-09-05 00:58:37,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:58:37,319] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 00:58:37,319] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [82]  [ 40/160]  eta: 0:02:11  lr: 0.000043  min_lr: 0.000011  loss: 1.6573 (1.7636)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7386 (7.3648)  time: 0.9925 (0.5227 -- 4.3604)  data: 0.4456 (0.0004 -- 3.8398)  max mem: 16413
Epoch: [82]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000011  loss: 1.7224 (1.7452)  loss_scale: 16384.0000 (25516.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3599 (7.2836)  time: 0.7464 (0.5198 -- 2.7134)  data: 0.1909 (0.0003 -- 2.1853)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000011  loss: 1.7989 (1.7680)  loss_scale: 16384.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7325 (7.1986)  time: 0.7638 (0.5325 -- 3.5024)  data: 0.2113 (0.0003 -- 2.9831)  max mem: 16413
Epoch: [82]  [100/160]  eta: 0:00:54  lr: 0.000043  min_lr: 0.000011  loss: 2.0006 (1.7925)  loss_scale: 16384.0000 (21899.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0844 (7.1369)  time: 0.8535 (0.5298 -- 2.3484)  data: 0.0363 (0.0009 -- 0.3379)  max mem: 16413
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000011  loss: 1.6226 (1.7707)  loss_scale: 16384.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1676 (7.1192)  time: 0.9387 (0.5413 -- 2.1276)  data: 0.1925 (0.0002 -- 1.2377)  max mem: 16413
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000011  loss: 1.5999 (1.7556)  loss_scale: 16384.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6587 (7.1071)  time: 0.9072 (0.5376 -- 2.5670)  data: 0.1285 (0.0004 -- 1.4195)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000011  loss: 1.6850 (1.7457)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4779 (7.0522)  time: 0.6634 (0.4957 -- 2.5007)  data: 0.0292 (0.0002 -- 0.5690)  max mem: 16413
Epoch: [82] Total time: 0:02:21 (0.8819 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000011  loss: 1.6850 (1.7559)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4779 (7.0522)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2161 (0.2161)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3399 (2.3399 -- 2.3399)  data: 2.1013 (2.1013 -- 2.1013)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5627 (0.6839)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (100.0000)  time: 0.4173 (0.2052 -- 2.3399)  data: 0.1985 (0.0008 -- 2.1013)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5581 (0.6304)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (97.8836)  time: 0.2247 (0.1697 -- 0.5102)  data: 0.0209 (0.0001 -- 0.3251)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5774 (0.6669)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2073 (0.1333 -- 0.5102)  data: 0.0206 (0.0001 -- 0.3251)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.674
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.23%
Epoch: [83]  [  0/160]  eta: 0:22:41  lr: 0.000043  min_lr: 0.000011  loss: 1.7833 (1.7833)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9950 (5.9950)  time: 8.5082 (8.5082 -- 8.5082)  data: 7.9812 (7.9812 -- 7.9812)  max mem: 16413
[2023-09-05 01:00:37,728] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:00:37,729] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:00:37,729] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:00:37,729] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [83]  [ 20/160]  eta: 0:02:50  lr: 0.000043  min_lr: 0.000011  loss: 1.5883 (1.7332)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7679 (6.8006)  time: 0.8527 (0.5276 -- 4.0050)  data: 0.3082 (0.0005 -- 3.4398)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:02:01  lr: 0.000043  min_lr: 0.000011  loss: 1.6602 (1.6782)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3416 (6.8984)  time: 0.7982 (0.5249 -- 2.4566)  data: 0.2300 (0.0001 -- 1.8983)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:40  lr: 0.000043  min_lr: 0.000011  loss: 1.8511 (1.7198)  loss_scale: 32768.0000 (31962.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5700 (6.8777)  time: 0.9751 (0.5301 -- 3.3859)  data: 0.0625 (0.0002 -- 1.2151)  max mem: 16413
Epoch: [83]  [ 80/160]  eta: 0:01:17  lr: 0.000042  min_lr: 0.000011  loss: 1.7689 (1.7220)  loss_scale: 32768.0000 (32161.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4488 (6.6269)  time: 0.8849 (0.5154 -- 5.0990)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [83]  [100/160]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000011  loss: 1.9540 (1.7627)  loss_scale: 32768.0000 (32281.3465)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6815 (6.6297)  time: 0.8637 (0.5280 -- 3.5363)  data: 0.0019 (0.0004 -- 0.0063)  max mem: 16413
Epoch: [83]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.5436 (1.7423)  loss_scale: 32768.0000 (32361.7851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9574 (6.7333)  time: 0.7892 (0.5233 -- 3.3411)  data: 0.0412 (0.0004 -- 0.7899)  max mem: 16413
[2023-09-05 01:02:31,959] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:02:31,960] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 01:02:31,966] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:02:31,966] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.4495 (1.7221)  loss_scale: 32768.0000 (34743.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2294 (6.7843)  time: 1.0665 (0.5194 -- 4.5991)  data: 0.0021 (0.0003 -- 0.0107)  max mem: 16413
[2023-09-05 01:02:46,159] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13428
[2023-09-05 01:02:46,159] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13428
[2023-09-05 01:02:46,159] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 01:02:46,159] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 01:02:46,159] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.8477 (1.7367)  loss_scale: 32768.0000 (35942.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8377 (6.9123)  time: 0.5736 (0.4885 -- 1.7496)  data: 0.0005 (0.0001 -- 0.0013)  max mem: 16413
Epoch: [83] Total time: 0:02:24 (0.9006 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.8477 (1.7558)  loss_scale: 32768.0000 (35942.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8377 (6.9123)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2234 (0.2234)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3646 (2.3646 -- 2.3646)  data: 2.1127 (2.1127 -- 2.1127)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3989 (0.6275)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4136 (0.2036 -- 2.3646)  data: 0.1933 (0.0005 -- 2.1127)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5337 (0.6099)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2210 (0.1691 -- 0.4838)  data: 0.0150 (0.0001 -- 0.2846)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5897 (0.6456)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2038 (0.1327 -- 0.4838)  data: 0.0147 (0.0001 -- 0.2846)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 81.535 Acc@5 97.303 loss 0.659
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 84.23%
Epoch: [84]  [  0/160]  eta: 0:23:24  lr: 0.000042  min_lr: 0.000011  loss: 1.7833 (1.7833)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1613 (6.1613)  time: 8.7808 (8.7808 -- 8.7808)  data: 8.2612 (8.2612 -- 8.2612)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:03:00  lr: 0.000042  min_lr: 0.000011  loss: 1.6341 (1.6137)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3727 (7.1750)  time: 0.9128 (0.5301 -- 4.4381)  data: 0.3656 (0.0003 -- 3.9209)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:08  lr: 0.000042  min_lr: 0.000011  loss: 1.9172 (1.6935)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3428 (6.8726)  time: 0.8426 (0.5325 -- 3.3903)  data: 0.2880 (0.0003 -- 2.8666)  max mem: 16413
Epoch: [84]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000011  loss: 1.8892 (1.7394)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4182 (6.8432)  time: 0.8305 (0.5299 -- 3.6509)  data: 0.2773 (0.0004 -- 3.1272)  max mem: 16413
Epoch: [84]  [ 80/160]  eta: 0:01:17  lr: 0.000042  min_lr: 0.000011  loss: 1.6157 (1.7166)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7418 (6.7977)  time: 0.9225 (0.5186 -- 4.0330)  data: 0.3646 (0.0003 -- 3.5092)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:55  lr: 0.000042  min_lr: 0.000011  loss: 1.6571 (1.7071)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8196 (6.6838)  time: 0.7602 (0.5302 -- 3.0405)  data: 0.2084 (0.0004 -- 2.5033)  max mem: 16413
[2023-09-05 01:04:49,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:04:49,547] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 01:04:49,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:04:49,548] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [84]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000011  loss: 1.7725 (1.7182)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1095 (6.6039)  time: 0.9162 (0.5347 -- 2.9509)  data: 0.2695 (0.0003 -- 2.4208)  max mem: 16413
[2023-09-05 01:04:55,762] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13564
[2023-09-05 01:04:55,762] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13564
[2023-09-05 01:04:55,762] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 01:04:55,762] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 01:04:55,762] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [84]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.7779 (1.7172)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3307 (6.6947)  time: 0.8524 (0.5344 -- 2.2236)  data: 0.0561 (0.0001 -- 0.5932)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.7378 (1.7084)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8978 (6.6760)  time: 0.7525 (0.4971 -- 1.9642)  data: 0.1656 (0.0002 -- 1.0033)  max mem: 16413
Epoch: [84] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.7378 (1.7437)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8978 (6.6760)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1733 (0.1733)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3963 (2.3963 -- 2.3963)  data: 2.1823 (2.1823 -- 2.1823)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4370 (0.6458)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4238 (0.2118 -- 2.3963)  data: 0.1993 (0.0005 -- 2.1823)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4607 (0.5922)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2195 (0.1699 -- 0.2638)  data: 0.0020 (0.0001 -- 0.0268)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4673 (0.6357)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.6805)  time: 0.2001 (0.1330 -- 0.2638)  data: 0.0017 (0.0001 -- 0.0268)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.665
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.23%
Epoch: [85]  [  0/160]  eta: 0:24:24  lr: 0.000042  min_lr: 0.000011  loss: 1.5210 (1.5210)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8970 (5.8970)  time: 9.1533 (9.1533 -- 9.1533)  data: 6.9691 (6.9691 -- 6.9691)  max mem: 16413
[2023-09-05 01:05:40,624] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13603
[2023-09-05 01:05:40,624] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13603
[2023-09-05 01:05:40,626] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:05:40,626] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:05:40,626] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [ 20/160]  eta: 0:02:43  lr: 0.000042  min_lr: 0.000011  loss: 1.7232 (1.7044)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3673 (6.7725)  time: 0.7671 (0.5239 -- 2.3267)  data: 0.0070 (0.0002 -- 0.0938)  max mem: 16413
Epoch: [85]  [ 40/160]  eta: 0:02:02  lr: 0.000042  min_lr: 0.000011  loss: 1.7618 (1.7014)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2349 (6.6842)  time: 0.8757 (0.5209 -- 2.9901)  data: 0.0164 (0.0008 -- 0.3008)  max mem: 16413
Epoch: [85]  [ 60/160]  eta: 0:01:35  lr: 0.000042  min_lr: 0.000011  loss: 1.4200 (1.6563)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2819 (6.8290)  time: 0.8018 (0.5344 -- 2.9635)  data: 0.1302 (0.0002 -- 2.2689)  max mem: 16413
Epoch: [85]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000011  loss: 1.8769 (1.7019)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0622 (6.7471)  time: 0.9920 (0.5143 -- 3.6645)  data: 0.3509 (0.0002 -- 3.1510)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.6007 (1.6751)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3909 (6.7804)  time: 0.8924 (0.5231 -- 4.5251)  data: 0.3341 (0.0002 -- 3.9960)  max mem: 16413
Epoch: [85]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000011  loss: 1.7152 (1.6906)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2622 (6.9589)  time: 0.8818 (0.5329 -- 3.7132)  data: 0.2459 (0.0002 -- 3.1866)  max mem: 16413
[2023-09-05 01:07:33,149] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:07:33,149] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:07:33,150] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:07:33,151] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.6976 (1.6979)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7317 (6.9296)  time: 0.8127 (0.5274 -- 3.7896)  data: 0.2629 (0.0004 -- 3.2702)  max mem: 16413
[2023-09-05 01:07:47,788] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13748
[2023-09-05 01:07:47,788] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13748
[2023-09-05 01:07:47,788] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:07:47,788] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:07:47,788] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 01:07:50,272] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13753
[2023-09-05 01:07:50,272] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13753
[2023-09-05 01:07:50,272] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 01:07:50,272] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 01:07:50,272] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.8085 (1.7152)  loss_scale: 16384.0000 (17971.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8878 (6.9626)  time: 0.7513 (0.4818 -- 3.6001)  data: 0.0440 (0.0002 -- 0.8679)  max mem: 16413
Epoch: [85] Total time: 0:02:24 (0.9008 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.8085 (1.7496)  loss_scale: 16384.0000 (17971.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8878 (6.9626)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1907 (0.1907)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3042 (2.3042 -- 2.3042)  data: 2.0513 (2.0513 -- 2.0513)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4922 (0.6322)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4079 (0.1971 -- 2.3042)  data: 0.1930 (0.0007 -- 2.0513)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4922 (0.6000)  acc1: 77.7778 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2256 (0.1697 -- 0.3633)  data: 0.0155 (0.0001 -- 0.1235)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5938 (0.6437)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (97.0954)  time: 0.2106 (0.1328 -- 0.3633)  data: 0.0152 (0.0001 -- 0.1235)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 81.950 Acc@5 96.888 loss 0.670
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 84.23%
Epoch: [86]  [  0/160]  eta: 0:19:21  lr: 0.000042  min_lr: 0.000011  loss: 1.9732 (1.9732)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7258 (5.7258)  time: 7.2621 (7.2621 -- 7.2621)  data: 6.7162 (6.7162 -- 6.7162)  max mem: 16413
Epoch: [86]  [ 20/160]  eta: 0:02:46  lr: 0.000042  min_lr: 0.000011  loss: 1.8956 (1.8109)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1817 (7.5136)  time: 0.8827 (0.5192 -- 3.2000)  data: 0.1787 (0.0004 -- 2.6696)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:09  lr: 0.000042  min_lr: 0.000011  loss: 1.6146 (1.7294)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7457 (7.2718)  time: 0.9620 (0.5214 -- 3.0501)  data: 0.0450 (0.0004 -- 0.5239)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:41  lr: 0.000042  min_lr: 0.000011  loss: 1.6366 (1.7158)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1017 (7.3895)  time: 0.8770 (0.5217 -- 3.0160)  data: 0.0046 (0.0002 -- 0.0579)  max mem: 16413
Epoch: [86]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000011  loss: 1.8627 (1.7452)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9075 (7.4615)  time: 0.7845 (0.5358 -- 2.2348)  data: 0.0792 (0.0002 -- 0.9493)  max mem: 16413
Epoch: [86]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.6346 (1.7319)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8030 (7.4195)  time: 0.8661 (0.5278 -- 3.2608)  data: 0.2173 (0.0002 -- 2.7087)  max mem: 16413
Epoch: [86]  [120/160]  eta: 0:00:38  lr: 0.000042  min_lr: 0.000011  loss: 1.9123 (1.7598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2302 (7.4494)  time: 1.0539 (0.5221 -- 5.0066)  data: 0.5072 (0.0003 -- 4.4833)  max mem: 16413
[2023-09-05 01:09:58,701] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:09:58,701] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 01:09:58,702] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:09:58,702] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.7025 (1.7488)  loss_scale: 16384.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6149 (7.3094)  time: 0.8075 (0.5276 -- 3.3702)  data: 0.2656 (0.0003 -- 2.8358)  max mem: 16413
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.7191 (1.7405)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4009 (7.2369)  time: 0.6663 (0.4933 -- 3.6012)  data: 0.1534 (0.0002 -- 3.0587)  max mem: 16413
Epoch: [86] Total time: 0:02:24 (0.9048 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.7191 (1.7102)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4009 (7.2369)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1549 (0.1549)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4494 (2.4494 -- 2.4494)  data: 2.1250 (2.1250 -- 2.1250)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4116 (0.6408)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4217 (0.2021 -- 2.4494)  data: 0.1982 (0.0006 -- 2.1250)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5014 (0.6085)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2218 (0.1701 -- 0.5292)  data: 0.0202 (0.0001 -- 0.3462)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5725 (0.6566)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (96.6805)  time: 0.2059 (0.1335 -- 0.5292)  data: 0.0198 (0.0001 -- 0.3462)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.664
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 84.23%
Epoch: [87]  [  0/160]  eta: 0:21:55  lr: 0.000042  min_lr: 0.000011  loss: 1.9358 (1.9358)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6779 (6.6779)  time: 8.2236 (8.2236 -- 8.2236)  data: 7.6488 (7.6488 -- 7.6488)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:50  lr: 0.000042  min_lr: 0.000011  loss: 1.7100 (1.6966)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1763 (7.3762)  time: 0.8641 (0.5302 -- 3.8781)  data: 0.3070 (0.0006 -- 3.3343)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:04  lr: 0.000042  min_lr: 0.000011  loss: 1.7473 (1.6925)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0379 (7.0725)  time: 0.8581 (0.5359 -- 2.9821)  data: 0.3048 (0.0007 -- 2.4196)  max mem: 16413
Epoch: [87]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000011  loss: 1.7668 (1.7013)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5469 (7.3140)  time: 0.9107 (0.5257 -- 3.2264)  data: 0.3581 (0.0001 -- 2.7021)  max mem: 16413
[2023-09-05 01:11:50,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=79, lr=[1.063220559158231e-05, 1.063220559158231e-05, 1.1813561768424791e-05, 1.1813561768424791e-05, 1.312617974269421e-05, 1.312617974269421e-05, 1.4584644158549123e-05, 1.4584644158549123e-05, 1.620516017616569e-05, 1.620516017616569e-05, 1.800573352907299e-05, 1.800573352907299e-05, 2.0006370587858877e-05, 2.0006370587858877e-05, 2.222930065317653e-05, 2.222930065317653e-05, 2.469922294797392e-05, 2.469922294797392e-05, 2.7443581053304353e-05, 2.7443581053304353e-05, 3.049286783700484e-05, 3.049286783700484e-05, 3.388096426333871e-05, 3.388096426333871e-05, 3.764551584815412e-05, 3.764551584815412e-05, 4.1828350942393466e-05, 4.1828350942393466e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 01:11:50,056] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=17.437942742019718, CurrSamplesPerSec=21.87721861984611, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000011  loss: 2.0264 (1.7624)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3939 (7.2139)  time: 0.8173 (0.5166 -- 3.8874)  data: 0.2157 (0.0004 -- 3.3585)  max mem: 16413
[2023-09-05 01:11:59,458] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:11:59,458] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:11:59,461] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:11:59,462] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [87]  [100/160]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000011  loss: 1.5878 (1.7401)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7600 (7.0062)  time: 0.9534 (0.5339 -- 3.2720)  data: 0.3428 (0.0005 -- 2.7537)  max mem: 16413
Epoch: [87]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000011  loss: 1.6949 (1.7412)  loss_scale: 32768.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6607 (7.0525)  time: 0.8158 (0.5245 -- 3.3269)  data: 0.2578 (0.0002 -- 2.7710)  max mem: 16413
Epoch: [87]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.7960 (1.7541)  loss_scale: 32768.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3893 (6.9942)  time: 0.9034 (0.5217 -- 3.5135)  data: 0.3572 (0.0003 -- 2.9665)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.7150 (1.7533)  loss_scale: 32768.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6896 (6.9671)  time: 0.7937 (0.4975 -- 3.5135)  data: 0.2779 (0.0002 -- 2.9665)  max mem: 16413
Epoch: [87] Total time: 0:02:23 (0.8942 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.7150 (1.7257)  loss_scale: 32768.0000 (23552.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6896 (6.9671)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1669 (0.1669)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5393 (2.5393 -- 2.5393)  data: 2.3313 (2.3313 -- 2.3313)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4093 (0.6033)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4318 (0.2021 -- 2.5393)  data: 0.2129 (0.0006 -- 2.3313)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4884 (0.5846)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (97.8836)  time: 0.2132 (0.1700 -- 0.2485)  data: 0.0023 (0.0001 -- 0.0330)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5530 (0.6122)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.9253)  time: 0.1973 (0.1328 -- 0.2485)  data: 0.0021 (0.0001 -- 0.0330)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 81.328 Acc@5 97.510 loss 0.641
Accuracy of the network on the 482 val images: 81.33%
Max accuracy: 84.23%
Epoch: [88]  [  0/160]  eta: 0:17:18  lr: 0.000042  min_lr: 0.000011  loss: 1.8439 (1.8439)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0744 (5.0744)  time: 6.4918 (6.4918 -- 6.4918)  data: 5.7904 (5.7904 -- 5.7904)  max mem: 16413
Epoch: [88]  [ 20/160]  eta: 0:02:48  lr: 0.000042  min_lr: 0.000011  loss: 1.8505 (1.7774)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0631 (7.5009)  time: 0.9373 (0.5308 -- 3.6096)  data: 0.3848 (0.0006 -- 3.0522)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:01:59  lr: 0.000042  min_lr: 0.000011  loss: 1.7038 (1.6801)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8896 (7.5913)  time: 0.7852 (0.5254 -- 3.3442)  data: 0.2341 (0.0004 -- 2.8119)  max mem: 16413
[2023-09-05 01:14:03,622] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:14:03,623] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 01:14:03,623] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:14:03,623] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [88]  [ 60/160]  eta: 0:01:37  lr: 0.000042  min_lr: 0.000011  loss: 1.7702 (1.7127)  loss_scale: 32768.0000 (34379.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9852 (7.2328)  time: 0.9254 (0.5221 -- 3.3220)  data: 0.3677 (0.0004 -- 2.8042)  max mem: 16413
[2023-09-05 01:14:10,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14145
[2023-09-05 01:14:10,191] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14145
[2023-09-05 01:14:10,191] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 01:14:10,191] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 01:14:10,191] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [88]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000011  loss: 1.7482 (1.7097)  loss_scale: 32768.0000 (35599.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8024 (6.9971)  time: 0.8319 (0.5201 -- 3.1918)  data: 0.2127 (0.0002 -- 2.6880)  max mem: 16413
Epoch: [88]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000011  loss: 1.9653 (1.7217)  loss_scale: 32768.0000 (35039.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0925 (7.0169)  time: 0.9884 (0.5321 -- 3.9163)  data: 0.0867 (0.0008 -- 1.3734)  max mem: 16413
Epoch: [88]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000011  loss: 1.7629 (1.7312)  loss_scale: 32768.0000 (34663.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6371 (6.9841)  time: 0.7434 (0.5172 -- 2.5531)  data: 0.0097 (0.0003 -- 0.1669)  max mem: 16413
Epoch: [88]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000011  loss: 1.5464 (1.7140)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8484 (7.1539)  time: 0.8669 (0.5235 -- 2.4771)  data: 0.0929 (0.0005 -- 0.9826)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000011  loss: 1.7290 (1.7112)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1949 (7.1584)  time: 0.7042 (0.4959 -- 2.4366)  data: 0.1396 (0.0002 -- 1.9171)  max mem: 16413
Epoch: [88] Total time: 0:02:21 (0.8841 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000011  loss: 1.7290 (1.7195)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1949 (7.1584)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1845 (0.1845)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5086 (2.5086 -- 2.5086)  data: 2.2926 (2.2926 -- 2.2926)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3977 (0.6375)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (96.9697)  time: 0.4390 (0.2020 -- 2.5086)  data: 0.2144 (0.0008 -- 2.2926)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4689 (0.5885)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2203 (0.1710 -- 0.3250)  data: 0.0106 (0.0001 -- 0.1436)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5044 (0.6218)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (96.2656)  time: 0.2021 (0.1332 -- 0.3250)  data: 0.0103 (0.0001 -- 0.1436)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 82.780 Acc@5 96.680 loss 0.662
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.23%
Epoch: [89]  [  0/160]  eta: 0:22:41  lr: 0.000042  min_lr: 0.000011  loss: 1.8677 (1.8677)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9371 (5.9371)  time: 8.5079 (8.5079 -- 8.5079)  data: 7.9876 (7.9876 -- 7.9876)  max mem: 16413
Epoch: [89]  [ 20/160]  eta: 0:02:44  lr: 0.000042  min_lr: 0.000011  loss: 1.7303 (1.7574)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2937 (6.6197)  time: 0.8089 (0.5214 -- 3.1177)  data: 0.2607 (0.0003 -- 2.5645)  max mem: 16413
[2023-09-05 01:16:07,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14271
[2023-09-05 01:16:07,966] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:16:07,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 01:16:07,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14271
[2023-09-05 01:16:07,968] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [89]  [ 40/160]  eta: 0:02:04  lr: 0.000042  min_lr: 0.000011  loss: 1.7676 (1.7554)  loss_scale: 16384.0000 (28771.9024)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3402 (6.7102)  time: 0.8947 (0.5335 -- 2.7573)  data: 0.3260 (0.0004 -- 2.2113)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:38  lr: 0.000042  min_lr: 0.000011  loss: 1.6881 (1.7589)  loss_scale: 16384.0000 (24710.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9443 (6.5961)  time: 0.8692 (0.5276 -- 3.6277)  data: 0.2774 (0.0005 -- 3.0852)  max mem: 16413
Epoch: [89]  [ 80/160]  eta: 0:01:15  lr: 0.000041  min_lr: 0.000011  loss: 1.6419 (1.7502)  loss_scale: 16384.0000 (22654.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7807 (6.6150)  time: 0.8104 (0.5371 -- 2.5303)  data: 0.2513 (0.0003 -- 2.0001)  max mem: 16413
Epoch: [89]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000011  loss: 1.5939 (1.7376)  loss_scale: 16384.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6214 (6.6374)  time: 0.8863 (0.5413 -- 4.4337)  data: 0.3343 (0.0003 -- 3.9170)  max mem: 16413
Epoch: [89]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000011  loss: 1.6110 (1.7077)  loss_scale: 16384.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5682 (6.6104)  time: 0.8207 (0.5193 -- 3.0356)  data: 0.2682 (0.0003 -- 2.4790)  max mem: 16413
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000011  loss: 1.8839 (1.7354)  loss_scale: 16384.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0150 (6.5765)  time: 1.0528 (0.5246 -- 4.3359)  data: 0.4300 (0.0004 -- 3.7984)  max mem: 16413
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000011  loss: 1.7014 (1.7284)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4113 (6.5977)  time: 0.5257 (0.4982 -- 0.5873)  data: 0.0007 (0.0001 -- 0.0018)  max mem: 16413
Epoch: [89] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000011  loss: 1.7014 (1.7579)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4113 (6.5977)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2225 (0.2225)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5549 (2.5549 -- 2.5549)  data: 2.3097 (2.3097 -- 2.3097)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4465 (0.5995)  acc1: 77.7778 (82.8283)  acc5: 100.0000 (100.0000)  time: 0.4295 (0.2016 -- 2.5549)  data: 0.2111 (0.0008 -- 2.3097)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5159 (0.5873)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (98.4127)  time: 0.2116 (0.1698 -- 0.2546)  data: 0.0041 (0.0001 -- 0.0637)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5586 (0.6288)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (97.5104)  time: 0.1967 (0.1334 -- 0.2546)  data: 0.0037 (0.0001 -- 0.0637)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 81.743 Acc@5 97.718 loss 0.645
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 84.23%
[2023-09-05 01:18:11,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:18:11,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:18:11,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:18:11,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [90]  [  0/160]  eta: 0:20:14  lr: 0.000041  min_lr: 0.000011  loss: 2.0998 (2.0998)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3910 (9.3910)  time: 7.5917 (7.5917 -- 7.5917)  data: 7.0736 (7.0736 -- 7.0736)  max mem: 16413
[2023-09-05 01:18:21,719] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14412
[2023-09-05 01:18:21,719] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:18:21,719] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 01:18:21,719] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14412
[2023-09-05 01:18:21,719] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [90]  [ 20/160]  eta: 0:02:52  lr: 0.000041  min_lr: 0.000011  loss: 1.8034 (1.7806)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1615 (7.2108)  time: 0.9156 (0.5227 -- 3.0124)  data: 0.1903 (0.0003 -- 1.4495)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:04  lr: 0.000041  min_lr: 0.000011  loss: 1.9738 (1.8333)  loss_scale: 16384.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1324 (6.9928)  time: 0.8261 (0.5318 -- 3.8347)  data: 0.2069 (0.0003 -- 1.9765)  max mem: 16413
Epoch: [90]  [ 60/160]  eta: 0:01:41  lr: 0.000041  min_lr: 0.000011  loss: 1.8684 (1.8525)  loss_scale: 16384.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5475 (6.9748)  time: 0.9688 (0.5212 -- 3.5470)  data: 0.1072 (0.0005 -- 1.2108)  max mem: 16413
Epoch: [90]  [ 80/160]  eta: 0:01:15  lr: 0.000041  min_lr: 0.000011  loss: 1.7837 (1.8410)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0186 (7.2222)  time: 0.7231 (0.5316 -- 2.5843)  data: 0.1400 (0.0004 -- 1.5191)  max mem: 16413
Epoch: [90]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000010  loss: 1.7532 (1.8155)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4132 (7.1133)  time: 0.8847 (0.5388 -- 3.0939)  data: 0.3325 (0.0006 -- 2.5589)  max mem: 16413
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.5867 (1.7946)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4268 (7.1656)  time: 0.8735 (0.5301 -- 3.9212)  data: 0.3203 (0.0003 -- 3.4109)  max mem: 16413
Epoch: [90]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.6959 (1.7866)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6053 (7.1429)  time: 0.8655 (0.5320 -- 3.4676)  data: 0.2902 (0.0003 -- 2.9326)  max mem: 16413
[2023-09-05 01:20:13,140] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:20:13,140] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:20:13,140] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:20:13,141] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:20:13,684] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14542
[2023-09-05 01:20:13,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:20:13,685] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14542
[2023-09-05 01:20:13,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:20:13,685] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.8315 (1.7894)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7007 (7.1490)  time: 0.6419 (0.4956 -- 2.9280)  data: 0.0527 (0.0002 -- 1.0345)  max mem: 16413
Epoch: [90] Total time: 0:02:21 (0.8816 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.8315 (1.7687)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7007 (7.1490)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1860 (0.1860)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4441 (2.4441 -- 2.4441)  data: 2.2128 (2.2128 -- 2.2128)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4535 (0.6395)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4236 (0.1912 -- 2.4441)  data: 0.2024 (0.0007 -- 2.2128)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4535 (0.5897)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2186 (0.1702 -- 0.3359)  data: 0.0080 (0.0001 -- 0.1419)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5303 (0.6328)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.6805)  time: 0.2009 (0.1327 -- 0.3359)  data: 0.0075 (0.0001 -- 0.1419)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 82.780 Acc@5 96.888 loss 0.663
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.23%
Epoch: [91]  [  0/160]  eta: 0:21:22  lr: 0.000041  min_lr: 0.000010  loss: 2.0919 (2.0919)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0370 (10.0370)  time: 8.0160 (8.0160 -- 8.0160)  data: 7.4409 (7.4409 -- 7.4409)  max mem: 16413
Epoch: [91]  [ 20/160]  eta: 0:02:49  lr: 0.000041  min_lr: 0.000010  loss: 1.6826 (1.6739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2854 (8.1651)  time: 0.8685 (0.5285 -- 5.3234)  data: 0.2230 (0.0004 -- 3.5326)  max mem: 16413
Epoch: [91]  [ 40/160]  eta: 0:02:11  lr: 0.000041  min_lr: 0.000010  loss: 1.8353 (1.6944)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3253 (7.7883)  time: 0.9767 (0.5319 -- 4.3765)  data: 0.0733 (0.0003 -- 1.4284)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:37  lr: 0.000041  min_lr: 0.000010  loss: 1.9370 (1.7669)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7499 (7.5068)  time: 0.7270 (0.5189 -- 2.0475)  data: 0.0134 (0.0004 -- 0.2391)  max mem: 16413
Epoch: [91]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000010  loss: 1.6742 (1.7377)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2239 (7.4364)  time: 0.9543 (0.5253 -- 3.1754)  data: 0.0024 (0.0002 -- 0.0148)  max mem: 16413
Epoch: [91]  [100/160]  eta: 0:00:55  lr: 0.000041  min_lr: 0.000010  loss: 1.6607 (1.7348)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2389 (7.3682)  time: 0.7814 (0.5338 -- 2.6509)  data: 0.0242 (0.0005 -- 0.4530)  max mem: 16413
[2023-09-05 01:22:18,304] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:22:18,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:22:18,305] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:22:18,305] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [91]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000010  loss: 1.4753 (1.7019)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0794 (7.2128)  time: 0.9241 (0.5430 -- 2.9194)  data: 0.0030 (0.0004 -- 0.0180)  max mem: 16413
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.5564 (1.6858)  loss_scale: 32768.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5753 (7.1616)  time: 0.8468 (0.5160 -- 2.7889)  data: 0.0015 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.4176 (1.6728)  loss_scale: 32768.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4760 (7.2240)  time: 0.6841 (0.4956 -- 3.0015)  data: 0.0007 (0.0001 -- 0.0031)  max mem: 16413
Epoch: [91] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.4176 (1.7025)  loss_scale: 32768.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4760 (7.2240)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1923 (0.1923)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4435 (2.4435 -- 2.4435)  data: 2.2156 (2.2156 -- 2.2156)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3624 (0.5985)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4408 (0.1866 -- 2.4435)  data: 0.2244 (0.0004 -- 2.2156)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4663 (0.5801)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2276 (0.1697 -- 0.4676)  data: 0.0219 (0.0001 -- 0.2419)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5426 (0.6172)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2128 (0.1338 -- 0.4676)  data: 0.0215 (0.0001 -- 0.2419)  max mem: 16413
Val: Total time: 0:00:07 (0.2957 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.655
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.23%
Epoch: [92]  [  0/160]  eta: 0:21:02  lr: 0.000041  min_lr: 0.000010  loss: 1.6103 (1.6103)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2333 (6.2333)  time: 7.8917 (7.8917 -- 7.8917)  data: 6.8110 (6.8110 -- 6.8110)  max mem: 16413
[2023-09-05 01:23:26,513] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14739
[2023-09-05 01:23:26,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14739
[2023-09-05 01:23:26,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:23:26,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:23:26,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [92]  [ 20/160]  eta: 0:02:50  lr: 0.000041  min_lr: 0.000010  loss: 1.8399 (1.7478)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5611 (6.5989)  time: 0.8865 (0.5267 -- 2.8162)  data: 0.0885 (0.0005 -- 0.9417)  max mem: 16413
Epoch: [92]  [ 40/160]  eta: 0:02:08  lr: 0.000041  min_lr: 0.000010  loss: 1.6594 (1.6799)  loss_scale: 16384.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8856 (6.1861)  time: 0.9097 (0.5208 -- 2.9987)  data: 0.1586 (0.0003 -- 2.4615)  max mem: 16413
Epoch: [92]  [ 60/160]  eta: 0:01:43  lr: 0.000041  min_lr: 0.000010  loss: 1.5546 (1.6753)  loss_scale: 16384.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7552 (6.4012)  time: 0.9646 (0.5326 -- 3.5120)  data: 0.0676 (0.0003 -- 1.2135)  max mem: 16413
Epoch: [92]  [ 80/160]  eta: 0:01:16  lr: 0.000041  min_lr: 0.000010  loss: 1.8330 (1.7137)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4208 (6.4860)  time: 0.7138 (0.5216 -- 2.5128)  data: 0.0016 (0.0001 -- 0.0043)  max mem: 16413
Epoch: [92]  [100/160]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000010  loss: 1.7486 (1.7049)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7564 (6.6414)  time: 0.9218 (0.5214 -- 4.2860)  data: 0.0017 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.5717 (1.6888)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2254 (6.8768)  time: 0.7892 (0.5317 -- 2.6427)  data: 0.1526 (0.0003 -- 2.1134)  max mem: 16413
Epoch: [92]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.6281 (1.6892)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5128 (6.7998)  time: 0.9876 (0.5316 -- 4.1512)  data: 0.3665 (0.0004 -- 3.6243)  max mem: 16413
[2023-09-05 01:25:21,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:25:21,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:25:21,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:25:21,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.6118 (1.6927)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3303 (6.7557)  time: 0.6633 (0.4954 -- 2.5003)  data: 0.0007 (0.0001 -- 0.0020)  max mem: 16413
Epoch: [92] Total time: 0:02:24 (0.9006 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.6118 (1.7049)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3303 (6.7557)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1610 (0.1610)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3992 (2.3992 -- 2.3992)  data: 2.1741 (2.1741 -- 2.1741)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3796 (0.6213)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4300 (0.2121 -- 2.3992)  data: 0.2015 (0.0006 -- 2.1741)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4038 (0.5858)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2253 (0.1684 -- 0.3998)  data: 0.0114 (0.0001 -- 0.1824)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5093 (0.6292)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2031 (0.1319 -- 0.3998)  data: 0.0104 (0.0001 -- 0.1824)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.636
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.23%
Epoch: [93]  [  0/160]  eta: 0:18:32  lr: 0.000041  min_lr: 0.000010  loss: 1.1963 (1.1963)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8461 (3.8461)  time: 6.9532 (6.9532 -- 6.9532)  data: 5.3468 (5.3468 -- 5.3468)  max mem: 16413
Epoch: [93]  [ 20/160]  eta: 0:02:41  lr: 0.000041  min_lr: 0.000010  loss: 1.6723 (1.6943)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6029 (6.5470)  time: 0.8600 (0.5333 -- 3.9297)  data: 0.0735 (0.0010 -- 0.6635)  max mem: 16413
[2023-09-05 01:26:06,602] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14907
[2023-09-05 01:26:06,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14907
[2023-09-05 01:26:06,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:26:06,603] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:26:06,603] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [93]  [ 40/160]  eta: 0:02:08  lr: 0.000041  min_lr: 0.000010  loss: 1.5987 (1.6488)  loss_scale: 16384.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2513 (6.5612)  time: 0.9852 (0.5147 -- 4.0071)  data: 0.0475 (0.0002 -- 0.7751)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:40  lr: 0.000041  min_lr: 0.000010  loss: 1.5991 (1.6471)  loss_scale: 16384.0000 (23635.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1239 (6.4994)  time: 0.8664 (0.5130 -- 3.8332)  data: 0.1285 (0.0002 -- 1.7907)  max mem: 16413
Epoch: [93]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000010  loss: 1.8524 (1.6681)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5687 (6.5671)  time: 0.8836 (0.5297 -- 2.6762)  data: 0.0024 (0.0004 -- 0.0136)  max mem: 16413
[2023-09-05 01:26:56,241] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14963
[2023-09-05 01:26:56,242] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14963
[2023-09-05 01:26:56,242] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 01:26:56,242] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 01:26:56,242] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [93]  [100/160]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000010  loss: 1.5918 (1.6775)  loss_scale: 8192.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9538 (6.8045)  time: 0.8104 (0.5263 -- 3.7534)  data: 0.0022 (0.0003 -- 0.0096)  max mem: 16413
[2023-09-05 01:27:26,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=86, lr=[1.035168387970652e-05, 1.035168387970652e-05, 1.150187097745169e-05, 1.150187097745169e-05, 1.2779856641612986e-05, 1.2779856641612986e-05, 1.4199840712903319e-05, 1.4199840712903319e-05, 1.5777600792114797e-05, 1.5777600792114797e-05, 1.753066754679422e-05, 1.753066754679422e-05, 1.947851949643802e-05, 1.947851949643802e-05, 2.1642799440486687e-05, 2.1642799440486687e-05, 2.40475549338741e-05, 2.40475549338741e-05, 2.6719505482082328e-05, 2.6719505482082328e-05, 2.9688339424535924e-05, 2.9688339424535924e-05, 3.298704380503991e-05, 3.298704380503991e-05, 3.6652270894488794e-05, 3.6652270894488794e-05, 4.072474543832088e-05, 4.072474543832088e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 01:27:26,018] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=17.45427205791479, CurrSamplesPerSec=21.531804466986575, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [93]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.7287 (1.6969)  loss_scale: 8192.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0212 (6.7514)  time: 0.8389 (0.5144 -- 2.3608)  data: 0.0020 (0.0001 -- 0.0056)  max mem: 16413
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.8477 (1.7009)  loss_scale: 8192.0000 (16151.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5679 (6.9816)  time: 0.8821 (0.5268 -- 2.1674)  data: 0.0014 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000010  loss: 1.5276 (1.6894)  loss_scale: 8192.0000 (15206.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4861 (6.9786)  time: 0.7832 (0.4951 -- 2.7412)  data: 0.1841 (0.0002 -- 2.2492)  max mem: 16413
Epoch: [93] Total time: 0:02:24 (0.9038 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000010  loss: 1.5276 (1.7214)  loss_scale: 8192.0000 (15206.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4861 (6.9786)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1712 (0.1712)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4900 (2.4900 -- 2.4900)  data: 2.2422 (2.2422 -- 2.2422)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5194 (0.6277)  acc1: 77.7778 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4277 (0.2011 -- 2.4900)  data: 0.2050 (0.0006 -- 2.2422)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5000 (0.5711)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2207 (0.1694 -- 0.4473)  data: 0.0136 (0.0001 -- 0.2564)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5000 (0.6230)  acc1: 85.7143 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2039 (0.1332 -- 0.4473)  data: 0.0132 (0.0001 -- 0.2564)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 83.402 Acc@5 97.095 loss 0.645
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.23%
Epoch: [94]  [  0/160]  eta: 0:20:49  lr: 0.000041  min_lr: 0.000010  loss: 1.8086 (1.8086)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0499 (7.0499)  time: 7.8093 (7.8093 -- 7.8093)  data: 7.2756 (7.2756 -- 7.2756)  max mem: 16413
Epoch: [94]  [ 20/160]  eta: 0:02:45  lr: 0.000041  min_lr: 0.000010  loss: 1.4449 (1.6147)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5998 (6.5386)  time: 0.8515 (0.5192 -- 3.6313)  data: 0.1726 (0.0004 -- 2.3515)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:01:58  lr: 0.000041  min_lr: 0.000010  loss: 1.7022 (1.6440)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6442 (6.6838)  time: 0.7848 (0.5320 -- 2.3715)  data: 0.1604 (0.0004 -- 1.8154)  max mem: 16413
[2023-09-05 01:28:59,181] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:28:59,181] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 01:28:59,182] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:28:59,182] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [94]  [ 60/160]  eta: 0:01:36  lr: 0.000041  min_lr: 0.000010  loss: 1.7821 (1.6716)  loss_scale: 8192.0000 (9400.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6666 (6.7960)  time: 0.9094 (0.5323 -- 1.8829)  data: 0.1738 (0.0004 -- 1.3639)  max mem: 16413
Epoch: [94]  [ 80/160]  eta: 0:01:16  lr: 0.000041  min_lr: 0.000010  loss: 1.5235 (1.6524)  loss_scale: 16384.0000 (11124.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7293 (6.7706)  time: 0.9453 (0.5193 -- 2.8849)  data: 0.3313 (0.0004 -- 2.3532)  max mem: 16413
Epoch: [94]  [100/160]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000010  loss: 1.7512 (1.6750)  loss_scale: 16384.0000 (12166.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8977 (6.8156)  time: 0.8670 (0.5397 -- 2.7972)  data: 0.3077 (0.0004 -- 2.2536)  max mem: 16413
Epoch: [94]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000010  loss: 1.6875 (1.6948)  loss_scale: 16384.0000 (12863.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4601 (6.8273)  time: 0.8089 (0.5300 -- 2.5493)  data: 0.2181 (0.0003 -- 2.0362)  max mem: 16413
Epoch: [94]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000010  loss: 1.8682 (1.7250)  loss_scale: 16384.0000 (13362.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0473 (6.8845)  time: 0.8824 (0.5282 -- 3.4180)  data: 0.2941 (0.0003 -- 2.8582)  max mem: 16413
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.7263 (1.7230)  loss_scale: 16384.0000 (13721.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7180 (6.9377)  time: 0.6617 (0.4962 -- 2.8515)  data: 0.0280 (0.0002 -- 0.5457)  max mem: 16413
Epoch: [94] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.7263 (1.7132)  loss_scale: 16384.0000 (13721.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7180 (6.9377)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2192 (0.2192)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3151 (2.3151 -- 2.3151)  data: 2.0675 (2.0675 -- 2.0675)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4344 (0.6411)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4309 (0.1979 -- 2.3151)  data: 0.2164 (0.0008 -- 2.0675)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5179 (0.6223)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2332 (0.1700 -- 0.5098)  data: 0.0308 (0.0001 -- 0.3002)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5874 (0.6540)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.2186 (0.1320 -- 0.5098)  data: 0.0304 (0.0001 -- 0.3002)  max mem: 16413
Val: Total time: 0:00:07 (0.2953 s / it)
* Acc@1 83.195 Acc@5 97.303 loss 0.659
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 84.23%
Epoch: [95]  [  0/160]  eta: 0:21:04  lr: 0.000040  min_lr: 0.000010  loss: 1.6436 (1.6436)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0276 (5.0276)  time: 7.9040 (7.9040 -- 7.9040)  data: 7.3483 (7.3483 -- 7.3483)  max mem: 16413
[2023-09-05 01:31:03,010] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:31:03,011] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:31:03,012] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:31:03,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [95]  [ 20/160]  eta: 0:02:49  lr: 0.000040  min_lr: 0.000010  loss: 1.8376 (1.8121)  loss_scale: 16384.0000 (17164.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5293 (6.6619)  time: 0.8780 (0.5185 -- 4.5880)  data: 0.2213 (0.0005 -- 2.4367)  max mem: 16413
Epoch: [95]  [ 40/160]  eta: 0:02:06  lr: 0.000040  min_lr: 0.000010  loss: 1.6545 (1.7507)  loss_scale: 32768.0000 (24775.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6698 (6.3757)  time: 0.8944 (0.5316 -- 4.7922)  data: 0.0019 (0.0005 -- 0.0138)  max mem: 16413
[2023-09-05 01:31:23,673] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15245
[2023-09-05 01:31:23,673] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15245
[2023-09-05 01:31:23,674] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:31:23,673] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:31:23,674] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [95]  [ 60/160]  eta: 0:01:37  lr: 0.000040  min_lr: 0.000010  loss: 1.7252 (1.7483)  loss_scale: 16384.0000 (23098.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5801 (6.6214)  time: 0.8001 (0.5311 -- 3.4154)  data: 0.0291 (0.0003 -- 0.4022)  max mem: 16413
Epoch: [95]  [ 80/160]  eta: 0:01:17  lr: 0.000040  min_lr: 0.000010  loss: 1.8241 (1.7499)  loss_scale: 16384.0000 (21440.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1385 (6.7677)  time: 0.9522 (0.5256 -- 2.9173)  data: 0.1381 (0.0002 -- 1.9759)  max mem: 16413
Epoch: [95]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 1.6499 (1.7424)  loss_scale: 16384.0000 (20439.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7745 (6.8405)  time: 0.7923 (0.5207 -- 2.0158)  data: 0.0908 (0.0004 -- 1.0287)  max mem: 16413
[2023-09-05 01:32:12,931] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15302
[2023-09-05 01:32:12,931] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15302
[2023-09-05 01:32:12,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 01:32:12,931] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 01:32:12,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [95]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000010  loss: 1.6715 (1.7281)  loss_scale: 8192.0000 (18482.7769)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9415 (6.8881)  time: 0.8304 (0.5180 -- 2.5680)  data: 0.1263 (0.0002 -- 2.0338)  max mem: 16413
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.6782 (1.7305)  loss_scale: 8192.0000 (17023.0922)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0351 (6.9608)  time: 0.9329 (0.5354 -- 3.1274)  data: 0.3205 (0.0007 -- 2.6015)  max mem: 16413
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.6774 (1.7272)  loss_scale: 8192.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1366 (6.9113)  time: 0.6943 (0.4938 -- 2.1969)  data: 0.0124 (0.0003 -- 0.2274)  max mem: 16413
Epoch: [95] Total time: 0:02:21 (0.8852 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.6774 (1.7263)  loss_scale: 8192.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1366 (6.9113)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2512 (0.2512)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3439 (2.3439 -- 2.3439)  data: 2.1475 (2.1475 -- 2.1475)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4826 (0.6415)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4069 (0.2006 -- 2.3439)  data: 0.1962 (0.0009 -- 2.1475)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4826 (0.5991)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2248 (0.1718 -- 0.5839)  data: 0.0196 (0.0001 -- 0.3773)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5304 (0.6382)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.0954)  time: 0.2094 (0.1327 -- 0.5839)  data: 0.0193 (0.0001 -- 0.3773)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 84.232 Acc@5 96.680 loss 0.662
Accuracy of the network on the 482 val images: 84.23%
[2023-09-05 01:33:07,029] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 01:33:07,031] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 01:33:07,031] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 01:33:07,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 01:33:08,450] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 01:33:08,450] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.23%
Epoch: [96]  [  0/160]  eta: 0:16:56  lr: 0.000040  min_lr: 0.000010  loss: 1.5793 (1.5793)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1251 (7.1251)  time: 6.3547 (6.3547 -- 6.3547)  data: 5.7082 (5.7082 -- 5.7082)  max mem: 16413
Epoch: [96]  [ 20/160]  eta: 0:02:37  lr: 0.000040  min_lr: 0.000010  loss: 1.6591 (1.8019)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5651 (7.2536)  time: 0.8601 (0.5313 -- 4.2125)  data: 0.3047 (0.0008 -- 3.6444)  max mem: 16413
Epoch: [96]  [ 40/160]  eta: 0:02:05  lr: 0.000040  min_lr: 0.000010  loss: 1.5613 (1.6813)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3373 (6.9509)  time: 0.9619 (0.5260 -- 4.4829)  data: 0.4166 (0.0004 -- 3.9448)  max mem: 16413
Epoch: [96]  [ 60/160]  eta: 0:01:35  lr: 0.000040  min_lr: 0.000010  loss: 1.5707 (1.6739)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8456 (6.9755)  time: 0.7882 (0.5184 -- 3.7973)  data: 0.2230 (0.0004 -- 3.2644)  max mem: 16413
[2023-09-05 01:34:16,909] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:34:16,909] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 01:34:16,910] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:34:16,910] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [96]  [ 80/160]  eta: 0:01:16  lr: 0.000040  min_lr: 0.000010  loss: 1.5879 (1.6768)  loss_scale: 8192.0000 (9203.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5771 (7.0081)  time: 0.9526 (0.5208 -- 3.8056)  data: 0.3811 (0.0003 -- 3.2987)  max mem: 16413
Epoch: [96]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 1.6040 (1.6769)  loss_scale: 16384.0000 (10625.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0950 (6.9590)  time: 0.8210 (0.5318 -- 3.3234)  data: 0.2701 (0.0005 -- 2.7893)  max mem: 16413
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000010  loss: 1.6723 (1.6582)  loss_scale: 16384.0000 (11577.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6662 (7.0096)  time: 0.7925 (0.5331 -- 2.5658)  data: 0.2151 (0.0003 -- 2.0265)  max mem: 16413
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.6796 (1.6724)  loss_scale: 16384.0000 (12258.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2456 (7.0717)  time: 0.9150 (0.5428 -- 3.8334)  data: 0.2278 (0.0005 -- 3.2812)  max mem: 16413
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.8250 (1.6955)  loss_scale: 16384.0000 (12748.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8630 (7.0788)  time: 0.6876 (0.4977 -- 2.5022)  data: 0.1551 (0.0001 -- 1.9738)  max mem: 16413
Epoch: [96] Total time: 0:02:21 (0.8827 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.8250 (1.7007)  loss_scale: 16384.0000 (12748.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8630 (7.0788)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1433 (0.1433)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3020 (2.3020 -- 2.3020)  data: 2.0912 (2.0912 -- 2.0912)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4551 (0.6357)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4074 (0.1989 -- 2.3020)  data: 0.1969 (0.0007 -- 2.0912)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3757 (0.5519)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.3545)  time: 0.2223 (0.1690 -- 0.3919)  data: 0.0185 (0.0001 -- 0.2050)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4551 (0.5991)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.5104)  time: 0.2079 (0.1331 -- 0.3919)  data: 0.0180 (0.0001 -- 0.2050)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 84.647 Acc@5 97.510 loss 0.628
Accuracy of the network on the 482 val images: 84.65%
[2023-09-05 01:35:37,423] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 01:35:37,425] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 01:35:37,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 01:35:37,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 01:35:38,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 01:35:38,873] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.65%
Epoch: [97]  [  0/160]  eta: 0:19:14  lr: 0.000040  min_lr: 0.000010  loss: 2.0948 (2.0948)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2302 (12.2302)  time: 7.2129 (7.2129 -- 7.2129)  data: 6.6801 (6.6801 -- 6.6801)  max mem: 16413
Epoch: [97]  [ 20/160]  eta: 0:02:40  lr: 0.000040  min_lr: 0.000010  loss: 1.6414 (1.7305)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4227 (8.0386)  time: 0.8448 (0.5301 -- 5.4466)  data: 0.2909 (0.0005 -- 4.9164)  max mem: 16413
[2023-09-05 01:36:18,500] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:36:18,501] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:36:18,501] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:36:18,502] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [97]  [ 40/160]  eta: 0:02:02  lr: 0.000040  min_lr: 0.000010  loss: 1.9238 (1.7751)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3098 (7.0478)  time: 0.8870 (0.5213 -- 2.7685)  data: 0.3111 (0.0008 -- 2.2296)  max mem: 16413
[2023-09-05 01:36:25,346] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15567
[2023-09-05 01:36:25,346] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15567
[2023-09-05 01:36:25,346] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:36:25,346] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:36:25,346] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [97]  [ 60/160]  eta: 0:01:39  lr: 0.000040  min_lr: 0.000010  loss: 1.8114 (1.7697)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9035 (7.1066)  time: 0.9450 (0.5247 -- 4.4918)  data: 0.3564 (0.0005 -- 3.9583)  max mem: 16413
Epoch: [97]  [ 80/160]  eta: 0:01:16  lr: 0.000040  min_lr: 0.000010  loss: 1.6157 (1.7431)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8866 (7.1231)  time: 0.8491 (0.5187 -- 4.2268)  data: 0.2938 (0.0003 -- 3.7060)  max mem: 16413
Epoch: [97]  [100/160]  eta: 0:00:57  lr: 0.000040  min_lr: 0.000010  loss: 1.6821 (1.7218)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5836 (7.1063)  time: 0.9113 (0.5196 -- 3.5741)  data: 0.3745 (0.0003 -- 3.0426)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000010  loss: 1.8399 (1.7286)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1041 (7.1277)  time: 0.8557 (0.5218 -- 3.7680)  data: 0.3116 (0.0003 -- 3.2430)  max mem: 16413
Epoch: [97]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.7878 (1.7416)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7988 (7.1013)  time: 0.8376 (0.5283 -- 2.7260)  data: 0.2821 (0.0006 -- 2.1699)  max mem: 16413
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.7133 (1.7335)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9683 (7.0862)  time: 0.6347 (0.4971 -- 2.8062)  data: 0.1144 (0.0002 -- 2.2753)  max mem: 16413
Epoch: [97] Total time: 0:02:22 (0.8877 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.7133 (1.7210)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9683 (7.0862)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1778 (0.1778)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3357 (2.3357 -- 2.3357)  data: 2.0963 (2.0963 -- 2.0963)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4325 (0.6170)  acc1: 77.7778 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4199 (0.2062 -- 2.3357)  data: 0.1919 (0.0007 -- 2.0963)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4814 (0.5867)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2193 (0.1694 -- 0.3193)  data: 0.0063 (0.0001 -- 0.1072)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5060 (0.6240)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.2005 (0.1363 -- 0.3193)  data: 0.0059 (0.0001 -- 0.1072)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 82.158 Acc@5 96.888 loss 0.643
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 84.65%
Epoch: [98]  [  0/160]  eta: 0:19:20  lr: 0.000040  min_lr: 0.000010  loss: 1.0172 (1.0172)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9028 (8.9028)  time: 7.2534 (7.2534 -- 7.2534)  data: 5.7755 (5.7755 -- 5.7755)  max mem: 16413
[2023-09-05 01:38:30,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:38:30,569] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:38:30,575] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:38:30,576] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [98]  [ 20/160]  eta: 0:02:40  lr: 0.000040  min_lr: 0.000010  loss: 1.7039 (1.6859)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8907 (6.1970)  time: 0.8413 (0.5287 -- 3.9238)  data: 0.2849 (0.0008 -- 3.3716)  max mem: 16413
Epoch: [98]  [ 40/160]  eta: 0:02:06  lr: 0.000040  min_lr: 0.000010  loss: 1.7409 (1.7218)  loss_scale: 32768.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2629 (6.5072)  time: 0.9560 (0.5090 -- 5.5324)  data: 0.2940 (0.0002 -- 3.1693)  max mem: 16413
Epoch: [98]  [ 60/160]  eta: 0:01:38  lr: 0.000040  min_lr: 0.000010  loss: 1.6854 (1.6888)  loss_scale: 32768.0000 (28470.5574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4721 (6.6501)  time: 0.8559 (0.5203 -- 5.6441)  data: 0.3069 (0.0002 -- 5.1379)  max mem: 16413
Epoch: [98]  [ 80/160]  eta: 0:01:18  lr: 0.000040  min_lr: 0.000010  loss: 1.6811 (1.7049)  loss_scale: 32768.0000 (29531.6543)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1968 (6.6637)  time: 0.9357 (0.5137 -- 3.8631)  data: 0.3852 (0.0004 -- 3.3041)  max mem: 16413
Epoch: [98]  [100/160]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000010  loss: 1.6181 (1.6796)  loss_scale: 32768.0000 (30172.5149)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8299 (6.8946)  time: 0.7943 (0.5309 -- 3.2253)  data: 0.1977 (0.0003 -- 2.7148)  max mem: 16413
[2023-09-05 01:39:58,324] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15799
[2023-09-05 01:39:58,325] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15799
[2023-09-05 01:39:58,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:39:58,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:39:58,325] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [98]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000010  loss: 1.8282 (1.6871)  loss_scale: 32768.0000 (30330.7107)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9697 (6.9657)  time: 0.8740 (0.5098 -- 3.6461)  data: 0.1455 (0.0004 -- 2.3030)  max mem: 16413
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.7707 (1.6942)  loss_scale: 16384.0000 (28352.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7015 (6.9394)  time: 0.7856 (0.5266 -- 2.6268)  data: 0.1138 (0.0003 -- 1.8666)  max mem: 16413
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.6729 (1.6993)  loss_scale: 16384.0000 (26931.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4153 (7.0039)  time: 0.7541 (0.4952 -- 2.4482)  data: 0.1531 (0.0002 -- 1.8916)  max mem: 16413
Epoch: [98] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.6729 (1.6988)  loss_scale: 16384.0000 (26931.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4153 (7.0039)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2313 (0.2313)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5143 (2.5143 -- 2.5143)  data: 2.2872 (2.2872 -- 2.2872)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4558 (0.6417)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4246 (0.2010 -- 2.5143)  data: 0.2092 (0.0006 -- 2.2872)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4558 (0.5930)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2122 (0.1705 -- 0.2946)  data: 0.0052 (0.0002 -- 0.0872)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5224 (0.6630)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (97.0954)  time: 0.1977 (0.1326 -- 0.2946)  data: 0.0048 (0.0001 -- 0.0872)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 82.365 Acc@5 96.888 loss 0.666
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 84.65%
Epoch: [99]  [  0/160]  eta: 0:17:30  lr: 0.000040  min_lr: 0.000010  loss: 2.0279 (2.0279)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6338 (8.6338)  time: 6.5635 (6.5635 -- 6.5635)  data: 5.8464 (5.8464 -- 5.8464)  max mem: 16413
Epoch: [99]  [ 20/160]  eta: 0:02:39  lr: 0.000040  min_lr: 0.000010  loss: 1.7327 (1.7135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3284 (7.3014)  time: 0.8662 (0.5187 -- 3.0288)  data: 0.0770 (0.0004 -- 0.8575)  max mem: 16413
Epoch: [99]  [ 40/160]  eta: 0:02:12  lr: 0.000040  min_lr: 0.000010  loss: 1.7486 (1.7445)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3625 (7.3115)  time: 1.0757 (0.5351 -- 4.3629)  data: 0.1150 (0.0002 -- 1.7643)  max mem: 16413
Epoch: [99]  [ 60/160]  eta: 0:01:38  lr: 0.000040  min_lr: 0.000010  loss: 1.7854 (1.7566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9252 (7.2473)  time: 0.7226 (0.5138 -- 2.8762)  data: 0.0015 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [99]  [ 80/160]  eta: 0:01:17  lr: 0.000040  min_lr: 0.000010  loss: 1.6454 (1.7118)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7931 (7.3324)  time: 0.9109 (0.5198 -- 3.6646)  data: 0.0166 (0.0004 -- 0.2937)  max mem: 16413
[2023-09-05 01:42:03,307] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:42:03,308] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:42:03,310] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:42:03,310] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [99]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000010  loss: 1.6870 (1.7018)  loss_scale: 32768.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4769 (7.2472)  time: 0.7823 (0.5149 -- 2.5802)  data: 0.2124 (0.0003 -- 2.0310)  max mem: 16413
Epoch: [99]  [120/160]  eta: 0:00:36  lr: 0.000040  min_lr: 0.000010  loss: 1.5551 (1.6872)  loss_scale: 32768.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6124 (7.1491)  time: 0.8232 (0.5346 -- 3.6438)  data: 0.2387 (0.0005 -- 3.1276)  max mem: 16413
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000010  loss: 1.6868 (1.6793)  loss_scale: 32768.0000 (22542.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7789 (7.1228)  time: 1.0324 (0.5175 -- 5.3880)  data: 0.0713 (0.0003 -- 1.4054)  max mem: 16413
[2023-09-05 01:42:59,038] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15994
[2023-09-05 01:42:59,038] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15994
[2023-09-05 01:42:59,038] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:42:59,038] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:42:59,038] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 01:43:01,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=91, lr=[1.0047965261717177e-05, 1.0047965261717177e-05, 1.1164405846352419e-05, 1.1164405846352419e-05, 1.240489538483602e-05, 1.240489538483602e-05, 1.3783217094262245e-05, 1.3783217094262245e-05, 1.531468566029138e-05, 1.531468566029138e-05, 1.7016317400323758e-05, 1.7016317400323758e-05, 1.8907019333693064e-05, 1.8907019333693064e-05, 2.100779925965896e-05, 2.100779925965896e-05, 2.3341999177398842e-05, 2.3341999177398842e-05, 2.5935554641554267e-05, 2.5935554641554267e-05, 2.88172829350603e-05, 2.88172829350603e-05, 3.201920326117811e-05, 3.201920326117811e-05, 3.5576892512420116e-05, 3.5576892512420116e-05, 3.952988056935569e-05, 3.952988056935569e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 01:43:01,547] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=17.537993535604503, CurrSamplesPerSec=24.65708665949791, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000010  loss: 1.7280 (1.6773)  loss_scale: 32768.0000 (23142.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8006 (7.1958)  time: 0.6010 (0.4818 -- 2.1946)  data: 0.0008 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [99] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000010  loss: 1.7280 (1.6974)  loss_scale: 32768.0000 (23142.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8006 (7.1958)
[2023-09-05 01:43:01,553] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-09-05 01:43:01,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-09-05 01:43:01,558] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-09-05 01:43:01,558] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-09-05 01:43:02,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-09-05 01:43:02,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1549 (0.1549)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2800 (2.2800 -- 2.2800)  data: 2.0542 (2.0542 -- 2.0542)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4462 (0.6339)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (97.9798)  time: 0.4091 (0.1897 -- 2.2800)  data: 0.1958 (0.0009 -- 2.0542)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4462 (0.5935)  acc1: 77.7778 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2281 (0.1677 -- 0.4970)  data: 0.0236 (0.0001 -- 0.2960)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5277 (0.6416)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2133 (0.1328 -- 0.4970)  data: 0.0232 (0.0001 -- 0.2960)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 82.365 Acc@5 97.718 loss 0.639
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 84.65%
Epoch: [100]  [  0/160]  eta: 0:21:25  lr: 0.000040  min_lr: 0.000010  loss: 1.9253 (1.9253)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5486 (5.5486)  time: 8.0333 (8.0333 -- 8.0333)  data: 5.1758 (5.1758 -- 5.1758)  max mem: 16413
Epoch: [100]  [ 20/160]  eta: 0:02:40  lr: 0.000040  min_lr: 0.000010  loss: 1.5754 (1.6761)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4625 (6.7538)  time: 0.8013 (0.5266 -- 3.6692)  data: 0.0024 (0.0002 -- 0.0158)  max mem: 16413
Epoch: [100]  [ 40/160]  eta: 0:02:11  lr: 0.000039  min_lr: 0.000010  loss: 1.4887 (1.6230)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0908 (6.7448)  time: 1.0520 (0.5194 -- 4.8355)  data: 0.0558 (0.0003 -- 1.0655)  max mem: 16413
Epoch: [100]  [ 60/160]  eta: 0:01:41  lr: 0.000039  min_lr: 0.000010  loss: 1.6461 (1.6622)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9131 (6.9583)  time: 0.8333 (0.5142 -- 3.6631)  data: 0.0015 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:20  lr: 0.000039  min_lr: 0.000010  loss: 1.8194 (1.6718)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2138 (7.1604)  time: 0.9929 (0.5032 -- 4.0770)  data: 0.0430 (0.0001 -- 0.8236)  max mem: 16413
Epoch: [100]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000010  loss: 1.7030 (1.6882)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9006 (7.1274)  time: 0.6950 (0.5156 -- 2.6279)  data: 0.0298 (0.0002 -- 0.5721)  max mem: 16413
Epoch: [100]  [120/160]  eta: 0:00:38  lr: 0.000039  min_lr: 0.000010  loss: 1.6952 (1.6940)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7315 (7.1446)  time: 0.9929 (0.5317 -- 5.1108)  data: 0.0465 (0.0007 -- 0.8668)  max mem: 16413
[2023-09-05 01:45:07,518] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:45:07,518] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:45:07,518] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:45:07,519] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:45:20,804] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16137
[2023-09-05 01:45:20,804] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16137
[2023-09-05 01:45:20,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:45:20,804] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:45:20,804] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.8603 (1.7034)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6074 (7.0299)  time: 0.8278 (0.5175 -- 4.3277)  data: 0.0023 (0.0004 -- 0.0100)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.8647 (1.7199)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3184 (7.0143)  time: 0.6268 (0.4955 -- 1.6952)  data: 0.0211 (0.0002 -- 0.3964)  max mem: 16413
Epoch: [100] Total time: 0:02:23 (0.8998 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.8647 (1.7275)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3184 (7.0143)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1651 (0.1651)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3432 (2.3432 -- 2.3432)  data: 2.0904 (2.0904 -- 2.0904)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4218 (0.6173)  acc1: 77.7778 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4037 (0.1914 -- 2.3432)  data: 0.1909 (0.0007 -- 2.0904)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4218 (0.5913)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2232 (0.1696 -- 0.3978)  data: 0.0196 (0.0001 -- 0.1897)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5522 (0.6315)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2100 (0.1333 -- 0.3978)  data: 0.0193 (0.0001 -- 0.1897)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 82.158 Acc@5 97.718 loss 0.654
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 84.65%
Epoch: [101]  [  0/160]  eta: 0:18:56  lr: 0.000039  min_lr: 0.000010  loss: 1.4522 (1.4522)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6937 (10.6937)  time: 7.1006 (7.1006 -- 7.1006)  data: 6.5595 (6.5595 -- 6.5595)  max mem: 16413
Epoch: [101]  [ 20/160]  eta: 0:02:54  lr: 0.000039  min_lr: 0.000010  loss: 1.7377 (1.6964)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1719 (7.1308)  time: 0.9503 (0.5242 -- 4.0452)  data: 0.1281 (0.0006 -- 1.1816)  max mem: 16413
Epoch: [101]  [ 40/160]  eta: 0:02:05  lr: 0.000039  min_lr: 0.000010  loss: 1.6830 (1.6752)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9164 (7.6276)  time: 0.8378 (0.5235 -- 4.3212)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:41  lr: 0.000039  min_lr: 0.000010  loss: 1.6078 (1.6619)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0336 (7.3371)  time: 0.9634 (0.5285 -- 4.3631)  data: 0.0017 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [101]  [ 80/160]  eta: 0:01:18  lr: 0.000039  min_lr: 0.000010  loss: 1.6482 (1.6537)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7759 (7.2030)  time: 0.8770 (0.5122 -- 5.3581)  data: 0.0011 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [101]  [100/160]  eta: 0:00:57  lr: 0.000039  min_lr: 0.000010  loss: 1.6980 (1.6662)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9993 (7.1462)  time: 0.8534 (0.5308 -- 2.9436)  data: 0.0024 (0.0006 -- 0.0138)  max mem: 16413
[2023-09-05 01:47:24,975] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:47:24,975] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:47:24,975] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:47:24,975] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:47:28,891] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16273
[2023-09-05 01:47:28,891] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16273
[2023-09-05 01:47:28,891] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:47:28,891] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:47:28,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [101]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.5540 (1.6557)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8144 (7.2568)  time: 0.8478 (0.5310 -- 4.0283)  data: 0.0016 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.7058 (1.6609)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3668 (7.2799)  time: 0.9410 (0.5055 -- 4.3129)  data: 0.0011 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.6657 (1.6602)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3192 (7.2456)  time: 0.6413 (0.4965 -- 2.7056)  data: 0.0008 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [101] Total time: 0:02:24 (0.9051 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.6657 (1.7075)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3192 (7.2456)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1434 (0.1434)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4948 (2.4948 -- 2.4948)  data: 2.2097 (2.2097 -- 2.2097)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4600 (0.6415)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4300 (0.2030 -- 2.4948)  data: 0.2050 (0.0006 -- 2.2097)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4615 (0.6033)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (97.3545)  time: 0.2255 (0.1695 -- 0.5069)  data: 0.0187 (0.0001 -- 0.3244)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6050 (0.6442)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (97.5104)  time: 0.2080 (0.1333 -- 0.5069)  data: 0.0181 (0.0001 -- 0.3244)  max mem: 16413
Val: Total time: 0:00:07 (0.2962 s / it)
* Acc@1 82.365 Acc@5 97.510 loss 0.645
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 84.65%
Epoch: [102]  [  0/160]  eta: 0:22:15  lr: 0.000039  min_lr: 0.000010  loss: 1.8624 (1.8624)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1990 (7.1990)  time: 8.3448 (8.3448 -- 8.3448)  data: 7.7931 (7.7931 -- 7.7931)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:46  lr: 0.000039  min_lr: 0.000010  loss: 1.6244 (1.7083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4017 (6.9573)  time: 0.8341 (0.5344 -- 4.2230)  data: 0.2370 (0.0004 -- 3.6663)  max mem: 16413
Epoch: [102]  [ 40/160]  eta: 0:02:02  lr: 0.000039  min_lr: 0.000010  loss: 1.6292 (1.6947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3022 (6.9722)  time: 0.8342 (0.5229 -- 1.9646)  data: 0.2557 (0.0006 -- 1.4281)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:37  lr: 0.000039  min_lr: 0.000010  loss: 1.7538 (1.6851)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3632 (7.1244)  time: 0.8849 (0.5213 -- 3.0151)  data: 0.2575 (0.0004 -- 2.4786)  max mem: 16413
Epoch: [102]  [ 80/160]  eta: 0:01:17  lr: 0.000039  min_lr: 0.000010  loss: 1.6817 (1.6888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6980 (6.9893)  time: 0.9561 (0.5321 -- 3.8236)  data: 0.4014 (0.0003 -- 3.2802)  max mem: 16413
[2023-09-05 01:49:34,979] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:49:34,979] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:49:34,982] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:49:34,982] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [102]  [100/160]  eta: 0:00:57  lr: 0.000039  min_lr: 0.000010  loss: 1.6856 (1.6926)  loss_scale: 32768.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2848 (6.8708)  time: 0.8721 (0.5161 -- 3.0377)  data: 0.3321 (0.0004 -- 2.5009)  max mem: 16413
Epoch: [102]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.7448 (1.7013)  loss_scale: 32768.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3431 (6.9184)  time: 0.8521 (0.5312 -- 4.2694)  data: 0.3032 (0.0001 -- 3.7522)  max mem: 16413
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.7157 (1.6963)  loss_scale: 32768.0000 (23239.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0350 (6.9265)  time: 0.8617 (0.5381 -- 3.9645)  data: 0.2979 (0.0004 -- 3.4275)  max mem: 16413
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.6930 (1.6958)  loss_scale: 32768.0000 (24371.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4483 (6.9117)  time: 0.6877 (0.4959 -- 3.8252)  data: 0.1665 (0.0001 -- 3.3144)  max mem: 16413
Epoch: [102] Total time: 0:02:23 (0.8968 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.6930 (1.6975)  loss_scale: 32768.0000 (24371.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4483 (6.9117)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1501 (0.1501)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4859 (2.4859 -- 2.4859)  data: 2.2733 (2.2733 -- 2.2733)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3613 (0.6594)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (98.9899)  time: 0.4316 (0.2046 -- 2.4859)  data: 0.2117 (0.0008 -- 2.2733)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4978 (0.6239)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2187 (0.1684 -- 0.3022)  data: 0.0102 (0.0001 -- 0.1242)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5785 (0.6734)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (97.0954)  time: 0.2013 (0.1323 -- 0.3022)  data: 0.0093 (0.0001 -- 0.1242)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 80.290 Acc@5 96.888 loss 0.687
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 84.65%
Epoch: [103]  [  0/160]  eta: 0:21:09  lr: 0.000039  min_lr: 0.000010  loss: 1.4577 (1.4577)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9735 (6.9735)  time: 7.9354 (7.9354 -- 7.9354)  data: 5.6967 (5.6967 -- 5.6967)  max mem: 16413
[2023-09-05 01:50:58,484] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16487
[2023-09-05 01:50:58,485] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:50:58,485] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16487
[2023-09-05 01:50:58,485] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:50:58,486] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [103]  [ 20/160]  eta: 0:02:43  lr: 0.000039  min_lr: 0.000010  loss: 1.8318 (1.7458)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0489 (7.1763)  time: 0.8287 (0.5283 -- 3.6975)  data: 0.1680 (0.0007 -- 1.6660)  max mem: 16413
Epoch: [103]  [ 40/160]  eta: 0:02:03  lr: 0.000039  min_lr: 0.000010  loss: 1.8180 (1.7671)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7533 (7.1061)  time: 0.8799 (0.5211 -- 3.3058)  data: 0.1340 (0.0003 -- 1.9560)  max mem: 16413
Epoch: [103]  [ 60/160]  eta: 0:01:38  lr: 0.000039  min_lr: 0.000010  loss: 1.7264 (1.7341)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0481 (7.0794)  time: 0.9079 (0.5278 -- 2.5030)  data: 0.0014 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [103]  [ 80/160]  eta: 0:01:16  lr: 0.000039  min_lr: 0.000010  loss: 1.5966 (1.7106)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4077 (7.0365)  time: 0.8612 (0.5253 -- 3.7177)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000010  loss: 1.7724 (1.7219)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0282 (6.8838)  time: 0.8998 (0.5295 -- 3.4495)  data: 0.0015 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [103]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000010  loss: 1.7799 (1.7104)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8603 (6.9382)  time: 0.7358 (0.5257 -- 2.4620)  data: 0.0014 (0.0002 -- 0.0027)  max mem: 16413
[2023-09-05 01:52:50,989] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:52:50,989] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:52:50,990] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:52:50,990] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [103]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.7394 (1.7069)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8359 (6.8521)  time: 0.8867 (0.5305 -- 2.1382)  data: 0.2133 (0.0002 -- 1.4609)  max mem: 16413
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.7087 (1.7062)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1992 (6.8173)  time: 0.7813 (0.4982 -- 2.4143)  data: 0.1568 (0.0002 -- 1.8943)  max mem: 16413
Epoch: [103] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.7087 (1.6896)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1992 (6.8173)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1748 (0.1748)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4873 (2.4873 -- 2.4873)  data: 2.2312 (2.2312 -- 2.2312)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4220 (0.6792)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4240 (0.2010 -- 2.4873)  data: 0.2102 (0.0006 -- 2.2312)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4306 (0.6101)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2187 (0.1694 -- 0.3511)  data: 0.0137 (0.0001 -- 0.1431)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5161 (0.6517)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (97.0954)  time: 0.2040 (0.1330 -- 0.3511)  data: 0.0135 (0.0001 -- 0.1431)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 82.573 Acc@5 96.680 loss 0.643
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.65%
Epoch: [104]  [  0/160]  eta: 0:22:14  lr: 0.000039  min_lr: 0.000010  loss: 2.0446 (2.0446)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8564 (7.8564)  time: 8.3421 (8.3421 -- 8.3421)  data: 6.4503 (6.4503 -- 6.4503)  max mem: 16413
[2023-09-05 01:53:43,338] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16660
[2023-09-05 01:53:43,338] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16660
[2023-09-05 01:53:43,339] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:53:43,339] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:53:43,339] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [104]  [ 20/160]  eta: 0:03:01  lr: 0.000039  min_lr: 0.000010  loss: 1.6102 (1.6517)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3845 (6.7997)  time: 0.9410 (0.5270 -- 4.4733)  data: 0.0343 (0.0003 -- 0.6598)  max mem: 16413
Epoch: [104]  [ 40/160]  eta: 0:02:17  lr: 0.000039  min_lr: 0.000010  loss: 1.6672 (1.6559)  loss_scale: 16384.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7534 (7.0561)  time: 0.9906 (0.5208 -- 4.1620)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [104]  [ 60/160]  eta: 0:01:41  lr: 0.000039  min_lr: 0.000010  loss: 1.8166 (1.6955)  loss_scale: 16384.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9175 (7.3440)  time: 0.7431 (0.5214 -- 3.5686)  data: 0.0015 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [104]  [ 80/160]  eta: 0:01:17  lr: 0.000039  min_lr: 0.000010  loss: 1.7171 (1.6724)  loss_scale: 16384.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7642 (7.0049)  time: 0.8550 (0.5253 -- 2.8864)  data: 0.0017 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000010  loss: 1.5549 (1.6664)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2468 (6.9332)  time: 0.8442 (0.5220 -- 3.3096)  data: 0.0015 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [104]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000010  loss: 1.9019 (1.6950)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1852 (6.8974)  time: 0.8999 (0.5336 -- 3.4974)  data: 0.0016 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [104]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000010  loss: 1.6097 (1.6951)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3088 (6.8819)  time: 0.9068 (0.5053 -- 4.5787)  data: 0.0015 (0.0002 -- 0.0040)  max mem: 16413
[2023-09-05 01:55:34,939] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:55:34,940] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:55:34,940] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 01:55:34,940] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000010  loss: 1.6612 (1.6918)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3033 (6.9617)  time: 0.6476 (0.4959 -- 2.6002)  data: 0.0006 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [104] Total time: 0:02:24 (0.9023 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000010  loss: 1.6612 (1.6927)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3033 (6.9617)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1706 (0.1706)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4832 (2.4832 -- 2.4832)  data: 2.2097 (2.2097 -- 2.2097)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3919 (0.6756)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4248 (0.2093 -- 2.4832)  data: 0.2026 (0.0003 -- 2.2097)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4898 (0.6169)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.8836)  time: 0.2164 (0.1696 -- 0.3975)  data: 0.0116 (0.0001 -- 0.2105)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5411 (0.6493)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2000 (0.1332 -- 0.3975)  data: 0.0109 (0.0001 -- 0.2105)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 83.817 Acc@5 96.888 loss 0.656
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.65%
Epoch: [105]  [  0/160]  eta: 0:26:00  lr: 0.000039  min_lr: 0.000010  loss: 1.5063 (1.5063)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3263 (5.3263)  time: 9.7533 (9.7533 -- 9.7533)  data: 9.2083 (9.2083 -- 9.2083)  max mem: 16413
Epoch: [105]  [ 20/160]  eta: 0:02:57  lr: 0.000038  min_lr: 0.000010  loss: 1.5518 (1.4838)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1675 (6.9348)  time: 0.8409 (0.5219 -- 3.9515)  data: 0.2855 (0.0004 -- 3.4118)  max mem: 16413
Epoch: [105]  [ 40/160]  eta: 0:02:05  lr: 0.000038  min_lr: 0.000010  loss: 1.8506 (1.6438)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2002 (7.1478)  time: 0.8215 (0.5359 -- 2.4821)  data: 0.2277 (0.0004 -- 1.9439)  max mem: 16413
Epoch: [105]  [ 60/160]  eta: 0:01:38  lr: 0.000038  min_lr: 0.000010  loss: 1.7166 (1.6663)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0022 (7.1474)  time: 0.8679 (0.5349 -- 2.7888)  data: 0.2282 (0.0003 -- 1.6181)  max mem: 16413
Epoch: [105]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000010  loss: 1.8499 (1.6903)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5260 (7.0023)  time: 0.7796 (0.5137 -- 2.0631)  data: 0.0875 (0.0005 -- 1.1459)  max mem: 16413
Epoch: [105]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.5622 (1.6677)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2901 (6.9321)  time: 0.9186 (0.5288 -- 3.1692)  data: 0.1149 (0.0005 -- 0.7835)  max mem: 16413
[2023-09-05 01:57:37,982] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:57:37,982] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 01:57:37,982] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 01:57:37,982] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [105]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.6085 (1.6693)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4292 (6.7747)  time: 0.8487 (0.5300 -- 3.3152)  data: 0.2340 (0.0003 -- 2.7825)  max mem: 16413
[2023-09-05 01:57:44,182] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16924
[2023-09-05 01:57:44,182] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 01:57:44,182] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-05 01:57:44,182] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16924
[2023-09-05 01:57:44,182] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [105]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.5353 (1.6737)  loss_scale: 32768.0000 (34394.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8709 (6.9557)  time: 0.8740 (0.5091 -- 3.0022)  data: 0.3016 (0.0004 -- 2.4099)  max mem: 16413
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.7843 (1.6790)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0003 (7.0288)  time: 0.6675 (0.4943 -- 1.6838)  data: 0.0602 (0.0002 -- 0.8483)  max mem: 16413
Epoch: [105] Total time: 0:02:21 (0.8850 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.7843 (1.6821)  loss_scale: 32768.0000 (34201.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0003 (7.0288)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1795 (0.1795)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3366 (2.3366 -- 2.3366)  data: 2.0953 (2.0953 -- 2.0953)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4339 (0.6855)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4278 (0.2023 -- 2.3366)  data: 0.2085 (0.0007 -- 2.0953)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4478 (0.6344)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2271 (0.1706 -- 0.4237)  data: 0.0207 (0.0001 -- 0.2137)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6196 (0.6877)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (97.5104)  time: 0.2099 (0.1324 -- 0.4237)  data: 0.0204 (0.0001 -- 0.2137)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.660
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.65%
Epoch: [106]  [  0/160]  eta: 0:15:47  lr: 0.000038  min_lr: 0.000010  loss: 1.2307 (1.2307)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3625 (10.3625)  time: 5.9217 (5.9217 -- 5.9217)  data: 5.3618 (5.3618 -- 5.3618)  max mem: 16413
Epoch: [106]  [ 20/160]  eta: 0:02:41  lr: 0.000038  min_lr: 0.000010  loss: 1.7062 (1.6960)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3922 (7.1053)  time: 0.9116 (0.5183 -- 2.3565)  data: 0.1604 (0.0004 -- 1.0515)  max mem: 16413
[2023-09-05 01:58:57,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=96, lr=[9.722655244828146e-06, 9.722655244828146e-06, 1.0802950272031275e-05, 1.0802950272031275e-05, 1.2003278080034748e-05, 1.2003278080034748e-05, 1.3336975644483054e-05, 1.3336975644483054e-05, 1.4818861827203392e-05, 1.4818861827203392e-05, 1.6465402030225993e-05, 1.6465402030225993e-05, 1.8294891144695544e-05, 1.8294891144695544e-05, 2.0327656827439494e-05, 2.0327656827439494e-05, 2.258628536382166e-05, 2.258628536382166e-05, 2.5095872626468507e-05, 2.5095872626468507e-05, 2.7884302918298344e-05, 2.7884302918298344e-05, 3.098255879810927e-05, 3.098255879810927e-05, 3.442506533123252e-05, 3.442506533123252e-05, 3.825007259025836e-05, 3.825007259025836e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 01:58:57,936] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=17.458086164344508, CurrSamplesPerSec=21.568370739282802, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:01:59  lr: 0.000038  min_lr: 0.000010  loss: 1.8399 (1.7594)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7731 (7.1159)  time: 0.8294 (0.5303 -- 2.6361)  data: 0.1885 (0.0003 -- 1.4216)  max mem: 16413
[2023-09-05 01:59:09,849] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17013
[2023-09-05 01:59:09,849] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17013
[2023-09-05 01:59:09,849] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:59:09,849] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 01:59:09,849] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [106]  [ 60/160]  eta: 0:01:36  lr: 0.000038  min_lr: 0.000010  loss: 1.6828 (1.7355)  loss_scale: 32768.0000 (30619.2787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8668 (7.1648)  time: 0.9052 (0.5221 -- 2.3180)  data: 0.1658 (0.0002 -- 1.1981)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000010  loss: 1.4887 (1.6966)  loss_scale: 16384.0000 (27104.3951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6714 (7.1611)  time: 0.8576 (0.5423 -- 3.8484)  data: 0.2609 (0.0007 -- 3.3261)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:55  lr: 0.000038  min_lr: 0.000010  loss: 1.6784 (1.6750)  loss_scale: 16384.0000 (24981.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4428 (7.1106)  time: 0.9120 (0.5341 -- 3.2007)  data: 0.3435 (0.0004 -- 2.6528)  max mem: 16413
Epoch: [106]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.5531 (1.6617)  loss_scale: 16384.0000 (23560.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6267 (6.9807)  time: 0.8353 (0.5374 -- 4.4957)  data: 0.2779 (0.0002 -- 3.9671)  max mem: 16413
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.7713 (1.6791)  loss_scale: 16384.0000 (22542.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9357 (7.0225)  time: 1.0825 (0.5119 -- 4.9687)  data: 0.5393 (0.0004 -- 4.4736)  max mem: 16413
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.7118 (1.6858)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1440 (7.0020)  time: 0.5637 (0.4953 -- 1.1606)  data: 0.0532 (0.0002 -- 0.6653)  max mem: 16413
Epoch: [106] Total time: 0:02:23 (0.8961 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.7118 (1.7059)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1440 (7.0020)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2012 (0.2012)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4696 (2.4696 -- 2.4696)  data: 2.2429 (2.2429 -- 2.2429)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5428 (0.6873)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4248 (0.2034 -- 2.4696)  data: 0.2054 (0.0007 -- 2.2429)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5428 (0.6539)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (97.8836)  time: 0.2181 (0.1689 -- 0.3038)  data: 0.0065 (0.0001 -- 0.1105)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6670 (0.7035)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (97.5104)  time: 0.2028 (0.1329 -- 0.3038)  data: 0.0060 (0.0001 -- 0.1105)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 81.950 Acc@5 97.718 loss 0.673
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 84.65%
Epoch: [107]  [  0/160]  eta: 0:18:31  lr: 0.000038  min_lr: 0.000010  loss: 1.4076 (1.4076)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1352 (6.1352)  time: 6.9484 (6.9484 -- 6.9484)  data: 6.4275 (6.4275 -- 6.4275)  max mem: 16413
Epoch: [107]  [ 20/160]  eta: 0:02:41  lr: 0.000038  min_lr: 0.000010  loss: 1.5798 (1.6579)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9012 (7.0621)  time: 0.8673 (0.5211 -- 4.0713)  data: 0.3018 (0.0003 -- 3.5518)  max mem: 16413
[2023-09-05 02:01:14,398] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:01:14,398] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:01:14,398] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:01:14,398] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:01:25,918] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17152
[2023-09-05 02:01:25,918] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17152
[2023-09-05 02:01:25,918] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:01:25,918] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:01:25,918] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [107]  [ 40/160]  eta: 0:02:09  lr: 0.000038  min_lr: 0.000010  loss: 1.7859 (1.7303)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4333 (6.7764)  time: 0.9915 (0.5235 -- 4.6261)  data: 0.4424 (0.0003 -- 4.1053)  max mem: 16413
Epoch: [107]  [ 60/160]  eta: 0:01:38  lr: 0.000038  min_lr: 0.000010  loss: 1.7437 (1.7403)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1911 (6.9034)  time: 0.7916 (0.5254 -- 3.6550)  data: 0.2331 (0.0006 -- 3.1170)  max mem: 16413
Epoch: [107]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000010  loss: 1.5724 (1.7103)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6377 (6.9588)  time: 0.8398 (0.5223 -- 3.2389)  data: 0.2810 (0.0006 -- 2.6955)  max mem: 16413
[2023-09-05 02:02:17,939] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17212
[2023-09-05 02:02:17,939] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17212
[2023-09-05 02:02:17,940] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 02:02:17,940] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 02:02:17,940] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [107]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.6652 (1.7323)  loss_scale: 16384.0000 (17276.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8768 (6.8736)  time: 0.9012 (0.5167 -- 3.6807)  data: 0.3508 (0.0004 -- 3.1428)  max mem: 16413
Epoch: [107]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000010  loss: 1.5621 (1.7202)  loss_scale: 8192.0000 (15774.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3691 (6.7962)  time: 0.7713 (0.5361 -- 2.4950)  data: 0.2178 (0.0004 -- 1.9763)  max mem: 16413
Epoch: [107]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.7429 (1.7282)  loss_scale: 8192.0000 (14699.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3776 (6.9761)  time: 1.0183 (0.5436 -- 4.3202)  data: 0.3843 (0.0004 -- 3.7814)  max mem: 16413
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.6331 (1.7068)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0763 (6.9902)  time: 0.5874 (0.4963 -- 1.2284)  data: 0.0716 (0.0002 -- 0.7183)  max mem: 16413
Epoch: [107] Total time: 0:02:21 (0.8863 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.6331 (1.6835)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0763 (6.9902)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1615 (0.1615)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3683 (2.3683 -- 2.3683)  data: 2.1416 (2.1416 -- 2.1416)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4004 (0.6600)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (96.9697)  time: 0.4294 (0.1982 -- 2.3683)  data: 0.2087 (0.0006 -- 2.1416)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5151 (0.6332)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2199 (0.1696 -- 0.3854)  data: 0.0120 (0.0001 -- 0.1450)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5931 (0.6762)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.2038 (0.1326 -- 0.3854)  data: 0.0118 (0.0001 -- 0.1450)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 82.365 Acc@5 96.888 loss 0.646
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 84.65%
Epoch: [108]  [  0/160]  eta: 0:20:38  lr: 0.000038  min_lr: 0.000010  loss: 1.7430 (1.7430)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5487 (6.5487)  time: 7.7386 (7.7386 -- 7.7386)  data: 5.3587 (5.3587 -- 5.3587)  max mem: 16413
Epoch: [108]  [ 20/160]  eta: 0:02:44  lr: 0.000038  min_lr: 0.000010  loss: 1.7348 (1.7299)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2087 (7.5193)  time: 0.8490 (0.5339 -- 3.0728)  data: 0.1267 (0.0003 -- 2.4980)  max mem: 16413
Epoch: [108]  [ 40/160]  eta: 0:02:07  lr: 0.000038  min_lr: 0.000010  loss: 1.7904 (1.7569)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8449 (7.3797)  time: 0.9390 (0.5372 -- 3.7341)  data: 0.0398 (0.0003 -- 0.4210)  max mem: 16413
Epoch: [108]  [ 60/160]  eta: 0:01:40  lr: 0.000038  min_lr: 0.000010  loss: 1.7581 (1.7460)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6601 (7.2123)  time: 0.9033 (0.5262 -- 2.9341)  data: 0.0013 (0.0003 -- 0.0027)  max mem: 16413
[2023-09-05 02:04:20,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:04:20,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 02:04:20,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:04:20,666] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [108]  [ 80/160]  eta: 0:01:17  lr: 0.000038  min_lr: 0.000010  loss: 1.6339 (1.7372)  loss_scale: 16384.0000 (10214.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4563 (7.1757)  time: 0.8665 (0.5137 -- 4.3747)  data: 0.0011 (0.0001 -- 0.0047)  max mem: 16413
Epoch: [108]  [100/160]  eta: 0:00:58  lr: 0.000038  min_lr: 0.000010  loss: 1.6843 (1.7168)  loss_scale: 16384.0000 (11436.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3691 (7.0826)  time: 0.9432 (0.5334 -- 3.7080)  data: 0.0022 (0.0002 -- 0.0132)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:37  lr: 0.000038  min_lr: 0.000010  loss: 1.7269 (1.7223)  loss_scale: 16384.0000 (12254.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8457 (7.1107)  time: 0.7761 (0.5269 -- 2.7788)  data: 0.0011 (0.0004 -- 0.0021)  max mem: 16413
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000010  loss: 1.8415 (1.7443)  loss_scale: 16384.0000 (12839.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4963 (7.1217)  time: 0.7865 (0.5373 -- 2.6741)  data: 0.0020 (0.0005 -- 0.0050)  max mem: 16413
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000010  loss: 1.5910 (1.7365)  loss_scale: 16384.0000 (13260.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8233 (7.0029)  time: 0.7209 (0.4943 -- 2.9821)  data: 0.0363 (0.0002 -- 0.6960)  max mem: 16413
Epoch: [108] Total time: 0:02:22 (0.8931 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000010  loss: 1.5910 (1.7084)  loss_scale: 16384.0000 (13260.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8233 (7.0029)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1757 (0.1757)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4173 (2.4173 -- 2.4173)  data: 2.1672 (2.1672 -- 2.1672)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3995 (0.6046)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4169 (0.1961 -- 2.4173)  data: 0.1980 (0.0005 -- 2.1672)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4612 (0.5690)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (98.4127)  time: 0.2211 (0.1691 -- 0.4640)  data: 0.0143 (0.0001 -- 0.2732)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5095 (0.6083)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (98.3402)  time: 0.2023 (0.1327 -- 0.4640)  data: 0.0141 (0.0001 -- 0.2732)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 83.402 Acc@5 98.133 loss 0.610
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.65%
Epoch: [109]  [  0/160]  eta: 0:18:37  lr: 0.000038  min_lr: 0.000010  loss: 1.9801 (1.9801)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0645 (5.0645)  time: 6.9842 (6.9842 -- 6.9842)  data: 5.9020 (5.9020 -- 5.9020)  max mem: 16413
Epoch: [109]  [ 20/160]  eta: 0:02:40  lr: 0.000038  min_lr: 0.000010  loss: 1.6062 (1.6584)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0235 (6.7543)  time: 0.8567 (0.5234 -- 2.7062)  data: 0.0017 (0.0004 -- 0.0057)  max mem: 16413
[2023-09-05 02:06:20,475] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:06:20,476] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:06:20,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:06:20,477] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [ 40/160]  eta: 0:02:01  lr: 0.000038  min_lr: 0.000010  loss: 1.9340 (1.7351)  loss_scale: 32768.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6833 (6.2361)  time: 0.8700 (0.5316 -- 3.1327)  data: 0.2574 (0.0003 -- 2.5864)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:37  lr: 0.000038  min_lr: 0.000010  loss: 1.8368 (1.7343)  loss_scale: 32768.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8761 (6.1661)  time: 0.9128 (0.5294 -- 3.4356)  data: 0.2017 (0.0002 -- 1.8654)  max mem: 16413
Epoch: [109]  [ 80/160]  eta: 0:01:16  lr: 0.000038  min_lr: 0.000010  loss: 1.8867 (1.7497)  loss_scale: 32768.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9549 (6.2865)  time: 0.8948 (0.5243 -- 3.4514)  data: 0.3109 (0.0003 -- 2.4653)  max mem: 16413
Epoch: [109]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000010  loss: 1.7853 (1.7362)  loss_scale: 32768.0000 (28063.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8403 (6.2881)  time: 0.8744 (0.5171 -- 4.1500)  data: 0.3161 (0.0006 -- 3.6136)  max mem: 16413
[2023-09-05 02:07:31,022] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17549
[2023-09-05 02:07:31,022] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17549
[2023-09-05 02:07:31,022] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:07:31,022] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:07:31,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [109]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000010  loss: 1.7611 (1.7276)  loss_scale: 16384.0000 (27216.3967)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6316 (6.3375)  time: 0.8566 (0.5300 -- 3.4651)  data: 0.3089 (0.0007 -- 2.9463)  max mem: 16413
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000010  loss: 1.7903 (1.7349)  loss_scale: 16384.0000 (25679.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4028 (6.4623)  time: 0.8677 (0.5143 -- 2.3174)  data: 0.3093 (0.0003 -- 1.7794)  max mem: 16413
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000010  loss: 1.6155 (1.7316)  loss_scale: 16384.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5854 (6.5408)  time: 0.7575 (0.4944 -- 2.2540)  data: 0.2378 (0.0002 -- 1.7381)  max mem: 16413
Epoch: [109] Total time: 0:02:22 (0.8933 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000010  loss: 1.6155 (1.6912)  loss_scale: 16384.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5854 (6.5408)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1416 (0.1416)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4054 (2.4054 -- 2.4054)  data: 2.1334 (2.1334 -- 2.1334)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3831 (0.6302)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (100.0000)  time: 0.4159 (0.1934 -- 2.4054)  data: 0.1975 (0.0007 -- 2.1334)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3831 (0.5860)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (98.9418)  time: 0.2253 (0.1739 -- 0.5317)  data: 0.0190 (0.0001 -- 0.3380)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4891 (0.6280)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (98.3402)  time: 0.2100 (0.1325 -- 0.5317)  data: 0.0187 (0.0001 -- 0.3380)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 84.232 Acc@5 98.340 loss 0.613
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.65%
Epoch: [110]  [  0/160]  eta: 0:18:40  lr: 0.000037  min_lr: 0.000010  loss: 1.9081 (1.9081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9657 (6.9657)  time: 7.0041 (7.0041 -- 7.0041)  data: 3.8734 (3.8734 -- 3.8734)  max mem: 16413
Epoch: [110]  [ 20/160]  eta: 0:02:49  lr: 0.000037  min_lr: 0.000010  loss: 1.7393 (1.7515)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0512 (6.4990)  time: 0.9241 (0.5234 -- 4.2488)  data: 0.0195 (0.0003 -- 0.3622)  max mem: 16413
Epoch: [110]  [ 40/160]  eta: 0:02:07  lr: 0.000037  min_lr: 0.000010  loss: 1.6329 (1.7505)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9245 (6.6034)  time: 0.8994 (0.5289 -- 3.5784)  data: 0.0015 (0.0003 -- 0.0076)  max mem: 16413
Epoch: [110]  [ 60/160]  eta: 0:01:41  lr: 0.000037  min_lr: 0.000009  loss: 1.6322 (1.7390)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1374 (6.5290)  time: 0.9186 (0.5245 -- 2.8189)  data: 0.0010 (0.0004 -- 0.0019)  max mem: 16413
[2023-09-05 02:09:35,650] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:09:35,650] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:09:35,652] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:09:35,652] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [110]  [ 80/160]  eta: 0:01:17  lr: 0.000037  min_lr: 0.000009  loss: 1.8079 (1.7473)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9119 (6.6201)  time: 0.8563 (0.5352 -- 3.3911)  data: 0.0016 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [110]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.4114 (1.7181)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6318 (6.6916)  time: 0.7845 (0.5261 -- 2.6318)  data: 0.0017 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [110]  [120/160]  eta: 0:00:38  lr: 0.000037  min_lr: 0.000009  loss: 1.5441 (1.7048)  loss_scale: 32768.0000 (22206.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6262 (6.5806)  time: 1.0628 (0.5259 -- 4.8540)  data: 0.0019 (0.0005 -- 0.0083)  max mem: 16413
[2023-09-05 02:10:32,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17740
[2023-09-05 02:10:32,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:10:32,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 02:10:32,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17740
[2023-09-05 02:10:32,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.6525 (1.6958)  loss_scale: 32768.0000 (23588.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9060 (6.7176)  time: 0.8178 (0.5228 -- 3.6703)  data: 0.0015 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.8295 (1.7093)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2323 (6.7045)  time: 0.6247 (0.4948 -- 2.5729)  data: 0.0009 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [110] Total time: 0:02:24 (0.9016 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.8295 (1.6952)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2323 (6.7045)
Val:  [ 0/27]  eta: 0:00:54  loss: 0.1658 (0.1658)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0299 (2.0299 -- 2.0299)  data: 1.7793 (1.7793 -- 1.7793)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5275 (0.7182)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (100.0000)  time: 0.4106 (0.2062 -- 2.0299)  data: 0.1902 (0.0009 -- 1.7793)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5363 (0.6815)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (98.4127)  time: 0.2370 (0.1732 -- 0.4312)  data: 0.0296 (0.0001 -- 0.2070)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6040 (0.6866)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (98.3402)  time: 0.2166 (0.1331 -- 0.4312)  data: 0.0249 (0.0001 -- 0.2070)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 83.817 Acc@5 98.133 loss 0.649
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.65%
Epoch: [111]  [  0/160]  eta: 0:20:36  lr: 0.000037  min_lr: 0.000009  loss: 1.6799 (1.6799)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4716 (6.4716)  time: 7.7303 (7.7303 -- 7.7303)  data: 7.1864 (7.1864 -- 7.1864)  max mem: 16413
Epoch: [111]  [ 20/160]  eta: 0:02:54  lr: 0.000037  min_lr: 0.000009  loss: 1.6416 (1.6529)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7293 (6.7620)  time: 0.9238 (0.5202 -- 4.8445)  data: 0.3726 (0.0003 -- 4.3061)  max mem: 16413
Epoch: [111]  [ 40/160]  eta: 0:02:08  lr: 0.000037  min_lr: 0.000009  loss: 1.7026 (1.6286)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5095 (6.6264)  time: 0.8829 (0.5123 -- 4.3112)  data: 0.3441 (0.0002 -- 3.8005)  max mem: 16413
Epoch: [111]  [ 60/160]  eta: 0:01:37  lr: 0.000037  min_lr: 0.000009  loss: 1.5719 (1.6168)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2163 (6.5753)  time: 0.7702 (0.5182 -- 3.7909)  data: 0.2260 (0.0001 -- 3.2522)  max mem: 16413
Epoch: [111]  [ 80/160]  eta: 0:01:14  lr: 0.000037  min_lr: 0.000009  loss: 1.8159 (1.6495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1180 (6.5244)  time: 0.8016 (0.5300 -- 3.1890)  data: 0.2461 (0.0001 -- 2.6312)  max mem: 16413
Epoch: [111]  [100/160]  eta: 0:00:55  lr: 0.000037  min_lr: 0.000009  loss: 1.6694 (1.6562)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6031 (6.5874)  time: 0.9176 (0.5086 -- 2.2663)  data: 0.2709 (0.0005 -- 1.7209)  max mem: 16413
[2023-09-05 02:12:33,707] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:12:33,708] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:12:33,708] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:12:33,708] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000009  loss: 1.6942 (1.6515)  loss_scale: 32768.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5871 (6.7038)  time: 0.8383 (0.5279 -- 2.7140)  data: 0.0759 (0.0006 -- 0.5520)  max mem: 16413
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.7410 (1.6589)  loss_scale: 32768.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2661 (6.7878)  time: 0.9035 (0.5344 -- 2.4158)  data: 0.1655 (0.0004 -- 1.8724)  max mem: 16413
[2023-09-05 02:13:11,395] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17912
[2023-09-05 02:13:11,395] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17912
[2023-09-05 02:13:11,395] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:13:11,396] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:13:11,396] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.6334 (1.6570)  loss_scale: 32768.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7680 (6.8068)  time: 0.7328 (0.4843 -- 4.0015)  data: 0.1756 (0.0002 -- 3.4985)  max mem: 16413
Epoch: [111] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.6334 (1.6848)  loss_scale: 32768.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7680 (6.8068)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1766 (0.1766)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2802 (2.2802 -- 2.2802)  data: 2.0628 (2.0628 -- 2.0628)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4541 (0.6328)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4119 (0.2059 -- 2.2802)  data: 0.1938 (0.0007 -- 2.0628)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5232 (0.6018)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (98.4127)  time: 0.2292 (0.1691 -- 0.6201)  data: 0.0252 (0.0001 -- 0.4326)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5702 (0.6319)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (97.9253)  time: 0.2122 (0.1326 -- 0.6201)  data: 0.0250 (0.0001 -- 0.4326)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 82.988 Acc@5 97.925 loss 0.629
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.65%
Epoch: [112]  [  0/160]  eta: 0:23:15  lr: 0.000037  min_lr: 0.000009  loss: 1.6366 (1.6366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0418 (8.0418)  time: 8.7190 (8.7190 -- 8.7190)  data: 8.1847 (8.1847 -- 8.1847)  max mem: 16413
Epoch: [112]  [ 20/160]  eta: 0:02:47  lr: 0.000037  min_lr: 0.000009  loss: 1.7701 (1.6506)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6109 (7.2914)  time: 0.8179 (0.5328 -- 3.0151)  data: 0.1336 (0.0005 -- 1.4033)  max mem: 16413
Epoch: [112]  [ 40/160]  eta: 0:02:00  lr: 0.000037  min_lr: 0.000009  loss: 1.6171 (1.6644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5915 (6.9538)  time: 0.8114 (0.5300 -- 2.2096)  data: 0.0798 (0.0001 -- 0.8291)  max mem: 16413
Epoch: [112]  [ 60/160]  eta: 0:01:36  lr: 0.000037  min_lr: 0.000009  loss: 1.8395 (1.6800)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5245 (6.5283)  time: 0.8770 (0.5329 -- 2.2942)  data: 0.0837 (0.0004 -- 1.0464)  max mem: 16413
[2023-09-05 02:14:36,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=102, lr=[9.377473471984982e-06, 9.377473471984982e-06, 1.0419414968872201e-05, 1.0419414968872201e-05, 1.1577127743191333e-05, 1.1577127743191333e-05, 1.2863475270212593e-05, 1.2863475270212593e-05, 1.4292750300236215e-05, 1.4292750300236215e-05, 1.5880833666929127e-05, 1.5880833666929127e-05, 1.7645370741032364e-05, 1.7645370741032364e-05, 1.9605967490035958e-05, 1.9605967490035958e-05, 2.1784408322262175e-05, 2.1784408322262175e-05, 2.420489813584686e-05, 2.420489813584686e-05, 2.6894331262052067e-05, 2.6894331262052067e-05, 2.9882590291168965e-05, 2.9882590291168965e-05, 3.3202878101298845e-05, 3.3202878101298845e-05, 3.689208677922094e-05, 3.689208677922094e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 02:14:36,772] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=17.47621082120367, CurrSamplesPerSec=23.802184924578437, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [112]  [ 80/160]  eta: 0:01:17  lr: 0.000037  min_lr: 0.000009  loss: 1.6797 (1.6792)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2612 (6.4739)  time: 0.9995 (0.5178 -- 4.8228)  data: 0.1599 (0.0004 -- 2.7333)  max mem: 16413
Epoch: [112]  [100/160]  eta: 0:00:57  lr: 0.000037  min_lr: 0.000009  loss: 1.7984 (1.6980)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4838 (6.6432)  time: 0.8785 (0.5253 -- 4.7398)  data: 0.0017 (0.0005 -- 0.0039)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000009  loss: 1.4980 (1.6685)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7308 (6.6552)  time: 0.9073 (0.5210 -- 3.3966)  data: 0.0019 (0.0001 -- 0.0066)  max mem: 16413
[2023-09-05 02:15:17,849] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:15:17,849] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:15:17,849] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:15:17,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [112]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.6944 (1.6628)  loss_scale: 32768.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7205 (6.7485)  time: 0.8240 (0.5223 -- 3.4030)  data: 0.0011 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.7770 (1.6705)  loss_scale: 32768.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5590 (6.7348)  time: 0.6276 (0.4968 -- 2.6690)  data: 0.0006 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [112] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.7770 (1.6651)  loss_scale: 32768.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5590 (6.7348)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1783 (0.1783)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3331 (2.3331 -- 2.3331)  data: 2.0918 (2.0918 -- 2.0918)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5135 (0.6304)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4134 (0.2066 -- 2.3331)  data: 0.1912 (0.0009 -- 2.0918)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5135 (0.6288)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (98.4127)  time: 0.2226 (0.1692 -- 0.4811)  data: 0.0156 (0.0001 -- 0.2976)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6153 (0.6585)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (98.3402)  time: 0.2046 (0.1333 -- 0.4811)  data: 0.0153 (0.0001 -- 0.2976)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.630
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.65%
Epoch: [113]  [  0/160]  eta: 0:19:35  lr: 0.000037  min_lr: 0.000009  loss: 0.9626 (0.9626)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5106 (6.5106)  time: 7.3473 (7.3473 -- 7.3473)  data: 6.5398 (6.5398 -- 6.5398)  max mem: 16413
Epoch: [113]  [ 20/160]  eta: 0:02:45  lr: 0.000037  min_lr: 0.000009  loss: 1.7957 (1.7874)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7912 (6.9255)  time: 0.8725 (0.5337 -- 4.5692)  data: 0.2780 (0.0004 -- 4.0268)  max mem: 16413
Epoch: [113]  [ 40/160]  eta: 0:02:05  lr: 0.000037  min_lr: 0.000009  loss: 1.6652 (1.7070)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3032 (6.7555)  time: 0.9096 (0.5193 -- 4.1505)  data: 0.3618 (0.0006 -- 3.6428)  max mem: 16413
Epoch: [113]  [ 60/160]  eta: 0:01:36  lr: 0.000037  min_lr: 0.000009  loss: 1.5697 (1.7076)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7231 (6.8995)  time: 0.7793 (0.5357 -- 3.2816)  data: 0.1979 (0.0005 -- 2.7502)  max mem: 16413
[2023-09-05 02:17:07,727] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18156
[2023-09-05 02:17:07,727] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:17:07,727] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18156
[2023-09-05 02:17:07,727] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:17:07,728] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [113]  [ 80/160]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000009  loss: 1.8192 (1.7358)  loss_scale: 32768.0000 (31756.6420)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2170 (6.9686)  time: 0.9288 (0.5267 -- 3.7654)  data: 0.1380 (0.0003 -- 1.8372)  max mem: 16413
Epoch: [113]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000009  loss: 1.7245 (1.7366)  loss_scale: 16384.0000 (28712.5545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3922 (7.0210)  time: 0.8571 (0.5233 -- 3.1693)  data: 0.0017 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000009  loss: 1.4275 (1.7066)  loss_scale: 16384.0000 (26674.7769)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6885 (7.0232)  time: 0.9172 (0.5268 -- 4.3219)  data: 0.0014 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000009  loss: 1.7207 (1.7147)  loss_scale: 16384.0000 (25215.0922)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3119 (7.1275)  time: 0.9761 (0.5255 -- 4.5863)  data: 0.0021 (0.0004 -- 0.0076)  max mem: 16413
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000009  loss: 1.8901 (1.7315)  loss_scale: 16384.0000 (24166.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2880 (7.1538)  time: 0.6204 (0.4954 -- 1.7863)  data: 0.0017 (0.0002 -- 0.0173)  max mem: 16413
Epoch: [113] Total time: 0:02:24 (0.9004 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000009  loss: 1.8901 (1.6948)  loss_scale: 16384.0000 (24166.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2880 (7.1538)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1899 (0.1899)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3473 (2.3473 -- 2.3473)  data: 2.1340 (2.1340 -- 2.1340)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3460 (0.6474)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4325 (0.2018 -- 2.3473)  data: 0.2072 (0.0007 -- 2.1340)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6155 (0.6387)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2192 (0.1692 -- 0.3799)  data: 0.0080 (0.0001 -- 0.1206)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6223 (0.6717)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.5104)  time: 0.2020 (0.1330 -- 0.3799)  data: 0.0075 (0.0001 -- 0.1206)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.652
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.65%
Epoch: [114]  [  0/160]  eta: 0:19:13  lr: 0.000037  min_lr: 0.000009  loss: 1.7198 (1.7198)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7352 (6.7352)  time: 7.2098 (7.2098 -- 7.2098)  data: 4.7254 (4.7254 -- 4.7254)  max mem: 16413
Epoch: [114]  [ 20/160]  eta: 0:02:39  lr: 0.000037  min_lr: 0.000009  loss: 1.6979 (1.7050)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6395 (8.3888)  time: 0.8334 (0.5180 -- 2.5601)  data: 0.0843 (0.0003 -- 1.6533)  max mem: 16413
Epoch: [114]  [ 40/160]  eta: 0:02:03  lr: 0.000036  min_lr: 0.000009  loss: 1.7509 (1.7346)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0943 (7.8175)  time: 0.9158 (0.5334 -- 3.6405)  data: 0.0124 (0.0004 -- 0.1510)  max mem: 16413
[2023-09-05 02:19:13,679] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:19:13,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:19:13,680] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:19:13,680] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [114]  [ 60/160]  eta: 0:01:38  lr: 0.000036  min_lr: 0.000009  loss: 1.6753 (1.6808)  loss_scale: 32768.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8210 (7.4748)  time: 0.8962 (0.5265 -- 3.8145)  data: 0.0016 (0.0004 -- 0.0067)  max mem: 16413
Epoch: [114]  [ 80/160]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000009  loss: 1.6796 (1.6795)  loss_scale: 32768.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3619 (7.4948)  time: 0.8477 (0.5313 -- 4.5548)  data: 0.0015 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:56  lr: 0.000036  min_lr: 0.000009  loss: 1.6310 (1.6780)  loss_scale: 32768.0000 (25468.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2209 (7.4902)  time: 0.8998 (0.5365 -- 3.3123)  data: 0.2360 (0.0004 -- 2.7594)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:36  lr: 0.000036  min_lr: 0.000009  loss: 1.5983 (1.6726)  loss_scale: 32768.0000 (26674.7769)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4184 (7.3587)  time: 0.7633 (0.5200 -- 2.1024)  data: 0.0740 (0.0005 -- 1.2464)  max mem: 16413
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.7448 (1.6944)  loss_scale: 32768.0000 (27539.0638)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7592 (7.2995)  time: 0.9414 (0.5277 -- 4.8040)  data: 0.3098 (0.0003 -- 4.2676)  max mem: 16413
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.7358 (1.6986)  loss_scale: 32768.0000 (28160.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1038 (7.3234)  time: 0.7190 (0.4935 -- 3.6589)  data: 0.1927 (0.0002 -- 3.1369)  max mem: 16413
Epoch: [114] Total time: 0:02:23 (0.8938 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.7358 (1.6838)  loss_scale: 32768.0000 (28160.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1038 (7.3234)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1458 (0.1458)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4311 (2.4311 -- 2.4311)  data: 2.1835 (2.1835 -- 2.1835)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3779 (0.5970)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4350 (0.1987 -- 2.4311)  data: 0.2182 (0.0007 -- 2.1835)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4310 (0.5750)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.8836)  time: 0.2214 (0.1705 -- 0.4399)  data: 0.0143 (0.0001 -- 0.2070)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5739 (0.6105)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.5104)  time: 0.2056 (0.1329 -- 0.4399)  data: 0.0140 (0.0001 -- 0.2070)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 85.270 Acc@5 97.510 loss 0.604
Accuracy of the network on the 482 val images: 85.27%
[2023-09-05 02:20:56,304] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 02:20:56,306] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 02:20:56,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 02:20:56,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 02:20:57,711] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 02:20:57,711] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.27%
Epoch: [115]  [  0/160]  eta: 0:22:30  lr: 0.000036  min_lr: 0.000009  loss: 1.4964 (1.4964)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2425 (5.2425)  time: 8.4389 (8.4389 -- 8.4389)  data: 5.7715 (5.7715 -- 5.7715)  max mem: 16413
[2023-09-05 02:21:17,374] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:21:17,374] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 02:21:17,377] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:21:17,378] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 02:21:17,947] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18414
[2023-09-05 02:21:17,947] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 02:21:17,947] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-05 02:21:17,947] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18414
[2023-09-05 02:21:17,947] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [115]  [ 20/160]  eta: 0:02:53  lr: 0.000036  min_lr: 0.000009  loss: 1.5885 (1.6132)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3312 (6.5949)  time: 0.8777 (0.5211 -- 3.2407)  data: 0.0127 (0.0003 -- 0.2268)  max mem: 16413
Epoch: [115]  [ 40/160]  eta: 0:02:07  lr: 0.000036  min_lr: 0.000009  loss: 1.4276 (1.5871)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8687 (6.9079)  time: 0.8828 (0.5231 -- 4.0387)  data: 0.0187 (0.0003 -- 0.3507)  max mem: 16413
Epoch: [115]  [ 60/160]  eta: 0:01:38  lr: 0.000036  min_lr: 0.000009  loss: 1.4663 (1.5895)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1415 (7.1332)  time: 0.8127 (0.5304 -- 2.7782)  data: 0.0015 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000009  loss: 1.6806 (1.6273)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1804 (7.2503)  time: 0.8596 (0.5295 -- 4.0001)  data: 0.0018 (0.0002 -- 0.0077)  max mem: 16413
[2023-09-05 02:22:25,193] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18491
[2023-09-05 02:22:25,193] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18491
[2023-09-05 02:22:25,194] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:22:25,194] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:22:25,194] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [115]  [100/160]  eta: 0:00:55  lr: 0.000036  min_lr: 0.000009  loss: 1.6709 (1.6386)  loss_scale: 16384.0000 (31470.2574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2307 (7.1724)  time: 0.8456 (0.5257 -- 3.4342)  data: 0.0015 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000036  min_lr: 0.000009  loss: 1.6801 (1.6509)  loss_scale: 16384.0000 (28976.6612)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3633 (7.1293)  time: 0.7619 (0.5284 -- 2.5565)  data: 0.0097 (0.0004 -- 0.1654)  max mem: 16413
Epoch: [115]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.6388 (1.6472)  loss_scale: 16384.0000 (27190.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2987 (7.1300)  time: 1.0485 (0.5234 -- 3.8403)  data: 0.2765 (0.0001 -- 3.2612)  max mem: 16413
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.7960 (1.6565)  loss_scale: 16384.0000 (25907.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9256 (7.1667)  time: 0.6049 (0.4962 -- 1.5550)  data: 0.0791 (0.0002 -- 1.0493)  max mem: 16413
Epoch: [115] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.7960 (1.6656)  loss_scale: 16384.0000 (25907.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9256 (7.1667)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1485 (0.1485)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3903 (2.3903 -- 2.3903)  data: 2.1397 (2.1397 -- 2.1397)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4453 (0.6358)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4330 (0.2035 -- 2.3903)  data: 0.2152 (0.0006 -- 2.1397)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4645 (0.5950)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2295 (0.1692 -- 0.4467)  data: 0.0247 (0.0001 -- 0.2633)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6132 (0.6279)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.2154 (0.1326 -- 0.4467)  data: 0.0244 (0.0001 -- 0.2633)  max mem: 16413
Val: Total time: 0:00:07 (0.2949 s / it)
* Acc@1 83.402 Acc@5 97.303 loss 0.620
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 85.27%
Epoch: [116]  [  0/160]  eta: 0:21:10  lr: 0.000036  min_lr: 0.000009  loss: 1.9495 (1.9495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4004 (6.4004)  time: 7.9396 (7.9396 -- 7.9396)  data: 7.3957 (7.3957 -- 7.3957)  max mem: 16413
Epoch: [116]  [ 20/160]  eta: 0:02:43  lr: 0.000036  min_lr: 0.000009  loss: 1.5151 (1.6246)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3459 (6.2525)  time: 0.8314 (0.5104 -- 3.4273)  data: 0.2757 (0.0004 -- 2.8855)  max mem: 16413
[2023-09-05 02:23:59,469] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18587
[2023-09-05 02:23:59,469] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 02:23:59,469] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18587
[2023-09-05 02:23:59,469] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 02:23:59,469] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [116]  [ 40/160]  eta: 0:02:05  lr: 0.000036  min_lr: 0.000009  loss: 1.6892 (1.6805)  loss_scale: 8192.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3014 (6.4865)  time: 0.9083 (0.5189 -- 4.0980)  data: 0.3635 (0.0001 -- 3.5217)  max mem: 16413
Epoch: [116]  [ 60/160]  eta: 0:01:39  lr: 0.000036  min_lr: 0.000009  loss: 1.6775 (1.6740)  loss_scale: 8192.0000 (11817.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5714 (6.6442)  time: 0.9057 (0.5257 -- 2.8295)  data: 0.2874 (0.0006 -- 2.3292)  max mem: 16413
Epoch: [116]  [ 80/160]  eta: 0:01:17  lr: 0.000036  min_lr: 0.000009  loss: 1.6238 (1.6592)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4018 (6.6610)  time: 0.8575 (0.5285 -- 2.7123)  data: 0.0551 (0.0003 -- 0.6402)  max mem: 16413
Epoch: [116]  [100/160]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000009  loss: 1.7661 (1.6694)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5884 (6.6775)  time: 0.9018 (0.5236 -- 3.0816)  data: 0.0443 (0.0007 -- 0.8441)  max mem: 16413
Epoch: [116]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.4482 (1.6480)  loss_scale: 8192.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2855 (6.6610)  time: 0.8017 (0.5364 -- 2.5795)  data: 0.1728 (0.0003 -- 2.0418)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.7545 (1.6582)  loss_scale: 8192.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0498 (6.6641)  time: 0.8890 (0.5335 -- 2.6905)  data: 0.1361 (0.0004 -- 1.5214)  max mem: 16413
[2023-09-05 02:25:49,681] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:25:49,681] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 02:25:49,682] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:25:49,682] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.5914 (1.6512)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2062 (6.6263)  time: 0.7310 (0.4940 -- 2.4376)  data: 0.1455 (0.0002 -- 1.9250)  max mem: 16413
Epoch: [116] Total time: 0:02:23 (0.8982 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.5914 (1.6550)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2062 (6.6263)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1740 (0.1740)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4812 (2.4812 -- 2.4812)  data: 2.2582 (2.2582 -- 2.2582)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5604 (0.6226)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4342 (0.1999 -- 2.4812)  data: 0.2119 (0.0005 -- 2.2582)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4806 (0.5794)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2198 (0.1682 -- 0.3340)  data: 0.0117 (0.0001 -- 0.1581)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5345 (0.6162)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2027 (0.1333 -- 0.3340)  data: 0.0115 (0.0001 -- 0.1581)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 83.610 Acc@5 97.718 loss 0.611
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 85.27%
Epoch: [117]  [  0/160]  eta: 0:17:13  lr: 0.000036  min_lr: 0.000009  loss: 1.4525 (1.4525)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5473 (7.5473)  time: 6.4574 (6.4574 -- 6.4574)  data: 5.9060 (5.9060 -- 5.9060)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:36  lr: 0.000036  min_lr: 0.000009  loss: 1.6887 (1.6241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9114 (6.9040)  time: 0.8478 (0.5312 -- 2.6497)  data: 0.0470 (0.0007 -- 0.4867)  max mem: 16413
Epoch: [117]  [ 40/160]  eta: 0:02:02  lr: 0.000036  min_lr: 0.000009  loss: 1.7412 (1.6457)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5692 (7.1915)  time: 0.9234 (0.5402 -- 2.4438)  data: 0.1586 (0.0005 -- 1.8776)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:36  lr: 0.000036  min_lr: 0.000009  loss: 1.8180 (1.7094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2662 (7.4137)  time: 0.8412 (0.5299 -- 2.1280)  data: 0.1152 (0.0003 -- 1.3095)  max mem: 16413
Epoch: [117]  [ 80/160]  eta: 0:01:15  lr: 0.000036  min_lr: 0.000009  loss: 1.7926 (1.7174)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0436 (7.2776)  time: 0.8694 (0.5240 -- 3.9239)  data: 0.0043 (0.0002 -- 0.0348)  max mem: 16413
Epoch: [117]  [100/160]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000009  loss: 1.6551 (1.7038)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0386 (7.1153)  time: 1.0548 (0.5208 -- 4.5827)  data: 0.0012 (0.0002 -- 0.0030)  max mem: 16413
[2023-09-05 02:27:51,663] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18840
[2023-09-05 02:27:51,663] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18840
[2023-09-05 02:27:51,664] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 02:27:51,664] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 02:27:51,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [117]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000009  loss: 1.6869 (1.6925)  loss_scale: 16384.0000 (16316.2975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4351 (6.9952)  time: 0.7699 (0.5200 -- 3.7301)  data: 0.0010 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000009  loss: 1.6820 (1.6954)  loss_scale: 8192.0000 (15163.9149)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4055 (6.9771)  time: 0.9109 (0.5216 -- 3.1855)  data: 0.0022 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000009  loss: 1.7395 (1.6926)  loss_scale: 8192.0000 (14336.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0793 (6.9196)  time: 0.7192 (0.4938 -- 4.2922)  data: 0.0009 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [117] Total time: 0:02:24 (0.9040 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000009  loss: 1.7395 (1.6807)  loss_scale: 8192.0000 (14336.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0793 (6.9196)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1429 (0.1429)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4292 (2.4292 -- 2.4292)  data: 2.1890 (2.1890 -- 2.1890)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5017 (0.6736)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4253 (0.2032 -- 2.4292)  data: 0.2129 (0.0009 -- 2.1890)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5017 (0.6389)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2227 (0.1700 -- 0.4425)  data: 0.0205 (0.0001 -- 0.2535)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6469 (0.6665)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2087 (0.1329 -- 0.4425)  data: 0.0201 (0.0001 -- 0.2535)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.626
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 85.27%
Epoch: [118]  [  0/160]  eta: 0:23:55  lr: 0.000036  min_lr: 0.000009  loss: 1.2568 (1.2568)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7011 (6.7011)  time: 8.9721 (8.9721 -- 8.9721)  data: 8.4242 (8.4242 -- 8.4242)  max mem: 16413
Epoch: [118]  [ 20/160]  eta: 0:02:40  lr: 0.000036  min_lr: 0.000009  loss: 1.6838 (1.6906)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1885 (6.8296)  time: 0.7566 (0.5153 -- 2.6765)  data: 0.2074 (0.0005 -- 2.1680)  max mem: 16413
Epoch: [118]  [ 40/160]  eta: 0:02:06  lr: 0.000036  min_lr: 0.000009  loss: 1.5523 (1.6783)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0617 (6.7629)  time: 0.9588 (0.5311 -- 3.0591)  data: 0.2125 (0.0004 -- 2.2073)  max mem: 16413
Epoch: [118]  [ 60/160]  eta: 0:01:39  lr: 0.000036  min_lr: 0.000009  loss: 1.6431 (1.6736)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8815 (6.9766)  time: 0.8608 (0.5284 -- 3.4273)  data: 0.1196 (0.0004 -- 1.9086)  max mem: 16413
Epoch: [118]  [ 80/160]  eta: 0:01:16  lr: 0.000036  min_lr: 0.000009  loss: 1.7921 (1.6958)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4592 (6.8610)  time: 0.8481 (0.5296 -- 2.9991)  data: 0.2208 (0.0002 -- 2.4509)  max mem: 16413
[2023-09-05 02:29:57,485] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:29:57,485] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:29:57,485] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 02:29:57,485] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [118]  [100/160]  eta: 0:00:57  lr: 0.000035  min_lr: 0.000009  loss: 1.6698 (1.6808)  loss_scale: 16384.0000 (9165.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6244 (6.8947)  time: 0.9725 (0.5283 -- 3.6942)  data: 0.4052 (0.0005 -- 3.1399)  max mem: 16413
[2023-09-05 02:30:23,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=107, lr=[9.014244631545819e-06, 9.014244631545819e-06, 1.0015827368384245e-05, 1.0015827368384245e-05, 1.1128697075982492e-05, 1.1128697075982492e-05, 1.236521897331388e-05, 1.236521897331388e-05, 1.3739132192570978e-05, 1.3739132192570978e-05, 1.5265702436189974e-05, 1.5265702436189974e-05, 1.6961891595766637e-05, 1.6961891595766637e-05, 1.8846546217518487e-05, 1.8846546217518487e-05, 2.0940606908353874e-05, 2.0940606908353874e-05, 2.3267341009282077e-05, 2.3267341009282077e-05, 2.5852601121424534e-05, 2.5852601121424534e-05, 2.872511235713837e-05, 2.872511235713837e-05, 3.191679150793152e-05, 3.191679150793152e-05, 3.546310167547947e-05, 3.546310167547947e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 02:30:23,972] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=17.409064614248642, CurrSamplesPerSec=21.708374595003054, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 1.5953 (1.6555)  loss_scale: 16384.0000 (10358.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2706 (7.1026)  time: 0.7990 (0.5205 -- 3.4652)  data: 0.2333 (0.0004 -- 2.8878)  max mem: 16413
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.6576 (1.6578)  loss_scale: 16384.0000 (11213.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2621 (7.2318)  time: 0.8932 (0.5139 -- 2.9971)  data: 0.3032 (0.0002 -- 2.4900)  max mem: 16413
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.6525 (1.6412)  loss_scale: 16384.0000 (11827.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0765 (7.1116)  time: 0.7069 (0.4972 -- 2.6719)  data: 0.0664 (0.0002 -- 1.3169)  max mem: 16413
Epoch: [118] Total time: 0:02:23 (0.8944 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.6525 (1.6626)  loss_scale: 16384.0000 (11827.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0765 (7.1116)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1668 (0.1668)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3702 (2.3702 -- 2.3702)  data: 2.1548 (2.1548 -- 2.1548)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3480 (0.6372)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (100.0000)  time: 0.4154 (0.1985 -- 2.3702)  data: 0.2055 (0.0008 -- 2.1548)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4962 (0.6320)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2235 (0.1695 -- 0.5107)  data: 0.0212 (0.0001 -- 0.3154)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6171 (0.6546)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2089 (0.1333 -- 0.5107)  data: 0.0209 (0.0001 -- 0.3154)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 83.195 Acc@5 97.718 loss 0.625
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 85.27%
Epoch: [119]  [  0/160]  eta: 0:22:13  lr: 0.000035  min_lr: 0.000009  loss: 1.7137 (1.7137)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0624 (7.0624)  time: 8.3339 (8.3339 -- 8.3339)  data: 7.1380 (7.1380 -- 7.1380)  max mem: 16413
Epoch: [119]  [ 20/160]  eta: 0:02:35  lr: 0.000035  min_lr: 0.000009  loss: 1.7074 (1.6954)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7292 (6.8455)  time: 0.7500 (0.5261 -- 3.0999)  data: 0.1391 (0.0003 -- 1.6838)  max mem: 16413
Epoch: [119]  [ 40/160]  eta: 0:02:01  lr: 0.000035  min_lr: 0.000009  loss: 1.8267 (1.7562)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9262 (7.0518)  time: 0.9171 (0.5375 -- 2.9652)  data: 0.1721 (0.0005 -- 1.6424)  max mem: 16413
[2023-09-05 02:32:02,628] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:32:02,629] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:32:02,632] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:32:02,633] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [119]  [ 60/160]  eta: 0:01:41  lr: 0.000035  min_lr: 0.000009  loss: 1.7791 (1.7398)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5381 (7.1724)  time: 1.0012 (0.5298 -- 4.8293)  data: 0.0092 (0.0005 -- 0.1594)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:17  lr: 0.000035  min_lr: 0.000009  loss: 1.7888 (1.7552)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2487 (7.1958)  time: 0.8351 (0.5142 -- 3.5426)  data: 0.0010 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [119]  [100/160]  eta: 0:00:57  lr: 0.000035  min_lr: 0.000009  loss: 1.5196 (1.7205)  loss_scale: 32768.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5242 (7.1394)  time: 0.9330 (0.5140 -- 3.5232)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
[2023-09-05 02:32:53,426] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19157
[2023-09-05 02:32:53,426] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19157
[2023-09-05 02:32:53,426] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:32:53,426] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:32:53,427] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [119]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 1.5130 (1.6977)  loss_scale: 32768.0000 (24508.2975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6855 (7.2479)  time: 0.7706 (0.5182 -- 3.0989)  data: 0.0015 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.7791 (1.7150)  loss_scale: 16384.0000 (23355.9149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0594 (7.2911)  time: 0.8446 (0.5322 -- 2.7083)  data: 0.0767 (0.0005 -- 0.5715)  max mem: 16413
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.6691 (1.7034)  loss_scale: 16384.0000 (22528.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8329 (7.2035)  time: 0.7000 (0.4941 -- 2.1212)  data: 0.1347 (0.0002 -- 1.5678)  max mem: 16413
Epoch: [119] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.6691 (1.6879)  loss_scale: 16384.0000 (22528.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8329 (7.2035)
[2023-09-05 02:33:24,868] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-09-05 02:33:24,871] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-09-05 02:33:24,873] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-09-05 02:33:24,873] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-09-05 02:33:25,956] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-09-05 02:33:25,956] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:14  loss: 0.1782 (0.1782)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7478 (2.7478 -- 2.7478)  data: 2.5312 (2.5312 -- 2.5312)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3679 (0.6298)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4468 (0.1989 -- 2.7478)  data: 0.2330 (0.0005 -- 2.5312)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4750 (0.5955)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2073 (0.1698 -- 0.2429)  data: 0.0044 (0.0001 -- 0.0528)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6459 (0.6417)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (98.3402)  time: 0.1911 (0.1328 -- 0.2429)  data: 0.0039 (0.0001 -- 0.0528)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 82.780 Acc@5 97.718 loss 0.650
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 85.27%
Epoch: [120]  [  0/160]  eta: 0:18:56  lr: 0.000035  min_lr: 0.000009  loss: 1.6996 (1.6996)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2083 (12.2083)  time: 7.1019 (7.1019 -- 7.1019)  data: 6.3361 (6.3361 -- 6.3361)  max mem: 16413
Epoch: [120]  [ 20/160]  eta: 0:02:40  lr: 0.000035  min_lr: 0.000009  loss: 1.8047 (1.7930)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2986 (7.3757)  time: 0.8482 (0.5298 -- 3.2782)  data: 0.0025 (0.0003 -- 0.0162)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:11  lr: 0.000035  min_lr: 0.000009  loss: 1.7234 (1.7638)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1379 (6.9742)  time: 1.0371 (0.5155 -- 4.7106)  data: 0.4734 (0.0004 -- 4.1581)  max mem: 16413
Epoch: [120]  [ 60/160]  eta: 0:01:39  lr: 0.000035  min_lr: 0.000009  loss: 1.6474 (1.7354)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8717 (7.0328)  time: 0.8029 (0.5200 -- 4.2404)  data: 0.2584 (0.0004 -- 3.7093)  max mem: 16413
Epoch: [120]  [ 80/160]  eta: 0:01:15  lr: 0.000035  min_lr: 0.000009  loss: 1.4219 (1.6832)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7651 (6.9517)  time: 0.7755 (0.5358 -- 2.0944)  data: 0.2178 (0.0004 -- 1.5458)  max mem: 16413
[2023-09-05 02:34:54,559] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:34:54,559] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:34:54,559] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:34:54,559] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [120]  [100/160]  eta: 0:00:55  lr: 0.000035  min_lr: 0.000009  loss: 1.7457 (1.7148)  loss_scale: 32768.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6869 (7.0628)  time: 0.8348 (0.5270 -- 4.1996)  data: 0.2886 (0.0002 -- 3.6545)  max mem: 16413
Epoch: [120]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000009  loss: 1.6320 (1.7144)  loss_scale: 32768.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6317 (7.0367)  time: 0.9132 (0.5278 -- 3.2781)  data: 0.3623 (0.0008 -- 2.7170)  max mem: 16413
Epoch: [120]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.5942 (1.6974)  loss_scale: 32768.0000 (22774.9220)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6300 (7.0189)  time: 0.8991 (0.5312 -- 4.2975)  data: 0.3468 (0.0004 -- 3.7657)  max mem: 16413
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.8083 (1.7023)  loss_scale: 32768.0000 (23961.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6134 (6.9750)  time: 0.7032 (0.4944 -- 3.3864)  data: 0.1856 (0.0002 -- 2.8533)  max mem: 16413
Epoch: [120] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.8083 (1.6833)  loss_scale: 32768.0000 (23961.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6134 (6.9750)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1635 (0.1635)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4253 (2.4253 -- 2.4253)  data: 2.1694 (2.1694 -- 2.1694)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4739 (0.5872)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4222 (0.2012 -- 2.4253)  data: 0.1987 (0.0005 -- 2.1694)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4739 (0.5609)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.9418)  time: 0.2224 (0.1760 -- 0.3290)  data: 0.0144 (0.0001 -- 0.1540)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5339 (0.5986)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.7552)  time: 0.2055 (0.1331 -- 0.3290)  data: 0.0141 (0.0001 -- 0.1540)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 83.610 Acc@5 97.925 loss 0.637
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 85.27%
Epoch: [121]  [  0/160]  eta: 0:20:25  lr: 0.000035  min_lr: 0.000009  loss: 2.2315 (2.2315)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1113 (7.1113)  time: 7.6600 (7.6600 -- 7.6600)  data: 6.7488 (6.7488 -- 6.7488)  max mem: 16413
Epoch: [121]  [ 20/160]  eta: 0:02:48  lr: 0.000035  min_lr: 0.000009  loss: 1.5981 (1.6199)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9679 (6.7624)  time: 0.8822 (0.5177 -- 2.7569)  data: 0.0587 (0.0003 -- 1.1302)  max mem: 16413
Epoch: [121]  [ 40/160]  eta: 0:02:13  lr: 0.000035  min_lr: 0.000009  loss: 1.3865 (1.5540)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4240 (6.8385)  time: 1.0237 (0.5266 -- 4.2389)  data: 0.0016 (0.0001 -- 0.0066)  max mem: 16413
[2023-09-05 02:36:58,815] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:36:58,815] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:36:58,815] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 02:36:58,815] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 02:37:05,706] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19420
[2023-09-05 02:37:05,706] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 02:37:05,707] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19420
[2023-09-05 02:37:05,707] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 02:37:05,707] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [121]  [ 60/160]  eta: 0:01:40  lr: 0.000035  min_lr: 0.000009  loss: 1.6596 (1.5946)  loss_scale: 32768.0000 (35991.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0353 (6.7253)  time: 0.7669 (0.5225 -- 4.1903)  data: 0.0014 (0.0003 -- 0.0054)  max mem: 16413
[2023-09-05 02:37:20,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19438
[2023-09-05 02:37:20,353] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:37:20,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19438
[2023-09-05 02:37:20,353] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:37:20,353] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [121]  [ 80/160]  eta: 0:01:17  lr: 0.000035  min_lr: 0.000009  loss: 1.5079 (1.6227)  loss_scale: 32768.0000 (34588.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2904 (6.8975)  time: 0.8633 (0.5213 -- 3.2730)  data: 0.0012 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [121]  [100/160]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000009  loss: 1.5880 (1.6290)  loss_scale: 16384.0000 (30983.6040)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3906 (6.8708)  time: 0.8024 (0.5282 -- 3.7207)  data: 0.0022 (0.0004 -- 0.0141)  max mem: 16413
Epoch: [121]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000009  loss: 1.8056 (1.6501)  loss_scale: 16384.0000 (28570.4463)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5516 (7.0330)  time: 0.9312 (0.5379 -- 4.0207)  data: 0.0015 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [121]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000009  loss: 1.7955 (1.6594)  loss_scale: 16384.0000 (26841.8723)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9810 (7.1084)  time: 0.8445 (0.5304 -- 3.7333)  data: 0.0031 (0.0004 -- 0.0162)  max mem: 16413
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000009  loss: 1.6300 (1.6536)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1519 (7.1188)  time: 0.6188 (0.4943 -- 1.4645)  data: 0.0009 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [121] Total time: 0:02:21 (0.8862 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000009  loss: 1.6300 (1.6581)  loss_scale: 16384.0000 (25600.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1519 (7.1188)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1842 (0.1842)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3854 (2.3854 -- 2.3854)  data: 2.1534 (2.1534 -- 2.1534)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6732 (0.6273)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4295 (0.2069 -- 2.3854)  data: 0.2109 (0.0004 -- 2.1534)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4662 (0.5723)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2204 (0.1695 -- 0.3789)  data: 0.0118 (0.0001 -- 0.1536)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4840 (0.6227)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2045 (0.1370 -- 0.3789)  data: 0.0114 (0.0001 -- 0.1536)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 82.988 Acc@5 97.303 loss 0.640
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 85.27%
Epoch: [122]  [  0/160]  eta: 0:20:36  lr: 0.000035  min_lr: 0.000009  loss: 1.9231 (1.9231)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5127 (6.5127)  time: 7.7279 (7.7279 -- 7.7279)  data: 7.1608 (7.1608 -- 7.1608)  max mem: 16413
Epoch: [122]  [ 20/160]  eta: 0:02:48  lr: 0.000035  min_lr: 0.000009  loss: 1.6556 (1.6887)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4481 (7.2157)  time: 0.8769 (0.5332 -- 4.2068)  data: 0.3204 (0.0009 -- 3.6144)  max mem: 16413
Epoch: [122]  [ 40/160]  eta: 0:02:09  lr: 0.000035  min_lr: 0.000009  loss: 1.6154 (1.7079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5935 (7.0122)  time: 0.9475 (0.5271 -- 4.1448)  data: 0.3938 (0.0002 -- 3.6083)  max mem: 16413
[2023-09-05 02:39:22,330] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:39:22,330] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:39:22,330] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:39:22,330] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [122]  [ 60/160]  eta: 0:01:38  lr: 0.000035  min_lr: 0.000009  loss: 1.7581 (1.7208)  loss_scale: 32768.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4983 (6.8565)  time: 0.8016 (0.5266 -- 3.7349)  data: 0.2512 (0.0005 -- 3.1529)  max mem: 16413
Epoch: [122]  [ 80/160]  eta: 0:01:18  lr: 0.000035  min_lr: 0.000009  loss: 1.6762 (1.7207)  loss_scale: 32768.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2585 (7.0080)  time: 0.9645 (0.5290 -- 4.7182)  data: 0.4068 (0.0002 -- 4.1949)  max mem: 16413
Epoch: [122]  [100/160]  eta: 0:00:57  lr: 0.000035  min_lr: 0.000009  loss: 1.6988 (1.7124)  loss_scale: 32768.0000 (25143.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9054 (6.8749)  time: 0.8850 (0.5158 -- 4.0037)  data: 0.3412 (0.0004 -- 3.4725)  max mem: 16413
Epoch: [122]  [120/160]  eta: 0:00:38  lr: 0.000035  min_lr: 0.000009  loss: 1.7447 (1.7105)  loss_scale: 32768.0000 (26403.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2567 (6.8579)  time: 0.8919 (0.5310 -- 3.4103)  data: 0.3349 (0.0004 -- 2.8635)  max mem: 16413
Epoch: [122]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.4689 (1.6851)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9995 (6.7726)  time: 0.7294 (0.5264 -- 2.5566)  data: 0.1718 (0.0002 -- 2.0212)  max mem: 16413
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.5097 (1.6645)  loss_scale: 32768.0000 (27955.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4302 (6.8869)  time: 0.7187 (0.4938 -- 2.5558)  data: 0.1862 (0.0002 -- 2.0593)  max mem: 16413
Epoch: [122] Total time: 0:02:23 (0.8969 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.5097 (1.6373)  loss_scale: 32768.0000 (27955.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4302 (6.8869)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1544 (0.1544)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5605 (2.5605 -- 2.5605)  data: 2.2732 (2.2732 -- 2.2732)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5757 (0.6485)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4360 (0.1969 -- 2.5605)  data: 0.2160 (0.0007 -- 2.2732)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4338 (0.5872)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2146 (0.1705 -- 0.3030)  data: 0.0099 (0.0001 -- 0.0925)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4860 (0.6291)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2005 (0.1328 -- 0.3030)  data: 0.0096 (0.0001 -- 0.0925)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 83.817 Acc@5 97.510 loss 0.615
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 85.27%
Epoch: [123]  [  0/160]  eta: 0:17:04  lr: 0.000034  min_lr: 0.000009  loss: 0.9196 (0.9196)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6144 (6.6144)  time: 6.4031 (6.4031 -- 6.4031)  data: 5.7387 (5.7387 -- 5.7387)  max mem: 16413
[2023-09-05 02:41:25,355] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:41:25,355] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 02:41:25,359] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:41:25,360] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [123]  [ 20/160]  eta: 0:02:42  lr: 0.000034  min_lr: 0.000009  loss: 1.7665 (1.6751)  loss_scale: 32768.0000 (42130.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6405 (7.8393)  time: 0.8993 (0.5347 -- 2.6224)  data: 0.1442 (0.0005 -- 1.4018)  max mem: 16413
[2023-09-05 02:41:39,447] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19710
[2023-09-05 02:41:39,447] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19710
[2023-09-05 02:41:39,448] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 02:41:39,448] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 02:41:39,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [123]  [ 40/160]  eta: 0:02:00  lr: 0.000034  min_lr: 0.000009  loss: 1.6335 (1.6748)  loss_scale: 32768.0000 (44756.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9678 (7.1626)  time: 0.8464 (0.5136 -- 3.2708)  data: 0.0780 (0.0003 -- 1.2882)  max mem: 16413
Epoch: [123]  [ 60/160]  eta: 0:01:37  lr: 0.000034  min_lr: 0.000009  loss: 1.8380 (1.7148)  loss_scale: 32768.0000 (40825.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7566 (7.0891)  time: 0.8958 (0.5286 -- 2.6541)  data: 0.0631 (0.0003 -- 0.8506)  max mem: 16413
Epoch: [123]  [ 80/160]  eta: 0:01:15  lr: 0.000034  min_lr: 0.000009  loss: 1.7888 (1.6986)  loss_scale: 32768.0000 (38836.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0427 (7.0037)  time: 0.8848 (0.5281 -- 2.1463)  data: 0.0650 (0.0003 -- 0.7924)  max mem: 16413
[2023-09-05 02:42:39,774] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19780
[2023-09-05 02:42:39,774] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19780
[2023-09-05 02:42:39,775] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:42:39,775] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:42:39,775] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [123]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000009  loss: 1.7911 (1.7161)  loss_scale: 32768.0000 (37472.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7271 (7.1020)  time: 0.8657 (0.5235 -- 2.7494)  data: 0.0015 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [123]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000009  loss: 1.8733 (1.7205)  loss_scale: 16384.0000 (33986.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1319 (7.0941)  time: 0.8681 (0.5389 -- 2.6698)  data: 0.0024 (0.0003 -- 0.0165)  max mem: 16413
Epoch: [123]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.6041 (1.7033)  loss_scale: 16384.0000 (31489.8156)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4684 (7.0174)  time: 0.8335 (0.5257 -- 3.2502)  data: 0.0485 (0.0002 -- 0.9417)  max mem: 16413
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.6390 (1.6900)  loss_scale: 16384.0000 (29696.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2072 (7.0179)  time: 0.7140 (0.4956 -- 3.1470)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [123] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.6390 (1.6825)  loss_scale: 16384.0000 (29696.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2072 (7.0179)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1483 (0.1483)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4176 (2.4176 -- 2.4176)  data: 2.1572 (2.1572 -- 2.1572)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3896 (0.6053)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4253 (0.2049 -- 2.4176)  data: 0.2068 (0.0006 -- 2.1572)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4714 (0.5676)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2152 (0.1737 -- 0.3443)  data: 0.0061 (0.0001 -- 0.1088)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5146 (0.6113)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2002 (0.1326 -- 0.3443)  data: 0.0058 (0.0001 -- 0.1088)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.608
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 85.27%
Epoch: [124]  [  0/160]  eta: 0:22:13  lr: 0.000034  min_lr: 0.000009  loss: 2.2195 (2.2195)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1526 (6.1526)  time: 8.3353 (8.3353 -- 8.3353)  data: 7.0712 (7.0712 -- 7.0712)  max mem: 16413
Epoch: [124]  [ 20/160]  eta: 0:03:00  lr: 0.000034  min_lr: 0.000009  loss: 1.6120 (1.6608)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4203 (6.3520)  time: 0.9395 (0.5092 -- 4.7863)  data: 0.0639 (0.0004 -- 1.2469)  max mem: 16413
Epoch: [124]  [ 40/160]  eta: 0:02:08  lr: 0.000034  min_lr: 0.000009  loss: 1.6148 (1.6453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1171 (6.2769)  time: 0.8473 (0.5134 -- 2.8692)  data: 0.0786 (0.0002 -- 1.5516)  max mem: 16413
Epoch: [124]  [ 60/160]  eta: 0:01:41  lr: 0.000034  min_lr: 0.000009  loss: 1.7842 (1.6713)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6055 (6.4612)  time: 0.9031 (0.5100 -- 4.3338)  data: 0.3497 (0.0004 -- 3.6142)  max mem: 16413
[2023-09-05 02:44:44,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:44:44,600] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:44:44,603] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:44:44,604] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [124]  [ 80/160]  eta: 0:01:17  lr: 0.000034  min_lr: 0.000009  loss: 1.6798 (1.6693)  loss_scale: 32768.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4851 (6.5058)  time: 0.8396 (0.5304 -- 3.4738)  data: 0.1772 (0.0004 -- 2.2330)  max mem: 16413
Epoch: [124]  [100/160]  eta: 0:00:57  lr: 0.000034  min_lr: 0.000009  loss: 1.8228 (1.6854)  loss_scale: 32768.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8044 (6.5678)  time: 0.8725 (0.5280 -- 3.9931)  data: 0.2725 (0.0003 -- 3.4604)  max mem: 16413
Epoch: [124]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000009  loss: 1.7907 (1.6857)  loss_scale: 32768.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5102 (6.6081)  time: 0.8298 (0.5265 -- 3.2792)  data: 0.2746 (0.0004 -- 2.7481)  max mem: 16413
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.7458 (1.6890)  loss_scale: 32768.0000 (24750.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9409 (6.7445)  time: 0.8224 (0.5229 -- 4.1353)  data: 0.2731 (0.0004 -- 3.5827)  max mem: 16413
[2023-09-05 02:45:59,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=112, lr=[8.634888811674727e-06, 8.634888811674727e-06, 9.59432090186081e-06, 9.59432090186081e-06, 1.066035655762312e-05, 1.066035655762312e-05, 1.1844840619581245e-05, 1.1844840619581245e-05, 1.3160934021756938e-05, 1.3160934021756938e-05, 1.4623260024174376e-05, 1.4623260024174376e-05, 1.6248066693527082e-05, 1.6248066693527082e-05, 1.8053407437252314e-05, 1.8053407437252314e-05, 2.0059341596947017e-05, 2.0059341596947017e-05, 2.2288157329941126e-05, 2.2288157329941126e-05, 2.4764619255490143e-05, 2.4764619255490143e-05, 2.751624361721127e-05, 2.751624361721127e-05, 3.057360401912363e-05, 3.057360401912363e-05, 3.397067113235959e-05, 3.397067113235959e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 02:45:59,251] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=17.41679365728549, CurrSamplesPerSec=24.636400080470725, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.5316 (1.6716)  loss_scale: 32768.0000 (25702.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6700 (6.7541)  time: 0.7456 (0.4954 -- 3.1829)  data: 0.2218 (0.0002 -- 2.6322)  max mem: 16413
Epoch: [124] Total time: 0:02:23 (0.8989 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.5316 (1.6373)  loss_scale: 32768.0000 (25702.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6700 (6.7541)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.1358 (0.1358)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1194 (2.1194 -- 2.1194)  data: 1.9147 (1.9147 -- 1.9147)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5101 (0.5807)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (98.9899)  time: 0.4124 (0.2074 -- 2.1194)  data: 0.1941 (0.0003 -- 1.9147)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5092 (0.5751)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2337 (0.1686 -- 0.4632)  data: 0.0286 (0.0001 -- 0.2791)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5750 (0.6343)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.9253)  time: 0.2170 (0.1332 -- 0.4632)  data: 0.0281 (0.0001 -- 0.2791)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 84.025 Acc@5 97.718 loss 0.613
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.27%
Epoch: [125]  [  0/160]  eta: 0:19:53  lr: 0.000034  min_lr: 0.000009  loss: 2.1403 (2.1403)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9946 (4.9946)  time: 7.4581 (7.4581 -- 7.4581)  data: 5.6962 (5.6962 -- 5.6962)  max mem: 16413
Epoch: [125]  [ 20/160]  eta: 0:02:49  lr: 0.000034  min_lr: 0.000009  loss: 1.8218 (1.7355)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7333 (6.8204)  time: 0.8961 (0.5304 -- 2.9653)  data: 0.0012 (0.0003 -- 0.0023)  max mem: 16413
[2023-09-05 02:46:46,243] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20034
[2023-09-05 02:46:46,244] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:46:46,244] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20034
[2023-09-05 02:46:46,244] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:46:46,244] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [125]  [ 40/160]  eta: 0:02:06  lr: 0.000034  min_lr: 0.000009  loss: 1.7340 (1.6866)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1117 (7.0451)  time: 0.8965 (0.5218 -- 2.1439)  data: 0.0015 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [125]  [ 60/160]  eta: 0:01:39  lr: 0.000034  min_lr: 0.000009  loss: 1.6611 (1.6903)  loss_scale: 16384.0000 (25516.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9522 (7.0289)  time: 0.8713 (0.5274 -- 3.5263)  data: 0.0020 (0.0004 -- 0.0139)  max mem: 16413
Epoch: [125]  [ 80/160]  eta: 0:01:15  lr: 0.000034  min_lr: 0.000009  loss: 1.8638 (1.7220)  loss_scale: 16384.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5336 (7.0021)  time: 0.8057 (0.5210 -- 3.5756)  data: 0.0017 (0.0006 -- 0.0038)  max mem: 16413
Epoch: [125]  [100/160]  eta: 0:00:57  lr: 0.000034  min_lr: 0.000009  loss: 1.7344 (1.7227)  loss_scale: 16384.0000 (21899.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1921 (6.9221)  time: 0.9555 (0.5241 -- 4.0924)  data: 0.0017 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [125]  [120/160]  eta: 0:00:38  lr: 0.000034  min_lr: 0.000009  loss: 1.4325 (1.6812)  loss_scale: 16384.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9946 (6.9064)  time: 0.9798 (0.5156 -- 4.6796)  data: 0.0010 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [125]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.7859 (1.6864)  loss_scale: 16384.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8894 (6.9580)  time: 0.8105 (0.5209 -- 3.5635)  data: 0.0020 (0.0004 -- 0.0100)  max mem: 16413
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000009  loss: 1.7421 (1.6892)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6956 (6.9678)  time: 0.6765 (0.4951 -- 2.2035)  data: 0.0006 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [125] Total time: 0:02:24 (0.9046 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000009  loss: 1.7421 (1.6770)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6956 (6.9678)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1461 (0.1461)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4460 (2.4460 -- 2.4460)  data: 2.2103 (2.2103 -- 2.2103)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6133 (0.6150)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4363 (0.2121 -- 2.4460)  data: 0.2113 (0.0009 -- 2.2103)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4623 (0.5617)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (98.4127)  time: 0.2174 (0.1689 -- 0.3153)  data: 0.0059 (0.0001 -- 0.0956)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5097 (0.6256)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (97.5104)  time: 0.2004 (0.1331 -- 0.3153)  data: 0.0056 (0.0001 -- 0.0956)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 83.195 Acc@5 97.303 loss 0.628
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 85.27%
Epoch: [126]  [  0/160]  eta: 0:26:02  lr: 0.000034  min_lr: 0.000009  loss: 1.6355 (1.6355)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8462 (6.8462)  time: 9.7660 (9.7660 -- 9.7660)  data: 9.2305 (9.2305 -- 9.2305)  max mem: 16413
[2023-09-05 02:48:51,022] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:48:51,022] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:48:51,022] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:48:51,022] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [126]  [ 20/160]  eta: 0:03:03  lr: 0.000034  min_lr: 0.000009  loss: 1.5124 (1.5770)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1813 (6.4787)  time: 0.8847 (0.5158 -- 4.6026)  data: 0.3384 (0.0004 -- 4.0736)  max mem: 16413
Epoch: [126]  [ 40/160]  eta: 0:02:12  lr: 0.000034  min_lr: 0.000009  loss: 1.6686 (1.5759)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6736 (6.5941)  time: 0.8903 (0.5126 -- 3.5087)  data: 0.3469 (0.0002 -- 2.9838)  max mem: 16413
Epoch: [126]  [ 60/160]  eta: 0:01:41  lr: 0.000034  min_lr: 0.000009  loss: 1.5006 (1.5839)  loss_scale: 32768.0000 (31962.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0305 (6.5352)  time: 0.8176 (0.5209 -- 3.2575)  data: 0.2679 (0.0009 -- 2.6982)  max mem: 16413
Epoch: [126]  [ 80/160]  eta: 0:01:17  lr: 0.000034  min_lr: 0.000009  loss: 1.7794 (1.6317)  loss_scale: 32768.0000 (32161.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8466 (6.5755)  time: 0.8301 (0.5187 -- 2.0660)  data: 0.2001 (0.0005 -- 1.5290)  max mem: 16413
[2023-09-05 02:50:08,677] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20252
[2023-09-05 02:50:08,677] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20252
[2023-09-05 02:50:08,677] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:50:08,677] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:50:08,678] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [126]  [100/160]  eta: 0:00:57  lr: 0.000034  min_lr: 0.000009  loss: 1.5957 (1.6269)  loss_scale: 32768.0000 (30821.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9688 (6.5305)  time: 0.8937 (0.5300 -- 2.2476)  data: 0.0016 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [126]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000009  loss: 1.7327 (1.6497)  loss_scale: 16384.0000 (28435.0413)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8671 (6.5965)  time: 0.8243 (0.5263 -- 3.5665)  data: 0.0014 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [126]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000009  loss: 1.7617 (1.6681)  loss_scale: 16384.0000 (26725.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5310 (6.6550)  time: 0.8785 (0.5241 -- 3.5066)  data: 0.2510 (0.0003 -- 2.9603)  max mem: 16413
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000009  loss: 1.6683 (1.6745)  loss_scale: 16384.0000 (25497.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3953 (6.7085)  time: 0.6815 (0.4966 -- 1.8796)  data: 0.0642 (0.0002 -- 1.2713)  max mem: 16413
Epoch: [126] Total time: 0:02:23 (0.8956 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000009  loss: 1.6683 (1.6627)  loss_scale: 16384.0000 (25497.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3953 (6.7085)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1303 (0.1303)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4399 (2.4399 -- 2.4399)  data: 2.1988 (2.1988 -- 2.1988)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4537 (0.5392)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4288 (0.1992 -- 2.4399)  data: 0.2160 (0.0008 -- 2.1988)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4537 (0.5277)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2254 (0.1717 -- 0.4566)  data: 0.0227 (0.0001 -- 0.2729)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4802 (0.5880)  acc1: 77.7778 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.2092 (0.1330 -- 0.4566)  data: 0.0215 (0.0001 -- 0.2729)  max mem: 16413
Val: Total time: 0:00:07 (0.2940 s / it)
* Acc@1 85.062 Acc@5 97.510 loss 0.585
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.27%
Epoch: [127]  [  0/160]  eta: 0:20:25  lr: 0.000033  min_lr: 0.000009  loss: 2.2026 (2.2026)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3313 (6.3313)  time: 7.6600 (7.6600 -- 7.6600)  data: 7.1225 (7.1225 -- 7.1225)  max mem: 16413
Epoch: [127]  [ 20/160]  eta: 0:02:45  lr: 0.000033  min_lr: 0.000009  loss: 1.7909 (1.7650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4710 (7.0819)  time: 0.8613 (0.5273 -- 3.8229)  data: 0.1412 (0.0006 -- 1.8635)  max mem: 16413
Epoch: [127]  [ 40/160]  eta: 0:02:08  lr: 0.000033  min_lr: 0.000008  loss: 1.3425 (1.6149)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8074 (6.8884)  time: 0.9537 (0.5258 -- 4.4591)  data: 0.3723 (0.0004 -- 3.9473)  max mem: 16413
Epoch: [127]  [ 60/160]  eta: 0:01:37  lr: 0.000033  min_lr: 0.000008  loss: 1.6365 (1.6463)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1845 (7.0164)  time: 0.7891 (0.5265 -- 4.1528)  data: 0.2443 (0.0002 -- 3.6284)  max mem: 16413
[2023-09-05 02:52:11,198] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:52:11,198] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:52:11,198] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:52:11,198] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [127]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000008  loss: 1.6432 (1.6630)  loss_scale: 32768.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9111 (6.9586)  time: 0.9040 (0.5288 -- 3.0797)  data: 0.3447 (0.0003 -- 2.5213)  max mem: 16413
[2023-09-05 02:52:29,260] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20401
[2023-09-05 02:52:29,260] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20401
[2023-09-05 02:52:29,260] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:52:29,260] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:52:29,260] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [127]  [100/160]  eta: 0:00:55  lr: 0.000033  min_lr: 0.000008  loss: 1.6386 (1.6582)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2144 (6.8164)  time: 0.7950 (0.5255 -- 3.1039)  data: 0.2392 (0.0004 -- 2.4343)  max mem: 16413
Epoch: [127]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000008  loss: 1.6168 (1.6504)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5121 (6.8482)  time: 0.9556 (0.5076 -- 5.7227)  data: 0.4142 (0.0003 -- 5.2131)  max mem: 16413
Epoch: [127]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.6309 (1.6551)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5915 (6.8718)  time: 0.8574 (0.5186 -- 4.2631)  data: 0.3169 (0.0002 -- 3.7522)  max mem: 16413
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.6988 (1.6648)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2015 (6.9194)  time: 0.6209 (0.4956 -- 1.4131)  data: 0.0881 (0.0002 -- 0.8747)  max mem: 16413
Epoch: [127] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.6988 (1.6769)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2015 (6.9194)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1681 (0.1681)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6053 (2.6053 -- 2.6053)  data: 2.3526 (2.3526 -- 2.3526)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4394 (0.5721)  acc1: 77.7778 (82.8283)  acc5: 100.0000 (98.9899)  time: 0.4457 (0.1954 -- 2.6053)  data: 0.2198 (0.0005 -- 2.3526)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4552 (0.5523)  acc1: 77.7778 (83.5979)  acc5: 100.0000 (98.4127)  time: 0.2158 (0.1695 -- 0.2802)  data: 0.0079 (0.0001 -- 0.0897)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5121 (0.6077)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (98.3402)  time: 0.1953 (0.1325 -- 0.2802)  data: 0.0076 (0.0001 -- 0.0897)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 83.817 Acc@5 97.718 loss 0.602
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 85.27%
Epoch: [128]  [  0/160]  eta: 0:19:55  lr: 0.000033  min_lr: 0.000008  loss: 1.8777 (1.8777)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8482 (6.8482)  time: 7.4690 (7.4690 -- 7.4690)  data: 6.9223 (6.9223 -- 6.9223)  max mem: 16413
Epoch: [128]  [ 20/160]  eta: 0:02:42  lr: 0.000033  min_lr: 0.000008  loss: 1.5306 (1.6023)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0379 (6.6436)  time: 0.8488 (0.5391 -- 2.4613)  data: 0.2867 (0.0003 -- 1.7390)  max mem: 16413
Epoch: [128]  [ 40/160]  eta: 0:02:10  lr: 0.000033  min_lr: 0.000008  loss: 1.7366 (1.6795)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4070 (7.0753)  time: 1.0117 (0.5253 -- 3.9498)  data: 0.4557 (0.0002 -- 3.4237)  max mem: 16413
[2023-09-05 02:54:34,232] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:54:34,232] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:54:34,234] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:54:34,234] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:54:39,150] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20537
[2023-09-05 02:54:39,150] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20537
[2023-09-05 02:54:39,150] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:54:39,150] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:54:39,150] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [128]  [ 60/160]  eta: 0:01:41  lr: 0.000033  min_lr: 0.000008  loss: 1.6575 (1.6874)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6447 (6.9726)  time: 0.8521 (0.5241 -- 3.9064)  data: 0.3095 (0.0003 -- 3.3730)  max mem: 16413
Epoch: [128]  [ 80/160]  eta: 0:01:19  lr: 0.000033  min_lr: 0.000008  loss: 1.6319 (1.6632)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8723 (7.0087)  time: 0.9160 (0.5340 -- 3.4717)  data: 0.3633 (0.0004 -- 2.9182)  max mem: 16413
Epoch: [128]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000008  loss: 1.8751 (1.6872)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6816 (6.8825)  time: 0.7849 (0.5210 -- 3.1322)  data: 0.2354 (0.0004 -- 2.5795)  max mem: 16413
Epoch: [128]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000008  loss: 1.7495 (1.7026)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9343 (6.9843)  time: 0.9431 (0.5218 -- 3.5160)  data: 0.3857 (0.0008 -- 2.9574)  max mem: 16413
Epoch: [128]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.7127 (1.6940)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2166 (6.8533)  time: 0.7752 (0.5316 -- 3.0036)  data: 0.2123 (0.0004 -- 2.4318)  max mem: 16413
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.5128 (1.6886)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6844 (6.8795)  time: 0.6999 (0.4971 -- 2.6815)  data: 0.1776 (0.0002 -- 2.1350)  max mem: 16413
Epoch: [128] Total time: 0:02:23 (0.8974 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.5128 (1.6755)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6844 (6.8795)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1573 (0.1573)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2424 (2.2424 -- 2.2424)  data: 2.0319 (2.0319 -- 2.0319)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3498 (0.5615)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4066 (0.2031 -- 2.2424)  data: 0.1924 (0.0004 -- 2.0319)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3997 (0.5278)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2254 (0.1690 -- 0.4446)  data: 0.0184 (0.0001 -- 0.2102)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4466 (0.5939)  acc1: 77.7778 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2118 (0.1335 -- 0.4446)  data: 0.0181 (0.0001 -- 0.2102)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 84.232 Acc@5 97.303 loss 0.603
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.27%
Epoch: [129]  [  0/160]  eta: 0:20:37  lr: 0.000033  min_lr: 0.000008  loss: 0.9475 (0.9475)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.4701 (13.4701)  time: 7.7342 (7.7342 -- 7.7342)  data: 7.1818 (7.1818 -- 7.1818)  max mem: 16413
Epoch: [129]  [ 20/160]  eta: 0:02:39  lr: 0.000033  min_lr: 0.000008  loss: 1.7515 (1.7326)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2253 (6.7915)  time: 0.8095 (0.5349 -- 3.5819)  data: 0.2599 (0.0010 -- 3.0315)  max mem: 16413
[2023-09-05 02:56:43,082] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:56:43,082] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 02:56:43,083] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:56:43,083] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [129]  [ 40/160]  eta: 0:02:09  lr: 0.000033  min_lr: 0.000008  loss: 1.7982 (1.7600)  loss_scale: 32768.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1107 (7.0753)  time: 1.0245 (0.5311 -- 4.3220)  data: 0.4608 (0.0002 -- 3.7931)  max mem: 16413
Epoch: [129]  [ 60/160]  eta: 0:01:40  lr: 0.000033  min_lr: 0.000008  loss: 1.6814 (1.7500)  loss_scale: 32768.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5471 (6.9534)  time: 0.8550 (0.5176 -- 3.6766)  data: 0.2907 (0.0003 -- 3.1551)  max mem: 16413
Epoch: [129]  [ 80/160]  eta: 0:01:18  lr: 0.000033  min_lr: 0.000008  loss: 1.7001 (1.7329)  loss_scale: 32768.0000 (27508.9383)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9533 (6.8268)  time: 0.8841 (0.5250 -- 4.0719)  data: 0.3327 (0.0003 -- 3.5257)  max mem: 16413
Epoch: [129]  [100/160]  eta: 0:00:57  lr: 0.000033  min_lr: 0.000008  loss: 1.5969 (1.7183)  loss_scale: 32768.0000 (28550.3366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7119 (7.0156)  time: 0.8545 (0.5263 -- 3.7494)  data: 0.2970 (0.0002 -- 3.1729)  max mem: 16413
Epoch: [129]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000008  loss: 1.6695 (1.7291)  loss_scale: 32768.0000 (29247.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5839 (7.0592)  time: 0.8951 (0.5137 -- 3.8675)  data: 0.3421 (0.0003 -- 3.3315)  max mem: 16413
Epoch: [129]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.6888 (1.7249)  loss_scale: 32768.0000 (29746.8369)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0837 (7.1152)  time: 0.7400 (0.5423 -- 2.5839)  data: 0.1774 (0.0003 -- 2.0652)  max mem: 16413
[2023-09-05 02:58:33,073] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:58:33,074] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 02:58:33,074] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 02:58:33,074] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 02:58:34,072] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20796
[2023-09-05 02:58:34,072] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 02:58:34,072] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-05 02:58:34,072] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20796
[2023-09-05 02:58:34,072] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 02:58:35,055] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20798
[2023-09-05 02:58:35,055] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20798
[2023-09-05 02:58:35,055] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:58:35,055] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 02:58:35,055] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000008  loss: 1.7099 (1.7161)  loss_scale: 32768.0000 (30310.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2006 (7.1769)  time: 0.7496 (0.4832 -- 2.6030)  data: 0.2290 (0.0002 -- 2.0772)  max mem: 16413
Epoch: [129] Total time: 0:02:23 (0.8965 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000008  loss: 1.7099 (1.7040)  loss_scale: 32768.0000 (30310.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2006 (7.1769)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1474 (0.1474)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2523 (2.2523 -- 2.2523)  data: 2.0340 (2.0340 -- 2.0340)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4475 (0.5347)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4317 (0.2100 -- 2.2523)  data: 0.2083 (0.0006 -- 2.0340)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4751 (0.5417)  acc1: 77.7778 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2292 (0.1695 -- 0.4861)  data: 0.0212 (0.0001 -- 0.2477)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5060 (0.5779)  acc1: 77.7778 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.2109 (0.1329 -- 0.4861)  data: 0.0209 (0.0001 -- 0.2477)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 84.855 Acc@5 97.303 loss 0.592
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.27%
Epoch: [130]  [  0/160]  eta: 0:17:02  lr: 0.000033  min_lr: 0.000008  loss: 1.7238 (1.7238)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2970 (9.2970)  time: 6.3915 (6.3915 -- 6.3915)  data: 5.8447 (5.8447 -- 5.8447)  max mem: 16413
Epoch: [130]  [ 20/160]  eta: 0:02:47  lr: 0.000033  min_lr: 0.000008  loss: 1.7916 (1.6885)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3340 (6.5864)  time: 0.9388 (0.5288 -- 3.9045)  data: 0.2975 (0.0003 -- 3.3323)  max mem: 16413
Epoch: [130]  [ 40/160]  eta: 0:02:12  lr: 0.000033  min_lr: 0.000008  loss: 1.7396 (1.7084)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8448 (6.8739)  time: 0.9992 (0.5237 -- 4.2010)  data: 0.4458 (0.0002 -- 3.6268)  max mem: 16413
Epoch: [130]  [ 60/160]  eta: 0:01:41  lr: 0.000033  min_lr: 0.000008  loss: 1.6348 (1.6528)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0038 (7.0070)  time: 0.8370 (0.5141 -- 3.6762)  data: 0.2863 (0.0004 -- 3.1219)  max mem: 16413
Epoch: [130]  [ 80/160]  eta: 0:01:17  lr: 0.000033  min_lr: 0.000008  loss: 1.5550 (1.6437)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6699 (7.0972)  time: 0.8312 (0.5229 -- 3.7658)  data: 0.2674 (0.0002 -- 3.2345)  max mem: 16413
Epoch: [130]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000008  loss: 1.6601 (1.6526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2362 (6.9511)  time: 0.8196 (0.5258 -- 5.0462)  data: 0.2756 (0.0003 -- 4.5337)  max mem: 16413
Epoch: [130]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000008  loss: 1.6500 (1.6659)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6411 (7.1319)  time: 0.9776 (0.5375 -- 3.8089)  data: 0.4280 (0.0005 -- 3.2776)  max mem: 16413
[2023-09-05 03:00:41,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:00:41,783] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:00:41,783] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:00:41,783] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [130]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000008  loss: 1.5006 (1.6534)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7612 (7.0633)  time: 0.8475 (0.5318 -- 4.2820)  data: 0.2973 (0.0005 -- 3.7143)  max mem: 16413
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.5862 (1.6421)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5442 (7.0441)  time: 0.6330 (0.4955 -- 2.7269)  data: 0.1102 (0.0002 -- 2.1909)  max mem: 16413
Epoch: [130] Total time: 0:02:23 (0.8973 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.5862 (1.6828)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5442 (7.0441)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1736 (0.1736)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3143 (2.3143 -- 2.3143)  data: 2.0954 (2.0954 -- 2.0954)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4076 (0.5953)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4676 (0.2028 -- 2.3143)  data: 0.2502 (0.0006 -- 2.0954)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4915 (0.5709)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2351 (0.1690 -- 0.8789)  data: 0.0330 (0.0001 -- 0.6482)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5331 (0.6042)  acc1: 77.7778 (83.8174)  acc5: 100.0000 (97.9253)  time: 0.2208 (0.1327 -- 0.8789)  data: 0.0327 (0.0001 -- 0.6482)  max mem: 16413
Val: Total time: 0:00:08 (0.2966 s / it)
* Acc@1 85.062 Acc@5 97.718 loss 0.594
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.27%
Epoch: [131]  [  0/160]  eta: 0:20:59  lr: 0.000032  min_lr: 0.000008  loss: 1.0815 (1.0815)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9097 (5.9097)  time: 7.8692 (7.8692 -- 7.8692)  data: 7.3510 (7.3510 -- 7.3510)  max mem: 16413
Epoch: [131]  [ 20/160]  eta: 0:02:50  lr: 0.000032  min_lr: 0.000008  loss: 1.6026 (1.5913)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0984 (6.4758)  time: 0.8874 (0.5230 -- 3.1529)  data: 0.2086 (0.0004 -- 2.3950)  max mem: 16413
[2023-09-05 03:01:56,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=118, lr=[8.241411350435795e-06, 8.241411350435795e-06, 9.15712372270644e-06, 9.15712372270644e-06, 1.0174581914118264e-05, 1.0174581914118264e-05, 1.1305091015686962e-05, 1.1305091015686962e-05, 1.2561212239652179e-05, 1.2561212239652179e-05, 1.395690248850242e-05, 1.395690248850242e-05, 1.5507669431669355e-05, 1.5507669431669355e-05, 1.7230743812965948e-05, 1.7230743812965948e-05, 1.91452709032955e-05, 1.91452709032955e-05, 2.1272523225883886e-05, 2.1272523225883886e-05, 2.3636136917648763e-05, 2.3636136917648763e-05, 2.626237435294307e-05, 2.626237435294307e-05, 2.9180415947714523e-05, 2.9180415947714523e-05, 3.242268438634947e-05, 3.242268438634947e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 03:01:56,147] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=17.508661587868474, CurrSamplesPerSec=22.550466048613863, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [131]  [ 40/160]  eta: 0:02:13  lr: 0.000032  min_lr: 0.000008  loss: 1.7175 (1.6564)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5030 (6.6750)  time: 1.0043 (0.5223 -- 4.5426)  data: 0.4026 (0.0001 -- 3.2285)  max mem: 16413
Epoch: [131]  [ 60/160]  eta: 0:01:40  lr: 0.000032  min_lr: 0.000008  loss: 1.6369 (1.6529)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2787 (7.1440)  time: 0.7703 (0.5159 -- 3.5102)  data: 0.2220 (0.0003 -- 2.9982)  max mem: 16413
Epoch: [131]  [ 80/160]  eta: 0:01:19  lr: 0.000032  min_lr: 0.000008  loss: 1.6089 (1.6438)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6871 (7.1939)  time: 0.9538 (0.5155 -- 4.3357)  data: 0.4130 (0.0004 -- 3.7911)  max mem: 16413
[2023-09-05 03:02:46,001] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:02:46,002] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 03:02:46,002] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:02:46,002] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [131]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000008  loss: 1.6183 (1.6449)  loss_scale: 32768.0000 (34714.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7031 (7.1660)  time: 0.6803 (0.5250 -- 3.1171)  data: 0.1304 (0.0002 -- 2.5810)  max mem: 16413
[2023-09-05 03:02:55,531] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21067
[2023-09-05 03:02:55,531] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21067
[2023-09-05 03:02:55,531] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 03:02:55,531] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 03:02:55,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [131]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.5485 (1.6301)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6552 (7.0878)  time: 0.9177 (0.5317 -- 3.4400)  data: 0.3616 (0.0004 -- 2.8906)  max mem: 16413
Epoch: [131]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.4999 (1.6253)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9322 (7.1342)  time: 0.8471 (0.5419 -- 3.5757)  data: 0.2928 (0.0004 -- 3.0488)  max mem: 16413
[2023-09-05 03:03:25,784] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21103
[2023-09-05 03:03:25,784] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21103
[2023-09-05 03:03:25,785] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:03:25,785] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:03:25,785] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.4829 (1.6084)  loss_scale: 16384.0000 (33484.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4750 (7.2037)  time: 0.7340 (0.4938 -- 2.8589)  data: 0.2080 (0.0002 -- 2.3019)  max mem: 16413
Epoch: [131] Total time: 0:02:23 (0.8953 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.4829 (1.6367)  loss_scale: 16384.0000 (33484.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4750 (7.2037)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1463 (0.1463)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3245 (2.3245 -- 2.3245)  data: 2.1120 (2.1120 -- 2.1120)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6068 (0.6081)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4097 (0.1961 -- 2.3245)  data: 0.1941 (0.0004 -- 2.1120)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5001 (0.5768)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2252 (0.1692 -- 0.3347)  data: 0.0155 (0.0001 -- 0.1635)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5188 (0.6131)  acc1: 77.7778 (84.2324)  acc5: 100.0000 (98.7552)  time: 0.2090 (0.1323 -- 0.3347)  data: 0.0152 (0.0001 -- 0.1635)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.599
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.27%
Epoch: [132]  [  0/160]  eta: 0:16:50  lr: 0.000032  min_lr: 0.000008  loss: 1.4375 (1.4375)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1382 (10.1382)  time: 6.3187 (6.3187 -- 6.3187)  data: 4.6781 (4.6781 -- 4.6781)  max mem: 16413
Epoch: [132]  [ 20/160]  eta: 0:02:41  lr: 0.000032  min_lr: 0.000008  loss: 1.6265 (1.5986)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5138 (7.6221)  time: 0.8978 (0.5357 -- 2.3251)  data: 0.3040 (0.0007 -- 1.7705)  max mem: 16413
Epoch: [132]  [ 40/160]  eta: 0:02:01  lr: 0.000032  min_lr: 0.000008  loss: 1.7010 (1.6657)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5138 (7.1995)  time: 0.8676 (0.5397 -- 2.8426)  data: 0.2368 (0.0002 -- 1.3867)  max mem: 16413
Epoch: [132]  [ 60/160]  eta: 0:01:41  lr: 0.000032  min_lr: 0.000008  loss: 1.6641 (1.6653)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6841 (6.9235)  time: 1.0037 (0.5209 -- 4.5189)  data: 0.3977 (0.0003 -- 3.9833)  max mem: 16413
Epoch: [132]  [ 80/160]  eta: 0:01:15  lr: 0.000032  min_lr: 0.000008  loss: 1.8089 (1.6910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9583 (6.6881)  time: 0.7137 (0.5252 -- 2.2059)  data: 0.1610 (0.0004 -- 1.6754)  max mem: 16413
Epoch: [132]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000008  loss: 1.6804 (1.6780)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7015 (6.7240)  time: 0.9145 (0.5335 -- 2.8245)  data: 0.1813 (0.0004 -- 2.2987)  max mem: 16413
[2023-09-05 03:05:29,408] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:05:29,408] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:05:29,408] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:05:29,408] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [132]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.7213 (1.6692)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4101 (6.8929)  time: 0.8979 (0.5241 -- 3.3064)  data: 0.1860 (0.0003 -- 1.1770)  max mem: 16413
[2023-09-05 03:05:54,898] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21260
[2023-09-05 03:05:54,898] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:05:54,898] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21260
[2023-09-05 03:05:54,899] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:05:54,899] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [132]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.8661 (1.6834)  loss_scale: 32768.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0615 (6.8684)  time: 0.8303 (0.5110 -- 2.8390)  data: 0.0014 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.6222 (1.6873)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4680 (6.9344)  time: 0.7273 (0.4961 -- 2.0777)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [132] Total time: 0:02:22 (0.8931 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.6222 (1.6570)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4680 (6.9344)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1318 (0.1318)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4250 (2.4250 -- 2.4250)  data: 2.2123 (2.2123 -- 2.2123)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6972 (0.6409)  acc1: 77.7778 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4275 (0.1961 -- 2.4250)  data: 0.2170 (0.0006 -- 2.2123)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5120 (0.5990)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (98.9418)  time: 0.2205 (0.1690 -- 0.3674)  data: 0.0146 (0.0001 -- 0.1646)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5990 (0.6240)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (99.1701)  time: 0.2078 (0.1326 -- 0.3674)  data: 0.0143 (0.0001 -- 0.1646)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 82.988 Acc@5 98.340 loss 0.622
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 85.27%
Epoch: [133]  [  0/160]  eta: 0:16:28  lr: 0.000032  min_lr: 0.000008  loss: 1.9105 (1.9105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7912 (9.7912)  time: 6.1769 (6.1769 -- 6.1769)  data: 5.6558 (5.6558 -- 5.6558)  max mem: 16413
Epoch: [133]  [ 20/160]  eta: 0:02:39  lr: 0.000032  min_lr: 0.000008  loss: 1.4603 (1.5391)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0056 (6.8637)  time: 0.8869 (0.5335 -- 2.9013)  data: 0.3393 (0.0003 -- 2.3598)  max mem: 16413
Epoch: [133]  [ 40/160]  eta: 0:02:05  lr: 0.000032  min_lr: 0.000008  loss: 1.4081 (1.4975)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7413 (6.3681)  time: 0.9410 (0.5323 -- 2.4740)  data: 0.2265 (0.0003 -- 1.4918)  max mem: 16413
Epoch: [133]  [ 60/160]  eta: 0:01:39  lr: 0.000032  min_lr: 0.000008  loss: 1.8633 (1.5932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8664 (6.5628)  time: 0.8828 (0.5401 -- 2.4892)  data: 0.0515 (0.0004 -- 0.5083)  max mem: 16413
Epoch: [133]  [ 80/160]  eta: 0:01:17  lr: 0.000032  min_lr: 0.000008  loss: 1.8341 (1.6403)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3591 (6.6157)  time: 0.8859 (0.5227 -- 2.4351)  data: 0.0014 (0.0005 -- 0.0052)  max mem: 16413
Epoch: [133]  [100/160]  eta: 0:00:56  lr: 0.000032  min_lr: 0.000008  loss: 1.5427 (1.6402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6642 (6.8023)  time: 0.8423 (0.5203 -- 3.5589)  data: 0.0015 (0.0003 -- 0.0050)  max mem: 16413
[2023-09-05 03:08:00,229] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:08:00,229] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:08:00,230] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:08:00,230] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [133]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000008  loss: 1.6659 (1.6492)  loss_scale: 32768.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8399 (6.9281)  time: 0.8769 (0.5193 -- 2.3833)  data: 0.1110 (0.0002 -- 1.7764)  max mem: 16413
Epoch: [133]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.5702 (1.6391)  loss_scale: 32768.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9592 (6.7964)  time: 0.8420 (0.5176 -- 2.8840)  data: 0.0013 (0.0002 -- 0.0036)  max mem: 16413
[2023-09-05 03:08:36,429] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21434
[2023-09-05 03:08:36,429] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21434
[2023-09-05 03:08:36,429] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:08:36,429] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:08:36,429] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000008  loss: 1.5815 (1.6321)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6082 (6.8521)  time: 0.6828 (0.4812 -- 2.8670)  data: 0.0007 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [133] Total time: 0:02:22 (0.8903 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000008  loss: 1.5815 (1.6597)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6082 (6.8521)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1398 (0.1398)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3553 (2.3553 -- 2.3553)  data: 2.1390 (2.1390 -- 2.1390)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4428 (0.5639)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4275 (0.1938 -- 2.3553)  data: 0.2194 (0.0006 -- 2.1390)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4484 (0.5415)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2260 (0.1700 -- 0.4754)  data: 0.0250 (0.0001 -- 0.2619)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4530 (0.5702)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (98.7552)  time: 0.2121 (0.1331 -- 0.4754)  data: 0.0246 (0.0001 -- 0.2619)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 84.025 Acc@5 98.340 loss 0.576
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.27%
Epoch: [134]  [  0/160]  eta: 0:21:50  lr: 0.000032  min_lr: 0.000008  loss: 2.2065 (2.2065)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1499 (6.1499)  time: 8.1880 (8.1880 -- 8.1880)  data: 5.5004 (5.5004 -- 5.5004)  max mem: 16413
Epoch: [134]  [ 20/160]  eta: 0:02:45  lr: 0.000032  min_lr: 0.000008  loss: 1.6453 (1.6684)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1235 (7.3360)  time: 0.8317 (0.5322 -- 3.5195)  data: 0.1282 (0.0004 -- 1.5786)  max mem: 16413
Epoch: [134]  [ 40/160]  eta: 0:02:03  lr: 0.000032  min_lr: 0.000008  loss: 1.6457 (1.6503)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6892 (7.1531)  time: 0.8755 (0.5332 -- 2.0787)  data: 0.0997 (0.0004 -- 1.0902)  max mem: 16413
Epoch: [134]  [ 60/160]  eta: 0:01:39  lr: 0.000032  min_lr: 0.000008  loss: 1.5858 (1.6253)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8089 (7.0618)  time: 0.9197 (0.5129 -- 3.2037)  data: 0.0979 (0.0003 -- 1.5145)  max mem: 16413
Epoch: [134]  [ 80/160]  eta: 0:01:17  lr: 0.000032  min_lr: 0.000008  loss: 1.6619 (1.6189)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4811 (6.9241)  time: 0.8885 (0.5214 -- 3.1733)  data: 0.0191 (0.0004 -- 0.3450)  max mem: 16413
Epoch: [134]  [100/160]  eta: 0:00:56  lr: 0.000032  min_lr: 0.000008  loss: 1.5417 (1.6079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7080 (7.2317)  time: 0.8354 (0.5328 -- 3.9349)  data: 0.0013 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [134]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000008  loss: 1.6520 (1.6004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3785 (7.4295)  time: 0.7829 (0.5309 -- 2.4962)  data: 0.0019 (0.0003 -- 0.0086)  max mem: 16413
[2023-09-05 03:10:40,664] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:10:40,664] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:10:40,668] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:10:40,668] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [134]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000008  loss: 1.8056 (1.6250)  loss_scale: 32768.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8242 (7.3724)  time: 0.9355 (0.5252 -- 3.0984)  data: 0.0029 (0.0005 -- 0.0153)  max mem: 16413
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.5006 (1.6137)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6081 (7.3049)  time: 0.6889 (0.4967 -- 2.6770)  data: 0.0011 (0.0001 -- 0.0048)  max mem: 16413
Epoch: [134] Total time: 0:02:22 (0.8927 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.5006 (1.6198)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6081 (7.3049)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1291 (0.1291)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3697 (2.3697 -- 2.3697)  data: 2.1530 (2.1530 -- 2.1530)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5515 (0.5805)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4281 (0.2044 -- 2.3697)  data: 0.2059 (0.0003 -- 2.1530)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4684 (0.5560)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2209 (0.1700 -- 0.3407)  data: 0.0113 (0.0001 -- 0.1097)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4684 (0.5875)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2031 (0.1328 -- 0.3407)  data: 0.0110 (0.0001 -- 0.1097)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.576
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.27%
Epoch: [135]  [  0/160]  eta: 0:20:30  lr: 0.000031  min_lr: 0.000008  loss: 1.2585 (1.2585)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3632 (6.3632)  time: 7.6884 (7.6884 -- 7.6884)  data: 7.1188 (7.1188 -- 7.1188)  max mem: 16413
Epoch: [135]  [ 20/160]  eta: 0:02:50  lr: 0.000031  min_lr: 0.000008  loss: 1.6031 (1.5918)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3644 (7.7729)  time: 0.8954 (0.5301 -- 2.8610)  data: 0.3433 (0.0004 -- 2.3224)  max mem: 16413
Epoch: [135]  [ 40/160]  eta: 0:02:07  lr: 0.000031  min_lr: 0.000008  loss: 1.5795 (1.6037)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8071 (7.4453)  time: 0.8934 (0.5131 -- 3.5973)  data: 0.3461 (0.0004 -- 3.0906)  max mem: 16413
Epoch: [135]  [ 60/160]  eta: 0:01:40  lr: 0.000031  min_lr: 0.000008  loss: 1.6166 (1.6141)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4943 (7.4407)  time: 0.8887 (0.5347 -- 2.6624)  data: 0.3370 (0.0005 -- 2.1462)  max mem: 16413
Epoch: [135]  [ 80/160]  eta: 0:01:15  lr: 0.000031  min_lr: 0.000008  loss: 1.7728 (1.6352)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0041 (7.3840)  time: 0.7805 (0.5339 -- 2.5401)  data: 0.2019 (0.0006 -- 1.9995)  max mem: 16413
[2023-09-05 03:12:36,788] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21683
[2023-09-05 03:12:36,789] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:12:36,788] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21683
[2023-09-05 03:12:36,789] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:12:36,789] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [135]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000008  loss: 1.6424 (1.6368)  loss_scale: 16384.0000 (29848.0792)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0422 (7.1732)  time: 0.9346 (0.5310 -- 2.3904)  data: 0.3702 (0.0004 -- 1.8414)  max mem: 16413
Epoch: [135]  [120/160]  eta: 0:00:37  lr: 0.000031  min_lr: 0.000008  loss: 1.7826 (1.6451)  loss_scale: 16384.0000 (27622.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5262 (7.1188)  time: 0.8453 (0.5216 -- 3.3876)  data: 0.3014 (0.0001 -- 2.8536)  max mem: 16413
Epoch: [135]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.4793 (1.6316)  loss_scale: 16384.0000 (26028.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4765 (7.0864)  time: 0.9314 (0.5250 -- 4.2158)  data: 0.3823 (0.0004 -- 3.6841)  max mem: 16413
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.6898 (1.6388)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6954 (7.0063)  time: 0.6548 (0.4961 -- 2.0208)  data: 0.1312 (0.0002 -- 1.5070)  max mem: 16413
Epoch: [135] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.6898 (1.6511)  loss_scale: 16384.0000 (24883.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6954 (7.0063)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1248 (0.1248)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4883 (2.4883 -- 2.4883)  data: 2.2478 (2.2478 -- 2.2478)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5216 (0.5719)  acc1: 77.7778 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4196 (0.2010 -- 2.4883)  data: 0.2054 (0.0005 -- 2.2478)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5216 (0.5659)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (97.8836)  time: 0.2197 (0.1707 -- 0.4966)  data: 0.0158 (0.0001 -- 0.3015)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5375 (0.5962)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (97.9253)  time: 0.2054 (0.1332 -- 0.4966)  data: 0.0155 (0.0001 -- 0.3015)  max mem: 16413
Val: Total time: 0:00:07 (0.2915 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.584
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.27%
Epoch: [136]  [  0/160]  eta: 0:20:07  lr: 0.000031  min_lr: 0.000008  loss: 2.0366 (2.0366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8530 (11.8530)  time: 7.5444 (7.5444 -- 7.5444)  data: 7.0176 (7.0176 -- 7.0176)  max mem: 16413
Epoch: [136]  [ 20/160]  eta: 0:02:45  lr: 0.000031  min_lr: 0.000008  loss: 1.6245 (1.6546)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7212 (7.4297)  time: 0.8671 (0.5124 -- 2.4552)  data: 0.2195 (0.0005 -- 1.9084)  max mem: 16413
Epoch: [136]  [ 40/160]  eta: 0:02:02  lr: 0.000031  min_lr: 0.000008  loss: 1.5976 (1.6751)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8008 (6.9891)  time: 0.8497 (0.5248 -- 2.8779)  data: 0.2477 (0.0003 -- 2.3239)  max mem: 16413
[2023-09-05 03:14:41,898] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:14:41,899] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:14:41,899] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:14:41,899] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [136]  [ 60/160]  eta: 0:01:38  lr: 0.000031  min_lr: 0.000008  loss: 1.7424 (1.7215)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5042 (6.8573)  time: 0.8976 (0.5254 -- 4.0536)  data: 0.3069 (0.0006 -- 3.5221)  max mem: 16413
Epoch: [136]  [ 80/160]  eta: 0:01:16  lr: 0.000031  min_lr: 0.000008  loss: 1.5128 (1.6880)  loss_scale: 32768.0000 (22249.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0607 (6.8167)  time: 0.8799 (0.5251 -- 2.8151)  data: 0.1358 (0.0004 -- 1.4050)  max mem: 16413
Epoch: [136]  [100/160]  eta: 0:00:57  lr: 0.000031  min_lr: 0.000008  loss: 1.6350 (1.6615)  loss_scale: 32768.0000 (24332.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3371 (6.8840)  time: 0.9332 (0.5205 -- 4.1855)  data: 0.0013 (0.0003 -- 0.0028)  max mem: 16413
[2023-09-05 03:15:30,816] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21868
[2023-09-05 03:15:30,816] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21868
[2023-09-05 03:15:30,816] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:15:30,816] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:15:30,816] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [136]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.7363 (1.6842)  loss_scale: 16384.0000 (23966.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9683 (6.9322)  time: 0.7679 (0.5275 -- 2.6102)  data: 0.0195 (0.0003 -- 0.3450)  max mem: 16413
Epoch: [136]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.5104 (1.6693)  loss_scale: 16384.0000 (22891.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8452 (6.9234)  time: 0.8123 (0.5384 -- 3.7073)  data: 0.0353 (0.0006 -- 0.6585)  max mem: 16413
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.6059 (1.6720)  loss_scale: 16384.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7058 (7.0390)  time: 0.7297 (0.4960 -- 3.7839)  data: 0.0343 (0.0002 -- 0.6585)  max mem: 16413
Epoch: [136] Total time: 0:02:21 (0.8823 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.6059 (1.6678)  loss_scale: 16384.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7058 (7.0390)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1347 (0.1347)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2934 (2.2934 -- 2.2934)  data: 2.0852 (2.0852 -- 2.0852)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5554 (0.6120)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4332 (0.2014 -- 2.2934)  data: 0.2096 (0.0008 -- 2.0852)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5554 (0.5955)  acc1: 77.7778 (84.1270)  acc5: 100.0000 (98.4127)  time: 0.2303 (0.1699 -- 0.4654)  data: 0.0213 (0.0001 -- 0.2038)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5928 (0.6126)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (98.7552)  time: 0.2128 (0.1332 -- 0.4654)  data: 0.0203 (0.0001 -- 0.2038)  max mem: 16413
Val: Total time: 0:00:07 (0.2921 s / it)
* Acc@1 84.025 Acc@5 98.340 loss 0.597
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.27%
Epoch: [137]  [  0/160]  eta: 0:20:42  lr: 0.000031  min_lr: 0.000008  loss: 2.0644 (2.0644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0684 (7.0684)  time: 7.7686 (7.7686 -- 7.7686)  data: 7.2358 (7.2358 -- 7.2358)  max mem: 16413
Epoch: [137]  [ 20/160]  eta: 0:02:48  lr: 0.000031  min_lr: 0.000008  loss: 1.7705 (1.7644)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8955 (7.4242)  time: 0.8777 (0.5148 -- 3.6341)  data: 0.3043 (0.0004 -- 3.0402)  max mem: 16413
Epoch: [137]  [ 40/160]  eta: 0:02:10  lr: 0.000031  min_lr: 0.000008  loss: 1.5993 (1.7278)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9210 (6.9419)  time: 0.9637 (0.5328 -- 3.7025)  data: 0.3780 (0.0002 -- 3.1507)  max mem: 16413
Epoch: [137]  [ 60/160]  eta: 0:01:40  lr: 0.000031  min_lr: 0.000008  loss: 1.6090 (1.7042)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3490 (6.9391)  time: 0.8231 (0.5132 -- 2.5623)  data: 0.0790 (0.0003 -- 1.5591)  max mem: 16413
[2023-09-05 03:17:31,342] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:17:31,342] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:17:31,344] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:17:31,344] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:17:32,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=124, lr=[7.835892235242024e-06, 7.835892235242024e-06, 8.706546928046695e-06, 8.706546928046695e-06, 9.673941031162992e-06, 9.673941031162992e-06, 1.074882336795888e-05, 1.074882336795888e-05, 1.1943137075509867e-05, 1.1943137075509867e-05, 1.3270152306122074e-05, 1.3270152306122074e-05, 1.4744613673468971e-05, 1.4744613673468971e-05, 1.6382904081632187e-05, 1.6382904081632187e-05, 1.82032267573691e-05, 1.82032267573691e-05, 2.0225807508187884e-05, 2.0225807508187884e-05, 2.2473119453542096e-05, 2.2473119453542096e-05, 2.4970132726157884e-05, 2.4970132726157884e-05, 2.7744591917953204e-05, 2.7744591917953204e-05, 3.0827324353281336e-05, 3.0827324353281336e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 03:17:32,496] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=17.527822208460687, CurrSamplesPerSec=21.991609992681354, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [137]  [ 80/160]  eta: 0:01:18  lr: 0.000031  min_lr: 0.000008  loss: 1.6346 (1.6967)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5631 (6.7390)  time: 0.9072 (0.5386 -- 3.9644)  data: 0.2526 (0.0001 -- 3.4054)  max mem: 16413
Epoch: [137]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000008  loss: 1.7360 (1.7014)  loss_scale: 32768.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9813 (6.8770)  time: 0.8247 (0.5304 -- 3.6838)  data: 0.2673 (0.0002 -- 3.1577)  max mem: 16413
Epoch: [137]  [120/160]  eta: 0:00:37  lr: 0.000031  min_lr: 0.000008  loss: 1.8271 (1.7016)  loss_scale: 32768.0000 (22341.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9319 (6.9350)  time: 0.8377 (0.5314 -- 2.7154)  data: 0.1729 (0.0003 -- 2.1879)  max mem: 16413
[2023-09-05 03:18:14,938] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22044
[2023-09-05 03:18:14,938] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22044
[2023-09-05 03:18:14,939] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:18:14,939] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:18:14,939] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [137]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000008  loss: 1.5190 (1.6834)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5378 (6.9884)  time: 0.9375 (0.5135 -- 3.5996)  data: 0.0013 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000008  loss: 1.8604 (1.7017)  loss_scale: 16384.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3571 (7.0594)  time: 0.6362 (0.4956 -- 2.8057)  data: 0.0009 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [137] Total time: 0:02:23 (0.8964 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000008  loss: 1.8604 (1.6574)  loss_scale: 16384.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3571 (7.0594)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1541 (0.1541)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2841 (2.2841 -- 2.2841)  data: 2.0548 (2.0548 -- 2.0548)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5657 (0.6139)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4153 (0.2020 -- 2.2841)  data: 0.1954 (0.0006 -- 2.0548)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5657 (0.5902)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2205 (0.1695 -- 0.3377)  data: 0.0121 (0.0001 -- 0.1275)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5752 (0.6314)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (98.3402)  time: 0.2046 (0.1323 -- 0.3377)  data: 0.0114 (0.0001 -- 0.1275)  max mem: 16413
Val: Total time: 0:00:07 (0.2846 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.614
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.27%
Epoch: [138]  [  0/160]  eta: 0:23:20  lr: 0.000031  min_lr: 0.000008  loss: 1.6951 (1.6951)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9561 (3.9561)  time: 8.7537 (8.7537 -- 8.7537)  data: 8.2150 (8.2150 -- 8.2150)  max mem: 16413
Epoch: [138]  [ 20/160]  eta: 0:02:47  lr: 0.000031  min_lr: 0.000008  loss: 1.5812 (1.5896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5988 (6.4849)  time: 0.8151 (0.5251 -- 3.6721)  data: 0.2675 (0.0005 -- 3.1449)  max mem: 16413
Epoch: [138]  [ 40/160]  eta: 0:02:04  lr: 0.000031  min_lr: 0.000008  loss: 1.5491 (1.5653)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4610 (6.6349)  time: 0.8674 (0.5288 -- 3.6706)  data: 0.2943 (0.0004 -- 3.1419)  max mem: 16413
Epoch: [138]  [ 60/160]  eta: 0:01:39  lr: 0.000031  min_lr: 0.000008  loss: 1.5081 (1.5408)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2874 (7.0802)  time: 0.9028 (0.5161 -- 2.7004)  data: 0.2299 (0.0004 -- 1.6676)  max mem: 16413
Epoch: [138]  [ 80/160]  eta: 0:01:17  lr: 0.000031  min_lr: 0.000008  loss: 1.5442 (1.5548)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7906 (7.0817)  time: 0.8923 (0.5237 -- 3.2749)  data: 0.3522 (0.0001 -- 2.7230)  max mem: 16413
[2023-09-05 03:20:15,915] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:20:15,916] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:20:15,916] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:20:15,916] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:20:19,275] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22176
[2023-09-05 03:20:19,275] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22176
[2023-09-05 03:20:19,275] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:20:19,275] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:20:19,275] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [138]  [100/160]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000008  loss: 1.6631 (1.5840)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2587 (7.1037)  time: 0.7573 (0.5381 -- 1.9146)  data: 0.1295 (0.0005 -- 1.2878)  max mem: 16413
Epoch: [138]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000008  loss: 1.6054 (1.5853)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6766 (7.0761)  time: 0.8699 (0.5339 -- 2.9135)  data: 0.0465 (0.0007 -- 0.6158)  max mem: 16413
Epoch: [138]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.5072 (1.5745)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1524 (7.0456)  time: 0.8696 (0.5311 -- 1.9765)  data: 0.1608 (0.0004 -- 1.4354)  max mem: 16413
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.6088 (1.5700)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8991 (7.0926)  time: 0.6859 (0.4995 -- 1.6149)  data: 0.1090 (0.0002 -- 1.0802)  max mem: 16413
Epoch: [138] Total time: 0:02:21 (0.8840 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.6088 (1.6118)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8991 (7.0926)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1398 (0.1398)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3098 (2.3098 -- 2.3098)  data: 2.0575 (2.0575 -- 2.0575)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5653 (0.6652)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (96.9697)  time: 0.4262 (0.1936 -- 2.3098)  data: 0.2052 (0.0006 -- 2.0575)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5653 (0.6320)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2243 (0.1699 -- 0.4551)  data: 0.0162 (0.0001 -- 0.1878)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5707 (0.6677)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.2083 (0.1331 -- 0.4551)  data: 0.0158 (0.0001 -- 0.1878)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 83.195 Acc@5 96.888 loss 0.639
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 85.27%
Epoch: [139]  [  0/160]  eta: 0:21:16  lr: 0.000030  min_lr: 0.000008  loss: 1.3057 (1.3057)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7408 (7.7408)  time: 7.9764 (7.9764 -- 7.9764)  data: 5.7169 (5.7169 -- 5.7169)  max mem: 16413
Epoch: [139]  [ 20/160]  eta: 0:02:45  lr: 0.000030  min_lr: 0.000008  loss: 1.6179 (1.6039)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3028 (6.7986)  time: 0.8414 (0.5223 -- 3.8281)  data: 0.0025 (0.0003 -- 0.0162)  max mem: 16413
Epoch: [139]  [ 40/160]  eta: 0:02:11  lr: 0.000030  min_lr: 0.000008  loss: 1.8443 (1.6996)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4365 (7.1080)  time: 1.0097 (0.5334 -- 3.8260)  data: 0.0751 (0.0001 -- 1.0618)  max mem: 16413
Epoch: [139]  [ 60/160]  eta: 0:01:36  lr: 0.000030  min_lr: 0.000008  loss: 1.5402 (1.6690)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1303 (7.0979)  time: 0.6784 (0.5333 -- 2.0299)  data: 0.0573 (0.0001 -- 1.1187)  max mem: 16413
[2023-09-05 03:22:21,384] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:22:21,384] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:22:21,384] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:22:21,384] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [139]  [ 80/160]  eta: 0:01:16  lr: 0.000030  min_lr: 0.000008  loss: 1.5608 (1.6407)  loss_scale: 32768.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4688 (6.9845)  time: 0.9355 (0.5195 -- 3.0534)  data: 0.2241 (0.0005 -- 2.5114)  max mem: 16413
Epoch: [139]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000008  loss: 1.7421 (1.6647)  loss_scale: 32768.0000 (22223.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8246 (6.9748)  time: 0.8880 (0.5363 -- 2.1160)  data: 0.0787 (0.0004 -- 1.3698)  max mem: 16413
[2023-09-05 03:23:01,361] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22349
[2023-09-05 03:23:01,361] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22349
[2023-09-05 03:23:01,362] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:23:01,362] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:23:01,362] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [139]  [120/160]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000008  loss: 1.5035 (1.6559)  loss_scale: 16384.0000 (22341.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6484 (7.0658)  time: 0.8690 (0.5290 -- 2.8399)  data: 0.0625 (0.0004 -- 1.2144)  max mem: 16413
Epoch: [139]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.7294 (1.6565)  loss_scale: 16384.0000 (21496.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3594 (7.0038)  time: 0.8405 (0.5353 -- 3.1071)  data: 0.0817 (0.0004 -- 1.4543)  max mem: 16413
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.6883 (1.6644)  loss_scale: 16384.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6148 (7.0138)  time: 0.6756 (0.4960 -- 2.1533)  data: 0.1062 (0.0001 -- 1.6098)  max mem: 16413
Epoch: [139] Total time: 0:02:22 (0.8877 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.6883 (1.6801)  loss_scale: 16384.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6148 (7.0138)
[2023-09-05 03:23:39,632] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-09-05 03:23:39,633] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-09-05 03:23:39,633] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-09-05 03:23:39,633] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-09-05 03:23:40,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-09-05 03:23:40,539] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1413 (0.1413)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5317 (2.5317 -- 2.5317)  data: 2.3100 (2.3100 -- 2.3100)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5649 (0.5884)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (100.0000)  time: 0.4315 (0.2013 -- 2.5317)  data: 0.2121 (0.0007 -- 2.3100)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5508 (0.5904)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (98.4127)  time: 0.2162 (0.1699 -- 0.3315)  data: 0.0077 (0.0001 -- 0.1289)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5508 (0.6190)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (98.3402)  time: 0.1972 (0.1330 -- 0.3315)  data: 0.0068 (0.0001 -- 0.1289)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 83.817 Acc@5 98.133 loss 0.605
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 85.27%
Epoch: [140]  [  0/160]  eta: 0:20:15  lr: 0.000030  min_lr: 0.000008  loss: 1.7068 (1.7068)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6370 (9.6370)  time: 7.5960 (7.5960 -- 7.5960)  data: 5.7967 (5.7967 -- 5.7967)  max mem: 16413
[2023-09-05 03:24:02,648] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22408
[2023-09-05 03:24:02,649] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22408
[2023-09-05 03:24:02,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:24:02,650] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:24:02,650] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [140]  [ 20/160]  eta: 0:02:51  lr: 0.000030  min_lr: 0.000008  loss: 1.8124 (1.7421)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9548 (6.9032)  time: 0.9094 (0.5294 -- 4.0839)  data: 0.0512 (0.0004 -- 0.9976)  max mem: 16413
Epoch: [140]  [ 40/160]  eta: 0:02:08  lr: 0.000030  min_lr: 0.000008  loss: 1.6946 (1.7163)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7336 (6.7583)  time: 0.9058 (0.5293 -- 4.4984)  data: 0.0014 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [140]  [ 60/160]  eta: 0:01:41  lr: 0.000030  min_lr: 0.000008  loss: 1.8529 (1.7544)  loss_scale: 8192.0000 (9266.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7725 (6.8120)  time: 0.8862 (0.5314 -- 3.5740)  data: 0.0018 (0.0004 -- 0.0069)  max mem: 16413
Epoch: [140]  [ 80/160]  eta: 0:01:16  lr: 0.000030  min_lr: 0.000008  loss: 1.4819 (1.7054)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0992 (6.8783)  time: 0.7728 (0.5204 -- 3.2614)  data: 0.0016 (0.0005 -- 0.0032)  max mem: 16413
Epoch: [140]  [100/160]  eta: 0:00:57  lr: 0.000030  min_lr: 0.000008  loss: 1.5880 (1.6887)  loss_scale: 8192.0000 (8840.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8974 (7.0050)  time: 0.9804 (0.5188 -- 4.0392)  data: 0.0012 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [140]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000008  loss: 1.7919 (1.7191)  loss_scale: 8192.0000 (8733.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9127 (7.0348)  time: 0.7328 (0.5347 -- 1.9805)  data: 0.0014 (0.0003 -- 0.0033)  max mem: 16413
[2023-09-05 03:25:54,978] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:25:54,978] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 03:25:54,980] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:25:54,980] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [140]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.7043 (1.7164)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7330 (6.9809)  time: 0.9708 (0.5255 -- 3.0531)  data: 0.0536 (0.0002 -- 1.0463)  max mem: 16413
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.6095 (1.7114)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9794 (6.9622)  time: 0.7945 (0.4988 -- 3.0531)  data: 0.0011 (0.0002 -- 0.0056)  max mem: 16413
Epoch: [140] Total time: 0:02:23 (0.8977 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.6095 (1.6895)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9794 (6.9622)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1414 (0.1414)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3253 (2.3253 -- 2.3253)  data: 2.0946 (2.0946 -- 2.0946)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3456 (0.5811)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4317 (0.2033 -- 2.3253)  data: 0.2178 (0.0008 -- 2.0946)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5304 (0.5818)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2285 (0.1698 -- 0.5037)  data: 0.0254 (0.0001 -- 0.2917)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5983 (0.6079)  acc1: 77.7778 (83.8174)  acc5: 100.0000 (96.6805)  time: 0.2130 (0.1328 -- 0.5037)  data: 0.0251 (0.0001 -- 0.2917)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 85.270 Acc@5 97.095 loss 0.605
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 85.27%
Epoch: [141]  [  0/160]  eta: 0:27:21  lr: 0.000030  min_lr: 0.000008  loss: 1.1896 (1.1896)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9101 (5.9101)  time: 10.2606 (10.2606 -- 10.2606)  data: 6.9941 (6.9941 -- 6.9941)  max mem: 16413
Epoch: [141]  [ 20/160]  eta: 0:02:56  lr: 0.000030  min_lr: 0.000008  loss: 1.7664 (1.6874)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3552 (7.4597)  time: 0.8096 (0.5265 -- 4.1914)  data: 0.0022 (0.0004 -- 0.0141)  max mem: 16413
Epoch: [141]  [ 40/160]  eta: 0:02:08  lr: 0.000030  min_lr: 0.000008  loss: 1.5745 (1.6586)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1568 (7.3531)  time: 0.8654 (0.5280 -- 3.4533)  data: 0.0023 (0.0003 -- 0.0161)  max mem: 16413
Epoch: [141]  [ 60/160]  eta: 0:01:42  lr: 0.000030  min_lr: 0.000008  loss: 1.6992 (1.6620)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0848 (7.5155)  time: 0.9491 (0.5300 -- 2.6668)  data: 0.2187 (0.0003 -- 1.8864)  max mem: 16413
[2023-09-05 03:27:35,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22636
[2023-09-05 03:27:35,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22636
[2023-09-05 03:27:35,103] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:27:35,103] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:27:35,103] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [141]  [ 80/160]  eta: 0:01:16  lr: 0.000030  min_lr: 0.000008  loss: 1.6284 (1.6779)  loss_scale: 16384.0000 (15878.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8625 (7.3840)  time: 0.7316 (0.5223 -- 2.7411)  data: 0.1800 (0.0002 -- 2.2064)  max mem: 16413
Epoch: [141]  [100/160]  eta: 0:00:57  lr: 0.000030  min_lr: 0.000008  loss: 1.6634 (1.6865)  loss_scale: 8192.0000 (14356.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2415 (7.2893)  time: 0.9917 (0.5305 -- 4.4669)  data: 0.3472 (0.0003 -- 3.9261)  max mem: 16413
Epoch: [141]  [120/160]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000008  loss: 1.5792 (1.6679)  loss_scale: 8192.0000 (13337.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0669 (7.2304)  time: 0.8621 (0.5293 -- 3.5967)  data: 0.0105 (0.0004 -- 0.1842)  max mem: 16413
Epoch: [141]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000008  loss: 1.4716 (1.6368)  loss_scale: 8192.0000 (12607.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7057 (7.3526)  time: 0.9187 (0.5271 -- 3.8131)  data: 0.0017 (0.0003 -- 0.0108)  max mem: 16413
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000008  loss: 1.5803 (1.6263)  loss_scale: 8192.0000 (12083.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4989 (7.4156)  time: 0.6781 (0.4958 -- 2.0104)  data: 0.0007 (0.0001 -- 0.0019)  max mem: 16413
Epoch: [141] Total time: 0:02:24 (0.9026 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000008  loss: 1.5803 (1.6384)  loss_scale: 8192.0000 (12083.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4989 (7.4156)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1101 (0.1101)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3281 (2.3281 -- 2.3281)  data: 2.0923 (2.0923 -- 2.0923)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4880 (0.5856)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4171 (0.2029 -- 2.3281)  data: 0.2032 (0.0008 -- 2.0923)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4760 (0.5653)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (98.4127)  time: 0.2277 (0.1698 -- 0.5475)  data: 0.0253 (0.0001 -- 0.3600)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5741 (0.5973)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (97.9253)  time: 0.2133 (0.1332 -- 0.5475)  data: 0.0246 (0.0001 -- 0.3600)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 83.402 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 85.27%
Epoch: [142]  [  0/160]  eta: 0:18:14  lr: 0.000030  min_lr: 0.000008  loss: 1.8742 (1.8742)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6879 (8.6879)  time: 6.8406 (6.8406 -- 6.8406)  data: 5.5753 (5.5753 -- 5.5753)  max mem: 16413
Epoch: [142]  [ 20/160]  eta: 0:02:49  lr: 0.000030  min_lr: 0.000008  loss: 1.7318 (1.6737)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7571 (7.3676)  time: 0.9263 (0.5388 -- 3.5227)  data: 0.0240 (0.0001 -- 0.2698)  max mem: 16413
Epoch: [142]  [ 40/160]  eta: 0:02:04  lr: 0.000030  min_lr: 0.000008  loss: 1.6840 (1.7103)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0659 (7.4364)  time: 0.8531 (0.5268 -- 2.9158)  data: 0.1353 (0.0004 -- 2.3741)  max mem: 16413
[2023-09-05 03:29:39,931] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:29:39,931] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 03:29:39,932] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:29:39,932] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [142]  [ 60/160]  eta: 0:01:41  lr: 0.000030  min_lr: 0.000008  loss: 1.6226 (1.6878)  loss_scale: 16384.0000 (10340.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4480 (7.4818)  time: 0.9786 (0.5299 -- 3.5540)  data: 0.3611 (0.0003 -- 3.0203)  max mem: 16413
Epoch: [142]  [ 80/160]  eta: 0:01:17  lr: 0.000030  min_lr: 0.000008  loss: 1.8181 (1.7049)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0506 (7.3753)  time: 0.8458 (0.5134 -- 3.8996)  data: 0.2275 (0.0003 -- 3.3764)  max mem: 16413
Epoch: [142]  [100/160]  eta: 0:00:57  lr: 0.000029  min_lr: 0.000007  loss: 1.7365 (1.7066)  loss_scale: 16384.0000 (12734.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3765 (7.2456)  time: 0.8681 (0.5441 -- 2.5948)  data: 0.2796 (0.0003 -- 2.0694)  max mem: 16413
Epoch: [142]  [120/160]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000007  loss: 1.5182 (1.6720)  loss_scale: 16384.0000 (13337.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3096 (7.3306)  time: 0.6965 (0.5101 -- 2.0223)  data: 0.0553 (0.0004 -- 1.0645)  max mem: 16413
Epoch: [142]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.6259 (1.6719)  loss_scale: 16384.0000 (13769.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2598 (7.3263)  time: 1.0837 (0.5192 -- 4.6425)  data: 0.0013 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.5268 (1.6517)  loss_scale: 16384.0000 (14080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8839 (7.3458)  time: 0.6050 (0.4947 -- 2.3110)  data: 0.0006 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [142] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.5268 (1.6563)  loss_scale: 16384.0000 (14080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8839 (7.3458)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1362 (0.1362)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4201 (2.4201 -- 2.4201)  data: 2.1925 (2.1925 -- 2.1925)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4878 (0.5761)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4322 (0.1987 -- 2.4201)  data: 0.2184 (0.0005 -- 2.1925)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4904 (0.5815)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2141 (0.1698 -- 0.4057)  data: 0.0106 (0.0001 -- 0.1985)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5716 (0.6261)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (98.3402)  time: 0.1998 (0.1324 -- 0.4057)  data: 0.0103 (0.0001 -- 0.1985)  max mem: 16413
Val: Total time: 0:00:07 (0.2848 s / it)
* Acc@1 84.025 Acc@5 97.510 loss 0.617
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.27%
Epoch: [143]  [  0/160]  eta: 0:23:40  lr: 0.000029  min_lr: 0.000007  loss: 1.4590 (1.4590)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9145 (7.9145)  time: 8.8773 (8.8773 -- 8.8773)  data: 6.1079 (6.1079 -- 6.1079)  max mem: 16413
[2023-09-05 03:31:42,498] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:31:42,499] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:31:42,499] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:31:42,499] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [143]  [ 20/160]  eta: 0:02:52  lr: 0.000029  min_lr: 0.000007  loss: 1.6040 (1.6103)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4347 (6.6833)  time: 0.8462 (0.5308 -- 3.5739)  data: 0.0450 (0.0004 -- 0.8683)  max mem: 16413
Epoch: [143]  [ 40/160]  eta: 0:02:08  lr: 0.000029  min_lr: 0.000007  loss: 1.7712 (1.6718)  loss_scale: 32768.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7981 (7.2300)  time: 0.9008 (0.5364 -- 2.8405)  data: 0.0020 (0.0008 -- 0.0035)  max mem: 16413
[2023-09-05 03:32:10,739] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22926
[2023-09-05 03:32:10,739] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:32:10,739] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22926
[2023-09-05 03:32:10,740] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 03:32:10,740] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [143]  [ 60/160]  eta: 0:01:39  lr: 0.000029  min_lr: 0.000007  loss: 1.5072 (1.6041)  loss_scale: 16384.0000 (25247.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9094 (7.2263)  time: 0.8418 (0.5182 -- 3.4084)  data: 0.0019 (0.0006 -- 0.0060)  max mem: 16413
Epoch: [143]  [ 80/160]  eta: 0:01:16  lr: 0.000029  min_lr: 0.000007  loss: 1.7238 (1.6390)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0510 (7.1652)  time: 0.8561 (0.5350 -- 3.0415)  data: 0.0016 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [143]  [100/160]  eta: 0:00:56  lr: 0.000029  min_lr: 0.000007  loss: 1.5508 (1.6411)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6723 (7.1611)  time: 0.8407 (0.5186 -- 3.5023)  data: 0.0021 (0.0005 -- 0.0155)  max mem: 16413
[2023-09-05 03:33:12,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=130, lr=[7.420475107695345e-06, 7.420475107695345e-06, 8.244972341883717e-06, 8.244972341883717e-06, 9.161080379870795e-06, 9.161080379870795e-06, 1.017897819985644e-05, 1.017897819985644e-05, 1.1309975777618267e-05, 1.1309975777618267e-05, 1.2566639752909185e-05, 1.2566639752909185e-05, 1.3962933058787983e-05, 1.3962933058787983e-05, 1.551437006531998e-05, 1.551437006531998e-05, 1.7238188961466645e-05, 1.7238188961466645e-05, 1.915354329051849e-05, 1.915354329051849e-05, 2.128171476724277e-05, 2.128171476724277e-05, 2.3646349741380856e-05, 2.3646349741380856e-05, 2.6273721934867617e-05, 2.6273721934867617e-05, 2.9193024372075128e-05, 2.9193024372075128e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 03:33:12,685] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=17.466494506411895, CurrSamplesPerSec=21.426338676616766, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [143]  [120/160]  eta: 0:00:36  lr: 0.000029  min_lr: 0.000007  loss: 1.7489 (1.6451)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0027 (7.2195)  time: 0.7811 (0.5370 -- 2.3259)  data: 0.1525 (0.0003 -- 1.7546)  max mem: 16413
Epoch: [143]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.8115 (1.6653)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6784 (7.3606)  time: 0.8930 (0.5229 -- 3.1048)  data: 0.0034 (0.0003 -- 0.0400)  max mem: 16413
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.7273 (1.6655)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8045 (7.2276)  time: 0.8015 (0.4935 -- 2.3834)  data: 0.0291 (0.0002 -- 0.5685)  max mem: 16413
Epoch: [143] Total time: 0:02:23 (0.8974 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.7273 (1.6569)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8045 (7.2276)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1379 (0.1379)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5564 (2.5564 -- 2.5564)  data: 2.3439 (2.3439 -- 2.3439)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5450 (0.5739)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (100.0000)  time: 0.4397 (0.2007 -- 2.5564)  data: 0.2210 (0.0009 -- 2.3439)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5173 (0.5699)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (98.4127)  time: 0.2186 (0.1710 -- 0.3265)  data: 0.0110 (0.0001 -- 0.1299)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5521 (0.6028)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.2033 (0.1326 -- 0.3265)  data: 0.0107 (0.0001 -- 0.1299)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 84.440 Acc@5 97.718 loss 0.600
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.27%
Epoch: [144]  [  0/160]  eta: 0:20:31  lr: 0.000029  min_lr: 0.000007  loss: 0.8319 (0.8319)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0188 (9.0188)  time: 7.6971 (7.6971 -- 7.6971)  data: 7.1743 (7.1743 -- 7.1743)  max mem: 16413
[2023-09-05 03:34:14,348] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:34:14,348] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:34:14,349] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:34:14,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [144]  [ 20/160]  eta: 0:02:41  lr: 0.000029  min_lr: 0.000007  loss: 1.5633 (1.6088)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3360 (7.3680)  time: 0.8294 (0.5327 -- 3.8269)  data: 0.1652 (0.0002 -- 1.6382)  max mem: 16413
Epoch: [144]  [ 40/160]  eta: 0:02:04  lr: 0.000029  min_lr: 0.000007  loss: 1.6457 (1.6120)  loss_scale: 32768.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8753 (7.3144)  time: 0.9148 (0.5192 -- 3.1253)  data: 0.1809 (0.0004 -- 2.6013)  max mem: 16413
Epoch: [144]  [ 60/160]  eta: 0:01:39  lr: 0.000029  min_lr: 0.000007  loss: 1.7641 (1.6536)  loss_scale: 32768.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3283 (7.3399)  time: 0.8927 (0.5237 -- 3.6591)  data: 0.0473 (0.0003 -- 0.5338)  max mem: 16413
Epoch: [144]  [ 80/160]  eta: 0:01:16  lr: 0.000029  min_lr: 0.000007  loss: 1.4825 (1.6153)  loss_scale: 32768.0000 (29733.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1642 (7.2122)  time: 0.8391 (0.5390 -- 3.2190)  data: 0.1725 (0.0003 -- 2.6421)  max mem: 16413
Epoch: [144]  [100/160]  eta: 0:00:57  lr: 0.000029  min_lr: 0.000007  loss: 1.7338 (1.6381)  loss_scale: 32768.0000 (30334.7327)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4453 (7.2339)  time: 0.9717 (0.5246 -- 3.8576)  data: 0.4153 (0.0003 -- 3.3214)  max mem: 16413
Epoch: [144]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000007  loss: 1.6767 (1.6501)  loss_scale: 32768.0000 (30736.9256)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6534 (7.2254)  time: 0.8392 (0.5268 -- 3.7527)  data: 0.2832 (0.0001 -- 3.2242)  max mem: 16413
Epoch: [144]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.5343 (1.6302)  loss_scale: 32768.0000 (31025.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0957 (7.1691)  time: 0.9602 (0.5222 -- 3.4907)  data: 0.4182 (0.0004 -- 2.9576)  max mem: 16413
[2023-09-05 03:36:09,272] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:36:09,273] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 03:36:09,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:36:09,273] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 03:36:09,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23184
[2023-09-05 03:36:09,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23184
[2023-09-05 03:36:09,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 03:36:09,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 03:36:09,799] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.5599 (1.6228)  loss_scale: 32768.0000 (31436.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2377 (7.2193)  time: 0.5386 (0.4968 -- 0.8619)  data: 0.0168 (0.0002 -- 0.3221)  max mem: 16413
Epoch: [144] Total time: 0:02:22 (0.8931 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.5599 (1.6229)  loss_scale: 32768.0000 (31436.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2377 (7.2193)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1319 (0.1319)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4905 (2.4905 -- 2.4905)  data: 2.2579 (2.2579 -- 2.2579)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5989 (0.5691)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4256 (0.1922 -- 2.4905)  data: 0.2088 (0.0005 -- 2.2579)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5265 (0.5710)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2190 (0.1699 -- 0.4186)  data: 0.0144 (0.0001 -- 0.2454)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5697 (0.6111)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2041 (0.1333 -- 0.4186)  data: 0.0141 (0.0001 -- 0.2454)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.596
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.27%
Epoch: [145]  [  0/160]  eta: 0:20:37  lr: 0.000029  min_lr: 0.000007  loss: 1.3100 (1.3100)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9098 (4.9098)  time: 7.7326 (7.7326 -- 7.7326)  data: 5.8336 (5.8336 -- 5.8336)  max mem: 16413
Epoch: [145]  [ 20/160]  eta: 0:03:00  lr: 0.000029  min_lr: 0.000007  loss: 1.5417 (1.6206)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2723 (7.0564)  time: 0.9679 (0.5298 -- 4.2350)  data: 0.0736 (0.0003 -- 1.4426)  max mem: 16413
Epoch: [145]  [ 40/160]  eta: 0:02:10  lr: 0.000029  min_lr: 0.000007  loss: 1.7286 (1.6530)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9215 (6.7991)  time: 0.8827 (0.5263 -- 3.6166)  data: 0.0017 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [145]  [ 60/160]  eta: 0:01:40  lr: 0.000029  min_lr: 0.000007  loss: 1.4816 (1.6311)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0631 (6.7170)  time: 0.8358 (0.5228 -- 1.9670)  data: 0.0013 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [145]  [ 80/160]  eta: 0:01:17  lr: 0.000029  min_lr: 0.000007  loss: 1.7667 (1.6640)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8391 (6.9726)  time: 0.8555 (0.5251 -- 2.9241)  data: 0.0017 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [145]  [100/160]  eta: 0:00:56  lr: 0.000029  min_lr: 0.000007  loss: 1.5355 (1.6353)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0120 (7.1746)  time: 0.7988 (0.5275 -- 3.2138)  data: 0.0435 (0.0002 -- 0.8447)  max mem: 16413
[2023-09-05 03:38:08,610] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23309
[2023-09-05 03:38:08,610] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23309
[2023-09-05 03:38:08,610] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:38:08,610] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:38:08,610] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [145]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000007  loss: 1.7539 (1.6550)  loss_scale: 16384.0000 (31143.1405)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6372 (7.1493)  time: 0.9706 (0.5187 -- 3.1489)  data: 0.2620 (0.0006 -- 2.6025)  max mem: 16413
Epoch: [145]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000007  loss: 1.5797 (1.6560)  loss_scale: 16384.0000 (29049.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4035 (7.0748)  time: 0.8695 (0.5146 -- 3.8629)  data: 0.2387 (0.0003 -- 3.3495)  max mem: 16413
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000007  loss: 1.5760 (1.6448)  loss_scale: 16384.0000 (27545.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1115 (6.9648)  time: 0.6960 (0.4968 -- 3.0200)  data: 0.1793 (0.0002 -- 2.4691)  max mem: 16413
Epoch: [145] Total time: 0:02:24 (0.9050 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000007  loss: 1.5760 (1.6541)  loss_scale: 16384.0000 (27545.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1115 (6.9648)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1367 (0.1367)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5697 (2.5697 -- 2.5697)  data: 2.3411 (2.3411 -- 2.3411)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3797 (0.5439)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (100.0000)  time: 0.4352 (0.2055 -- 2.5697)  data: 0.2136 (0.0006 -- 2.3411)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4791 (0.5433)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (98.9418)  time: 0.2161 (0.1689 -- 0.3460)  data: 0.0084 (0.0001 -- 0.1558)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5375 (0.6011)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (98.3402)  time: 0.1996 (0.1325 -- 0.3460)  data: 0.0082 (0.0001 -- 0.1558)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 84.025 Acc@5 97.718 loss 0.587
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.27%
Epoch: [146]  [  0/160]  eta: 0:21:42  lr: 0.000029  min_lr: 0.000007  loss: 1.8806 (1.8806)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5722 (5.5722)  time: 8.1376 (8.1376 -- 8.1376)  data: 7.6009 (7.6009 -- 7.6009)  max mem: 16413
Epoch: [146]  [ 20/160]  eta: 0:02:56  lr: 0.000029  min_lr: 0.000007  loss: 1.5646 (1.5895)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6854 (6.6991)  time: 0.9140 (0.5058 -- 4.8947)  data: 0.3648 (0.0002 -- 4.3448)  max mem: 16413
Epoch: [146]  [ 40/160]  eta: 0:02:11  lr: 0.000029  min_lr: 0.000007  loss: 1.5451 (1.5650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9559 (6.7895)  time: 0.9299 (0.5029 -- 5.0978)  data: 0.3918 (0.0002 -- 4.5688)  max mem: 16413
Epoch: [146]  [ 60/160]  eta: 0:01:40  lr: 0.000028  min_lr: 0.000007  loss: 1.6063 (1.5633)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1898 (6.8819)  time: 0.8162 (0.5297 -- 3.6031)  data: 0.2616 (0.0003 -- 3.0685)  max mem: 16413
[2023-09-05 03:40:14,609] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:40:14,609] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:40:14,610] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:40:14,610] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [146]  [ 80/160]  eta: 0:01:19  lr: 0.000028  min_lr: 0.000007  loss: 1.6396 (1.5740)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9381 (6.9775)  time: 0.9615 (0.5247 -- 3.7887)  data: 0.4057 (0.0003 -- 3.2572)  max mem: 16413
Epoch: [146]  [100/160]  eta: 0:00:56  lr: 0.000028  min_lr: 0.000007  loss: 1.8398 (1.6269)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8128 (6.9799)  time: 0.7237 (0.5295 -- 3.0568)  data: 0.1715 (0.0003 -- 2.5372)  max mem: 16413
[2023-09-05 03:40:42,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23471
[2023-09-05 03:40:42,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23471
[2023-09-05 03:40:42,945] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:40:42,945] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:40:42,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [146]  [120/160]  eta: 0:00:37  lr: 0.000028  min_lr: 0.000007  loss: 1.6505 (1.6345)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2650 (6.8720)  time: 0.9225 (0.5358 -- 3.8899)  data: 0.3665 (0.0009 -- 3.3281)  max mem: 16413
Epoch: [146]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 1.4672 (1.6141)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4351 (6.8636)  time: 0.8455 (0.5265 -- 4.3353)  data: 0.2980 (0.0005 -- 3.8017)  max mem: 16413
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.5241 (1.6248)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2387 (6.8235)  time: 0.6853 (0.4939 -- 3.2597)  data: 0.1664 (0.0002 -- 2.7464)  max mem: 16413
Epoch: [146] Total time: 0:02:23 (0.8975 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.5241 (1.6180)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2387 (6.8235)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1301 (0.1301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5293 (2.5293 -- 2.5293)  data: 2.2972 (2.2972 -- 2.2972)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5364 (0.5814)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4289 (0.1931 -- 2.5293)  data: 0.2116 (0.0007 -- 2.2972)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4801 (0.5443)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.9418)  time: 0.2192 (0.1687 -- 0.4398)  data: 0.0146 (0.0001 -- 0.2592)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5048 (0.6021)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (98.3402)  time: 0.2017 (0.1329 -- 0.4398)  data: 0.0136 (0.0001 -- 0.2592)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.582
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.27%
Epoch: [147]  [  0/160]  eta: 0:20:39  lr: 0.000028  min_lr: 0.000007  loss: 1.3241 (1.3241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2418 (6.2418)  time: 7.7492 (7.7492 -- 7.7492)  data: 7.2197 (7.2197 -- 7.2197)  max mem: 16413
Epoch: [147]  [ 20/160]  eta: 0:02:44  lr: 0.000028  min_lr: 0.000007  loss: 1.6573 (1.5942)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3844 (6.3783)  time: 0.8442 (0.5315 -- 4.8755)  data: 0.2927 (0.0003 -- 4.3452)  max mem: 16413
Epoch: [147]  [ 40/160]  eta: 0:02:09  lr: 0.000028  min_lr: 0.000007  loss: 1.6764 (1.6486)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2742 (6.8158)  time: 0.9884 (0.5304 -- 3.5747)  data: 0.4357 (0.0005 -- 3.0627)  max mem: 16413
Epoch: [147]  [ 60/160]  eta: 0:01:38  lr: 0.000028  min_lr: 0.000007  loss: 1.5828 (1.6210)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5062 (6.7776)  time: 0.7709 (0.5253 -- 2.9280)  data: 0.2269 (0.0001 -- 2.4022)  max mem: 16413
[2023-09-05 03:42:50,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:42:50,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:42:50,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:42:50,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [147]  [ 80/160]  eta: 0:01:19  lr: 0.000028  min_lr: 0.000007  loss: 1.6980 (1.6405)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1353 (6.8845)  time: 1.0367 (0.5179 -- 4.4463)  data: 0.4856 (0.0003 -- 3.9210)  max mem: 16413
Epoch: [147]  [100/160]  eta: 0:00:57  lr: 0.000028  min_lr: 0.000007  loss: 1.7730 (1.6762)  loss_scale: 32768.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4854 (7.0202)  time: 0.7903 (0.5166 -- 3.0410)  data: 0.2430 (0.0003 -- 2.4958)  max mem: 16413
Epoch: [147]  [120/160]  eta: 0:00:38  lr: 0.000028  min_lr: 0.000007  loss: 1.6533 (1.6693)  loss_scale: 32768.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3099 (6.9216)  time: 0.9738 (0.5214 -- 3.6913)  data: 0.4261 (0.0003 -- 3.1270)  max mem: 16413
Epoch: [147]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 1.5710 (1.6592)  loss_scale: 32768.0000 (23472.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6786 (6.8911)  time: 0.8223 (0.5287 -- 4.2594)  data: 0.2683 (0.0004 -- 3.7394)  max mem: 16413
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.7368 (1.6677)  loss_scale: 32768.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1039 (6.9721)  time: 0.6256 (0.4961 -- 1.8030)  data: 0.1107 (0.0001 -- 1.2815)  max mem: 16413
Epoch: [147] Total time: 0:02:24 (0.9017 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.7368 (1.6395)  loss_scale: 32768.0000 (24576.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1039 (6.9721)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1340 (0.1340)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4636 (2.4636 -- 2.4636)  data: 2.2149 (2.2149 -- 2.2149)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5280 (0.5707)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4218 (0.2001 -- 2.4636)  data: 0.2060 (0.0007 -- 2.2149)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5194 (0.5599)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (99.4709)  time: 0.2188 (0.1699 -- 0.3996)  data: 0.0147 (0.0001 -- 0.2256)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5327 (0.6097)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (98.7552)  time: 0.2024 (0.1336 -- 0.3996)  data: 0.0144 (0.0001 -- 0.2256)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.601
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.27%
Epoch: [148]  [  0/160]  eta: 0:22:26  lr: 0.000028  min_lr: 0.000007  loss: 1.4425 (1.4425)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4548 (6.4548)  time: 8.4172 (8.4172 -- 8.4172)  data: 6.9356 (6.9356 -- 6.9356)  max mem: 16413
Epoch: [148]  [ 20/160]  eta: 0:02:53  lr: 0.000028  min_lr: 0.000007  loss: 1.6805 (1.6182)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0810 (6.7790)  time: 0.8810 (0.5152 -- 3.4924)  data: 0.3421 (0.0004 -- 2.9810)  max mem: 16413
[2023-09-05 03:44:29,757] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23703
[2023-09-05 03:44:29,757] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23703
[2023-09-05 03:44:29,757] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:44:29,757] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:44:29,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [148]  [ 40/160]  eta: 0:02:05  lr: 0.000028  min_lr: 0.000007  loss: 1.6384 (1.6284)  loss_scale: 16384.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0786 (6.6828)  time: 0.8337 (0.5320 -- 2.1769)  data: 0.2783 (0.0003 -- 1.6507)  max mem: 16413
Epoch: [148]  [ 60/160]  eta: 0:01:42  lr: 0.000028  min_lr: 0.000007  loss: 1.6292 (1.6251)  loss_scale: 16384.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4589 (6.6507)  time: 0.9802 (0.5171 -- 3.0874)  data: 0.3740 (0.0003 -- 2.5339)  max mem: 16413
Epoch: [148]  [ 80/160]  eta: 0:01:18  lr: 0.000028  min_lr: 0.000007  loss: 1.5011 (1.6206)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5334 (6.7405)  time: 0.8530 (0.5317 -- 3.0829)  data: 0.3088 (0.0003 -- 2.5557)  max mem: 16413
Epoch: [148]  [100/160]  eta: 0:00:57  lr: 0.000028  min_lr: 0.000007  loss: 1.6275 (1.6256)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3521 (6.8638)  time: 0.8289 (0.5327 -- 3.1880)  data: 0.2799 (0.0003 -- 2.6537)  max mem: 16413
[2023-09-05 03:45:54,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23799
[2023-09-05 03:45:54,838] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:45:54,838] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23799
[2023-09-05 03:45:54,838] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:45:54,838] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [148]  [120/160]  eta: 0:00:37  lr: 0.000028  min_lr: 0.000007  loss: 1.5071 (1.6173)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6213 (7.0390)  time: 0.8669 (0.5458 -- 2.1148)  data: 0.0999 (0.0004 -- 1.4668)  max mem: 16413
Epoch: [148]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 1.5606 (1.6224)  loss_scale: 8192.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6717 (7.0146)  time: 0.8759 (0.5244 -- 2.3977)  data: 0.0836 (0.0004 -- 1.6495)  max mem: 16413
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.6734 (1.6339)  loss_scale: 8192.0000 (16640.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8485 (7.0213)  time: 0.6975 (0.4970 -- 1.8443)  data: 0.0854 (0.0002 -- 0.8865)  max mem: 16413
Epoch: [148] Total time: 0:02:23 (0.8986 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.6734 (1.6456)  loss_scale: 8192.0000 (16640.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8485 (7.0213)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1251 (0.1251)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4569 (2.4569 -- 2.4569)  data: 2.2383 (2.2383 -- 2.2383)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5691 (0.5660)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (100.0000)  time: 0.4378 (0.1980 -- 2.4569)  data: 0.2186 (0.0004 -- 2.2383)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4955 (0.5571)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (98.9418)  time: 0.2173 (0.1694 -- 0.3576)  data: 0.0099 (0.0001 -- 0.1590)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4955 (0.6025)  acc1: 77.7778 (81.7427)  acc5: 100.0000 (98.3402)  time: 0.2009 (0.1330 -- 0.3576)  data: 0.0096 (0.0001 -- 0.1590)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 83.817 Acc@5 97.718 loss 0.586
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 85.27%
Epoch: [149]  [  0/160]  eta: 0:16:10  lr: 0.000028  min_lr: 0.000007  loss: 1.3569 (1.3569)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1265 (8.1265)  time: 6.0681 (6.0681 -- 6.0681)  data: 5.5123 (5.5123 -- 5.5123)  max mem: 16413
Epoch: [149]  [ 20/160]  eta: 0:02:39  lr: 0.000028  min_lr: 0.000007  loss: 1.5455 (1.5916)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7245 (7.0034)  time: 0.8897 (0.5358 -- 3.0082)  data: 0.1940 (0.0006 -- 1.5772)  max mem: 16413
Epoch: [149]  [ 40/160]  eta: 0:02:04  lr: 0.000028  min_lr: 0.000007  loss: 1.5920 (1.6296)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0307 (6.7098)  time: 0.9316 (0.5338 -- 3.6390)  data: 0.3223 (0.0003 -- 3.1023)  max mem: 16413
Epoch: [149]  [ 60/160]  eta: 0:01:38  lr: 0.000028  min_lr: 0.000007  loss: 1.7577 (1.6416)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0776 (6.7897)  time: 0.8691 (0.5355 -- 3.0540)  data: 0.2096 (0.0006 -- 2.5317)  max mem: 16413
Epoch: [149]  [ 80/160]  eta: 0:01:16  lr: 0.000028  min_lr: 0.000007  loss: 1.5965 (1.6306)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5236 (6.8053)  time: 0.8805 (0.5138 -- 3.0186)  data: 0.0968 (0.0003 -- 0.9953)  max mem: 16413
[2023-09-05 03:47:58,023] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:47:58,023] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 03:47:58,024] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:47:58,024] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [149]  [100/160]  eta: 0:00:57  lr: 0.000028  min_lr: 0.000007  loss: 1.4628 (1.6241)  loss_scale: 16384.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3082 (6.9104)  time: 0.9628 (0.5312 -- 3.5168)  data: 0.0013 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [149]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000007  loss: 1.7048 (1.6327)  loss_scale: 16384.0000 (10426.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7482 (7.0188)  time: 0.7387 (0.5289 -- 2.7948)  data: 0.0029 (0.0002 -- 0.0171)  max mem: 16413
Epoch: [149]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000007  loss: 1.3924 (1.6194)  loss_scale: 16384.0000 (11271.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7506 (7.1016)  time: 0.8752 (0.5381 -- 4.0473)  data: 0.0013 (0.0003 -- 0.0026)  max mem: 16413
[2023-09-05 03:48:56,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=135, lr=[6.997355931939994e-06, 6.997355931939994e-06, 7.77483992437777e-06, 7.77483992437777e-06, 8.63871102708641e-06, 8.63871102708641e-06, 9.59856780787379e-06, 9.59856780787379e-06, 1.0665075342081988e-05, 1.0665075342081988e-05, 1.1850083713424432e-05, 1.1850083713424432e-05, 1.3166759681582702e-05, 1.3166759681582702e-05, 1.4629732979536333e-05, 1.4629732979536333e-05, 1.6255258866151482e-05, 1.6255258866151482e-05, 1.806139874016831e-05, 1.806139874016831e-05, 2.006822082240924e-05, 2.006822082240924e-05, 2.229802313601026e-05, 2.229802313601026e-05, 2.4775581262233623e-05, 2.4775581262233623e-05, 2.7528423624704026e-05, 2.7528423624704026e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 03:48:56,867] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=17.515256842495067, CurrSamplesPerSec=24.390687937350986, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000007  loss: 1.7417 (1.6317)  loss_scale: 16384.0000 (11878.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8069 (7.1742)  time: 0.7344 (0.4950 -- 4.1128)  data: 0.0007 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [149] Total time: 0:02:23 (0.8949 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000007  loss: 1.7417 (1.6588)  loss_scale: 16384.0000 (11878.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8069 (7.1742)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1353 (0.1353)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4047 (2.4047 -- 2.4047)  data: 2.1865 (2.1865 -- 2.1865)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6170 (0.6113)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4202 (0.2035 -- 2.4047)  data: 0.2080 (0.0006 -- 2.1865)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5824 (0.6076)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2266 (0.1695 -- 0.5490)  data: 0.0232 (0.0001 -- 0.3599)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5824 (0.6459)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (97.9253)  time: 0.2126 (0.1337 -- 0.5490)  data: 0.0230 (0.0001 -- 0.3599)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 82.988 Acc@5 97.925 loss 0.602
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 85.27%
Epoch: [150]  [  0/160]  eta: 0:22:26  lr: 0.000028  min_lr: 0.000007  loss: 1.9255 (1.9255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6312 (4.6312)  time: 8.4148 (8.4148 -- 8.4148)  data: 7.4399 (7.4399 -- 7.4399)  max mem: 16413
Epoch: [150]  [ 20/160]  eta: 0:02:48  lr: 0.000027  min_lr: 0.000007  loss: 1.6374 (1.7843)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5221 (7.3601)  time: 0.8429 (0.5302 -- 3.5135)  data: 0.2336 (0.0006 -- 2.1355)  max mem: 16413
[2023-09-05 03:49:32,239] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24023
[2023-09-05 03:49:32,239] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24023
[2023-09-05 03:49:32,239] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:49:32,239] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:49:32,240] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [150]  [ 40/160]  eta: 0:02:08  lr: 0.000027  min_lr: 0.000007  loss: 1.4408 (1.6399)  loss_scale: 8192.0000 (12787.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3084 (7.5049)  time: 0.9316 (0.5213 -- 2.9256)  data: 0.3876 (0.0003 -- 2.3893)  max mem: 16413
Epoch: [150]  [ 60/160]  eta: 0:01:40  lr: 0.000027  min_lr: 0.000007  loss: 1.6881 (1.6480)  loss_scale: 8192.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9666 (7.6722)  time: 0.8803 (0.5046 -- 4.1943)  data: 0.3372 (0.0003 -- 3.6822)  max mem: 16413
Epoch: [150]  [ 80/160]  eta: 0:01:18  lr: 0.000027  min_lr: 0.000007  loss: 1.4405 (1.6310)  loss_scale: 8192.0000 (10518.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1233 (7.6438)  time: 0.8952 (0.5182 -- 3.5417)  data: 0.3434 (0.0002 -- 3.0165)  max mem: 16413
Epoch: [150]  [100/160]  eta: 0:00:56  lr: 0.000027  min_lr: 0.000007  loss: 1.7581 (1.6491)  loss_scale: 8192.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0503 (7.7608)  time: 0.8162 (0.5268 -- 3.4728)  data: 0.2433 (0.0006 -- 2.9340)  max mem: 16413
Epoch: [150]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000007  loss: 1.5891 (1.6323)  loss_scale: 8192.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9796 (7.5216)  time: 0.8666 (0.5195 -- 3.3884)  data: 0.3139 (0.0004 -- 2.8778)  max mem: 16413
Epoch: [150]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.7341 (1.6474)  loss_scale: 8192.0000 (9528.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2500 (7.3854)  time: 0.8365 (0.5277 -- 3.4373)  data: 0.2406 (0.0004 -- 2.9132)  max mem: 16413
[2023-09-05 03:51:23,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:51:23,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:51:23,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 03:51:23,991] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.6197 (1.6487)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8371 (7.3237)  time: 0.6718 (0.4959 -- 3.3573)  data: 0.1456 (0.0002 -- 2.8213)  max mem: 16413
Epoch: [150] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.6197 (1.6343)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8371 (7.3237)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1394 (0.1394)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3525 (2.3525 -- 2.3525)  data: 2.0717 (2.0717 -- 2.0717)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4880 (0.6056)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4176 (0.2014 -- 2.3525)  data: 0.1981 (0.0007 -- 2.0717)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4880 (0.5842)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2186 (0.1693 -- 0.3834)  data: 0.0155 (0.0001 -- 0.1985)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5762 (0.6261)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (98.3402)  time: 0.2037 (0.1326 -- 0.3834)  data: 0.0152 (0.0001 -- 0.1985)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 84.440 Acc@5 97.718 loss 0.596
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.27%
Epoch: [151]  [  0/160]  eta: 0:17:25  lr: 0.000027  min_lr: 0.000007  loss: 1.0627 (1.0627)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2461 (6.2461)  time: 6.5330 (6.5330 -- 6.5330)  data: 5.9832 (5.9832 -- 5.9832)  max mem: 16413
Epoch: [151]  [ 20/160]  eta: 0:02:35  lr: 0.000027  min_lr: 0.000007  loss: 1.5998 (1.5638)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2575 (6.5447)  time: 0.8372 (0.5258 -- 2.0846)  data: 0.2226 (0.0006 -- 1.3425)  max mem: 16413
Epoch: [151]  [ 40/160]  eta: 0:01:59  lr: 0.000027  min_lr: 0.000007  loss: 1.6851 (1.5957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3191 (6.8196)  time: 0.8764 (0.5424 -- 2.7031)  data: 0.2485 (0.0006 -- 2.1878)  max mem: 16413
Epoch: [151]  [ 60/160]  eta: 0:01:34  lr: 0.000027  min_lr: 0.000007  loss: 1.5719 (1.5723)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3436 (6.6768)  time: 0.8406 (0.5136 -- 2.1255)  data: 0.1567 (0.0002 -- 1.5620)  max mem: 16413
Epoch: [151]  [ 80/160]  eta: 0:01:15  lr: 0.000027  min_lr: 0.000007  loss: 1.6010 (1.5796)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0651 (6.8749)  time: 0.9589 (0.5222 -- 3.8946)  data: 0.3330 (0.0007 -- 3.3760)  max mem: 16413
Epoch: [151]  [100/160]  eta: 0:00:54  lr: 0.000027  min_lr: 0.000007  loss: 1.5740 (1.5855)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2203 (7.0126)  time: 0.7861 (0.5332 -- 2.5569)  data: 0.2186 (0.0003 -- 2.0205)  max mem: 16413
[2023-09-05 03:53:26,533] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:53:26,533] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:53:26,535] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:53:26,535] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [151]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000007  loss: 1.6883 (1.6126)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7681 (6.9750)  time: 0.9216 (0.5271 -- 3.1208)  data: 0.3679 (0.0005 -- 2.5840)  max mem: 16413
[2023-09-05 03:53:40,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24298
[2023-09-05 03:53:40,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24298
[2023-09-05 03:53:40,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:53:40,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:53:40,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [151]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.3902 (1.5951)  loss_scale: 32768.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1779 (6.9223)  time: 0.9012 (0.5217 -- 3.4679)  data: 0.0991 (0.0004 -- 1.1802)  max mem: 16413
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.9045 (1.6232)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4624 (6.9378)  time: 0.7632 (0.4962 -- 3.4679)  data: 0.0167 (0.0001 -- 0.3226)  max mem: 16413
Epoch: [151] Total time: 0:02:20 (0.8800 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.9045 (1.6367)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4624 (6.9378)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1312 (0.1312)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3405 (2.3405 -- 2.3405)  data: 2.1032 (2.1032 -- 2.1032)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3460 (0.6121)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4184 (0.1952 -- 2.3405)  data: 0.1924 (0.0007 -- 2.1032)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6232 (0.5943)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2209 (0.1699 -- 0.3016)  data: 0.0105 (0.0001 -- 0.1156)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6557 (0.6443)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.2018 (0.1328 -- 0.3016)  data: 0.0102 (0.0001 -- 0.1156)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.603
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.27%
Epoch: [152]  [  0/160]  eta: 0:18:47  lr: 0.000027  min_lr: 0.000007  loss: 1.7517 (1.7517)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5242 (8.5242)  time: 7.0476 (7.0476 -- 7.0476)  data: 6.0053 (6.0053 -- 6.0053)  max mem: 16413
Epoch: [152]  [ 20/160]  eta: 0:02:36  lr: 0.000027  min_lr: 0.000007  loss: 1.6327 (1.6325)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4898 (7.0328)  time: 0.8241 (0.5170 -- 3.2798)  data: 0.2716 (0.0008 -- 2.7199)  max mem: 16413
Epoch: [152]  [ 40/160]  eta: 0:01:59  lr: 0.000027  min_lr: 0.000007  loss: 1.7245 (1.6770)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4438 (7.0584)  time: 0.8615 (0.5343 -- 2.4976)  data: 0.2362 (0.0006 -- 1.9473)  max mem: 16413
Epoch: [152]  [ 60/160]  eta: 0:01:40  lr: 0.000027  min_lr: 0.000007  loss: 1.5962 (1.6458)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3656 (6.9585)  time: 1.0240 (0.5182 -- 3.9381)  data: 0.4412 (0.0002 -- 3.3935)  max mem: 16413
Epoch: [152]  [ 80/160]  eta: 0:01:16  lr: 0.000027  min_lr: 0.000007  loss: 1.6711 (1.6388)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8207 (7.0392)  time: 0.8225 (0.5100 -- 3.6444)  data: 0.2772 (0.0002 -- 3.1209)  max mem: 16413
Epoch: [152]  [100/160]  eta: 0:00:57  lr: 0.000027  min_lr: 0.000007  loss: 1.7434 (1.6468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1077 (7.0544)  time: 0.9139 (0.5270 -- 4.6747)  data: 0.3611 (0.0003 -- 4.1482)  max mem: 16413
[2023-09-05 03:55:46,070] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:55:46,070] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 03:55:46,070] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 03:55:46,070] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [152]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000007  loss: 1.5014 (1.6341)  loss_scale: 32768.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7519 (7.0293)  time: 0.8395 (0.5263 -- 2.6048)  data: 0.2065 (0.0007 -- 2.0226)  max mem: 16413
Epoch: [152]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000007  loss: 1.5748 (1.6229)  loss_scale: 32768.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8557 (7.1191)  time: 0.8925 (0.5207 -- 3.2171)  data: 0.2913 (0.0002 -- 2.6934)  max mem: 16413
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000007  loss: 1.6616 (1.6311)  loss_scale: 32768.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1262 (7.1808)  time: 0.6834 (0.4962 -- 2.0354)  data: 0.0952 (0.0002 -- 1.5354)  max mem: 16413
Epoch: [152] Total time: 0:02:23 (0.8986 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000007  loss: 1.6616 (1.6280)  loss_scale: 32768.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1262 (7.1808)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1281 (0.1281)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5135 (2.5135 -- 2.5135)  data: 2.3049 (2.3049 -- 2.3049)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5529 (0.6308)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4260 (0.2000 -- 2.5135)  data: 0.2122 (0.0006 -- 2.3049)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5529 (0.5972)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (98.9418)  time: 0.2196 (0.1687 -- 0.4706)  data: 0.0166 (0.0001 -- 0.2996)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6115 (0.6481)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.2034 (0.1320 -- 0.4706)  data: 0.0163 (0.0001 -- 0.2996)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 83.610 Acc@5 97.925 loss 0.610
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 85.27%
Epoch: [153]  [  0/160]  eta: 0:21:48  lr: 0.000027  min_lr: 0.000007  loss: 1.4583 (1.4583)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3029 (8.3029)  time: 8.1786 (8.1786 -- 8.1786)  data: 7.1568 (7.1568 -- 7.1568)  max mem: 16413
[2023-09-05 03:57:00,805] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24497
[2023-09-05 03:57:00,805] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24497
[2023-09-05 03:57:00,805] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:57:00,805] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 03:57:00,805] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [153]  [ 20/160]  eta: 0:02:57  lr: 0.000027  min_lr: 0.000007  loss: 1.6072 (1.5928)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2156 (6.6596)  time: 0.9232 (0.5150 -- 4.4017)  data: 0.3736 (0.0003 -- 3.8609)  max mem: 16413
Epoch: [153]  [ 40/160]  eta: 0:02:14  lr: 0.000027  min_lr: 0.000007  loss: 1.6986 (1.6263)  loss_scale: 16384.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6225 (7.2790)  time: 0.9582 (0.5062 -- 4.6756)  data: 0.4243 (0.0003 -- 4.1454)  max mem: 16413
Epoch: [153]  [ 60/160]  eta: 0:01:43  lr: 0.000027  min_lr: 0.000007  loss: 1.7132 (1.6589)  loss_scale: 16384.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0699 (7.4101)  time: 0.8572 (0.5225 -- 4.1494)  data: 0.3066 (0.0004 -- 3.6076)  max mem: 16413
Epoch: [153]  [ 80/160]  eta: 0:01:19  lr: 0.000027  min_lr: 0.000007  loss: 1.5414 (1.6459)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9747 (7.4150)  time: 0.8660 (0.5200 -- 3.7829)  data: 0.3173 (0.0001 -- 3.2407)  max mem: 16413
Epoch: [153]  [100/160]  eta: 0:00:56  lr: 0.000027  min_lr: 0.000007  loss: 1.8303 (1.6675)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7572 (7.2970)  time: 0.7351 (0.5312 -- 1.7331)  data: 0.1518 (0.0004 -- 1.1740)  max mem: 16413
[2023-09-05 03:58:25,062] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24596
[2023-09-05 03:58:25,062] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:58:25,062] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24596
[2023-09-05 03:58:25,062] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 03:58:25,063] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [153]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000007  loss: 1.5968 (1.6544)  loss_scale: 16384.0000 (18347.3719)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7908 (7.2932)  time: 0.9037 (0.5402 -- 2.1614)  data: 0.3388 (0.0003 -- 1.6268)  max mem: 16413
Epoch: [153]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.6279 (1.6537)  loss_scale: 8192.0000 (16906.8936)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8233 (7.4129)  time: 0.8647 (0.5227 -- 2.8562)  data: 0.3165 (0.0004 -- 2.3072)  max mem: 16413
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.6536 (1.6452)  loss_scale: 8192.0000 (15872.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8289 (7.3755)  time: 0.7008 (0.4959 -- 2.3749)  data: 0.1751 (0.0001 -- 1.8416)  max mem: 16413
Epoch: [153] Total time: 0:02:23 (0.8945 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.6536 (1.6610)  loss_scale: 8192.0000 (15872.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8289 (7.3755)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1457 (0.1457)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3338 (2.3338 -- 2.3338)  data: 2.1001 (2.1001 -- 2.1001)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4309 (0.5847)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (100.0000)  time: 0.4297 (0.2054 -- 2.3338)  data: 0.2112 (0.0007 -- 2.1001)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5333 (0.5654)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (98.4127)  time: 0.2217 (0.1717 -- 0.4392)  data: 0.0151 (0.0001 -- 0.2071)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5452 (0.6095)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2058 (0.1332 -- 0.4392)  data: 0.0146 (0.0001 -- 0.2071)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.598
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.27%
Epoch: [154]  [  0/160]  eta: 0:21:12  lr: 0.000026  min_lr: 0.000007  loss: 1.3473 (1.3473)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1144 (10.1144)  time: 7.9525 (7.9525 -- 7.9525)  data: 4.8532 (4.8532 -- 4.8532)  max mem: 16413
Epoch: [154]  [ 20/160]  eta: 0:02:51  lr: 0.000026  min_lr: 0.000007  loss: 1.7170 (1.6666)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9462 (8.4471)  time: 0.8876 (0.5256 -- 3.0714)  data: 0.0015 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [154]  [ 40/160]  eta: 0:02:05  lr: 0.000026  min_lr: 0.000007  loss: 1.5885 (1.6793)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8023 (7.7227)  time: 0.8521 (0.5250 -- 4.5123)  data: 0.0014 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [154]  [ 60/160]  eta: 0:01:38  lr: 0.000026  min_lr: 0.000007  loss: 1.6308 (1.6798)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9986 (7.5782)  time: 0.8585 (0.5341 -- 2.1521)  data: 0.1618 (0.0004 -- 1.3194)  max mem: 16413
Epoch: [154]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000007  loss: 1.6323 (1.6700)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6577 (7.4752)  time: 0.8361 (0.5332 -- 2.1192)  data: 0.0869 (0.0001 -- 1.1463)  max mem: 16413
[2023-09-05 04:00:27,811] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:00:27,811] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 04:00:27,811] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:00:27,811] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [154]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000007  loss: 1.6229 (1.6637)  loss_scale: 16384.0000 (9489.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2094 (7.3845)  time: 0.8514 (0.5335 -- 2.3211)  data: 0.2146 (0.0005 -- 1.7666)  max mem: 16413
Epoch: [154]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000007  loss: 1.7495 (1.6828)  loss_scale: 16384.0000 (10629.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7645 (7.1979)  time: 0.9403 (0.5231 -- 3.8775)  data: 0.3597 (0.0004 -- 3.3561)  max mem: 16413
Epoch: [154]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.6556 (1.6835)  loss_scale: 16384.0000 (11445.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0529 (7.1335)  time: 0.8243 (0.5116 -- 2.8623)  data: 0.0617 (0.0005 -- 1.1991)  max mem: 16413
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.5111 (1.6623)  loss_scale: 16384.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1744 (7.0385)  time: 0.6907 (0.4977 -- 1.8542)  data: 0.0887 (0.0002 -- 0.9298)  max mem: 16413
Epoch: [154] Total time: 0:02:22 (0.8891 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.5111 (1.6382)  loss_scale: 16384.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1744 (7.0385)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1336 (0.1336)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4761 (2.4761 -- 2.4761)  data: 2.2385 (2.2385 -- 2.2385)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4059 (0.5264)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4328 (0.1996 -- 2.4761)  data: 0.2096 (0.0005 -- 2.2385)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4787 (0.5374)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2258 (0.1702 -- 0.4887)  data: 0.0192 (0.0001 -- 0.3139)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5788 (0.5827)  acc1: 77.7778 (84.6473)  acc5: 100.0000 (98.7552)  time: 0.2066 (0.1323 -- 0.4887)  data: 0.0187 (0.0001 -- 0.3139)  max mem: 16413
Val: Total time: 0:00:07 (0.2957 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.574
Accuracy of the network on the 482 val images: 85.27%
[2023-09-05 04:01:36,939] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 04:01:36,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 04:01:36,941] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 04:01:36,941] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 04:01:38,323] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 04:01:38,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.27%
Epoch: [155]  [  0/160]  eta: 0:21:28  lr: 0.000026  min_lr: 0.000007  loss: 1.3566 (1.3566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2294 (7.2294)  time: 8.0556 (8.0556 -- 8.0556)  data: 7.4978 (7.4978 -- 7.4978)  max mem: 16413
Epoch: [155]  [ 20/160]  eta: 0:03:02  lr: 0.000026  min_lr: 0.000007  loss: 1.6744 (1.6256)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3416 (7.4751)  time: 0.9672 (0.5229 -- 4.0459)  data: 0.2476 (0.0005 -- 2.0907)  max mem: 16413
Epoch: [155]  [ 40/160]  eta: 0:02:07  lr: 0.000026  min_lr: 0.000007  loss: 1.5689 (1.6086)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6796 (7.0674)  time: 0.8089 (0.5300 -- 3.1101)  data: 0.1301 (0.0003 -- 2.5804)  max mem: 16413
[2023-09-05 04:02:33,049] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:02:33,049] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:02:33,050] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:02:33,051] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:02:34,669] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24856
[2023-09-05 04:02:34,669] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:02:34,669] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24856
[2023-09-05 04:02:34,669] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 04:02:34,669] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [155]  [ 60/160]  eta: 0:01:40  lr: 0.000026  min_lr: 0.000007  loss: 1.7287 (1.6476)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5109 (6.8789)  time: 0.8828 (0.5174 -- 3.2530)  data: 0.2744 (0.0005 -- 2.6891)  max mem: 16413
Epoch: [155]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000007  loss: 1.5801 (1.6277)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9916 (6.8229)  time: 0.7799 (0.5163 -- 2.2410)  data: 0.1409 (0.0004 -- 1.7101)  max mem: 16413
Epoch: [155]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000007  loss: 1.6839 (1.6270)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6152 (6.8893)  time: 0.9339 (0.5332 -- 3.2006)  data: 0.3154 (0.0004 -- 2.6713)  max mem: 16413
Epoch: [155]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000007  loss: 1.5920 (1.6073)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5283 (6.8651)  time: 0.7971 (0.5337 -- 2.2932)  data: 0.1165 (0.0003 -- 0.9964)  max mem: 16413
Epoch: [155]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.4917 (1.6054)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5494 (6.9198)  time: 0.9403 (0.5241 -- 3.4416)  data: 0.3894 (0.0003 -- 2.8767)  max mem: 16413
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.8693 (1.6286)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7514 (6.9110)  time: 0.7056 (0.4967 -- 2.1041)  data: 0.1565 (0.0002 -- 1.5889)  max mem: 16413
Epoch: [155] Total time: 0:02:22 (0.8897 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.8693 (1.6427)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7514 (6.9110)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1660 (0.1660)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3369 (2.3369 -- 2.3369)  data: 2.1383 (2.1383 -- 2.1383)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5333 (0.5546)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (98.9899)  time: 0.4129 (0.2033 -- 2.3369)  data: 0.1962 (0.0006 -- 2.1383)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5333 (0.5720)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2235 (0.1686 -- 0.3506)  data: 0.0168 (0.0001 -- 0.1618)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5810 (0.6334)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (97.5104)  time: 0.2061 (0.1330 -- 0.3506)  data: 0.0160 (0.0001 -- 0.1618)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 84.025 Acc@5 97.718 loss 0.608
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.27%
Epoch: [156]  [  0/160]  eta: 0:22:04  lr: 0.000026  min_lr: 0.000007  loss: 1.8288 (1.8288)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9869 (7.9869)  time: 8.2803 (8.2803 -- 8.2803)  data: 5.4741 (5.4741 -- 5.4741)  max mem: 16413
Epoch: [156]  [ 20/160]  eta: 0:02:44  lr: 0.000026  min_lr: 0.000007  loss: 1.6987 (1.7626)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8032 (8.2047)  time: 0.8225 (0.5307 -- 2.2953)  data: 0.1746 (0.0003 -- 1.7419)  max mem: 16413
[2023-09-05 04:04:36,692] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:04:36,692] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:04:36,693] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:04:36,693] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:04:49,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=140, lr=[6.568771386430179e-06, 6.568771386430179e-06, 7.298634873811311e-06, 7.298634873811311e-06, 8.109594304234788e-06, 8.109594304234788e-06, 9.010660338038654e-06, 9.010660338038654e-06, 1.0011844820042949e-05, 1.0011844820042949e-05, 1.1124272022269943e-05, 1.1124272022269943e-05, 1.2360302246966603e-05, 1.2360302246966603e-05, 1.3733669163296225e-05, 1.3733669163296225e-05, 1.525963240366247e-05, 1.525963240366247e-05, 1.695514711518052e-05, 1.695514711518052e-05, 1.883905235020058e-05, 1.883905235020058e-05, 2.0932280389111755e-05, 2.0932280389111755e-05, 2.3258089321235284e-05, 2.3258089321235284e-05, 2.5842321468039204e-05, 2.5842321468039204e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 04:04:49,019] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=17.592395800858093, CurrSamplesPerSec=20.604156475113918, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [156]  [ 40/160]  eta: 0:02:03  lr: 0.000026  min_lr: 0.000007  loss: 1.6143 (1.6702)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6721 (7.3046)  time: 0.8662 (0.5319 -- 2.9618)  data: 0.1092 (0.0008 -- 1.5242)  max mem: 16413
Epoch: [156]  [ 60/160]  eta: 0:01:35  lr: 0.000026  min_lr: 0.000007  loss: 1.6462 (1.6536)  loss_scale: 32768.0000 (26053.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5930 (7.3883)  time: 0.8148 (0.5434 -- 2.1076)  data: 0.1955 (0.0005 -- 1.5609)  max mem: 16413
Epoch: [156]  [ 80/160]  eta: 0:01:15  lr: 0.000026  min_lr: 0.000007  loss: 1.5334 (1.6380)  loss_scale: 32768.0000 (27711.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9345 (7.4603)  time: 0.9116 (0.5367 -- 2.8789)  data: 0.0808 (0.0005 -- 1.3830)  max mem: 16413
Epoch: [156]  [100/160]  eta: 0:00:57  lr: 0.000026  min_lr: 0.000007  loss: 1.5550 (1.6216)  loss_scale: 32768.0000 (28712.5545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7337 (7.6116)  time: 1.0051 (0.5349 -- 2.7574)  data: 0.1923 (0.0005 -- 2.2170)  max mem: 16413
Epoch: [156]  [120/160]  eta: 0:00:37  lr: 0.000026  min_lr: 0.000007  loss: 1.5465 (1.6080)  loss_scale: 32768.0000 (29382.8760)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2926 (7.4516)  time: 0.9043 (0.5291 -- 2.6723)  data: 0.1913 (0.0003 -- 1.9804)  max mem: 16413
Epoch: [156]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000007  loss: 1.8514 (1.6284)  loss_scale: 32768.0000 (29863.0355)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8655 (7.4162)  time: 0.8937 (0.5168 -- 4.4631)  data: 0.3505 (0.0002 -- 3.9268)  max mem: 16413
[2023-09-05 04:06:30,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:06:30,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:06:30,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 04:06:30,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000007  loss: 1.6985 (1.6221)  loss_scale: 32768.0000 (31641.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1220 (7.3451)  time: 0.6873 (0.4958 -- 2.5497)  data: 0.1702 (0.0002 -- 2.0521)  max mem: 16413
Epoch: [156] Total time: 0:02:25 (0.9063 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000007  loss: 1.6985 (1.6176)  loss_scale: 32768.0000 (31641.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1220 (7.3451)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1475 (0.1475)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5053 (2.5053 -- 2.5053)  data: 2.2869 (2.2869 -- 2.2869)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6491 (0.6154)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (98.9899)  time: 0.4393 (0.1949 -- 2.5053)  data: 0.2229 (0.0007 -- 2.2869)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6176 (0.6082)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2208 (0.1698 -- 0.3714)  data: 0.0154 (0.0001 -- 0.1547)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6176 (0.6496)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2041 (0.1329 -- 0.3714)  data: 0.0151 (0.0001 -- 0.1547)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 84.647 Acc@5 97.510 loss 0.609
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.27%
Epoch: [157]  [  0/160]  eta: 0:20:54  lr: 0.000026  min_lr: 0.000007  loss: 1.0979 (1.0979)  loss_scale: 65536.0000 (65536.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5191 (6.5191)  time: 7.8382 (7.8382 -- 7.8382)  data: 7.2833 (7.2833 -- 7.2833)  max mem: 16413
[2023-09-05 04:06:51,489] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25124
[2023-09-05 04:06:51,489] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25124
[2023-09-05 04:06:51,489] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 04:06:51,490] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 04:06:51,490] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [157]  [ 20/160]  eta: 0:02:51  lr: 0.000026  min_lr: 0.000007  loss: 1.6807 (1.5688)  loss_scale: 32768.0000 (39009.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8267 (7.5208)  time: 0.8943 (0.5082 -- 4.6590)  data: 0.2670 (0.0001 -- 3.0519)  max mem: 16413
Epoch: [157]  [ 40/160]  eta: 0:02:15  lr: 0.000026  min_lr: 0.000006  loss: 1.5621 (1.5866)  loss_scale: 32768.0000 (35964.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2833 (7.3106)  time: 1.0221 (0.5275 -- 4.5394)  data: 0.0522 (0.0004 -- 1.0144)  max mem: 16413
Epoch: [157]  [ 60/160]  eta: 0:01:41  lr: 0.000026  min_lr: 0.000006  loss: 1.5968 (1.5976)  loss_scale: 32768.0000 (34916.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4644 (7.1414)  time: 0.7952 (0.5208 -- 3.1768)  data: 0.0013 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [157]  [ 80/160]  eta: 0:01:18  lr: 0.000026  min_lr: 0.000006  loss: 1.6192 (1.6088)  loss_scale: 32768.0000 (34386.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9340 (7.0237)  time: 0.8941 (0.5228 -- 3.9491)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [157]  [100/160]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000006  loss: 1.8726 (1.6545)  loss_scale: 32768.0000 (34065.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3292 (7.0961)  time: 0.8453 (0.5186 -- 4.3614)  data: 0.0017 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [157]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000006  loss: 1.6370 (1.6603)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0447 (7.0445)  time: 0.8228 (0.5236 -- 2.9011)  data: 0.0033 (0.0002 -- 0.0202)  max mem: 16413
[2023-09-05 04:08:44,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:08:44,799] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 04:08:44,800] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:08:44,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [157]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 1.4896 (1.6470)  loss_scale: 32768.0000 (35556.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5812 (7.1818)  time: 0.8834 (0.5285 -- 2.6344)  data: 0.0018 (0.0002 -- 0.0045)  max mem: 16413
[2023-09-05 04:09:01,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25273
[2023-09-05 04:09:01,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 04:09:01,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25273
[2023-09-05 04:09:01,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 04:09:01,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.4124 (1.6199)  loss_scale: 65536.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3182 (7.1342)  time: 0.7654 (0.4818 -- 1.8972)  data: 0.0011 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [157] Total time: 0:02:24 (0.9033 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.4124 (1.6472)  loss_scale: 65536.0000 (37683.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3182 (7.1342)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1422 (0.1422)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3816 (2.3816 -- 2.3816)  data: 2.1502 (2.1502 -- 2.1502)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6482 (0.5792)  acc1: 77.7778 (82.8283)  acc5: 100.0000 (100.0000)  time: 0.4279 (0.2013 -- 2.3816)  data: 0.2139 (0.0006 -- 2.1502)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5435 (0.5825)  acc1: 77.7778 (83.0688)  acc5: 100.0000 (98.4127)  time: 0.2304 (0.1708 -- 0.5349)  data: 0.0272 (0.0001 -- 0.3397)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5518 (0.6330)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.5104)  time: 0.2154 (0.1327 -- 0.5349)  data: 0.0269 (0.0001 -- 0.3397)  max mem: 16413
Val: Total time: 0:00:07 (0.2958 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.605
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.27%
Epoch: [158]  [  0/160]  eta: 0:19:31  lr: 0.000025  min_lr: 0.000006  loss: 1.8976 (1.8976)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1433 (7.1433)  time: 7.3226 (7.3226 -- 7.3226)  data: 6.7925 (6.7925 -- 6.7925)  max mem: 16413
Epoch: [158]  [ 20/160]  eta: 0:02:44  lr: 0.000025  min_lr: 0.000006  loss: 1.7281 (1.7308)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2856 (7.1660)  time: 0.8659 (0.5239 -- 3.5708)  data: 0.2662 (0.0005 -- 3.0373)  max mem: 16413
Epoch: [158]  [ 40/160]  eta: 0:02:07  lr: 0.000025  min_lr: 0.000006  loss: 1.5982 (1.6553)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7646 (6.9208)  time: 0.9545 (0.5255 -- 4.4152)  data: 0.0807 (0.0004 -- 1.0209)  max mem: 16413
Epoch: [158]  [ 60/160]  eta: 0:01:37  lr: 0.000025  min_lr: 0.000006  loss: 1.6658 (1.6682)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8866 (6.7418)  time: 0.7840 (0.5273 -- 3.7714)  data: 0.0015 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [158]  [ 80/160]  eta: 0:01:18  lr: 0.000025  min_lr: 0.000006  loss: 1.6552 (1.6618)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5186 (6.8267)  time: 1.0018 (0.5208 -- 3.9954)  data: 0.0469 (0.0005 -- 0.9136)  max mem: 16413
Epoch: [158]  [100/160]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000006  loss: 1.5092 (1.6385)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9849 (6.8177)  time: 0.7167 (0.5362 -- 2.1208)  data: 0.0026 (0.0006 -- 0.0160)  max mem: 16413
[2023-09-05 04:11:02,641] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25398
[2023-09-05 04:11:02,641] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25398
[2023-09-05 04:11:02,641] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:11:02,641] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:11:02,641] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [158]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000006  loss: 1.5727 (1.6312)  loss_scale: 32768.0000 (32361.7851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0254 (6.8482)  time: 0.9100 (0.5259 -- 2.3764)  data: 0.1753 (0.0006 -- 1.7925)  max mem: 16413
Epoch: [158]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 1.5789 (1.6289)  loss_scale: 16384.0000 (30095.4326)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8821 (6.9574)  time: 0.9438 (0.5235 -- 2.5907)  data: 0.3916 (0.0002 -- 2.0628)  max mem: 16413
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.6313 (1.6301)  loss_scale: 16384.0000 (28467.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8932 (7.0391)  time: 0.6588 (0.4952 -- 1.7381)  data: 0.1361 (0.0002 -- 1.2405)  max mem: 16413
Epoch: [158] Total time: 0:02:23 (0.8970 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.6313 (1.6120)  loss_scale: 16384.0000 (28467.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8932 (7.0391)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1432 (0.1432)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2858 (2.2858 -- 2.2858)  data: 2.0654 (2.0654 -- 2.0654)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5971 (0.5903)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4344 (0.2137 -- 2.2858)  data: 0.2030 (0.0008 -- 2.0654)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5971 (0.6003)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (98.9418)  time: 0.2326 (0.1702 -- 0.3732)  data: 0.0178 (0.0001 -- 0.1867)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6258 (0.6287)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (98.3402)  time: 0.2097 (0.1327 -- 0.3732)  data: 0.0170 (0.0001 -- 0.1867)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 83.402 Acc@5 98.133 loss 0.599
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 85.27%
Epoch: [159]  [  0/160]  eta: 0:21:03  lr: 0.000025  min_lr: 0.000006  loss: 2.1667 (2.1667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4588 (9.4588)  time: 7.8939 (7.8939 -- 7.8939)  data: 7.3585 (7.3585 -- 7.3585)  max mem: 16413
Epoch: [159]  [ 20/160]  eta: 0:02:44  lr: 0.000025  min_lr: 0.000006  loss: 1.5424 (1.6466)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8275 (9.2565)  time: 0.8412 (0.5281 -- 3.4933)  data: 0.2827 (0.0005 -- 2.8654)  max mem: 16413
Epoch: [159]  [ 40/160]  eta: 0:02:11  lr: 0.000025  min_lr: 0.000006  loss: 1.6322 (1.6537)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7782 (8.0318)  time: 1.0030 (0.5249 -- 4.2141)  data: 0.4482 (0.0002 -- 3.6715)  max mem: 16413
Epoch: [159]  [ 60/160]  eta: 0:01:37  lr: 0.000025  min_lr: 0.000006  loss: 1.5231 (1.6070)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3793 (7.5201)  time: 0.7441 (0.5315 -- 3.0482)  data: 0.1929 (0.0004 -- 2.5355)  max mem: 16413
Epoch: [159]  [ 80/160]  eta: 0:01:17  lr: 0.000025  min_lr: 0.000006  loss: 1.5216 (1.5966)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3858 (7.3319)  time: 0.9317 (0.5252 -- 5.2713)  data: 0.3920 (0.0004 -- 4.7618)  max mem: 16413
[2023-09-05 04:13:07,771] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:13:07,771] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:13:07,771] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:13:07,771] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [159]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000006  loss: 1.5161 (1.5954)  loss_scale: 32768.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7683 (7.1147)  time: 0.8253 (0.5320 -- 2.7931)  data: 0.2691 (0.0003 -- 2.2610)  max mem: 16413
Epoch: [159]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000006  loss: 1.7149 (1.6078)  loss_scale: 32768.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8115 (7.1282)  time: 0.9048 (0.5093 -- 4.9825)  data: 0.3652 (0.0003 -- 4.4819)  max mem: 16413
[2023-09-05 04:13:51,726] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25577
[2023-09-05 04:13:51,726] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25577
[2023-09-05 04:13:51,726] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:13:51,726] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:13:51,726] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 1.6199 (1.6157)  loss_scale: 32768.0000 (22193.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0364 (7.0753)  time: 0.7707 (0.5195 -- 2.0913)  data: 0.1383 (0.0004 -- 1.4481)  max mem: 16413
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.7195 (1.6162)  loss_scale: 16384.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8586 (7.0552)  time: 0.7193 (0.4953 -- 1.9999)  data: 0.0518 (0.0002 -- 0.4882)  max mem: 16413
Epoch: [159] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.7195 (1.6159)  loss_scale: 16384.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8586 (7.0552)
[2023-09-05 04:14:07,188] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-09-05 04:14:07,189] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-09-05 04:14:07,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-09-05 04:14:07,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-09-05 04:14:08,125] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-09-05 04:14:08,125] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1419 (0.1419)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4057 (2.4057 -- 2.4057)  data: 2.1424 (2.1424 -- 2.1424)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6001 (0.5930)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4221 (0.1945 -- 2.4057)  data: 0.2050 (0.0005 -- 2.1424)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5367 (0.5821)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2238 (0.1681 -- 0.4959)  data: 0.0206 (0.0001 -- 0.2962)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5510 (0.6144)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2067 (0.1324 -- 0.4959)  data: 0.0204 (0.0001 -- 0.2962)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 85.477 Acc@5 97.510 loss 0.601
Accuracy of the network on the 482 val images: 85.48%
[2023-09-05 04:14:16,041] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 04:14:16,043] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 04:14:16,043] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 04:14:16,043] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 04:14:17,453] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 04:14:17,453] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.48%
Epoch: [160]  [  0/160]  eta: 0:22:48  lr: 0.000025  min_lr: 0.000006  loss: 2.1036 (2.1036)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3712 (5.3712)  time: 8.5545 (8.5545 -- 8.5545)  data: 6.8262 (6.8262 -- 6.8262)  max mem: 16413
Epoch: [160]  [ 20/160]  eta: 0:02:47  lr: 0.000025  min_lr: 0.000006  loss: 1.6486 (1.6547)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9454 (7.6613)  time: 0.8271 (0.5190 -- 4.0077)  data: 0.2744 (0.0002 -- 3.4645)  max mem: 16413
Epoch: [160]  [ 40/160]  eta: 0:02:16  lr: 0.000025  min_lr: 0.000006  loss: 1.5183 (1.6045)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3268 (7.7554)  time: 1.0826 (0.5284 -- 5.7259)  data: 0.5316 (0.0004 -- 5.2109)  max mem: 16413
Epoch: [160]  [ 60/160]  eta: 0:01:40  lr: 0.000025  min_lr: 0.000006  loss: 1.7448 (1.6271)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2939 (7.5048)  time: 0.7332 (0.5085 -- 3.5099)  data: 0.1950 (0.0001 -- 2.9636)  max mem: 16413
[2023-09-05 04:15:31,962] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25674
[2023-09-05 04:15:31,962] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25674
[2023-09-05 04:15:31,962] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 04:15:31,962] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 04:15:31,962] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [160]  [ 80/160]  eta: 0:01:19  lr: 0.000025  min_lr: 0.000006  loss: 1.4830 (1.5865)  loss_scale: 16384.0000 (15676.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4387 (7.5252)  time: 0.9628 (0.5193 -- 4.0690)  data: 0.4060 (0.0002 -- 3.5351)  max mem: 16413
Epoch: [160]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000006  loss: 1.4868 (1.5852)  loss_scale: 8192.0000 (14194.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6208 (7.4809)  time: 0.6876 (0.5270 -- 2.3660)  data: 0.1382 (0.0006 -- 1.8543)  max mem: 16413
Epoch: [160]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000006  loss: 1.6111 (1.5980)  loss_scale: 8192.0000 (13201.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2643 (7.4998)  time: 0.9821 (0.5353 -- 3.3436)  data: 0.3747 (0.0004 -- 2.7503)  max mem: 16413
Epoch: [160]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000006  loss: 1.7182 (1.6080)  loss_scale: 8192.0000 (12491.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9229 (7.6233)  time: 0.7687 (0.5239 -- 2.5635)  data: 0.2190 (0.0002 -- 2.0283)  max mem: 16413
Epoch: [160]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000006  loss: 1.5839 (1.6035)  loss_scale: 8192.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8519 (7.6155)  time: 0.7038 (0.4978 -- 2.7345)  data: 0.1816 (0.0002 -- 2.1987)  max mem: 16413
Epoch: [160] Total time: 0:02:23 (0.8939 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000006  loss: 1.5839 (1.6176)  loss_scale: 8192.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8519 (7.6155)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1282 (0.1282)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4007 (2.4007 -- 2.4007)  data: 2.1914 (2.1914 -- 2.1914)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6126 (0.6268)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4286 (0.1984 -- 2.4007)  data: 0.2141 (0.0003 -- 2.1914)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5353 (0.5966)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (97.8836)  time: 0.2232 (0.1699 -- 0.3777)  data: 0.0170 (0.0001 -- 0.1723)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5137 (0.6223)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.5104)  time: 0.2088 (0.1329 -- 0.3777)  data: 0.0168 (0.0001 -- 0.1723)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 84.440 Acc@5 97.303 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.48%
Epoch: [161]  [  0/160]  eta: 0:18:55  lr: 0.000025  min_lr: 0.000006  loss: 2.4432 (2.4432)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4543 (6.4543)  time: 7.0963 (7.0963 -- 7.0963)  data: 6.1961 (6.1961 -- 6.1961)  max mem: 16413
Epoch: [161]  [ 20/160]  eta: 0:02:40  lr: 0.000025  min_lr: 0.000006  loss: 1.4324 (1.5031)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1382 (6.7692)  time: 0.8514 (0.5177 -- 2.4798)  data: 0.2795 (0.0004 -- 1.9582)  max mem: 16413
Epoch: [161]  [ 40/160]  eta: 0:02:02  lr: 0.000024  min_lr: 0.000006  loss: 1.4347 (1.5006)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9271 (6.8626)  time: 0.8812 (0.5299 -- 2.1587)  data: 0.3292 (0.0003 -- 1.6245)  max mem: 16413
[2023-09-05 04:17:32,288] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:17:32,289] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 04:17:32,289] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:17:32,289] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 04:17:44,951] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25816
[2023-09-05 04:17:44,952] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25816
[2023-09-05 04:17:44,952] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 04:17:44,952] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 04:17:44,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [161]  [ 60/160]  eta: 0:01:36  lr: 0.000024  min_lr: 0.000006  loss: 1.6042 (1.5536)  loss_scale: 16384.0000 (9937.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1647 (7.1468)  time: 0.8564 (0.5218 -- 2.5912)  data: 0.3102 (0.0006 -- 2.0388)  max mem: 16413
Epoch: [161]  [ 80/160]  eta: 0:01:16  lr: 0.000024  min_lr: 0.000006  loss: 1.5567 (1.5800)  loss_scale: 8192.0000 (9506.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0266 (7.1349)  time: 0.9420 (0.5378 -- 2.7209)  data: 0.3941 (0.0002 -- 2.1823)  max mem: 16413
Epoch: [161]  [100/160]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000006  loss: 1.7193 (1.5985)  loss_scale: 8192.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8751 (7.1275)  time: 0.8634 (0.5269 -- 2.0874)  data: 0.2721 (0.0003 -- 1.5506)  max mem: 16413
Epoch: [161]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000006  loss: 1.5740 (1.5945)  loss_scale: 8192.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8757 (7.0587)  time: 0.8593 (0.5286 -- 2.7759)  data: 0.2739 (0.0005 -- 2.2349)  max mem: 16413
Epoch: [161]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000006  loss: 1.5611 (1.5781)  loss_scale: 8192.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3955 (6.9895)  time: 0.8102 (0.5301 -- 2.5976)  data: 0.2560 (0.0003 -- 2.0570)  max mem: 16413
Epoch: [161]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.5793 (1.5763)  loss_scale: 8192.0000 (8857.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7136 (6.9340)  time: 0.7467 (0.4955 -- 2.1532)  data: 0.2037 (0.0002 -- 1.6289)  max mem: 16413
Epoch: [161] Total time: 0:02:22 (0.8922 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.5793 (1.6195)  loss_scale: 8192.0000 (8857.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7136 (6.9340)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1334 (0.1334)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5524 (2.5524 -- 2.5524)  data: 2.3334 (2.3334 -- 2.3334)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7156 (0.6438)  acc1: 77.7778 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4336 (0.1976 -- 2.5524)  data: 0.2130 (0.0007 -- 2.3334)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5159 (0.5972)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (98.4127)  time: 0.2129 (0.1689 -- 0.2518)  data: 0.0040 (0.0001 -- 0.0674)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5753 (0.6382)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (97.5104)  time: 0.1964 (0.1327 -- 0.2493)  data: 0.0038 (0.0001 -- 0.0674)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 83.195 Acc@5 97.510 loss 0.623
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 85.48%
Epoch: [162]  [  0/160]  eta: 0:17:37  lr: 0.000024  min_lr: 0.000006  loss: 1.7215 (1.7215)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9782 (6.9782)  time: 6.6070 (6.6070 -- 6.6070)  data: 5.9373 (5.9373 -- 5.9373)  max mem: 16413
Epoch: [162]  [ 20/160]  eta: 0:02:41  lr: 0.000024  min_lr: 0.000006  loss: 1.6181 (1.6255)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5390 (7.0961)  time: 0.8839 (0.5329 -- 3.1832)  data: 0.2223 (0.0007 -- 2.4958)  max mem: 16413
[2023-09-05 04:19:49,083] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:19:49,083] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:19:49,083] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 04:19:49,084] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [162]  [ 40/160]  eta: 0:02:04  lr: 0.000024  min_lr: 0.000006  loss: 1.4781 (1.5829)  loss_scale: 16384.0000 (11388.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8389 (6.8775)  time: 0.9137 (0.5275 -- 4.6585)  data: 0.3306 (0.0002 -- 4.0856)  max mem: 16413
Epoch: [162]  [ 60/160]  eta: 0:01:39  lr: 0.000024  min_lr: 0.000006  loss: 1.5011 (1.5673)  loss_scale: 16384.0000 (13026.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2620 (7.1711)  time: 0.9022 (0.5319 -- 3.2862)  data: 0.3489 (0.0004 -- 2.7647)  max mem: 16413
[2023-09-05 04:20:38,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=146, lr=[6.136987040475162e-06, 6.136987040475162e-06, 6.818874489416848e-06, 6.818874489416848e-06, 7.576527210463162e-06, 7.576527210463162e-06, 8.418363567181292e-06, 8.418363567181292e-06, 9.353737296868102e-06, 9.353737296868102e-06, 1.0393041440964559e-05, 1.0393041440964559e-05, 1.1547823823293952e-05, 1.1547823823293952e-05, 1.2830915359215503e-05, 1.2830915359215503e-05, 1.4256572621350558e-05, 1.4256572621350558e-05, 1.5840636245945063e-05, 1.5840636245945063e-05, 1.7600706939938962e-05, 1.7600706939938962e-05, 1.9556341044376622e-05, 1.9556341044376622e-05, 2.1729267827085134e-05, 2.1729267827085134e-05, 2.4143630918983482e-05, 2.4143630918983482e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 04:20:38,066] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=17.640872846258944, CurrSamplesPerSec=22.632591933743942, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [162]  [ 80/160]  eta: 0:01:18  lr: 0.000024  min_lr: 0.000006  loss: 1.5690 (1.5823)  loss_scale: 16384.0000 (13855.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8281 (7.1203)  time: 0.9540 (0.5262 -- 3.3190)  data: 0.4103 (0.0002 -- 2.7757)  max mem: 16413
Epoch: [162]  [100/160]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000006  loss: 1.6568 (1.5868)  loss_scale: 16384.0000 (14356.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9895 (7.1939)  time: 0.7349 (0.5128 -- 3.1344)  data: 0.1839 (0.0003 -- 2.5998)  max mem: 16413
Epoch: [162]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000006  loss: 1.5760 (1.5876)  loss_scale: 16384.0000 (14691.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7169 (7.2205)  time: 0.8765 (0.5362 -- 2.9298)  data: 0.2806 (0.0006 -- 2.3851)  max mem: 16413
Epoch: [162]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000006  loss: 1.4546 (1.5819)  loss_scale: 16384.0000 (14931.5177)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5838 (7.1170)  time: 0.8478 (0.5386 -- 2.7595)  data: 0.2978 (0.0004 -- 2.1919)  max mem: 16413
[2023-09-05 04:21:37,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:21:37,521] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:21:37,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:21:37,521] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [162]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.4891 (1.5820)  loss_scale: 16384.0000 (15820.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3609 (7.0582)  time: 0.6813 (0.4966 -- 2.4253)  data: 0.1525 (0.0002 -- 1.8953)  max mem: 16413
Epoch: [162] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.4891 (1.6135)  loss_scale: 16384.0000 (15820.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3609 (7.0582)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1337 (0.1337)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5165 (2.5165 -- 2.5165)  data: 2.2763 (2.2763 -- 2.2763)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6115 (0.5760)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4269 (0.1888 -- 2.5165)  data: 0.2090 (0.0008 -- 2.2763)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5410 (0.5779)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2150 (0.1694 -- 0.3128)  data: 0.0097 (0.0001 -- 0.0930)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5735 (0.6091)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.1986 (0.1327 -- 0.3128)  data: 0.0088 (0.0001 -- 0.0930)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 83.817 Acc@5 97.718 loss 0.600
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 85.48%
Epoch: [163]  [  0/160]  eta: 0:15:29  lr: 0.000024  min_lr: 0.000006  loss: 0.9352 (0.9352)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0138 (6.0138)  time: 5.8072 (5.8072 -- 5.8072)  data: 5.2823 (5.2823 -- 5.2823)  max mem: 16413
Epoch: [163]  [ 20/160]  eta: 0:02:47  lr: 0.000024  min_lr: 0.000006  loss: 1.7006 (1.6265)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8055 (6.8983)  time: 0.9668 (0.5217 -- 2.8475)  data: 0.4208 (0.0007 -- 2.3144)  max mem: 16413
Epoch: [163]  [ 40/160]  eta: 0:02:06  lr: 0.000024  min_lr: 0.000006  loss: 1.7151 (1.6311)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4248 (7.1735)  time: 0.9093 (0.5349 -- 4.2171)  data: 0.3532 (0.0001 -- 3.7119)  max mem: 16413
[2023-09-05 04:22:35,342] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26123
[2023-09-05 04:22:35,342] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:22:35,342] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26123
[2023-09-05 04:22:35,342] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:22:35,342] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [163]  [ 60/160]  eta: 0:01:41  lr: 0.000024  min_lr: 0.000006  loss: 1.5826 (1.6205)  loss_scale: 16384.0000 (27933.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7400 (7.3629)  time: 0.9392 (0.5170 -- 4.1038)  data: 0.3908 (0.0003 -- 3.5768)  max mem: 16413
Epoch: [163]  [ 80/160]  eta: 0:01:16  lr: 0.000024  min_lr: 0.000006  loss: 1.5082 (1.6180)  loss_scale: 16384.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5878 (7.2773)  time: 0.7647 (0.5189 -- 3.3282)  data: 0.2208 (0.0004 -- 2.7980)  max mem: 16413
Epoch: [163]  [100/160]  eta: 0:00:55  lr: 0.000024  min_lr: 0.000006  loss: 1.7318 (1.6232)  loss_scale: 16384.0000 (23359.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6540 (7.5214)  time: 0.8314 (0.5349 -- 2.0828)  data: 0.2822 (0.0003 -- 1.5616)  max mem: 16413
Epoch: [163]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000006  loss: 1.6510 (1.6180)  loss_scale: 16384.0000 (22206.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7825 (7.3394)  time: 0.8453 (0.5334 -- 2.2648)  data: 0.1253 (0.0003 -- 1.7104)  max mem: 16413
Epoch: [163]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000006  loss: 1.5340 (1.6133)  loss_scale: 16384.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8595 (7.4205)  time: 0.9412 (0.5223 -- 3.9585)  data: 0.1078 (0.0003 -- 2.0174)  max mem: 16413
Epoch: [163]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000006  loss: 1.6358 (1.6131)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2113 (7.3602)  time: 0.6929 (0.4947 -- 2.3600)  data: 0.0013 (0.0003 -- 0.0128)  max mem: 16413
Epoch: [163] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000006  loss: 1.6358 (1.6187)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2113 (7.3602)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1386 (0.1386)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5317 (2.5317 -- 2.5317)  data: 2.2956 (2.2956 -- 2.2956)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6922 (0.6046)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (100.0000)  time: 0.4465 (0.2032 -- 2.5317)  data: 0.2270 (0.0007 -- 2.2956)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5182 (0.5745)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (98.4127)  time: 0.2200 (0.1702 -- 0.4147)  data: 0.0142 (0.0001 -- 0.1811)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5466 (0.6191)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.5104)  time: 0.2031 (0.1329 -- 0.4147)  data: 0.0134 (0.0001 -- 0.1811)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 83.610 Acc@5 97.510 loss 0.618
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 85.48%
Epoch: [164]  [  0/160]  eta: 0:20:56  lr: 0.000024  min_lr: 0.000006  loss: 1.9938 (1.9938)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8730 (6.8730)  time: 7.8501 (7.8501 -- 7.8501)  data: 6.5431 (6.5431 -- 6.5431)  max mem: 16413
[2023-09-05 04:24:36,374] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:24:36,374] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:24:36,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:24:36,377] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [164]  [ 20/160]  eta: 0:02:38  lr: 0.000024  min_lr: 0.000006  loss: 1.6291 (1.6378)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3139 (6.8624)  time: 0.7929 (0.5293 -- 3.0745)  data: 0.0509 (0.0007 -- 0.9250)  max mem: 16413
Epoch: [164]  [ 40/160]  eta: 0:02:01  lr: 0.000024  min_lr: 0.000006  loss: 1.5491 (1.5929)  loss_scale: 32768.0000 (27972.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8699 (7.0950)  time: 0.8958 (0.5309 -- 2.4120)  data: 0.1455 (0.0004 -- 1.7668)  max mem: 16413
Epoch: [164]  [ 60/160]  eta: 0:01:38  lr: 0.000024  min_lr: 0.000006  loss: 1.6940 (1.5939)  loss_scale: 32768.0000 (29544.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4875 (7.4504)  time: 0.9179 (0.5304 -- 3.3356)  data: 0.2638 (0.0010 -- 2.7989)  max mem: 16413
Epoch: [164]  [ 80/160]  eta: 0:01:16  lr: 0.000024  min_lr: 0.000006  loss: 1.7126 (1.6316)  loss_scale: 32768.0000 (30340.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6069 (7.5783)  time: 0.8658 (0.5292 -- 3.0032)  data: 0.3171 (0.0001 -- 2.4611)  max mem: 16413
[2023-09-05 04:25:40,241] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26324
[2023-09-05 04:25:40,241] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26324
[2023-09-05 04:25:40,241] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:25:40,241] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:25:40,241] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [164]  [100/160]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000006  loss: 1.6095 (1.6154)  loss_scale: 16384.0000 (28063.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9188 (7.7906)  time: 0.9140 (0.5050 -- 3.6335)  data: 0.3598 (0.0008 -- 3.1093)  max mem: 16413
Epoch: [164]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000006  loss: 1.5377 (1.6170)  loss_scale: 16384.0000 (26133.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4025 (7.6589)  time: 0.8280 (0.5388 -- 3.2961)  data: 0.2733 (0.0003 -- 2.7582)  max mem: 16413
Epoch: [164]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.8053 (1.6305)  loss_scale: 16384.0000 (24750.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5459 (7.4803)  time: 0.9768 (0.5299 -- 3.1172)  data: 0.4318 (0.0004 -- 2.5666)  max mem: 16413
Epoch: [164]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.4965 (1.6143)  loss_scale: 16384.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6496 (7.3870)  time: 0.7372 (0.4944 -- 2.9925)  data: 0.2188 (0.0002 -- 2.4552)  max mem: 16413
Epoch: [164] Total time: 0:02:23 (0.8966 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.4965 (1.6234)  loss_scale: 16384.0000 (23756.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6496 (7.3870)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1301 (0.1301)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4339 (2.4339 -- 2.4339)  data: 2.2174 (2.2174 -- 2.2174)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5108 (0.5439)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4107 (0.1939 -- 2.4339)  data: 0.2030 (0.0003 -- 2.2174)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5108 (0.5556)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2172 (0.1682 -- 0.4424)  data: 0.0160 (0.0001 -- 0.2396)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5325 (0.5845)  acc1: 85.7143 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2037 (0.1324 -- 0.4424)  data: 0.0155 (0.0001 -- 0.2396)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 85.270 Acc@5 97.510 loss 0.596
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 85.48%
Epoch: [165]  [  0/160]  eta: 0:20:12  lr: 0.000023  min_lr: 0.000006  loss: 2.3893 (2.3893)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4355 (4.4355)  time: 7.5793 (7.5793 -- 7.5793)  data: 6.5478 (6.5478 -- 6.5478)  max mem: 16413
Epoch: [165]  [ 20/160]  eta: 0:02:41  lr: 0.000023  min_lr: 0.000006  loss: 1.6468 (1.6784)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0712 (7.1753)  time: 0.8347 (0.5349 -- 3.2773)  data: 0.0420 (0.0002 -- 0.7938)  max mem: 16413
Epoch: [165]  [ 40/160]  eta: 0:02:03  lr: 0.000023  min_lr: 0.000006  loss: 1.5569 (1.6372)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0597 (7.2219)  time: 0.8888 (0.5289 -- 2.8076)  data: 0.0666 (0.0004 -- 0.6221)  max mem: 16413
[2023-09-05 04:27:44,114] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:27:44,115] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:27:44,116] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:27:44,116] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [165]  [ 60/160]  eta: 0:01:41  lr: 0.000023  min_lr: 0.000006  loss: 1.5761 (1.6175)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1197 (6.9433)  time: 1.0032 (0.5188 -- 5.3810)  data: 0.0062 (0.0004 -- 0.0844)  max mem: 16413
Epoch: [165]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000006  loss: 1.4839 (1.5967)  loss_scale: 32768.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4505 (7.0749)  time: 0.8238 (0.5284 -- 3.3837)  data: 0.0020 (0.0005 -- 0.0092)  max mem: 16413
Epoch: [165]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000006  loss: 1.6745 (1.6152)  loss_scale: 32768.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2946 (7.1462)  time: 0.7999 (0.5272 -- 3.7839)  data: 0.0016 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [165]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000006  loss: 1.6143 (1.6182)  loss_scale: 32768.0000 (25591.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9713 (7.0019)  time: 0.9185 (0.5338 -- 3.6489)  data: 0.0024 (0.0006 -- 0.0165)  max mem: 16413
Epoch: [165]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.5963 (1.6264)  loss_scale: 32768.0000 (26609.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9952 (6.9595)  time: 0.8813 (0.5182 -- 5.1590)  data: 0.0013 (0.0004 -- 0.0024)  max mem: 16413
Epoch: [165]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.6583 (1.6159)  loss_scale: 32768.0000 (27340.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9952 (6.9186)  time: 0.7595 (0.4944 -- 3.8455)  data: 0.0007 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [165] Total time: 0:02:25 (0.9076 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.6583 (1.6481)  loss_scale: 32768.0000 (27340.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9952 (6.9186)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1350 (0.1350)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5033 (2.5033 -- 2.5033)  data: 2.2932 (2.2932 -- 2.2932)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5636 (0.5495)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4226 (0.1975 -- 2.5033)  data: 0.2100 (0.0004 -- 2.2932)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5089 (0.5554)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2214 (0.1697 -- 0.5099)  data: 0.0170 (0.0001 -- 0.3212)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5322 (0.5987)  acc1: 77.7778 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2073 (0.1320 -- 0.5099)  data: 0.0168 (0.0001 -- 0.3212)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 84.647 Acc@5 97.718 loss 0.597
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.48%
Epoch: [166]  [  0/160]  eta: 0:16:33  lr: 0.000023  min_lr: 0.000006  loss: 1.7684 (1.7684)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7255 (4.7255)  time: 6.2079 (6.2079 -- 6.2079)  data: 5.6917 (5.6917 -- 5.6917)  max mem: 16413
[2023-09-05 04:29:35,003] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26567
[2023-09-05 04:29:35,003] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26567
[2023-09-05 04:29:35,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:29:35,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:29:35,004] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [166]  [ 20/160]  eta: 0:02:48  lr: 0.000023  min_lr: 0.000006  loss: 1.7338 (1.7428)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9496 (7.2122)  time: 0.9510 (0.5270 -- 3.5239)  data: 0.1776 (0.0003 -- 2.9930)  max mem: 16413
Epoch: [166]  [ 40/160]  eta: 0:01:59  lr: 0.000023  min_lr: 0.000006  loss: 1.6093 (1.7016)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7602 (7.1826)  time: 0.7726 (0.5403 -- 2.2741)  data: 0.1606 (0.0004 -- 1.5288)  max mem: 16413
Epoch: [166]  [ 60/160]  eta: 0:01:41  lr: 0.000023  min_lr: 0.000006  loss: 1.4480 (1.6249)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0333 (6.9067)  time: 1.0479 (0.5364 -- 3.3393)  data: 0.0520 (0.0006 -- 0.9067)  max mem: 16413
Epoch: [166]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000006  loss: 1.3754 (1.6024)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9160 (7.0384)  time: 0.8696 (0.5356 -- 3.9033)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [166]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000006  loss: 1.6533 (1.6261)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8235 (6.9690)  time: 0.7343 (0.5284 -- 1.9153)  data: 0.0021 (0.0003 -- 0.0161)  max mem: 16413
Epoch: [166]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000006  loss: 1.6728 (1.6287)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0568 (7.0443)  time: 0.8780 (0.5256 -- 4.1172)  data: 0.0290 (0.0002 -- 0.5511)  max mem: 16413
[2023-09-05 04:31:31,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:31:31,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:31:31,904] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:31:31,905] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [166]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.5860 (1.6237)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1993 (7.0090)  time: 0.9483 (0.5234 -- 4.9020)  data: 0.3122 (0.0003 -- 4.3908)  max mem: 16413
Epoch: [166]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.5743 (1.6288)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1977 (6.9961)  time: 0.6502 (0.4974 -- 2.1272)  data: 0.1239 (0.0002 -- 1.6258)  max mem: 16413
Epoch: [166] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.5743 (1.6312)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1977 (6.9961)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1168 (0.1168)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4407 (2.4407 -- 2.4407)  data: 2.1761 (2.1761 -- 2.1761)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6015 (0.5725)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4127 (0.1949 -- 2.4407)  data: 0.1997 (0.0009 -- 2.1761)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4962 (0.5691)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2161 (0.1687 -- 0.3897)  data: 0.0134 (0.0001 -- 0.1767)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5375 (0.6253)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2031 (0.1328 -- 0.3897)  data: 0.0130 (0.0001 -- 0.1767)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 84.025 Acc@5 97.510 loss 0.614
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.48%
Epoch: [167]  [  0/160]  eta: 0:20:18  lr: 0.000023  min_lr: 0.000006  loss: 1.9448 (1.9448)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2275 (4.2275)  time: 7.6136 (7.6136 -- 7.6136)  data: 7.0796 (7.0796 -- 7.0796)  max mem: 16413
Epoch: [167]  [ 20/160]  eta: 0:02:52  lr: 0.000023  min_lr: 0.000006  loss: 1.6254 (1.6287)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9036 (7.3738)  time: 0.9122 (0.5301 -- 3.2854)  data: 0.3263 (0.0003 -- 2.7608)  max mem: 16413
[2023-09-05 04:32:32,916] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26755
[2023-09-05 04:32:32,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:32:32,916] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26755
[2023-09-05 04:32:32,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:32:32,917] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [167]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000006  loss: 1.4988 (1.5701)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2634 (7.2369)  time: 0.7778 (0.5141 -- 3.9727)  data: 0.2328 (0.0004 -- 3.4207)  max mem: 16413
Epoch: [167]  [ 60/160]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000006  loss: 1.6294 (1.5894)  loss_scale: 16384.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4569 (7.2121)  time: 0.9583 (0.5227 -- 4.1025)  data: 0.4097 (0.0005 -- 3.5632)  max mem: 16413
Epoch: [167]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000006  loss: 1.6138 (1.5906)  loss_scale: 16384.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7809 (7.2102)  time: 0.7587 (0.5299 -- 3.5854)  data: 0.2118 (0.0002 -- 3.0708)  max mem: 16413
Epoch: [167]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000006  loss: 1.7513 (1.6193)  loss_scale: 16384.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1624 (7.3247)  time: 0.9742 (0.5240 -- 4.8623)  data: 0.0955 (0.0007 -- 1.3747)  max mem: 16413
Epoch: [167]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000006  loss: 1.7032 (1.6202)  loss_scale: 16384.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9109 (7.3040)  time: 0.9773 (0.5292 -- 3.4076)  data: 0.0576 (0.0003 -- 1.1304)  max mem: 16413
Epoch: [167]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000006  loss: 1.6653 (1.6324)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7059 (7.3968)  time: 0.8163 (0.5351 -- 3.0753)  data: 0.2634 (0.0003 -- 2.5671)  max mem: 16413
Epoch: [167]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000006  loss: 1.6585 (1.6366)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8587 (7.3180)  time: 0.7170 (0.4968 -- 2.4808)  data: 0.1961 (0.0002 -- 1.9792)  max mem: 16413
Epoch: [167] Total time: 0:02:24 (0.9056 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000006  loss: 1.6585 (1.6294)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8587 (7.3180)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1389 (0.1389)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4118 (2.4118 -- 2.4118)  data: 2.1520 (2.1520 -- 2.1520)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5987 (0.5396)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4387 (0.1955 -- 2.4118)  data: 0.2205 (0.0008 -- 2.1520)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5856 (0.5538)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2210 (0.1712 -- 0.5012)  data: 0.0174 (0.0001 -- 0.2637)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5987 (0.6108)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (97.5104)  time: 0.2067 (0.1328 -- 0.5012)  data: 0.0171 (0.0001 -- 0.2637)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 84.232 Acc@5 97.303 loss 0.612
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.48%
Epoch: [168]  [  0/160]  eta: 0:21:26  lr: 0.000023  min_lr: 0.000006  loss: 2.0148 (2.0148)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.0794 (15.0794)  time: 8.0417 (8.0417 -- 8.0417)  data: 6.2736 (6.2736 -- 6.2736)  max mem: 16413
[2023-09-05 04:34:37,249] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:34:37,249] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:34:37,250] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:34:37,250] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [168]  [ 20/160]  eta: 0:02:57  lr: 0.000023  min_lr: 0.000006  loss: 1.6135 (1.6021)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3243 (7.0789)  time: 0.9324 (0.5237 -- 4.9844)  data: 0.1468 (0.0002 -- 2.6648)  max mem: 16413
[2023-09-05 04:35:00,462] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26909
[2023-09-05 04:35:00,462] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26909
[2023-09-05 04:35:00,463] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:35:00,463] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:35:00,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [168]  [ 40/160]  eta: 0:02:03  lr: 0.000023  min_lr: 0.000006  loss: 1.5540 (1.6175)  loss_scale: 16384.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3934 (7.1663)  time: 0.7802 (0.5196 -- 2.6113)  data: 0.0016 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [168]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000006  loss: 1.6625 (1.6312)  loss_scale: 16384.0000 (23098.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1244 (7.1275)  time: 0.8290 (0.5169 -- 3.4635)  data: 0.0253 (0.0007 -- 0.2097)  max mem: 16413
Epoch: [168]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000006  loss: 1.6346 (1.6355)  loss_scale: 16384.0000 (21440.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5965 (7.0132)  time: 1.0199 (0.5265 -- 3.9610)  data: 0.0365 (0.0003 -- 0.7047)  max mem: 16413
Epoch: [168]  [100/160]  eta: 0:00:57  lr: 0.000022  min_lr: 0.000006  loss: 1.5927 (1.6263)  loss_scale: 16384.0000 (20439.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4061 (7.0777)  time: 0.8409 (0.5261 -- 3.9037)  data: 0.0014 (0.0004 -- 0.0054)  max mem: 16413
[2023-09-05 04:36:16,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=151, lr=[5.704285378062439e-06, 5.704285378062439e-06, 6.3380948645138216e-06, 6.3380948645138216e-06, 7.042327627237578e-06, 7.042327627237578e-06, 7.824808474708421e-06, 7.824808474708421e-06, 8.694231638564912e-06, 8.694231638564912e-06, 9.660257376183236e-06, 9.660257376183236e-06, 1.073361930687026e-05, 1.073361930687026e-05, 1.1926243674300289e-05, 1.1926243674300289e-05, 1.3251381860333654e-05, 1.3251381860333654e-05, 1.4723757622592949e-05, 1.4723757622592949e-05, 1.6359730691769945e-05, 1.6359730691769945e-05, 1.8177478546411047e-05, 1.8177478546411047e-05, 2.0197198384901165e-05, 2.0197198384901165e-05, 2.244133153877907e-05, 2.244133153877907e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 04:36:16,816] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=17.63553117169215, CurrSamplesPerSec=22.20504739098133, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [168]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000006  loss: 1.5669 (1.6258)  loss_scale: 16384.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7438 (7.0918)  time: 0.7271 (0.5337 -- 2.3295)  data: 0.0559 (0.0006 -- 1.0864)  max mem: 16413
Epoch: [168]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000006  loss: 1.3705 (1.6026)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0804 (7.0542)  time: 0.9605 (0.5275 -- 5.0264)  data: 0.4134 (0.0004 -- 4.5162)  max mem: 16413
[2023-09-05 04:36:49,028] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:36:49,028] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:36:49,028] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:36:49,028] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [168]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.7859 (1.6093)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5128 (6.9524)  time: 0.6591 (0.4952 -- 3.3326)  data: 0.1422 (0.0002 -- 2.8239)  max mem: 16413
Epoch: [168] Total time: 0:02:22 (0.8908 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.7859 (1.5919)  loss_scale: 16384.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5128 (6.9524)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1208 (0.1208)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2877 (2.2877 -- 2.2877)  data: 2.0897 (2.0897 -- 2.0897)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5838 (0.5383)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4124 (0.2006 -- 2.2877)  data: 0.1989 (0.0005 -- 2.0897)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5474 (0.5532)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2274 (0.1693 -- 0.3900)  data: 0.0227 (0.0001 -- 0.2140)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6289 (0.6100)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.2113 (0.1332 -- 0.3900)  data: 0.0223 (0.0001 -- 0.2140)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.614
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 85.48%
Epoch: [169]  [  0/160]  eta: 0:22:17  lr: 0.000022  min_lr: 0.000006  loss: 1.5149 (1.5149)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6144 (4.6144)  time: 8.3603 (8.3603 -- 8.3603)  data: 7.8290 (7.8290 -- 7.8290)  max mem: 16413
[2023-09-05 04:37:20,536] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27058
[2023-09-05 04:37:20,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:37:20,536] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27058
[2023-09-05 04:37:20,537] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:37:20,537] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [169]  [ 20/160]  eta: 0:02:43  lr: 0.000022  min_lr: 0.000006  loss: 1.7021 (1.6992)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5137 (6.7675)  time: 0.8061 (0.5109 -- 3.8049)  data: 0.2589 (0.0005 -- 3.2650)  max mem: 16413
Epoch: [169]  [ 40/160]  eta: 0:02:04  lr: 0.000022  min_lr: 0.000006  loss: 1.4209 (1.5772)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4776 (6.6278)  time: 0.9023 (0.5271 -- 3.1003)  data: 0.3487 (0.0003 -- 2.5307)  max mem: 16413
Epoch: [169]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000006  loss: 1.7730 (1.6380)  loss_scale: 16384.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6113 (6.6534)  time: 0.8622 (0.5226 -- 2.4896)  data: 0.3157 (0.0004 -- 1.9651)  max mem: 16413
Epoch: [169]  [ 80/160]  eta: 0:01:17  lr: 0.000022  min_lr: 0.000006  loss: 1.5667 (1.6400)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4113 (6.7497)  time: 0.9504 (0.5267 -- 3.0543)  data: 0.4024 (0.0002 -- 2.5125)  max mem: 16413
Epoch: [169]  [100/160]  eta: 0:00:57  lr: 0.000022  min_lr: 0.000006  loss: 1.6032 (1.6363)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3117 (6.7939)  time: 0.9211 (0.5120 -- 3.9435)  data: 0.3708 (0.0006 -- 3.3720)  max mem: 16413
Epoch: [169]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000006  loss: 1.6235 (1.6442)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5122 (6.8301)  time: 0.6583 (0.5237 -- 1.7234)  data: 0.1111 (0.0002 -- 1.1973)  max mem: 16413
Epoch: [169]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000006  loss: 1.5077 (1.6404)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1162 (6.8137)  time: 1.0604 (0.5293 -- 5.3394)  data: 0.5064 (0.0006 -- 4.8031)  max mem: 16413
[2023-09-05 04:39:15,245] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:39:15,245] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:39:15,245] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:39:15,245] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [169]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.6046 (1.6404)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3678 (6.8158)  time: 0.6413 (0.4962 -- 2.9925)  data: 0.1244 (0.0002 -- 2.4752)  max mem: 16413
Epoch: [169] Total time: 0:02:23 (0.8994 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.6046 (1.6170)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3678 (6.8158)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.1298 (0.1298)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0842 (2.0842 -- 2.0842)  data: 1.8516 (1.8516 -- 1.8516)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5598 (0.5505)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4296 (0.2045 -- 2.0842)  data: 0.1997 (0.0005 -- 1.8516)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5598 (0.5566)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2359 (0.1712 -- 0.4692)  data: 0.0235 (0.0001 -- 0.2127)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5634 (0.5989)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (97.5104)  time: 0.2095 (0.1331 -- 0.4692)  data: 0.0173 (0.0001 -- 0.2127)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 84.232 Acc@5 97.095 loss 0.591
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.48%
Epoch: [170]  [  0/160]  eta: 0:17:31  lr: 0.000022  min_lr: 0.000006  loss: 1.7413 (1.7413)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1051 (9.1051)  time: 6.5734 (6.5734 -- 6.5734)  data: 5.4796 (5.4796 -- 5.4796)  max mem: 16413
Epoch: [170]  [ 20/160]  eta: 0:02:33  lr: 0.000022  min_lr: 0.000006  loss: 1.4596 (1.5324)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2341 (7.2010)  time: 0.8189 (0.5295 -- 2.7200)  data: 0.2045 (0.0008 -- 2.1949)  max mem: 16413
Epoch: [170]  [ 40/160]  eta: 0:02:06  lr: 0.000022  min_lr: 0.000006  loss: 1.2722 (1.4626)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2794 (6.7424)  time: 1.0071 (0.5341 -- 3.3485)  data: 0.1851 (0.0004 -- 1.1774)  max mem: 16413
[2023-09-05 04:40:20,923] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27250
[2023-09-05 04:40:20,923] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27250
[2023-09-05 04:40:20,923] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:40:20,923] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:40:20,923] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [170]  [ 60/160]  eta: 0:01:40  lr: 0.000022  min_lr: 0.000006  loss: 1.7438 (1.5611)  loss_scale: 16384.0000 (29813.5082)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7334 (7.1075)  time: 0.9004 (0.5185 -- 3.6953)  data: 0.0961 (0.0002 -- 1.0603)  max mem: 16413
Epoch: [170]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000006  loss: 1.6237 (1.5982)  loss_scale: 16384.0000 (26497.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9664 (7.0655)  time: 0.7877 (0.5256 -- 2.4378)  data: 0.0012 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [170]  [100/160]  eta: 0:00:57  lr: 0.000022  min_lr: 0.000006  loss: 1.5640 (1.5969)  loss_scale: 16384.0000 (24494.8911)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4414 (7.0507)  time: 0.9850 (0.5242 -- 3.9794)  data: 0.0012 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [170]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000006  loss: 1.5464 (1.5968)  loss_scale: 16384.0000 (23154.2479)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4386 (7.0667)  time: 0.7237 (0.5283 -- 3.0189)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [170]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000006  loss: 1.5528 (1.5983)  loss_scale: 16384.0000 (22193.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4425 (7.1393)  time: 0.9512 (0.5298 -- 4.3213)  data: 0.0012 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [170]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000006  loss: 1.5720 (1.5999)  loss_scale: 16384.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6875 (7.0983)  time: 0.6225 (0.4973 -- 1.7570)  data: 0.0007 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [170] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000006  loss: 1.5720 (1.6011)  loss_scale: 16384.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6875 (7.0983)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1454 (0.1454)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3017 (2.3017 -- 2.3017)  data: 2.0932 (2.0932 -- 2.0932)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5890 (0.5552)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4208 (0.2055 -- 2.3017)  data: 0.2048 (0.0004 -- 2.0932)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5593 (0.5569)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2203 (0.1727 -- 0.3759)  data: 0.0146 (0.0001 -- 0.1502)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5952 (0.6079)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (97.9253)  time: 0.2045 (0.1330 -- 0.3759)  data: 0.0143 (0.0001 -- 0.1502)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.597
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.48%
Epoch: [171]  [  0/160]  eta: 0:25:12  lr: 0.000022  min_lr: 0.000006  loss: 1.6521 (1.6521)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5705 (8.5705)  time: 9.4500 (9.4500 -- 9.4500)  data: 8.9068 (8.9068 -- 8.9068)  max mem: 16413
[2023-09-05 04:42:23,415] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:42:23,415] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:42:23,416] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:42:23,416] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [171]  [ 20/160]  eta: 0:02:46  lr: 0.000022  min_lr: 0.000006  loss: 1.5955 (1.5788)  loss_scale: 16384.0000 (17944.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3762 (6.5337)  time: 0.7769 (0.5120 -- 4.3397)  data: 0.2312 (0.0002 -- 3.7617)  max mem: 16413
Epoch: [171]  [ 40/160]  eta: 0:01:59  lr: 0.000022  min_lr: 0.000006  loss: 1.7466 (1.6236)  loss_scale: 32768.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2052 (6.5560)  time: 0.7992 (0.5403 -- 2.4900)  data: 0.1170 (0.0002 -- 1.3823)  max mem: 16413
Epoch: [171]  [ 60/160]  eta: 0:01:40  lr: 0.000022  min_lr: 0.000006  loss: 1.3626 (1.5796)  loss_scale: 32768.0000 (27664.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3542 (6.8513)  time: 1.0051 (0.5209 -- 3.5714)  data: 0.3058 (0.0003 -- 3.0254)  max mem: 16413
Epoch: [171]  [ 80/160]  eta: 0:01:17  lr: 0.000022  min_lr: 0.000006  loss: 1.5981 (1.5816)  loss_scale: 32768.0000 (28924.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0505 (6.8007)  time: 0.8599 (0.5135 -- 4.2351)  data: 0.3186 (0.0003 -- 3.7225)  max mem: 16413
Epoch: [171]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000006  loss: 1.4591 (1.5861)  loss_scale: 32768.0000 (29685.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2298 (6.8349)  time: 0.8630 (0.5334 -- 3.7008)  data: 0.3083 (0.0002 -- 3.1639)  max mem: 16413
Epoch: [171]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000005  loss: 1.6280 (1.6009)  loss_scale: 32768.0000 (30195.3058)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1594 (6.8828)  time: 0.7868 (0.5440 -- 3.6290)  data: 0.2260 (0.0007 -- 3.0536)  max mem: 16413
Epoch: [171]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000005  loss: 1.4861 (1.5878)  loss_scale: 32768.0000 (30560.2270)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5675 (6.9381)  time: 1.0019 (0.5335 -- 4.7464)  data: 0.4595 (0.0005 -- 4.2281)  max mem: 16413
[2023-09-05 04:44:14,387] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:44:14,388] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 04:44:14,389] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:44:14,389] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 04:44:19,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27515
[2023-09-05 04:44:19,699] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27515
[2023-09-05 04:44:19,699] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 04:44:19,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 04:44:19,700] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [171]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000005  loss: 1.6248 (1.5835)  loss_scale: 32768.0000 (32460.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1738 (6.9216)  time: 0.7445 (0.4861 -- 3.4849)  data: 0.2211 (0.0002 -- 2.9500)  max mem: 16413
Epoch: [171] Total time: 0:02:22 (0.8921 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000005  loss: 1.6248 (1.5941)  loss_scale: 32768.0000 (32460.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1738 (6.9216)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1448 (0.1448)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5293 (2.5293 -- 2.5293)  data: 2.3179 (2.3179 -- 2.3179)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5479 (0.5365)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4286 (0.2059 -- 2.5293)  data: 0.2116 (0.0006 -- 2.3179)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5033 (0.5235)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2161 (0.1687 -- 0.3703)  data: 0.0095 (0.0001 -- 0.1774)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5406 (0.5874)  acc1: 77.7778 (84.2324)  acc5: 100.0000 (97.9253)  time: 0.1987 (0.1326 -- 0.3703)  data: 0.0093 (0.0001 -- 0.1774)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 84.232 Acc@5 97.718 loss 0.599
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.48%
Epoch: [172]  [  0/160]  eta: 0:24:50  lr: 0.000022  min_lr: 0.000005  loss: 1.8842 (1.8842)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4639 (6.4639)  time: 9.3135 (9.3135 -- 9.3135)  data: 8.7925 (8.7925 -- 8.7925)  max mem: 16413
Epoch: [172]  [ 20/160]  eta: 0:02:58  lr: 0.000022  min_lr: 0.000005  loss: 1.6830 (1.6302)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3702 (6.8589)  time: 0.8753 (0.5126 -- 4.4764)  data: 0.1230 (0.0003 -- 1.5313)  max mem: 16413
Epoch: [172]  [ 40/160]  eta: 0:02:14  lr: 0.000021  min_lr: 0.000005  loss: 1.6409 (1.6155)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7813 (6.8585)  time: 0.9590 (0.5191 -- 4.9644)  data: 0.0009 (0.0001 -- 0.0025)  max mem: 16413
[2023-09-05 04:45:24,497] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27572
[2023-09-05 04:45:24,497] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27572
[2023-09-05 04:45:24,498] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:45:24,498] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:45:24,498] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [172]  [ 60/160]  eta: 0:01:41  lr: 0.000021  min_lr: 0.000005  loss: 1.6835 (1.6159)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0947 (6.8581)  time: 0.7947 (0.5333 -- 3.0248)  data: 0.0014 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [172]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000005  loss: 1.7745 (1.6374)  loss_scale: 16384.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9270 (7.0771)  time: 0.7973 (0.5257 -- 3.3012)  data: 0.1410 (0.0004 -- 2.7845)  max mem: 16413
Epoch: [172]  [100/160]  eta: 0:00:57  lr: 0.000021  min_lr: 0.000005  loss: 1.6606 (1.6434)  loss_scale: 16384.0000 (24819.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3524 (6.9592)  time: 0.9447 (0.5267 -- 2.6112)  data: 0.2653 (0.0001 -- 2.0755)  max mem: 16413
Epoch: [172]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000005  loss: 1.7669 (1.6686)  loss_scale: 16384.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3506 (7.0733)  time: 0.8951 (0.5224 -- 3.9203)  data: 0.2586 (0.0002 -- 3.3524)  max mem: 16413
Epoch: [172]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.2980 (1.6456)  loss_scale: 16384.0000 (22426.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6771 (7.1089)  time: 0.8910 (0.5190 -- 3.1514)  data: 0.3438 (0.0002 -- 2.6360)  max mem: 16413
Epoch: [172]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.5900 (1.6408)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6335 (7.2194)  time: 0.6547 (0.4956 -- 1.8966)  data: 0.1342 (0.0003 -- 1.3856)  max mem: 16413
Epoch: [172] Total time: 0:02:25 (0.9064 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.5900 (1.6162)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6335 (7.2194)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1269 (0.1269)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2683 (2.2683 -- 2.2683)  data: 2.0299 (2.0299 -- 2.0299)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5963 (0.5142)  acc1: 100.0000 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4090 (0.1963 -- 2.2683)  data: 0.1975 (0.0006 -- 2.0299)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4656 (0.4993)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2264 (0.1698 -- 0.4528)  data: 0.0259 (0.0001 -- 0.2337)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5206 (0.5478)  acc1: 77.7778 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2114 (0.1327 -- 0.4528)  data: 0.0249 (0.0001 -- 0.2337)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 84.647 Acc@5 98.133 loss 0.581
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.48%
Epoch: [173]  [  0/160]  eta: 0:19:49  lr: 0.000021  min_lr: 0.000005  loss: 1.9338 (1.9338)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5795 (5.5795)  time: 7.4355 (7.4355 -- 7.4355)  data: 5.7884 (5.7884 -- 5.7884)  max mem: 16413
Epoch: [173]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000005  loss: 1.7949 (1.7316)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6870 (6.7510)  time: 0.9027 (0.5309 -- 2.8808)  data: 0.0015 (0.0006 -- 0.0030)  max mem: 16413
[2023-09-05 04:47:28,362] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:47:28,362] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:47:28,366] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:47:28,366] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [173]  [ 40/160]  eta: 0:02:10  lr: 0.000021  min_lr: 0.000005  loss: 1.6075 (1.6765)  loss_scale: 32768.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1698 (6.8595)  time: 0.9624 (0.5242 -- 3.9943)  data: 0.0188 (0.0004 -- 0.3453)  max mem: 16413
Epoch: [173]  [ 60/160]  eta: 0:01:39  lr: 0.000021  min_lr: 0.000005  loss: 1.3768 (1.6151)  loss_scale: 32768.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1103 (6.8214)  time: 0.8088 (0.5311 -- 3.7758)  data: 0.0013 (0.0005 -- 0.0024)  max mem: 16413
Epoch: [173]  [ 80/160]  eta: 0:01:17  lr: 0.000021  min_lr: 0.000005  loss: 1.4123 (1.5952)  loss_scale: 32768.0000 (28520.2963)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6922 (6.5339)  time: 0.8929 (0.5258 -- 3.5751)  data: 0.0050 (0.0003 -- 0.0628)  max mem: 16413
Epoch: [173]  [100/160]  eta: 0:00:58  lr: 0.000021  min_lr: 0.000005  loss: 1.6213 (1.5975)  loss_scale: 32768.0000 (29361.4257)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1648 (6.8111)  time: 0.9634 (0.4993 -- 4.8832)  data: 0.0009 (0.0003 -- 0.0017)  max mem: 16413
Epoch: [173]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000005  loss: 1.6204 (1.5950)  loss_scale: 32768.0000 (29924.4959)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7903 (6.9184)  time: 0.7576 (0.5287 -- 3.3150)  data: 0.0014 (0.0004 -- 0.0026)  max mem: 16413
[2023-09-05 04:48:56,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27801
[2023-09-05 04:48:56,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27801
[2023-09-05 04:48:56,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:48:56,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:48:56,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [173]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.8153 (1.6148)  loss_scale: 16384.0000 (28003.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8802 (6.8693)  time: 0.8867 (0.5155 -- 4.0205)  data: 0.0014 (0.0005 -- 0.0027)  max mem: 16413
Epoch: [173]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.6214 (1.6154)  loss_scale: 16384.0000 (26624.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4566 (6.8807)  time: 0.6564 (0.4968 -- 3.1590)  data: 0.0009 (0.0002 -- 0.0042)  max mem: 16413
Epoch: [173] Total time: 0:02:23 (0.8970 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.6214 (1.6197)  loss_scale: 16384.0000 (26624.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4566 (6.8807)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1359 (0.1359)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3992 (2.3992 -- 2.3992)  data: 2.1382 (2.1382 -- 2.1382)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5762 (0.4978)  acc1: 100.0000 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4379 (0.1999 -- 2.3992)  data: 0.2118 (0.0007 -- 2.1382)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4990 (0.5279)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2185 (0.1700 -- 0.4152)  data: 0.0098 (0.0001 -- 0.1823)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5350 (0.5796)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.1994 (0.1334 -- 0.4152)  data: 0.0095 (0.0001 -- 0.1823)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 86.100 Acc@5 98.133 loss 0.579
Accuracy of the network on the 482 val images: 86.10%
[2023-09-05 04:49:33,643] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 04:49:33,645] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 04:49:33,645] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 04:49:33,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 04:49:35,065] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 04:49:35,066] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.10%
Epoch: [174]  [  0/160]  eta: 0:17:40  lr: 0.000021  min_lr: 0.000005  loss: 1.4527 (1.4527)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1194 (7.1194)  time: 6.6294 (6.6294 -- 6.6294)  data: 5.3102 (5.3102 -- 5.3102)  max mem: 16413
Epoch: [174]  [ 20/160]  eta: 0:02:40  lr: 0.000021  min_lr: 0.000005  loss: 1.5729 (1.5642)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9092 (7.0194)  time: 0.8730 (0.5435 -- 2.2065)  data: 0.0809 (0.0002 -- 0.8903)  max mem: 16413
Epoch: [174]  [ 40/160]  eta: 0:02:02  lr: 0.000021  min_lr: 0.000005  loss: 1.6645 (1.6143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7188 (6.9486)  time: 0.8836 (0.5178 -- 2.5369)  data: 0.1959 (0.0003 -- 2.0190)  max mem: 16413
Epoch: [174]  [ 60/160]  eta: 0:01:40  lr: 0.000021  min_lr: 0.000005  loss: 1.5995 (1.5986)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0475 (6.9113)  time: 0.9847 (0.5154 -- 3.8206)  data: 0.4192 (0.0003 -- 3.3107)  max mem: 16413
Epoch: [174]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000005  loss: 1.4657 (1.5724)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3927 (7.1964)  time: 0.7792 (0.5150 -- 3.4396)  data: 0.1710 (0.0001 -- 2.9137)  max mem: 16413
[2023-09-05 04:51:01,451] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:51:01,452] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:51:01,456] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:51:01,457] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:51:07,004] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27935
[2023-09-05 04:51:07,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:51:07,004] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27935
[2023-09-05 04:51:07,004] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 04:51:07,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [174]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000005  loss: 1.4004 (1.5520)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8092 (7.2328)  time: 0.9354 (0.5147 -- 4.4423)  data: 0.1475 (0.0003 -- 1.9883)  max mem: 16413
Epoch: [174]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000005  loss: 1.5968 (1.5624)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9533 (7.3054)  time: 0.8469 (0.5265 -- 3.9698)  data: 0.2975 (0.0004 -- 3.4129)  max mem: 16413
Epoch: [174]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.6821 (1.5821)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1604 (7.2546)  time: 0.8659 (0.5209 -- 3.8105)  data: 0.2882 (0.0003 -- 3.2999)  max mem: 16413
[2023-09-05 04:51:57,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=157, lr=[5.272953732267125e-06, 5.272953732267125e-06, 5.858837480296806e-06, 5.858837480296806e-06, 6.509819422552006e-06, 6.509819422552006e-06, 7.2331326917244515e-06, 7.2331326917244515e-06, 8.036814101916057e-06, 8.036814101916057e-06, 8.929793446573396e-06, 8.929793446573396e-06, 9.921992718414884e-06, 9.921992718414884e-06, 1.1024436353794316e-05, 1.1024436353794316e-05, 1.2249373726438128e-05, 1.2249373726438128e-05, 1.3610415251597919e-05, 1.3610415251597919e-05, 1.5122683612886578e-05, 1.5122683612886578e-05, 1.6802981792096196e-05, 1.6802981792096196e-05, 1.8669979768995773e-05, 1.8669979768995773e-05, 2.074442196555086e-05, 2.074442196555086e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 04:51:57,950] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=17.64852988676937, CurrSamplesPerSec=24.647921865756718, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [174]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000005  loss: 1.5449 (1.5799)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6573 (7.1642)  time: 0.6701 (0.4962 -- 3.2987)  data: 0.1431 (0.0003 -- 2.7953)  max mem: 16413
Epoch: [174] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000005  loss: 1.5449 (1.5868)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6573 (7.1642)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1341 (0.1341)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3512 (2.3512 -- 2.3512)  data: 2.1032 (2.1032 -- 2.1032)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5426 (0.5391)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4384 (0.1988 -- 2.3512)  data: 0.2147 (0.0005 -- 2.1032)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4806 (0.5418)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2261 (0.1691 -- 0.4986)  data: 0.0157 (0.0001 -- 0.2311)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4812 (0.5851)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2103 (0.1336 -- 0.4986)  data: 0.0154 (0.0001 -- 0.2311)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 85.270 Acc@5 97.718 loss 0.586
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.10%
Epoch: [175]  [  0/160]  eta: 0:17:34  lr: 0.000021  min_lr: 0.000005  loss: 1.4295 (1.4295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2013 (5.2013)  time: 6.5909 (6.5909 -- 6.5909)  data: 5.7017 (5.7017 -- 5.7017)  max mem: 16413
Epoch: [175]  [ 20/160]  eta: 0:02:36  lr: 0.000021  min_lr: 0.000005  loss: 1.4360 (1.5378)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7910 (7.2820)  time: 0.8410 (0.5309 -- 2.8291)  data: 0.1692 (0.0008 -- 2.3076)  max mem: 16413
Epoch: [175]  [ 40/160]  eta: 0:02:00  lr: 0.000021  min_lr: 0.000005  loss: 1.5383 (1.5264)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5348 (7.4448)  time: 0.8851 (0.5390 -- 3.6217)  data: 0.1697 (0.0008 -- 2.2854)  max mem: 16413
Epoch: [175]  [ 60/160]  eta: 0:01:40  lr: 0.000021  min_lr: 0.000005  loss: 1.5660 (1.5505)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7004 (7.3767)  time: 0.9953 (0.5180 -- 3.9706)  data: 0.4191 (0.0003 -- 3.4524)  max mem: 16413
[2023-09-05 04:53:09,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:53:09,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:53:09,078] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:53:09,079] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:53:14,801] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28070
[2023-09-05 04:53:14,801] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28070
[2023-09-05 04:53:14,802] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:53:14,802] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:53:14,802] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [175]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000005  loss: 1.7405 (1.5741)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7140 (7.3768)  time: 0.8269 (0.5293 -- 3.5284)  data: 0.2750 (0.0003 -- 3.0122)  max mem: 16413
Epoch: [175]  [100/160]  eta: 0:00:58  lr: 0.000021  min_lr: 0.000005  loss: 1.5657 (1.5702)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7629 (7.2196)  time: 1.0319 (0.5323 -- 4.7713)  data: 0.4844 (0.0006 -- 4.2596)  max mem: 16413
Epoch: [175]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000005  loss: 1.5034 (1.5631)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0427 (7.1524)  time: 0.6508 (0.5268 -- 2.0035)  data: 0.1004 (0.0001 -- 1.4591)  max mem: 16413
Epoch: [175]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000005  loss: 1.3918 (1.5318)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9568 (7.0266)  time: 0.9562 (0.5246 -- 4.1215)  data: 0.4026 (0.0005 -- 3.5846)  max mem: 16413
Epoch: [175]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.6116 (1.5466)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9522 (7.0406)  time: 0.7002 (0.4974 -- 3.3113)  data: 0.1791 (0.0002 -- 2.7879)  max mem: 16413
Epoch: [175] Total time: 0:02:23 (0.8988 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.6116 (1.5824)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9522 (7.0406)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1198 (0.1198)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3120 (2.3120 -- 2.3120)  data: 2.0685 (2.0685 -- 2.0685)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5341 (0.5255)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4168 (0.2017 -- 2.3120)  data: 0.1988 (0.0007 -- 2.0685)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5047 (0.5245)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2292 (0.1710 -- 0.5689)  data: 0.0251 (0.0001 -- 0.3805)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5047 (0.5738)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2126 (0.1335 -- 0.5689)  data: 0.0248 (0.0001 -- 0.3805)  max mem: 16413
Val: Total time: 0:00:07 (0.2921 s / it)
* Acc@1 85.270 Acc@5 97.510 loss 0.578
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.10%
Epoch: [176]  [  0/160]  eta: 0:15:25  lr: 0.000020  min_lr: 0.000005  loss: 2.0810 (2.0810)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3371 (4.3371)  time: 5.7826 (5.7826 -- 5.7826)  data: 5.1283 (5.1283 -- 5.1283)  max mem: 16413
Epoch: [176]  [ 20/160]  eta: 0:02:50  lr: 0.000020  min_lr: 0.000005  loss: 1.7148 (1.7583)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5585 (6.8491)  time: 0.9887 (0.5304 -- 3.5306)  data: 0.4365 (0.0007 -- 3.0080)  max mem: 16413
[2023-09-05 04:55:19,376] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:55:19,376] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:55:19,382] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:55:19,383] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [176]  [ 40/160]  eta: 0:02:04  lr: 0.000020  min_lr: 0.000005  loss: 1.5065 (1.6308)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1453 (7.0739)  time: 0.8425 (0.5393 -- 3.5649)  data: 0.2918 (0.0004 -- 3.0433)  max mem: 16413
[2023-09-05 04:55:28,273] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28209
[2023-09-05 04:55:28,273] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:55:28,273] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28209
[2023-09-05 04:55:28,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 04:55:28,274] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [176]  [ 60/160]  eta: 0:01:40  lr: 0.000020  min_lr: 0.000005  loss: 1.6186 (1.6158)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4730 (7.0616)  time: 0.9315 (0.5317 -- 2.8561)  data: 0.3858 (0.0005 -- 2.3366)  max mem: 16413
Epoch: [176]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000005  loss: 1.7498 (1.6303)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3327 (7.1869)  time: 0.7257 (0.5284 -- 2.0680)  data: 0.1342 (0.0004 -- 1.5275)  max mem: 16413
Epoch: [176]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000005  loss: 1.6666 (1.6446)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4464 (7.0611)  time: 0.8940 (0.5312 -- 3.4241)  data: 0.1055 (0.0005 -- 1.3971)  max mem: 16413
Epoch: [176]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000005  loss: 1.5581 (1.6480)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6043 (7.0894)  time: 0.9882 (0.5203 -- 3.8950)  data: 0.0013 (0.0004 -- 0.0033)  max mem: 16413
Epoch: [176]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.4586 (1.6309)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9160 (7.1486)  time: 0.7416 (0.5315 -- 2.4170)  data: 0.0019 (0.0008 -- 0.0042)  max mem: 16413
Epoch: [176]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.7848 (1.6350)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3321 (7.1667)  time: 0.7917 (0.4941 -- 5.2055)  data: 0.0007 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [176] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.7848 (1.6013)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3321 (7.1667)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1325 (0.1325)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2390 (2.2390 -- 2.2390)  data: 2.0183 (2.0183 -- 2.0183)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5456 (0.4891)  acc1: 100.0000 (90.9091)  acc5: 100.0000 (100.0000)  time: 0.4214 (0.1952 -- 2.2390)  data: 0.2068 (0.0006 -- 2.0183)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5032 (0.5169)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2268 (0.1692 -- 0.4487)  data: 0.0187 (0.0001 -- 0.2384)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5032 (0.5682)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2110 (0.1335 -- 0.4487)  data: 0.0180 (0.0001 -- 0.2384)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 86.307 Acc@5 97.510 loss 0.588
Accuracy of the network on the 482 val images: 86.31%
[2023-09-05 04:57:08,631] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 04:57:08,633] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 04:57:08,633] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 04:57:08,633] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 04:57:10,095] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 04:57:10,096] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.31%
Epoch: [177]  [  0/160]  eta: 0:23:57  lr: 0.000020  min_lr: 0.000005  loss: 1.5433 (1.5433)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3735 (6.3735)  time: 8.9847 (8.9847 -- 8.9847)  data: 7.6465 (7.6465 -- 7.6465)  max mem: 16413
[2023-09-05 04:57:34,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:57:34,753] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:57:34,757] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:57:34,757] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [177]  [ 20/160]  eta: 0:02:51  lr: 0.000020  min_lr: 0.000005  loss: 1.6169 (1.5424)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1747 (6.8151)  time: 0.8391 (0.5109 -- 4.4363)  data: 0.2911 (0.0004 -- 3.9095)  max mem: 16413
Epoch: [177]  [ 40/160]  eta: 0:02:05  lr: 0.000020  min_lr: 0.000005  loss: 1.7106 (1.6041)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6881 (7.0082)  time: 0.8629 (0.5223 -- 3.4039)  data: 0.2600 (0.0004 -- 2.5938)  max mem: 16413
[2023-09-05 04:57:56,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28366
[2023-09-05 04:57:56,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:57:56,505] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28366
[2023-09-05 04:57:56,505] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 04:57:56,505] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [177]  [ 60/160]  eta: 0:01:37  lr: 0.000020  min_lr: 0.000005  loss: 1.7162 (1.6387)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2797 (6.8467)  time: 0.8324 (0.5137 -- 4.3745)  data: 0.0348 (0.0004 -- 0.6646)  max mem: 16413
Epoch: [177]  [ 80/160]  eta: 0:01:17  lr: 0.000020  min_lr: 0.000005  loss: 1.3086 (1.5801)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9323 (6.9801)  time: 0.9416 (0.5266 -- 3.8459)  data: 0.0153 (0.0002 -- 0.2842)  max mem: 16413
Epoch: [177]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000005  loss: 1.7600 (1.6065)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8955 (6.9355)  time: 0.8387 (0.5401 -- 4.0182)  data: 0.0016 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [177]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000005  loss: 1.4892 (1.5974)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4218 (6.9114)  time: 0.9600 (0.5295 -- 3.8233)  data: 0.0022 (0.0003 -- 0.0103)  max mem: 16413
Epoch: [177]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.5439 (1.5977)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8743 (6.9490)  time: 0.7111 (0.5397 -- 2.1418)  data: 0.0018 (0.0002 -- 0.0069)  max mem: 16413
Epoch: [177]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.6866 (1.5989)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2724 (7.0187)  time: 0.7083 (0.4955 -- 3.5179)  data: 0.0014 (0.0002 -- 0.0077)  max mem: 16413
Epoch: [177] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.6866 (1.5916)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2724 (7.0187)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1289 (0.1289)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3924 (2.3924 -- 2.3924)  data: 2.1904 (2.1904 -- 2.1904)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5196 (0.5038)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.9899)  time: 0.4222 (0.2051 -- 2.3924)  data: 0.2005 (0.0005 -- 2.1904)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5196 (0.5290)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (97.8836)  time: 0.2163 (0.1702 -- 0.2428)  data: 0.0032 (0.0001 -- 0.0443)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5284 (0.5660)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.1984 (0.1333 -- 0.2428)  data: 0.0027 (0.0001 -- 0.0443)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 85.685 Acc@5 97.510 loss 0.578
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.31%
Epoch: [178]  [  0/160]  eta: 0:18:52  lr: 0.000020  min_lr: 0.000005  loss: 2.1868 (2.1868)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4001 (6.4001)  time: 7.0797 (7.0797 -- 7.0797)  data: 6.5172 (6.5172 -- 6.5172)  max mem: 16413
[2023-09-05 04:59:57,391] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:59:57,391] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 04:59:57,391] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 04:59:57,392] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:00:03,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28500
[2023-09-05 05:00:03,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28500
[2023-09-05 05:00:03,932] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:00:03,932] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:00:03,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [178]  [ 20/160]  eta: 0:02:41  lr: 0.000020  min_lr: 0.000005  loss: 1.5710 (1.6475)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1206 (6.4763)  time: 0.8577 (0.5412 -- 3.6338)  data: 0.2335 (0.0006 -- 3.0814)  max mem: 16413
Epoch: [178]  [ 40/160]  eta: 0:02:08  lr: 0.000020  min_lr: 0.000005  loss: 1.7006 (1.6433)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0419 (6.7031)  time: 0.9920 (0.5152 -- 3.9480)  data: 0.4342 (0.0007 -- 3.3906)  max mem: 16413
Epoch: [178]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000005  loss: 1.6568 (1.6122)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6897 (6.7299)  time: 0.7281 (0.5250 -- 3.1633)  data: 0.1764 (0.0004 -- 2.6368)  max mem: 16413
Epoch: [178]  [ 80/160]  eta: 0:01:16  lr: 0.000020  min_lr: 0.000005  loss: 1.6280 (1.6110)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5271 (6.8922)  time: 0.9232 (0.5329 -- 3.4305)  data: 0.3723 (0.0003 -- 2.8751)  max mem: 16413
Epoch: [178]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000005  loss: 1.5062 (1.5984)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6312 (6.8592)  time: 0.8247 (0.5308 -- 4.9412)  data: 0.2726 (0.0005 -- 4.4208)  max mem: 16413
Epoch: [178]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000005  loss: 1.6702 (1.6120)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8249 (6.9460)  time: 1.0013 (0.5387 -- 4.1225)  data: 0.3984 (0.0002 -- 3.0288)  max mem: 16413
Epoch: [178]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000005  loss: 1.7160 (1.6183)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7898 (7.0599)  time: 0.8967 (0.5191 -- 4.3326)  data: 0.3553 (0.0003 -- 3.8036)  max mem: 16413
[2023-09-05 05:01:57,661] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:01:57,661] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:01:57,661] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:01:57,661] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [178]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000005  loss: 1.7134 (1.6297)  loss_scale: 32768.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1311 (7.2158)  time: 0.6085 (0.4961 -- 2.1010)  data: 0.0925 (0.0002 -- 1.5770)  max mem: 16413
Epoch: [178] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000005  loss: 1.7134 (1.6123)  loss_scale: 32768.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1311 (7.2158)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1286 (0.1286)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6148 (2.6148 -- 2.6148)  data: 2.4062 (2.4062 -- 2.4062)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5916 (0.5546)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4352 (0.1993 -- 2.6148)  data: 0.2199 (0.0008 -- 2.4062)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4862 (0.5389)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2062 (0.1695 -- 0.2442)  data: 0.0010 (0.0001 -- 0.0048)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5142 (0.5828)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.3402)  time: 0.1909 (0.1324 -- 0.2442)  data: 0.0007 (0.0001 -- 0.0048)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.593
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [179]  [  0/160]  eta: 0:17:28  lr: 0.000020  min_lr: 0.000005  loss: 1.0468 (1.0468)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6338 (5.6338)  time: 6.5550 (6.5550 -- 6.5550)  data: 5.3663 (5.3663 -- 5.3663)  max mem: 16413
Epoch: [179]  [ 20/160]  eta: 0:02:48  lr: 0.000020  min_lr: 0.000005  loss: 1.6543 (1.6181)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9814 (7.1839)  time: 0.9351 (0.5343 -- 2.9189)  data: 0.3243 (0.0004 -- 2.3688)  max mem: 16413
Epoch: [179]  [ 40/160]  eta: 0:02:06  lr: 0.000020  min_lr: 0.000005  loss: 1.4117 (1.5885)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9796 (6.6496)  time: 0.9031 (0.5299 -- 4.1857)  data: 0.3429 (0.0002 -- 3.6467)  max mem: 16413
Epoch: [179]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000005  loss: 1.5020 (1.6018)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7853 (6.7786)  time: 0.7811 (0.5330 -- 1.7656)  data: 0.2265 (0.0005 -- 1.2275)  max mem: 16413
Epoch: [179]  [ 80/160]  eta: 0:01:17  lr: 0.000020  min_lr: 0.000005  loss: 1.6748 (1.6021)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9429 (6.9643)  time: 0.9724 (0.5175 -- 2.9840)  data: 0.4199 (0.0003 -- 2.4470)  max mem: 16413
Epoch: [179]  [100/160]  eta: 0:00:57  lr: 0.000019  min_lr: 0.000005  loss: 1.6759 (1.6102)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7200 (7.0107)  time: 0.9134 (0.5075 -- 3.6580)  data: 0.3791 (0.0003 -- 3.1413)  max mem: 16413
[2023-09-05 05:04:02,720] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:04:02,720] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:04:02,721] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 05:04:02,721] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [179]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000005  loss: 1.7840 (1.6283)  loss_scale: 32768.0000 (33851.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3925 (7.0242)  time: 0.8489 (0.5235 -- 3.6551)  data: 0.3019 (0.0002 -- 3.1355)  max mem: 16413
Epoch: [179]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.3613 (1.5974)  loss_scale: 65536.0000 (38345.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0311 (6.9793)  time: 1.0030 (0.5094 -- 4.5854)  data: 0.4583 (0.0004 -- 4.0458)  max mem: 16413
[2023-09-05 05:04:31,047] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28791
[2023-09-05 05:04:31,047] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28791
[2023-09-05 05:04:31,047] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 05:04:31,047] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 05:04:31,047] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [179]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.5094 (1.5835)  loss_scale: 65536.0000 (39731.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3567 (7.0045)  time: 0.6195 (0.4835 -- 1.7990)  data: 0.1070 (0.0002 -- 1.3010)  max mem: 16413
Epoch: [179] Total time: 0:02:25 (0.9099 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.5094 (1.6072)  loss_scale: 65536.0000 (39731.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3567 (7.0045)
[2023-09-05 05:04:36,335] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-09-05 05:04:36,336] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-09-05 05:04:36,336] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt
[2023-09-05 05:04:36,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt...
[2023-09-05 05:04:37,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt.
[2023-09-05 05:04:37,351] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1336 (0.1336)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5586 (2.5586 -- 2.5586)  data: 2.3192 (2.3192 -- 2.3192)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5880 (0.5418)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4325 (0.2042 -- 2.5586)  data: 0.2120 (0.0008 -- 2.3192)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4969 (0.5340)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2161 (0.1706 -- 0.3746)  data: 0.0101 (0.0001 -- 0.1860)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5171 (0.5802)  acc1: 77.7778 (84.2324)  acc5: 100.0000 (97.9253)  time: 0.2000 (0.1331 -- 0.3746)  data: 0.0097 (0.0001 -- 0.1860)  max mem: 16413
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.600
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [180]  [  0/160]  eta: 0:23:38  lr: 0.000019  min_lr: 0.000005  loss: 1.3929 (1.3929)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2883 (8.2883)  time: 8.8658 (8.8658 -- 8.8658)  data: 5.6539 (5.6539 -- 5.6539)  max mem: 16413
Epoch: [180]  [ 20/160]  eta: 0:02:44  lr: 0.000019  min_lr: 0.000005  loss: 1.7187 (1.7084)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0326 (7.3877)  time: 0.7919 (0.5266 -- 2.4620)  data: 0.1628 (0.0005 -- 1.1686)  max mem: 16413
Epoch: [180]  [ 40/160]  eta: 0:02:13  lr: 0.000019  min_lr: 0.000005  loss: 1.6587 (1.6539)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2569 (7.2868)  time: 1.0452 (0.5229 -- 5.1489)  data: 0.4887 (0.0003 -- 4.5658)  max mem: 16413
[2023-09-05 05:05:31,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28841
[2023-09-05 05:05:31,343] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28841
[2023-09-05 05:05:31,343] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:05:31,343] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:05:31,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [180]  [ 60/160]  eta: 0:01:40  lr: 0.000019  min_lr: 0.000005  loss: 1.6072 (1.6265)  loss_scale: 16384.0000 (27396.1967)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6639 (7.2545)  time: 0.7904 (0.5073 -- 4.4226)  data: 0.2495 (0.0004 -- 3.8674)  max mem: 16413
Epoch: [180]  [ 80/160]  eta: 0:01:20  lr: 0.000019  min_lr: 0.000005  loss: 1.5359 (1.6231)  loss_scale: 16384.0000 (24677.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2920 (7.2514)  time: 0.9945 (0.5350 -- 4.2135)  data: 0.4405 (0.0003 -- 3.6749)  max mem: 16413
Epoch: [180]  [100/160]  eta: 0:00:57  lr: 0.000019  min_lr: 0.000005  loss: 1.4645 (1.5973)  loss_scale: 16384.0000 (23034.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9928 (7.3298)  time: 0.7409 (0.5166 -- 2.6891)  data: 0.1931 (0.0003 -- 2.1537)  max mem: 16413
Epoch: [180]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000005  loss: 1.4192 (1.5802)  loss_scale: 16384.0000 (21935.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1203 (7.3029)  time: 0.8169 (0.5419 -- 2.7875)  data: 0.2595 (0.0003 -- 2.2592)  max mem: 16413
Epoch: [180]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.4187 (1.5690)  loss_scale: 16384.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6541 (7.3732)  time: 0.8148 (0.5255 -- 2.7756)  data: 0.2480 (0.0003 -- 2.2439)  max mem: 16413
Epoch: [180]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.4550 (1.5678)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7580 (7.2377)  time: 0.7965 (0.4972 -- 2.8158)  data: 0.2388 (0.0002 -- 2.2613)  max mem: 16413
Epoch: [180] Total time: 0:02:24 (0.9010 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.4550 (1.5872)  loss_scale: 16384.0000 (20582.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7580 (7.2377)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1168 (0.1168)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5240 (2.5240 -- 2.5240)  data: 2.2779 (2.2779 -- 2.2779)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6332 (0.5596)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4303 (0.2023 -- 2.5240)  data: 0.2081 (0.0009 -- 2.2779)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5040 (0.5324)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2175 (0.1695 -- 0.3847)  data: 0.0110 (0.0001 -- 0.2040)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5936 (0.5866)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (97.5104)  time: 0.2001 (0.1334 -- 0.3847)  data: 0.0107 (0.0001 -- 0.2040)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 84.647 Acc@5 97.303 loss 0.602
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [181]  [  0/160]  eta: 0:20:54  lr: 0.000019  min_lr: 0.000005  loss: 2.0711 (2.0711)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2775 (9.2775)  time: 7.8426 (7.8426 -- 7.8426)  data: 6.8200 (6.8200 -- 6.8200)  max mem: 16413
[2023-09-05 05:07:33,296] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:07:33,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:07:33,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:07:33,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:07:43,381] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28979
[2023-09-05 05:07:43,381] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28979
[2023-09-05 05:07:43,381] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:07:43,381] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:07:43,381] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [181]  [ 20/160]  eta: 0:02:57  lr: 0.000019  min_lr: 0.000005  loss: 1.6356 (1.6761)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0446 (6.6064)  time: 0.9408 (0.4978 -- 5.7220)  data: 0.3876 (0.0003 -- 5.1548)  max mem: 16413
[2023-09-05 05:07:58,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=164, lr=[4.845272194028254e-06, 4.845272194028254e-06, 5.3836357711425055e-06, 5.3836357711425055e-06, 5.9818175234916715e-06, 5.9818175234916715e-06, 6.6464639149907465e-06, 6.6464639149907465e-06, 7.384959905545273e-06, 7.384959905545273e-06, 8.205511006161416e-06, 8.205511006161416e-06, 9.117234451290461e-06, 9.117234451290461e-06, 1.0130260501433844e-05, 1.0130260501433844e-05, 1.125584500159316e-05, 1.125584500159316e-05, 1.2506494446214622e-05, 1.2506494446214622e-05, 1.389610494023847e-05, 1.389610494023847e-05, 1.5440116600264967e-05, 1.5440116600264967e-05, 1.715568511140552e-05, 1.715568511140552e-05, 1.906187234600613e-05, 1.906187234600613e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 05:07:58,279] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=17.712328420697958, CurrSamplesPerSec=21.37466397478441, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [181]  [ 40/160]  eta: 0:02:03  lr: 0.000019  min_lr: 0.000005  loss: 1.5040 (1.6204)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0684 (6.5125)  time: 0.7786 (0.5288 -- 2.8907)  data: 0.2224 (0.0001 -- 2.3631)  max mem: 16413
Epoch: [181]  [ 60/160]  eta: 0:01:35  lr: 0.000019  min_lr: 0.000005  loss: 1.6689 (1.6480)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0856 (6.5577)  time: 0.8144 (0.5358 -- 3.4320)  data: 0.2620 (0.0008 -- 2.8962)  max mem: 16413
Epoch: [181]  [ 80/160]  eta: 0:01:15  lr: 0.000019  min_lr: 0.000005  loss: 1.6314 (1.6598)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1094 (6.5358)  time: 0.9178 (0.5264 -- 3.5759)  data: 0.3660 (0.0003 -- 3.0589)  max mem: 16413
Epoch: [181]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000005  loss: 1.5897 (1.6494)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6174 (6.6157)  time: 0.8671 (0.5269 -- 3.3645)  data: 0.3069 (0.0005 -- 2.8299)  max mem: 16413
Epoch: [181]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000005  loss: 1.6701 (1.6442)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4764 (6.7074)  time: 0.9253 (0.5205 -- 3.8454)  data: 0.3783 (0.0008 -- 3.3322)  max mem: 16413
Epoch: [181]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.5954 (1.6292)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2933 (6.7043)  time: 0.8071 (0.5245 -- 4.3179)  data: 0.2543 (0.0002 -- 3.7823)  max mem: 16413
[2023-09-05 05:09:32,626] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:09:32,626] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:09:32,627] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:09:32,627] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [181]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.4791 (1.6179)  loss_scale: 32768.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8163 (6.7809)  time: 0.7254 (0.4963 -- 2.6302)  data: 0.2010 (0.0002 -- 2.1200)  max mem: 16413
Epoch: [181] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.4791 (1.6154)  loss_scale: 32768.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8163 (6.7809)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1226 (0.1226)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4130 (2.4130 -- 2.4130)  data: 2.1672 (2.1672 -- 2.1672)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6282 (0.5368)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4149 (0.2074 -- 2.4130)  data: 0.1980 (0.0008 -- 2.1672)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4988 (0.5262)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2217 (0.1691 -- 0.4449)  data: 0.0170 (0.0001 -- 0.2535)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5058 (0.5790)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2056 (0.1328 -- 0.4449)  data: 0.0167 (0.0001 -- 0.2535)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.592
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.31%
Epoch: [182]  [  0/160]  eta: 0:23:05  lr: 0.000019  min_lr: 0.000005  loss: 1.2425 (1.2425)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2620 (8.2620)  time: 8.6611 (8.6611 -- 8.6611)  data: 8.1322 (8.1322 -- 8.1322)  max mem: 16413
Epoch: [182]  [ 20/160]  eta: 0:02:45  lr: 0.000019  min_lr: 0.000005  loss: 1.7345 (1.6570)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4161 (7.2629)  time: 0.8109 (0.5290 -- 3.4043)  data: 0.2633 (0.0005 -- 2.8670)  max mem: 16413
Epoch: [182]  [ 40/160]  eta: 0:02:00  lr: 0.000019  min_lr: 0.000005  loss: 1.5525 (1.6464)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6778 (6.9576)  time: 0.8181 (0.5202 -- 1.9509)  data: 0.2771 (0.0004 -- 1.4331)  max mem: 16413
Epoch: [182]  [ 60/160]  eta: 0:01:38  lr: 0.000019  min_lr: 0.000005  loss: 1.4529 (1.5803)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8997 (7.2565)  time: 0.9327 (0.5337 -- 2.3022)  data: 0.3709 (0.0007 -- 1.7365)  max mem: 16413
Epoch: [182]  [ 80/160]  eta: 0:01:15  lr: 0.000019  min_lr: 0.000005  loss: 1.6598 (1.5942)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5374 (7.2406)  time: 0.8455 (0.5340 -- 2.3343)  data: 0.2274 (0.0003 -- 1.7938)  max mem: 16413
Epoch: [182]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000005  loss: 1.5004 (1.5776)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1303 (7.1832)  time: 0.9509 (0.5342 -- 2.1327)  data: 0.3597 (0.0003 -- 1.5817)  max mem: 16413
[2023-09-05 05:11:40,172] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:11:40,172] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 05:11:40,173] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:11:40,173] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 05:11:41,764] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29239
[2023-09-05 05:11:41,764] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29239
[2023-09-05 05:11:41,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 05:11:41,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 05:11:41,765] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [182]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000005  loss: 1.5159 (1.5751)  loss_scale: 32768.0000 (33580.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6825 (7.1407)  time: 0.9280 (0.5052 -- 4.7494)  data: 0.3890 (0.0003 -- 4.2362)  max mem: 16413
Epoch: [182]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000005  loss: 1.5414 (1.5700)  loss_scale: 32768.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3829 (7.0714)  time: 0.9086 (0.5207 -- 3.2774)  data: 0.3552 (0.0003 -- 2.7691)  max mem: 16413
Epoch: [182]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000005  loss: 1.7178 (1.5966)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5999 (7.1438)  time: 0.6983 (0.4981 -- 2.3709)  data: 0.1797 (0.0002 -- 1.8699)  max mem: 16413
Epoch: [182] Total time: 0:02:24 (0.9018 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000005  loss: 1.7178 (1.5930)  loss_scale: 32768.0000 (33382.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5999 (7.1438)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1261 (0.1261)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4566 (2.4566 -- 2.4566)  data: 2.2173 (2.2173 -- 2.2173)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7007 (0.5365)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4261 (0.1945 -- 2.4566)  data: 0.2096 (0.0007 -- 2.2173)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5018 (0.5313)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2214 (0.1699 -- 0.4417)  data: 0.0178 (0.0001 -- 0.2528)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5108 (0.5847)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (98.3402)  time: 0.2074 (0.1329 -- 0.4417)  data: 0.0175 (0.0001 -- 0.2528)  max mem: 16413
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 84.025 Acc@5 97.925 loss 0.593
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 86.31%
Epoch: [183]  [  0/160]  eta: 0:19:15  lr: 0.000019  min_lr: 0.000005  loss: 1.7761 (1.7761)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0422 (7.0422)  time: 7.2234 (7.2234 -- 7.2234)  data: 6.6861 (6.6861 -- 6.6861)  max mem: 16413
[2023-09-05 05:12:38,479] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29293
[2023-09-05 05:12:38,479] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:12:38,479] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29293
[2023-09-05 05:12:38,479] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:12:38,479] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [183]  [ 20/160]  eta: 0:02:49  lr: 0.000019  min_lr: 0.000005  loss: 1.6107 (1.6355)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8215 (7.3842)  time: 0.9113 (0.5250 -- 4.0587)  data: 0.3646 (0.0004 -- 3.5199)  max mem: 16413
Epoch: [183]  [ 40/160]  eta: 0:02:02  lr: 0.000019  min_lr: 0.000005  loss: 1.4025 (1.5895)  loss_scale: 16384.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6897 (6.6840)  time: 0.8147 (0.5298 -- 2.5324)  data: 0.1196 (0.0004 -- 0.8088)  max mem: 16413
Epoch: [183]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000005  loss: 1.5711 (1.6061)  loss_scale: 16384.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5008 (6.7045)  time: 0.9166 (0.5324 -- 2.5456)  data: 0.2374 (0.0005 -- 1.9715)  max mem: 16413
Epoch: [183]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000005  loss: 1.5130 (1.5846)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0622 (6.8032)  time: 0.8444 (0.5252 -- 2.2891)  data: 0.1324 (0.0004 -- 1.3746)  max mem: 16413
Epoch: [183]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000005  loss: 1.5788 (1.5971)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7247 (6.8140)  time: 0.9440 (0.5141 -- 4.3099)  data: 0.0012 (0.0003 -- 0.0021)  max mem: 16413
Epoch: [183]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000005  loss: 1.4940 (1.5828)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6617 (6.8094)  time: 0.7724 (0.5345 -- 2.2591)  data: 0.0535 (0.0005 -- 0.7702)  max mem: 16413
Epoch: [183]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000005  loss: 1.4616 (1.5751)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5238 (6.8061)  time: 0.9024 (0.5179 -- 3.5982)  data: 0.1833 (0.0004 -- 2.1838)  max mem: 16413
[2023-09-05 05:14:30,489] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:14:30,489] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:14:30,489] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:14:30,489] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [183]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.6084 (1.5807)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0810 (6.8776)  time: 0.7230 (0.4950 -- 3.0245)  data: 0.2019 (0.0002 -- 2.4869)  max mem: 16413
Epoch: [183] Total time: 0:02:23 (0.8955 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.6084 (1.5876)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0810 (6.8776)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1390 (0.1390)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5187 (2.5187 -- 2.5187)  data: 2.3191 (2.3191 -- 2.3191)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6787 (0.5473)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4340 (0.2046 -- 2.5187)  data: 0.2165 (0.0005 -- 2.3191)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4976 (0.5443)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2140 (0.1701 -- 0.2854)  data: 0.0069 (0.0001 -- 0.0717)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5555 (0.5803)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (98.3402)  time: 0.1985 (0.1335 -- 0.2854)  data: 0.0066 (0.0001 -- 0.0717)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.592
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.31%
Epoch: [184]  [  0/160]  eta: 0:20:13  lr: 0.000018  min_lr: 0.000005  loss: 1.4150 (1.4150)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5762 (6.5762)  time: 7.5844 (7.5844 -- 7.5844)  data: 7.0338 (7.0338 -- 7.0338)  max mem: 16413
Epoch: [184]  [ 20/160]  eta: 0:02:49  lr: 0.000018  min_lr: 0.000005  loss: 1.5572 (1.5176)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9786 (7.5622)  time: 0.8918 (0.5315 -- 4.4384)  data: 0.0124 (0.0005 -- 0.2079)  max mem: 16413
[2023-09-05 05:15:33,894] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29480
[2023-09-05 05:15:33,894] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:15:33,894] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29480
[2023-09-05 05:15:33,895] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:15:33,895] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [184]  [ 40/160]  eta: 0:02:05  lr: 0.000018  min_lr: 0.000005  loss: 1.6872 (1.5812)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7777 (7.2471)  time: 0.8669 (0.5210 -- 3.9399)  data: 0.0013 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [184]  [ 60/160]  eta: 0:01:41  lr: 0.000018  min_lr: 0.000005  loss: 1.6776 (1.6049)  loss_scale: 16384.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9493 (7.0788)  time: 0.9517 (0.5197 -- 3.4615)  data: 0.0019 (0.0003 -- 0.0145)  max mem: 16413
Epoch: [184]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000005  loss: 1.6376 (1.5850)  loss_scale: 16384.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8467 (7.1350)  time: 0.7932 (0.5229 -- 3.2017)  data: 0.0018 (0.0005 -- 0.0120)  max mem: 16413
Epoch: [184]  [100/160]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000005  loss: 1.6163 (1.5782)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2060 (7.0529)  time: 0.9941 (0.5269 -- 4.2453)  data: 0.0016 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [184]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000005  loss: 1.6548 (1.5672)  loss_scale: 16384.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8880 (7.0492)  time: 0.8216 (0.5264 -- 4.1224)  data: 0.0012 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [184]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000005  loss: 1.7206 (1.5965)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6610 (7.0585)  time: 0.9196 (0.5168 -- 4.1224)  data: 0.0018 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [184]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.4502 (1.5813)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0712 (6.9557)  time: 0.7155 (0.4992 -- 4.1224)  data: 0.0009 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [184] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.4502 (1.6008)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0712 (6.9557)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1467 (0.1467)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4558 (2.4558 -- 2.4558)  data: 2.1841 (2.1841 -- 2.1841)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6471 (0.5350)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4224 (0.1906 -- 2.4558)  data: 0.2052 (0.0005 -- 2.1841)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4850 (0.5324)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2185 (0.1691 -- 0.4564)  data: 0.0172 (0.0001 -- 0.2674)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5797 (0.5906)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (98.7552)  time: 0.2043 (0.1328 -- 0.4564)  data: 0.0169 (0.0001 -- 0.2674)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 83.610 Acc@5 98.133 loss 0.597
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 86.31%
Epoch: [185]  [  0/160]  eta: 0:20:49  lr: 0.000018  min_lr: 0.000005  loss: 1.6677 (1.6677)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5924 (10.5924)  time: 7.8070 (7.8070 -- 7.8070)  data: 7.2176 (7.2176 -- 7.2176)  max mem: 16413
[2023-09-05 05:17:37,040] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:17:37,040] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:17:37,040] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:17:37,040] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [185]  [ 20/160]  eta: 0:02:41  lr: 0.000018  min_lr: 0.000005  loss: 1.4011 (1.5030)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8074 (7.0401)  time: 0.8234 (0.5278 -- 3.1607)  data: 0.2676 (0.0004 -- 2.6199)  max mem: 16413
Epoch: [185]  [ 40/160]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000005  loss: 1.5548 (1.5266)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9825 (6.7492)  time: 0.9118 (0.5262 -- 3.0384)  data: 0.3613 (0.0001 -- 2.5146)  max mem: 16413
Epoch: [185]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000005  loss: 1.4002 (1.5029)  loss_scale: 32768.0000 (30350.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4348 (6.7931)  time: 0.8259 (0.5284 -- 4.0223)  data: 0.2730 (0.0004 -- 3.4856)  max mem: 16413
Epoch: [185]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000005  loss: 1.6999 (1.5575)  loss_scale: 32768.0000 (30947.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7785 (7.0473)  time: 0.9161 (0.5353 -- 4.5289)  data: 0.3260 (0.0008 -- 3.9642)  max mem: 16413
Epoch: [185]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000005  loss: 1.6286 (1.5630)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1001 (7.0079)  time: 0.7617 (0.5439 -- 2.7199)  data: 0.1976 (0.0004 -- 2.1688)  max mem: 16413
Epoch: [185]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000005  loss: 1.6814 (1.5736)  loss_scale: 32768.0000 (31549.3554)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7878 (7.0324)  time: 0.9672 (0.5231 -- 3.1621)  data: 0.3742 (0.0002 -- 2.6122)  max mem: 16413
[2023-09-05 05:19:28,296] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:19:28,296] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:19:28,296] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 05:19:28,296] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 05:19:29,443] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29739
[2023-09-05 05:19:29,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 05:19:29,443] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29739
[2023-09-05 05:19:29,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 05:19:29,444] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [185]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000005  loss: 1.6719 (1.5781)  loss_scale: 32768.0000 (32187.0071)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1040 (6.9805)  time: 0.8513 (0.5338 -- 3.7760)  data: 0.2969 (0.0005 -- 3.2472)  max mem: 16413
Epoch: [185]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000005  loss: 1.5531 (1.5744)  loss_scale: 32768.0000 (32256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7526 (7.0803)  time: 0.6984 (0.4968 -- 2.2768)  data: 0.1689 (0.0002 -- 1.7633)  max mem: 16413
Epoch: [185] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000005  loss: 1.5531 (1.5908)  loss_scale: 32768.0000 (32256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7526 (7.0803)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3825 (2.3825 -- 2.3825)  data: 2.1716 (2.1716 -- 2.1716)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6591 (0.5458)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4357 (0.2080 -- 2.3825)  data: 0.2142 (0.0006 -- 2.1716)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4866 (0.5314)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2312 (0.1689 -- 0.4314)  data: 0.0220 (0.0001 -- 0.2529)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4910 (0.5784)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.5104)  time: 0.2135 (0.1329 -- 0.4314)  data: 0.0209 (0.0001 -- 0.2529)  max mem: 16413
Val: Total time: 0:00:07 (0.2959 s / it)
* Acc@1 85.062 Acc@5 97.510 loss 0.605
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [186]  [  0/160]  eta: 0:19:12  lr: 0.000018  min_lr: 0.000005  loss: 1.2400 (1.2400)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1698 (5.1698)  time: 7.2055 (7.2055 -- 7.2055)  data: 6.5094 (6.5094 -- 6.5094)  max mem: 16413
Epoch: [186]  [ 20/160]  eta: 0:02:55  lr: 0.000018  min_lr: 0.000005  loss: 1.5368 (1.4859)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8835 (7.0075)  time: 0.9569 (0.5118 -- 3.2446)  data: 0.1300 (0.0002 -- 1.7580)  max mem: 16413
Epoch: [186]  [ 40/160]  eta: 0:02:03  lr: 0.000018  min_lr: 0.000005  loss: 1.8237 (1.6606)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2837 (6.7621)  time: 0.7875 (0.5301 -- 2.6326)  data: 0.0699 (0.0003 -- 1.3646)  max mem: 16413
Epoch: [186]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000004  loss: 1.6902 (1.6665)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6709 (6.8703)  time: 0.8966 (0.5309 -- 3.7211)  data: 0.2251 (0.0005 -- 3.1599)  max mem: 16413
[2023-09-05 05:21:03,986] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29833
[2023-09-05 05:21:03,986] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:21:03,986] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 05:21:03,987] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29833
[2023-09-05 05:21:03,987] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [186]  [ 80/160]  eta: 0:01:18  lr: 0.000018  min_lr: 0.000004  loss: 1.6707 (1.6601)  loss_scale: 32768.0000 (31149.8272)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0747 (6.7974)  time: 0.9655 (0.5172 -- 4.1293)  data: 0.4155 (0.0003 -- 3.5971)  max mem: 16413
Epoch: [186]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000004  loss: 1.6056 (1.6379)  loss_scale: 16384.0000 (28225.9010)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4219 (6.9252)  time: 0.7919 (0.5309 -- 2.7257)  data: 0.1862 (0.0004 -- 2.1837)  max mem: 16413
Epoch: [186]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000004  loss: 1.4141 (1.6174)  loss_scale: 16384.0000 (26268.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6390 (6.9432)  time: 0.8163 (0.5270 -- 2.6612)  data: 0.0703 (0.0004 -- 1.0006)  max mem: 16413
Epoch: [186]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000004  loss: 1.5550 (1.6073)  loss_scale: 16384.0000 (24866.4965)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1863 (7.1121)  time: 0.8878 (0.5257 -- 2.6597)  data: 0.1305 (0.0005 -- 1.9530)  max mem: 16413
Epoch: [186]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000004  loss: 1.6132 (1.6066)  loss_scale: 16384.0000 (23859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6790 (7.0802)  time: 0.7144 (0.4984 -- 3.8480)  data: 0.1811 (0.0002 -- 3.3103)  max mem: 16413
Epoch: [186] Total time: 0:02:23 (0.8940 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000004  loss: 1.6132 (1.6166)  loss_scale: 16384.0000 (23859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6790 (7.0802)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1283 (0.1283)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4135 (2.4135 -- 2.4135)  data: 2.1311 (2.1311 -- 2.1311)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6152 (0.5302)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4359 (0.1922 -- 2.4135)  data: 0.2093 (0.0008 -- 2.1311)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5129 (0.5136)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2186 (0.1701 -- 0.3992)  data: 0.0108 (0.0001 -- 0.1591)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5129 (0.5610)  acc1: 77.7778 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2035 (0.1334 -- 0.3992)  data: 0.0105 (0.0001 -- 0.1591)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 85.892 Acc@5 97.718 loss 0.590
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.31%
Epoch: [187]  [  0/160]  eta: 0:22:46  lr: 0.000018  min_lr: 0.000004  loss: 2.2088 (2.2088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0131 (7.0131)  time: 8.5413 (8.5413 -- 8.5413)  data: 7.3997 (7.3997 -- 7.3997)  max mem: 16413
Epoch: [187]  [ 20/160]  eta: 0:02:53  lr: 0.000017  min_lr: 0.000004  loss: 1.4218 (1.5861)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8690 (6.8511)  time: 0.8776 (0.5268 -- 3.7068)  data: 0.3315 (0.0002 -- 3.1570)  max mem: 16413
Epoch: [187]  [ 40/160]  eta: 0:02:08  lr: 0.000017  min_lr: 0.000004  loss: 1.8178 (1.6467)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5369 (6.6879)  time: 0.8960 (0.5249 -- 3.5783)  data: 0.3434 (0.0006 -- 3.0407)  max mem: 16413
[2023-09-05 05:23:07,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:23:07,694] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:23:07,694] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:23:07,694] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:23:09,396] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29965
[2023-09-05 05:23:09,397] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:23:09,396] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29965
[2023-09-05 05:23:09,397] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:23:09,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [187]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000004  loss: 1.5097 (1.6323)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8900 (6.6068)  time: 0.8118 (0.5336 -- 2.9943)  data: 0.2634 (0.0004 -- 2.4629)  max mem: 16413
[2023-09-05 05:23:37,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=170, lr=[4.423501559208211e-06, 4.423501559208211e-06, 4.915001732453568e-06, 4.915001732453568e-06, 5.461113036059519e-06, 5.461113036059519e-06, 6.067903373399466e-06, 6.067903373399466e-06, 6.74211485933274e-06, 6.74211485933274e-06, 7.4912387325919336e-06, 7.4912387325919336e-06, 8.323598591768814e-06, 8.323598591768814e-06, 9.248442879743126e-06, 9.248442879743126e-06, 1.0276047644159029e-05, 1.0276047644159029e-05, 1.1417830715732254e-05, 1.1417830715732254e-05, 1.2686478573035838e-05, 1.2686478573035838e-05, 1.4096087303373153e-05, 1.4096087303373153e-05, 1.566231922597017e-05, 1.566231922597017e-05, 1.7402576917744633e-05, 1.7402576917744633e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 05:23:37,330] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=17.752975932840805, CurrSamplesPerSec=23.27207086906525, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [187]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000004  loss: 1.6455 (1.6458)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1558 (7.0614)  time: 0.8193 (0.5283 -- 3.0645)  data: 0.2688 (0.0004 -- 2.5210)  max mem: 16413
Epoch: [187]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000004  loss: 1.5422 (1.6383)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9992 (7.0551)  time: 0.8986 (0.5265 -- 4.0425)  data: 0.2870 (0.0004 -- 3.4855)  max mem: 16413
Epoch: [187]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000004  loss: 1.4996 (1.6212)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0586 (6.9170)  time: 0.9601 (0.5280 -- 4.5874)  data: 0.3430 (0.0002 -- 4.0644)  max mem: 16413
Epoch: [187]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.7930 (1.6249)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5042 (7.1466)  time: 0.8705 (0.5252 -- 3.6332)  data: 0.0449 (0.0003 -- 0.8804)  max mem: 16413
Epoch: [187]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.4461 (1.6063)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2941 (7.0829)  time: 0.6897 (0.4951 -- 3.5111)  data: 0.0006 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [187] Total time: 0:02:24 (0.9032 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.4461 (1.6004)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2941 (7.0829)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1242 (0.1242)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1990 (2.1990 -- 2.1990)  data: 1.9668 (1.9668 -- 1.9668)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5568 (0.5531)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.3977 (0.1961 -- 2.1990)  data: 0.1797 (0.0006 -- 1.9668)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5132 (0.5276)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2294 (0.1688 -- 0.5480)  data: 0.0219 (0.0001 -- 0.3487)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5211 (0.5697)  acc1: 77.7778 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2107 (0.1328 -- 0.5480)  data: 0.0217 (0.0001 -- 0.3487)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 84.647 Acc@5 97.718 loss 0.598
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [188]  [  0/160]  eta: 0:21:57  lr: 0.000017  min_lr: 0.000004  loss: 1.0889 (1.0889)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1441 (5.1441)  time: 8.2361 (8.2361 -- 8.2361)  data: 7.6745 (7.6745 -- 7.6745)  max mem: 16413
[2023-09-05 05:25:05,633] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30085
[2023-09-05 05:25:05,633] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30085
[2023-09-05 05:25:05,633] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:25:05,633] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:25:05,633] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [188]  [ 20/160]  eta: 0:02:41  lr: 0.000017  min_lr: 0.000004  loss: 1.6584 (1.5746)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0618 (7.2887)  time: 0.7984 (0.5223 -- 3.9441)  data: 0.2375 (0.0003 -- 3.3878)  max mem: 16413
Epoch: [188]  [ 40/160]  eta: 0:02:09  lr: 0.000017  min_lr: 0.000004  loss: 1.6951 (1.6341)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7519 (7.2314)  time: 1.0055 (0.5253 -- 3.8717)  data: 0.4611 (0.0004 -- 3.3135)  max mem: 16413
Epoch: [188]  [ 60/160]  eta: 0:01:39  lr: 0.000017  min_lr: 0.000004  loss: 1.8526 (1.6708)  loss_scale: 8192.0000 (8863.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6641 (7.2387)  time: 0.8267 (0.5193 -- 3.5706)  data: 0.2811 (0.0004 -- 3.0537)  max mem: 16413
Epoch: [188]  [ 80/160]  eta: 0:01:17  lr: 0.000017  min_lr: 0.000004  loss: 1.4881 (1.6401)  loss_scale: 8192.0000 (8697.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0023 (7.1744)  time: 0.8788 (0.5114 -- 3.2669)  data: 0.3142 (0.0002 -- 2.7433)  max mem: 16413
Epoch: [188]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000004  loss: 1.6856 (1.6311)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0239 (7.1487)  time: 0.8548 (0.5281 -- 2.7724)  data: 0.0011 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [188]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000004  loss: 1.5612 (1.6193)  loss_scale: 8192.0000 (8530.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3231 (7.2878)  time: 0.7885 (0.5291 -- 3.8890)  data: 0.0015 (0.0003 -- 0.0038)  max mem: 16413
[2023-09-05 05:26:57,454] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:26:57,454] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:26:57,455] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 05:26:57,455] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [188]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.6303 (1.6286)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5646 (7.2100)  time: 0.8990 (0.5422 -- 3.6738)  data: 0.0031 (0.0006 -- 0.0137)  max mem: 16413
Epoch: [188]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.3762 (1.6083)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7422 (7.2326)  time: 0.6794 (0.4981 -- 2.4959)  data: 0.0010 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [188] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.3762 (1.6138)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7422 (7.2326)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1219 (0.1219)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4364 (2.4364 -- 2.4364)  data: 2.1941 (2.1941 -- 2.1941)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5964 (0.5403)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4196 (0.2020 -- 2.4364)  data: 0.2003 (0.0006 -- 2.1941)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4745 (0.5304)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2238 (0.1686 -- 0.5161)  data: 0.0172 (0.0001 -- 0.3326)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5371 (0.5852)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2064 (0.1326 -- 0.5161)  data: 0.0170 (0.0001 -- 0.3326)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 84.647 Acc@5 97.303 loss 0.585
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [189]  [  0/160]  eta: 0:23:51  lr: 0.000017  min_lr: 0.000004  loss: 0.9096 (0.9096)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4815 (8.4815)  time: 8.9494 (8.9494 -- 8.9494)  data: 8.3702 (8.3702 -- 8.3702)  max mem: 16413
Epoch: [189]  [ 20/160]  eta: 0:02:44  lr: 0.000017  min_lr: 0.000004  loss: 1.6688 (1.6557)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3002 (7.9070)  time: 0.7848 (0.5222 -- 3.0916)  data: 0.2318 (0.0004 -- 2.5709)  max mem: 16413
Epoch: [189]  [ 40/160]  eta: 0:02:11  lr: 0.000017  min_lr: 0.000004  loss: 1.5141 (1.5984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7433 (7.3739)  time: 1.0182 (0.5149 -- 5.3865)  data: 0.4701 (0.0004 -- 4.8697)  max mem: 16413
Epoch: [189]  [ 60/160]  eta: 0:01:36  lr: 0.000017  min_lr: 0.000004  loss: 1.5692 (1.6000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0563 (7.3898)  time: 0.7070 (0.5312 -- 2.1907)  data: 0.1392 (0.0006 -- 1.6472)  max mem: 16413
[2023-09-05 05:28:29,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30306
[2023-09-05 05:28:29,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:28:29,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30306
[2023-09-05 05:28:29,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:28:29,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [189]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000004  loss: 1.5838 (1.6012)  loss_scale: 8192.0000 (14866.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1987 (7.3975)  time: 0.8584 (0.5269 -- 2.6509)  data: 0.2946 (0.0002 -- 2.1300)  max mem: 16413
Epoch: [189]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000004  loss: 1.7782 (1.6293)  loss_scale: 8192.0000 (13545.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1079 (7.4417)  time: 0.8807 (0.5306 -- 2.2684)  data: 0.2150 (0.0004 -- 0.9724)  max mem: 16413
Epoch: [189]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000004  loss: 1.5428 (1.6239)  loss_scale: 8192.0000 (12660.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7807 (7.3718)  time: 0.9679 (0.5313 -- 3.8320)  data: 0.3032 (0.0005 -- 3.3158)  max mem: 16413
Epoch: [189]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.6604 (1.6174)  loss_scale: 8192.0000 (12026.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8628 (7.3408)  time: 0.8082 (0.5240 -- 2.4004)  data: 0.1862 (0.0003 -- 1.8574)  max mem: 16413
Epoch: [189]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000004  loss: 1.4983 (1.5995)  loss_scale: 8192.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0746 (7.2351)  time: 0.7592 (0.4937 -- 3.9255)  data: 0.2427 (0.0002 -- 3.3837)  max mem: 16413
Epoch: [189] Total time: 0:02:24 (0.9008 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000004  loss: 1.4983 (1.6053)  loss_scale: 8192.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0746 (7.2351)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1255 (0.1255)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2337 (2.2337 -- 2.2337)  data: 2.0192 (2.0192 -- 2.0192)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6090 (0.5342)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4115 (0.2035 -- 2.2337)  data: 0.1920 (0.0008 -- 2.0192)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5031 (0.5343)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2280 (0.1704 -- 0.3628)  data: 0.0208 (0.0001 -- 0.1673)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5031 (0.5754)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2107 (0.1331 -- 0.3628)  data: 0.0200 (0.0001 -- 0.1673)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 84.855 Acc@5 97.718 loss 0.592
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [190]  [  0/160]  eta: 0:19:12  lr: 0.000017  min_lr: 0.000004  loss: 0.9651 (0.9651)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0171 (7.0171)  time: 7.2000 (7.2000 -- 7.2000)  data: 6.6549 (6.6549 -- 6.6549)  max mem: 16413
Epoch: [190]  [ 20/160]  eta: 0:02:51  lr: 0.000017  min_lr: 0.000004  loss: 1.3275 (1.3895)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5563 (7.7932)  time: 0.9271 (0.5312 -- 2.9450)  data: 0.3745 (0.0002 -- 2.4252)  max mem: 16413
[2023-09-05 05:30:35,410] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:30:35,410] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 05:30:35,412] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:30:35,412] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [190]  [ 40/160]  eta: 0:02:07  lr: 0.000017  min_lr: 0.000004  loss: 1.6923 (1.5356)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7770 (8.1626)  time: 0.8847 (0.5323 -- 3.8370)  data: 0.1938 (0.0003 -- 3.3121)  max mem: 16413
Epoch: [190]  [ 60/160]  eta: 0:01:37  lr: 0.000017  min_lr: 0.000004  loss: 1.4943 (1.5290)  loss_scale: 16384.0000 (11683.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3470 (7.5923)  time: 0.8075 (0.5298 -- 2.4993)  data: 0.1080 (0.0004 -- 1.0632)  max mem: 16413
Epoch: [190]  [ 80/160]  eta: 0:01:17  lr: 0.000017  min_lr: 0.000004  loss: 1.5616 (1.5514)  loss_scale: 16384.0000 (12844.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9761 (7.7836)  time: 0.9267 (0.5326 -- 2.3539)  data: 0.2470 (0.0004 -- 1.6462)  max mem: 16413
Epoch: [190]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000004  loss: 1.6255 (1.5700)  loss_scale: 16384.0000 (13545.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0337 (7.7994)  time: 0.8235 (0.5357 -- 2.5652)  data: 0.1994 (0.0005 -- 2.0177)  max mem: 16413
Epoch: [190]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000004  loss: 1.5455 (1.5696)  loss_scale: 16384.0000 (14014.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5512 (7.6743)  time: 0.9288 (0.5152 -- 3.3906)  data: 0.3861 (0.0003 -- 2.8641)  max mem: 16413
Epoch: [190]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000004  loss: 1.4183 (1.5641)  loss_scale: 16384.0000 (14350.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0474 (7.5734)  time: 0.8095 (0.5219 -- 3.8078)  data: 0.2374 (0.0005 -- 3.2710)  max mem: 16413
Epoch: [190]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.6285 (1.5759)  loss_scale: 16384.0000 (14592.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6158 (7.4768)  time: 0.7636 (0.4964 -- 3.0203)  data: 0.2385 (0.0001 -- 2.4701)  max mem: 16413
Epoch: [190] Total time: 0:02:24 (0.9007 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.6285 (1.5839)  loss_scale: 16384.0000 (14592.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6158 (7.4768)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1219 (0.1219)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3375 (2.3375 -- 2.3375)  data: 2.1050 (2.1050 -- 2.1050)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5223 (0.5371)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4091 (0.2016 -- 2.3375)  data: 0.1975 (0.0008 -- 2.1050)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4860 (0.5251)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2224 (0.1694 -- 0.5141)  data: 0.0180 (0.0001 -- 0.2892)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4860 (0.5710)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2080 (0.1323 -- 0.5141)  data: 0.0177 (0.0001 -- 0.2892)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 86.307 Acc@5 97.925 loss 0.583
Accuracy of the network on the 482 val images: 86.31%
Max accuracy: 86.31%
Epoch: [191]  [  0/160]  eta: 0:20:05  lr: 0.000016  min_lr: 0.000004  loss: 1.0422 (1.0422)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8703 (7.8703)  time: 7.5316 (7.5316 -- 7.5316)  data: 6.7719 (6.7719 -- 6.7719)  max mem: 16413
[2023-09-05 05:32:38,248] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:32:38,249] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:32:38,250] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:32:38,250] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [191]  [ 20/160]  eta: 0:02:38  lr: 0.000016  min_lr: 0.000004  loss: 1.5157 (1.5283)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5230 (6.6847)  time: 0.8144 (0.5388 -- 2.4545)  data: 0.2626 (0.0004 -- 1.9211)  max mem: 16413
Epoch: [191]  [ 40/160]  eta: 0:02:07  lr: 0.000016  min_lr: 0.000004  loss: 1.7794 (1.6190)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2360 (6.9891)  time: 0.9940 (0.5340 -- 3.2655)  data: 0.4421 (0.0004 -- 2.7276)  max mem: 16413
Epoch: [191]  [ 60/160]  eta: 0:01:39  lr: 0.000016  min_lr: 0.000004  loss: 1.5871 (1.6123)  loss_scale: 32768.0000 (31962.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7426 (7.1134)  time: 0.8640 (0.5222 -- 3.6084)  data: 0.3172 (0.0003 -- 3.0797)  max mem: 16413
[2023-09-05 05:33:36,123] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30629
[2023-09-05 05:33:36,123] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30629
[2023-09-05 05:33:36,123] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:33:36,123] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:33:36,123] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [191]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000004  loss: 1.5861 (1.5951)  loss_scale: 16384.0000 (29733.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2188 (7.0009)  time: 0.8196 (0.5057 -- 4.4481)  data: 0.2700 (0.0002 -- 3.8969)  max mem: 16413
Epoch: [191]  [100/160]  eta: 0:00:57  lr: 0.000016  min_lr: 0.000004  loss: 1.7348 (1.6160)  loss_scale: 16384.0000 (27090.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9123 (6.9772)  time: 0.9386 (0.5149 -- 4.0376)  data: 0.3895 (0.0004 -- 3.4925)  max mem: 16413
Epoch: [191]  [120/160]  eta: 0:00:37  lr: 0.000016  min_lr: 0.000004  loss: 1.4808 (1.6016)  loss_scale: 16384.0000 (25320.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0038 (7.1883)  time: 0.8659 (0.5211 -- 4.2699)  data: 0.3266 (0.0002 -- 3.7638)  max mem: 16413
Epoch: [191]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000004  loss: 1.4418 (1.5776)  loss_scale: 16384.0000 (24053.1064)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2327 (7.0698)  time: 0.9322 (0.5225 -- 3.9277)  data: 0.3829 (0.0003 -- 3.4044)  max mem: 16413
Epoch: [191]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.6618 (1.5818)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8708 (7.0618)  time: 0.6358 (0.4967 -- 2.2375)  data: 0.1148 (0.0001 -- 1.7470)  max mem: 16413
Epoch: [191] Total time: 0:02:24 (0.9017 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.6618 (1.5794)  loss_scale: 16384.0000 (23142.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8708 (7.0618)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1178 (0.1178)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5666 (2.5666 -- 2.5666)  data: 2.3468 (2.3468 -- 2.3468)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5916 (0.5525)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4330 (0.2078 -- 2.5666)  data: 0.2145 (0.0006 -- 2.3468)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4981 (0.5348)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2086 (0.1705 -- 0.2346)  data: 0.0017 (0.0001 -- 0.0169)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4981 (0.5750)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.1920 (0.1331 -- 0.2279)  data: 0.0013 (0.0001 -- 0.0169)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 85.892 Acc@5 97.718 loss 0.580
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.31%
Epoch: [192]  [  0/160]  eta: 0:21:19  lr: 0.000016  min_lr: 0.000004  loss: 0.9734 (0.9734)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3313 (4.3313)  time: 7.9966 (7.9966 -- 7.9966)  data: 7.4603 (7.4603 -- 7.4603)  max mem: 16413
Epoch: [192]  [ 20/160]  eta: 0:02:48  lr: 0.000016  min_lr: 0.000004  loss: 1.5442 (1.5567)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3231 (6.3901)  time: 0.8651 (0.5392 -- 3.7729)  data: 0.2892 (0.0007 -- 3.2487)  max mem: 16413
[2023-09-05 05:35:42,444] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:35:42,444] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:35:42,444] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:35:42,445] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [192]  [ 40/160]  eta: 0:02:12  lr: 0.000016  min_lr: 0.000004  loss: 1.6514 (1.5682)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5072 (6.5940)  time: 1.0045 (0.5132 -- 4.1480)  data: 0.4549 (0.0004 -- 3.5893)  max mem: 16413
Epoch: [192]  [ 60/160]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000004  loss: 1.6960 (1.5952)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6594 (6.8065)  time: 0.6617 (0.5182 -- 1.5795)  data: 0.0992 (0.0002 -- 1.0424)  max mem: 16413
Epoch: [192]  [ 80/160]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000004  loss: 1.7771 (1.6260)  loss_scale: 32768.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9990 (6.8835)  time: 0.8651 (0.5357 -- 2.5357)  data: 0.1320 (0.0004 -- 1.9679)  max mem: 16413
Epoch: [192]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000004  loss: 1.6712 (1.6446)  loss_scale: 32768.0000 (26603.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7907 (6.9463)  time: 0.9726 (0.5288 -- 3.6134)  data: 0.4231 (0.0003 -- 3.0934)  max mem: 16413
Epoch: [192]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000004  loss: 1.5368 (1.6342)  loss_scale: 32768.0000 (27622.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0797 (7.0232)  time: 0.7588 (0.5288 -- 3.6779)  data: 0.2063 (0.0004 -- 3.1447)  max mem: 16413
Epoch: [192]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000004  loss: 1.6895 (1.6426)  loss_scale: 32768.0000 (28352.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3674 (6.9765)  time: 0.9540 (0.5310 -- 3.0994)  data: 0.3975 (0.0007 -- 2.5449)  max mem: 16413
[2023-09-05 05:37:15,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30866
[2023-09-05 05:37:15,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30866
[2023-09-05 05:37:15,837] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:37:15,837] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:37:15,837] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [192]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.5736 (1.6413)  loss_scale: 16384.0000 (27443.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6654 (6.9383)  time: 0.6757 (0.4966 -- 2.7241)  data: 0.1547 (0.0002 -- 2.2086)  max mem: 16413
Epoch: [192] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.5736 (1.6353)  loss_scale: 16384.0000 (27443.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6654 (6.9383)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1225 (0.1225)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3825 (2.3825 -- 2.3825)  data: 2.1554 (2.1554 -- 2.1554)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4883 (0.5507)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4213 (0.1900 -- 2.3825)  data: 0.2068 (0.0006 -- 2.1554)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4883 (0.5332)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2215 (0.1696 -- 0.4449)  data: 0.0193 (0.0001 -- 0.2633)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5069 (0.5841)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.9253)  time: 0.2070 (0.1335 -- 0.4449)  data: 0.0189 (0.0001 -- 0.2633)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.583
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [193]  [  0/160]  eta: 0:22:53  lr: 0.000016  min_lr: 0.000004  loss: 1.6381 (1.6381)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1279 (5.1279)  time: 8.5824 (8.5824 -- 8.5824)  data: 5.2053 (5.2053 -- 5.2053)  max mem: 16413
Epoch: [193]  [ 20/160]  eta: 0:02:48  lr: 0.000016  min_lr: 0.000004  loss: 1.6365 (1.5948)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9543 (6.6347)  time: 0.8349 (0.5364 -- 2.8292)  data: 0.1251 (0.0004 -- 2.2938)  max mem: 16413
Epoch: [193]  [ 40/160]  eta: 0:02:01  lr: 0.000016  min_lr: 0.000004  loss: 1.7263 (1.6193)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6243 (6.8973)  time: 0.8200 (0.5275 -- 2.0788)  data: 0.2127 (0.0003 -- 1.5345)  max mem: 16413
Epoch: [193]  [ 60/160]  eta: 0:01:38  lr: 0.000016  min_lr: 0.000004  loss: 1.7365 (1.6307)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0398 (7.0999)  time: 0.9297 (0.5354 -- 2.0069)  data: 0.2146 (0.0006 -- 1.4697)  max mem: 16413
Epoch: [193]  [ 80/160]  eta: 0:01:15  lr: 0.000016  min_lr: 0.000004  loss: 1.5073 (1.6094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5394 (6.9799)  time: 0.8250 (0.5252 -- 3.2640)  data: 0.2053 (0.0004 -- 2.7107)  max mem: 16413
Epoch: [193]  [100/160]  eta: 0:00:57  lr: 0.000016  min_lr: 0.000004  loss: 1.5149 (1.5994)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8390 (7.1287)  time: 0.9598 (0.5290 -- 3.8796)  data: 0.3975 (0.0004 -- 3.3356)  max mem: 16413
[2023-09-05 05:39:20,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:39:20,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:39:20,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:39:20,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:39:22,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=174, lr=[4.009871377649155e-06, 4.009871377649155e-06, 4.455412641832395e-06, 4.455412641832395e-06, 4.950458490924882e-06, 4.950458490924882e-06, 5.500509434360981e-06, 5.500509434360981e-06, 6.111677149289979e-06, 6.111677149289979e-06, 6.790752388099976e-06, 6.790752388099976e-06, 7.545280431222195e-06, 7.545280431222195e-06, 8.383644923580217e-06, 8.383644923580217e-06, 9.31516102620024e-06, 9.31516102620024e-06, 1.0350178918000266e-05, 1.0350178918000266e-05, 1.1500198797778075e-05, 1.1500198797778075e-05, 1.277799866419786e-05, 1.277799866419786e-05, 1.4197776293553177e-05, 1.4197776293553177e-05, 1.5775306992836863e-05, 1.5775306992836863e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 05:39:22,489] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=17.81744407951708, CurrSamplesPerSec=22.204293101189233, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [193]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000004  loss: 1.5629 (1.5946)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9954 (6.9661)  time: 0.7953 (0.5278 -- 4.0329)  data: 0.2411 (0.0002 -- 3.5157)  max mem: 16413
[2023-09-05 05:39:40,906] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31020
[2023-09-05 05:39:40,906] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31020
[2023-09-05 05:39:40,907] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:39:40,907] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:39:40,907] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [193]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000004  loss: 1.7135 (1.6003)  loss_scale: 32768.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0775 (6.9123)  time: 0.8925 (0.5277 -- 3.4308)  data: 0.3444 (0.0003 -- 2.9166)  max mem: 16413
Epoch: [193]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000004  loss: 1.5621 (1.6092)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8800 (6.9440)  time: 0.6449 (0.4975 -- 1.8852)  data: 0.0717 (0.0002 -- 1.3631)  max mem: 16413
Epoch: [193] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000004  loss: 1.5621 (1.6003)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8800 (6.9440)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1245 (0.1245)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4778 (2.4778 -- 2.4778)  data: 2.2376 (2.2376 -- 2.2376)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5764 (0.5345)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4208 (0.1980 -- 2.4778)  data: 0.2048 (0.0006 -- 2.2376)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5037 (0.5340)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2176 (0.1694 -- 0.4262)  data: 0.0122 (0.0001 -- 0.2252)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5037 (0.5738)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2018 (0.1329 -- 0.4262)  data: 0.0116 (0.0001 -- 0.2252)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.576
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.31%
Epoch: [194]  [  0/160]  eta: 0:24:45  lr: 0.000016  min_lr: 0.000004  loss: 1.7779 (1.7779)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2368 (11.2368)  time: 9.2838 (9.2838 -- 9.2838)  data: 6.1534 (6.1534 -- 6.1534)  max mem: 16413
[2023-09-05 05:40:21,455] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31055
[2023-09-05 05:40:21,455] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31055
[2023-09-05 05:40:21,455] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:40:21,455] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:40:21,455] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [194]  [ 20/160]  eta: 0:02:45  lr: 0.000016  min_lr: 0.000004  loss: 1.6401 (1.6552)  loss_scale: 16384.0000 (14043.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3775 (8.1533)  time: 0.7807 (0.5253 -- 3.3770)  data: 0.0025 (0.0002 -- 0.0129)  max mem: 16413
Epoch: [194]  [ 40/160]  eta: 0:02:14  lr: 0.000016  min_lr: 0.000004  loss: 1.5832 (1.6315)  loss_scale: 8192.0000 (11189.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4487 (7.4089)  time: 1.0524 (0.5204 -- 4.6440)  data: 0.0019 (0.0003 -- 0.0144)  max mem: 16413
Epoch: [194]  [ 60/160]  eta: 0:01:38  lr: 0.000016  min_lr: 0.000004  loss: 1.5920 (1.6184)  loss_scale: 8192.0000 (10206.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4658 (7.3331)  time: 0.7114 (0.5212 -- 3.5888)  data: 0.0019 (0.0004 -- 0.0100)  max mem: 16413
Epoch: [194]  [ 80/160]  eta: 0:01:15  lr: 0.000016  min_lr: 0.000004  loss: 1.6294 (1.6197)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9684 (7.0748)  time: 0.7991 (0.5243 -- 3.1702)  data: 0.1434 (0.0004 -- 2.6376)  max mem: 16413
Epoch: [194]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000004  loss: 1.7271 (1.6392)  loss_scale: 8192.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4051 (7.2864)  time: 0.9137 (0.5419 -- 3.0923)  data: 0.1967 (0.0006 -- 2.5610)  max mem: 16413
[2023-09-05 05:41:48,696] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31157
[2023-09-05 05:41:48,697] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31157
[2023-09-05 05:41:48,697] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-09-05 05:41:48,697] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-09-05 05:41:48,697] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [194]  [120/160]  eta: 0:00:37  lr: 0.000016  min_lr: 0.000004  loss: 1.5836 (1.6422)  loss_scale: 8192.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9227 (7.3703)  time: 0.8773 (0.5316 -- 3.2862)  data: 0.0017 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [194]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 1.5197 (1.6261)  loss_scale: 4096.0000 (8366.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4452 (7.3903)  time: 0.8170 (0.5312 -- 2.9488)  data: 0.0746 (0.0005 -- 1.0605)  max mem: 16413
Epoch: [194]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000004  loss: 1.5668 (1.6244)  loss_scale: 4096.0000 (7859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7556 (7.4584)  time: 0.7308 (0.4937 -- 2.7223)  data: 0.0100 (0.0001 -- 0.1811)  max mem: 16413
Epoch: [194] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000004  loss: 1.5668 (1.6273)  loss_scale: 4096.0000 (7859.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7556 (7.4584)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1278 (0.1278)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2686 (2.2686 -- 2.2686)  data: 2.0556 (2.0556 -- 2.0556)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5868 (0.5302)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4216 (0.1966 -- 2.2686)  data: 0.2035 (0.0004 -- 2.0556)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4880 (0.5330)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2275 (0.1701 -- 0.3794)  data: 0.0182 (0.0001 -- 0.1603)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5313 (0.5772)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (98.3402)  time: 0.2109 (0.1335 -- 0.3794)  data: 0.0173 (0.0001 -- 0.1603)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.589
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.31%
Epoch: [195]  [  0/160]  eta: 0:25:48  lr: 0.000015  min_lr: 0.000004  loss: 1.3058 (1.3058)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6938 (6.6938)  time: 9.6791 (9.6791 -- 9.6791)  data: 6.4448 (6.4448 -- 6.4448)  max mem: 16413
Epoch: [195]  [ 20/160]  eta: 0:02:41  lr: 0.000015  min_lr: 0.000004  loss: 1.4372 (1.4695)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4308 (7.0551)  time: 0.7272 (0.5137 -- 3.1239)  data: 0.0110 (0.0005 -- 0.1830)  max mem: 16413
Epoch: [195]  [ 40/160]  eta: 0:02:09  lr: 0.000015  min_lr: 0.000004  loss: 1.6863 (1.5713)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7924 (7.0382)  time: 1.0025 (0.5205 -- 3.9970)  data: 0.4074 (0.0003 -- 3.4510)  max mem: 16413
Epoch: [195]  [ 60/160]  eta: 0:01:39  lr: 0.000015  min_lr: 0.000004  loss: 1.5953 (1.5828)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5905 (7.3588)  time: 0.8101 (0.5319 -- 3.1654)  data: 0.0425 (0.0002 -- 0.5385)  max mem: 16413
Epoch: [195]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000004  loss: 1.6993 (1.6112)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1716 (7.4409)  time: 0.8300 (0.5253 -- 2.7282)  data: 0.0025 (0.0003 -- 0.0223)  max mem: 16413
[2023-09-05 05:43:53,279] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:43:53,279] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-09-05 05:43:53,285] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:43:53,286] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [195]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000004  loss: 1.4542 (1.5942)  loss_scale: 8192.0000 (4704.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1935 (7.5228)  time: 0.8480 (0.5257 -- 2.4761)  data: 0.0480 (0.0009 -- 0.9177)  max mem: 16413
Epoch: [195]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000004  loss: 1.5571 (1.6014)  loss_scale: 8192.0000 (5280.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9825 (7.3818)  time: 0.9710 (0.5327 -- 3.8289)  data: 0.0559 (0.0004 -- 0.8697)  max mem: 16413
Epoch: [195]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 1.8673 (1.6260)  loss_scale: 8192.0000 (5693.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0638 (7.3365)  time: 0.8798 (0.5176 -- 5.7420)  data: 0.1076 (0.0004 -- 1.3592)  max mem: 16413
Epoch: [195]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000004  loss: 1.4864 (1.6105)  loss_scale: 8192.0000 (5990.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6148 (7.1890)  time: 0.6323 (0.4939 -- 2.8631)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [195] Total time: 0:02:23 (0.8950 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000004  loss: 1.4864 (1.6125)  loss_scale: 8192.0000 (5990.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6148 (7.1890)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1308 (0.1308)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4201 (2.4201 -- 2.4201)  data: 2.2098 (2.2098 -- 2.2098)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4844 (0.5306)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4461 (0.1928 -- 2.4201)  data: 0.2274 (0.0005 -- 2.2098)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4844 (0.5312)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.9418)  time: 0.2228 (0.1697 -- 0.5003)  data: 0.0169 (0.0001 -- 0.2669)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4844 (0.5669)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2062 (0.1329 -- 0.5003)  data: 0.0164 (0.0001 -- 0.2669)  max mem: 16413
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.579
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.31%
Epoch: [196]  [  0/160]  eta: 0:16:29  lr: 0.000015  min_lr: 0.000004  loss: 1.3888 (1.3888)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7697 (7.7697)  time: 6.1829 (6.1829 -- 6.1829)  data: 5.6236 (5.6236 -- 5.6236)  max mem: 16413
Epoch: [196]  [ 20/160]  eta: 0:02:48  lr: 0.000015  min_lr: 0.000004  loss: 1.4534 (1.5400)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9029 (7.0464)  time: 0.9536 (0.5365 -- 2.6421)  data: 0.2235 (0.0003 -- 2.0926)  max mem: 16413
Epoch: [196]  [ 40/160]  eta: 0:02:06  lr: 0.000015  min_lr: 0.000004  loss: 1.6401 (1.5662)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6618 (7.0630)  time: 0.8924 (0.5129 -- 3.6988)  data: 0.1804 (0.0002 -- 2.5965)  max mem: 16413
[2023-09-05 05:45:55,646] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:45:55,646] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:45:55,647] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 05:45:55,647] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [196]  [ 60/160]  eta: 0:01:38  lr: 0.000015  min_lr: 0.000004  loss: 1.7033 (1.6066)  loss_scale: 8192.0000 (9132.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9256 (6.8049)  time: 0.8449 (0.5204 -- 3.9822)  data: 0.1897 (0.0003 -- 3.4749)  max mem: 16413
Epoch: [196]  [ 80/160]  eta: 0:01:18  lr: 0.000015  min_lr: 0.000004  loss: 1.6577 (1.6266)  loss_scale: 16384.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7460 (7.0265)  time: 0.9693 (0.5332 -- 5.0804)  data: 0.4197 (0.0004 -- 4.5168)  max mem: 16413
Epoch: [196]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000004  loss: 1.5671 (1.6158)  loss_scale: 16384.0000 (12004.1188)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1820 (7.2510)  time: 0.8074 (0.5229 -- 3.3839)  data: 0.2573 (0.0004 -- 2.8451)  max mem: 16413
Epoch: [196]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000004  loss: 1.6958 (1.6168)  loss_scale: 16384.0000 (12728.0661)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1970 (7.3105)  time: 0.8683 (0.5323 -- 3.3043)  data: 0.2662 (0.0003 -- 2.7441)  max mem: 16413
Epoch: [196]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 1.5897 (1.6166)  loss_scale: 16384.0000 (13246.6383)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6793 (7.3287)  time: 0.9960 (0.5182 -- 4.7122)  data: 0.3315 (0.0003 -- 4.1788)  max mem: 16413
Epoch: [196]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000004  loss: 1.5799 (1.6142)  loss_scale: 16384.0000 (13619.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8924 (7.3691)  time: 0.6104 (0.4951 -- 1.4398)  data: 0.0912 (0.0001 -- 0.9194)  max mem: 16413
Epoch: [196] Total time: 0:02:24 (0.9034 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000004  loss: 1.5799 (1.6108)  loss_scale: 16384.0000 (13619.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8924 (7.3691)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1364 (0.1364)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6000 (2.6000 -- 2.6000)  data: 2.3766 (2.3766 -- 2.3766)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6215 (0.5495)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4250 (0.1866 -- 2.6000)  data: 0.2181 (0.0007 -- 2.3766)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5220 (0.5450)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2137 (0.1695 -- 0.3998)  data: 0.0121 (0.0001 -- 0.2149)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5551 (0.5959)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (98.3402)  time: 0.2010 (0.1327 -- 0.3998)  data: 0.0117 (0.0001 -- 0.2149)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 84.440 Acc@5 98.133 loss 0.593
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [197]  [  0/160]  eta: 0:23:38  lr: 0.000015  min_lr: 0.000004  loss: 1.6343 (1.6343)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5878 (5.5878)  time: 8.8640 (8.8640 -- 8.8640)  data: 8.3362 (8.3362 -- 8.3362)  max mem: 16413
Epoch: [197]  [ 20/160]  eta: 0:02:43  lr: 0.000015  min_lr: 0.000004  loss: 1.6772 (1.6175)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6971 (6.8739)  time: 0.7853 (0.5350 -- 3.0940)  data: 0.0256 (0.0003 -- 0.2732)  max mem: 16413
[2023-09-05 05:48:00,411] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:48:00,411] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:48:00,411] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:48:00,411] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [197]  [ 40/160]  eta: 0:02:06  lr: 0.000015  min_lr: 0.000004  loss: 1.5328 (1.6025)  loss_scale: 32768.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7703 (7.1296)  time: 0.9279 (0.5306 -- 3.2572)  data: 0.0383 (0.0003 -- 0.4959)  max mem: 16413
[2023-09-05 05:48:21,659] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31566
[2023-09-05 05:48:21,659] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31566
[2023-09-05 05:48:21,659] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:48:21,659] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:48:21,660] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 05:48:26,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31570
[2023-09-05 05:48:26,320] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:48:26,320] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31570
[2023-09-05 05:48:26,320] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:48:26,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [197]  [ 60/160]  eta: 0:01:38  lr: 0.000015  min_lr: 0.000004  loss: 1.7964 (1.6520)  loss_scale: 8192.0000 (21352.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5974 (7.1669)  time: 0.8613 (0.5271 -- 3.6950)  data: 0.1557 (0.0002 -- 1.5862)  max mem: 16413
Epoch: [197]  [ 80/160]  eta: 0:01:17  lr: 0.000015  min_lr: 0.000004  loss: 1.5348 (1.6238)  loss_scale: 8192.0000 (18103.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4647 (7.1566)  time: 0.9245 (0.5160 -- 3.4787)  data: 0.2434 (0.0005 -- 1.6390)  max mem: 16413
Epoch: [197]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000004  loss: 1.4260 (1.5906)  loss_scale: 8192.0000 (16140.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8432 (7.1826)  time: 0.7764 (0.5236 -- 2.5407)  data: 0.0018 (0.0003 -- 0.0063)  max mem: 16413
Epoch: [197]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000004  loss: 1.6686 (1.6029)  loss_scale: 8192.0000 (14826.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6210 (7.1292)  time: 0.9328 (0.5237 -- 3.2474)  data: 0.0415 (0.0003 -- 0.8084)  max mem: 16413
Epoch: [197]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000004  loss: 1.5748 (1.6017)  loss_scale: 8192.0000 (13885.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2416 (7.1140)  time: 0.9356 (0.5200 -- 3.0462)  data: 0.0508 (0.0003 -- 0.9955)  max mem: 16413
Epoch: [197]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000004  loss: 1.6642 (1.6082)  loss_scale: 8192.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8692 (7.1658)  time: 0.6730 (0.4959 -- 3.4153)  data: 0.0006 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [197] Total time: 0:02:24 (0.9044 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000004  loss: 1.6642 (1.5734)  loss_scale: 8192.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8692 (7.1658)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1332 (0.1332)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3242 (2.3242 -- 2.3242)  data: 2.0962 (2.0962 -- 2.0962)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6595 (0.5353)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4115 (0.1986 -- 2.3242)  data: 0.1927 (0.0006 -- 2.0962)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5486 (0.5296)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2230 (0.1748 -- 0.4176)  data: 0.0157 (0.0001 -- 0.2085)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5486 (0.5631)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2031 (0.1331 -- 0.4176)  data: 0.0146 (0.0001 -- 0.2085)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 85.477 Acc@5 97.925 loss 0.576
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.31%
Epoch: [198]  [  0/160]  eta: 0:18:30  lr: 0.000015  min_lr: 0.000004  loss: 1.4202 (1.4202)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6739 (4.6739)  time: 6.9428 (6.9428 -- 6.9428)  data: 6.4005 (6.4005 -- 6.4005)  max mem: 16413
[2023-09-05 05:50:33,138] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:50:33,139] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 05:50:33,140] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:50:33,141] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [198]  [ 20/160]  eta: 0:02:56  lr: 0.000015  min_lr: 0.000004  loss: 1.6939 (1.6301)  loss_scale: 8192.0000 (8972.1905)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0759 (7.3340)  time: 0.9757 (0.5266 -- 5.6056)  data: 0.1016 (0.0005 -- 1.8293)  max mem: 16413
Epoch: [198]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000004  loss: 1.6555 (1.6758)  loss_scale: 16384.0000 (12587.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3606 (7.4423)  time: 0.8233 (0.5236 -- 3.2426)  data: 0.0038 (0.0003 -- 0.0505)  max mem: 16413
Epoch: [198]  [ 60/160]  eta: 0:01:41  lr: 0.000015  min_lr: 0.000004  loss: 1.4974 (1.5990)  loss_scale: 16384.0000 (13832.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5637 (7.5331)  time: 0.9521 (0.5339 -- 3.8271)  data: 0.3495 (0.0003 -- 3.2895)  max mem: 16413
Epoch: [198]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000004  loss: 1.7803 (1.6073)  loss_scale: 16384.0000 (14462.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1692 (7.3890)  time: 0.7873 (0.5081 -- 2.3352)  data: 0.1514 (0.0002 -- 1.8163)  max mem: 16413
Epoch: [198]  [100/160]  eta: 0:00:57  lr: 0.000015  min_lr: 0.000004  loss: 1.7300 (1.6233)  loss_scale: 16384.0000 (14842.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9567 (7.1888)  time: 0.9182 (0.5153 -- 2.5430)  data: 0.2067 (0.0005 -- 1.5316)  max mem: 16413
Epoch: [198]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000004  loss: 1.6822 (1.6368)  loss_scale: 16384.0000 (15097.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6499 (7.1420)  time: 0.8439 (0.5212 -- 4.9879)  data: 0.2673 (0.0003 -- 4.4677)  max mem: 16413
Epoch: [198]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000004  loss: 1.5410 (1.6248)  loss_scale: 16384.0000 (15280.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6904 (7.0971)  time: 0.9738 (0.5272 -- 3.6263)  data: 0.4271 (0.0005 -- 3.1125)  max mem: 16413
[2023-09-05 05:52:25,918] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:52:25,918] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:52:25,918] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:52:25,918] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [198]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 1.5940 (1.6180)  loss_scale: 32768.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1921 (7.1286)  time: 0.6396 (0.4976 -- 2.8671)  data: 0.1172 (0.0002 -- 2.3300)  max mem: 16413
Epoch: [198] Total time: 0:02:24 (0.9044 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 1.5940 (1.6107)  loss_scale: 32768.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1921 (7.1286)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1302 (0.1302)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4258 (2.4258 -- 2.4258)  data: 2.2069 (2.2069 -- 2.2069)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6131 (0.5304)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4215 (0.2080 -- 2.4258)  data: 0.2046 (0.0007 -- 2.2069)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4542 (0.5183)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2240 (0.1694 -- 0.5200)  data: 0.0190 (0.0001 -- 0.3318)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4542 (0.5586)  acc1: 77.7778 (83.8174)  acc5: 100.0000 (97.9253)  time: 0.2059 (0.1329 -- 0.5200)  data: 0.0185 (0.0001 -- 0.3318)  max mem: 16413
Val: Total time: 0:00:07 (0.2926 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.579
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [199]  [  0/160]  eta: 0:19:31  lr: 0.000014  min_lr: 0.000004  loss: 1.2904 (1.2904)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6237 (5.6237)  time: 7.3234 (7.3234 -- 7.3234)  data: 6.2328 (6.2328 -- 6.2328)  max mem: 16413
[2023-09-05 05:52:51,182] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31847
[2023-09-05 05:52:51,183] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31847
[2023-09-05 05:52:51,183] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:52:51,183] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:52:51,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [199]  [ 20/160]  eta: 0:02:38  lr: 0.000014  min_lr: 0.000004  loss: 1.5063 (1.5356)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1200 (6.7337)  time: 0.8188 (0.5026 -- 3.3039)  data: 0.2144 (0.0004 -- 2.7596)  max mem: 16413
Epoch: [199]  [ 40/160]  eta: 0:02:00  lr: 0.000014  min_lr: 0.000004  loss: 1.6490 (1.5748)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8086 (7.8289)  time: 0.8727 (0.5361 -- 2.5778)  data: 0.2879 (0.0008 -- 2.0371)  max mem: 16413
Epoch: [199]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000004  loss: 1.5558 (1.5530)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2683 (7.8484)  time: 0.9138 (0.5329 -- 3.4808)  data: 0.2700 (0.0004 -- 2.9534)  max mem: 16413
Epoch: [199]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000004  loss: 1.8098 (1.6148)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6268 (8.0851)  time: 0.8906 (0.5328 -- 2.1600)  data: 0.2762 (0.0003 -- 1.6423)  max mem: 16413
Epoch: [199]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000004  loss: 1.6130 (1.6030)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6952 (7.8041)  time: 0.8624 (0.5273 -- 2.2892)  data: 0.3082 (0.0002 -- 1.7477)  max mem: 16413
Epoch: [199]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000004  loss: 1.5565 (1.6049)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4616 (7.7718)  time: 0.9114 (0.5231 -- 2.8762)  data: 0.3604 (0.0003 -- 2.3655)  max mem: 16413
[2023-09-05 05:54:45,871] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:54:45,872] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 05:54:45,872] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:54:45,872] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [199]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000004  loss: 1.5139 (1.5906)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4075 (7.6763)  time: 0.9710 (0.5280 -- 3.0677)  data: 0.4248 (0.0003 -- 2.5515)  max mem: 16413
[2023-09-05 05:55:03,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=180, lr=[3.60656816740113e-06, 3.60656816740113e-06, 4.007297963779033e-06, 4.007297963779033e-06, 4.452553293087814e-06, 4.452553293087814e-06, 4.947281436764238e-06, 4.947281436764238e-06, 5.496979374182487e-06, 5.496979374182487e-06, 6.107754860202763e-06, 6.107754860202763e-06, 6.786394289114181e-06, 6.786394289114181e-06, 7.540438099015756e-06, 7.540438099015756e-06, 8.378264554461952e-06, 8.378264554461952e-06, 9.309182838291057e-06, 9.309182838291057e-06, 1.0343536486990064e-05, 1.0343536486990064e-05, 1.1492818318877847e-05, 1.1492818318877847e-05, 1.2769798132086496e-05, 1.2769798132086496e-05, 1.4188664591207218e-05, 1.4188664591207218e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 05:55:03,256] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=17.808713399835167, CurrSamplesPerSec=24.65905574537676, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [199]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 1.4861 (1.5837)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6395 (7.6306)  time: 0.5885 (0.4952 -- 1.9035)  data: 0.0708 (0.0001 -- 1.4053)  max mem: 16413
Epoch: [199] Total time: 0:02:23 (0.8962 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 1.4861 (1.5882)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6395 (7.6306)
[2023-09-05 05:55:03,259] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-09-05 05:55:03,261] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-09-05 05:55:03,261] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt
[2023-09-05 05:55:03,261] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt...
[2023-09-05 05:55:04,291] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt.
[2023-09-05 05:55:04,291] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1365 (0.1365)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5114 (2.5114 -- 2.5114)  data: 2.2988 (2.2988 -- 2.2988)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6153 (0.5343)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4300 (0.1999 -- 2.5114)  data: 0.2187 (0.0010 -- 2.2988)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5050 (0.5400)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2212 (0.1708 -- 0.4241)  data: 0.0172 (0.0001 -- 0.2339)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5136 (0.5741)  acc1: 77.7778 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2058 (0.1324 -- 0.4241)  data: 0.0168 (0.0001 -- 0.2339)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.588
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [200]  [  0/160]  eta: 0:19:36  lr: 0.000014  min_lr: 0.000004  loss: 0.9437 (0.9437)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6006 (9.6006)  time: 7.3532 (7.3532 -- 7.3532)  data: 6.4689 (6.4689 -- 6.4689)  max mem: 16413
[2023-09-05 05:55:35,330] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32016
[2023-09-05 05:55:35,330] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32016
[2023-09-05 05:55:35,331] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:55:35,331] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 05:55:35,331] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [200]  [ 20/160]  eta: 0:02:48  lr: 0.000014  min_lr: 0.000004  loss: 1.6268 (1.5728)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2341 (7.3676)  time: 0.8966 (0.5206 -- 4.7932)  data: 0.3451 (0.0007 -- 4.2887)  max mem: 16413
Epoch: [200]  [ 40/160]  eta: 0:02:13  lr: 0.000014  min_lr: 0.000004  loss: 1.4676 (1.5196)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4980 (7.2071)  time: 1.0181 (0.5091 -- 4.7574)  data: 0.4711 (0.0002 -- 4.2368)  max mem: 16413
Epoch: [200]  [ 60/160]  eta: 0:01:42  lr: 0.000014  min_lr: 0.000004  loss: 1.5358 (1.5538)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4016 (7.2358)  time: 0.8398 (0.5145 -- 3.6909)  data: 0.3052 (0.0003 -- 3.1654)  max mem: 16413
Epoch: [200]  [ 80/160]  eta: 0:01:18  lr: 0.000014  min_lr: 0.000004  loss: 1.4758 (1.5575)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3480 (7.2360)  time: 0.8455 (0.5270 -- 3.6112)  data: 0.2877 (0.0001 -- 3.0722)  max mem: 16413
[2023-09-05 05:56:40,427] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32090
[2023-09-05 05:56:40,428] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:56:40,427] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32090
[2023-09-05 05:56:40,428] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 05:56:40,428] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [200]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000004  loss: 1.6268 (1.5582)  loss_scale: 8192.0000 (18087.2871)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4723 (7.1470)  time: 0.8146 (0.5413 -- 3.8419)  data: 0.2642 (0.0005 -- 3.3236)  max mem: 16413
Epoch: [200]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000004  loss: 1.6875 (1.5772)  loss_scale: 8192.0000 (16451.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9130 (7.1398)  time: 0.9160 (0.5222 -- 4.7528)  data: 0.3656 (0.0005 -- 4.2217)  max mem: 16413
Epoch: [200]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000004  loss: 1.6309 (1.5801)  loss_scale: 8192.0000 (15280.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2472 (7.1777)  time: 0.7899 (0.5263 -- 4.2214)  data: 0.2435 (0.0001 -- 3.7101)  max mem: 16413
Epoch: [200]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000004  loss: 1.6252 (1.5899)  loss_scale: 8192.0000 (14438.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6250 (7.1449)  time: 0.7344 (0.4973 -- 4.3338)  data: 0.2205 (0.0001 -- 3.7818)  max mem: 16413
Epoch: [200] Total time: 0:02:23 (0.8996 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000004  loss: 1.6252 (1.5907)  loss_scale: 8192.0000 (14438.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6250 (7.1449)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1353 (0.1353)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3472 (2.3472 -- 2.3472)  data: 2.0750 (2.0750 -- 2.0750)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6663 (0.5370)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4058 (0.1989 -- 2.3472)  data: 0.1917 (0.0005 -- 2.0750)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4970 (0.5385)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (98.4127)  time: 0.2214 (0.1684 -- 0.4719)  data: 0.0219 (0.0001 -- 0.2465)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6091 (0.5845)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.2078 (0.1330 -- 0.4719)  data: 0.0217 (0.0001 -- 0.2465)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.603
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.31%
Epoch: [201]  [  0/160]  eta: 0:17:52  lr: 0.000014  min_lr: 0.000004  loss: 1.8768 (1.8768)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9727 (7.9727)  time: 6.7024 (6.7024 -- 6.7024)  data: 5.7198 (5.7198 -- 5.7198)  max mem: 16413
Epoch: [201]  [ 20/160]  eta: 0:02:52  lr: 0.000014  min_lr: 0.000004  loss: 1.8054 (1.7655)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6771 (7.1478)  time: 0.9549 (0.5321 -- 5.1739)  data: 0.0889 (0.0004 -- 1.3739)  max mem: 16413
Epoch: [201]  [ 40/160]  eta: 0:02:04  lr: 0.000014  min_lr: 0.000004  loss: 1.5516 (1.6793)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1312 (6.8068)  time: 0.8348 (0.5180 -- 3.3941)  data: 0.0014 (0.0005 -- 0.0027)  max mem: 16413
[2023-09-05 05:58:42,731] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:58:42,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 05:58:42,732] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 05:58:42,732] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [201]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000004  loss: 1.5414 (1.6367)  loss_scale: 8192.0000 (8460.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1440 (6.9237)  time: 0.8440 (0.5193 -- 3.2414)  data: 0.0353 (0.0002 -- 0.5013)  max mem: 16413
Epoch: [201]  [ 80/160]  eta: 0:01:14  lr: 0.000014  min_lr: 0.000004  loss: 1.3638 (1.5855)  loss_scale: 16384.0000 (10416.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6959 (6.8465)  time: 0.8204 (0.5279 -- 3.2535)  data: 0.2268 (0.0005 -- 2.7108)  max mem: 16413
Epoch: [201]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000004  loss: 1.7661 (1.5980)  loss_scale: 16384.0000 (11598.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2569 (6.9772)  time: 0.8643 (0.5392 -- 2.7075)  data: 0.2756 (0.0006 -- 2.1573)  max mem: 16413
Epoch: [201]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000003  loss: 1.7586 (1.5985)  loss_scale: 16384.0000 (12389.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3061 (6.9677)  time: 0.8018 (0.5278 -- 2.9593)  data: 0.1993 (0.0004 -- 2.4297)  max mem: 16413
Epoch: [201]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000003  loss: 1.4801 (1.5919)  loss_scale: 16384.0000 (12956.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8213 (6.9346)  time: 1.0225 (0.5219 -- 4.3406)  data: 0.0944 (0.0003 -- 1.1412)  max mem: 16413
Epoch: [201]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000003  loss: 1.5883 (1.5970)  loss_scale: 16384.0000 (13363.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7711 (6.9421)  time: 0.7225 (0.4966 -- 1.8522)  data: 0.1707 (0.0002 -- 1.3274)  max mem: 16413
Epoch: [201] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000003  loss: 1.5883 (1.6014)  loss_scale: 16384.0000 (13363.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7711 (6.9421)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1275 (0.1275)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3839 (2.3839 -- 2.3839)  data: 2.1738 (2.1738 -- 2.1738)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6534 (0.5417)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4107 (0.1985 -- 2.3839)  data: 0.1993 (0.0008 -- 2.1738)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5212 (0.5297)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2244 (0.1695 -- 0.6071)  data: 0.0214 (0.0001 -- 0.4068)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5382 (0.5771)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.9253)  time: 0.2117 (0.1329 -- 0.6071)  data: 0.0211 (0.0001 -- 0.4068)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 84.647 Acc@5 98.133 loss 0.582
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [202]  [  0/160]  eta: 0:22:09  lr: 0.000014  min_lr: 0.000003  loss: 1.7071 (1.7071)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3127 (8.3127)  time: 8.3103 (8.3103 -- 8.3103)  data: 7.7633 (7.7633 -- 7.7633)  max mem: 16413
Epoch: [202]  [ 20/160]  eta: 0:02:35  lr: 0.000014  min_lr: 0.000003  loss: 1.6363 (1.6232)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8421 (7.4758)  time: 0.7515 (0.5272 -- 3.1379)  data: 0.1575 (0.0002 -- 1.7573)  max mem: 16413
[2023-09-05 06:00:45,818] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:00:45,818] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:00:45,819] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:00:45,819] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [202]  [ 40/160]  eta: 0:02:02  lr: 0.000014  min_lr: 0.000003  loss: 1.6690 (1.6214)  loss_scale: 32768.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0332 (7.5943)  time: 0.9317 (0.5163 -- 4.4231)  data: 0.3626 (0.0004 -- 3.8741)  max mem: 16413
[2023-09-05 06:01:06,775] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32373
[2023-09-05 06:01:06,775] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32373
[2023-09-05 06:01:06,775] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:01:06,775] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:01:06,775] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [202]  [ 60/160]  eta: 0:01:36  lr: 0.000014  min_lr: 0.000003  loss: 1.6160 (1.5890)  loss_scale: 32768.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6924 (7.9421)  time: 0.8557 (0.5204 -- 3.6809)  data: 0.2720 (0.0006 -- 3.1383)  max mem: 16413
Epoch: [202]  [ 80/160]  eta: 0:01:14  lr: 0.000014  min_lr: 0.000003  loss: 1.5648 (1.5752)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6423 (7.7609)  time: 0.8236 (0.5227 -- 2.9121)  data: 0.2095 (0.0004 -- 1.8097)  max mem: 16413
Epoch: [202]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000003  loss: 1.6190 (1.5796)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9353 (7.6636)  time: 0.9422 (0.5286 -- 4.0290)  data: 0.1087 (0.0003 -- 0.7727)  max mem: 16413
Epoch: [202]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000003  loss: 1.5920 (1.5646)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1557 (7.5243)  time: 0.9142 (0.5265 -- 3.2937)  data: 0.0016 (0.0003 -- 0.0063)  max mem: 16413
Epoch: [202]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000003  loss: 1.5980 (1.5712)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2914 (7.5393)  time: 0.9370 (0.5086 -- 5.7362)  data: 0.0016 (0.0003 -- 0.0095)  max mem: 16413
Epoch: [202]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.6705 (1.5864)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5191 (7.4741)  time: 0.5921 (0.4969 -- 1.6672)  data: 0.0006 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [202] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.6705 (1.5868)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5191 (7.4741)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1374 (0.1374)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2892 (2.2892 -- 2.2892)  data: 2.0405 (2.0405 -- 2.0405)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6382 (0.5126)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4152 (0.1970 -- 2.2892)  data: 0.1995 (0.0005 -- 2.0405)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4915 (0.5310)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2259 (0.1705 -- 0.5016)  data: 0.0236 (0.0001 -- 0.3159)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5093 (0.5733)  acc1: 77.7778 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2113 (0.1332 -- 0.5016)  data: 0.0234 (0.0001 -- 0.3159)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.583
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [203]  [  0/160]  eta: 0:22:40  lr: 0.000013  min_lr: 0.000003  loss: 1.5518 (1.5518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9113 (4.9113)  time: 8.5030 (8.5030 -- 8.5030)  data: 7.9646 (7.9646 -- 7.9646)  max mem: 16413
Epoch: [203]  [ 20/160]  eta: 0:02:44  lr: 0.000013  min_lr: 0.000003  loss: 1.6897 (1.5739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9143 (6.7105)  time: 0.8123 (0.5396 -- 2.2891)  data: 0.1618 (0.0005 -- 1.7073)  max mem: 16413
[2023-09-05 06:03:11,222] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:03:11,222] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:03:11,223] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:03:11,223] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [203]  [ 40/160]  eta: 0:02:02  lr: 0.000013  min_lr: 0.000003  loss: 1.3764 (1.4937)  loss_scale: 32768.0000 (23976.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2211 (6.5314)  time: 0.8607 (0.5255 -- 2.8838)  data: 0.0380 (0.0003 -- 0.4589)  max mem: 16413
Epoch: [203]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000003  loss: 1.4721 (1.5272)  loss_scale: 32768.0000 (26859.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9771 (6.7553)  time: 0.8807 (0.5331 -- 4.5268)  data: 0.0019 (0.0002 -- 0.0063)  max mem: 16413
Epoch: [203]  [ 80/160]  eta: 0:01:16  lr: 0.000013  min_lr: 0.000003  loss: 1.5730 (1.5478)  loss_scale: 32768.0000 (28318.0247)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4230 (6.6900)  time: 0.8910 (0.5387 -- 2.7023)  data: 0.0490 (0.0006 -- 0.9525)  max mem: 16413
Epoch: [203]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000003  loss: 1.6996 (1.5781)  loss_scale: 32768.0000 (29199.2079)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7533 (6.7638)  time: 0.8415 (0.5237 -- 3.7388)  data: 0.0023 (0.0003 -- 0.0171)  max mem: 16413
Epoch: [203]  [120/160]  eta: 0:00:38  lr: 0.000013  min_lr: 0.000003  loss: 1.6421 (1.5699)  loss_scale: 32768.0000 (29789.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5519 (6.7373)  time: 1.0763 (0.5223 -- 4.5307)  data: 0.0014 (0.0001 -- 0.0049)  max mem: 16413
[2023-09-05 06:04:41,553] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32601
[2023-09-05 06:04:41,553] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:04:41,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 06:04:41,553] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32601
[2023-09-05 06:04:41,553] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [203]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000003  loss: 1.4970 (1.5613)  loss_scale: 16384.0000 (27887.6596)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2958 (6.9489)  time: 0.7387 (0.4865 -- 2.5684)  data: 0.0016 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [203]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.6322 (1.5728)  loss_scale: 16384.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9668 (7.0227)  time: 0.6444 (0.4975 -- 1.7379)  data: 0.0143 (0.0002 -- 0.2539)  max mem: 16413
Epoch: [203] Total time: 0:02:22 (0.8929 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.6322 (1.5897)  loss_scale: 16384.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9668 (7.0227)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.1274 (0.1274)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1216 (2.1216 -- 2.1216)  data: 1.9216 (1.9216 -- 1.9216)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5829 (0.5017)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4335 (0.1907 -- 2.1216)  data: 0.2171 (0.0006 -- 1.9216)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4810 (0.5061)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2348 (0.1702 -- 0.4764)  data: 0.0280 (0.0001 -- 0.2347)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5107 (0.5578)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2201 (0.1328 -- 0.4764)  data: 0.0277 (0.0001 -- 0.2347)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.572
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [204]  [  0/160]  eta: 0:22:40  lr: 0.000013  min_lr: 0.000003  loss: 1.8598 (1.8598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2856 (8.2856)  time: 8.5055 (8.5055 -- 8.5055)  data: 7.9522 (7.9522 -- 7.9522)  max mem: 16413
Epoch: [204]  [ 20/160]  eta: 0:02:48  lr: 0.000013  min_lr: 0.000003  loss: 1.4574 (1.4817)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8780 (7.4542)  time: 0.8421 (0.5314 -- 3.8959)  data: 0.1211 (0.0003 -- 2.1923)  max mem: 16413
Epoch: [204]  [ 40/160]  eta: 0:02:02  lr: 0.000013  min_lr: 0.000003  loss: 1.6588 (1.5626)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8396 (7.4376)  time: 0.8175 (0.5256 -- 2.5476)  data: 0.0018 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [204]  [ 60/160]  eta: 0:01:36  lr: 0.000013  min_lr: 0.000003  loss: 1.5942 (1.5522)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4278 (7.4100)  time: 0.8580 (0.5404 -- 2.8227)  data: 0.1406 (0.0005 -- 1.1975)  max mem: 16413
Epoch: [204]  [ 80/160]  eta: 0:01:14  lr: 0.000013  min_lr: 0.000003  loss: 1.5714 (1.5539)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2624 (7.2912)  time: 0.8222 (0.5363 -- 2.8444)  data: 0.1102 (0.0005 -- 0.9139)  max mem: 16413
[2023-09-05 06:06:41,358] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:06:41,359] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:06:41,359] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:06:41,360] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:06:45,983] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32736
[2023-09-05 06:06:45,983] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32736
[2023-09-05 06:06:45,984] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:06:45,984] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:06:45,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [204]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000003  loss: 1.6892 (1.5787)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5439 (7.1606)  time: 0.9476 (0.5327 -- 2.7152)  data: 0.3347 (0.0003 -- 1.6830)  max mem: 16413
Epoch: [204]  [120/160]  eta: 0:00:37  lr: 0.000013  min_lr: 0.000003  loss: 1.4460 (1.5568)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8313 (7.1257)  time: 0.9475 (0.5269 -- 2.7866)  data: 0.1865 (0.0006 -- 1.6537)  max mem: 16413
Epoch: [204]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000003  loss: 1.4864 (1.5631)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0719 (7.1090)  time: 0.8202 (0.5328 -- 2.5926)  data: 0.0822 (0.0003 -- 1.0406)  max mem: 16413
Epoch: [204]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.5551 (1.5676)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3186 (7.1707)  time: 0.7096 (0.4964 -- 2.5715)  data: 0.0614 (0.0002 -- 1.0406)  max mem: 16413
Epoch: [204] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.5551 (1.5763)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3186 (7.1707)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1281 (0.1281)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4521 (2.4521 -- 2.4521)  data: 2.1978 (2.1978 -- 2.1978)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5914 (0.5036)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4370 (0.1807 -- 2.4521)  data: 0.2213 (0.0007 -- 2.1978)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4700 (0.4991)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2157 (0.1700 -- 0.4381)  data: 0.0133 (0.0001 -- 0.2120)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5168 (0.5472)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2001 (0.1326 -- 0.4381)  data: 0.0123 (0.0001 -- 0.2120)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.31%
Epoch: [205]  [  0/160]  eta: 0:28:17  lr: 0.000013  min_lr: 0.000003  loss: 1.8188 (1.8188)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8648 (8.8648)  time: 10.6065 (10.6065 -- 10.6065)  data: 7.1020 (7.1020 -- 7.1020)  max mem: 16413
Epoch: [205]  [ 20/160]  eta: 0:02:58  lr: 0.000013  min_lr: 0.000003  loss: 1.5790 (1.6339)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7881 (6.9983)  time: 0.8096 (0.5072 -- 3.9267)  data: 0.0966 (0.0004 -- 1.8958)  max mem: 16413
Epoch: [205]  [ 40/160]  eta: 0:02:14  lr: 0.000013  min_lr: 0.000003  loss: 1.6526 (1.6094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2185 (7.3059)  time: 0.9631 (0.5219 -- 3.9883)  data: 0.0647 (0.0002 -- 0.5889)  max mem: 16413
Epoch: [205]  [ 60/160]  eta: 0:01:42  lr: 0.000013  min_lr: 0.000003  loss: 1.6304 (1.6096)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6046 (7.2398)  time: 0.8106 (0.5222 -- 3.9326)  data: 0.0578 (0.0002 -- 0.9811)  max mem: 16413
[2023-09-05 06:08:53,579] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:08:53,579] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:08:53,580] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:08:53,581] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [205]  [ 80/160]  eta: 0:01:20  lr: 0.000013  min_lr: 0.000003  loss: 1.5669 (1.5922)  loss_scale: 32768.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0146 (7.2771)  time: 0.9537 (0.5307 -- 3.7919)  data: 0.0490 (0.0001 -- 0.9513)  max mem: 16413
Epoch: [205]  [100/160]  eta: 0:00:58  lr: 0.000013  min_lr: 0.000003  loss: 1.6325 (1.5786)  loss_scale: 32768.0000 (22223.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6531 (7.1429)  time: 0.8402 (0.5138 -- 4.0905)  data: 0.0016 (0.0002 -- 0.0040)  max mem: 16413
[2023-09-05 06:09:35,877] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32916
[2023-09-05 06:09:35,877] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 32916
[2023-09-05 06:09:35,877] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:09:35,877] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:09:35,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [205]  [120/160]  eta: 0:00:38  lr: 0.000013  min_lr: 0.000003  loss: 1.5100 (1.5838)  loss_scale: 32768.0000 (23289.6529)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9248 (7.1268)  time: 0.8562 (0.5214 -- 3.6985)  data: 0.0018 (0.0003 -- 0.0088)  max mem: 16413
Epoch: [205]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000003  loss: 1.5620 (1.5844)  loss_scale: 16384.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8001 (7.1876)  time: 0.7972 (0.5172 -- 4.7392)  data: 0.0018 (0.0003 -- 0.0089)  max mem: 16413
Epoch: [205]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000003  loss: 1.4772 (1.5699)  loss_scale: 16384.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0806 (7.1310)  time: 0.7298 (0.4957 -- 4.4464)  data: 0.0013 (0.0002 -- 0.0089)  max mem: 16413
Epoch: [205] Total time: 0:02:25 (0.9077 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000003  loss: 1.4772 (1.5885)  loss_scale: 16384.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0806 (7.1310)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4554 (2.4554 -- 2.4554)  data: 2.1817 (2.1817 -- 2.1817)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4855 (0.5116)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4225 (0.1875 -- 2.4554)  data: 0.1995 (0.0008 -- 2.1817)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4635 (0.5015)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.9418)  time: 0.2217 (0.1706 -- 0.4667)  data: 0.0148 (0.0001 -- 0.2806)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4852 (0.5534)  acc1: 77.7778 (83.8174)  acc5: 100.0000 (98.3402)  time: 0.2045 (0.1331 -- 0.4667)  data: 0.0145 (0.0001 -- 0.2806)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 84.025 Acc@5 98.133 loss 0.575
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 86.31%
Epoch: [206]  [  0/160]  eta: 0:23:44  lr: 0.000013  min_lr: 0.000003  loss: 2.1734 (2.1734)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2354 (6.2354)  time: 8.9045 (8.9045 -- 8.9045)  data: 8.3787 (8.3787 -- 8.3787)  max mem: 16413
Epoch: [206]  [ 20/160]  eta: 0:02:42  lr: 0.000013  min_lr: 0.000003  loss: 1.4695 (1.4923)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8669 (7.6885)  time: 0.7746 (0.5203 -- 3.3047)  data: 0.2287 (0.0004 -- 2.7650)  max mem: 16413
[2023-09-05 06:10:59,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=33000, skipped=186, lr=[3.2157238564233544e-06, 3.2157238564233544e-06, 3.573026507137061e-06, 3.573026507137061e-06, 3.970029452374511e-06, 3.970029452374511e-06, 4.41114383597168e-06, 4.41114383597168e-06, 4.901270928857421e-06, 4.901270928857421e-06, 5.445856587619357e-06, 5.445856587619357e-06, 6.050951764021508e-06, 6.050951764021508e-06, 6.7232797378016745e-06, 6.7232797378016745e-06, 7.470310819779639e-06, 7.470310819779639e-06, 8.300345355310708e-06, 8.300345355310708e-06, 9.222605950345233e-06, 9.222605950345233e-06, 1.0247339944828037e-05, 1.0247339944828037e-05, 1.138593327203115e-05, 1.138593327203115e-05, 1.26510369689235e-05, 1.26510369689235e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 06:10:59,568] [INFO] [timer.py:260:stop] epoch=0/micro_step=33000/global_step=33000, RunningAvgSamplesPerSec=17.78608343555095, CurrSamplesPerSec=22.248470992621513, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [206]  [ 40/160]  eta: 0:02:07  lr: 0.000013  min_lr: 0.000003  loss: 1.5018 (1.5411)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7439 (7.4436)  time: 0.9602 (0.5148 -- 3.3290)  data: 0.4000 (0.0002 -- 2.7959)  max mem: 16413
Epoch: [206]  [ 60/160]  eta: 0:01:41  lr: 0.000013  min_lr: 0.000003  loss: 1.6759 (1.5729)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2808 (7.2587)  time: 0.9086 (0.5182 -- 2.3793)  data: 0.1642 (0.0003 -- 1.7580)  max mem: 16413
Epoch: [206]  [ 80/160]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000003  loss: 1.6147 (1.5857)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1922 (7.2570)  time: 0.8185 (0.5190 -- 4.2080)  data: 0.0220 (0.0001 -- 0.4212)  max mem: 16413
[2023-09-05 06:11:42,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:11:42,077] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:11:42,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:11:42,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [206]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000003  loss: 1.6941 (1.6127)  loss_scale: 32768.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6253 (7.1686)  time: 0.8548 (0.5309 -- 2.6198)  data: 0.0764 (0.0002 -- 0.8608)  max mem: 16413
Epoch: [206]  [120/160]  eta: 0:00:37  lr: 0.000013  min_lr: 0.000003  loss: 1.4918 (1.6106)  loss_scale: 32768.0000 (21258.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5015 (7.2331)  time: 0.9299 (0.5181 -- 4.0060)  data: 0.0842 (0.0004 -- 1.1562)  max mem: 16413
[2023-09-05 06:12:25,324] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33095
[2023-09-05 06:12:25,325] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33095
[2023-09-05 06:12:25,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:12:25,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:12:25,325] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [206]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000003  loss: 1.6468 (1.6105)  loss_scale: 32768.0000 (22193.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2642 (7.2272)  time: 0.9868 (0.5301 -- 5.0889)  data: 0.0015 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [206]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.5177 (1.6109)  loss_scale: 16384.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1434 (7.2539)  time: 0.5257 (0.4953 -- 0.5911)  data: 0.0006 (0.0001 -- 0.0016)  max mem: 16413
Epoch: [206] Total time: 0:02:23 (0.8972 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.5177 (1.6043)  loss_scale: 16384.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1434 (7.2539)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1311 (0.1311)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3379 (2.3379 -- 2.3379)  data: 2.1272 (2.1272 -- 2.1272)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4069 (0.5023)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4200 (0.2050 -- 2.3379)  data: 0.2011 (0.0008 -- 2.1272)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4069 (0.4960)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (99.4709)  time: 0.2243 (0.1690 -- 0.4298)  data: 0.0159 (0.0001 -- 0.2283)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4510 (0.5451)  acc1: 77.7778 (83.8174)  acc5: 100.0000 (98.7552)  time: 0.2080 (0.1322 -- 0.4298)  data: 0.0153 (0.0001 -- 0.2283)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 84.440 Acc@5 98.548 loss 0.578
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [207]  [  0/160]  eta: 0:21:12  lr: 0.000012  min_lr: 0.000003  loss: 1.8615 (1.8615)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7712 (5.7712)  time: 7.9510 (7.9510 -- 7.9510)  data: 5.1296 (5.1296 -- 5.1296)  max mem: 16413
Epoch: [207]  [ 20/160]  eta: 0:02:56  lr: 0.000012  min_lr: 0.000003  loss: 1.4267 (1.5612)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3689 (8.1298)  time: 0.9240 (0.5203 -- 4.9409)  data: 0.1065 (0.0005 -- 1.0712)  max mem: 16413
Epoch: [207]  [ 40/160]  eta: 0:02:10  lr: 0.000012  min_lr: 0.000003  loss: 1.4609 (1.5373)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8603 (7.6847)  time: 0.9035 (0.5186 -- 3.3980)  data: 0.0414 (0.0002 -- 0.5048)  max mem: 16413
Epoch: [207]  [ 60/160]  eta: 0:01:41  lr: 0.000012  min_lr: 0.000003  loss: 1.5789 (1.5676)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7086 (7.4763)  time: 0.8579 (0.5270 -- 4.1231)  data: 0.1874 (0.0004 -- 2.1427)  max mem: 16413
Epoch: [207]  [ 80/160]  eta: 0:01:19  lr: 0.000012  min_lr: 0.000003  loss: 1.6127 (1.5749)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3021 (7.2711)  time: 0.9292 (0.5191 -- 4.6578)  data: 0.0228 (0.0003 -- 0.4295)  max mem: 16413
Epoch: [207]  [100/160]  eta: 0:00:58  lr: 0.000012  min_lr: 0.000003  loss: 1.5959 (1.5660)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5365 (7.3572)  time: 0.9014 (0.5258 -- 4.7318)  data: 0.0016 (0.0003 -- 0.0048)  max mem: 16413
[2023-09-05 06:14:29,299] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33221
[2023-09-05 06:14:29,299] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33221
[2023-09-05 06:14:29,300] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 06:14:29,300] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 06:14:29,300] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [207]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000003  loss: 1.7567 (1.5906)  loss_scale: 8192.0000 (15029.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9872 (7.4637)  time: 0.7294 (0.5288 -- 2.3178)  data: 0.0019 (0.0002 -- 0.0097)  max mem: 16413
Epoch: [207]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000003  loss: 1.5688 (1.5873)  loss_scale: 8192.0000 (14060.0284)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8688 (7.4979)  time: 0.9056 (0.5448 -- 2.4354)  data: 0.1950 (0.0004 -- 1.8797)  max mem: 16413
Epoch: [207]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.4684 (1.5842)  loss_scale: 8192.0000 (13363.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5880 (7.4658)  time: 0.7635 (0.4961 -- 2.0253)  data: 0.0895 (0.0002 -- 1.2763)  max mem: 16413
Epoch: [207] Total time: 0:02:25 (0.9084 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.4684 (1.5897)  loss_scale: 8192.0000 (13363.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5880 (7.4658)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.1330 (0.1330)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1295 (2.1295 -- 2.1295)  data: 1.8471 (1.8471 -- 1.8471)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5412 (0.5188)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4245 (0.1936 -- 2.1295)  data: 0.1962 (0.0005 -- 1.8471)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4701 (0.5219)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.9418)  time: 0.2333 (0.1711 -- 0.5187)  data: 0.0206 (0.0001 -- 0.2435)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4701 (0.5649)  acc1: 77.7778 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2202 (0.1334 -- 0.5187)  data: 0.0203 (0.0001 -- 0.2435)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.577
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [208]  [  0/160]  eta: 0:16:49  lr: 0.000012  min_lr: 0.000003  loss: 1.2238 (1.2238)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4956 (5.4956)  time: 6.3107 (6.3107 -- 6.3107)  data: 5.6087 (5.6087 -- 5.6087)  max mem: 16413
Epoch: [208]  [ 20/160]  eta: 0:02:55  lr: 0.000012  min_lr: 0.000003  loss: 1.8322 (1.7860)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6721 (7.4758)  time: 0.9992 (0.5236 -- 4.4388)  data: 0.4007 (0.0004 -- 3.9422)  max mem: 16413
Epoch: [208]  [ 40/160]  eta: 0:02:00  lr: 0.000012  min_lr: 0.000003  loss: 1.7104 (1.7349)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2697 (7.0893)  time: 0.7517 (0.5271 -- 3.2603)  data: 0.1939 (0.0006 -- 2.7101)  max mem: 16413
Epoch: [208]  [ 60/160]  eta: 0:01:38  lr: 0.000012  min_lr: 0.000003  loss: 1.5376 (1.6779)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2078 (7.1615)  time: 0.9396 (0.5339 -- 2.5632)  data: 0.1982 (0.0002 -- 2.0226)  max mem: 16413
[2023-09-05 06:16:33,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:16:33,568] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 06:16:33,569] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:16:33,569] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [208]  [ 80/160]  eta: 0:01:17  lr: 0.000012  min_lr: 0.000003  loss: 1.4106 (1.6317)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6672 (7.2346)  time: 0.9022 (0.5184 -- 3.5766)  data: 0.0314 (0.0002 -- 0.5903)  max mem: 16413
Epoch: [208]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000003  loss: 1.5739 (1.6131)  loss_scale: 16384.0000 (10706.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2755 (7.2387)  time: 0.8608 (0.5204 -- 3.8357)  data: 0.0016 (0.0004 -- 0.0060)  max mem: 16413
Epoch: [208]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000003  loss: 1.6036 (1.6079)  loss_scale: 16384.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5931 (7.1816)  time: 0.8444 (0.5349 -- 1.6471)  data: 0.0710 (0.0005 -- 0.7406)  max mem: 16413
Epoch: [208]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000003  loss: 1.3843 (1.5844)  loss_scale: 16384.0000 (12317.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8673 (7.1919)  time: 0.9684 (0.5279 -- 3.1289)  data: 0.2722 (0.0003 -- 2.6084)  max mem: 16413
Epoch: [208]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.4807 (1.5798)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4632 (7.2505)  time: 0.6479 (0.4963 -- 2.5399)  data: 0.1046 (0.0002 -- 2.0162)  max mem: 16413
Epoch: [208] Total time: 0:02:24 (0.9005 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.4807 (1.5901)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4632 (7.2505)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1371 (0.1371)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5624 (2.5624 -- 2.5624)  data: 2.3280 (2.3280 -- 2.3280)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4726 (0.5118)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4355 (0.1997 -- 2.5624)  data: 0.2151 (0.0006 -- 2.3280)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4726 (0.5252)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2142 (0.1702 -- 0.3269)  data: 0.0094 (0.0001 -- 0.1465)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4732 (0.5684)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (98.3402)  time: 0.1981 (0.1332 -- 0.3269)  data: 0.0092 (0.0001 -- 0.1465)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.575
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.31%
Epoch: [209]  [  0/160]  eta: 0:20:34  lr: 0.000012  min_lr: 0.000003  loss: 1.7990 (1.7990)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4278 (5.4278)  time: 7.7133 (7.7133 -- 7.7133)  data: 5.0261 (5.0261 -- 5.0261)  max mem: 16413
Epoch: [209]  [ 20/160]  eta: 0:02:57  lr: 0.000012  min_lr: 0.000003  loss: 1.6919 (1.6070)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4024 (7.0334)  time: 0.9493 (0.5292 -- 4.0934)  data: 0.1819 (0.0006 -- 1.9512)  max mem: 16413
[2023-09-05 06:18:35,656] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:18:35,657] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:18:35,661] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:18:35,662] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [209]  [ 40/160]  eta: 0:02:05  lr: 0.000012  min_lr: 0.000003  loss: 1.6171 (1.6199)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0951 (7.0552)  time: 0.8010 (0.5157 -- 2.0398)  data: 0.0933 (0.0001 -- 1.1515)  max mem: 16413
Epoch: [209]  [ 60/160]  eta: 0:01:36  lr: 0.000012  min_lr: 0.000003  loss: 1.6265 (1.6169)  loss_scale: 32768.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2010 (7.2681)  time: 0.8140 (0.5273 -- 3.6785)  data: 0.0326 (0.0002 -- 0.6284)  max mem: 16413
Epoch: [209]  [ 80/160]  eta: 0:01:14  lr: 0.000012  min_lr: 0.000003  loss: 1.7705 (1.6429)  loss_scale: 32768.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6531 (7.3977)  time: 0.8257 (0.5293 -- 3.1679)  data: 0.1053 (0.0002 -- 0.9602)  max mem: 16413
Epoch: [209]  [100/160]  eta: 0:00:57  lr: 0.000012  min_lr: 0.000003  loss: 1.6703 (1.6496)  loss_scale: 32768.0000 (26603.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3089 (7.2179)  time: 1.0535 (0.5189 -- 4.0337)  data: 0.4665 (0.0003 -- 3.5245)  max mem: 16413
Epoch: [209]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000003  loss: 1.6564 (1.6407)  loss_scale: 32768.0000 (27622.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1175 (7.0779)  time: 0.8282 (0.5198 -- 4.5221)  data: 0.2824 (0.0003 -- 4.0027)  max mem: 16413
Epoch: [209]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000003  loss: 1.3844 (1.6118)  loss_scale: 32768.0000 (28352.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3713 (7.0819)  time: 0.9329 (0.5285 -- 3.6479)  data: 0.3812 (0.0004 -- 3.1258)  max mem: 16413
Epoch: [209]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.5779 (1.6040)  loss_scale: 32768.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1590 (7.1061)  time: 0.6309 (0.5004 -- 2.1243)  data: 0.0574 (0.0002 -- 1.1332)  max mem: 16413
Epoch: [209] Total time: 0:02:23 (0.8994 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.5779 (1.5953)  loss_scale: 32768.0000 (28876.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1590 (7.1061)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1432 (0.1432)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3360 (2.3360 -- 2.3360)  data: 2.0726 (2.0726 -- 2.0726)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5134 (0.5310)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4234 (0.2030 -- 2.3360)  data: 0.1980 (0.0007 -- 2.0726)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4892 (0.5379)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2267 (0.1699 -- 0.3484)  data: 0.0184 (0.0001 -- 0.1766)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4892 (0.5845)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (98.3402)  time: 0.2083 (0.1333 -- 0.3484)  data: 0.0181 (0.0001 -- 0.1766)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 84.232 Acc@5 98.133 loss 0.581
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.31%
Epoch: [210]  [  0/160]  eta: 0:20:29  lr: 0.000012  min_lr: 0.000003  loss: 1.2778 (1.2778)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5593 (5.5593)  time: 7.6835 (7.6835 -- 7.6835)  data: 6.7623 (6.7623 -- 6.7623)  max mem: 16413
[2023-09-05 06:20:38,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:20:38,754] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 06:20:38,754] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:20:38,754] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 06:20:39,325] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33607
[2023-09-05 06:20:39,325] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33607
[2023-09-05 06:20:39,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 06:20:39,325] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 06:20:39,325] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [210]  [ 20/160]  eta: 0:02:48  lr: 0.000012  min_lr: 0.000003  loss: 1.6986 (1.6514)  loss_scale: 32768.0000 (34328.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1559 (7.1222)  time: 0.8773 (0.5397 -- 3.6517)  data: 0.3258 (0.0004 -- 3.1055)  max mem: 16413
Epoch: [210]  [ 40/160]  eta: 0:02:07  lr: 0.000012  min_lr: 0.000003  loss: 1.5195 (1.5855)  loss_scale: 32768.0000 (33567.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9054 (7.1374)  time: 0.9189 (0.5290 -- 4.1296)  data: 0.1356 (0.0002 -- 1.6977)  max mem: 16413
[2023-09-05 06:21:13,169] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33644
[2023-09-05 06:21:13,169] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33644
[2023-09-05 06:21:13,169] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:21:13,169] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:21:13,169] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [210]  [ 60/160]  eta: 0:01:41  lr: 0.000012  min_lr: 0.000003  loss: 1.7865 (1.6264)  loss_scale: 16384.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6183 (7.1117)  time: 0.9239 (0.5170 -- 5.1972)  data: 0.0020 (0.0003 -- 0.0121)  max mem: 16413
Epoch: [210]  [ 80/160]  eta: 0:01:19  lr: 0.000012  min_lr: 0.000003  loss: 1.6144 (1.6143)  loss_scale: 16384.0000 (25688.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7630 (7.2791)  time: 0.9259 (0.5152 -- 4.2225)  data: 0.0010 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [210]  [100/160]  eta: 0:00:57  lr: 0.000012  min_lr: 0.000003  loss: 1.5498 (1.6060)  loss_scale: 16384.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6885 (7.4921)  time: 0.8161 (0.5266 -- 3.9445)  data: 0.0012 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [210]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000003  loss: 1.4941 (1.5941)  loss_scale: 16384.0000 (22612.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7192 (7.3054)  time: 0.8508 (0.5266 -- 2.9444)  data: 0.0018 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [210]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000003  loss: 1.5581 (1.5918)  loss_scale: 16384.0000 (21729.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0612 (7.3293)  time: 0.7082 (0.5301 -- 1.9922)  data: 0.0451 (0.0004 -- 0.8740)  max mem: 16413
Epoch: [210]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000003  loss: 1.5358 (1.5883)  loss_scale: 16384.0000 (21094.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0916 (7.3652)  time: 0.7727 (0.4974 -- 2.4387)  data: 0.0845 (0.0002 -- 1.2394)  max mem: 16413
Epoch: [210] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000003  loss: 1.5358 (1.5986)  loss_scale: 16384.0000 (21094.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0916 (7.3652)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1363 (0.1363)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5045 (2.5045 -- 2.5045)  data: 2.2961 (2.2961 -- 2.2961)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6104 (0.5608)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4409 (0.2156 -- 2.5045)  data: 0.2105 (0.0005 -- 2.2961)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4947 (0.5504)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.9418)  time: 0.2216 (0.1699 -- 0.3791)  data: 0.0113 (0.0001 -- 0.2047)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4947 (0.5889)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (98.3402)  time: 0.1981 (0.1329 -- 0.3791)  data: 0.0105 (0.0001 -- 0.2047)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 84.440 Acc@5 98.133 loss 0.587
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [211]  [  0/160]  eta: 0:21:56  lr: 0.000012  min_lr: 0.000003  loss: 1.9700 (1.9700)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9835 (8.9835)  time: 8.2265 (8.2265 -- 8.2265)  data: 5.1130 (5.1130 -- 5.1130)  max mem: 16413
[2023-09-05 06:23:16,860] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:23:16,860] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:23:16,860] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:23:16,860] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [211]  [ 20/160]  eta: 0:03:04  lr: 0.000011  min_lr: 0.000003  loss: 1.3870 (1.4732)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8300 (7.8534)  time: 0.9753 (0.5216 -- 3.4027)  data: 0.2013 (0.0002 -- 2.8747)  max mem: 16413
Epoch: [211]  [ 40/160]  eta: 0:02:06  lr: 0.000011  min_lr: 0.000003  loss: 1.3954 (1.4752)  loss_scale: 32768.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3694 (7.4752)  time: 0.7672 (0.5278 -- 3.6157)  data: 0.2093 (0.0003 -- 3.0476)  max mem: 16413
Epoch: [211]  [ 60/160]  eta: 0:01:43  lr: 0.000011  min_lr: 0.000003  loss: 1.7181 (1.5242)  loss_scale: 32768.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2271 (7.6732)  time: 1.0104 (0.5140 -- 4.9376)  data: 0.4601 (0.0003 -- 4.3891)  max mem: 16413
Epoch: [211]  [ 80/160]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000003  loss: 1.5603 (1.5327)  loss_scale: 32768.0000 (30138.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8327 (7.2364)  time: 0.7620 (0.5125 -- 2.8097)  data: 0.2226 (0.0003 -- 2.2762)  max mem: 16413
Epoch: [211]  [100/160]  eta: 0:00:57  lr: 0.000011  min_lr: 0.000003  loss: 1.6229 (1.5511)  loss_scale: 32768.0000 (30659.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8296 (7.3009)  time: 0.8920 (0.5177 -- 3.2107)  data: 0.1210 (0.0003 -- 1.2621)  max mem: 16413
Epoch: [211]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000003  loss: 1.7015 (1.5721)  loss_scale: 32768.0000 (31007.7355)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6755 (7.3000)  time: 0.8465 (0.5236 -- 3.6716)  data: 0.0553 (0.0004 -- 1.0813)  max mem: 16413
Epoch: [211]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.7459 (1.5829)  loss_scale: 32768.0000 (31257.4184)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2194 (7.2798)  time: 0.8975 (0.5296 -- 3.1864)  data: 0.0524 (0.0002 -- 0.9969)  max mem: 16413
[2023-09-05 06:25:09,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:25:09,754] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 06:25:09,756] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:25:09,757] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 06:25:13,067] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33907
[2023-09-05 06:25:13,067] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 33907
[2023-09-05 06:25:13,067] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 06:25:13,068] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 06:25:13,068] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [211]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.4555 (1.5743)  loss_scale: 32768.0000 (32665.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2022 (7.1727)  time: 0.7032 (0.4981 -- 2.3604)  data: 0.0911 (0.0002 -- 1.5438)  max mem: 16413
Epoch: [211] Total time: 0:02:22 (0.8936 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.4555 (1.5866)  loss_scale: 32768.0000 (32665.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2022 (7.1727)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1347 (0.1347)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3911 (2.3911 -- 2.3911)  data: 2.1562 (2.1562 -- 2.1562)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5515 (0.5150)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4163 (0.1935 -- 2.3911)  data: 0.2032 (0.0007 -- 2.1562)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4987 (0.5310)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2221 (0.1718 -- 0.4393)  data: 0.0209 (0.0001 -- 0.2506)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4987 (0.5736)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2090 (0.1334 -- 0.4393)  data: 0.0202 (0.0001 -- 0.2506)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.594
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.31%
Epoch: [212]  [  0/160]  eta: 0:20:05  lr: 0.000011  min_lr: 0.000003  loss: 1.4439 (1.4439)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9392 (4.9392)  time: 7.5313 (7.5313 -- 7.5313)  data: 6.9767 (6.9767 -- 6.9767)  max mem: 16413
Epoch: [212]  [ 20/160]  eta: 0:02:47  lr: 0.000011  min_lr: 0.000003  loss: 1.5495 (1.5214)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6274 (6.8520)  time: 0.8800 (0.5177 -- 4.5160)  data: 0.3259 (0.0004 -- 3.9960)  max mem: 16413
Epoch: [212]  [ 40/160]  eta: 0:02:02  lr: 0.000011  min_lr: 0.000003  loss: 1.3490 (1.4657)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3375 (7.1817)  time: 0.8415 (0.5284 -- 3.2083)  data: 0.2892 (0.0002 -- 2.6623)  max mem: 16413
Epoch: [212]  [ 60/160]  eta: 0:01:35  lr: 0.000011  min_lr: 0.000003  loss: 1.5803 (1.5009)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8967 (6.9376)  time: 0.8068 (0.5353 -- 1.9912)  data: 0.0507 (0.0004 -- 0.9384)  max mem: 16413
[2023-09-05 06:26:44,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=34000, skipped=191, lr=[2.8394045128578726e-06, 2.8394045128578726e-06, 3.1548939031754142e-06, 3.1548939031754142e-06, 3.5054376701949044e-06, 3.5054376701949044e-06, 3.894930744661005e-06, 3.894930744661005e-06, 4.3277008274011165e-06, 4.3277008274011165e-06, 4.8085564748901294e-06, 4.8085564748901294e-06, 5.342840527655699e-06, 5.342840527655699e-06, 5.936489475172999e-06, 5.936489475172999e-06, 6.596099416858887e-06, 6.596099416858887e-06, 7.32899935206543e-06, 7.32899935206543e-06, 8.143332613406034e-06, 8.143332613406034e-06, 9.048147348228925e-06, 9.048147348228925e-06, 1.0053497053587695e-05, 1.0053497053587695e-05, 1.1170552281764105e-05, 1.1170552281764105e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 06:26:44,197] [INFO] [timer.py:260:stop] epoch=0/micro_step=34000/global_step=34000, RunningAvgSamplesPerSec=17.753890306984708, CurrSamplesPerSec=4.833833924009202, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [212]  [ 80/160]  eta: 0:01:15  lr: 0.000011  min_lr: 0.000003  loss: 1.6408 (1.5180)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6523 (6.9987)  time: 0.8960 (0.5314 -- 3.3363)  data: 0.0351 (0.0007 -- 0.5550)  max mem: 16413
Epoch: [212]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000003  loss: 1.5566 (1.5247)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4449 (6.9459)  time: 0.8696 (0.5279 -- 3.5022)  data: 0.1654 (0.0004 -- 2.9804)  max mem: 16413
[2023-09-05 06:27:15,720] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:27:15,720] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:27:15,720] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 06:27:15,720] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [212]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000003  loss: 1.6141 (1.5295)  loss_scale: 32768.0000 (34122.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0115 (6.8128)  time: 0.8896 (0.5299 -- 2.5487)  data: 0.2726 (0.0003 -- 1.9593)  max mem: 16413
[2023-09-05 06:27:21,762] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34043
[2023-09-05 06:27:21,762] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 06:27:21,764] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34043
[2023-09-05 06:27:21,765] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 06:27:21,766] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-05 06:27:34,126] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34057
[2023-09-05 06:27:34,126] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34057
[2023-09-05 06:27:34,127] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:27:34,127] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:27:34,127] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [212]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.7096 (1.5377)  loss_scale: 32768.0000 (33929.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2272 (6.7350)  time: 0.8312 (0.5200 -- 2.8018)  data: 0.0497 (0.0002 -- 0.5874)  max mem: 16413
Epoch: [212]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.8141 (1.5559)  loss_scale: 16384.0000 (31846.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6967 (6.8103)  time: 0.7613 (0.4969 -- 2.5979)  data: 0.0193 (0.0000 -- 0.3664)  max mem: 16413
Epoch: [212] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.8141 (1.5572)  loss_scale: 16384.0000 (31846.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6967 (6.8103)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1330 (0.1330)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4489 (2.4489 -- 2.4489)  data: 2.2015 (2.2015 -- 2.2015)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5411 (0.5341)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4301 (0.2153 -- 2.4489)  data: 0.2024 (0.0004 -- 2.2015)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4884 (0.5254)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2243 (0.1694 -- 0.3609)  data: 0.0091 (0.0001 -- 0.1553)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4884 (0.5810)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.9253)  time: 0.2038 (0.1330 -- 0.3609)  data: 0.0088 (0.0001 -- 0.1553)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.586
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.31%
Epoch: [213]  [  0/160]  eta: 0:24:50  lr: 0.000011  min_lr: 0.000003  loss: 1.4545 (1.4545)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5687 (9.5687)  time: 9.3130 (9.3130 -- 9.3130)  data: 8.7638 (8.7638 -- 8.7638)  max mem: 16413
Epoch: [213]  [ 20/160]  eta: 0:02:43  lr: 0.000011  min_lr: 0.000003  loss: 1.5809 (1.6877)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2608 (7.5637)  time: 0.7574 (0.5239 -- 3.3505)  data: 0.2041 (0.0003 -- 2.8028)  max mem: 16413
Epoch: [213]  [ 40/160]  eta: 0:02:05  lr: 0.000011  min_lr: 0.000003  loss: 1.5793 (1.6239)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4068 (7.4530)  time: 0.9162 (0.5284 -- 3.6113)  data: 0.2884 (0.0004 -- 3.0602)  max mem: 16413
Epoch: [213]  [ 60/160]  eta: 0:01:44  lr: 0.000011  min_lr: 0.000003  loss: 1.6140 (1.6438)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6719 (7.1636)  time: 1.0502 (0.4990 -- 5.9861)  data: 0.0025 (0.0004 -- 0.0207)  max mem: 16413
Epoch: [213]  [ 80/160]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000003  loss: 1.4911 (1.6127)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8923 (6.9616)  time: 0.7130 (0.5245 -- 2.6595)  data: 0.0010 (0.0004 -- 0.0016)  max mem: 16413
Epoch: [213]  [100/160]  eta: 0:00:57  lr: 0.000011  min_lr: 0.000003  loss: 1.3836 (1.5749)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3131 (6.8893)  time: 0.9510 (0.5313 -- 3.7469)  data: 0.0018 (0.0004 -- 0.0036)  max mem: 16413
[2023-09-05 06:29:40,992] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:29:40,992] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:29:40,993] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:29:40,993] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [213]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000003  loss: 1.4005 (1.5324)  loss_scale: 32768.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2965 (6.7634)  time: 0.8134 (0.5121 -- 4.2532)  data: 0.0011 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [213]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.5415 (1.5329)  loss_scale: 32768.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2696 (6.8855)  time: 0.9394 (0.5225 -- 4.2363)  data: 0.0012 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [213]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.5595 (1.5358)  loss_scale: 32768.0000 (21913.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8396 (6.9167)  time: 0.6234 (0.4954 -- 2.6161)  data: 0.0008 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [213] Total time: 0:02:24 (0.9004 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.5595 (1.5536)  loss_scale: 32768.0000 (21913.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8396 (6.9167)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1340 (0.1340)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5501 (2.5501 -- 2.5501)  data: 2.2968 (2.2968 -- 2.2968)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5796 (0.5154)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4325 (0.2011 -- 2.5501)  data: 0.2099 (0.0007 -- 2.2968)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3953 (0.5092)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2106 (0.1714 -- 0.2589)  data: 0.0016 (0.0001 -- 0.0123)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4351 (0.5689)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (98.3402)  time: 0.1941 (0.1344 -- 0.2360)  data: 0.0013 (0.0001 -- 0.0123)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 84.232 Acc@5 98.133 loss 0.582
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.31%
Epoch: [214]  [  0/160]  eta: 0:16:31  lr: 0.000011  min_lr: 0.000003  loss: 0.8478 (0.8478)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1495 (7.1495)  time: 6.1961 (6.1961 -- 6.1961)  data: 5.6710 (5.6710 -- 5.6710)  max mem: 16413
[2023-09-05 06:30:50,640] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34255
[2023-09-05 06:30:50,641] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:30:50,641] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34255
[2023-09-05 06:30:50,641] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 06:30:50,641] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [214]  [ 20/160]  eta: 0:02:47  lr: 0.000011  min_lr: 0.000003  loss: 1.5184 (1.5262)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7858 (7.0370)  time: 0.9460 (0.5288 -- 3.2237)  data: 0.2327 (0.0003 -- 2.6381)  max mem: 16413
Epoch: [214]  [ 40/160]  eta: 0:02:13  lr: 0.000011  min_lr: 0.000003  loss: 1.7293 (1.5809)  loss_scale: 16384.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5879 (6.7367)  time: 1.0230 (0.5261 -- 4.0899)  data: 0.3857 (0.0002 -- 3.5526)  max mem: 16413
Epoch: [214]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000003  loss: 1.4166 (1.5768)  loss_scale: 16384.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5191 (6.8138)  time: 0.6935 (0.5329 -- 2.3724)  data: 0.1395 (0.0002 -- 1.8373)  max mem: 16413
Epoch: [214]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000003  loss: 1.7250 (1.6079)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3227 (6.8982)  time: 0.8777 (0.5408 -- 2.4558)  data: 0.2981 (0.0005 -- 1.9194)  max mem: 16413
Epoch: [214]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000003  loss: 1.5319 (1.5883)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8153 (6.8657)  time: 0.8355 (0.5367 -- 2.8904)  data: 0.0725 (0.0002 -- 0.7615)  max mem: 16413
Epoch: [214]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000003  loss: 1.3298 (1.5645)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4033 (6.8624)  time: 0.8648 (0.5267 -- 2.5270)  data: 0.0710 (0.0005 -- 1.3918)  max mem: 16413
Epoch: [214]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000003  loss: 1.7377 (1.5887)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1541 (6.8651)  time: 0.8640 (0.5187 -- 2.5403)  data: 0.1621 (0.0004 -- 1.9885)  max mem: 16413
[2023-09-05 06:32:44,371] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:32:44,371] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:32:44,371] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:32:44,371] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:32:53,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34397
[2023-09-05 06:32:53,320] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:32:53,319] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34397
[2023-09-05 06:32:53,320] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:32:53,320] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [214]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000003  loss: 1.5780 (1.5883)  loss_scale: 32768.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5812 (6.9587)  time: 0.7930 (0.4829 -- 3.6068)  data: 0.0008 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [214] Total time: 0:02:23 (0.8975 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000003  loss: 1.5780 (1.5567)  loss_scale: 32768.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5812 (6.9587)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1330 (0.1330)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5054 (2.5054 -- 2.5054)  data: 2.2819 (2.2819 -- 2.2819)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5089 (0.5109)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4249 (0.1903 -- 2.5054)  data: 0.2086 (0.0008 -- 2.2819)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5089 (0.5241)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2169 (0.1694 -- 0.4041)  data: 0.0121 (0.0001 -- 0.2258)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5208 (0.5764)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2022 (0.1325 -- 0.4041)  data: 0.0118 (0.0001 -- 0.2258)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.571
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [215]  [  0/160]  eta: 0:22:46  lr: 0.000011  min_lr: 0.000003  loss: 1.8446 (1.8446)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6008 (11.6008)  time: 8.5434 (8.5434 -- 8.5434)  data: 7.5729 (7.5729 -- 7.5729)  max mem: 16413
Epoch: [215]  [ 20/160]  eta: 0:02:47  lr: 0.000011  min_lr: 0.000003  loss: 1.4824 (1.5366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3372 (7.7676)  time: 0.8266 (0.5239 -- 4.9684)  data: 0.2748 (0.0002 -- 4.4528)  max mem: 16413
Epoch: [215]  [ 40/160]  eta: 0:02:07  lr: 0.000011  min_lr: 0.000003  loss: 1.4964 (1.5211)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9713 (7.4968)  time: 0.9208 (0.5350 -- 2.9387)  data: 0.3679 (0.0003 -- 2.3873)  max mem: 16413
Epoch: [215]  [ 60/160]  eta: 0:01:38  lr: 0.000011  min_lr: 0.000003  loss: 1.6098 (1.5361)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6383 (7.5503)  time: 0.8427 (0.5192 -- 3.3607)  data: 0.2943 (0.0003 -- 2.8241)  max mem: 16413
Epoch: [215]  [ 80/160]  eta: 0:01:15  lr: 0.000010  min_lr: 0.000003  loss: 1.6270 (1.5513)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2429 (7.3004)  time: 0.8032 (0.5280 -- 2.9062)  data: 0.2399 (0.0003 -- 2.3570)  max mem: 16413
Epoch: [215]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000003  loss: 1.7157 (1.5678)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8687 (7.2865)  time: 0.8741 (0.5250 -- 2.7807)  data: 0.2890 (0.0004 -- 2.2413)  max mem: 16413
Epoch: [215]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000003  loss: 1.4857 (1.5621)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1068 (7.4420)  time: 0.9257 (0.5247 -- 3.2235)  data: 0.0768 (0.0004 -- 0.7587)  max mem: 16413
[2023-09-05 06:34:59,241] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:34:59,241] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:34:59,241] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:34:59,241] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [215]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000003  loss: 1.4391 (1.5571)  loss_scale: 32768.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4402 (7.3587)  time: 0.7918 (0.5273 -- 2.4714)  data: 0.2370 (0.0004 -- 1.9056)  max mem: 16413
Epoch: [215]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.5897 (1.5644)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2828 (7.3346)  time: 0.7831 (0.4968 -- 4.3650)  data: 0.2612 (0.0002 -- 3.8545)  max mem: 16413
Epoch: [215] Total time: 0:02:23 (0.8951 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.5897 (1.5544)  loss_scale: 32768.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2828 (7.3346)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1416 (0.1416)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2262 (2.2262 -- 2.2262)  data: 2.0016 (2.0016 -- 2.0016)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5759 (0.5298)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4267 (0.1955 -- 2.2262)  data: 0.2146 (0.0007 -- 2.0016)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4892 (0.5262)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2302 (0.1688 -- 0.5781)  data: 0.0305 (0.0001 -- 0.3482)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4892 (0.5771)  acc1: 77.7778 (82.9876)  acc5: 100.0000 (97.9253)  time: 0.2151 (0.1333 -- 0.5781)  data: 0.0302 (0.0001 -- 0.3482)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 84.440 Acc@5 97.718 loss 0.565
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.31%
Epoch: [216]  [  0/160]  eta: 0:19:07  lr: 0.000010  min_lr: 0.000003  loss: 1.5864 (1.5864)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3766 (3.3766)  time: 7.1737 (7.1737 -- 7.1737)  data: 6.6352 (6.6352 -- 6.6352)  max mem: 16413
[2023-09-05 06:35:45,053] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34567
[2023-09-05 06:35:45,053] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:35:45,053] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34567
[2023-09-05 06:35:45,054] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:35:45,054] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [216]  [ 20/160]  eta: 0:02:40  lr: 0.000010  min_lr: 0.000003  loss: 1.5269 (1.5009)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5337 (6.7011)  time: 0.8418 (0.5247 -- 2.9344)  data: 0.1519 (0.0008 -- 1.8272)  max mem: 16413
Epoch: [216]  [ 40/160]  eta: 0:02:00  lr: 0.000010  min_lr: 0.000003  loss: 1.4446 (1.5404)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6536 (6.8596)  time: 0.8536 (0.5274 -- 3.0121)  data: 0.0314 (0.0005 -- 0.5850)  max mem: 16413
Epoch: [216]  [ 60/160]  eta: 0:01:39  lr: 0.000010  min_lr: 0.000003  loss: 1.5429 (1.5412)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9819 (7.0180)  time: 0.9865 (0.5282 -- 3.8353)  data: 0.0375 (0.0004 -- 0.7217)  max mem: 16413
Epoch: [216]  [ 80/160]  eta: 0:01:15  lr: 0.000010  min_lr: 0.000003  loss: 1.3269 (1.5137)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2709 (6.9511)  time: 0.7638 (0.5280 -- 2.2180)  data: 0.0678 (0.0002 -- 0.8334)  max mem: 16413
Epoch: [216]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000003  loss: 1.4744 (1.5164)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1040 (6.8826)  time: 0.9299 (0.5352 -- 4.2194)  data: 0.0382 (0.0003 -- 0.7408)  max mem: 16413
Epoch: [216]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000003  loss: 1.6973 (1.5445)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4686 (6.8145)  time: 0.8858 (0.5179 -- 4.2169)  data: 0.0731 (0.0004 -- 0.8863)  max mem: 16413
[2023-09-05 06:37:38,334] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:37:38,335] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:37:38,334] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:37:38,335] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [216]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000003  loss: 1.5183 (1.5555)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9868 (6.8491)  time: 0.8583 (0.5317 -- 2.8735)  data: 0.0015 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [216]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.7172 (1.5749)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0503 (6.9226)  time: 0.7594 (0.4966 -- 2.8080)  data: 0.0011 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [216] Total time: 0:02:21 (0.8874 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.7172 (1.5747)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0503 (6.9226)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1325 (0.1325)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2531 (2.2531 -- 2.2531)  data: 2.0362 (2.0362 -- 2.0362)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4504 (0.5212)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4080 (0.2081 -- 2.2531)  data: 0.1922 (0.0003 -- 2.0362)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4691 (0.5140)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2279 (0.1694 -- 0.3490)  data: 0.0193 (0.0001 -- 0.1462)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4691 (0.5597)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2120 (0.1328 -- 0.3490)  data: 0.0190 (0.0001 -- 0.1462)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 85.477 Acc@5 97.925 loss 0.561
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.31%
Epoch: [217]  [  0/160]  eta: 0:21:09  lr: 0.000010  min_lr: 0.000003  loss: 1.3257 (1.3257)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6711 (7.6711)  time: 7.9323 (7.9323 -- 7.9323)  data: 5.3872 (5.3872 -- 5.3872)  max mem: 16413
Epoch: [217]  [ 20/160]  eta: 0:02:55  lr: 0.000010  min_lr: 0.000003  loss: 1.5189 (1.5782)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7650 (7.3853)  time: 0.9232 (0.5327 -- 2.5701)  data: 0.2267 (0.0004 -- 1.4976)  max mem: 16413
[2023-09-05 06:38:40,770] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34752
[2023-09-05 06:38:40,770] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 34752
[2023-09-05 06:38:40,770] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:38:40,770] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:38:40,770] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [217]  [ 40/160]  eta: 0:02:14  lr: 0.000010  min_lr: 0.000003  loss: 1.5512 (1.5973)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3997 (6.9654)  time: 0.9716 (0.5130 -- 4.1131)  data: 0.0212 (0.0003 -- 0.3956)  max mem: 16413
Epoch: [217]  [ 60/160]  eta: 0:01:39  lr: 0.000010  min_lr: 0.000003  loss: 1.7404 (1.6154)  loss_scale: 16384.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5907 (7.0961)  time: 0.7334 (0.5313 -- 2.4410)  data: 0.0018 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [217]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000003  loss: 1.4451 (1.5741)  loss_scale: 16384.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3539 (7.1496)  time: 0.8790 (0.5242 -- 3.9408)  data: 0.0023 (0.0002 -- 0.0148)  max mem: 16413
Epoch: [217]  [100/160]  eta: 0:00:58  lr: 0.000010  min_lr: 0.000003  loss: 1.5235 (1.5691)  loss_scale: 16384.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1546 (7.1037)  time: 0.9844 (0.5227 -- 4.8351)  data: 0.0011 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [217]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000003  loss: 1.5748 (1.5725)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5351 (7.0554)  time: 0.7956 (0.5216 -- 2.5990)  data: 0.0857 (0.0004 -- 1.6786)  max mem: 16413
Epoch: [217]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000003  loss: 1.4980 (1.5600)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9449 (6.9927)  time: 0.9355 (0.5295 -- 3.0574)  data: 0.3505 (0.0006 -- 2.5074)  max mem: 16413
Epoch: [217]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000003  loss: 1.6751 (1.5658)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9678 (6.9703)  time: 0.6467 (0.4943 -- 3.1044)  data: 0.1309 (0.0002 -- 2.6044)  max mem: 16413
Epoch: [217] Total time: 0:02:24 (0.9048 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000003  loss: 1.6751 (1.5579)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9678 (6.9703)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1362 (0.1362)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5013 (2.5013 -- 2.5013)  data: 2.2732 (2.2732 -- 2.2732)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4104 (0.5110)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4430 (0.2044 -- 2.5013)  data: 0.2215 (0.0006 -- 2.2732)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5053 (0.5099)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.9418)  time: 0.2184 (0.1690 -- 0.4064)  data: 0.0110 (0.0001 -- 0.1539)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5053 (0.5589)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (98.3402)  time: 0.2013 (0.1327 -- 0.4064)  data: 0.0107 (0.0001 -- 0.1539)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 84.647 Acc@5 98.133 loss 0.566
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [218]  [  0/160]  eta: 0:22:54  lr: 0.000010  min_lr: 0.000003  loss: 0.9852 (0.9852)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7936 (5.7936)  time: 8.5908 (8.5908 -- 8.5908)  data: 8.0222 (8.0222 -- 8.0222)  max mem: 16413
[2023-09-05 06:40:44,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:40:44,775] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:40:44,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:40:44,775] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [218]  [ 20/160]  eta: 0:02:40  lr: 0.000010  min_lr: 0.000003  loss: 1.5678 (1.6411)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6029 (7.2151)  time: 0.7713 (0.5224 -- 3.9949)  data: 0.1698 (0.0005 -- 2.3551)  max mem: 16413
Epoch: [218]  [ 40/160]  eta: 0:02:05  lr: 0.000010  min_lr: 0.000003  loss: 1.5369 (1.5962)  loss_scale: 32768.0000 (32368.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4404 (7.7576)  time: 0.9477 (0.5206 -- 4.2154)  data: 0.1446 (0.0003 -- 1.2119)  max mem: 16413
Epoch: [218]  [ 60/160]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000003  loss: 1.7457 (1.6356)  loss_scale: 32768.0000 (32499.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8984 (7.5906)  time: 0.9113 (0.5328 -- 4.6578)  data: 0.1443 (0.0006 -- 1.9214)  max mem: 16413
Epoch: [218]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000002  loss: 1.6617 (1.6465)  loss_scale: 32768.0000 (32565.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4740 (7.3815)  time: 0.8469 (0.5280 -- 3.3419)  data: 0.0017 (0.0006 -- 0.0067)  max mem: 16413
Epoch: [218]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000002  loss: 1.4599 (1.6183)  loss_scale: 32768.0000 (32605.7822)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7401 (7.4739)  time: 0.8432 (0.5194 -- 2.2465)  data: 0.1284 (0.0003 -- 1.4457)  max mem: 16413
[2023-09-05 06:42:26,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=35000, skipped=197, lr=[2.4795994234491456e-06, 2.4795994234491456e-06, 2.7551104704990512e-06, 2.7551104704990512e-06, 3.0612338561100563e-06, 3.0612338561100563e-06, 3.401370951233396e-06, 3.401370951233396e-06, 3.7793010569259955e-06, 3.7793010569259955e-06, 4.199223396584439e-06, 4.199223396584439e-06, 4.66580377398271e-06, 4.66580377398271e-06, 5.184226415536344e-06, 5.184226415536344e-06, 5.76025157281816e-06, 5.76025157281816e-06, 6.4002795253535106e-06, 6.4002795253535106e-06, 7.111421694837235e-06, 7.111421694837235e-06, 7.901579660930261e-06, 7.901579660930261e-06, 8.779532956589178e-06, 8.779532956589178e-06, 9.75503661843242e-06, 9.75503661843242e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 06:42:26,288] [INFO] [timer.py:260:stop] epoch=0/micro_step=35000/global_step=35000, RunningAvgSamplesPerSec=17.727501652110526, CurrSamplesPerSec=22.09072834253421, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [218]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000002  loss: 1.5941 (1.6196)  loss_scale: 32768.0000 (32632.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2604 (7.3633)  time: 0.8098 (0.5286 -- 2.9172)  data: 0.2513 (0.0004 -- 2.3198)  max mem: 16413
[2023-09-05 06:42:33,500] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:42:33,500] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 06:42:33,500] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:42:33,500] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 06:42:37,060] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35014
[2023-09-05 06:42:37,060] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 06:42:37,060] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35014
[2023-09-05 06:42:37,060] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-05 06:42:37,060] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
Epoch: [218]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000002  loss: 1.4338 (1.6090)  loss_scale: 32768.0000 (33813.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5498 (7.3391)  time: 0.8131 (0.5320 -- 2.0372)  data: 0.0943 (0.0003 -- 0.9314)  max mem: 16413
[2023-09-05 06:42:53,615] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35032
[2023-09-05 06:42:53,615] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35032
[2023-09-05 06:42:53,615] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:42:53,615] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:42:53,616] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [218]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000002  loss: 1.6544 (1.6162)  loss_scale: 32768.0000 (32870.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5945 (7.3809)  time: 0.7732 (0.4875 -- 3.5750)  data: 0.0786 (0.0002 -- 1.0397)  max mem: 16413
Epoch: [218] Total time: 0:02:22 (0.8901 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000002  loss: 1.6544 (1.5831)  loss_scale: 32768.0000 (32870.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5945 (7.3809)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1279 (0.1279)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5135 (2.5135 -- 2.5135)  data: 2.2808 (2.2808 -- 2.2808)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4713 (0.5300)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (100.0000)  time: 0.4384 (0.1984 -- 2.5135)  data: 0.2186 (0.0002 -- 2.2808)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4586 (0.4997)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2202 (0.1690 -- 0.3528)  data: 0.0153 (0.0001 -- 0.1801)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4586 (0.5619)  acc1: 77.7778 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.2036 (0.1335 -- 0.3528)  data: 0.0151 (0.0001 -- 0.1801)  max mem: 16413
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 84.025 Acc@5 97.718 loss 0.570
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 86.31%
Epoch: [219]  [  0/160]  eta: 0:24:18  lr: 0.000010  min_lr: 0.000002  loss: 0.9570 (0.9570)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3763 (5.3763)  time: 9.1134 (9.1134 -- 9.1134)  data: 7.8426 (7.8426 -- 7.8426)  max mem: 16413
Epoch: [219]  [ 20/160]  eta: 0:02:46  lr: 0.000010  min_lr: 0.000002  loss: 1.5632 (1.5042)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8218 (7.0026)  time: 0.7916 (0.5234 -- 3.2164)  data: 0.1608 (0.0003 -- 2.1537)  max mem: 16413
Epoch: [219]  [ 40/160]  eta: 0:02:08  lr: 0.000010  min_lr: 0.000002  loss: 1.2623 (1.5063)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9238 (7.0443)  time: 0.9467 (0.5227 -- 3.4132)  data: 0.3873 (0.0003 -- 2.8682)  max mem: 16413
Epoch: [219]  [ 60/160]  eta: 0:01:38  lr: 0.000010  min_lr: 0.000002  loss: 1.6113 (1.5242)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7894 (7.1976)  time: 0.8021 (0.5175 -- 3.6771)  data: 0.2243 (0.0003 -- 2.6570)  max mem: 16413
Epoch: [219]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000002  loss: 1.5470 (1.5365)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0838 (7.2264)  time: 0.9363 (0.5166 -- 4.1182)  data: 0.3487 (0.0004 -- 3.5789)  max mem: 16413
Epoch: [219]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000002  loss: 1.7341 (1.5606)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2454 (7.1865)  time: 0.8230 (0.5346 -- 3.5948)  data: 0.2671 (0.0003 -- 3.0510)  max mem: 16413
Epoch: [219]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000002  loss: 1.4883 (1.5598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7496 (7.1785)  time: 0.9490 (0.5171 -- 4.3925)  data: 0.4025 (0.0002 -- 3.8667)  max mem: 16413
[2023-09-05 06:45:00,612] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:45:00,612] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:45:00,612] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:45:00,612] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [219]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000002  loss: 1.5292 (1.5569)  loss_scale: 32768.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7063 (7.1429)  time: 0.7441 (0.5245 -- 2.4789)  data: 0.1497 (0.0002 -- 1.8752)  max mem: 16413
Epoch: [219]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.7098 (1.5738)  loss_scale: 32768.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4420 (7.0695)  time: 0.6841 (0.4958 -- 2.0706)  data: 0.1306 (0.0002 -- 1.3939)  max mem: 16413
Epoch: [219] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.7098 (1.5969)  loss_scale: 32768.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4420 (7.0695)
[2023-09-05 06:45:28,129] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-219 is about to be saved!
[2023-09-05 06:45:28,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-219 is ready now!
[2023-09-05 06:45:28,133] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-219/mp_rank_00_model_states.pt
[2023-09-05 06:45:28,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-219/mp_rank_00_model_states.pt...
[2023-09-05 06:45:29,256] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-219/mp_rank_00_model_states.pt.
[2023-09-05 06:45:29,257] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-219 is ready now!
Val:  [ 0/27]  eta: 0:01:11  loss: 0.1251 (0.1251)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6635 (2.6635 -- 2.6635)  data: 2.4060 (2.4060 -- 2.4060)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5074 (0.5134)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4374 (0.1864 -- 2.6635)  data: 0.2232 (0.0007 -- 2.4060)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4446 (0.5008)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.9418)  time: 0.2096 (0.1693 -- 0.2951)  data: 0.0080 (0.0001 -- 0.1081)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4446 (0.5438)  acc1: 77.7778 (83.8174)  acc5: 100.0000 (98.3402)  time: 0.1968 (0.1333 -- 0.2951)  data: 0.0075 (0.0001 -- 0.1081)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.561
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 86.31%
Epoch: [220]  [  0/160]  eta: 0:19:10  lr: 0.000009  min_lr: 0.000002  loss: 1.5451 (1.5451)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4558 (8.4558)  time: 7.1876 (7.1876 -- 7.1876)  data: 6.6671 (6.6671 -- 6.6671)  max mem: 16413
[2023-09-05 06:45:48,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35204
[2023-09-05 06:45:48,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35204
[2023-09-05 06:45:48,294] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:45:48,294] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:45:48,294] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [220]  [ 20/160]  eta: 0:02:52  lr: 0.000009  min_lr: 0.000002  loss: 1.6824 (1.6078)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4419 (7.7279)  time: 0.9368 (0.5326 -- 3.5987)  data: 0.3008 (0.0003 -- 3.0599)  max mem: 16413
Epoch: [220]  [ 40/160]  eta: 0:02:09  lr: 0.000009  min_lr: 0.000002  loss: 1.6604 (1.6279)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6428 (7.5186)  time: 0.9159 (0.5166 -- 3.7645)  data: 0.3682 (0.0003 -- 3.2207)  max mem: 16413
Epoch: [220]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000002  loss: 1.6220 (1.6497)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1497 (7.1357)  time: 0.7875 (0.5259 -- 1.6897)  data: 0.1468 (0.0005 -- 0.9219)  max mem: 16413
Epoch: [220]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000002  loss: 1.4424 (1.6158)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8210 (7.1248)  time: 0.8951 (0.5274 -- 3.5403)  data: 0.0782 (0.0002 -- 1.3038)  max mem: 16413
Epoch: [220]  [100/160]  eta: 0:00:57  lr: 0.000009  min_lr: 0.000002  loss: 1.4911 (1.5943)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8492 (7.1482)  time: 0.9674 (0.5281 -- 4.2979)  data: 0.3353 (0.0004 -- 3.7783)  max mem: 16413
Epoch: [220]  [120/160]  eta: 0:00:38  lr: 0.000009  min_lr: 0.000002  loss: 1.5379 (1.5812)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4389 (7.1317)  time: 0.9245 (0.5191 -- 3.6468)  data: 0.3703 (0.0002 -- 3.1274)  max mem: 16413
[2023-09-05 06:47:42,891] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:47:42,891] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:47:42,893] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:47:42,894] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [220]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000002  loss: 1.6417 (1.5851)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5332 (7.1916)  time: 0.8970 (0.5254 -- 4.7112)  data: 0.3460 (0.0003 -- 4.1700)  max mem: 16413
Epoch: [220]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.6198 (1.5896)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6655 (7.1471)  time: 0.6587 (0.4949 -- 2.6754)  data: 0.1459 (0.0002 -- 2.1706)  max mem: 16413
Epoch: [220] Total time: 0:02:26 (0.9145 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.6198 (1.5854)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6655 (7.1471)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1323 (0.1323)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4761 (2.4761 -- 2.4761)  data: 2.2299 (2.2299 -- 2.2299)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5414 (0.5145)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4247 (0.1997 -- 2.4761)  data: 0.2045 (0.0004 -- 2.2299)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4721 (0.5072)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.9418)  time: 0.2209 (0.1694 -- 0.4686)  data: 0.0148 (0.0001 -- 0.2733)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4721 (0.5490)  acc1: 77.7778 (84.6473)  acc5: 100.0000 (98.3402)  time: 0.2050 (0.1322 -- 0.4686)  data: 0.0146 (0.0001 -- 0.2733)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 84.647 Acc@5 98.133 loss 0.560
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.31%
Epoch: [221]  [  0/160]  eta: 0:21:30  lr: 0.000009  min_lr: 0.000002  loss: 1.5300 (1.5300)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8034 (7.8034)  time: 8.0674 (8.0674 -- 8.0674)  data: 5.9535 (5.9535 -- 5.9535)  max mem: 16413
[2023-09-05 06:48:26,337] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35369
[2023-09-05 06:48:26,337] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35369
[2023-09-05 06:48:26,337] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:48:26,337] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:48:26,337] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [221]  [ 20/160]  eta: 0:02:43  lr: 0.000009  min_lr: 0.000002  loss: 1.5445 (1.4971)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0037 (7.2412)  time: 0.8240 (0.5178 -- 3.1756)  data: 0.0955 (0.0004 -- 1.0321)  max mem: 16413
Epoch: [221]  [ 40/160]  eta: 0:02:05  lr: 0.000009  min_lr: 0.000002  loss: 1.4869 (1.5185)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3656 (6.7952)  time: 0.9140 (0.5187 -- 3.2860)  data: 0.0012 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [221]  [ 60/160]  eta: 0:01:37  lr: 0.000009  min_lr: 0.000002  loss: 1.5565 (1.5197)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7414 (7.1525)  time: 0.8281 (0.5317 -- 2.6530)  data: 0.0018 (0.0009 -- 0.0055)  max mem: 16413
Epoch: [221]  [ 80/160]  eta: 0:01:19  lr: 0.000009  min_lr: 0.000002  loss: 1.6854 (1.5573)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8111 (7.2320)  time: 1.0528 (0.5140 -- 5.2229)  data: 0.0012 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [221]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000002  loss: 1.4801 (1.5359)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4551 (7.2141)  time: 0.7678 (0.5261 -- 3.9444)  data: 0.0023 (0.0004 -- 0.0168)  max mem: 16413
Epoch: [221]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000002  loss: 1.5756 (1.5419)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5289 (7.1822)  time: 0.9120 (0.5233 -- 4.5353)  data: 0.0018 (0.0003 -- 0.0054)  max mem: 16413
[2023-09-05 06:50:19,741] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:50:19,741] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:50:19,742] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:50:19,742] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [221]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000002  loss: 1.6054 (1.5588)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0415 (7.1655)  time: 0.7691 (0.5428 -- 2.9121)  data: 0.0022 (0.0003 -- 0.0137)  max mem: 16413
Epoch: [221]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.3675 (1.5430)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6032 (7.1291)  time: 0.6945 (0.4962 -- 1.6359)  data: 0.0897 (0.0001 -- 1.0999)  max mem: 16413
Epoch: [221] Total time: 0:02:22 (0.8924 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.3675 (1.5413)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6032 (7.1291)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1332 (0.1332)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4666 (2.4666 -- 2.4666)  data: 2.2490 (2.2490 -- 2.2490)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5183 (0.4935)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4264 (0.1991 -- 2.4666)  data: 0.2130 (0.0006 -- 2.2490)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4961 (0.5040)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2156 (0.1704 -- 0.3196)  data: 0.0110 (0.0001 -- 0.0869)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4961 (0.5422)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.9253)  time: 0.2015 (0.1329 -- 0.3196)  data: 0.0107 (0.0001 -- 0.0869)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.552
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [222]  [  0/160]  eta: 0:20:07  lr: 0.000009  min_lr: 0.000002  loss: 0.9301 (0.9301)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4572 (6.4572)  time: 7.5454 (7.5454 -- 7.5454)  data: 6.5865 (6.5865 -- 6.5865)  max mem: 16413
Epoch: [222]  [ 20/160]  eta: 0:02:43  lr: 0.000009  min_lr: 0.000002  loss: 1.3129 (1.3675)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2977 (7.0870)  time: 0.8498 (0.5196 -- 3.8781)  data: 0.1212 (0.0001 -- 1.6393)  max mem: 16413
[2023-09-05 06:51:16,656] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35552
[2023-09-05 06:51:16,656] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35552
[2023-09-05 06:51:16,657] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:51:16,657] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:51:16,657] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [222]  [ 40/160]  eta: 0:02:04  lr: 0.000009  min_lr: 0.000002  loss: 1.6262 (1.5027)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2915 (7.1284)  time: 0.8917 (0.5225 -- 2.9482)  data: 0.0563 (0.0003 -- 0.8990)  max mem: 16413
Epoch: [222]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000002  loss: 1.6156 (1.5409)  loss_scale: 16384.0000 (24978.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4750 (6.9909)  time: 0.8806 (0.5390 -- 2.7214)  data: 0.2967 (0.0002 -- 2.1564)  max mem: 16413
Epoch: [222]  [ 80/160]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000002  loss: 1.7848 (1.5733)  loss_scale: 16384.0000 (22856.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1341 (7.4296)  time: 0.8411 (0.5387 -- 2.7886)  data: 0.2124 (0.0002 -- 2.2529)  max mem: 16413
Epoch: [222]  [100/160]  eta: 0:00:57  lr: 0.000009  min_lr: 0.000002  loss: 1.6294 (1.5889)  loss_scale: 16384.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3517 (7.3805)  time: 0.9607 (0.5364 -- 3.6339)  data: 0.4080 (0.0004 -- 3.0939)  max mem: 16413
Epoch: [222]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000002  loss: 1.5886 (1.5898)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6486 (7.3046)  time: 0.7191 (0.5277 -- 2.4739)  data: 0.1660 (0.0002 -- 1.9114)  max mem: 16413
Epoch: [222]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000002  loss: 1.5514 (1.5885)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0933 (7.2616)  time: 0.8852 (0.5367 -- 2.5860)  data: 0.3352 (0.0006 -- 2.0591)  max mem: 16413
Epoch: [222]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.4419 (1.5701)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3709 (7.2711)  time: 0.6853 (0.4968 -- 3.0027)  data: 0.1565 (0.0002 -- 2.4402)  max mem: 16413
Epoch: [222] Total time: 0:02:21 (0.8829 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.4419 (1.5738)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3709 (7.2711)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1392 (0.1392)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3786 (2.3786 -- 2.3786)  data: 2.1456 (2.1456 -- 2.1456)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5322 (0.4962)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4257 (0.1975 -- 2.3786)  data: 0.2108 (0.0005 -- 2.1456)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4990 (0.4989)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2214 (0.1689 -- 0.4016)  data: 0.0184 (0.0001 -- 0.1653)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4990 (0.5435)  acc1: 77.7778 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2064 (0.1332 -- 0.4016)  data: 0.0182 (0.0001 -- 0.1653)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 84.855 Acc@5 98.133 loss 0.570
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.31%
Epoch: [223]  [  0/160]  eta: 0:22:35  lr: 0.000009  min_lr: 0.000002  loss: 1.8179 (1.8179)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3366 (11.3366)  time: 8.4730 (8.4730 -- 8.4730)  data: 7.9040 (7.9040 -- 7.9040)  max mem: 16413
[2023-09-05 06:53:20,088] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:53:20,089] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:53:20,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:53:20,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [223]  [ 20/160]  eta: 0:02:46  lr: 0.000009  min_lr: 0.000002  loss: 1.6063 (1.5631)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7394 (6.9237)  time: 0.8244 (0.5275 -- 3.3001)  data: 0.1951 (0.0006 -- 2.7089)  max mem: 16413
[2023-09-05 06:53:37,739] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35703
[2023-09-05 06:53:37,739] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35703
[2023-09-05 06:53:37,739] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:53:37,739] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 06:53:37,739] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [223]  [ 40/160]  eta: 0:02:07  lr: 0.000009  min_lr: 0.000002  loss: 1.5829 (1.5437)  loss_scale: 16384.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0469 (7.5690)  time: 0.9276 (0.5305 -- 2.8041)  data: 0.2920 (0.0001 -- 2.2615)  max mem: 16413
Epoch: [223]  [ 60/160]  eta: 0:01:42  lr: 0.000009  min_lr: 0.000002  loss: 1.4928 (1.5479)  loss_scale: 16384.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7347 (7.3470)  time: 0.9561 (0.5149 -- 3.2653)  data: 0.0889 (0.0004 -- 0.9613)  max mem: 16413
Epoch: [223]  [ 80/160]  eta: 0:01:18  lr: 0.000009  min_lr: 0.000002  loss: 1.5849 (1.5866)  loss_scale: 16384.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4865 (7.3265)  time: 0.8298 (0.5260 -- 2.6606)  data: 0.2110 (0.0001 -- 2.1341)  max mem: 16413
Epoch: [223]  [100/160]  eta: 0:00:57  lr: 0.000009  min_lr: 0.000002  loss: 1.4200 (1.5567)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8665 (7.2744)  time: 0.8733 (0.5264 -- 3.3097)  data: 0.2191 (0.0004 -- 2.7592)  max mem: 16413
[2023-09-05 06:55:02,945] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35800
[2023-09-05 06:55:02,945] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 35800
[2023-09-05 06:55:02,945] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 06:55:02,945] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 06:55:02,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [223]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000002  loss: 1.7407 (1.5757)  loss_scale: 16384.0000 (19295.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5395 (7.2378)  time: 0.7587 (0.5311 -- 2.5205)  data: 0.0650 (0.0002 -- 0.6198)  max mem: 16413
Epoch: [223]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000002  loss: 1.4955 (1.5587)  loss_scale: 8192.0000 (17720.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8964 (7.2461)  time: 1.0367 (0.5126 -- 4.1397)  data: 0.0018 (0.0003 -- 0.0088)  max mem: 16413
Epoch: [223]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000002  loss: 1.6519 (1.5746)  loss_scale: 8192.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5853 (7.2030)  time: 0.5502 (0.4968 -- 1.1051)  data: 0.0006 (0.0001 -- 0.0013)  max mem: 16413
Epoch: [223] Total time: 0:02:23 (0.8942 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000002  loss: 1.6519 (1.5731)  loss_scale: 8192.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5853 (7.2030)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1281 (0.1281)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5303 (2.5303 -- 2.5303)  data: 2.2902 (2.2902 -- 2.2902)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5358 (0.5032)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4503 (0.2030 -- 2.5303)  data: 0.2346 (0.0009 -- 2.2902)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4715 (0.4908)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.9418)  time: 0.2226 (0.1703 -- 0.5020)  data: 0.0178 (0.0001 -- 0.2739)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4715 (0.5344)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2073 (0.1327 -- 0.5020)  data: 0.0173 (0.0001 -- 0.2739)  max mem: 16413
Val: Total time: 0:00:07 (0.2953 s / it)
* Acc@1 85.270 Acc@5 98.340 loss 0.568
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.31%
Epoch: [224]  [  0/160]  eta: 0:19:45  lr: 0.000009  min_lr: 0.000002  loss: 1.8407 (1.8407)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9499 (9.9498)  time: 7.4102 (7.4102 -- 7.4102)  data: 6.5871 (6.5871 -- 6.5871)  max mem: 16413
Epoch: [224]  [ 20/160]  eta: 0:02:47  lr: 0.000009  min_lr: 0.000002  loss: 1.5923 (1.5911)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6905 (8.0405)  time: 0.8846 (0.5236 -- 3.4114)  data: 0.3363 (0.0004 -- 2.8538)  max mem: 16413
Epoch: [224]  [ 40/160]  eta: 0:02:09  lr: 0.000009  min_lr: 0.000002  loss: 1.4833 (1.5682)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6554 (7.5557)  time: 0.9494 (0.5213 -- 3.5770)  data: 0.4023 (0.0003 -- 3.0452)  max mem: 16413
Epoch: [224]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000002  loss: 1.5935 (1.5631)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8475 (7.3340)  time: 0.7901 (0.5326 -- 2.2636)  data: 0.2350 (0.0004 -- 1.7277)  max mem: 16413
Epoch: [224]  [ 80/160]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000002  loss: 1.6013 (1.5675)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1057 (7.2261)  time: 0.8523 (0.5312 -- 3.4197)  data: 0.3028 (0.0006 -- 2.8795)  max mem: 16413
[2023-09-05 06:57:07,478] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:57:07,479] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 06:57:07,484] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:57:07,485] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [224]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000002  loss: 1.6676 (1.5805)  loss_scale: 16384.0000 (9165.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2841 (7.1541)  time: 0.8275 (0.5335 -- 2.3155)  data: 0.2708 (0.0009 -- 1.7617)  max mem: 16413
Epoch: [224]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000002  loss: 1.5645 (1.5793)  loss_scale: 16384.0000 (10358.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1482 (7.3180)  time: 0.8524 (0.5250 -- 2.2657)  data: 0.2958 (0.0005 -- 1.7472)  max mem: 16413
Epoch: [224]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.1711 (1.5422)  loss_scale: 16384.0000 (11213.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9788 (7.3512)  time: 0.8671 (0.5151 -- 3.5697)  data: 0.3079 (0.0008 -- 3.0489)  max mem: 16413
[2023-09-05 06:58:04,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=36000, skipped=204, lr=[2.1382105778429044e-06, 2.1382105778429044e-06, 2.3757895309365606e-06, 2.3757895309365606e-06, 2.639766145485067e-06, 2.639766145485067e-06, 2.933073494983408e-06, 2.933073494983408e-06, 3.258970549981564e-06, 3.258970549981564e-06, 3.6210783888684048e-06, 3.6210783888684048e-06, 4.023420432076005e-06, 4.023420432076005e-06, 4.4704671467511165e-06, 4.4704671467511165e-06, 4.967185718612352e-06, 4.967185718612352e-06, 5.519095242902612e-06, 5.519095242902612e-06, 6.13232804766957e-06, 6.13232804766957e-06, 6.8136978307439665e-06, 6.8136978307439665e-06, 7.570775367493296e-06, 7.570775367493296e-06, 8.411972630548106e-06, 8.411972630548106e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 06:58:04,278] [INFO] [timer.py:260:stop] epoch=0/micro_step=36000/global_step=36000, RunningAvgSamplesPerSec=17.753805539872832, CurrSamplesPerSec=24.71027767590584, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [224]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.5121 (1.5381)  loss_scale: 16384.0000 (11827.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0707 (7.3698)  time: 0.7433 (0.4964 -- 3.3077)  data: 0.2095 (0.0001 -- 2.7597)  max mem: 16413
Epoch: [224] Total time: 0:02:22 (0.8886 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.5121 (1.5344)  loss_scale: 16384.0000 (11827.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0707 (7.3698)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1336 (0.1336)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4338 (2.4338 -- 2.4338)  data: 2.1967 (2.1967 -- 2.1967)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5420 (0.5012)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4423 (0.2091 -- 2.4338)  data: 0.2145 (0.0006 -- 2.1967)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4454 (0.4910)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2272 (0.1697 -- 0.3894)  data: 0.0184 (0.0001 -- 0.2009)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4454 (0.5258)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2070 (0.1330 -- 0.3894)  data: 0.0180 (0.0001 -- 0.2009)  max mem: 16413
Val: Total time: 0:00:07 (0.2949 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.552
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.31%
Epoch: [225]  [  0/160]  eta: 0:19:52  lr: 0.000008  min_lr: 0.000002  loss: 0.7366 (0.7366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3217 (9.3217)  time: 7.4532 (7.4532 -- 7.4532)  data: 6.8898 (6.8898 -- 6.8898)  max mem: 16413
Epoch: [225]  [ 20/160]  eta: 0:02:36  lr: 0.000008  min_lr: 0.000002  loss: 1.6295 (1.5745)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6095 (8.1744)  time: 0.8021 (0.5224 -- 3.8776)  data: 0.2481 (0.0004 -- 3.3084)  max mem: 16413
Epoch: [225]  [ 40/160]  eta: 0:02:02  lr: 0.000008  min_lr: 0.000002  loss: 1.6155 (1.6079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7981 (7.5240)  time: 0.9119 (0.5244 -- 3.4430)  data: 0.3285 (0.0002 -- 2.9154)  max mem: 16413
[2023-09-05 06:59:10,237] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:59:10,237] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 06:59:10,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 06:59:10,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [225]  [ 60/160]  eta: 0:01:37  lr: 0.000008  min_lr: 0.000002  loss: 1.6572 (1.6266)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0673 (7.4786)  time: 0.8960 (0.5308 -- 4.7194)  data: 0.3315 (0.0004 -- 4.1720)  max mem: 16413
Epoch: [225]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000002  loss: 1.5496 (1.5959)  loss_scale: 32768.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9744 (7.2096)  time: 0.9120 (0.5262 -- 3.7725)  data: 0.3397 (0.0006 -- 3.2250)  max mem: 16413
Epoch: [225]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000002  loss: 1.4605 (1.5771)  loss_scale: 32768.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8762 (7.2511)  time: 0.8567 (0.5268 -- 3.8723)  data: 0.3017 (0.0004 -- 3.3421)  max mem: 16413
Epoch: [225]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000002  loss: 1.5494 (1.5627)  loss_scale: 32768.0000 (25049.9174)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7859 (7.1909)  time: 0.9325 (0.5190 -- 3.9145)  data: 0.3919 (0.0003 -- 3.3879)  max mem: 16413
Epoch: [225]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.5992 (1.5596)  loss_scale: 32768.0000 (26144.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0407 (7.2275)  time: 0.7929 (0.5269 -- 3.7603)  data: 0.2388 (0.0002 -- 3.2513)  max mem: 16413
Epoch: [225]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.5386 (1.5694)  loss_scale: 32768.0000 (26931.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4150 (7.1151)  time: 0.6781 (0.4965 -- 3.7543)  data: 0.1609 (0.0003 -- 3.2075)  max mem: 16413
Epoch: [225] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.5386 (1.5756)  loss_scale: 32768.0000 (26931.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4150 (7.1151)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1300 (0.1300)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2925 (2.2925 -- 2.2925)  data: 2.0686 (2.0686 -- 2.0686)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5133 (0.4863)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4107 (0.2060 -- 2.2925)  data: 0.1926 (0.0007 -- 2.0686)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4654 (0.4774)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2267 (0.1700 -- 0.4948)  data: 0.0217 (0.0001 -- 0.3013)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4353 (0.5100)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2084 (0.1324 -- 0.4948)  data: 0.0212 (0.0001 -- 0.3013)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.552
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.31%
Epoch: [226]  [  0/160]  eta: 0:20:14  lr: 0.000008  min_lr: 0.000002  loss: 1.6801 (1.6801)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2835 (6.2835)  time: 7.5927 (7.5927 -- 7.5927)  data: 7.0602 (7.0602 -- 7.0602)  max mem: 16413
Epoch: [226]  [ 20/160]  eta: 0:02:47  lr: 0.000008  min_lr: 0.000002  loss: 1.5663 (1.5163)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6102 (6.7631)  time: 0.8758 (0.5372 -- 2.4135)  data: 0.2886 (0.0006 -- 1.8599)  max mem: 16413
[2023-09-05 07:01:10,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:01:10,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:01:10,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 07:01:10,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 07:01:14,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36187
[2023-09-05 07:01:14,204] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 07:01:14,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36187
[2023-09-05 07:01:14,204] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 07:01:14,205] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-09-05 07:01:24,681] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36200
[2023-09-05 07:01:24,681] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:01:24,681] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36200
[2023-09-05 07:01:24,681] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:01:24,682] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [226]  [ 40/160]  eta: 0:02:03  lr: 0.000008  min_lr: 0.000002  loss: 1.4533 (1.5066)  loss_scale: 32768.0000 (33966.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5776 (6.7504)  time: 0.8468 (0.5230 -- 3.2460)  data: 0.2359 (0.0004 -- 2.7188)  max mem: 16413
Epoch: [226]  [ 60/160]  eta: 0:01:41  lr: 0.000008  min_lr: 0.000002  loss: 1.5964 (1.5510)  loss_scale: 16384.0000 (28201.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1348 (6.9584)  time: 0.9932 (0.5220 -- 4.0348)  data: 0.0306 (0.0003 -- 0.3569)  max mem: 16413
Epoch: [226]  [ 80/160]  eta: 0:01:17  lr: 0.000008  min_lr: 0.000002  loss: 1.5601 (1.5527)  loss_scale: 16384.0000 (25283.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9243 (6.9695)  time: 0.8244 (0.5239 -- 4.3230)  data: 0.0011 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [226]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000002  loss: 1.5332 (1.5566)  loss_scale: 16384.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8649 (6.9339)  time: 0.8691 (0.5239 -- 3.2082)  data: 0.0015 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [226]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000002  loss: 1.6844 (1.5862)  loss_scale: 16384.0000 (22341.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1477 (7.0184)  time: 0.8130 (0.5390 -- 3.0493)  data: 0.0017 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [226]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.6015 (1.5934)  loss_scale: 16384.0000 (21496.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5481 (7.1002)  time: 0.9982 (0.5194 -- 4.3978)  data: 0.0020 (0.0006 -- 0.0137)  max mem: 16413
Epoch: [226]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.5024 (1.5809)  loss_scale: 16384.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8017 (7.1610)  time: 0.7068 (0.4936 -- 3.0271)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [226] Total time: 0:02:25 (0.9101 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.5024 (1.5746)  loss_scale: 16384.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8017 (7.1610)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1322 (0.1322)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4599 (2.4599 -- 2.4599)  data: 2.2275 (2.2275 -- 2.2275)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5356 (0.5066)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4294 (0.2014 -- 2.4599)  data: 0.2070 (0.0006 -- 2.2275)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4841 (0.4985)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.9418)  time: 0.2185 (0.1692 -- 0.3014)  data: 0.0088 (0.0001 -- 0.0780)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4841 (0.5289)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2017 (0.1330 -- 0.3014)  data: 0.0085 (0.0001 -- 0.0780)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 85.685 Acc@5 98.340 loss 0.553
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.31%
Epoch: [227]  [  0/160]  eta: 0:22:17  lr: 0.000008  min_lr: 0.000002  loss: 0.9068 (0.9068)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2474 (10.2474)  time: 8.3583 (8.3583 -- 8.3583)  data: 7.8060 (7.8060 -- 7.8060)  max mem: 16413
[2023-09-05 07:03:32,942] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:03:32,942] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:03:32,942] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:03:32,942] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [227]  [ 20/160]  eta: 0:02:46  lr: 0.000008  min_lr: 0.000002  loss: 1.3308 (1.4248)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1131 (6.8831)  time: 0.8295 (0.5305 -- 3.9868)  data: 0.2754 (0.0003 -- 3.4587)  max mem: 16413
[2023-09-05 07:03:41,527] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36341
[2023-09-05 07:03:41,527] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36341
[2023-09-05 07:03:41,527] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:03:41,527] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:03:41,527] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [227]  [ 40/160]  eta: 0:02:09  lr: 0.000008  min_lr: 0.000002  loss: 1.3927 (1.4126)  loss_scale: 16384.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7942 (6.9824)  time: 0.9669 (0.5046 -- 3.5045)  data: 0.4167 (0.0002 -- 2.9710)  max mem: 16413
Epoch: [227]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000002  loss: 1.6656 (1.4801)  loss_scale: 16384.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4142 (7.1149)  time: 0.7973 (0.5228 -- 2.8811)  data: 0.2471 (0.0003 -- 2.3427)  max mem: 16413
Epoch: [227]  [ 80/160]  eta: 0:01:18  lr: 0.000008  min_lr: 0.000002  loss: 1.5684 (1.5071)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5960 (7.3492)  time: 0.9483 (0.5277 -- 3.4409)  data: 0.3519 (0.0003 -- 2.9111)  max mem: 16413
Epoch: [227]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000002  loss: 1.7510 (1.5437)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5502 (7.2492)  time: 0.7951 (0.5217 -- 3.0716)  data: 0.2532 (0.0001 -- 2.4944)  max mem: 16413
Epoch: [227]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000002  loss: 1.5706 (1.5487)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0892 (7.2526)  time: 0.9154 (0.5143 -- 4.6016)  data: 0.0015 (0.0004 -- 0.0054)  max mem: 16413
Epoch: [227]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.5626 (1.5561)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1833 (7.1366)  time: 0.7520 (0.5291 -- 1.7940)  data: 0.0231 (0.0003 -- 0.4323)  max mem: 16413
[2023-09-05 07:05:33,063] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:05:33,063] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:05:33,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:05:33,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [227]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.3536 (1.5381)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8410 (7.1572)  time: 0.7395 (0.4957 -- 3.6304)  data: 0.0170 (0.0002 -- 0.3234)  max mem: 16413
Epoch: [227] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.3536 (1.5517)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8410 (7.1572)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1277 (0.1277)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4274 (2.4274 -- 2.4274)  data: 2.2137 (2.2137 -- 2.2137)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5534 (0.5046)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4279 (0.1968 -- 2.4274)  data: 0.2111 (0.0007 -- 2.2137)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4350 (0.4918)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.9418)  time: 0.2175 (0.1701 -- 0.3454)  data: 0.0134 (0.0001 -- 0.0970)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4350 (0.5235)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2022 (0.1333 -- 0.3454)  data: 0.0129 (0.0001 -- 0.0970)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 85.270 Acc@5 98.340 loss 0.560
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.31%
Epoch: [228]  [  0/160]  eta: 0:23:41  lr: 0.000008  min_lr: 0.000002  loss: 1.2495 (1.2495)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5119 (6.5119)  time: 8.8857 (8.8857 -- 8.8857)  data: 8.3416 (8.3416 -- 8.3416)  max mem: 16413
Epoch: [228]  [ 20/160]  eta: 0:02:43  lr: 0.000008  min_lr: 0.000002  loss: 1.6080 (1.6286)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3883 (7.0091)  time: 0.7845 (0.5243 -- 2.7071)  data: 0.1667 (0.0004 -- 2.1742)  max mem: 16413
[2023-09-05 07:06:26,771] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36519
[2023-09-05 07:06:26,771] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36519
[2023-09-05 07:06:26,772] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:06:26,772] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:06:26,773] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [228]  [ 40/160]  eta: 0:02:02  lr: 0.000008  min_lr: 0.000002  loss: 1.5767 (1.6351)  loss_scale: 32768.0000 (31968.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6404 (6.8454)  time: 0.8654 (0.5312 -- 3.1077)  data: 0.3150 (0.0002 -- 2.5372)  max mem: 16413
Epoch: [228]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000002  loss: 1.5560 (1.5931)  loss_scale: 16384.0000 (26859.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6739 (7.1029)  time: 0.9101 (0.5251 -- 2.8526)  data: 0.3653 (0.0004 -- 2.3221)  max mem: 16413
Epoch: [228]  [ 80/160]  eta: 0:01:15  lr: 0.000008  min_lr: 0.000002  loss: 1.5394 (1.6022)  loss_scale: 16384.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2852 (7.2414)  time: 0.8021 (0.5244 -- 2.5332)  data: 0.1087 (0.0002 -- 1.2500)  max mem: 16413
Epoch: [228]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000002  loss: 1.6686 (1.6006)  loss_scale: 16384.0000 (22710.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8628 (7.1707)  time: 0.9747 (0.5259 -- 4.1970)  data: 0.0014 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [228]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000002  loss: 1.6500 (1.6097)  loss_scale: 16384.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2056 (7.0624)  time: 0.7622 (0.5181 -- 2.8492)  data: 0.0138 (0.0003 -- 0.2484)  max mem: 16413
Epoch: [228]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000002  loss: 1.6137 (1.6062)  loss_scale: 16384.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3591 (7.0211)  time: 0.9645 (0.5341 -- 3.2515)  data: 0.2240 (0.0003 -- 2.0520)  max mem: 16413
[2023-09-05 07:07:57,938] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36622
[2023-09-05 07:07:57,938] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36622
[2023-09-05 07:07:57,938] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:07:57,938] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:07:57,938] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [228]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000002  loss: 1.3756 (1.5919)  loss_scale: 8192.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9760 (6.9689)  time: 0.6270 (0.4978 -- 2.0490)  data: 0.0238 (0.0001 -- 0.4496)  max mem: 16413
Epoch: [228] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000002  loss: 1.3756 (1.5675)  loss_scale: 8192.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9760 (6.9689)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1307 (0.1307)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4878 (2.4878 -- 2.4878)  data: 2.2558 (2.2558 -- 2.2558)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5548 (0.4936)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4382 (0.2078 -- 2.4878)  data: 0.2188 (0.0008 -- 2.2558)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4782 (0.4983)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2171 (0.1701 -- 0.3601)  data: 0.0085 (0.0001 -- 0.1417)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4681 (0.5286)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2006 (0.1336 -- 0.3601)  data: 0.0082 (0.0001 -- 0.1417)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 86.515 Acc@5 98.133 loss 0.553
Accuracy of the network on the 482 val images: 86.51%
[2023-09-05 07:08:16,623] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-09-05 07:08:16,625] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-09-05 07:08:16,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-09-05 07:08:16,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-09-05 07:08:18,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-09-05 07:08:18,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 86.51%
Epoch: [229]  [  0/160]  eta: 0:19:52  lr: 0.000008  min_lr: 0.000002  loss: 1.2010 (1.2010)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0373 (8.0373)  time: 7.4543 (7.4543 -- 7.4543)  data: 6.8645 (6.8645 -- 6.8645)  max mem: 16413
Epoch: [229]  [ 20/160]  eta: 0:02:49  lr: 0.000008  min_lr: 0.000002  loss: 1.8135 (1.6798)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1895 (7.4999)  time: 0.9019 (0.5152 -- 3.5073)  data: 0.2391 (0.0004 -- 2.5462)  max mem: 16413
Epoch: [229]  [ 40/160]  eta: 0:02:03  lr: 0.000008  min_lr: 0.000002  loss: 1.4815 (1.5979)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2596 (7.2821)  time: 0.8432 (0.5339 -- 3.3384)  data: 0.0514 (0.0003 -- 0.6959)  max mem: 16413
Epoch: [229]  [ 60/160]  eta: 0:01:35  lr: 0.000008  min_lr: 0.000002  loss: 1.5262 (1.5704)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6138 (7.1297)  time: 0.8077 (0.5381 -- 1.9269)  data: 0.0907 (0.0005 -- 0.6861)  max mem: 16413
Epoch: [229]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000002  loss: 1.5476 (1.5729)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9765 (7.1938)  time: 0.9233 (0.5340 -- 2.3525)  data: 0.1124 (0.0005 -- 1.3320)  max mem: 16413
Epoch: [229]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000002  loss: 1.5596 (1.5711)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8347 (7.2787)  time: 0.9137 (0.5311 -- 2.7093)  data: 0.0016 (0.0004 -- 0.0045)  max mem: 16413
[2023-09-05 07:10:01,558] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:10:01,558] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 07:10:01,558] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:10:01,558] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 07:10:06,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36753
[2023-09-05 07:10:06,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:10:06,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 36753
[2023-09-05 07:10:06,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:10:06,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [229]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000002  loss: 1.6086 (1.5782)  loss_scale: 8192.0000 (8327.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3689 (7.2370)  time: 0.9445 (0.5219 -- 4.5146)  data: 0.0014 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [229]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000002  loss: 1.4134 (1.5672)  loss_scale: 8192.0000 (8308.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5448 (7.1833)  time: 0.8001 (0.5077 -- 4.8560)  data: 0.0014 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [229]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.4859 (1.5627)  loss_scale: 8192.0000 (8294.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7520 (7.2162)  time: 0.6357 (0.4946 -- 1.8287)  data: 0.0011 (0.0001 -- 0.0028)  max mem: 16413
Epoch: [229] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.4859 (1.5876)  loss_scale: 8192.0000 (8294.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7520 (7.2162)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1353 (0.1353)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3696 (2.3696 -- 2.3696)  data: 2.1584 (2.1584 -- 2.1584)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4917 (0.4918)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4182 (0.1940 -- 2.3696)  data: 0.2094 (0.0005 -- 2.1584)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4895 (0.5104)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2261 (0.1703 -- 0.5384)  data: 0.0256 (0.0001 -- 0.3648)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4657 (0.5403)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2122 (0.1327 -- 0.5384)  data: 0.0248 (0.0001 -- 0.3648)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 86.100 Acc@5 98.340 loss 0.563
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.51%
Epoch: [230]  [  0/160]  eta: 0:24:50  lr: 0.000007  min_lr: 0.000002  loss: 1.2416 (1.2416)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.2812 (13.2812)  time: 9.3144 (9.3144 -- 9.3144)  data: 6.9263 (6.9263 -- 6.9263)  max mem: 16413
Epoch: [230]  [ 20/160]  eta: 0:02:58  lr: 0.000007  min_lr: 0.000002  loss: 1.6447 (1.5262)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0869 (8.1174)  time: 0.8761 (0.5177 -- 4.2897)  data: 0.0998 (0.0002 -- 1.0177)  max mem: 16413
Epoch: [230]  [ 40/160]  eta: 0:02:09  lr: 0.000007  min_lr: 0.000002  loss: 1.5419 (1.5764)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2677 (8.4876)  time: 0.8742 (0.5336 -- 2.9397)  data: 0.0653 (0.0001 -- 1.1027)  max mem: 16413
Epoch: [230]  [ 60/160]  eta: 0:01:39  lr: 0.000007  min_lr: 0.000002  loss: 1.5510 (1.5669)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3067 (8.0764)  time: 0.8039 (0.5303 -- 3.1196)  data: 0.0412 (0.0003 -- 0.7936)  max mem: 16413
Epoch: [230]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000002  loss: 1.5886 (1.5736)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7342 (8.1607)  time: 0.8338 (0.5334 -- 2.8493)  data: 0.0465 (0.0003 -- 0.6275)  max mem: 16413
[2023-09-05 07:12:09,899] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:12:09,899] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 07:12:09,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:12:09,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [230]  [100/160]  eta: 0:00:57  lr: 0.000007  min_lr: 0.000002  loss: 1.5858 (1.5791)  loss_scale: 16384.0000 (9733.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4515 (8.0733)  time: 0.9737 (0.5217 -- 4.0743)  data: 0.0017 (0.0005 -- 0.0062)  max mem: 16413
Epoch: [230]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000002  loss: 1.6365 (1.5860)  loss_scale: 16384.0000 (10832.3967)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4508 (7.8825)  time: 0.7683 (0.5419 -- 2.9819)  data: 0.0020 (0.0003 -- 0.0061)  max mem: 16413
Epoch: [230]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000002  loss: 1.5411 (1.5779)  loss_scale: 16384.0000 (11619.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8615 (7.8629)  time: 0.8985 (0.5345 -- 4.5187)  data: 0.0018 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [230]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.6196 (1.5781)  loss_scale: 16384.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5225 (7.7775)  time: 0.6626 (0.4973 -- 2.7842)  data: 0.0009 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [230] Total time: 0:02:22 (0.8913 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.6196 (1.5673)  loss_scale: 16384.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5225 (7.7775)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1380 (0.1380)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3461 (2.3461 -- 2.3461)  data: 2.1295 (2.1295 -- 2.1295)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5123 (0.4932)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4260 (0.1957 -- 2.3461)  data: 0.2098 (0.0006 -- 2.1295)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5071 (0.5023)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2219 (0.1727 -- 0.3398)  data: 0.0128 (0.0001 -- 0.0924)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5071 (0.5379)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2081 (0.1329 -- 0.3398)  data: 0.0124 (0.0001 -- 0.0924)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.561
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [231]  [  0/160]  eta: 0:20:08  lr: 0.000007  min_lr: 0.000002  loss: 1.9834 (1.9834)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8687 (5.8687)  time: 7.5562 (7.5562 -- 7.5562)  data: 7.0167 (7.0167 -- 7.0167)  max mem: 16413
Epoch: [231]  [ 20/160]  eta: 0:02:50  lr: 0.000007  min_lr: 0.000002  loss: 1.7521 (1.6438)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7780 (6.8473)  time: 0.8975 (0.5350 -- 2.6220)  data: 0.0493 (0.0005 -- 0.7451)  max mem: 16413
[2023-09-05 07:14:01,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=37000, skipped=210, lr=[1.8170426143520095e-06, 1.8170426143520095e-06, 2.0189362381688994e-06, 2.0189362381688994e-06, 2.2432624868543325e-06, 2.2432624868543325e-06, 2.4925138742825918e-06, 2.4925138742825918e-06, 2.769459860313991e-06, 2.769459860313991e-06, 3.077177622571101e-06, 3.077177622571101e-06, 3.419086247301223e-06, 3.419086247301223e-06, 3.798984719223581e-06, 3.798984719223581e-06, 4.221094132470646e-06, 4.221094132470646e-06, 4.69010459163405e-06, 4.69010459163405e-06, 5.211227324037834e-06, 5.211227324037834e-06, 5.7902525822642595e-06, 5.7902525822642595e-06, 6.4336139802936214e-06, 6.4336139802936214e-06, 7.148459978104024e-06, 7.148459978104024e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 07:14:01,912] [INFO] [timer.py:260:stop] epoch=0/micro_step=37000/global_step=37000, RunningAvgSamplesPerSec=17.736566822127884, CurrSamplesPerSec=21.35254730984955, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [231]  [ 40/160]  eta: 0:02:08  lr: 0.000007  min_lr: 0.000002  loss: 1.6451 (1.6722)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2162 (6.6886)  time: 0.9169 (0.5270 -- 4.6418)  data: 0.3548 (0.0003 -- 4.1394)  max mem: 16413
[2023-09-05 07:14:12,731] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:14:12,731] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:14:12,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:14:12,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [231]  [ 60/160]  eta: 0:01:38  lr: 0.000007  min_lr: 0.000002  loss: 1.5755 (1.6141)  loss_scale: 32768.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7679 (6.5849)  time: 0.8083 (0.5158 -- 3.4349)  data: 0.2582 (0.0002 -- 2.9158)  max mem: 16413
Epoch: [231]  [ 80/160]  eta: 0:01:14  lr: 0.000007  min_lr: 0.000002  loss: 1.6016 (1.6013)  loss_scale: 32768.0000 (22654.4198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7566 (6.6866)  time: 0.7722 (0.5266 -- 3.0872)  data: 0.1979 (0.0004 -- 2.5221)  max mem: 16413
[2023-09-05 07:14:45,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37050
[2023-09-05 07:14:45,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37050
[2023-09-05 07:14:45,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:14:45,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:14:45,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [231]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000002  loss: 1.4123 (1.5681)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2710 (6.7052)  time: 0.9904 (0.5257 -- 3.9418)  data: 0.4020 (0.0004 -- 3.3855)  max mem: 16413
[2023-09-05 07:14:59,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37066
[2023-09-05 07:14:59,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37066
[2023-09-05 07:14:59,477] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:14:59,477] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:14:59,477] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [231]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000002  loss: 1.5864 (1.5680)  loss_scale: 8192.0000 (20784.6612)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9174 (6.7738)  time: 0.7744 (0.5224 -- 2.8770)  data: 0.2296 (0.0003 -- 2.3725)  max mem: 16413
Epoch: [231]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000002  loss: 1.4006 (1.5518)  loss_scale: 8192.0000 (18998.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1159 (6.8453)  time: 0.9883 (0.5225 -- 3.3769)  data: 0.2666 (0.0002 -- 2.7644)  max mem: 16413
Epoch: [231]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.4564 (1.5456)  loss_scale: 8192.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2357 (6.9627)  time: 0.7685 (0.4950 -- 3.3769)  data: 0.0560 (0.0001 -- 0.5915)  max mem: 16413
Epoch: [231] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.4564 (1.5528)  loss_scale: 8192.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2357 (6.9627)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1374 (0.1374)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4860 (2.4860 -- 2.4860)  data: 2.2326 (2.2326 -- 2.2326)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3994 (0.4922)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4225 (0.2029 -- 2.4860)  data: 0.2052 (0.0008 -- 2.2326)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4494 (0.5007)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2127 (0.1694 -- 0.3724)  data: 0.0111 (0.0001 -- 0.1673)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4494 (0.5359)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.1952 (0.1330 -- 0.3724)  data: 0.0108 (0.0001 -- 0.1673)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.557
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.51%
Epoch: [232]  [  0/160]  eta: 0:20:29  lr: 0.000007  min_lr: 0.000002  loss: 1.0087 (1.0087)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8334 (6.8334)  time: 7.6822 (7.6822 -- 7.6822)  data: 5.4571 (5.4571 -- 5.4571)  max mem: 16413
Epoch: [232]  [ 20/160]  eta: 0:02:54  lr: 0.000007  min_lr: 0.000002  loss: 1.6517 (1.6723)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2120 (7.8729)  time: 0.9256 (0.5247 -- 2.3167)  data: 0.2253 (0.0004 -- 1.7728)  max mem: 16413
Epoch: [232]  [ 40/160]  eta: 0:02:05  lr: 0.000007  min_lr: 0.000002  loss: 1.4043 (1.5962)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0327 (7.4865)  time: 0.8344 (0.5311 -- 1.9727)  data: 0.0726 (0.0004 -- 1.4264)  max mem: 16413
Epoch: [232]  [ 60/160]  eta: 0:01:38  lr: 0.000007  min_lr: 0.000002  loss: 1.5335 (1.6092)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3864 (7.5668)  time: 0.8660 (0.5297 -- 2.6148)  data: 0.2463 (0.0003 -- 2.0790)  max mem: 16413
[2023-09-05 07:17:03,106] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:17:03,107] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 07:17:03,108] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:17:03,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [232]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000002  loss: 1.5354 (1.5939)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4649 (7.3827)  time: 0.8348 (0.5299 -- 2.5674)  data: 0.1240 (0.0003 -- 1.4940)  max mem: 16413
Epoch: [232]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000002  loss: 1.5900 (1.5893)  loss_scale: 16384.0000 (10300.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1540 (7.2909)  time: 0.9277 (0.5336 -- 3.3975)  data: 0.0830 (0.0005 -- 1.0570)  max mem: 16413
Epoch: [232]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000002  loss: 1.6021 (1.5954)  loss_scale: 16384.0000 (11306.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2932 (7.2246)  time: 0.8381 (0.5207 -- 3.3034)  data: 0.0158 (0.0003 -- 0.2883)  max mem: 16413
Epoch: [232]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000002  loss: 1.4783 (1.5876)  loss_scale: 16384.0000 (12026.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8662 (7.2169)  time: 0.9183 (0.5117 -- 4.6383)  data: 0.0013 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [232]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.6013 (1.5930)  loss_scale: 16384.0000 (12544.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5268 (7.1978)  time: 0.6159 (0.4953 -- 2.3629)  data: 0.0008 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [232] Total time: 0:02:22 (0.8897 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.6013 (1.5758)  loss_scale: 16384.0000 (12544.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5268 (7.1978)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1424 (0.1424)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4566 (2.4566 -- 2.4566)  data: 2.1973 (2.1973 -- 2.1973)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4976 (0.5019)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4365 (0.1942 -- 2.4566)  data: 0.2075 (0.0003 -- 2.1973)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4976 (0.5116)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2232 (0.1745 -- 0.3402)  data: 0.0121 (0.0001 -- 0.1541)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4976 (0.5447)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2055 (0.1332 -- 0.3402)  data: 0.0118 (0.0001 -- 0.1541)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 85.685 Acc@5 98.340 loss 0.566
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [233]  [  0/160]  eta: 0:24:22  lr: 0.000007  min_lr: 0.000002  loss: 1.7651 (1.7651)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5059 (8.5059)  time: 9.1383 (9.1383 -- 9.1383)  data: 4.0398 (4.0398 -- 4.0398)  max mem: 16413
Epoch: [233]  [ 20/160]  eta: 0:02:41  lr: 0.000007  min_lr: 0.000002  loss: 1.4413 (1.5694)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6339 (7.3920)  time: 0.7566 (0.5197 -- 2.5794)  data: 0.0016 (0.0004 -- 0.0057)  max mem: 16413
Epoch: [233]  [ 40/160]  eta: 0:02:04  lr: 0.000007  min_lr: 0.000002  loss: 1.6534 (1.6079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7169 (7.3430)  time: 0.9173 (0.5268 -- 4.8900)  data: 0.2193 (0.0001 -- 4.3529)  max mem: 16413
[2023-09-05 07:19:06,386] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:19:06,386] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:19:06,386] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:19:06,387] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [233]  [ 60/160]  eta: 0:01:40  lr: 0.000007  min_lr: 0.000002  loss: 1.5693 (1.6006)  loss_scale: 32768.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1871 (7.1076)  time: 0.9457 (0.5169 -- 3.7290)  data: 0.3981 (0.0004 -- 3.1610)  max mem: 16413
Epoch: [233]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000002  loss: 1.5798 (1.6061)  loss_scale: 32768.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3022 (7.0889)  time: 0.7639 (0.5347 -- 2.7844)  data: 0.2088 (0.0002 -- 2.2392)  max mem: 16413
Epoch: [233]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000002  loss: 1.4999 (1.5793)  loss_scale: 32768.0000 (25792.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2073 (7.0895)  time: 0.8570 (0.5429 -- 3.4124)  data: 0.2885 (0.0010 -- 2.9021)  max mem: 16413
Epoch: [233]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000002  loss: 1.6285 (1.5848)  loss_scale: 32768.0000 (26945.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2258 (7.1339)  time: 0.8086 (0.5385 -- 3.0825)  data: 0.2537 (0.0006 -- 2.5287)  max mem: 16413
[2023-09-05 07:20:28,360] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37420
[2023-09-05 07:20:28,361] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:20:28,361] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37420
[2023-09-05 07:20:28,361] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:20:28,361] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [233]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000002  loss: 1.5980 (1.5799)  loss_scale: 32768.0000 (27655.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8077 (7.0460)  time: 0.9505 (0.5149 -- 2.4895)  data: 0.3937 (0.0004 -- 1.9333)  max mem: 16413
Epoch: [233]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000002  loss: 1.5340 (1.5731)  loss_scale: 16384.0000 (26316.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4556 (7.1749)  time: 0.6187 (0.4962 -- 2.0888)  data: 0.0912 (0.0002 -- 1.4879)  max mem: 16413
Epoch: [233] Total time: 0:02:20 (0.8812 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000002  loss: 1.5340 (1.5464)  loss_scale: 16384.0000 (26316.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4556 (7.1749)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.1323 (0.1323)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1826 (2.1826 -- 2.1826)  data: 1.9496 (1.9496 -- 1.9496)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4755 (0.4909)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4097 (0.1920 -- 2.1826)  data: 0.1971 (0.0006 -- 1.9496)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4474 (0.4971)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2322 (0.1694 -- 0.3644)  data: 0.0265 (0.0001 -- 0.1878)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4474 (0.5333)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2189 (0.1329 -- 0.3644)  data: 0.0262 (0.0001 -- 0.1878)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.568
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.51%
Epoch: [234]  [  0/160]  eta: 0:22:33  lr: 0.000007  min_lr: 0.000002  loss: 1.4485 (1.4485)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0681 (7.0681)  time: 8.4614 (8.4614 -- 8.4614)  data: 7.9070 (7.9070 -- 7.9070)  max mem: 16413
Epoch: [234]  [ 20/160]  eta: 0:02:43  lr: 0.000007  min_lr: 0.000002  loss: 1.5886 (1.4887)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2813 (7.3402)  time: 0.8051 (0.5356 -- 3.0749)  data: 0.2541 (0.0005 -- 2.5478)  max mem: 16413
Epoch: [234]  [ 40/160]  eta: 0:02:14  lr: 0.000007  min_lr: 0.000002  loss: 1.5843 (1.5221)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5907 (7.3393)  time: 1.0779 (0.5257 -- 5.5885)  data: 0.5323 (0.0006 -- 5.0762)  max mem: 16413
Epoch: [234]  [ 60/160]  eta: 0:01:38  lr: 0.000007  min_lr: 0.000002  loss: 1.3460 (1.4848)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3983 (7.1697)  time: 0.6989 (0.5252 -- 2.3753)  data: 0.1510 (0.0002 -- 1.8203)  max mem: 16413
Epoch: [234]  [ 80/160]  eta: 0:01:20  lr: 0.000007  min_lr: 0.000002  loss: 1.4693 (1.4951)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7616 (7.3243)  time: 1.0542 (0.5163 -- 4.1975)  data: 0.5115 (0.0006 -- 3.6800)  max mem: 16413
Epoch: [234]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000002  loss: 1.4851 (1.5163)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1287 (7.2050)  time: 0.6859 (0.5198 -- 2.3024)  data: 0.1389 (0.0004 -- 1.7625)  max mem: 16413
[2023-09-05 07:22:31,520] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:22:31,521] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:22:31,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:22:31,522] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [234]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000002  loss: 1.6620 (1.5260)  loss_scale: 32768.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0413 (7.2556)  time: 0.9046 (0.5243 -- 4.1270)  data: 0.3564 (0.0004 -- 3.5994)  max mem: 16413
Epoch: [234]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000002  loss: 1.6138 (1.5495)  loss_scale: 32768.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6014 (7.1845)  time: 0.8714 (0.5169 -- 4.0355)  data: 0.3241 (0.0004 -- 3.5173)  max mem: 16413
Epoch: [234]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.4609 (1.5309)  loss_scale: 32768.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8587 (7.2238)  time: 0.6383 (0.4942 -- 2.4397)  data: 0.1144 (0.0002 -- 1.8930)  max mem: 16413
Epoch: [234] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.4609 (1.5424)  loss_scale: 32768.0000 (21606.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8587 (7.2238)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1342 (0.1342)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6144 (2.6144 -- 2.6144)  data: 2.3958 (2.3958 -- 2.3958)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4770 (0.4736)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4371 (0.2031 -- 2.6144)  data: 0.2192 (0.0007 -- 2.3958)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4420 (0.4848)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2095 (0.1696 -- 0.2358)  data: 0.0009 (0.0001 -- 0.0042)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4420 (0.5244)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.1912 (0.1338 -- 0.2207)  data: 0.0004 (0.0001 -- 0.0013)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 85.892 Acc@5 98.133 loss 0.566
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.51%
Epoch: [235]  [  0/160]  eta: 0:20:51  lr: 0.000006  min_lr: 0.000002  loss: 1.2387 (1.2387)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7004 (6.7004)  time: 7.8218 (7.8218 -- 7.8218)  data: 7.2869 (7.2869 -- 7.2869)  max mem: 16413
Epoch: [235]  [ 20/160]  eta: 0:02:43  lr: 0.000006  min_lr: 0.000002  loss: 1.5322 (1.5295)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3565 (7.1637)  time: 0.8351 (0.5234 -- 3.6391)  data: 0.2374 (0.0004 -- 2.7425)  max mem: 16413
Epoch: [235]  [ 40/160]  eta: 0:02:02  lr: 0.000006  min_lr: 0.000002  loss: 1.5001 (1.5427)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4812 (7.0298)  time: 0.8623 (0.5295 -- 3.0964)  data: 0.2820 (0.0005 -- 2.4227)  max mem: 16413
Epoch: [235]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000002  loss: 1.7379 (1.5905)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9076 (7.2144)  time: 0.8782 (0.5316 -- 3.3377)  data: 0.3201 (0.0005 -- 2.7959)  max mem: 16413
[2023-09-05 07:24:32,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:24:32,482] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 07:24:32,482] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:24:32,483] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 07:24:33,617] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37679
[2023-09-05 07:24:33,617] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 07:24:33,617] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37679
[2023-09-05 07:24:33,618] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 07:24:33,618] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [235]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000002  loss: 1.3700 (1.5519)  loss_scale: 32768.0000 (33577.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3509 (7.1347)  time: 0.9181 (0.5155 -- 3.9030)  data: 0.3636 (0.0002 -- 3.3638)  max mem: 16413
Epoch: [235]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000002  loss: 1.6498 (1.5734)  loss_scale: 32768.0000 (33416.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1194 (7.1283)  time: 0.8733 (0.5207 -- 2.2427)  data: 0.3205 (0.0005 -- 1.6951)  max mem: 16413
Epoch: [235]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000002  loss: 1.6571 (1.5819)  loss_scale: 32768.0000 (33309.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9276 (7.1231)  time: 0.8084 (0.5229 -- 2.9977)  data: 0.2612 (0.0001 -- 2.4720)  max mem: 16413
Epoch: [235]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000002  loss: 1.5088 (1.5680)  loss_scale: 32768.0000 (33232.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0595 (7.0228)  time: 0.8675 (0.5355 -- 2.6012)  data: 0.2602 (0.0002 -- 2.0692)  max mem: 16413
Epoch: [235]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.4193 (1.5670)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5544 (7.0667)  time: 0.6981 (0.4940 -- 3.3209)  data: 0.0321 (0.0002 -- 0.5226)  max mem: 16413
Epoch: [235] Total time: 0:02:22 (0.8883 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.4193 (1.5629)  loss_scale: 32768.0000 (33177.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5544 (7.0667)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1288 (0.1288)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5095 (2.5095 -- 2.5095)  data: 2.2723 (2.2723 -- 2.2723)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4859 (0.4886)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4236 (0.2019 -- 2.5095)  data: 0.2099 (0.0005 -- 2.2723)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4025 (0.4819)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2170 (0.1685 -- 0.4080)  data: 0.0120 (0.0001 -- 0.1989)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4135 (0.5211)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.7552)  time: 0.2037 (0.1323 -- 0.4080)  data: 0.0117 (0.0001 -- 0.1989)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 86.100 Acc@5 98.548 loss 0.554
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.51%
Epoch: [236]  [  0/160]  eta: 0:20:05  lr: 0.000006  min_lr: 0.000002  loss: 1.2924 (1.2924)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2456 (5.2456)  time: 7.5354 (7.5354 -- 7.5354)  data: 6.4317 (6.4317 -- 6.4317)  max mem: 16413
[2023-09-05 07:26:04,443] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37769
[2023-09-05 07:26:04,443] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 37769
[2023-09-05 07:26:04,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:26:04,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:26:04,443] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [236]  [ 20/160]  eta: 0:02:46  lr: 0.000006  min_lr: 0.000002  loss: 1.5084 (1.5544)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9518 (7.5672)  time: 0.8697 (0.5322 -- 3.9555)  data: 0.3102 (0.0007 -- 3.3231)  max mem: 16413
Epoch: [236]  [ 40/160]  eta: 0:02:08  lr: 0.000006  min_lr: 0.000002  loss: 1.4833 (1.5556)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4747 (7.2935)  time: 0.9431 (0.5164 -- 3.3179)  data: 0.3909 (0.0004 -- 2.7800)  max mem: 16413
Epoch: [236]  [ 60/160]  eta: 0:01:39  lr: 0.000006  min_lr: 0.000002  loss: 1.4391 (1.5273)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9271 (7.3511)  time: 0.8335 (0.5170 -- 3.3609)  data: 0.2842 (0.0006 -- 2.8463)  max mem: 16413
Epoch: [236]  [ 80/160]  eta: 0:01:17  lr: 0.000006  min_lr: 0.000002  loss: 1.4635 (1.5137)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2762 (7.1490)  time: 0.9168 (0.5248 -- 3.7667)  data: 0.3727 (0.0001 -- 3.2276)  max mem: 16413
Epoch: [236]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000002  loss: 1.5741 (1.5377)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6525 (7.1375)  time: 0.8168 (0.5266 -- 3.3985)  data: 0.2603 (0.0003 -- 2.8680)  max mem: 16413
Epoch: [236]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000002  loss: 1.4477 (1.5351)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5616 (7.1341)  time: 0.8458 (0.5278 -- 3.2090)  data: 0.1515 (0.0003 -- 2.0501)  max mem: 16413
[2023-09-05 07:27:54,451] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:27:54,451] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:27:54,455] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:27:54,456] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [236]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000002  loss: 1.4886 (1.5327)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7832 (7.1022)  time: 0.8430 (0.5248 -- 3.5028)  data: 0.0016 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [236]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000002  loss: 1.4369 (1.5356)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6115 (7.0947)  time: 0.7573 (0.4954 -- 2.4061)  data: 0.0011 (0.0002 -- 0.0100)  max mem: 16413
Epoch: [236] Total time: 0:02:21 (0.8859 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000002  loss: 1.4369 (1.5574)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6115 (7.0947)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1275 (0.1275)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4354 (2.4354 -- 2.4354)  data: 2.1879 (2.1879 -- 2.1879)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4940 (0.4740)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4200 (0.2037 -- 2.4354)  data: 0.2013 (0.0007 -- 2.1879)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4360 (0.4797)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2147 (0.1692 -- 0.3061)  data: 0.0065 (0.0001 -- 0.0883)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4360 (0.5186)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.7552)  time: 0.1983 (0.1332 -- 0.3061)  data: 0.0062 (0.0001 -- 0.0883)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 85.685 Acc@5 98.548 loss 0.554
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [237]  [  0/160]  eta: 0:24:23  lr: 0.000006  min_lr: 0.000002  loss: 2.3117 (2.3117)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2808 (10.2808)  time: 9.1472 (9.1472 -- 9.1472)  data: 8.6253 (8.6253 -- 8.6253)  max mem: 16413
Epoch: [237]  [ 20/160]  eta: 0:02:39  lr: 0.000006  min_lr: 0.000002  loss: 1.7132 (1.6262)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1766 (7.7905)  time: 0.7418 (0.5161 -- 3.5868)  data: 0.1915 (0.0004 -- 3.0443)  max mem: 16413
Epoch: [237]  [ 40/160]  eta: 0:02:10  lr: 0.000006  min_lr: 0.000002  loss: 1.6446 (1.6148)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6829 (7.3041)  time: 1.0315 (0.5231 -- 4.9887)  data: 0.4833 (0.0006 -- 4.4500)  max mem: 16413
Epoch: [237]  [ 60/160]  eta: 0:01:41  lr: 0.000006  min_lr: 0.000002  loss: 1.4230 (1.5514)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4330 (7.2511)  time: 0.8704 (0.5238 -- 4.0450)  data: 0.3250 (0.0003 -- 3.5173)  max mem: 16413
[2023-09-05 07:29:33,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=38000, skipped=215, lr=[1.517793280337696e-06, 1.517793280337696e-06, 1.6864369781529957e-06, 1.6864369781529957e-06, 1.8738188646144394e-06, 1.8738188646144394e-06, 2.0820209606827106e-06, 2.0820209606827106e-06, 2.3133566229807893e-06, 2.3133566229807893e-06, 2.5703962477564326e-06, 2.5703962477564326e-06, 2.8559958308404805e-06, 2.8559958308404805e-06, 3.173328700933867e-06, 3.173328700933867e-06, 3.525920778815408e-06, 3.525920778815408e-06, 3.9176897542393415e-06, 3.9176897542393415e-06, 4.352988615821491e-06, 4.352988615821491e-06, 4.836654017579434e-06, 4.836654017579434e-06, 5.374060019532705e-06, 5.374060019532705e-06, 5.971177799480783e-06, 5.971177799480783e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 07:29:33,260] [INFO] [timer.py:260:stop] epoch=0/micro_step=38000/global_step=38000, RunningAvgSamplesPerSec=17.7866462403462, CurrSamplesPerSec=22.25211041071108, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [237]  [ 80/160]  eta: 0:01:15  lr: 0.000006  min_lr: 0.000002  loss: 1.6193 (1.5601)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7658 (7.0226)  time: 0.7397 (0.5160 -- 2.1289)  data: 0.1884 (0.0002 -- 1.5848)  max mem: 16413
[2023-09-05 07:29:45,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38011
[2023-09-05 07:29:45,146] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38011
[2023-09-05 07:29:45,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:29:45,146] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 07:29:45,146] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [237]  [100/160]  eta: 0:00:57  lr: 0.000006  min_lr: 0.000002  loss: 1.4143 (1.5394)  loss_scale: 16384.0000 (31145.8218)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7334 (6.9782)  time: 0.9594 (0.5267 -- 3.9363)  data: 0.2421 (0.0003 -- 3.4077)  max mem: 16413
Epoch: [237]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000002  loss: 1.4885 (1.5359)  loss_scale: 16384.0000 (28705.8512)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7049 (7.1022)  time: 0.8964 (0.5218 -- 3.8289)  data: 0.1078 (0.0002 -- 1.6743)  max mem: 16413
Epoch: [237]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000002  loss: 1.5848 (1.5461)  loss_scale: 16384.0000 (26958.0709)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2367 (7.1821)  time: 0.8489 (0.5349 -- 2.4262)  data: 0.0329 (0.0002 -- 0.6060)  max mem: 16413
Epoch: [237]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000001  loss: 1.5600 (1.5547)  loss_scale: 16384.0000 (25702.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7787 (7.2369)  time: 0.7295 (0.4956 -- 2.7083)  data: 0.0021 (0.0001 -- 0.0140)  max mem: 16413
Epoch: [237] Total time: 0:02:23 (0.8953 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000001  loss: 1.5600 (1.5298)  loss_scale: 16384.0000 (25702.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7787 (7.2369)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1297 (0.1297)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3595 (2.3595 -- 2.3595)  data: 2.1370 (2.1370 -- 2.1370)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5385 (0.4817)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4259 (0.2001 -- 2.3595)  data: 0.2149 (0.0007 -- 2.1370)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4478 (0.4820)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2257 (0.1693 -- 0.4287)  data: 0.0243 (0.0001 -- 0.2559)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4478 (0.5189)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.7552)  time: 0.2114 (0.1365 -- 0.4287)  data: 0.0241 (0.0001 -- 0.2559)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 86.307 Acc@5 98.548 loss 0.555
Accuracy of the network on the 482 val images: 86.31%
Max accuracy: 86.51%
Epoch: [238]  [  0/160]  eta: 0:24:26  lr: 0.000006  min_lr: 0.000001  loss: 1.0720 (1.0720)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7049 (4.7049)  time: 9.1645 (9.1645 -- 9.1645)  data: 8.6138 (8.6138 -- 8.6138)  max mem: 16413
Epoch: [238]  [ 20/160]  eta: 0:02:49  lr: 0.000006  min_lr: 0.000001  loss: 1.6036 (1.6252)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4848 (7.6192)  time: 0.8164 (0.5278 -- 3.0849)  data: 0.2379 (0.0004 -- 2.5610)  max mem: 16413
Epoch: [238]  [ 40/160]  eta: 0:02:10  lr: 0.000006  min_lr: 0.000001  loss: 1.6591 (1.6352)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9633 (7.1958)  time: 0.9483 (0.5288 -- 3.6431)  data: 0.2111 (0.0003 -- 2.0876)  max mem: 16413
[2023-09-05 07:31:48,868] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:31:48,868] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:31:48,869] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:31:48,869] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [238]  [ 60/160]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000001  loss: 1.7211 (1.6343)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6731 (7.0710)  time: 0.7570 (0.5279 -- 2.3078)  data: 0.1965 (0.0002 -- 1.7763)  max mem: 16413
Epoch: [238]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000001  loss: 1.3154 (1.5903)  loss_scale: 32768.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7643 (6.9394)  time: 0.9067 (0.5298 -- 3.5113)  data: 0.3534 (0.0004 -- 2.9831)  max mem: 16413
Epoch: [238]  [100/160]  eta: 0:00:54  lr: 0.000006  min_lr: 0.000001  loss: 1.5353 (1.5817)  loss_scale: 32768.0000 (23034.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9002 (6.9752)  time: 0.7272 (0.5421 -- 2.1778)  data: 0.1674 (0.0009 -- 1.6272)  max mem: 16413
[2023-09-05 07:32:34,115] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38193
[2023-09-05 07:32:34,116] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:32:34,116] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38193
[2023-09-05 07:32:34,116] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:32:34,117] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [238]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000001  loss: 1.6247 (1.5885)  loss_scale: 32768.0000 (23560.4628)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7641 (7.0053)  time: 1.0221 (0.5309 -- 4.4510)  data: 0.4556 (0.0007 -- 3.8690)  max mem: 16413
Epoch: [238]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000001  loss: 1.5867 (1.5900)  loss_scale: 16384.0000 (22542.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6793 (6.9431)  time: 0.7902 (0.5277 -- 3.0281)  data: 0.2378 (0.0001 -- 2.4721)  max mem: 16413
Epoch: [238]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000001  loss: 1.5640 (1.5819)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9943 (6.9640)  time: 0.6989 (0.4968 -- 2.3112)  data: 0.1722 (0.0002 -- 1.8104)  max mem: 16413
Epoch: [238] Total time: 0:02:22 (0.8875 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000001  loss: 1.5640 (1.5569)  loss_scale: 16384.0000 (21811.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9943 (6.9640)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1293 (0.1293)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4272 (2.4272 -- 2.4272)  data: 2.1891 (2.1891 -- 2.1891)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4729 (0.4858)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4151 (0.2061 -- 2.4272)  data: 0.1999 (0.0005 -- 2.1891)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4729 (0.4888)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2188 (0.1683 -- 0.4637)  data: 0.0130 (0.0001 -- 0.2466)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4729 (0.5310)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2035 (0.1329 -- 0.4637)  data: 0.0128 (0.0001 -- 0.2466)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 85.892 Acc@5 98.340 loss 0.552
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.51%
Epoch: [239]  [  0/160]  eta: 0:20:04  lr: 0.000006  min_lr: 0.000001  loss: 1.7463 (1.7463)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4662 (5.4662)  time: 7.5258 (7.5258 -- 7.5258)  data: 6.9991 (6.9991 -- 6.9991)  max mem: 16413
Epoch: [239]  [ 20/160]  eta: 0:02:39  lr: 0.000006  min_lr: 0.000001  loss: 1.5399 (1.5832)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0909 (7.7622)  time: 0.8188 (0.5306 -- 2.2697)  data: 0.0870 (0.0001 -- 1.0756)  max mem: 16413
Epoch: [239]  [ 40/160]  eta: 0:02:06  lr: 0.000006  min_lr: 0.000001  loss: 1.5752 (1.5572)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1209 (7.6902)  time: 0.9716 (0.5344 -- 3.3915)  data: 0.4031 (0.0003 -- 2.8539)  max mem: 16413
Epoch: [239]  [ 60/160]  eta: 0:01:40  lr: 0.000006  min_lr: 0.000001  loss: 1.5540 (1.5342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9211 (7.5137)  time: 0.8975 (0.5196 -- 4.1931)  data: 0.3491 (0.0004 -- 3.6366)  max mem: 16413
Epoch: [239]  [ 80/160]  eta: 0:01:18  lr: 0.000006  min_lr: 0.000001  loss: 1.4344 (1.5209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8475 (7.6915)  time: 0.9300 (0.5209 -- 5.0020)  data: 0.3789 (0.0004 -- 4.4571)  max mem: 16413
[2023-09-05 07:34:40,065] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:34:40,065] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:34:40,065] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:34:40,066] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [239]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000001  loss: 1.5465 (1.5129)  loss_scale: 32768.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0420 (7.4396)  time: 0.7703 (0.5207 -- 2.8294)  data: 0.2141 (0.0003 -- 2.2622)  max mem: 16413
Epoch: [239]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000001  loss: 1.3962 (1.5052)  loss_scale: 32768.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2078 (7.3902)  time: 0.8905 (0.5255 -- 4.0867)  data: 0.3227 (0.0002 -- 3.5360)  max mem: 16413
[2023-09-05 07:35:23,248] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38374
[2023-09-05 07:35:23,249] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:35:23,249] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38374
[2023-09-05 07:35:23,249] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:35:23,249] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [239]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000001  loss: 1.4331 (1.5080)  loss_scale: 32768.0000 (22426.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9745 (7.3647)  time: 0.8716 (0.5159 -- 3.5599)  data: 0.1995 (0.0001 -- 3.0100)  max mem: 16413
Epoch: [239]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000001  loss: 1.5596 (1.5047)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2862 (7.4774)  time: 0.6916 (0.4961 -- 2.2575)  data: 0.1245 (0.0003 -- 1.7198)  max mem: 16413
Epoch: [239] Total time: 0:02:23 (0.8991 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000001  loss: 1.5596 (1.5528)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2862 (7.4774)
[2023-09-05 07:35:42,917] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-239 is about to be saved!
[2023-09-05 07:35:42,919] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-239/mp_rank_00_model_states.pt
[2023-09-05 07:35:42,919] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-239/mp_rank_00_model_states.pt...
[2023-09-05 07:35:42,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-239 is ready now!
[2023-09-05 07:35:43,945] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-239/mp_rank_00_model_states.pt.
[2023-09-05 07:35:43,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-239 is ready now!
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1341 (0.1341)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4597 (2.4597 -- 2.4597)  data: 2.2141 (2.2141 -- 2.2141)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5079 (0.4968)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4505 (0.2026 -- 2.4597)  data: 0.2346 (0.0005 -- 2.2141)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4784 (0.4909)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2295 (0.1697 -- 0.5442)  data: 0.0266 (0.0001 -- 0.3371)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4784 (0.5334)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.7552)  time: 0.2131 (0.1321 -- 0.5442)  data: 0.0258 (0.0001 -- 0.3371)  max mem: 16413
Val: Total time: 0:00:08 (0.2977 s / it)
* Acc@1 85.477 Acc@5 98.340 loss 0.553
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [240]  [  0/160]  eta: 0:20:09  lr: 0.000006  min_lr: 0.000001  loss: 0.7568 (0.7568)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8082 (6.8082)  time: 7.5586 (7.5586 -- 7.5586)  data: 6.9954 (6.9954 -- 6.9954)  max mem: 16413
Epoch: [240]  [ 20/160]  eta: 0:02:52  lr: 0.000006  min_lr: 0.000001  loss: 1.5894 (1.4686)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8913 (7.3868)  time: 0.9173 (0.5239 -- 2.9363)  data: 0.2844 (0.0010 -- 2.1782)  max mem: 16413
Epoch: [240]  [ 40/160]  eta: 0:02:02  lr: 0.000005  min_lr: 0.000001  loss: 1.5836 (1.5461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6381 (7.3871)  time: 0.7956 (0.5284 -- 3.3051)  data: 0.0245 (0.0003 -- 0.4429)  max mem: 16413
Epoch: [240]  [ 60/160]  eta: 0:01:37  lr: 0.000005  min_lr: 0.000001  loss: 1.5072 (1.5315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2383 (7.0918)  time: 0.8738 (0.5169 -- 2.5935)  data: 0.1137 (0.0002 -- 1.0991)  max mem: 16413
Epoch: [240]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000001  loss: 1.6097 (1.5487)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5759 (7.1484)  time: 0.8673 (0.5283 -- 2.6716)  data: 0.1813 (0.0001 -- 2.1558)  max mem: 16413
Epoch: [240]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 1.5711 (1.5477)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6894 (7.2271)  time: 0.8931 (0.5219 -- 4.6521)  data: 0.3320 (0.0004 -- 4.1280)  max mem: 16413
[2023-09-05 07:37:28,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:37:28,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:37:28,110] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:37:28,110] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [240]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000001  loss: 1.6419 (1.5602)  loss_scale: 32768.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4521 (7.2652)  time: 0.9064 (0.5215 -- 2.5729)  data: 0.3572 (0.0002 -- 2.0260)  max mem: 16413
Epoch: [240]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.5602 (1.5584)  loss_scale: 32768.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7520 (7.2978)  time: 0.8005 (0.5318 -- 3.1066)  data: 0.2473 (0.0005 -- 2.5844)  max mem: 16413
Epoch: [240]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.3697 (1.5437)  loss_scale: 32768.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3091 (7.1976)  time: 0.7189 (0.4969 -- 2.8226)  data: 0.1837 (0.0002 -- 2.2694)  max mem: 16413
Epoch: [240] Total time: 0:02:22 (0.8906 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.3697 (1.5403)  loss_scale: 32768.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3091 (7.1976)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1320 (0.1320)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5276 (2.5276 -- 2.5276)  data: 2.2765 (2.2765 -- 2.2765)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5443 (0.5000)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4295 (0.2015 -- 2.5276)  data: 0.2111 (0.0008 -- 2.2765)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4609 (0.4938)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2129 (0.1693 -- 0.2904)  data: 0.0040 (0.0001 -- 0.0353)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4609 (0.5342)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.1984 (0.1331 -- 0.2904)  data: 0.0037 (0.0001 -- 0.0353)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 85.685 Acc@5 98.133 loss 0.558
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [241]  [  0/160]  eta: 0:20:08  lr: 0.000005  min_lr: 0.000001  loss: 1.5761 (1.5761)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5259 (5.5259)  time: 7.5557 (7.5557 -- 7.5557)  data: 7.0077 (7.0077 -- 7.0077)  max mem: 16413
Epoch: [241]  [ 20/160]  eta: 0:02:37  lr: 0.000005  min_lr: 0.000001  loss: 1.6214 (1.6187)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5042 (7.2996)  time: 0.8024 (0.5348 -- 3.2164)  data: 0.2503 (0.0006 -- 2.6607)  max mem: 16413
[2023-09-05 07:38:51,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38586
[2023-09-05 07:38:51,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38586
[2023-09-05 07:38:51,991] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:38:51,991] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:38:51,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [241]  [ 40/160]  eta: 0:02:03  lr: 0.000005  min_lr: 0.000001  loss: 1.5923 (1.6137)  loss_scale: 16384.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3599 (7.5566)  time: 0.9257 (0.5342 -- 3.4639)  data: 0.2109 (0.0004 -- 1.7536)  max mem: 16413
Epoch: [241]  [ 60/160]  eta: 0:01:37  lr: 0.000005  min_lr: 0.000001  loss: 1.6144 (1.6436)  loss_scale: 16384.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3834 (7.4990)  time: 0.8737 (0.5247 -- 4.0567)  data: 0.3229 (0.0005 -- 3.5082)  max mem: 16413
[2023-09-05 07:39:22,965] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38622
[2023-09-05 07:39:22,966] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:39:22,966] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38622
[2023-09-05 07:39:22,967] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:39:22,967] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [241]  [ 80/160]  eta: 0:01:18  lr: 0.000005  min_lr: 0.000001  loss: 1.4161 (1.6053)  loss_scale: 8192.0000 (19721.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1548 (7.5361)  time: 1.0001 (0.5339 -- 5.4528)  data: 0.2612 (0.0002 -- 2.7567)  max mem: 16413
Epoch: [241]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 1.5541 (1.5722)  loss_scale: 8192.0000 (17438.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1041 (7.5093)  time: 0.7919 (0.5273 -- 3.8377)  data: 0.0013 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [241]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000001  loss: 1.5272 (1.5701)  loss_scale: 8192.0000 (15910.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1485 (7.4081)  time: 0.9566 (0.5322 -- 3.8091)  data: 0.0021 (0.0004 -- 0.0141)  max mem: 16413
Epoch: [241]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.5750 (1.5701)  loss_scale: 8192.0000 (14815.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1948 (7.3703)  time: 0.8580 (0.5182 -- 3.8028)  data: 0.0017 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [241]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.5808 (1.5804)  loss_scale: 8192.0000 (14028.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8433 (7.4021)  time: 0.5909 (0.4953 -- 1.4688)  data: 0.0009 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [241] Total time: 0:02:23 (0.8939 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.5808 (1.5728)  loss_scale: 8192.0000 (14028.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8433 (7.4021)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1360 (0.1360)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3289 (2.3289 -- 2.3289)  data: 2.0948 (2.0948 -- 2.0948)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5681 (0.5036)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4228 (0.2032 -- 2.3289)  data: 0.2026 (0.0005 -- 2.0948)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5007 (0.5093)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2243 (0.1701 -- 0.3440)  data: 0.0174 (0.0001 -- 0.1510)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5007 (0.5499)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.3402)  time: 0.2084 (0.1333 -- 0.3440)  data: 0.0172 (0.0001 -- 0.1510)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.565
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.51%
Epoch: [242]  [  0/160]  eta: 0:22:45  lr: 0.000005  min_lr: 0.000001  loss: 0.9556 (0.9556)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6351 (10.6351)  time: 8.5375 (8.5375 -- 8.5375)  data: 6.1442 (6.1442 -- 6.1442)  max mem: 16413
Epoch: [242]  [ 20/160]  eta: 0:02:47  lr: 0.000005  min_lr: 0.000001  loss: 1.6476 (1.5362)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1093 (7.6599)  time: 0.8320 (0.5201 -- 3.7303)  data: 0.1321 (0.0003 -- 1.4638)  max mem: 16413
[2023-09-05 07:41:28,244] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:41:28,244] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 07:41:28,246] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:41:28,247] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [242]  [ 40/160]  eta: 0:02:09  lr: 0.000005  min_lr: 0.000001  loss: 1.5790 (1.5501)  loss_scale: 8192.0000 (10190.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7818 (7.3275)  time: 0.9462 (0.5261 -- 3.5607)  data: 0.1300 (0.0004 -- 1.9247)  max mem: 16413
Epoch: [242]  [ 60/160]  eta: 0:01:41  lr: 0.000005  min_lr: 0.000001  loss: 1.2041 (1.4869)  loss_scale: 16384.0000 (12220.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3349 (7.4491)  time: 0.8920 (0.5266 -- 4.1941)  data: 0.0122 (0.0004 -- 0.2103)  max mem: 16413
Epoch: [242]  [ 80/160]  eta: 0:01:20  lr: 0.000005  min_lr: 0.000001  loss: 1.6054 (1.5063)  loss_scale: 16384.0000 (13248.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7832 (7.2650)  time: 0.9651 (0.5227 -- 4.9159)  data: 0.0016 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [242]  [100/160]  eta: 0:00:57  lr: 0.000005  min_lr: 0.000001  loss: 1.4664 (1.5028)  loss_scale: 16384.0000 (13869.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9800 (7.3028)  time: 0.7642 (0.5188 -- 2.9472)  data: 0.0024 (0.0004 -- 0.0161)  max mem: 16413
Epoch: [242]  [120/160]  eta: 0:00:38  lr: 0.000005  min_lr: 0.000001  loss: 1.3058 (1.4801)  loss_scale: 16384.0000 (14285.2231)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4342 (7.3374)  time: 1.0092 (0.5191 -- 4.2312)  data: 0.0014 (0.0002 -- 0.0062)  max mem: 16413
Epoch: [242]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.4915 (1.4847)  loss_scale: 16384.0000 (14582.9220)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5114 (7.4183)  time: 0.8276 (0.4994 -- 4.1696)  data: 0.0011 (0.0001 -- 0.0033)  max mem: 16413
[2023-09-05 07:43:18,591] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:43:18,591] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:43:18,591] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:43:18,592] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [242]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.7834 (1.5251)  loss_scale: 16384.0000 (14899.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8109 (7.3729)  time: 0.6398 (0.4961 -- 2.0241)  data: 0.0010 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [242] Total time: 0:02:25 (0.9094 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.7834 (1.5292)  loss_scale: 16384.0000 (14899.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8109 (7.3729)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.1289 (0.1289)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1228 (2.1228 -- 2.1228)  data: 1.8717 (1.8717 -- 1.8717)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5158 (0.5082)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4115 (0.1883 -- 2.1228)  data: 0.1967 (0.0005 -- 1.8717)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4688 (0.5002)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2276 (0.1701 -- 0.5024)  data: 0.0240 (0.0001 -- 0.2826)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4688 (0.5442)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2150 (0.1330 -- 0.5024)  data: 0.0238 (0.0001 -- 0.2826)  max mem: 16413
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 85.062 Acc@5 98.340 loss 0.567
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.51%
Epoch: [243]  [  0/160]  eta: 0:23:40  lr: 0.000005  min_lr: 0.000001  loss: 1.4808 (1.4808)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2753 (4.2753)  time: 8.8761 (8.8761 -- 8.8761)  data: 8.3142 (8.3142 -- 8.3142)  max mem: 16413
Epoch: [243]  [ 20/160]  eta: 0:02:56  lr: 0.000005  min_lr: 0.000001  loss: 1.4445 (1.5383)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8221 (6.9630)  time: 0.8779 (0.5286 -- 3.8803)  data: 0.1586 (0.0003 -- 1.7238)  max mem: 16413
Epoch: [243]  [ 40/160]  eta: 0:02:07  lr: 0.000005  min_lr: 0.000001  loss: 1.6218 (1.5837)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1162 (7.3009)  time: 0.8498 (0.5305 -- 2.4035)  data: 0.2104 (0.0003 -- 1.8735)  max mem: 16413
Epoch: [243]  [ 60/160]  eta: 0:01:40  lr: 0.000005  min_lr: 0.000001  loss: 1.4632 (1.5718)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8663 (7.2173)  time: 0.8930 (0.5330 -- 3.5659)  data: 0.3371 (0.0001 -- 3.0172)  max mem: 16413
Epoch: [243]  [ 80/160]  eta: 0:01:17  lr: 0.000005  min_lr: 0.000001  loss: 1.6886 (1.5895)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6250 (7.4199)  time: 0.8548 (0.5306 -- 2.6195)  data: 0.3041 (0.0004 -- 2.0949)  max mem: 16413
[2023-09-05 07:44:54,625] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38972
[2023-09-05 07:44:54,625] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 38972
[2023-09-05 07:44:54,625] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:44:54,625] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:44:54,625] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [243]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 1.5328 (1.5706)  loss_scale: 32768.0000 (31308.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6506 (7.3856)  time: 0.8657 (0.5221 -- 3.5646)  data: 0.3159 (0.0004 -- 3.0490)  max mem: 16413
[2023-09-05 07:45:19,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=221, lr=[1.242044457634155e-06, 1.242044457634155e-06, 1.3800493973712836e-06, 1.3800493973712836e-06, 1.533388219301426e-06, 1.533388219301426e-06, 1.7037646881126957e-06, 1.7037646881126957e-06, 1.893071875680773e-06, 1.893071875680773e-06, 2.1034131952008588e-06, 2.1034131952008588e-06, 2.3371257724453986e-06, 2.3371257724453986e-06, 2.5968064138282205e-06, 2.5968064138282205e-06, 2.8853404598091338e-06, 2.8853404598091338e-06, 3.2059338442323704e-06, 3.2059338442323704e-06, 3.5621487158137454e-06, 3.5621487158137454e-06, 3.957943017570828e-06, 3.957943017570828e-06, 4.397714463967587e-06, 4.397714463967587e-06, 4.886349404408429e-06, 4.886349404408429e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 07:45:19,145] [INFO] [timer.py:260:stop] epoch=0/micro_step=39000/global_step=39000, RunningAvgSamplesPerSec=17.79969305925402, CurrSamplesPerSec=21.250162759179744, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [243]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000001  loss: 1.6345 (1.5834)  loss_scale: 16384.0000 (28841.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2483 (7.2413)  time: 0.9293 (0.5276 -- 2.6570)  data: 0.3850 (0.0003 -- 2.0889)  max mem: 16413
Epoch: [243]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.4133 (1.5614)  loss_scale: 16384.0000 (27074.2695)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4188 (7.2082)  time: 0.9193 (0.5175 -- 3.7508)  data: 0.3830 (0.0003 -- 3.2486)  max mem: 16413
Epoch: [243]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.5181 (1.5690)  loss_scale: 16384.0000 (25804.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6047 (7.2590)  time: 0.5975 (0.4953 -- 2.1817)  data: 0.0842 (0.0002 -- 1.6758)  max mem: 16413
Epoch: [243] Total time: 0:02:24 (0.9007 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.5181 (1.5857)  loss_scale: 16384.0000 (25804.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6047 (7.2590)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1304 (0.1304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4924 (2.4924 -- 2.4924)  data: 2.2321 (2.2321 -- 2.2321)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5234 (0.5150)  acc1: 88.8889 (85.8586)  acc5: 100.0000 (100.0000)  time: 0.4219 (0.1956 -- 2.4924)  data: 0.2040 (0.0008 -- 2.2321)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4959 (0.5120)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (98.4127)  time: 0.2141 (0.1685 -- 0.3783)  data: 0.0097 (0.0001 -- 0.1734)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4959 (0.5587)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.3402)  time: 0.1981 (0.1335 -- 0.3783)  data: 0.0094 (0.0001 -- 0.1734)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.566
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.51%
Epoch: [244]  [  0/160]  eta: 0:20:30  lr: 0.000005  min_lr: 0.000001  loss: 0.8099 (0.8099)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0646 (4.0646)  time: 7.6918 (7.6918 -- 7.6918)  data: 7.1479 (7.1479 -- 7.1479)  max mem: 16413
Epoch: [244]  [ 20/160]  eta: 0:02:38  lr: 0.000005  min_lr: 0.000001  loss: 1.6153 (1.5508)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7053 (6.9040)  time: 0.8045 (0.5262 -- 2.7453)  data: 0.2026 (0.0005 -- 2.1857)  max mem: 16413
Epoch: [244]  [ 40/160]  eta: 0:02:00  lr: 0.000005  min_lr: 0.000001  loss: 1.6093 (1.5989)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1012 (6.8576)  time: 0.8743 (0.5220 -- 3.0777)  data: 0.2991 (0.0004 -- 2.5246)  max mem: 16413
Epoch: [244]  [ 60/160]  eta: 0:01:39  lr: 0.000005  min_lr: 0.000001  loss: 1.5243 (1.5847)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0580 (6.8608)  time: 0.9828 (0.5205 -- 3.3615)  data: 0.1725 (0.0005 -- 2.0901)  max mem: 16413
[2023-09-05 07:46:59,943] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:46:59,943] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:46:59,944] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:46:59,945] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [244]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000001  loss: 1.6636 (1.5926)  loss_scale: 32768.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5325 (7.1624)  time: 0.8192 (0.5247 -- 2.2214)  data: 0.1359 (0.0006 -- 1.6605)  max mem: 16413
Epoch: [244]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000001  loss: 1.5642 (1.5669)  loss_scale: 32768.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5293 (7.1125)  time: 0.8637 (0.5277 -- 2.5380)  data: 0.2473 (0.0006 -- 2.0190)  max mem: 16413
Epoch: [244]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000001  loss: 1.5949 (1.5660)  loss_scale: 32768.0000 (24508.2975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7112 (7.1320)  time: 0.8324 (0.5243 -- 2.3656)  data: 0.0291 (0.0003 -- 0.3823)  max mem: 16413
[2023-09-05 07:47:51,019] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39162
[2023-09-05 07:47:51,019] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:47:51,019] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39162
[2023-09-05 07:47:51,020] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:47:51,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [244]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.6097 (1.5772)  loss_scale: 16384.0000 (23472.1135)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6160 (7.3286)  time: 0.9778 (0.5023 -- 4.5583)  data: 0.0148 (0.0002 -- 0.2731)  max mem: 16413
[2023-09-05 07:48:16,422] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39193
[2023-09-05 07:48:16,422] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39193
[2023-09-05 07:48:16,422] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:48:16,422] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:48:16,422] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [244]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.4827 (1.5676)  loss_scale: 16384.0000 (22272.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5559 (7.4196)  time: 0.6179 (0.4863 -- 2.1911)  data: 0.0010 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [244] Total time: 0:02:20 (0.8812 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.4827 (1.5590)  loss_scale: 16384.0000 (22272.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5559 (7.4196)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1339 (0.1339)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4795 (2.4795 -- 2.4795)  data: 2.2548 (2.2548 -- 2.2548)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5353 (0.4878)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4161 (0.1888 -- 2.4795)  data: 0.2069 (0.0006 -- 2.2548)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4933 (0.5024)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2190 (0.1697 -- 0.4850)  data: 0.0201 (0.0001 -- 0.2899)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4933 (0.5497)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (98.7552)  time: 0.2052 (0.1325 -- 0.4850)  data: 0.0194 (0.0001 -- 0.2899)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 85.062 Acc@5 98.548 loss 0.563
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.51%
Epoch: [245]  [  0/160]  eta: 0:18:30  lr: 0.000005  min_lr: 0.000001  loss: 0.9502 (0.9502)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4419 (6.4419)  time: 6.9389 (6.9389 -- 6.9389)  data: 6.4102 (6.4102 -- 6.4102)  max mem: 16413
Epoch: [245]  [ 20/160]  eta: 0:02:48  lr: 0.000005  min_lr: 0.000001  loss: 1.6262 (1.6054)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7876 (6.8164)  time: 0.9205 (0.5196 -- 3.7018)  data: 0.3750 (0.0003 -- 3.1507)  max mem: 16413
Epoch: [245]  [ 40/160]  eta: 0:02:06  lr: 0.000005  min_lr: 0.000001  loss: 1.6437 (1.5966)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7837 (6.9506)  time: 0.8977 (0.5154 -- 2.3598)  data: 0.2552 (0.0003 -- 1.7340)  max mem: 16413
Epoch: [245]  [ 60/160]  eta: 0:01:40  lr: 0.000005  min_lr: 0.000001  loss: 1.6533 (1.5885)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4182 (7.0549)  time: 0.8972 (0.5359 -- 3.7835)  data: 0.0331 (0.0009 -- 0.5167)  max mem: 16413
Epoch: [245]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000001  loss: 1.4535 (1.5471)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7915 (7.1115)  time: 0.8333 (0.5369 -- 2.9120)  data: 0.2363 (0.0004 -- 2.3762)  max mem: 16413
Epoch: [245]  [100/160]  eta: 0:00:57  lr: 0.000005  min_lr: 0.000001  loss: 1.3491 (1.5155)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4866 (7.2112)  time: 0.9689 (0.5187 -- 3.3067)  data: 0.3198 (0.0005 -- 2.7698)  max mem: 16413
Epoch: [245]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000001  loss: 1.5265 (1.5277)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9580 (7.1770)  time: 0.8653 (0.5208 -- 3.3172)  data: 0.1991 (0.0003 -- 2.8078)  max mem: 16413
[2023-09-05 07:50:24,137] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:50:24,137] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 07:50:24,139] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:50:24,140] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [245]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000001  loss: 1.5454 (1.5193)  loss_scale: 16384.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7538 (7.1216)  time: 0.8304 (0.5362 -- 2.0041)  data: 0.0758 (0.0003 -- 1.4637)  max mem: 16413
Epoch: [245]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000001  loss: 1.3996 (1.5151)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1529 (7.0669)  time: 0.7201 (0.4926 -- 2.2903)  data: 0.0979 (0.0002 -- 1.7927)  max mem: 16413
Epoch: [245] Total time: 0:02:25 (0.9067 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000001  loss: 1.3996 (1.5605)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1529 (7.0669)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.1383 (0.1383)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6514 (2.6514 -- 2.6514)  data: 2.4196 (2.4196 -- 2.4196)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4877 (0.4959)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4391 (0.1941 -- 2.6514)  data: 0.2213 (0.0007 -- 2.4196)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4877 (0.5056)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2102 (0.1698 -- 0.2427)  data: 0.0009 (0.0001 -- 0.0057)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4877 (0.5525)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.3402)  time: 0.1969 (0.1331 -- 0.2427)  data: 0.0007 (0.0001 -- 0.0057)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 84.855 Acc@5 98.340 loss 0.565
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.51%
Epoch: [246]  [  0/160]  eta: 0:20:20  lr: 0.000005  min_lr: 0.000001  loss: 1.5043 (1.5043)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3819 (7.3819)  time: 7.6279 (7.6279 -- 7.6279)  data: 7.1042 (7.1042 -- 7.1042)  max mem: 16413
Epoch: [246]  [ 20/160]  eta: 0:02:42  lr: 0.000004  min_lr: 0.000001  loss: 1.5997 (1.4912)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2232 (6.5971)  time: 0.8338 (0.5170 -- 3.5269)  data: 0.2892 (0.0002 -- 3.0201)  max mem: 16413
[2023-09-05 07:51:29,356] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39386
[2023-09-05 07:51:29,356] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39386
[2023-09-05 07:51:29,356] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:51:29,356] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 07:51:29,356] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [246]  [ 40/160]  eta: 0:02:08  lr: 0.000004  min_lr: 0.000001  loss: 1.5670 (1.5475)  loss_scale: 8192.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1647 (7.2600)  time: 0.9829 (0.5138 -- 4.5794)  data: 0.3460 (0.0004 -- 2.9401)  max mem: 16413
Epoch: [246]  [ 60/160]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000001  loss: 1.7695 (1.5751)  loss_scale: 8192.0000 (11683.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1203 (7.0731)  time: 0.8505 (0.5328 -- 4.5187)  data: 0.0745 (0.0003 -- 1.2509)  max mem: 16413
Epoch: [246]  [ 80/160]  eta: 0:01:20  lr: 0.000004  min_lr: 0.000001  loss: 1.4816 (1.5599)  loss_scale: 8192.0000 (10821.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1845 (7.2175)  time: 1.0255 (0.5187 -- 4.2419)  data: 0.0013 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [246]  [100/160]  eta: 0:00:57  lr: 0.000004  min_lr: 0.000001  loss: 1.6879 (1.5809)  loss_scale: 8192.0000 (10300.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7008 (7.3525)  time: 0.7304 (0.5204 -- 2.4306)  data: 0.0014 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [246]  [120/160]  eta: 0:00:38  lr: 0.000004  min_lr: 0.000001  loss: 1.6963 (1.6000)  loss_scale: 8192.0000 (9952.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5316 (7.2277)  time: 1.0504 (0.5292 -- 4.4186)  data: 0.0016 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [246]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.4228 (1.5799)  loss_scale: 8192.0000 (9702.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6193 (7.1718)  time: 0.7453 (0.5184 -- 2.7684)  data: 0.0013 (0.0003 -- 0.0050)  max mem: 16413
[2023-09-05 07:53:23,553] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:53:23,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 07:53:23,553] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:53:23,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [246]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.6976 (1.5987)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3569 (7.1376)  time: 0.6930 (0.4962 -- 3.7341)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [246] Total time: 0:02:25 (0.9084 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.6976 (1.5793)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3569 (7.1376)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1354 (0.1354)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4482 (2.4482 -- 2.4482)  data: 2.1992 (2.1992 -- 2.1992)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5045 (0.4917)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4188 (0.1933 -- 2.4482)  data: 0.2026 (0.0006 -- 2.1992)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4728 (0.4975)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2211 (0.1693 -- 0.3716)  data: 0.0147 (0.0001 -- 0.1741)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4728 (0.5427)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.7552)  time: 0.2075 (0.1330 -- 0.3716)  data: 0.0145 (0.0001 -- 0.1741)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 85.477 Acc@5 98.548 loss 0.559
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [247]  [  0/160]  eta: 0:25:28  lr: 0.000004  min_lr: 0.000001  loss: 1.5399 (1.5399)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6252 (5.6252)  time: 9.5512 (9.5512 -- 9.5512)  data: 7.5994 (7.5994 -- 7.5994)  max mem: 16413
Epoch: [247]  [ 20/160]  eta: 0:02:49  lr: 0.000004  min_lr: 0.000001  loss: 1.6099 (1.5456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5554 (7.8449)  time: 0.7920 (0.5230 -- 3.1340)  data: 0.0354 (0.0004 -- 0.6684)  max mem: 16413
Epoch: [247]  [ 40/160]  eta: 0:02:03  lr: 0.000004  min_lr: 0.000001  loss: 1.5696 (1.5177)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1405 (7.7754)  time: 0.8444 (0.5300 -- 2.8626)  data: 0.0177 (0.0002 -- 0.3217)  max mem: 16413
Epoch: [247]  [ 60/160]  eta: 0:01:41  lr: 0.000004  min_lr: 0.000001  loss: 1.6411 (1.5472)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4350 (7.8370)  time: 0.9896 (0.5268 -- 3.6141)  data: 0.0851 (0.0007 -- 0.6587)  max mem: 16413
Epoch: [247]  [ 80/160]  eta: 0:01:17  lr: 0.000004  min_lr: 0.000001  loss: 1.7176 (1.5780)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9198 (7.7405)  time: 0.8150 (0.5244 -- 3.2963)  data: 0.0601 (0.0004 -- 1.1748)  max mem: 16413
Epoch: [247]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000001  loss: 1.6487 (1.5878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3000 (7.6134)  time: 0.8289 (0.5124 -- 4.1713)  data: 0.0357 (0.0004 -- 0.6836)  max mem: 16413
Epoch: [247]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000001  loss: 1.6311 (1.6004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7525 (7.4996)  time: 0.8059 (0.5300 -- 2.2477)  data: 0.1458 (0.0003 -- 1.0217)  max mem: 16413
[2023-09-05 07:55:27,170] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:55:27,170] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:55:27,177] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:55:27,178] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [247]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.5345 (1.5939)  loss_scale: 32768.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3887 (7.4128)  time: 0.9030 (0.5521 -- 2.9513)  data: 0.1917 (0.0009 -- 1.1638)  max mem: 16413
Epoch: [247]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.6165 (1.6002)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7743 (7.4658)  time: 0.7034 (0.4964 -- 2.3381)  data: 0.1243 (0.0002 -- 1.8104)  max mem: 16413
Epoch: [247] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.6165 (1.6003)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7743 (7.4658)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1406 (0.1406)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3930 (2.3930 -- 2.3930)  data: 2.1491 (2.1491 -- 2.1491)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4570 (0.4954)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4267 (0.2024 -- 2.3930)  data: 0.2056 (0.0004 -- 2.1491)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4570 (0.5021)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2225 (0.1725 -- 0.3469)  data: 0.0152 (0.0001 -- 0.1476)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4570 (0.5510)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2058 (0.1323 -- 0.3469)  data: 0.0148 (0.0001 -- 0.1476)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 85.270 Acc@5 98.340 loss 0.566
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [248]  [  0/160]  eta: 0:20:15  lr: 0.000004  min_lr: 0.000001  loss: 1.7201 (1.7201)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2062 (6.2062)  time: 7.5968 (7.5968 -- 7.5968)  data: 7.0427 (7.0427 -- 7.0427)  max mem: 16413
[2023-09-05 07:56:27,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39697
[2023-09-05 07:56:27,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39697
[2023-09-05 07:56:27,193] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:56:27,193] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:56:27,194] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [248]  [ 20/160]  eta: 0:02:46  lr: 0.000004  min_lr: 0.000001  loss: 1.5416 (1.6364)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5693 (6.9725)  time: 0.8654 (0.5308 -- 3.0336)  data: 0.2393 (0.0005 -- 2.4884)  max mem: 16413
Epoch: [248]  [ 40/160]  eta: 0:02:05  lr: 0.000004  min_lr: 0.000001  loss: 1.4471 (1.5513)  loss_scale: 16384.0000 (23177.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0960 (6.7269)  time: 0.8904 (0.5190 -- 3.1772)  data: 0.0017 (0.0001 -- 0.0051)  max mem: 16413
Epoch: [248]  [ 60/160]  eta: 0:01:40  lr: 0.000004  min_lr: 0.000001  loss: 1.6360 (1.5948)  loss_scale: 16384.0000 (20950.0328)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7972 (6.7610)  time: 0.9261 (0.5253 -- 4.7230)  data: 0.0235 (0.0005 -- 0.4308)  max mem: 16413
Epoch: [248]  [ 80/160]  eta: 0:01:18  lr: 0.000004  min_lr: 0.000001  loss: 1.5791 (1.5884)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0085 (6.9419)  time: 0.8998 (0.5357 -- 3.2618)  data: 0.0013 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [248]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000001  loss: 1.5241 (1.5680)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0433 (6.9305)  time: 0.7733 (0.5257 -- 2.2623)  data: 0.0605 (0.0007 -- 0.8524)  max mem: 16413
Epoch: [248]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000001  loss: 1.5177 (1.5574)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7192 (6.9290)  time: 0.9838 (0.5325 -- 3.8463)  data: 0.0022 (0.0003 -- 0.0082)  max mem: 16413
Epoch: [248]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.5673 (1.5627)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6183 (7.0575)  time: 0.7488 (0.5289 -- 3.7920)  data: 0.0368 (0.0004 -- 0.5201)  max mem: 16413
[2023-09-05 07:58:19,197] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:58:19,197] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 07:58:19,197] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 07:58:19,197] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [248]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.5032 (1.5666)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4238 (7.0079)  time: 0.7160 (0.4947 -- 3.1260)  data: 0.0391 (0.0002 -- 0.5201)  max mem: 16413
Epoch: [248] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.5032 (1.5632)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4238 (7.0079)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1334 (0.1334)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4573 (2.4573 -- 2.4573)  data: 2.1825 (2.1825 -- 2.1825)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5064 (0.4900)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4239 (0.2054 -- 2.4573)  data: 0.2009 (0.0004 -- 2.1825)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4838 (0.4969)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2211 (0.1710 -- 0.4391)  data: 0.0143 (0.0001 -- 0.2559)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4838 (0.5450)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2019 (0.1330 -- 0.4391)  data: 0.0132 (0.0001 -- 0.2559)  max mem: 16413
Val: Total time: 0:00:07 (0.2915 s / it)
* Acc@1 84.855 Acc@5 98.548 loss 0.560
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.51%
Epoch: [249]  [  0/160]  eta: 0:20:43  lr: 0.000004  min_lr: 0.000001  loss: 1.3364 (1.3364)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8205 (8.8205)  time: 7.7709 (7.7709 -- 7.7709)  data: 6.1182 (6.1182 -- 6.1182)  max mem: 16413
Epoch: [249]  [ 20/160]  eta: 0:02:51  lr: 0.000004  min_lr: 0.000001  loss: 1.6396 (1.5839)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1190 (7.1902)  time: 0.8967 (0.5223 -- 3.4977)  data: 0.0396 (0.0003 -- 0.7423)  max mem: 16413
Epoch: [249]  [ 40/160]  eta: 0:02:00  lr: 0.000004  min_lr: 0.000001  loss: 1.8107 (1.6534)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0893 (7.0108)  time: 0.7674 (0.5262 -- 2.2992)  data: 0.1819 (0.0003 -- 1.7687)  max mem: 16413
Epoch: [249]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000001  loss: 1.5118 (1.5916)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8951 (7.0430)  time: 0.8759 (0.5200 -- 2.3642)  data: 0.3111 (0.0006 -- 1.8338)  max mem: 16413
[2023-09-05 07:59:42,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39911
[2023-09-05 07:59:42,622] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39911
[2023-09-05 07:59:42,622] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:59:42,622] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 07:59:42,622] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [249]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000001  loss: 1.5551 (1.5770)  loss_scale: 16384.0000 (30745.2840)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8954 (7.3173)  time: 0.9330 (0.5303 -- 2.8692)  data: 0.2709 (0.0006 -- 1.8691)  max mem: 16413
Epoch: [249]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000001  loss: 1.4688 (1.5569)  loss_scale: 16384.0000 (27901.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8512 (7.2821)  time: 0.9100 (0.5202 -- 2.9224)  data: 0.1985 (0.0004 -- 2.3901)  max mem: 16413
Epoch: [249]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000001  loss: 1.4448 (1.5502)  loss_scale: 16384.0000 (25997.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2529 (7.3375)  time: 0.7894 (0.5300 -- 2.4454)  data: 0.0974 (0.0004 -- 1.9047)  max mem: 16413
Epoch: [249]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.5834 (1.5472)  loss_scale: 16384.0000 (24634.0993)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6283 (7.3655)  time: 0.9850 (0.5154 -- 4.6720)  data: 0.0020 (0.0003 -- 0.0062)  max mem: 16413
[2023-09-05 08:00:57,532] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39995
[2023-09-05 08:00:57,532] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:00:57,532] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 39995
[2023-09-05 08:00:57,532] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:00:57,533] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-05 08:00:59,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=227, lr=[9.912538004576367e-07, 9.912538004576367e-07, 1.1013931116195966e-06, 1.1013931116195966e-06, 1.2237701240217737e-06, 1.2237701240217737e-06, 1.3597445822464155e-06, 1.3597445822464155e-06, 1.510827313607128e-06, 1.510827313607128e-06, 1.6786970151190312e-06, 1.6786970151190312e-06, 1.8652189056878124e-06, 1.8652189056878124e-06, 2.0724654507642358e-06, 2.0724654507642358e-06, 2.3027393897380398e-06, 2.3027393897380398e-06, 2.558599321931155e-06, 2.558599321931155e-06, 2.8428881354790617e-06, 2.8428881354790617e-06, 3.1587645949767347e-06, 3.1587645949767347e-06, 3.5097384388630384e-06, 3.5097384388630384e-06, 3.899709376514487e-06, 3.899709376514487e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 08:00:59,517] [INFO] [timer.py:260:stop] epoch=0/micro_step=40000/global_step=40000, RunningAvgSamplesPerSec=17.771651150091174, CurrSamplesPerSec=24.773022948134383, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [249]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.6196 (1.5656)  loss_scale: 16384.0000 (23398.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6690 (7.4175)  time: 0.7339 (0.4866 -- 4.8445)  data: 0.0011 (0.0001 -- 0.0092)  max mem: 16413
Epoch: [249] Total time: 0:02:25 (0.9066 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.6196 (1.5576)  loss_scale: 16384.0000 (23398.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6690 (7.4175)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1306 (0.1306)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4439 (2.4439 -- 2.4439)  data: 2.1715 (2.1715 -- 2.1715)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5414 (0.4892)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4191 (0.2022 -- 2.4439)  data: 0.1993 (0.0007 -- 2.1715)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4779 (0.4948)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2222 (0.1694 -- 0.5392)  data: 0.0187 (0.0001 -- 0.3499)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4779 (0.5436)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2057 (0.1332 -- 0.5392)  data: 0.0179 (0.0001 -- 0.3499)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 85.477 Acc@5 98.548 loss 0.561
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [250]  [  0/160]  eta: 0:22:00  lr: 0.000004  min_lr: 0.000001  loss: 1.4699 (1.4699)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6094 (8.6094)  time: 8.2519 (8.2519 -- 8.2519)  data: 6.3320 (6.3320 -- 6.3320)  max mem: 16413
Epoch: [250]  [ 20/160]  eta: 0:02:42  lr: 0.000004  min_lr: 0.000001  loss: 1.3028 (1.4319)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4540 (7.9664)  time: 0.8073 (0.5276 -- 2.0596)  data: 0.1887 (0.0003 -- 1.5221)  max mem: 16413
Epoch: [250]  [ 40/160]  eta: 0:02:10  lr: 0.000004  min_lr: 0.000001  loss: 1.4003 (1.4383)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3894 (7.4967)  time: 1.0033 (0.5231 -- 4.7584)  data: 0.4610 (0.0005 -- 4.2481)  max mem: 16413
Epoch: [250]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000001  loss: 1.4492 (1.4370)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8264 (7.4249)  time: 0.7854 (0.5199 -- 3.5767)  data: 0.2308 (0.0005 -- 3.0162)  max mem: 16413
Epoch: [250]  [ 80/160]  eta: 0:01:18  lr: 0.000004  min_lr: 0.000001  loss: 1.4582 (1.4671)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3550 (7.2967)  time: 0.9525 (0.5065 -- 4.4144)  data: 0.0645 (0.0003 -- 1.2727)  max mem: 16413
Epoch: [250]  [100/160]  eta: 0:00:58  lr: 0.000004  min_lr: 0.000001  loss: 1.3783 (1.4774)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5436 (7.1690)  time: 0.9285 (0.5189 -- 3.8649)  data: 0.0010 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [250]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000001  loss: 1.5897 (1.4791)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6085 (7.2012)  time: 0.7499 (0.5149 -- 3.1842)  data: 0.0013 (0.0003 -- 0.0032)  max mem: 16413
[2023-09-05 08:03:04,133] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:03:04,133] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 08:03:04,135] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:03:04,136] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [250]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.4764 (1.4780)  loss_scale: 16384.0000 (9179.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5860 (7.1565)  time: 0.9017 (0.5238 -- 3.8584)  data: 0.0016 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [250]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.6235 (1.5023)  loss_scale: 16384.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5952 (7.1671)  time: 0.6549 (0.4945 -- 2.3327)  data: 0.0280 (0.0001 -- 0.5385)  max mem: 16413
Epoch: [250] Total time: 0:02:23 (0.8964 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.6235 (1.5476)  loss_scale: 16384.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5952 (7.1671)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1288 (0.1288)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2401 (2.2401 -- 2.2401)  data: 2.0335 (2.0335 -- 2.0335)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5008 (0.5002)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4203 (0.2029 -- 2.2401)  data: 0.2020 (0.0005 -- 2.0335)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4395 (0.4909)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2301 (0.1699 -- 0.4384)  data: 0.0247 (0.0001 -- 0.2625)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4395 (0.5418)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (98.3402)  time: 0.2143 (0.1328 -- 0.4384)  data: 0.0244 (0.0001 -- 0.2625)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 84.440 Acc@5 98.548 loss 0.563
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 86.51%
Epoch: [251]  [  0/160]  eta: 0:19:52  lr: 0.000004  min_lr: 0.000001  loss: 1.5899 (1.5899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5188 (7.5188)  time: 7.4504 (7.4504 -- 7.4504)  data: 6.9097 (6.9097 -- 6.9097)  max mem: 16413
Epoch: [251]  [ 20/160]  eta: 0:02:52  lr: 0.000004  min_lr: 0.000001  loss: 1.5353 (1.5756)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5013 (6.9752)  time: 0.9228 (0.5336 -- 2.9458)  data: 0.0665 (0.0005 -- 0.8291)  max mem: 16413
Epoch: [251]  [ 40/160]  eta: 0:02:03  lr: 0.000004  min_lr: 0.000001  loss: 1.4310 (1.5184)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0053 (7.1981)  time: 0.8086 (0.5371 -- 3.1216)  data: 0.0380 (0.0005 -- 0.4417)  max mem: 16413
Epoch: [251]  [ 60/160]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000001  loss: 1.5735 (1.5209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7830 (7.1292)  time: 0.9371 (0.5210 -- 3.2606)  data: 0.0357 (0.0001 -- 0.6779)  max mem: 16413
Epoch: [251]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000001  loss: 1.4037 (1.5098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5163 (7.0181)  time: 0.8499 (0.5203 -- 3.0914)  data: 0.0427 (0.0002 -- 0.8250)  max mem: 16413
[2023-09-05 08:05:08,418] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:05:08,418] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:05:08,418] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:05:08,418] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:05:11,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 40257
[2023-09-05 08:05:11,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 40257
[2023-09-05 08:05:11,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:05:11,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:05:11,157] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [251]  [100/160]  eta: 0:00:57  lr: 0.000004  min_lr: 0.000001  loss: 1.6055 (1.5282)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6120 (7.0541)  time: 0.9407 (0.5295 -- 3.4366)  data: 0.0014 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [251]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000001  loss: 1.6488 (1.5576)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3127 (7.0195)  time: 0.8159 (0.5283 -- 3.5465)  data: 0.0023 (0.0003 -- 0.0163)  max mem: 16413
Epoch: [251]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000001  loss: 1.7612 (1.5809)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9589 (7.0560)  time: 0.9279 (0.5197 -- 3.4428)  data: 0.0017 (0.0003 -- 0.0051)  max mem: 16413
Epoch: [251]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000001  loss: 1.5499 (1.5746)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4707 (7.0840)  time: 0.6365 (0.4950 -- 2.2855)  data: 0.0012 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [251] Total time: 0:02:23 (0.8984 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000001  loss: 1.5499 (1.5705)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4707 (7.0840)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1321 (0.1321)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4653 (2.4653 -- 2.4653)  data: 2.2522 (2.2522 -- 2.2522)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4919 (0.4956)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4225 (0.2001 -- 2.4653)  data: 0.2056 (0.0006 -- 2.2522)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4563 (0.4920)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2214 (0.1709 -- 0.4019)  data: 0.0113 (0.0001 -- 0.2132)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4563 (0.5383)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (98.3402)  time: 0.2061 (0.1331 -- 0.4019)  data: 0.0111 (0.0001 -- 0.2132)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 85.477 Acc@5 98.340 loss 0.557
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [252]  [  0/160]  eta: 0:18:14  lr: 0.000004  min_lr: 0.000001  loss: 1.7579 (1.7579)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8228 (8.8228)  time: 6.8391 (6.8391 -- 6.8391)  data: 6.0422 (6.0422 -- 6.0422)  max mem: 16413
Epoch: [252]  [ 20/160]  eta: 0:02:44  lr: 0.000004  min_lr: 0.000001  loss: 1.4828 (1.4805)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1641 (7.3920)  time: 0.8896 (0.5157 -- 2.8107)  data: 0.0903 (0.0005 -- 1.7683)  max mem: 16413
Epoch: [252]  [ 40/160]  eta: 0:02:10  lr: 0.000004  min_lr: 0.000001  loss: 1.5186 (1.5171)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9594 (7.2987)  time: 1.0032 (0.5361 -- 5.0950)  data: 0.1642 (0.0006 -- 1.4878)  max mem: 16413
Epoch: [252]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000001  loss: 1.6682 (1.5366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5712 (7.1299)  time: 0.7794 (0.5208 -- 2.7672)  data: 0.0013 (0.0004 -- 0.0049)  max mem: 16413
[2023-09-05 08:07:16,069] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:07:16,069] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:07:16,072] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:07:16,073] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [252]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000001  loss: 1.5029 (1.5364)  loss_scale: 32768.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8482 (7.0011)  time: 0.7959 (0.5257 -- 2.3340)  data: 0.0014 (0.0007 -- 0.0024)  max mem: 16413
Epoch: [252]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000001  loss: 1.4380 (1.5373)  loss_scale: 32768.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2226 (6.9953)  time: 0.8451 (0.5497 -- 2.2784)  data: 0.0852 (0.0003 -- 1.4965)  max mem: 16413
Epoch: [252]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 1.5331 (1.5347)  loss_scale: 32768.0000 (23831.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4791 (7.1081)  time: 0.8764 (0.5406 -- 2.1852)  data: 0.1893 (0.0005 -- 1.6329)  max mem: 16413
Epoch: [252]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.6122 (1.5593)  loss_scale: 32768.0000 (25098.8936)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5819 (7.0783)  time: 0.9217 (0.5262 -- 5.1637)  data: 0.1790 (0.0003 -- 1.2775)  max mem: 16413
Epoch: [252]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.6764 (1.5668)  loss_scale: 32768.0000 (26009.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7217 (7.0510)  time: 0.6698 (0.4971 -- 3.3267)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [252] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.6764 (1.5704)  loss_scale: 32768.0000 (26009.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7217 (7.0510)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1324 (0.1324)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4075 (2.4075 -- 2.4075)  data: 2.1801 (2.1801 -- 2.1801)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5598 (0.4998)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4275 (0.2155 -- 2.4075)  data: 0.2027 (0.0007 -- 2.1801)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4765 (0.4973)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (98.4127)  time: 0.2208 (0.1697 -- 0.3458)  data: 0.0107 (0.0001 -- 0.1603)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4765 (0.5439)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2006 (0.1330 -- 0.3458)  data: 0.0100 (0.0001 -- 0.1603)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 85.892 Acc@5 98.548 loss 0.559
Accuracy of the network on the 482 val images: 85.89%
Max accuracy: 86.51%
Epoch: [253]  [  0/160]  eta: 0:17:27  lr: 0.000003  min_lr: 0.000001  loss: 1.9007 (1.9007)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5901 (5.5901)  time: 6.5462 (6.5462 -- 6.5462)  data: 6.0184 (6.0184 -- 6.0184)  max mem: 16413
Epoch: [253]  [ 20/160]  eta: 0:02:39  lr: 0.000003  min_lr: 0.000001  loss: 1.6826 (1.6499)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9984 (7.2398)  time: 0.8725 (0.5330 -- 2.1926)  data: 0.1690 (0.0005 -- 1.6299)  max mem: 16413
[2023-09-05 08:09:05,262] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 40502
[2023-09-05 08:09:05,262] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 40502
[2023-09-05 08:09:05,262] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:09:05,262] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:09:05,263] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [253]  [ 40/160]  eta: 0:02:03  lr: 0.000003  min_lr: 0.000001  loss: 1.6966 (1.6924)  loss_scale: 16384.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8054 (7.3144)  time: 0.9046 (0.5370 -- 2.0955)  data: 0.3451 (0.0005 -- 1.5612)  max mem: 16413
Epoch: [253]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000001  loss: 1.6514 (1.6303)  loss_scale: 16384.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4926 (7.1428)  time: 0.8644 (0.5233 -- 2.8206)  data: 0.2362 (0.0002 -- 2.2915)  max mem: 16413
Epoch: [253]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000001  loss: 1.4645 (1.6014)  loss_scale: 16384.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8429 (7.1751)  time: 0.8458 (0.5407 -- 2.7991)  data: 0.2029 (0.0003 -- 2.2593)  max mem: 16413
Epoch: [253]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000001  loss: 1.6716 (1.6211)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4661 (7.0799)  time: 0.9076 (0.5222 -- 3.3777)  data: 0.1648 (0.0005 -- 1.7728)  max mem: 16413
Epoch: [253]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000001  loss: 1.4693 (1.6021)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7930 (7.1020)  time: 0.9363 (0.5260 -- 2.5305)  data: 0.0855 (0.0007 -- 0.8757)  max mem: 16413
Epoch: [253]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.5057 (1.5956)  loss_scale: 16384.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1033 (7.1325)  time: 0.8701 (0.5248 -- 2.8758)  data: 0.2244 (0.0004 -- 2.3546)  max mem: 16413
[2023-09-05 08:10:58,504] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:10:58,504] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:10:58,506] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:10:58,506] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [253]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.4195 (1.5835)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9891 (7.1011)  time: 0.6786 (0.4986 -- 2.3031)  data: 0.1509 (0.0002 -- 1.7709)  max mem: 16413
Epoch: [253] Total time: 0:02:23 (0.8976 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.4195 (1.5773)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9891 (7.1011)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1321 (0.1321)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4717 (2.4717 -- 2.4717)  data: 2.2417 (2.2417 -- 2.2417)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5549 (0.5000)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4355 (0.2162 -- 2.4717)  data: 0.2094 (0.0007 -- 2.2417)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4664 (0.4990)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2218 (0.1692 -- 0.3643)  data: 0.0116 (0.0001 -- 0.1671)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4664 (0.5424)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2011 (0.1325 -- 0.3643)  data: 0.0107 (0.0001 -- 0.1671)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 85.270 Acc@5 98.548 loss 0.559
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [254]  [  0/160]  eta: 0:17:48  lr: 0.000003  min_lr: 0.000001  loss: 1.4161 (1.4161)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9264 (11.9264)  time: 6.6759 (6.6759 -- 6.6759)  data: 6.1516 (6.1516 -- 6.1516)  max mem: 16413
[2023-09-05 08:11:32,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 40654
[2023-09-05 08:11:32,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 40654
[2023-09-05 08:11:32,103] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:11:32,103] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:11:32,103] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [254]  [ 20/160]  eta: 0:02:43  lr: 0.000003  min_lr: 0.000001  loss: 1.4322 (1.4808)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1732 (7.1956)  time: 0.8901 (0.5209 -- 3.6488)  data: 0.0737 (0.0003 -- 1.4412)  max mem: 16413
Epoch: [254]  [ 40/160]  eta: 0:02:04  lr: 0.000003  min_lr: 0.000001  loss: 1.6464 (1.5469)  loss_scale: 16384.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8219 (7.4835)  time: 0.9079 (0.5324 -- 2.4424)  data: 0.1687 (0.0008 -- 1.2287)  max mem: 16413
Epoch: [254]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000001  loss: 1.5985 (1.5517)  loss_scale: 16384.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7511 (7.4634)  time: 0.8001 (0.5220 -- 1.8938)  data: 0.1507 (0.0005 -- 1.3317)  max mem: 16413
Epoch: [254]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000001  loss: 1.5394 (1.5683)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0445 (7.4351)  time: 0.9107 (0.5304 -- 3.0445)  data: 0.3371 (0.0004 -- 2.5106)  max mem: 16413
Epoch: [254]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000001  loss: 1.4527 (1.5683)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7734 (7.3915)  time: 0.8424 (0.5162 -- 3.8460)  data: 0.2916 (0.0002 -- 3.2878)  max mem: 16413
Epoch: [254]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000001  loss: 1.4743 (1.5518)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9408 (7.3443)  time: 0.9141 (0.5230 -- 3.6218)  data: 0.3543 (0.0003 -- 3.0875)  max mem: 16413
Epoch: [254]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.5017 (1.5429)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3768 (7.3141)  time: 0.8592 (0.5241 -- 5.5544)  data: 0.3142 (0.0004 -- 5.0364)  max mem: 16413
[2023-09-05 08:13:25,672] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:13:25,672] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:13:25,673] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:13:25,674] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [254]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.2514 (1.5278)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4475 (7.2425)  time: 0.7571 (0.4951 -- 3.7240)  data: 0.1162 (0.0002 -- 2.3148)  max mem: 16413
Epoch: [254] Total time: 0:02:23 (0.8986 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.2514 (1.5289)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4475 (7.2425)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1296 (0.1296)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4298 (2.4298 -- 2.4298)  data: 2.1875 (2.1875 -- 2.1875)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5509 (0.4963)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4294 (0.2060 -- 2.4298)  data: 0.2086 (0.0007 -- 2.1875)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4764 (0.4983)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2192 (0.1704 -- 0.3054)  data: 0.0112 (0.0001 -- 0.1148)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4764 (0.5426)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2015 (0.1328 -- 0.3054)  data: 0.0109 (0.0001 -- 0.1148)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 85.685 Acc@5 98.548 loss 0.560
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [255]  [  0/160]  eta: 0:21:31  lr: 0.000003  min_lr: 0.000001  loss: 1.6757 (1.6757)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1872 (9.1872)  time: 8.0739 (8.0739 -- 8.0739)  data: 7.5365 (7.5365 -- 7.5365)  max mem: 16413
Epoch: [255]  [ 20/160]  eta: 0:02:49  lr: 0.000003  min_lr: 0.000001  loss: 1.5975 (1.5569)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0603 (7.0227)  time: 0.8711 (0.5334 -- 5.0371)  data: 0.2137 (0.0004 -- 2.3624)  max mem: 16413
Epoch: [255]  [ 40/160]  eta: 0:02:12  lr: 0.000003  min_lr: 0.000001  loss: 1.5061 (1.5311)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0376 (6.9010)  time: 0.9952 (0.5337 -- 4.1602)  data: 0.4338 (0.0002 -- 3.6218)  max mem: 16413
Epoch: [255]  [ 60/160]  eta: 0:01:42  lr: 0.000003  min_lr: 0.000001  loss: 1.4657 (1.5202)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2743 (6.4652)  time: 0.8555 (0.5316 -- 4.9922)  data: 0.3109 (0.0004 -- 4.4647)  max mem: 16413
Epoch: [255]  [ 80/160]  eta: 0:01:20  lr: 0.000003  min_lr: 0.000001  loss: 1.6466 (1.5423)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8605 (6.6668)  time: 0.9344 (0.5292 -- 3.9702)  data: 0.3847 (0.0003 -- 3.4335)  max mem: 16413
Epoch: [255]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000001  loss: 1.4786 (1.5244)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8068 (6.8147)  time: 0.6988 (0.5327 -- 2.5905)  data: 0.1440 (0.0003 -- 2.0612)  max mem: 16413
[2023-09-05 08:15:24,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 40905
[2023-09-05 08:15:24,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 40905
[2023-09-05 08:15:24,359] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:15:24,359] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:15:24,359] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [255]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000001  loss: 1.4409 (1.5249)  loss_scale: 16384.0000 (30601.5207)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2957 (6.9094)  time: 0.8909 (0.5166 -- 3.7629)  data: 0.3476 (0.0003 -- 3.2209)  max mem: 16413
Epoch: [255]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.5483 (1.5303)  loss_scale: 16384.0000 (28584.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7486 (7.0789)  time: 0.8319 (0.5307 -- 2.1758)  data: 0.2287 (0.0013 -- 1.5299)  max mem: 16413
Epoch: [255]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.5206 (1.5280)  loss_scale: 16384.0000 (27136.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7366 (7.0573)  time: 0.6828 (0.4962 -- 1.6962)  data: 0.1011 (0.0002 -- 0.8948)  max mem: 16413
Epoch: [255] Total time: 0:02:22 (0.8922 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.5206 (1.5570)  loss_scale: 16384.0000 (27136.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7366 (7.0573)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1290 (0.1290)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5610 (2.5610 -- 2.5610)  data: 2.2916 (2.2916 -- 2.2916)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5626 (0.5008)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4263 (0.1980 -- 2.5610)  data: 0.2100 (0.0008 -- 2.2916)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4802 (0.5027)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2134 (0.1693 -- 0.3688)  data: 0.0105 (0.0001 -- 0.1890)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4802 (0.5482)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.1993 (0.1328 -- 0.3688)  data: 0.0100 (0.0001 -- 0.1890)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.565
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [256]  [  0/160]  eta: 0:19:56  lr: 0.000003  min_lr: 0.000001  loss: 2.0503 (2.0503)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2926 (9.2926)  time: 7.4766 (7.4766 -- 7.4766)  data: 6.5345 (6.5345 -- 6.5345)  max mem: 16413
Epoch: [256]  [ 20/160]  eta: 0:02:36  lr: 0.000003  min_lr: 0.000001  loss: 1.5134 (1.5477)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4583 (6.9985)  time: 0.7971 (0.5271 -- 2.6755)  data: 0.1619 (0.0002 -- 1.6678)  max mem: 16413
[2023-09-05 08:16:55,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=231, lr=[7.667470300034135e-07, 7.667470300034135e-07, 8.519411444482372e-07, 8.519411444482372e-07, 9.466012716091523e-07, 9.466012716091523e-07, 1.051779190676836e-06, 1.051779190676836e-06, 1.1686435451964845e-06, 1.1686435451964845e-06, 1.2984928279960939e-06, 1.2984928279960939e-06, 1.4427698088845486e-06, 1.4427698088845486e-06, 1.6030775654272762e-06, 1.6030775654272762e-06, 1.7811972949191958e-06, 1.7811972949191958e-06, 1.9791081054657727e-06, 1.9791081054657727e-06, 2.199009006073081e-06, 2.199009006073081e-06, 2.4433433400812013e-06, 2.4433433400812013e-06, 2.714825933423557e-06, 2.714825933423557e-06, 3.0164732593595074e-06, 3.0164732593595074e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 08:16:55,812] [INFO] [timer.py:260:stop] epoch=0/micro_step=41000/global_step=41000, RunningAvgSamplesPerSec=17.76439004420993, CurrSamplesPerSec=22.41889307834329, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [256]  [ 40/160]  eta: 0:02:04  lr: 0.000003  min_lr: 0.000001  loss: 1.5298 (1.5702)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7325 (6.9950)  time: 0.9573 (0.5256 -- 4.8442)  data: 0.2296 (0.0005 -- 2.3922)  max mem: 16413
Epoch: [256]  [ 60/160]  eta: 0:01:42  lr: 0.000003  min_lr: 0.000001  loss: 1.5473 (1.5987)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5579 (7.0760)  time: 0.9922 (0.5269 -- 5.0278)  data: 0.0022 (0.0005 -- 0.0066)  max mem: 16413
[2023-09-05 08:17:18,457] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41024
[2023-09-05 08:17:18,457] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41024
[2023-09-05 08:17:18,457] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:17:18,457] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:17:18,458] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [256]  [ 80/160]  eta: 0:01:18  lr: 0.000003  min_lr: 0.000001  loss: 1.5285 (1.5937)  loss_scale: 8192.0000 (14664.6914)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5247 (7.0061)  time: 0.8568 (0.5158 -- 3.8281)  data: 0.0011 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [256]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000001  loss: 1.6358 (1.5891)  loss_scale: 8192.0000 (13382.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3803 (7.1168)  time: 0.7840 (0.5190 -- 2.8688)  data: 0.0011 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [256]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 1.6839 (1.6041)  loss_scale: 8192.0000 (12524.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0120 (7.1396)  time: 0.7898 (0.5314 -- 3.0756)  data: 0.0133 (0.0005 -- 0.2212)  max mem: 16413
Epoch: [256]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.6642 (1.6117)  loss_scale: 8192.0000 (11910.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5709 (7.3229)  time: 0.9175 (0.5325 -- 3.7096)  data: 0.0033 (0.0003 -- 0.0168)  max mem: 16413
Epoch: [256]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.5906 (1.6084)  loss_scale: 8192.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9141 (7.2495)  time: 0.7261 (0.4947 -- 3.0697)  data: 0.0189 (0.0002 -- 0.3636)  max mem: 16413
Epoch: [256] Total time: 0:02:23 (0.8960 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.5906 (1.5739)  loss_scale: 8192.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9141 (7.2495)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4554 (2.4554 -- 2.4554)  data: 2.2229 (2.2229 -- 2.2229)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5873 (0.5103)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4381 (0.1943 -- 2.4554)  data: 0.2198 (0.0006 -- 2.2229)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4904 (0.5109)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2197 (0.1689 -- 0.3468)  data: 0.0145 (0.0001 -- 0.0999)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4904 (0.5568)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2057 (0.1332 -- 0.3468)  data: 0.0142 (0.0001 -- 0.0999)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 84.855 Acc@5 98.133 loss 0.568
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 86.51%
Epoch: [257]  [  0/160]  eta: 0:20:13  lr: 0.000003  min_lr: 0.000001  loss: 1.9491 (1.9491)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6780 (6.6780)  time: 7.5837 (7.5837 -- 7.5837)  data: 5.6496 (5.6496 -- 5.6496)  max mem: 16413
Epoch: [257]  [ 20/160]  eta: 0:02:39  lr: 0.000003  min_lr: 0.000001  loss: 1.5226 (1.5952)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4554 (7.0764)  time: 0.8153 (0.5262 -- 2.5058)  data: 0.1414 (0.0003 -- 1.9538)  max mem: 16413
[2023-09-05 08:19:21,819] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:19:21,819] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 08:19:21,820] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:19:21,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [257]  [ 40/160]  eta: 0:02:06  lr: 0.000003  min_lr: 0.000001  loss: 1.5371 (1.5750)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8489 (6.8717)  time: 0.9726 (0.5388 -- 3.5849)  data: 0.4205 (0.0010 -- 3.0427)  max mem: 16413
Epoch: [257]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000001  loss: 1.5562 (1.5943)  loss_scale: 16384.0000 (11952.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0957 (6.8838)  time: 0.9030 (0.5291 -- 4.3458)  data: 0.0457 (0.0003 -- 0.8661)  max mem: 16413
Epoch: [257]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000001  loss: 1.5867 (1.5799)  loss_scale: 16384.0000 (13046.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0503 (6.9262)  time: 0.8340 (0.5281 -- 3.9140)  data: 0.0014 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [257]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000001  loss: 1.3919 (1.5688)  loss_scale: 16384.0000 (13707.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7208 (6.9220)  time: 0.8715 (0.5175 -- 3.8632)  data: 0.0014 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [257]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 1.5034 (1.5548)  loss_scale: 16384.0000 (14149.8182)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3933 (7.0400)  time: 0.7763 (0.5148 -- 4.1690)  data: 0.0022 (0.0002 -- 0.0144)  max mem: 16413
Epoch: [257]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.6460 (1.5665)  loss_scale: 16384.0000 (14466.7234)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1349 (7.1326)  time: 0.9616 (0.5214 -- 3.9457)  data: 0.0017 (0.0002 -- 0.0077)  max mem: 16413
Epoch: [257]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.5204 (1.5575)  loss_scale: 16384.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8766 (7.1833)  time: 0.6935 (0.4962 -- 1.8980)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [257] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.5204 (1.5582)  loss_scale: 16384.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8766 (7.1833)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1280 (0.1280)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5392 (2.5392 -- 2.5392)  data: 2.2972 (2.2972 -- 2.2972)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5853 (0.4995)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4280 (0.1884 -- 2.5392)  data: 0.2146 (0.0004 -- 2.2972)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4877 (0.5089)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2130 (0.1705 -- 0.3553)  data: 0.0117 (0.0001 -- 0.1671)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4877 (0.5499)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.1994 (0.1332 -- 0.3553)  data: 0.0114 (0.0001 -- 0.1671)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 85.062 Acc@5 98.340 loss 0.566
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.51%
Epoch: [258]  [  0/160]  eta: 0:19:03  lr: 0.000003  min_lr: 0.000001  loss: 2.1227 (2.1227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6462 (11.6462)  time: 7.1472 (7.1472 -- 7.1472)  data: 4.7715 (4.7715 -- 4.7715)  max mem: 16413
[2023-09-05 08:21:24,060] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:21:24,061] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:21:24,066] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:21:24,066] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [258]  [ 20/160]  eta: 0:02:43  lr: 0.000003  min_lr: 0.000001  loss: 1.6010 (1.6098)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0267 (7.2736)  time: 0.8702 (0.5224 -- 2.9947)  data: 0.2240 (0.0003 -- 2.4811)  max mem: 16413
[2023-09-05 08:21:54,003] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41315
[2023-09-05 08:21:54,003] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41315
[2023-09-05 08:21:54,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:21:54,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:21:54,004] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [258]  [ 40/160]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000001  loss: 1.3619 (1.5049)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1132 (6.8646)  time: 0.8438 (0.5163 -- 3.2122)  data: 0.2957 (0.0005 -- 2.6916)  max mem: 16413
Epoch: [258]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000001  loss: 1.6318 (1.5571)  loss_scale: 16384.0000 (25516.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2318 (7.1043)  time: 0.9966 (0.5227 -- 4.3532)  data: 0.4514 (0.0003 -- 3.8089)  max mem: 16413
Epoch: [258]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000001  loss: 1.4763 (1.5516)  loss_scale: 16384.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1124 (7.2851)  time: 0.8552 (0.5230 -- 4.7482)  data: 0.3043 (0.0002 -- 4.2245)  max mem: 16413
Epoch: [258]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000001  loss: 1.8735 (1.5830)  loss_scale: 16384.0000 (21899.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7052 (7.4199)  time: 0.8297 (0.5281 -- 2.8228)  data: 0.2795 (0.0005 -- 2.2738)  max mem: 16413
Epoch: [258]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000001  loss: 1.5826 (1.5775)  loss_scale: 16384.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8723 (7.3507)  time: 0.8514 (0.5275 -- 2.5381)  data: 0.3027 (0.0003 -- 2.0014)  max mem: 16413
Epoch: [258]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.4306 (1.5603)  loss_scale: 16384.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7886 (7.2738)  time: 0.8992 (0.5274 -- 3.3661)  data: 0.3525 (0.0004 -- 2.8044)  max mem: 16413
Epoch: [258]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.5039 (1.5461)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5144 (7.3981)  time: 0.7045 (0.4976 -- 2.6059)  data: 0.1788 (0.0002 -- 2.0679)  max mem: 16413
Epoch: [258] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.5039 (1.5376)  loss_scale: 16384.0000 (19865.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5144 (7.3981)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1294 (0.1294)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2915 (2.2915 -- 2.2915)  data: 2.0647 (2.0647 -- 2.0647)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5850 (0.4978)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4074 (0.2068 -- 2.2915)  data: 0.1902 (0.0007 -- 2.0647)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4721 (0.5083)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2145 (0.1696 -- 0.2999)  data: 0.0069 (0.0001 -- 0.1080)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4721 (0.5533)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.1987 (0.1336 -- 0.2999)  data: 0.0066 (0.0001 -- 0.1080)  max mem: 16413
Val: Total time: 0:00:07 (0.2805 s / it)
* Acc@1 85.062 Acc@5 98.340 loss 0.571
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.51%
Epoch: [259]  [  0/160]  eta: 0:23:55  lr: 0.000003  min_lr: 0.000001  loss: 1.5925 (1.5925)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6158 (7.6158)  time: 8.9710 (8.9710 -- 8.9710)  data: 5.2131 (5.2131 -- 5.2131)  max mem: 16413
[2023-09-05 08:23:58,410] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:23:58,411] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:23:58,410] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:23:58,411] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [259]  [ 20/160]  eta: 0:02:38  lr: 0.000003  min_lr: 0.000001  loss: 1.5214 (1.5862)  loss_scale: 32768.0000 (29647.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0169 (7.9142)  time: 0.7368 (0.5390 -- 2.6890)  data: 0.1541 (0.0006 -- 2.1579)  max mem: 16413
Epoch: [259]  [ 40/160]  eta: 0:02:04  lr: 0.000003  min_lr: 0.000001  loss: 1.2880 (1.4959)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1222 (7.6578)  time: 0.9485 (0.5313 -- 3.5751)  data: 0.0098 (0.0002 -- 0.1377)  max mem: 16413
Epoch: [259]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000001  loss: 1.4606 (1.4843)  loss_scale: 32768.0000 (31693.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7431 (7.4400)  time: 0.8279 (0.5209 -- 4.7058)  data: 0.0022 (0.0003 -- 0.0080)  max mem: 16413
Epoch: [259]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000001  loss: 1.6328 (1.5255)  loss_scale: 32768.0000 (31958.9136)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6186 (7.2276)  time: 0.9118 (0.5293 -- 3.2818)  data: 0.0022 (0.0002 -- 0.0149)  max mem: 16413
[2023-09-05 08:25:08,012] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41526
[2023-09-05 08:25:08,012] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41526
[2023-09-05 08:25:08,012] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:25:08,012] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:25:08,012] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [259]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000001  loss: 1.5838 (1.5182)  loss_scale: 16384.0000 (29685.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9796 (7.1392)  time: 0.7968 (0.5263 -- 3.6070)  data: 0.2486 (0.0002 -- 3.0902)  max mem: 16413
Epoch: [259]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000001  loss: 1.6420 (1.5331)  loss_scale: 16384.0000 (27487.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5057 (7.0281)  time: 0.8392 (0.5436 -- 1.9655)  data: 0.1213 (0.0006 -- 1.0860)  max mem: 16413
Epoch: [259]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000001  loss: 1.5283 (1.5437)  loss_scale: 16384.0000 (25912.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2189 (7.1425)  time: 0.9023 (0.5214 -- 2.4276)  data: 0.2090 (0.0003 -- 1.6702)  max mem: 16413
Epoch: [259]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000001  loss: 1.7299 (1.5594)  loss_scale: 16384.0000 (24780.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8508 (7.0966)  time: 0.6768 (0.4950 -- 2.4174)  data: 0.0126 (0.0002 -- 0.2348)  max mem: 16413
Epoch: [259] Total time: 0:02:21 (0.8826 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000001  loss: 1.7299 (1.5714)  loss_scale: 16384.0000 (24780.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8508 (7.0966)
[2023-09-05 08:26:08,427] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-259 is about to be saved!
[2023-09-05 08:26:08,428] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-259 is ready now!
[2023-09-05 08:26:08,429] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-259/mp_rank_00_model_states.pt
[2023-09-05 08:26:08,429] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-259/mp_rank_00_model_states.pt...
[2023-09-05 08:26:09,362] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-259/mp_rank_00_model_states.pt.
[2023-09-05 08:26:09,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-259 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1270 (0.1270)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5468 (2.5468 -- 2.5468)  data: 2.3241 (2.3241 -- 2.3241)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5795 (0.4944)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4410 (0.2020 -- 2.5468)  data: 0.2204 (0.0005 -- 2.3241)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4678 (0.5017)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2179 (0.1711 -- 0.3261)  data: 0.0085 (0.0001 -- 0.0853)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4678 (0.5474)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2027 (0.1338 -- 0.3261)  data: 0.0079 (0.0001 -- 0.0853)  max mem: 16413
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 85.685 Acc@5 98.340 loss 0.568
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [260]  [  0/160]  eta: 0:19:19  lr: 0.000003  min_lr: 0.000001  loss: 1.7244 (1.7244)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3103 (5.3103)  time: 7.2496 (7.2496 -- 7.2496)  data: 5.7988 (5.7988 -- 5.7988)  max mem: 16413
Epoch: [260]  [ 20/160]  eta: 0:03:01  lr: 0.000003  min_lr: 0.000001  loss: 1.5363 (1.4941)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0849 (6.6168)  time: 0.9994 (0.5112 -- 4.4641)  data: 0.0014 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [260]  [ 40/160]  eta: 0:02:07  lr: 0.000003  min_lr: 0.000001  loss: 1.4036 (1.4806)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4928 (7.9115)  time: 0.8241 (0.5245 -- 3.6471)  data: 0.0014 (0.0002 -- 0.0055)  max mem: 16413
[2023-09-05 08:27:14,295] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:27:14,295] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:27:14,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:27:14,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:27:20,447] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41660
[2023-09-05 08:27:20,447] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41660
[2023-09-05 08:27:20,447] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:27:20,447] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:27:20,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [260]  [ 60/160]  eta: 0:01:43  lr: 0.000002  min_lr: 0.000001  loss: 1.4848 (1.5105)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1337 (7.6984)  time: 0.9725 (0.5129 -- 4.3087)  data: 0.0021 (0.0003 -- 0.0161)  max mem: 16413
Epoch: [260]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000001  loss: 1.5207 (1.5197)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7815 (7.4850)  time: 0.6891 (0.5147 -- 1.7129)  data: 0.0016 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [260]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000001  loss: 1.4353 (1.5173)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9138 (7.4005)  time: 0.9171 (0.5302 -- 4.1772)  data: 0.0013 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [260]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000001  loss: 1.5166 (1.5164)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7145 (7.3302)  time: 0.8382 (0.5287 -- 4.0064)  data: 0.0009 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [260]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000001  loss: 1.5400 (1.5193)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0629 (7.3253)  time: 0.9323 (0.5185 -- 4.1418)  data: 0.0016 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [260]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.4713 (1.5190)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1633 (7.3386)  time: 0.7018 (0.4940 -- 4.2686)  data: 0.0006 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [260] Total time: 0:02:24 (0.9012 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.4713 (1.5491)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1633 (7.3386)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1269 (0.1269)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2556 (2.2556 -- 2.2556)  data: 2.0168 (2.0168 -- 2.0168)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5906 (0.4966)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4089 (0.2075 -- 2.2556)  data: 0.1884 (0.0010 -- 2.0168)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4681 (0.5006)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2290 (0.1701 -- 0.5628)  data: 0.0216 (0.0001 -- 0.3735)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4681 (0.5452)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2131 (0.1327 -- 0.5628)  data: 0.0212 (0.0001 -- 0.3735)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 85.062 Acc@5 98.340 loss 0.569
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.51%
Epoch: [261]  [  0/160]  eta: 0:18:52  lr: 0.000002  min_lr: 0.000001  loss: 1.8330 (1.8330)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6944 (8.6944)  time: 7.0789 (7.0789 -- 7.0789)  data: 6.5339 (6.5339 -- 6.5339)  max mem: 16413
Epoch: [261]  [ 20/160]  eta: 0:02:36  lr: 0.000002  min_lr: 0.000001  loss: 1.6203 (1.6241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9976 (7.5213)  time: 0.8172 (0.5297 -- 2.7747)  data: 0.1240 (0.0002 -- 1.4715)  max mem: 16413
[2023-09-05 08:29:20,783] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:29:20,784] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:29:20,786] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:29:20,787] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:29:28,730] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41799
[2023-09-05 08:29:28,730] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41799
[2023-09-05 08:29:28,744] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:29:28,744] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:29:28,744] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [261]  [ 40/160]  eta: 0:02:06  lr: 0.000002  min_lr: 0.000001  loss: 1.3722 (1.5452)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7824 (7.1995)  time: 0.9840 (0.5196 -- 3.6537)  data: 0.3663 (0.0003 -- 3.0713)  max mem: 16413
Epoch: [261]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000001  loss: 1.4447 (1.5053)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3198 (6.9690)  time: 0.8262 (0.5209 -- 2.5483)  data: 0.0481 (0.0003 -- 0.9350)  max mem: 16413
Epoch: [261]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000001  loss: 1.2900 (1.4788)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2332 (6.8994)  time: 0.9282 (0.5323 -- 3.2735)  data: 0.1515 (0.0005 -- 1.4715)  max mem: 16413
Epoch: [261]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000001  loss: 1.6589 (1.5058)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7170 (6.9144)  time: 0.8119 (0.5215 -- 3.2330)  data: 0.2656 (0.0003 -- 2.7147)  max mem: 16413
Epoch: [261]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000001  loss: 1.5123 (1.5196)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3372 (7.0068)  time: 0.9511 (0.5191 -- 4.0365)  data: 0.3472 (0.0001 -- 3.4784)  max mem: 16413
Epoch: [261]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000001  loss: 1.5276 (1.5213)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2880 (7.0085)  time: 0.8327 (0.5218 -- 2.9665)  data: 0.0624 (0.0003 -- 1.2126)  max mem: 16413
Epoch: [261]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.6091 (1.5197)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2493 (7.1365)  time: 0.6917 (0.4953 -- 2.4161)  data: 0.0271 (0.0001 -- 0.5297)  max mem: 16413
Epoch: [261] Total time: 0:02:23 (0.8965 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.6091 (1.5447)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2493 (7.1365)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1282 (0.1282)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3253 (2.3253 -- 2.3253)  data: 2.1271 (2.1271 -- 2.1271)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5845 (0.5001)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4122 (0.2021 -- 2.3253)  data: 0.1943 (0.0007 -- 2.1271)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4944 (0.5041)  acc1: 88.8889 (88.3598)  acc5: 100.0000 (98.4127)  time: 0.2237 (0.1696 -- 0.4027)  data: 0.0159 (0.0001 -- 0.2073)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4944 (0.5470)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2053 (0.1330 -- 0.4027)  data: 0.0156 (0.0001 -- 0.2073)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 85.477 Acc@5 98.340 loss 0.567
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [262]  [  0/160]  eta: 0:21:40  lr: 0.000002  min_lr: 0.000001  loss: 0.9051 (0.9051)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4206 (5.4206)  time: 8.1282 (8.1282 -- 8.1282)  data: 4.1263 (4.1263 -- 4.1263)  max mem: 16413
[2023-09-05 08:31:33,319] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:31:33,320] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:31:33,321] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:31:33,321] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:31:43,491] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41939
[2023-09-05 08:31:43,491] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 41939
[2023-09-05 08:31:43,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:31:43,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:31:43,491] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [262]  [ 20/160]  eta: 0:02:36  lr: 0.000002  min_lr: 0.000001  loss: 1.3531 (1.4373)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2360 (6.7602)  time: 0.7701 (0.5393 -- 2.1716)  data: 0.0416 (0.0006 -- 0.6393)  max mem: 16413
Epoch: [262]  [ 40/160]  eta: 0:02:00  lr: 0.000002  min_lr: 0.000001  loss: 1.5167 (1.4984)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6725 (6.9276)  time: 0.8902 (0.5331 -- 2.3334)  data: 0.2247 (0.0006 -- 1.8094)  max mem: 16413
Epoch: [262]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000001  loss: 1.5727 (1.5262)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3959 (6.9957)  time: 0.9027 (0.5309 -- 3.0010)  data: 0.3497 (0.0004 -- 2.4665)  max mem: 16413
[2023-09-05 08:32:33,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=237, lr=[5.697109264626769e-07, 5.697109264626769e-07, 6.330121405140854e-07, 6.330121405140854e-07, 7.033468227934281e-07, 7.033468227934281e-07, 7.814964697704757e-07, 7.814964697704757e-07, 8.683294108560841e-07, 8.683294108560841e-07, 9.648104565067602e-07, 9.648104565067602e-07, 1.0720116183408444e-06, 1.0720116183408444e-06, 1.191124020378716e-06, 1.191124020378716e-06, 1.323471133754129e-06, 1.323471133754129e-06, 1.4705234819490321e-06, 1.4705234819490321e-06, 1.6339149799433691e-06, 1.6339149799433691e-06, 1.8154610888259657e-06, 1.8154610888259657e-06, 2.017178987584406e-06, 2.017178987584406e-06, 2.2413099862048956e-06, 2.2413099862048956e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 08:32:33,801] [INFO] [timer.py:260:stop] epoch=0/micro_step=42000/global_step=42000, RunningAvgSamplesPerSec=17.733423664122952, CurrSamplesPerSec=21.73893145399017, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [262]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000001  loss: 1.7315 (1.5483)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3065 (7.1429)  time: 0.8302 (0.5215 -- 2.7278)  data: 0.2738 (0.0005 -- 2.1867)  max mem: 16413
Epoch: [262]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000001  loss: 1.4236 (1.5267)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4880 (7.1013)  time: 0.8772 (0.5340 -- 2.5395)  data: 0.2877 (0.0001 -- 1.9680)  max mem: 16413
[2023-09-05 08:33:10,000] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42037
[2023-09-05 08:33:10,000] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42037
[2023-09-05 08:33:10,000] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:33:10,000] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:33:10,000] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [262]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000001  loss: 1.7052 (1.5606)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7852 (7.1021)  time: 1.0071 (0.5249 -- 4.1229)  data: 0.4598 (0.0003 -- 3.5764)  max mem: 16413
Epoch: [262]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000001  loss: 1.5668 (1.5569)  loss_scale: 8192.0000 (16267.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3613 (7.1525)  time: 0.8237 (0.5247 -- 4.9842)  data: 0.2827 (0.0002 -- 4.4534)  max mem: 16413
Epoch: [262]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.6778 (1.5593)  loss_scale: 8192.0000 (15308.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0368 (7.2155)  time: 0.6546 (0.4949 -- 3.0832)  data: 0.1328 (0.0002 -- 2.5519)  max mem: 16413
Epoch: [262] Total time: 0:02:22 (0.8920 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.6778 (1.5625)  loss_scale: 8192.0000 (15308.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0368 (7.2155)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1284 (0.1284)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4103 (2.4103 -- 2.4103)  data: 2.1997 (2.1997 -- 2.1997)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5772 (0.5048)  acc1: 88.8889 (89.8990)  acc5: 100.0000 (100.0000)  time: 0.4355 (0.2056 -- 2.4103)  data: 0.2243 (0.0009 -- 2.1997)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4996 (0.5138)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2189 (0.1727 -- 0.4773)  data: 0.0168 (0.0002 -- 0.2573)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4996 (0.5548)  acc1: 88.8889 (86.7220)  acc5: 100.0000 (97.9253)  time: 0.2050 (0.1329 -- 0.4773)  data: 0.0165 (0.0001 -- 0.2573)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 86.100 Acc@5 98.133 loss 0.568
Accuracy of the network on the 482 val images: 86.10%
Max accuracy: 86.51%
Epoch: [263]  [  0/160]  eta: 0:20:48  lr: 0.000002  min_lr: 0.000001  loss: 2.1760 (2.1760)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5301 (5.5301)  time: 7.8003 (7.8003 -- 7.8003)  data: 5.6690 (5.6690 -- 5.6690)  max mem: 16413
Epoch: [263]  [ 20/160]  eta: 0:03:04  lr: 0.000002  min_lr: 0.000001  loss: 1.5711 (1.5138)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5105 (7.6245)  time: 0.9922 (0.5183 -- 5.3040)  data: 0.0369 (0.0003 -- 0.7138)  max mem: 16413
Epoch: [263]  [ 40/160]  eta: 0:02:02  lr: 0.000002  min_lr: 0.000001  loss: 1.6484 (1.5462)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9794 (7.2134)  time: 0.7181 (0.5287 -- 2.0761)  data: 0.0191 (0.0003 -- 0.3534)  max mem: 16413
Epoch: [263]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000001  loss: 1.3846 (1.5296)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4862 (7.3658)  time: 0.9381 (0.5446 -- 2.7794)  data: 0.3131 (0.0003 -- 2.2643)  max mem: 16413
Epoch: [263]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000001  loss: 1.6407 (1.5336)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0871 (7.2296)  time: 0.8875 (0.5231 -- 2.7162)  data: 0.3367 (0.0004 -- 2.1769)  max mem: 16413
[2023-09-05 08:35:13,102] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:35:13,102] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:35:13,103] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 08:35:13,103] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [263]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000001  loss: 1.6291 (1.5537)  loss_scale: 16384.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6272 (7.1704)  time: 0.8270 (0.5306 -- 2.4267)  data: 0.2789 (0.0005 -- 1.9132)  max mem: 16413
Epoch: [263]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000001  loss: 1.5990 (1.5535)  loss_scale: 16384.0000 (10561.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3989 (7.2662)  time: 0.9776 (0.5099 -- 4.8379)  data: 0.4385 (0.0003 -- 4.3066)  max mem: 16413
Epoch: [263]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000001  loss: 1.4430 (1.5435)  loss_scale: 16384.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2755 (7.2914)  time: 0.7217 (0.5254 -- 2.7787)  data: 0.1159 (0.0004 -- 1.4360)  max mem: 16413
Epoch: [263]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000001  loss: 1.6396 (1.5526)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5911 (7.3132)  time: 0.7651 (0.4936 -- 4.0589)  data: 0.2392 (0.0001 -- 3.5248)  max mem: 16413
Epoch: [263] Total time: 0:02:23 (0.8989 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000001  loss: 1.6396 (1.5614)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5911 (7.3132)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1285 (0.1285)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3185 (2.3185 -- 2.3185)  data: 2.0894 (2.0894 -- 2.0894)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5787 (0.5012)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4052 (0.2005 -- 2.3185)  data: 0.1940 (0.0007 -- 2.0894)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4868 (0.5049)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2249 (0.1693 -- 0.5648)  data: 0.0217 (0.0001 -- 0.3776)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4868 (0.5444)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (98.3402)  time: 0.2116 (0.1324 -- 0.5648)  data: 0.0214 (0.0001 -- 0.3776)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.566
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [264]  [  0/160]  eta: 0:21:12  lr: 0.000002  min_lr: 0.000001  loss: 1.3315 (1.3315)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4667 (10.4667)  time: 7.9526 (7.9526 -- 7.9526)  data: 7.4111 (7.4111 -- 7.4111)  max mem: 16413
Epoch: [264]  [ 20/160]  eta: 0:02:50  lr: 0.000002  min_lr: 0.000001  loss: 1.5945 (1.5994)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6143 (8.1071)  time: 0.8846 (0.5313 -- 3.5020)  data: 0.2900 (0.0003 -- 2.9656)  max mem: 16413
Epoch: [264]  [ 40/160]  eta: 0:02:03  lr: 0.000002  min_lr: 0.000001  loss: 1.4633 (1.5143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7819 (7.6418)  time: 0.8260 (0.5126 -- 2.7427)  data: 0.0521 (0.0003 -- 0.9297)  max mem: 16413
[2023-09-05 08:37:16,214] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:37:16,214] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:37:16,214] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:37:16,214] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [264]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000001  loss: 1.4182 (1.4977)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6852 (7.2444)  time: 0.8657 (0.5395 -- 3.0921)  data: 0.0170 (0.0002 -- 0.2597)  max mem: 16413
[2023-09-05 08:37:23,270] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42302
[2023-09-05 08:37:23,270] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:37:23,270] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42302
[2023-09-05 08:37:23,270] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:37:23,271] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 08:37:37,749] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42318
[2023-09-05 08:37:37,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:37:37,749] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42318
[2023-09-05 08:37:37,749] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:37:37,749] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [264]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000001  loss: 1.5651 (1.5205)  loss_scale: 16384.0000 (17698.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7825 (7.3065)  time: 0.8469 (0.5309 -- 3.3269)  data: 0.0016 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [264]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000001  loss: 1.7929 (1.5689)  loss_scale: 8192.0000 (15816.2376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0419 (7.2882)  time: 0.9490 (0.5330 -- 4.2044)  data: 0.0015 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [264]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000001  loss: 1.4296 (1.5510)  loss_scale: 8192.0000 (14556.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3134 (7.3308)  time: 0.8221 (0.5253 -- 3.7711)  data: 0.0015 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [264]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000001  loss: 1.6024 (1.5558)  loss_scale: 8192.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6569 (7.2731)  time: 0.9391 (0.5244 -- 3.6231)  data: 0.0027 (0.0007 -- 0.0218)  max mem: 16413
Epoch: [264]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7343 (1.5637)  loss_scale: 8192.0000 (13004.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9345 (7.2880)  time: 0.6725 (0.4950 -- 3.6010)  data: 0.0009 (0.0001 -- 0.0044)  max mem: 16413
Epoch: [264] Total time: 0:02:23 (0.8974 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7343 (1.5486)  loss_scale: 8192.0000 (13004.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9345 (7.2880)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1291 (0.1291)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3357 (2.3357 -- 2.3357)  data: 2.1075 (2.1075 -- 2.1075)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5669 (0.5036)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4236 (0.2100 -- 2.3357)  data: 0.2013 (0.0004 -- 2.1075)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4750 (0.5057)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2235 (0.1721 -- 0.3129)  data: 0.0126 (0.0001 -- 0.0974)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4750 (0.5460)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (98.3402)  time: 0.2050 (0.1328 -- 0.3129)  data: 0.0123 (0.0001 -- 0.0974)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 85.270 Acc@5 98.548 loss 0.565
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [265]  [  0/160]  eta: 0:20:32  lr: 0.000002  min_lr: 0.000000  loss: 1.9004 (1.9004)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8037 (3.8037)  time: 7.7005 (7.7005 -- 7.7005)  data: 6.4034 (6.4034 -- 6.4034)  max mem: 16413
Epoch: [265]  [ 20/160]  eta: 0:02:58  lr: 0.000002  min_lr: 0.000000  loss: 1.4799 (1.5058)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6132 (6.9045)  time: 0.9524 (0.5183 -- 5.1824)  data: 0.0177 (0.0006 -- 0.3170)  max mem: 16413
Epoch: [265]  [ 40/160]  eta: 0:02:10  lr: 0.000002  min_lr: 0.000000  loss: 1.5272 (1.4831)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0651 (7.2207)  time: 0.8871 (0.5260 -- 3.9412)  data: 0.0020 (0.0004 -- 0.0081)  max mem: 16413
[2023-09-05 08:39:42,520] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:39:42,520] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 08:39:42,523] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:39:42,523] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [265]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.6129 (1.5094)  loss_scale: 16384.0000 (10072.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8724 (7.3160)  time: 0.7959 (0.5169 -- 3.8134)  data: 0.0020 (0.0005 -- 0.0109)  max mem: 16413
Epoch: [265]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.5656 (1.5316)  loss_scale: 16384.0000 (11630.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7353 (7.3711)  time: 0.8372 (0.5329 -- 3.8625)  data: 0.0293 (0.0006 -- 0.5541)  max mem: 16413
Epoch: [265]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.6228 (1.5478)  loss_scale: 16384.0000 (12571.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9196 (7.2398)  time: 0.8945 (0.5277 -- 2.0709)  data: 0.1098 (0.0004 -- 1.1288)  max mem: 16413
Epoch: [265]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7027 (1.5700)  loss_scale: 16384.0000 (13201.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5000 (7.1345)  time: 0.8217 (0.5243 -- 2.9398)  data: 0.0018 (0.0005 -- 0.0071)  max mem: 16413
Epoch: [265]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.4792 (1.5626)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0653 (7.1815)  time: 0.8751 (0.5189 -- 3.7864)  data: 0.0894 (0.0003 -- 0.9003)  max mem: 16413
Epoch: [265]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.3264 (1.5395)  loss_scale: 16384.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9841 (7.0791)  time: 0.7031 (0.4949 -- 2.6426)  data: 0.1377 (0.0002 -- 2.1318)  max mem: 16413
Epoch: [265] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.3264 (1.5398)  loss_scale: 16384.0000 (13977.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9841 (7.0791)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1284 (0.1284)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6159 (2.6159 -- 2.6159)  data: 2.4001 (2.4001 -- 2.4001)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5759 (0.5046)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4391 (0.1920 -- 2.6159)  data: 0.2191 (0.0006 -- 2.4001)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4734 (0.5072)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2158 (0.1705 -- 0.2799)  data: 0.0055 (0.0001 -- 0.0972)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4734 (0.5470)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.1969 (0.1331 -- 0.2799)  data: 0.0052 (0.0001 -- 0.0972)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 85.477 Acc@5 97.925 loss 0.568
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [266]  [  0/160]  eta: 0:22:09  lr: 0.000002  min_lr: 0.000000  loss: 0.8932 (0.8932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4871 (5.4871)  time: 8.3082 (8.3082 -- 8.3082)  data: 7.7710 (7.7710 -- 7.7710)  max mem: 16413
[2023-09-05 08:41:43,144] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:41:43,144] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:41:43,145] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:41:43,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [266]  [ 20/160]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000000  loss: 1.6450 (1.5804)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0857 (6.8139)  time: 0.9029 (0.5247 -- 3.3310)  data: 0.3557 (0.0005 -- 2.7956)  max mem: 16413
Epoch: [266]  [ 40/160]  eta: 0:02:02  lr: 0.000002  min_lr: 0.000000  loss: 1.5253 (1.5383)  loss_scale: 32768.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8909 (7.0961)  time: 0.7670 (0.5278 -- 2.5560)  data: 0.2174 (0.0002 -- 2.0188)  max mem: 16413
Epoch: [266]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.4604 (1.5150)  loss_scale: 32768.0000 (28739.1475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5025 (7.3253)  time: 0.8650 (0.5220 -- 4.0048)  data: 0.3154 (0.0005 -- 3.4673)  max mem: 16413
Epoch: [266]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 1.5323 (1.5303)  loss_scale: 32768.0000 (29733.9259)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7374 (7.3147)  time: 0.8213 (0.5214 -- 2.7415)  data: 0.1880 (0.0002 -- 2.2144)  max mem: 16413
Epoch: [266]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.6319 (1.5562)  loss_scale: 32768.0000 (30334.7327)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7159 (7.5523)  time: 0.9209 (0.5174 -- 3.2970)  data: 0.3667 (0.0005 -- 2.7493)  max mem: 16413
Epoch: [266]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.4971 (1.5559)  loss_scale: 32768.0000 (30736.9256)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9092 (7.5270)  time: 0.8390 (0.5253 -- 3.4209)  data: 0.2770 (0.0004 -- 2.8682)  max mem: 16413
Epoch: [266]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5752 (1.5594)  loss_scale: 32768.0000 (31025.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4512 (7.5209)  time: 0.9593 (0.5245 -- 3.8178)  data: 0.4143 (0.0005 -- 3.3114)  max mem: 16413
[2023-09-05 08:43:35,995] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:43:35,995] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 08:43:35,996] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:43:35,996] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 08:43:40,433] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42708
[2023-09-05 08:43:40,433] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 08:43:40,433] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42708
[2023-09-05 08:43:40,434] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 08:43:40,434] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [266]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5105 (1.5556)  loss_scale: 32768.0000 (32256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2243 (7.4450)  time: 0.6831 (0.4961 -- 2.2500)  data: 0.1586 (0.0002 -- 1.7092)  max mem: 16413
Epoch: [266] Total time: 0:02:22 (0.8933 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5105 (1.5602)  loss_scale: 32768.0000 (32256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2243 (7.4450)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1298 (0.1298)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4609 (2.4609 -- 2.4609)  data: 2.2351 (2.2351 -- 2.2351)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5708 (0.5020)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4419 (0.1920 -- 2.4609)  data: 0.2269 (0.0008 -- 2.2351)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4736 (0.5037)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2229 (0.1686 -- 0.4574)  data: 0.0204 (0.0001 -- 0.2459)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4736 (0.5438)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2084 (0.1322 -- 0.4574)  data: 0.0201 (0.0001 -- 0.2459)  max mem: 16413
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.569
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [267]  [  0/160]  eta: 0:20:03  lr: 0.000002  min_lr: 0.000000  loss: 2.0270 (2.0270)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2252 (6.2252)  time: 7.5223 (7.5223 -- 7.5223)  data: 5.6137 (5.6137 -- 5.6137)  max mem: 16413
Epoch: [267]  [ 20/160]  eta: 0:02:42  lr: 0.000002  min_lr: 0.000000  loss: 1.4737 (1.5688)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1848 (7.2878)  time: 0.8433 (0.5288 -- 2.8688)  data: 0.2449 (0.0004 -- 2.3495)  max mem: 16413
[2023-09-05 08:44:33,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42757
[2023-09-05 08:44:33,837] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42757
[2023-09-05 08:44:33,837] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:44:33,837] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:44:33,837] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [267]  [ 40/160]  eta: 0:02:10  lr: 0.000002  min_lr: 0.000000  loss: 1.5871 (1.5319)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7703 (7.0874)  time: 1.0115 (0.5098 -- 5.0452)  data: 0.4134 (0.0003 -- 4.5241)  max mem: 16413
Epoch: [267]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.4587 (1.5301)  loss_scale: 16384.0000 (26321.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5363 (7.5951)  time: 0.7874 (0.5346 -- 3.2829)  data: 0.2151 (0.0002 -- 2.6686)  max mem: 16413
Epoch: [267]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000000  loss: 1.5852 (1.5450)  loss_scale: 16384.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0704 (7.4810)  time: 0.9169 (0.5177 -- 3.8346)  data: 0.1989 (0.0003 -- 1.8402)  max mem: 16413
Epoch: [267]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.4608 (1.5311)  loss_scale: 16384.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1879 (7.5823)  time: 0.8254 (0.5330 -- 3.7739)  data: 0.1018 (0.0006 -- 1.6662)  max mem: 16413
Epoch: [267]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.6201 (1.5456)  loss_scale: 16384.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0035 (7.5728)  time: 0.9164 (0.5168 -- 3.7898)  data: 0.0016 (0.0004 -- 0.0064)  max mem: 16413
Epoch: [267]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5870 (1.5470)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1793 (7.5700)  time: 0.8037 (0.5329 -- 3.4610)  data: 0.0013 (0.0001 -- 0.0029)  max mem: 16413
Epoch: [267]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5470 (1.5498)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1630 (7.5010)  time: 0.6229 (0.4957 -- 1.9494)  data: 0.0011 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [267] Total time: 0:02:21 (0.8848 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5470 (1.5543)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1630 (7.5010)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1290 (0.1290)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3964 (2.3964 -- 2.3964)  data: 2.1621 (2.1621 -- 2.1621)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5693 (0.5083)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4246 (0.1949 -- 2.3964)  data: 0.2118 (0.0006 -- 2.1621)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4762 (0.5055)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2305 (0.1685 -- 0.5888)  data: 0.0282 (0.0001 -- 0.3941)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4762 (0.5452)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2156 (0.1323 -- 0.5888)  data: 0.0280 (0.0001 -- 0.3941)  max mem: 16413
Val: Total time: 0:00:07 (0.2958 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.567
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 86.51%
Epoch: [268]  [  0/160]  eta: 0:22:54  lr: 0.000002  min_lr: 0.000000  loss: 1.2372 (1.2372)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8281 (4.8281)  time: 8.5917 (8.5917 -- 8.5917)  data: 8.0681 (8.0681 -- 8.0681)  max mem: 16413
[2023-09-05 08:46:36,795] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:46:36,796] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:46:36,796] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:46:36,796] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [268]  [ 20/160]  eta: 0:02:41  lr: 0.000002  min_lr: 0.000000  loss: 1.4312 (1.4691)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4023 (6.5478)  time: 0.7831 (0.5391 -- 3.6586)  data: 0.2292 (0.0002 -- 3.1295)  max mem: 16413
Epoch: [268]  [ 40/160]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 1.4713 (1.5164)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2276 (7.2183)  time: 0.9194 (0.5308 -- 3.2579)  data: 0.0841 (0.0003 -- 1.2988)  max mem: 16413
[2023-09-05 08:47:10,365] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42925
[2023-09-05 08:47:10,365] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 42925
[2023-09-05 08:47:10,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:47:10,366] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:47:10,366] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [268]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.4091 (1.4903)  loss_scale: 16384.0000 (26859.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9280 (7.2516)  time: 0.8577 (0.5146 -- 4.1654)  data: 0.0020 (0.0004 -- 0.0085)  max mem: 16413
Epoch: [268]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.4232 (1.4863)  loss_scale: 16384.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9431 (7.4804)  time: 0.8521 (0.5261 -- 1.9173)  data: 0.0016 (0.0005 -- 0.0028)  max mem: 16413
Epoch: [268]  [100/160]  eta: 0:00:57  lr: 0.000002  min_lr: 0.000000  loss: 1.6219 (1.5030)  loss_scale: 16384.0000 (22710.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8264 (7.4130)  time: 0.9653 (0.5263 -- 3.3341)  data: 0.0793 (0.0004 -- 1.0847)  max mem: 16413
[2023-09-05 08:48:16,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=243, lr=[4.01187055504734e-07, 4.01187055504734e-07, 4.4576339500526e-07, 4.4576339500526e-07, 4.952926611169555e-07, 4.952926611169555e-07, 5.503251790188395e-07, 5.503251790188395e-07, 6.114724211320439e-07, 6.114724211320439e-07, 6.794138012578264e-07, 6.794138012578264e-07, 7.549042236198072e-07, 7.549042236198072e-07, 8.387824706886745e-07, 8.387824706886745e-07, 9.319805229874162e-07, 9.319805229874162e-07, 1.0355339144304622e-06, 1.0355339144304622e-06, 1.1505932382560694e-06, 1.1505932382560694e-06, 1.2784369313956325e-06, 1.2784369313956325e-06, 1.4204854793284806e-06, 1.4204854793284806e-06, 1.5783171992538672e-06, 1.5783171992538672e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 08:48:16,869] [INFO] [timer.py:260:stop] epoch=0/micro_step=43000/global_step=43000, RunningAvgSamplesPerSec=17.726345417842573, CurrSamplesPerSec=22.93138941536667, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [268]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.4733 (1.5060)  loss_scale: 16384.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4634 (7.5231)  time: 0.8177 (0.5213 -- 3.9064)  data: 0.0098 (0.0002 -- 0.1715)  max mem: 16413
Epoch: [268]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5231 (1.5113)  loss_scale: 16384.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7584 (7.3894)  time: 0.9515 (0.5195 -- 3.7124)  data: 0.0019 (0.0003 -- 0.0079)  max mem: 16413
Epoch: [268]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.5504 (1.5194)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2578 (7.3128)  time: 0.7540 (0.4931 -- 3.3474)  data: 0.0008 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [268] Total time: 0:02:24 (0.9015 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.5504 (1.5369)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2578 (7.3128)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.1280 (0.1280)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0943 (2.0943 -- 2.0943)  data: 1.8747 (1.8747 -- 1.8747)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5692 (0.5105)  acc1: 88.8889 (86.8687)  acc5: 100.0000 (100.0000)  time: 0.4240 (0.1885 -- 2.0943)  data: 0.1999 (0.0007 -- 1.8747)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4727 (0.5070)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2363 (0.1691 -- 0.3826)  data: 0.0253 (0.0001 -- 0.1794)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4727 (0.5477)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2137 (0.1328 -- 0.3826)  data: 0.0199 (0.0001 -- 0.1794)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.568
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 86.51%
Epoch: [269]  [  0/160]  eta: 0:18:06  lr: 0.000002  min_lr: 0.000000  loss: 1.6164 (1.6164)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6017 (5.6017)  time: 6.7896 (6.7896 -- 6.7896)  data: 6.2583 (6.2583 -- 6.2583)  max mem: 16413
[2023-09-05 08:49:15,875] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:49:15,875] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:49:15,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:49:15,876] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [269]  [ 20/160]  eta: 0:02:56  lr: 0.000002  min_lr: 0.000000  loss: 1.5544 (1.5473)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9260 (7.0856)  time: 0.9812 (0.5311 -- 4.4750)  data: 0.4212 (0.0003 -- 3.9133)  max mem: 16413
Epoch: [269]  [ 40/160]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 1.7210 (1.6199)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4652 (7.0395)  time: 0.8118 (0.5123 -- 5.7140)  data: 0.2707 (0.0002 -- 5.2108)  max mem: 16413
[2023-09-05 08:49:54,207] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43097
[2023-09-05 08:49:54,207] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43097
[2023-09-05 08:49:54,208] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:49:54,208] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:49:54,208] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [269]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000000  loss: 1.6023 (1.5994)  loss_scale: 32768.0000 (27933.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1974 (6.8947)  time: 0.9464 (0.5153 -- 3.2980)  data: 0.3999 (0.0004 -- 2.7659)  max mem: 16413
Epoch: [269]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.5563 (1.6110)  loss_scale: 16384.0000 (25081.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8356 (6.9038)  time: 0.8138 (0.5362 -- 2.6769)  data: 0.1707 (0.0002 -- 2.1448)  max mem: 16413
Epoch: [269]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.4027 (1.5881)  loss_scale: 16384.0000 (23359.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6431 (6.9715)  time: 0.8710 (0.5376 -- 2.4365)  data: 0.1931 (0.0007 -- 1.9146)  max mem: 16413
Epoch: [269]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7074 (1.5934)  loss_scale: 16384.0000 (22206.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3068 (6.9885)  time: 0.8132 (0.5294 -- 2.3647)  data: 0.1411 (0.0006 -- 1.8286)  max mem: 16413
Epoch: [269]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6289 (1.5887)  loss_scale: 16384.0000 (21380.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9164 (6.9232)  time: 0.9831 (0.5205 -- 3.9663)  data: 0.1248 (0.0006 -- 1.9244)  max mem: 16413
Epoch: [269]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7750 (1.5983)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6734 (6.8708)  time: 0.7141 (0.4970 -- 2.4499)  data: 0.1874 (0.0001 -- 1.9244)  max mem: 16413
Epoch: [269] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7750 (1.5955)  loss_scale: 16384.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6734 (6.8708)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1293 (0.1293)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4262 (2.4262 -- 2.4262)  data: 2.1569 (2.1569 -- 2.1569)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5587 (0.5123)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4207 (0.1953 -- 2.4262)  data: 0.1971 (0.0007 -- 2.1569)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4783 (0.5079)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2159 (0.1705 -- 0.3622)  data: 0.0076 (0.0001 -- 0.1369)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4783 (0.5516)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.1999 (0.1326 -- 0.3622)  data: 0.0073 (0.0001 -- 0.1369)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.571
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [270]  [  0/160]  eta: 0:17:42  lr: 0.000001  min_lr: 0.000000  loss: 1.2763 (1.2763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4971 (6.4971)  time: 6.6400 (6.6400 -- 6.6400)  data: 6.0570 (6.0570 -- 6.0570)  max mem: 16413
Epoch: [270]  [ 20/160]  eta: 0:02:42  lr: 0.000001  min_lr: 0.000000  loss: 1.6856 (1.6574)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9603 (7.4820)  time: 0.8841 (0.5214 -- 3.4073)  data: 0.3011 (0.0007 -- 2.8826)  max mem: 16413
[2023-09-05 08:51:57,262] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:51:57,263] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:51:57,263] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:51:57,264] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:51:57,779] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43227
[2023-09-05 08:51:57,779] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43227
[2023-09-05 08:51:57,780] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:51:57,780] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:51:57,780] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [270]  [ 40/160]  eta: 0:01:59  lr: 0.000001  min_lr: 0.000000  loss: 1.5414 (1.6112)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1424 (7.2793)  time: 0.8305 (0.5073 -- 2.3028)  data: 0.1748 (0.0003 -- 1.7816)  max mem: 16413
Epoch: [270]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6710 (1.6422)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9860 (7.4403)  time: 0.8900 (0.5208 -- 2.3677)  data: 0.1551 (0.0003 -- 1.8148)  max mem: 16413
Epoch: [270]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.5893 (1.6310)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5703 (7.3504)  time: 0.8333 (0.5297 -- 2.2387)  data: 0.1237 (0.0004 -- 1.4931)  max mem: 16413
Epoch: [270]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.5288 (1.6221)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8312 (7.3655)  time: 1.0242 (0.5276 -- 3.9147)  data: 0.0015 (0.0005 -- 0.0029)  max mem: 16413
Epoch: [270]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.4579 (1.6125)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2142 (7.1831)  time: 0.8961 (0.5131 -- 4.4261)  data: 0.0014 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [270]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6264 (1.6117)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4229 (7.0900)  time: 0.8435 (0.5337 -- 3.2185)  data: 0.0013 (0.0005 -- 0.0033)  max mem: 16413
[2023-09-05 08:53:49,281] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:53:49,281] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 08:53:49,282] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:53:49,282] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [270]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6192 (1.6062)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7564 (7.2355)  time: 0.6377 (0.4969 -- 2.7407)  data: 0.0009 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [270] Total time: 0:02:22 (0.8933 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6192 (1.5509)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7564 (7.2355)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1282 (0.1282)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5124 (2.5124 -- 2.5124)  data: 2.2480 (2.2480 -- 2.2480)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5657 (0.5093)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4360 (0.1891 -- 2.5124)  data: 0.2165 (0.0006 -- 2.2480)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4836 (0.5065)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2157 (0.1694 -- 0.3630)  data: 0.0135 (0.0001 -- 0.1331)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4836 (0.5494)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2003 (0.1362 -- 0.3630)  data: 0.0132 (0.0001 -- 0.1331)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.569
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [271]  [  0/160]  eta: 0:19:07  lr: 0.000001  min_lr: 0.000000  loss: 1.6931 (1.6931)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8025 (7.8025)  time: 7.1689 (7.1689 -- 7.1689)  data: 5.2482 (5.2482 -- 5.2482)  max mem: 16413
Epoch: [271]  [ 20/160]  eta: 0:02:42  lr: 0.000001  min_lr: 0.000000  loss: 1.4560 (1.5048)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7301 (6.6857)  time: 0.8603 (0.5241 -- 3.7252)  data: 0.1281 (0.0007 -- 1.5248)  max mem: 16413
[2023-09-05 08:54:32,372] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43390
[2023-09-05 08:54:32,372] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43390
[2023-09-05 08:54:32,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:54:32,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 08:54:32,373] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [271]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.4690 (1.4703)  loss_scale: 16384.0000 (28372.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4618 (7.1634)  time: 0.8753 (0.5222 -- 4.4600)  data: 0.1362 (0.0003 -- 1.6152)  max mem: 16413
Epoch: [271]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.5897 (1.5270)  loss_scale: 16384.0000 (24441.7049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3323 (7.2134)  time: 0.8695 (0.5261 -- 3.0237)  data: 0.1303 (0.0004 -- 0.9695)  max mem: 16413
Epoch: [271]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.6788 (1.5677)  loss_scale: 16384.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0267 (7.0058)  time: 0.8983 (0.5324 -- 2.9999)  data: 0.1748 (0.0002 -- 2.0307)  max mem: 16413
Epoch: [271]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.4731 (1.5514)  loss_scale: 16384.0000 (21250.5347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4457 (6.9698)  time: 0.8571 (0.5373 -- 3.2677)  data: 0.1644 (0.0004 -- 2.4381)  max mem: 16413
Epoch: [271]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5965 (1.5507)  loss_scale: 16384.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4323 (6.9036)  time: 0.8535 (0.5255 -- 2.0190)  data: 0.1186 (0.0004 -- 1.3947)  max mem: 16413
Epoch: [271]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5865 (1.5532)  loss_scale: 16384.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4414 (6.9215)  time: 0.8894 (0.5257 -- 2.7114)  data: 0.2247 (0.0004 -- 2.1450)  max mem: 16413
[2023-09-05 08:56:11,074] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43504
[2023-09-05 08:56:11,074] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43504
[2023-09-05 08:56:11,074] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:56:11,074] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 08:56:11,074] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [271]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4693 (1.5448)  loss_scale: 8192.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6970 (6.8964)  time: 0.6763 (0.4946 -- 2.2496)  data: 0.0008 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [271] Total time: 0:02:21 (0.8866 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4693 (1.5473)  loss_scale: 8192.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6970 (6.8964)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1269 (0.1269)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3847 (2.3847 -- 2.3847)  data: 2.1683 (2.1683 -- 2.1683)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5667 (0.5061)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4293 (0.1941 -- 2.3847)  data: 0.2150 (0.0007 -- 2.1683)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4888 (0.5049)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2242 (0.1694 -- 0.4034)  data: 0.0184 (0.0001 -- 0.1860)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4888 (0.5462)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2075 (0.1331 -- 0.4034)  data: 0.0180 (0.0001 -- 0.1860)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.569
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [272]  [  0/160]  eta: 0:17:58  lr: 0.000001  min_lr: 0.000000  loss: 1.7098 (1.7098)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7043 (7.7043)  time: 6.7388 (6.7388 -- 6.7388)  data: 4.9750 (4.9750 -- 4.9750)  max mem: 16413
Epoch: [272]  [ 20/160]  eta: 0:02:44  lr: 0.000001  min_lr: 0.000000  loss: 1.6301 (1.5531)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3220 (7.2401)  time: 0.8952 (0.5413 -- 2.6565)  data: 0.2347 (0.0008 -- 1.7558)  max mem: 16413
Epoch: [272]  [ 40/160]  eta: 0:02:03  lr: 0.000001  min_lr: 0.000000  loss: 1.5199 (1.5421)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3423 (7.5941)  time: 0.8852 (0.5186 -- 3.3336)  data: 0.3404 (0.0004 -- 2.7917)  max mem: 16413
Epoch: [272]  [ 60/160]  eta: 0:01:42  lr: 0.000001  min_lr: 0.000000  loss: 1.5639 (1.5344)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9572 (7.5396)  time: 1.0002 (0.5327 -- 3.2568)  data: 0.4521 (0.0006 -- 2.7294)  max mem: 16413
Epoch: [272]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7984 (1.5822)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1812 (7.6428)  time: 0.8398 (0.5349 -- 2.7195)  data: 0.2950 (0.0002 -- 2.2050)  max mem: 16413
Epoch: [272]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.4961 (1.5696)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0802 (7.5973)  time: 0.9223 (0.5178 -- 2.9634)  data: 0.3685 (0.0003 -- 2.4106)  max mem: 16413
[2023-09-05 08:58:16,159] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:58:16,159] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 08:58:16,160] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 08:58:16,160] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [272]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.4885 (1.5533)  loss_scale: 8192.0000 (8733.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8621 (7.4915)  time: 0.8322 (0.5277 -- 3.6759)  data: 0.2878 (0.0003 -- 3.1351)  max mem: 16413
Epoch: [272]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5768 (1.5599)  loss_scale: 16384.0000 (9818.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6660 (7.3949)  time: 0.9438 (0.5227 -- 4.1765)  data: 0.4013 (0.0003 -- 3.6651)  max mem: 16413
Epoch: [272]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5274 (1.5622)  loss_scale: 16384.0000 (10598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1659 (7.3096)  time: 0.6837 (0.4965 -- 2.7307)  data: 0.1643 (0.0001 -- 2.2014)  max mem: 16413
Epoch: [272] Total time: 0:02:24 (0.9005 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5274 (1.5748)  loss_scale: 16384.0000 (10598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1659 (7.3096)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1276 (0.1276)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2993 (2.2993 -- 2.2993)  data: 2.0641 (2.0641 -- 2.0641)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5758 (0.5083)  acc1: 88.8889 (87.8788)  acc5: 100.0000 (100.0000)  time: 0.4101 (0.2063 -- 2.2993)  data: 0.1917 (0.0007 -- 2.0641)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4856 (0.5043)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (98.4127)  time: 0.2254 (0.1696 -- 0.3323)  data: 0.0154 (0.0001 -- 0.1490)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4856 (0.5469)  acc1: 88.8889 (85.4772)  acc5: 100.0000 (97.9253)  time: 0.2085 (0.1324 -- 0.3323)  data: 0.0151 (0.0001 -- 0.1490)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.571
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [273]  [  0/160]  eta: 0:22:03  lr: 0.000001  min_lr: 0.000000  loss: 1.4307 (1.4307)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7335 (5.7335)  time: 8.2734 (8.2734 -- 8.2734)  data: 7.7539 (7.7539 -- 7.7539)  max mem: 16413
Epoch: [273]  [ 20/160]  eta: 0:02:39  lr: 0.000001  min_lr: 0.000000  loss: 1.5415 (1.5456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7764 (7.3397)  time: 0.7821 (0.5259 -- 2.9880)  data: 0.2339 (0.0008 -- 2.4719)  max mem: 16413
Epoch: [273]  [ 40/160]  eta: 0:02:13  lr: 0.000001  min_lr: 0.000000  loss: 1.6304 (1.5991)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1006 (7.6609)  time: 1.0787 (0.5239 -- 5.3550)  data: 0.4827 (0.0002 -- 4.8131)  max mem: 16413
Epoch: [273]  [ 60/160]  eta: 0:01:41  lr: 0.000001  min_lr: 0.000000  loss: 1.3804 (1.5605)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2603 (7.8476)  time: 0.8135 (0.5205 -- 5.2029)  data: 0.2653 (0.0002 -- 4.6480)  max mem: 16413
Epoch: [273]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.5290 (1.5732)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2840 (7.8222)  time: 0.8373 (0.5251 -- 2.9125)  data: 0.2719 (0.0003 -- 2.3956)  max mem: 16413
[2023-09-05 09:00:19,702] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:00:19,702] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:00:19,702] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:00:19,702] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [273]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.4554 (1.5545)  loss_scale: 32768.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5393 (7.4774)  time: 0.8540 (0.5213 -- 3.8493)  data: 0.2245 (0.0002 -- 3.3219)  max mem: 16413
[2023-09-05 09:00:51,405] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43799
[2023-09-05 09:00:51,405] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:00:51,405] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43799
[2023-09-05 09:00:51,405] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:00:51,405] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [273]  [120/160]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000000  loss: 1.6203 (1.5620)  loss_scale: 32768.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2777 (7.4823)  time: 0.9751 (0.5180 -- 3.9298)  data: 0.3461 (0.0004 -- 3.4032)  max mem: 16413
Epoch: [273]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5790 (1.5674)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5173 (7.4059)  time: 0.8287 (0.5215 -- 4.1678)  data: 0.2854 (0.0003 -- 3.6263)  max mem: 16413
[2023-09-05 09:01:21,531] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43834
[2023-09-05 09:01:21,531] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:01:21,531] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 43834
[2023-09-05 09:01:21,532] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:01:21,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [273]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5701 (1.5663)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1018 (7.3741)  time: 0.6518 (0.4807 -- 2.8859)  data: 0.1373 (0.0002 -- 2.3525)  max mem: 16413
Epoch: [273] Total time: 0:02:24 (0.9011 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5701 (1.5630)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1018 (7.3741)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1278 (0.1278)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4233 (2.4233 -- 2.4233)  data: 2.2205 (2.2205 -- 2.2205)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5715 (0.5086)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4192 (0.2005 -- 2.4233)  data: 0.2039 (0.0005 -- 2.2205)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4804 (0.5050)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2199 (0.1693 -- 0.4875)  data: 0.0163 (0.0001 -- 0.3011)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4804 (0.5489)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2025 (0.1328 -- 0.4875)  data: 0.0154 (0.0001 -- 0.3011)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.573
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [274]  [  0/160]  eta: 0:23:20  lr: 0.000001  min_lr: 0.000000  loss: 2.0123 (2.0123)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2619 (8.2619)  time: 8.7538 (8.7538 -- 8.7538)  data: 7.7670 (7.7670 -- 7.7670)  max mem: 16413
Epoch: [274]  [ 20/160]  eta: 0:02:49  lr: 0.000001  min_lr: 0.000000  loss: 1.5368 (1.5923)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4620 (6.7946)  time: 0.8308 (0.5235 -- 2.7615)  data: 0.1487 (0.0003 -- 1.7242)  max mem: 16413
Epoch: [274]  [ 40/160]  eta: 0:02:04  lr: 0.000001  min_lr: 0.000000  loss: 1.3038 (1.4802)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8910 (6.9678)  time: 0.8633 (0.5270 -- 3.5638)  data: 0.1002 (0.0003 -- 1.0082)  max mem: 16413
Epoch: [274]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.5825 (1.4972)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4564 (7.1080)  time: 0.8626 (0.5096 -- 4.3757)  data: 0.0019 (0.0002 -- 0.0151)  max mem: 16413
Epoch: [274]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6788 (1.5435)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1260 (7.0720)  time: 0.8267 (0.5240 -- 2.9723)  data: 0.0020 (0.0006 -- 0.0039)  max mem: 16413
Epoch: [274]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6137 (1.5604)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0454 (6.9878)  time: 0.8939 (0.5401 -- 3.7491)  data: 0.0018 (0.0001 -- 0.0054)  max mem: 16413
Epoch: [274]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.4982 (1.5552)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6267 (7.0798)  time: 0.7773 (0.5304 -- 1.8100)  data: 0.1443 (0.0005 -- 1.2500)  max mem: 16413
[2023-09-05 09:03:25,187] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:03:25,187] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 09:03:25,190] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:03:25,190] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [274]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7623 (1.5844)  loss_scale: 16384.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6992 (7.1165)  time: 1.0280 (0.5154 -- 3.4015)  data: 0.1426 (0.0005 -- 1.5286)  max mem: 16413
[2023-09-05 09:03:55,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=44000, skipped=249, lr=[2.620662623873495e-07, 2.620662623873495e-07, 2.911847359859439e-07, 2.911847359859439e-07, 3.235385955399376e-07, 3.235385955399376e-07, 3.594873283777085e-07, 3.594873283777085e-07, 3.9943036486412053e-07, 3.9943036486412053e-07, 4.438115165156895e-07, 4.438115165156895e-07, 4.931239072396549e-07, 4.931239072396549e-07, 5.479154524885054e-07, 5.479154524885054e-07, 6.087949472094505e-07, 6.087949472094505e-07, 6.764388302327228e-07, 6.764388302327228e-07, 7.515987002585808e-07, 7.515987002585808e-07, 8.351096669539787e-07, 8.351096669539787e-07, 9.278996299488651e-07, 9.278996299488651e-07, 1.0309995888320724e-06, 1.0309995888320724e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 09:03:55,619] [INFO] [timer.py:260:stop] epoch=0/micro_step=44000/global_step=44000, RunningAvgSamplesPerSec=17.734630496785314, CurrSamplesPerSec=24.446331786039188, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [274]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4375 (1.5755)  loss_scale: 16384.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0062 (7.0930)  time: 0.6737 (0.4948 -- 3.3916)  data: 0.0160 (0.0002 -- 0.3095)  max mem: 16413
Epoch: [274] Total time: 0:02:23 (0.8962 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4375 (1.5706)  loss_scale: 16384.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0062 (7.0930)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1295 (0.1295)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3521 (2.3521 -- 2.3521)  data: 2.1250 (2.1250 -- 2.1250)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5695 (0.5071)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4127 (0.1971 -- 2.3521)  data: 0.1941 (0.0004 -- 2.1250)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4802 (0.5051)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2210 (0.1691 -- 0.3384)  data: 0.0131 (0.0001 -- 0.1256)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4802 (0.5488)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2059 (0.1330 -- 0.3384)  data: 0.0129 (0.0001 -- 0.1256)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.573
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [275]  [  0/160]  eta: 0:22:43  lr: 0.000001  min_lr: 0.000000  loss: 1.6375 (1.6375)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0290 (6.0290)  time: 8.5217 (8.5217 -- 8.5217)  data: 7.8285 (7.8285 -- 7.8285)  max mem: 16413
Epoch: [275]  [ 20/160]  eta: 0:02:43  lr: 0.000001  min_lr: 0.000000  loss: 1.4999 (1.4698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2501 (6.6511)  time: 0.8023 (0.5260 -- 3.1261)  data: 0.1714 (0.0004 -- 2.5469)  max mem: 16413
Epoch: [275]  [ 40/160]  eta: 0:02:08  lr: 0.000001  min_lr: 0.000000  loss: 1.5488 (1.5499)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4705 (7.0986)  time: 0.9715 (0.5145 -- 4.4871)  data: 0.1050 (0.0002 -- 1.1157)  max mem: 16413
Epoch: [275]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.3954 (1.4992)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1689 (6.8677)  time: 0.7695 (0.5287 -- 3.0138)  data: 0.0015 (0.0008 -- 0.0029)  max mem: 16413
Epoch: [275]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.5267 (1.5231)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7600 (6.9402)  time: 0.9272 (0.5382 -- 3.7283)  data: 0.0014 (0.0002 -- 0.0025)  max mem: 16413
[2023-09-05 09:05:32,889] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:05:32,889] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:05:32,891] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:05:32,891] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [275]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.5836 (1.5289)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8270 (6.9571)  time: 0.9116 (0.5201 -- 3.1898)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [275]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5207 (1.5379)  loss_scale: 32768.0000 (20446.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5911 (7.1633)  time: 0.7844 (0.5298 -- 2.4221)  data: 0.0015 (0.0005 -- 0.0088)  max mem: 16413
Epoch: [275]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.4951 (1.5390)  loss_scale: 32768.0000 (22193.9291)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7423 (7.1502)  time: 0.8498 (0.5313 -- 2.1948)  data: 0.0022 (0.0002 -- 0.0077)  max mem: 16413
Epoch: [275]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6125 (1.5421)  loss_scale: 32768.0000 (23449.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3997 (7.1608)  time: 0.7950 (0.4984 -- 4.4342)  data: 0.0011 (0.0002 -- 0.0077)  max mem: 16413
Epoch: [275] Total time: 0:02:24 (0.9015 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6125 (1.5531)  loss_scale: 32768.0000 (23449.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3997 (7.1608)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1298 (0.1298)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3469 (2.3469 -- 2.3469)  data: 2.1018 (2.1018 -- 2.1018)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5633 (0.5066)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4127 (0.2053 -- 2.3469)  data: 0.1923 (0.0006 -- 2.1018)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4691 (0.5029)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2199 (0.1695 -- 0.4436)  data: 0.0132 (0.0001 -- 0.2301)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4691 (0.5484)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2041 (0.1327 -- 0.4436)  data: 0.0130 (0.0001 -- 0.2301)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.572
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [276]  [  0/160]  eta: 0:21:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6378 (1.6378)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9322 (8.9322)  time: 8.2249 (8.2249 -- 8.2249)  data: 6.2124 (6.2124 -- 6.2124)  max mem: 16413
Epoch: [276]  [ 20/160]  eta: 0:02:46  lr: 0.000001  min_lr: 0.000000  loss: 1.6956 (1.7248)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0763 (7.5570)  time: 0.8373 (0.5362 -- 3.8835)  data: 0.1690 (0.0005 -- 2.2854)  max mem: 16413
Epoch: [276]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.6091 (1.6579)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2789 (7.6793)  time: 0.9247 (0.5248 -- 3.8463)  data: 0.3766 (0.0004 -- 3.2901)  max mem: 16413
[2023-09-05 09:07:35,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:07:35,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-09-05 09:07:35,446] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:07:35,447] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [276]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.5292 (1.5918)  loss_scale: 32768.0000 (33842.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2726 (7.1079)  time: 0.8573 (0.5261 -- 2.9917)  data: 0.1034 (0.0002 -- 2.0454)  max mem: 16413
[2023-09-05 09:07:39,672] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44225
[2023-09-05 09:07:39,672] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44225
[2023-09-05 09:07:39,672] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 09:07:39,672] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-09-05 09:07:39,673] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [276]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.5919 (1.5952)  loss_scale: 32768.0000 (35195.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7830 (7.0149)  time: 0.8963 (0.5210 -- 4.9786)  data: 0.0503 (0.0004 -- 0.9779)  max mem: 16413
[2023-09-05 09:07:59,377] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44247
[2023-09-05 09:07:59,377] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44247
[2023-09-05 09:07:59,377] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:07:59,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 09:07:59,377] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [276]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6031 (1.5920)  loss_scale: 16384.0000 (32443.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1046 (6.9863)  time: 0.8433 (0.5309 -- 2.7697)  data: 0.0018 (0.0003 -- 0.0093)  max mem: 16413
Epoch: [276]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5287 (1.5911)  loss_scale: 16384.0000 (29789.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8758 (7.0990)  time: 0.7529 (0.5237 -- 4.0704)  data: 0.1931 (0.0004 -- 3.5299)  max mem: 16413
Epoch: [276]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.3830 (1.5760)  loss_scale: 16384.0000 (27887.6596)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7205 (7.0885)  time: 0.9978 (0.5184 -- 3.7688)  data: 0.4502 (0.0004 -- 3.2202)  max mem: 16413
Epoch: [276]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5540 (1.5738)  loss_scale: 16384.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3841 (6.9691)  time: 0.6663 (0.4961 -- 3.4521)  data: 0.1459 (0.0001 -- 2.9053)  max mem: 16413
Epoch: [276] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5540 (1.5552)  loss_scale: 16384.0000 (26521.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3841 (6.9691)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.1298 (0.1298)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1428 (2.1428 -- 2.1428)  data: 1.9033 (1.9033 -- 1.9033)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5614 (0.5063)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4356 (0.2022 -- 2.1428)  data: 0.2169 (0.0007 -- 1.9033)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4696 (0.5031)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2366 (0.1691 -- 0.6831)  data: 0.0314 (0.0001 -- 0.4724)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4696 (0.5489)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2184 (0.1323 -- 0.6831)  data: 0.0311 (0.0001 -- 0.4724)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.571
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [277]  [  0/160]  eta: 0:22:06  lr: 0.000001  min_lr: 0.000000  loss: 1.5369 (1.5369)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6198 (5.6198)  time: 8.2894 (8.2894 -- 8.2894)  data: 5.9674 (5.9674 -- 5.9674)  max mem: 16413
Epoch: [277]  [ 20/160]  eta: 0:02:50  lr: 0.000001  min_lr: 0.000000  loss: 1.6560 (1.6897)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4824 (7.7283)  time: 0.8677 (0.5304 -- 3.0561)  data: 0.1551 (0.0006 -- 1.5556)  max mem: 16413
Epoch: [277]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.4160 (1.6234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0324 (7.4341)  time: 0.8129 (0.5325 -- 2.6583)  data: 0.1905 (0.0007 -- 2.1183)  max mem: 16413
[2023-09-05 09:10:01,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:10:01,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:10:01,481] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:10:01,482] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [277]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.5908 (1.6398)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4060 (7.4090)  time: 0.9294 (0.5344 -- 3.8248)  data: 0.2963 (0.0004 -- 3.2733)  max mem: 16413
Epoch: [277]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.5176 (1.6232)  loss_scale: 32768.0000 (21440.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9209 (7.4362)  time: 0.8204 (0.5334 -- 4.3946)  data: 0.2031 (0.0004 -- 3.8742)  max mem: 16413
[2023-09-05 09:10:28,852] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44407
[2023-09-05 09:10:28,852] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44407
[2023-09-05 09:10:28,852] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:10:28,852] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:10:28,852] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [277]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.5115 (1.5964)  loss_scale: 16384.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3198 (7.3604)  time: 0.9047 (0.5208 -- 3.4284)  data: 0.3554 (0.0005 -- 2.8800)  max mem: 16413
Epoch: [277]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6543 (1.5924)  loss_scale: 16384.0000 (20581.5537)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4785 (7.5163)  time: 0.8160 (0.5357 -- 3.9699)  data: 0.2671 (0.0002 -- 3.4386)  max mem: 16413
Epoch: [277]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5675 (1.5868)  loss_scale: 16384.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9560 (7.5273)  time: 0.9252 (0.5290 -- 2.3736)  data: 0.3416 (0.0007 -- 1.8424)  max mem: 16413
[2023-09-05 09:11:25,299] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44473
[2023-09-05 09:11:25,299] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44473
[2023-09-05 09:11:25,300] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:11:25,300] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:11:25,300] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [277]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6003 (1.5833)  loss_scale: 16384.0000 (19200.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0236 (7.5969)  time: 0.6866 (0.4957 -- 2.1513)  data: 0.1672 (0.0002 -- 1.6261)  max mem: 16413
Epoch: [277] Total time: 0:02:22 (0.8898 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6003 (1.5754)  loss_scale: 16384.0000 (19200.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0236 (7.5969)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1304 (0.1304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3182 (2.3182 -- 2.3182)  data: 2.0570 (2.0570 -- 2.0570)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5684 (0.5052)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4066 (0.1952 -- 2.3182)  data: 0.1895 (0.0008 -- 2.0570)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4723 (0.5040)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2229 (0.1697 -- 0.5217)  data: 0.0179 (0.0001 -- 0.3263)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4723 (0.5497)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2069 (0.1334 -- 0.5217)  data: 0.0168 (0.0001 -- 0.3263)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.571
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [278]  [  0/160]  eta: 0:22:24  lr: 0.000001  min_lr: 0.000000  loss: 1.8588 (1.8588)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9270 (5.9270)  time: 8.4046 (8.4046 -- 8.4046)  data: 5.1109 (5.1109 -- 5.1109)  max mem: 16413
Epoch: [278]  [ 20/160]  eta: 0:02:38  lr: 0.000001  min_lr: 0.000000  loss: 1.6372 (1.5614)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7400 (6.4686)  time: 0.7695 (0.5369 -- 2.1172)  data: 0.1043 (0.0006 -- 0.8725)  max mem: 16413
Epoch: [278]  [ 40/160]  eta: 0:02:08  lr: 0.000001  min_lr: 0.000000  loss: 1.4704 (1.4984)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7443 (7.2657)  time: 0.9969 (0.5270 -- 4.3200)  data: 0.1340 (0.0002 -- 1.5933)  max mem: 16413
Epoch: [278]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.3443 (1.4867)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5863 (7.1543)  time: 0.7779 (0.5194 -- 1.8990)  data: 0.1169 (0.0003 -- 0.8819)  max mem: 16413
Epoch: [278]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.5477 (1.4936)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6814 (7.0741)  time: 0.8646 (0.5297 -- 1.9291)  data: 0.2266 (0.0003 -- 1.3913)  max mem: 16413
Epoch: [278]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.5291 (1.5024)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0495 (7.1327)  time: 0.8969 (0.5280 -- 2.4420)  data: 0.3094 (0.0003 -- 1.8859)  max mem: 16413
Epoch: [278]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.4630 (1.4983)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5847 (7.1341)  time: 0.9190 (0.5164 -- 3.9908)  data: 0.3775 (0.0005 -- 3.4284)  max mem: 16413
[2023-09-05 09:13:31,394] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:13:31,395] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 09:13:31,396] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:13:31,396] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [278]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.4981 (1.5031)  loss_scale: 16384.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7554 (6.9394)  time: 0.8609 (0.5172 -- 3.2080)  data: 0.2610 (0.0005 -- 2.6646)  max mem: 16413
[2023-09-05 09:13:56,421] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44634
[2023-09-05 09:13:56,421] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44634
[2023-09-05 09:13:56,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:13:56,421] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:13:56,421] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [278]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4610 (1.5192)  loss_scale: 16384.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3318 (7.0183)  time: 0.6615 (0.4828 -- 1.9679)  data: 0.0729 (0.0002 -- 1.4410)  max mem: 16413
Epoch: [278] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4610 (1.5363)  loss_scale: 16384.0000 (9830.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3318 (7.0183)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1310 (0.1310)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4940 (2.4940 -- 2.4940)  data: 2.2398 (2.2398 -- 2.2398)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5659 (0.5057)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4323 (0.1909 -- 2.4940)  data: 0.2097 (0.0005 -- 2.2398)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4763 (0.5050)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2145 (0.1691 -- 0.2645)  data: 0.0069 (0.0001 -- 0.0649)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4763 (0.5501)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.1932 (0.1328 -- 0.2527)  data: 0.0058 (0.0001 -- 0.0649)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.571
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [279]  [  0/160]  eta: 0:21:22  lr: 0.000001  min_lr: 0.000000  loss: 1.3959 (1.3959)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1393 (9.1393)  time: 8.0169 (8.0169 -- 8.0169)  data: 7.4789 (7.4789 -- 7.4789)  max mem: 16413
Epoch: [279]  [ 20/160]  eta: 0:02:53  lr: 0.000001  min_lr: 0.000000  loss: 1.5100 (1.4933)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5470 (7.0295)  time: 0.9027 (0.5164 -- 4.2065)  data: 0.3249 (0.0004 -- 3.6742)  max mem: 16413
Epoch: [279]  [ 40/160]  eta: 0:02:05  lr: 0.000001  min_lr: 0.000000  loss: 1.5098 (1.5101)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5686 (7.4361)  time: 0.8366 (0.5284 -- 3.3136)  data: 0.2819 (0.0003 -- 2.7631)  max mem: 16413
Epoch: [279]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.5085 (1.5174)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8039 (7.2246)  time: 0.8828 (0.5228 -- 2.4538)  data: 0.3361 (0.0004 -- 1.9245)  max mem: 16413
Epoch: [279]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.5944 (1.5348)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3804 (7.0624)  time: 0.8356 (0.5218 -- 3.1977)  data: 0.2872 (0.0004 -- 2.6465)  max mem: 16413
Epoch: [279]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.5559 (1.5501)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4270 (7.1753)  time: 0.8377 (0.5295 -- 3.4625)  data: 0.2823 (0.0005 -- 2.9219)  max mem: 16413
Epoch: [279]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.5059 (1.5453)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9090 (7.2311)  time: 1.0178 (0.5314 -- 4.6753)  data: 0.4424 (0.0002 -- 4.1109)  max mem: 16413
[2023-09-05 09:16:02,641] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:16:02,641] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:16:02,641] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 09:16:02,641] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [279]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5326 (1.5443)  loss_scale: 16384.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4895 (7.2161)  time: 0.7952 (0.5180 -- 3.2091)  data: 0.2503 (0.0003 -- 2.6584)  max mem: 16413
[2023-09-05 09:16:28,550] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44797
[2023-09-05 09:16:28,550] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:16:28,550] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44797
[2023-09-05 09:16:28,550] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:16:28,550] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [279]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6817 (1.5598)  loss_scale: 16384.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1364 (7.1981)  time: 0.6555 (0.4827 -- 3.0083)  data: 0.1405 (0.0002 -- 2.4875)  max mem: 16413
Epoch: [279] Total time: 0:02:22 (0.8925 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6817 (1.5451)  loss_scale: 16384.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1364 (7.1981)
[2023-09-05 09:16:29,543] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-279 is about to be saved!
[2023-09-05 09:16:29,545] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-279 is ready now!
[2023-09-05 09:16:29,545] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-279/mp_rank_00_model_states.pt
[2023-09-05 09:16:29,545] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-279/mp_rank_00_model_states.pt...
[2023-09-05 09:16:30,576] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-279/mp_rank_00_model_states.pt.
[2023-09-05 09:16:30,577] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-279 is ready now!
Val:  [ 0/27]  eta: 0:01:16  loss: 0.1309 (0.1309)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.8446 (2.8446 -- 2.8446)  data: 2.6050 (2.6050 -- 2.6050)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5678 (0.5043)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4594 (0.1985 -- 2.8446)  data: 0.2383 (0.0007 -- 2.6050)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4733 (0.5041)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2075 (0.1691 -- 0.2423)  data: 0.0010 (0.0001 -- 0.0035)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4733 (0.5493)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.1908 (0.1327 -- 0.2423)  data: 0.0007 (0.0001 -- 0.0035)  max mem: 16413
Val: Total time: 0:00:07 (0.2955 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.571
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [280]  [  0/160]  eta: 0:17:20  lr: 0.000001  min_lr: 0.000000  loss: 1.2342 (1.2342)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6963 (5.6963)  time: 6.5017 (6.5017 -- 6.5017)  data: 5.9759 (5.9759 -- 5.9759)  max mem: 16413
Epoch: [280]  [ 20/160]  eta: 0:02:48  lr: 0.000001  min_lr: 0.000000  loss: 1.5428 (1.4444)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8695 (7.2376)  time: 0.9380 (0.5283 -- 2.3061)  data: 0.3227 (0.0003 -- 1.6433)  max mem: 16413
Epoch: [280]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.4816 (1.4824)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1476 (7.5761)  time: 0.9167 (0.5366 -- 3.0480)  data: 0.3495 (0.0005 -- 2.4850)  max mem: 16413
Epoch: [280]  [ 60/160]  eta: 0:01:42  lr: 0.000001  min_lr: 0.000000  loss: 1.4855 (1.4916)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9368 (7.4710)  time: 0.9344 (0.5141 -- 4.6963)  data: 0.3919 (0.0002 -- 4.1780)  max mem: 16413
Epoch: [280]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.4991 (1.4908)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9983 (7.3869)  time: 0.8731 (0.5293 -- 3.7103)  data: 0.3234 (0.0002 -- 3.1437)  max mem: 16413
Epoch: [280]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.4720 (1.4834)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5604 (7.3898)  time: 0.7049 (0.5391 -- 1.9641)  data: 0.1461 (0.0006 -- 1.4393)  max mem: 16413
Epoch: [280]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.4451 (1.4832)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7421 (7.3030)  time: 0.9209 (0.5123 -- 3.7450)  data: 0.3521 (0.0003 -- 3.2321)  max mem: 16413
[2023-09-05 09:18:35,385] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:18:35,385] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 09:18:35,386] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:18:35,386] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [280]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5485 (1.4809)  loss_scale: 16384.0000 (9063.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0007 (7.2295)  time: 0.8440 (0.5349 -- 2.5745)  data: 0.2339 (0.0005 -- 2.0461)  max mem: 16413
Epoch: [280]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5010 (1.4848)  loss_scale: 16384.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7220 (7.2621)  time: 0.7207 (0.4950 -- 2.0081)  data: 0.1286 (0.0001 -- 1.4895)  max mem: 16413
Epoch: [280] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5010 (1.5316)  loss_scale: 16384.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7220 (7.2621)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1314 (0.1314)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4757 (2.4757 -- 2.4757)  data: 2.2036 (2.2036 -- 2.2036)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5674 (0.5050)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4205 (0.2040 -- 2.4757)  data: 0.2013 (0.0006 -- 2.2036)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4696 (0.5044)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2182 (0.1695 -- 0.4559)  data: 0.0130 (0.0001 -- 0.2383)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4696 (0.5489)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2029 (0.1326 -- 0.4559)  data: 0.0127 (0.0001 -- 0.2383)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 85.477 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [281]  [  0/160]  eta: 0:20:57  lr: 0.000001  min_lr: 0.000000  loss: 1.1940 (1.1940)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4811 (6.4811)  time: 7.8613 (7.8613 -- 7.8613)  data: 5.5282 (5.5282 -- 5.5282)  max mem: 16413
Epoch: [281]  [ 20/160]  eta: 0:02:41  lr: 0.000001  min_lr: 0.000000  loss: 1.6668 (1.6170)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9286 (7.1955)  time: 0.8178 (0.5227 -- 2.4047)  data: 0.2286 (0.0007 -- 1.8892)  max mem: 16413
[2023-09-05 09:19:47,286] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44996
[2023-09-05 09:19:47,286] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 44996
[2023-09-05 09:19:47,286] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:19:47,286] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:19:47,286] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-09-05 09:19:48,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=45000, skipped=256, lr=[1.5308396280030944e-07, 1.5308396280030944e-07, 1.7009329200034383e-07, 1.7009329200034383e-07, 1.8899254666704866e-07, 1.8899254666704866e-07, 2.0999171851894297e-07, 2.0999171851894297e-07, 2.333241316877144e-07, 2.333241316877144e-07, 2.5924903520857153e-07, 2.5924903520857153e-07, 2.880544835650795e-07, 2.880544835650795e-07, 3.2006053729453274e-07, 3.2006053729453274e-07, 3.5562281921614747e-07, 3.5562281921614747e-07, 3.951364657957194e-07, 3.951364657957194e-07, 4.390405175507994e-07, 4.390405175507994e-07, 4.878227972786659e-07, 4.878227972786659e-07, 5.420253303096288e-07, 5.420253303096288e-07, 6.022503670106987e-07, 6.022503670106987e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 09:19:48,955] [INFO] [timer.py:260:stop] epoch=0/micro_step=45000/global_step=45000, RunningAvgSamplesPerSec=17.75969646612807, CurrSamplesPerSec=21.688449781226595, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [281]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.5906 (1.5686)  loss_scale: 16384.0000 (15384.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3991 (7.3353)  time: 0.8779 (0.5276 -- 2.3112)  data: 0.2292 (0.0003 -- 1.4778)  max mem: 16413
Epoch: [281]  [ 60/160]  eta: 0:01:42  lr: 0.000001  min_lr: 0.000000  loss: 1.7066 (1.5999)  loss_scale: 8192.0000 (13026.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6453 (7.2822)  time: 1.0274 (0.5202 -- 3.7469)  data: 0.4233 (0.0004 -- 3.1868)  max mem: 16413
Epoch: [281]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.6265 (1.6055)  loss_scale: 8192.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0710 (7.0440)  time: 0.7674 (0.5242 -- 3.0212)  data: 0.2140 (0.0001 -- 2.4863)  max mem: 16413
Epoch: [281]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.5391 (1.6027)  loss_scale: 8192.0000 (11111.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9438 (7.0567)  time: 0.8063 (0.5212 -- 2.0920)  data: 0.2320 (0.0003 -- 1.5567)  max mem: 16413
Epoch: [281]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.6013 (1.6038)  loss_scale: 8192.0000 (10629.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7718 (7.0695)  time: 0.9224 (0.5362 -- 2.6566)  data: 0.2179 (0.0004 -- 1.9563)  max mem: 16413
Epoch: [281]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7266 (1.6156)  loss_scale: 8192.0000 (10283.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0197 (7.1237)  time: 1.0358 (0.5248 -- 5.3511)  data: 0.4603 (0.0003 -- 4.8354)  max mem: 16413
Epoch: [281]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5820 (1.6192)  loss_scale: 8192.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5410 (7.0634)  time: 0.6149 (0.4943 -- 1.9024)  data: 0.0931 (0.0002 -- 1.3852)  max mem: 16413
Epoch: [281] Total time: 0:02:24 (0.9047 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5820 (1.5824)  loss_scale: 8192.0000 (10035.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5410 (7.0634)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.1311 (0.1311)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6888 (2.6888 -- 2.6888)  data: 2.4138 (2.4138 -- 2.4138)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5692 (0.5051)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4493 (0.2062 -- 2.6888)  data: 0.2228 (0.0006 -- 2.4138)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4683 (0.5041)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2105 (0.1697 -- 0.2482)  data: 0.0031 (0.0001 -- 0.0250)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4683 (0.5482)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.1922 (0.1322 -- 0.2462)  data: 0.0027 (0.0001 -- 0.0250)  max mem: 16413
Val: Total time: 0:00:07 (0.2921 s / it)
* Acc@1 85.685 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.68%
Max accuracy: 86.51%
Epoch: [282]  [  0/160]  eta: 0:21:46  lr: 0.000001  min_lr: 0.000000  loss: 2.1151 (2.1151)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8601 (8.8601)  time: 8.1643 (8.1643 -- 8.1643)  data: 7.6137 (7.6137 -- 7.6137)  max mem: 16413
[2023-09-05 09:21:52,450] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:21:52,450] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 09:21:52,452] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:21:52,452] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [282]  [ 20/160]  eta: 0:02:40  lr: 0.000001  min_lr: 0.000000  loss: 1.5330 (1.5558)  loss_scale: 16384.0000 (14433.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7125 (6.8696)  time: 0.7928 (0.5264 -- 3.1647)  data: 0.2393 (0.0003 -- 2.6312)  max mem: 16413
Epoch: [282]  [ 40/160]  eta: 0:02:08  lr: 0.000001  min_lr: 0.000000  loss: 1.4299 (1.5506)  loss_scale: 16384.0000 (15384.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7391 (6.9825)  time: 0.9893 (0.5194 -- 6.2096)  data: 0.4447 (0.0004 -- 5.6766)  max mem: 16413
Epoch: [282]  [ 60/160]  eta: 0:01:35  lr: 0.000001  min_lr: 0.000000  loss: 1.6839 (1.5958)  loss_scale: 16384.0000 (15712.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8226 (6.9288)  time: 0.7113 (0.5291 -- 2.0487)  data: 0.1581 (0.0002 -- 1.5256)  max mem: 16413
Epoch: [282]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.4138 (1.5674)  loss_scale: 16384.0000 (15878.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0591 (7.0325)  time: 0.8828 (0.5359 -- 2.4127)  data: 0.1722 (0.0003 -- 1.1886)  max mem: 16413
Epoch: [282]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6865 (1.5844)  loss_scale: 16384.0000 (15978.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0696 (7.1256)  time: 0.9688 (0.5296 -- 3.6500)  data: 0.1089 (0.0005 -- 0.9321)  max mem: 16413
Epoch: [282]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.4592 (1.5622)  loss_scale: 16384.0000 (16045.4876)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7634 (7.2102)  time: 0.8378 (0.5197 -- 3.6927)  data: 0.0828 (0.0003 -- 1.6256)  max mem: 16413
[2023-09-05 09:23:44,245] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:23:44,246] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:23:44,246] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:23:44,246] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:23:45,410] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45255
[2023-09-05 09:23:45,410] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:23:45,410] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45255
[2023-09-05 09:23:45,410] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:23:45,410] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [282]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.3554 (1.5388)  loss_scale: 16384.0000 (16325.9007)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1734 (7.1690)  time: 0.9362 (0.5191 -- 2.4698)  data: 0.2834 (0.0009 -- 1.9347)  max mem: 16413
Epoch: [282]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6409 (1.5398)  loss_scale: 16384.0000 (16332.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4163 (7.0841)  time: 0.6755 (0.4959 -- 1.5688)  data: 0.1052 (0.0002 -- 1.0207)  max mem: 16413
Epoch: [282] Total time: 0:02:23 (0.8973 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6409 (1.5421)  loss_scale: 16384.0000 (16332.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4163 (7.0841)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1308 (0.1308)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3859 (2.3859 -- 2.3859)  data: 2.1818 (2.1818 -- 2.1818)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5638 (0.5054)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4178 (0.2021 -- 2.3859)  data: 0.2059 (0.0010 -- 2.1818)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4649 (0.5033)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2231 (0.1697 -- 0.5061)  data: 0.0205 (0.0001 -- 0.3230)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4649 (0.5471)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2088 (0.1324 -- 0.5061)  data: 0.0202 (0.0001 -- 0.3230)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 85.477 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [283]  [  0/160]  eta: 0:21:35  lr: 0.000001  min_lr: 0.000000  loss: 1.7478 (1.7478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2500 (6.2500)  time: 8.0991 (8.0991 -- 8.0991)  data: 6.7823 (6.7823 -- 6.7823)  max mem: 16413
Epoch: [283]  [ 20/160]  eta: 0:02:52  lr: 0.000000  min_lr: 0.000000  loss: 1.6010 (1.5651)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8170 (8.0220)  time: 0.8863 (0.5184 -- 4.8015)  data: 0.3389 (0.0002 -- 4.2480)  max mem: 16413
Epoch: [283]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.6692 (1.6502)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7677 (7.4442)  time: 0.8133 (0.5207 -- 2.8987)  data: 0.2671 (0.0003 -- 2.3650)  max mem: 16413
Epoch: [283]  [ 60/160]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.4639 (1.5883)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6013 (7.5098)  time: 0.9460 (0.5362 -- 2.8459)  data: 0.0851 (0.0004 -- 1.5425)  max mem: 16413
Epoch: [283]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.5562 (1.5862)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8470 (7.4572)  time: 0.8406 (0.5148 -- 4.0235)  data: 0.0013 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [283]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.3339 (1.5472)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4159 (7.2993)  time: 0.8748 (0.5226 -- 1.9086)  data: 0.0545 (0.0003 -- 1.0641)  max mem: 16413
[2023-09-05 09:25:50,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:25:50,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:25:50,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:25:50,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:25:58,559] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45392
[2023-09-05 09:25:58,560] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:25:58,561] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45392
[2023-09-05 09:25:58,563] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:25:58,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [283]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6977 (1.5621)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9548 (7.3028)  time: 0.8278 (0.5295 -- 2.7739)  data: 0.0969 (0.0002 -- 1.4516)  max mem: 16413
Epoch: [283]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.3511 (1.5478)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0207 (7.2158)  time: 0.9628 (0.5303 -- 4.2345)  data: 0.2746 (0.0002 -- 3.7069)  max mem: 16413
Epoch: [283]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3675 (1.5325)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6013 (7.1991)  time: 0.6525 (0.4950 -- 2.5340)  data: 0.1298 (0.0002 -- 1.9994)  max mem: 16413
Epoch: [283] Total time: 0:02:23 (0.8979 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3675 (1.5669)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6013 (7.1991)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1305 (0.1305)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2727 (2.2727 -- 2.2727)  data: 2.0078 (2.0078 -- 2.0078)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5621 (0.5045)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4441 (0.1880 -- 2.2727)  data: 0.2274 (0.0005 -- 2.0078)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4709 (0.5029)  acc1: 88.8889 (87.8307)  acc5: 100.0000 (98.4127)  time: 0.2284 (0.1694 -- 0.7093)  data: 0.0249 (0.0001 -- 0.4862)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4709 (0.5473)  acc1: 88.8889 (86.3071)  acc5: 100.0000 (97.9253)  time: 0.2138 (0.1327 -- 0.7093)  data: 0.0247 (0.0001 -- 0.4862)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 85.477 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 86.51%
Epoch: [284]  [  0/160]  eta: 0:16:03  lr: 0.000000  min_lr: 0.000000  loss: 1.4546 (1.4546)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6973 (5.6973)  time: 6.0199 (6.0199 -- 6.0199)  data: 5.2924 (5.2924 -- 5.2924)  max mem: 16413
Epoch: [284]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.6206 (1.5981)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4625 (6.6474)  time: 0.9138 (0.5254 -- 2.6959)  data: 0.3626 (0.0009 -- 2.1344)  max mem: 16413
Epoch: [284]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.5558 (1.6084)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2178 (7.0552)  time: 0.8990 (0.5277 -- 3.0948)  data: 0.2236 (0.0008 -- 2.0813)  max mem: 16413
Epoch: [284]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.4921 (1.5749)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9751 (7.3058)  time: 0.8987 (0.5349 -- 2.9164)  data: 0.1221 (0.0005 -- 2.3936)  max mem: 16413
Epoch: [284]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.4379 (1.5601)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6659 (7.4155)  time: 0.8359 (0.5232 -- 3.1466)  data: 0.0880 (0.0005 -- 1.2702)  max mem: 16413
[2023-09-05 09:28:01,915] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:28:01,915] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:28:01,918] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:28:01,918] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:28:09,640] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45528
[2023-09-05 09:28:09,640] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45528
[2023-09-05 09:28:09,641] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:28:09,641] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:28:09,641] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [284]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.4492 (1.5346)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6930 (7.2900)  time: 0.9715 (0.5277 -- 2.9607)  data: 0.0819 (0.0004 -- 1.6079)  max mem: 16413
[2023-09-05 09:28:21,290] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45541
[2023-09-05 09:28:21,290] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45541
[2023-09-05 09:28:21,290] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:28:21,290] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:28:21,290] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [284]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7580 (1.5680)  loss_scale: 8192.0000 (15977.7851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2279 (7.2630)  time: 0.7148 (0.5210 -- 2.6146)  data: 0.0018 (0.0005 -- 0.0067)  max mem: 16413
Epoch: [284]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5282 (1.5622)  loss_scale: 8192.0000 (14873.4184)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7718 (7.2048)  time: 1.0440 (0.5127 -- 5.3970)  data: 0.0013 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [284]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7029 (1.5703)  loss_scale: 8192.0000 (14080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1830 (7.1077)  time: 0.6221 (0.4937 -- 2.4476)  data: 0.0009 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [284] Total time: 0:02:23 (0.8970 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7029 (1.5797)  loss_scale: 8192.0000 (14080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1830 (7.1077)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1304 (0.1304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3974 (2.3974 -- 2.3974)  data: 2.1967 (2.1967 -- 2.1967)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5657 (0.5051)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4272 (0.1989 -- 2.3974)  data: 0.2175 (0.0006 -- 2.1967)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4708 (0.5037)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2262 (0.1690 -- 0.4250)  data: 0.0219 (0.0001 -- 0.2385)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4708 (0.5482)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2119 (0.1329 -- 0.4250)  data: 0.0216 (0.0001 -- 0.2385)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [285]  [  0/160]  eta: 0:19:25  lr: 0.000000  min_lr: 0.000000  loss: 1.2046 (1.2046)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.4417 (14.4417)  time: 7.2829 (7.2829 -- 7.2829)  data: 6.7218 (6.7218 -- 6.7218)  max mem: 16413
Epoch: [285]  [ 20/160]  eta: 0:02:38  lr: 0.000000  min_lr: 0.000000  loss: 1.4022 (1.4438)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5204 (7.4785)  time: 0.8222 (0.5196 -- 3.7088)  data: 0.2323 (0.0008 -- 2.8391)  max mem: 16413
Epoch: [285]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.6390 (1.4931)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1042 (7.3616)  time: 0.9874 (0.5267 -- 4.0328)  data: 0.4108 (0.0004 -- 3.0477)  max mem: 16413
Epoch: [285]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.2851 (1.4659)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0170 (7.3721)  time: 0.7951 (0.5260 -- 3.5798)  data: 0.1131 (0.0009 -- 1.5271)  max mem: 16413
[2023-09-05 09:30:23,914] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:30:23,914] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 09:30:23,915] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:30:23,915] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [285]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.5982 (1.5219)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1583 (7.2985)  time: 0.9259 (0.5226 -- 3.7978)  data: 0.3266 (0.0003 -- 2.9842)  max mem: 16413
Epoch: [285]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.5631 (1.5312)  loss_scale: 16384.0000 (10706.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5038 (7.2661)  time: 0.8635 (0.5269 -- 2.7154)  data: 0.0237 (0.0003 -- 0.4423)  max mem: 16413
Epoch: [285]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7538 (1.5600)  loss_scale: 16384.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6911 (7.2347)  time: 0.8329 (0.5241 -- 3.3555)  data: 0.0021 (0.0003 -- 0.0061)  max mem: 16413
Epoch: [285]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.4486 (1.5501)  loss_scale: 16384.0000 (12317.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7481 (7.2336)  time: 0.9118 (0.5214 -- 3.4838)  data: 0.0200 (0.0002 -- 0.3671)  max mem: 16413
Epoch: [285]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4931 (1.5418)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0264 (7.3155)  time: 0.6484 (0.4963 -- 2.0155)  data: 0.0024 (0.0002 -- 0.0315)  max mem: 16413
Epoch: [285] Total time: 0:02:22 (0.8907 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4931 (1.5308)  loss_scale: 16384.0000 (12800.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0264 (7.3155)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1302 (0.1302)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5823 (2.5823 -- 2.5823)  data: 2.3404 (2.3404 -- 2.3404)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5668 (0.5048)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4350 (0.2059 -- 2.5823)  data: 0.2162 (0.0006 -- 2.3404)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4688 (0.5040)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2081 (0.1701 -- 0.2430)  data: 0.0020 (0.0001 -- 0.0292)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4688 (0.5484)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.1901 (0.1331 -- 0.2367)  data: 0.0018 (0.0001 -- 0.0292)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [286]  [  0/160]  eta: 0:18:08  lr: 0.000000  min_lr: 0.000000  loss: 1.3063 (1.3063)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2572 (4.2572)  time: 6.8057 (6.8057 -- 6.8057)  data: 5.2021 (5.2021 -- 5.2021)  max mem: 16413
Epoch: [286]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.6575 (1.7584)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5043 (6.8066)  time: 0.8743 (0.5446 -- 2.9899)  data: 0.0869 (0.0007 -- 1.2228)  max mem: 16413
[2023-09-05 09:32:12,080] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45783
[2023-09-05 09:32:12,080] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 45783
[2023-09-05 09:32:12,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:32:12,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:32:12,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [286]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 1.3383 (1.5596)  loss_scale: 8192.0000 (12787.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2693 (7.1156)  time: 0.9535 (0.5152 -- 3.5796)  data: 0.0018 (0.0006 -- 0.0035)  max mem: 16413
Epoch: [286]  [ 60/160]  eta: 0:01:42  lr: 0.000000  min_lr: 0.000000  loss: 1.5344 (1.5491)  loss_scale: 8192.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9919 (7.2794)  time: 0.9523 (0.5165 -- 5.0853)  data: 0.0019 (0.0004 -- 0.0104)  max mem: 16413
Epoch: [286]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.5766 (1.5429)  loss_scale: 8192.0000 (10518.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6837 (7.3329)  time: 0.7886 (0.5225 -- 2.8158)  data: 0.0017 (0.0002 -- 0.0052)  max mem: 16413
Epoch: [286]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.3967 (1.5285)  loss_scale: 8192.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6062 (7.2529)  time: 0.8252 (0.5153 -- 3.1619)  data: 0.0016 (0.0003 -- 0.0092)  max mem: 16413
Epoch: [286]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.4776 (1.5219)  loss_scale: 8192.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6139 (7.1984)  time: 0.9155 (0.5297 -- 2.3964)  data: 0.0016 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [286]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5267 (1.5217)  loss_scale: 8192.0000 (9528.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8447 (7.1221)  time: 0.9729 (0.5273 -- 3.0867)  data: 0.0733 (0.0003 -- 1.4416)  max mem: 16413
[2023-09-05 09:34:06,276] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:34:06,276] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 09:34:06,277] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:34:06,277] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [286]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3547 (1.5124)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5647 (7.0503)  time: 0.7172 (0.4964 -- 3.0490)  data: 0.0005 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [286] Total time: 0:02:23 (0.8986 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3547 (1.5230)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5647 (7.0503)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1304 (0.1304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5236 (2.5236 -- 2.5236)  data: 2.2991 (2.2991 -- 2.2991)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5679 (0.5056)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4259 (0.2019 -- 2.5236)  data: 0.2101 (0.0008 -- 2.2991)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4682 (0.5049)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2113 (0.1715 -- 0.2785)  data: 0.0049 (0.0001 -- 0.0722)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4682 (0.5490)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.1953 (0.1326 -- 0.2785)  data: 0.0047 (0.0001 -- 0.0722)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [287]  [  0/160]  eta: 0:19:51  lr: 0.000000  min_lr: 0.000000  loss: 1.8506 (1.8506)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7446 (5.7446)  time: 7.4491 (7.4491 -- 7.4491)  data: 6.9154 (6.9154 -- 6.9154)  max mem: 16413
Epoch: [287]  [ 20/160]  eta: 0:02:34  lr: 0.000000  min_lr: 0.000000  loss: 1.8541 (1.6884)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6554 (7.1325)  time: 0.7882 (0.5317 -- 2.1687)  data: 0.1627 (0.0005 -- 1.6340)  max mem: 16413
Epoch: [287]  [ 40/160]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 1.7577 (1.7163)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5381 (7.3670)  time: 0.9875 (0.5319 -- 3.6816)  data: 0.3889 (0.0005 -- 3.1376)  max mem: 16413
Epoch: [287]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.5699 (1.6437)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1945 (7.1374)  time: 0.8456 (0.5287 -- 3.1156)  data: 0.2414 (0.0002 -- 2.5620)  max mem: 16413
[2023-09-05 09:35:34,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=46000, skipped=261, lr=[7.481625533555652e-08, 7.481625533555652e-08, 8.31291725950628e-08, 8.31291725950628e-08, 9.236574732784755e-08, 9.236574732784755e-08, 1.0262860814205283e-07, 1.0262860814205283e-07, 1.1403178682450314e-07, 1.1403178682450314e-07, 1.2670198536055905e-07, 1.2670198536055905e-07, 1.4077998373395448e-07, 1.4077998373395448e-07, 1.5642220414883832e-07, 1.5642220414883832e-07, 1.738024490542648e-07, 1.738024490542648e-07, 1.9311383228251642e-07, 1.9311383228251642e-07, 2.145709247583516e-07, 2.145709247583516e-07, 2.3841213862039065e-07, 2.3841213862039065e-07, 2.649023762448785e-07, 2.649023762448785e-07, 2.9433597360542054e-07, 2.9433597360542054e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 09:35:34,520] [INFO] [timer.py:260:stop] epoch=0/micro_step=46000/global_step=46000, RunningAvgSamplesPerSec=17.7488210474164, CurrSamplesPerSec=22.079545175384723, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [287]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.3963 (1.6105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3153 (6.9760)  time: 0.8789 (0.5235 -- 3.4821)  data: 0.1362 (0.0003 -- 2.0793)  max mem: 16413
Epoch: [287]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.5151 (1.6085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9951 (7.0787)  time: 0.9272 (0.5209 -- 4.0366)  data: 0.0347 (0.0003 -- 0.6600)  max mem: 16413
[2023-09-05 09:36:08,847] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:36:08,847] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:36:08,848] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:36:08,848] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [287]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5213 (1.6042)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3654 (7.2706)  time: 0.7599 (0.5131 -- 2.2176)  data: 0.0023 (0.0004 -- 0.0163)  max mem: 16413
[2023-09-05 09:36:12,274] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46044
[2023-09-05 09:36:12,274] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46044
[2023-09-05 09:36:12,274] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:36:12,274] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:36:12,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [287]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5373 (1.5937)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6785 (7.1701)  time: 0.8977 (0.5295 -- 3.4042)  data: 0.0019 (0.0002 -- 0.0060)  max mem: 16413
Epoch: [287]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5399 (1.5908)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3113 (7.0818)  time: 0.7063 (0.4966 -- 2.5419)  data: 0.0790 (0.0002 -- 1.1083)  max mem: 16413
Epoch: [287] Total time: 0:02:22 (0.8922 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5399 (1.5832)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3113 (7.0818)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1302 (0.1302)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4546 (2.4546 -- 2.4546)  data: 2.2286 (2.2286 -- 2.2286)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5658 (0.5052)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4207 (0.1972 -- 2.4546)  data: 0.2036 (0.0003 -- 2.2286)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4681 (0.5047)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2179 (0.1695 -- 0.3653)  data: 0.0117 (0.0001 -- 0.1799)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4681 (0.5489)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2021 (0.1331 -- 0.3653)  data: 0.0114 (0.0001 -- 0.1799)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [288]  [  0/160]  eta: 0:22:47  lr: 0.000000  min_lr: 0.000000  loss: 1.4209 (1.4209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6390 (10.6390)  time: 8.5465 (8.5465 -- 8.5465)  data: 6.4120 (6.4120 -- 6.4120)  max mem: 16413
Epoch: [288]  [ 20/160]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.3552 (1.4745)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6029 (6.4062)  time: 0.8232 (0.5291 -- 3.7599)  data: 0.1495 (0.0003 -- 1.8745)  max mem: 16413
Epoch: [288]  [ 40/160]  eta: 0:02:13  lr: 0.000000  min_lr: 0.000000  loss: 1.7060 (1.5872)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5359 (7.0852)  time: 1.0303 (0.5300 -- 4.9150)  data: 0.2530 (0.0006 -- 2.3095)  max mem: 16413
Epoch: [288]  [ 60/160]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.4005 (1.5420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3453 (7.0210)  time: 0.7779 (0.5308 -- 3.3537)  data: 0.0065 (0.0002 -- 0.1004)  max mem: 16413
Epoch: [288]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5451 (1.5415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4168 (7.1262)  time: 0.8982 (0.5204 -- 3.8685)  data: 0.3262 (0.0006 -- 3.3437)  max mem: 16413
[2023-09-05 09:38:18,350] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:38:18,350] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:38:18,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:38:18,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [288]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.5377 (1.5466)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5214 (7.0955)  time: 0.8085 (0.5211 -- 4.2786)  data: 0.2531 (0.0009 -- 3.7548)  max mem: 16413
Epoch: [288]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6840 (1.5597)  loss_scale: 32768.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4864 (7.1965)  time: 0.8912 (0.5428 -- 2.8636)  data: 0.2171 (0.0006 -- 1.6791)  max mem: 16413
Epoch: [288]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.4048 (1.5423)  loss_scale: 32768.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0180 (7.1494)  time: 0.8352 (0.5203 -- 3.2677)  data: 0.0980 (0.0004 -- 1.2730)  max mem: 16413
[2023-09-05 09:39:09,209] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46232
[2023-09-05 09:39:09,209] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46232
[2023-09-05 09:39:09,209] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:39:09,209] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:39:09,210] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [288]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6119 (1.5388)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4801 (7.2114)  time: 0.7551 (0.4932 -- 4.0972)  data: 0.0194 (0.0002 -- 0.3526)  max mem: 16413
Epoch: [288] Total time: 0:02:24 (0.9027 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6119 (1.5450)  loss_scale: 32768.0000 (22425.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4801 (7.2114)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1306 (0.1306)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4842 (2.4842 -- 2.4842)  data: 2.2303 (2.2303 -- 2.2303)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5654 (0.5045)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4229 (0.2040 -- 2.4842)  data: 0.2040 (0.0010 -- 2.2303)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4681 (0.5042)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2145 (0.1692 -- 0.2934)  data: 0.0077 (0.0001 -- 0.1010)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4681 (0.5485)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.1978 (0.1329 -- 0.2934)  data: 0.0073 (0.0001 -- 0.1010)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [289]  [  0/160]  eta: 0:16:25  lr: 0.000000  min_lr: 0.000000  loss: 1.7224 (1.7224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6836 (5.6836)  time: 6.1564 (6.1564 -- 6.1564)  data: 5.6259 (5.6259 -- 5.6259)  max mem: 16413
Epoch: [289]  [ 20/160]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 1.4744 (1.5376)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6949 (7.4463)  time: 0.9146 (0.5218 -- 2.4575)  data: 0.3318 (0.0007 -- 1.8565)  max mem: 16413
Epoch: [289]  [ 40/160]  eta: 0:01:56  lr: 0.000000  min_lr: 0.000000  loss: 1.6059 (1.5684)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6255 (7.7004)  time: 0.7736 (0.5219 -- 3.1040)  data: 0.0623 (0.0003 -- 1.0909)  max mem: 16413
Epoch: [289]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.4970 (1.5780)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8420 (7.5015)  time: 1.0164 (0.5248 -- 4.0527)  data: 0.0623 (0.0004 -- 1.2152)  max mem: 16413
Epoch: [289]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5285 (1.5508)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8822 (7.3362)  time: 0.9402 (0.5079 -- 2.7272)  data: 0.1051 (0.0004 -- 2.0759)  max mem: 16413
Epoch: [289]  [100/160]  eta: 0:00:58  lr: 0.000000  min_lr: 0.000000  loss: 1.6251 (1.5582)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2831 (7.3015)  time: 0.9392 (0.5169 -- 3.5521)  data: 0.3439 (0.0004 -- 3.0342)  max mem: 16413
Epoch: [289]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5888 (1.5566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6589 (7.2008)  time: 0.8064 (0.5045 -- 5.6506)  data: 0.2657 (0.0002 -- 5.1505)  max mem: 16413
[2023-09-05 09:41:15,137] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:41:15,137] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:41:15,137] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:41:15,137] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:41:18,331] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46366
[2023-09-05 09:41:18,332] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46366
[2023-09-05 09:41:18,332] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:41:18,332] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:41:18,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [289]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5417 (1.5579)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7056 (7.0533)  time: 0.8210 (0.5308 -- 2.7336)  data: 0.2729 (0.0004 -- 2.2157)  max mem: 16413
Epoch: [289]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4623 (1.5518)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3485 (7.0130)  time: 0.6961 (0.4964 -- 1.8974)  data: 0.1609 (0.0002 -- 1.3326)  max mem: 16413
Epoch: [289] Total time: 0:02:23 (0.8987 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4623 (1.5401)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3485 (7.0130)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1304 (0.1304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3640 (2.3640 -- 2.3640)  data: 2.1038 (2.1038 -- 2.1038)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5645 (0.5040)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4180 (0.1951 -- 2.3640)  data: 0.1972 (0.0007 -- 2.1038)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4680 (0.5040)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2251 (0.1703 -- 0.4104)  data: 0.0191 (0.0001 -- 0.1994)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4680 (0.5485)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (98.3402)  time: 0.2095 (0.1325 -- 0.4104)  data: 0.0186 (0.0001 -- 0.1994)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 85.270 Acc@5 98.133 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [290]  [  0/160]  eta: 0:20:56  lr: 0.000000  min_lr: 0.000000  loss: 1.2685 (1.2685)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5595 (6.5595)  time: 7.8519 (7.8519 -- 7.8519)  data: 7.3120 (7.3120 -- 7.3120)  max mem: 16413
Epoch: [290]  [ 20/160]  eta: 0:02:44  lr: 0.000000  min_lr: 0.000000  loss: 1.5946 (1.5097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9687 (7.2581)  time: 0.8391 (0.5368 -- 4.0510)  data: 0.2856 (0.0003 -- 3.5309)  max mem: 16413
Epoch: [290]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.6038 (1.5234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3502 (7.0753)  time: 0.8819 (0.5249 -- 2.2544)  data: 0.2262 (0.0003 -- 1.7170)  max mem: 16413
Epoch: [290]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6967 (1.5743)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2847 (7.7121)  time: 0.8854 (0.5263 -- 3.8035)  data: 0.0147 (0.0004 -- 0.2659)  max mem: 16413
Epoch: [290]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.5086 (1.5594)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6792 (7.7338)  time: 0.9415 (0.5389 -- 4.0621)  data: 0.3572 (0.0002 -- 3.5125)  max mem: 16413
[2023-09-05 09:43:21,488] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:43:21,488] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:43:21,488] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:43:21,488] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [290]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.4712 (1.5581)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1530 (7.5087)  time: 0.8361 (0.5283 -- 2.5364)  data: 0.2784 (0.0003 -- 2.0157)  max mem: 16413
[2023-09-05 09:43:40,150] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46515
[2023-09-05 09:43:40,150] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46515
[2023-09-05 09:43:40,150] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:43:40,150] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:43:40,150] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [290]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5076 (1.5468)  loss_scale: 32768.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4065 (7.4646)  time: 0.8411 (0.5256 -- 2.8500)  data: 0.2930 (0.0002 -- 2.3202)  max mem: 16413
Epoch: [290]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6111 (1.5601)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4329 (7.3997)  time: 0.9267 (0.5208 -- 3.5650)  data: 0.3651 (0.0002 -- 3.0223)  max mem: 16413
Epoch: [290]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.2335 (1.5287)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1774 (7.3561)  time: 0.7102 (0.4970 -- 2.9719)  data: 0.1882 (0.0001 -- 2.4474)  max mem: 16413
Epoch: [290] Total time: 0:02:24 (0.9035 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.2335 (1.5397)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1774 (7.3561)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1308 (0.1308)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5272 (2.5272 -- 2.5272)  data: 2.2440 (2.2440 -- 2.2440)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5646 (0.5040)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4346 (0.2086 -- 2.5272)  data: 0.2050 (0.0005 -- 2.2440)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4682 (0.5035)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2189 (0.1702 -- 0.3317)  data: 0.0079 (0.0001 -- 0.1433)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4682 (0.5480)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.1997 (0.1326 -- 0.3317)  data: 0.0076 (0.0001 -- 0.1433)  max mem: 16413
Val: Total time: 0:00:07 (0.2926 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.569
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [291]  [  0/160]  eta: 0:21:26  lr: 0.000000  min_lr: 0.000000  loss: 1.6081 (1.6081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7441 (4.7441)  time: 8.0398 (8.0398 -- 8.0398)  data: 7.4358 (7.4358 -- 7.4358)  max mem: 16413
Epoch: [291]  [ 20/160]  eta: 0:02:46  lr: 0.000000  min_lr: 0.000000  loss: 1.5778 (1.5495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0577 (7.7489)  time: 0.8467 (0.5193 -- 3.4221)  data: 0.2832 (0.0005 -- 2.8601)  max mem: 16413
Epoch: [291]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.4667 (1.5464)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2165 (7.4960)  time: 0.8657 (0.5314 -- 3.3544)  data: 0.0611 (0.0001 -- 1.0180)  max mem: 16413
Epoch: [291]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.3509 (1.5025)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0327 (7.4144)  time: 0.8837 (0.5271 -- 3.1509)  data: 0.0574 (0.0003 -- 0.9545)  max mem: 16413
Epoch: [291]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.5701 (1.5113)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0003 (7.6195)  time: 0.8580 (0.5164 -- 3.7247)  data: 0.2618 (0.0004 -- 3.1814)  max mem: 16413
[2023-09-05 09:45:46,920] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:45:46,920] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:45:46,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:45:46,924] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:45:54,495] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46654
[2023-09-05 09:45:54,496] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:45:54,496] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46654
[2023-09-05 09:45:54,496] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:45:54,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [291]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.5981 (1.5157)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5621 (7.4430)  time: 0.8871 (0.5176 -- 2.2289)  data: 0.2346 (0.0003 -- 1.7188)  max mem: 16413
Epoch: [291]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6916 (1.5519)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9267 (7.4927)  time: 0.7649 (0.5341 -- 2.4467)  data: 0.2068 (0.0006 -- 1.9096)  max mem: 16413
Epoch: [291]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.3113 (1.5395)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2941 (7.4311)  time: 0.9690 (0.5233 -- 3.4400)  data: 0.3117 (0.0005 -- 2.8913)  max mem: 16413
Epoch: [291]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3857 (1.5261)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6135 (7.4792)  time: 0.7501 (0.4953 -- 2.1418)  data: 0.0576 (0.0001 -- 1.1381)  max mem: 16413
Epoch: [291] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3857 (1.5494)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6135 (7.4792)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1304 (0.1304)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4447 (2.4447 -- 2.4447)  data: 2.2091 (2.2091 -- 2.2091)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5660 (0.5047)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4144 (0.1987 -- 2.4447)  data: 0.2033 (0.0008 -- 2.2091)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4674 (0.5039)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2155 (0.1689 -- 0.3985)  data: 0.0107 (0.0001 -- 0.1826)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4674 (0.5486)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2012 (0.1327 -- 0.3985)  data: 0.0104 (0.0001 -- 0.1826)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [292]  [  0/160]  eta: 0:21:08  lr: 0.000000  min_lr: 0.000000  loss: 1.6551 (1.6551)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7638 (6.7638)  time: 7.9264 (7.9264 -- 7.9264)  data: 4.5056 (4.5056 -- 4.5056)  max mem: 16413
Epoch: [292]  [ 20/160]  eta: 0:02:45  lr: 0.000000  min_lr: 0.000000  loss: 1.4351 (1.4430)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0958 (6.7005)  time: 0.8479 (0.5263 -- 3.9805)  data: 0.0018 (0.0004 -- 0.0119)  max mem: 16413
Epoch: [292]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.3863 (1.4523)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3783 (6.7445)  time: 0.8519 (0.5254 -- 3.1254)  data: 0.0018 (0.0003 -- 0.0038)  max mem: 16413
[2023-09-05 09:47:38,279] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46761
[2023-09-05 09:47:38,279] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 46761
[2023-09-05 09:47:38,279] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:47:38,279] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 09:47:38,280] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [292]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6049 (1.5268)  loss_scale: 8192.0000 (13698.0984)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7824 (7.2094)  time: 0.8878 (0.5217 -- 4.6372)  data: 0.2473 (0.0002 -- 4.0821)  max mem: 16413
Epoch: [292]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6185 (1.5487)  loss_scale: 8192.0000 (12338.5679)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5453 (7.1592)  time: 0.9786 (0.5211 -- 4.6910)  data: 0.3747 (0.0001 -- 4.1612)  max mem: 16413
Epoch: [292]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.5149 (1.5522)  loss_scale: 8192.0000 (11517.4653)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1205 (7.3757)  time: 0.8393 (0.5188 -- 4.2012)  data: 0.2911 (0.0002 -- 3.6813)  max mem: 16413
Epoch: [292]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6184 (1.5707)  loss_scale: 8192.0000 (10967.8017)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6995 (7.4181)  time: 0.8197 (0.5338 -- 2.9455)  data: 0.2671 (0.0003 -- 2.3897)  max mem: 16413
Epoch: [292]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6623 (1.5702)  loss_scale: 8192.0000 (10574.0709)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7612 (7.4353)  time: 0.9135 (0.5302 -- 4.0061)  data: 0.3442 (0.0002 -- 3.4505)  max mem: 16413
Epoch: [292]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4704 (1.5556)  loss_scale: 8192.0000 (10291.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6285 (7.3416)  time: 0.6700 (0.4949 -- 3.0504)  data: 0.1554 (0.0002 -- 2.5220)  max mem: 16413
Epoch: [292] Total time: 0:02:23 (0.8975 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4704 (1.5590)  loss_scale: 8192.0000 (10291.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6285 (7.3416)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1305 (0.1305)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4213 (2.4213 -- 2.4213)  data: 2.1764 (2.1764 -- 2.1764)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5649 (0.5049)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4264 (0.2006 -- 2.4213)  data: 0.2062 (0.0005 -- 2.1764)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4675 (0.5039)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2197 (0.1694 -- 0.3079)  data: 0.0127 (0.0001 -- 0.0967)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4675 (0.5488)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2032 (0.1327 -- 0.3079)  data: 0.0125 (0.0001 -- 0.0967)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [293]  [  0/160]  eta: 0:19:15  lr: 0.000000  min_lr: 0.000000  loss: 2.0898 (2.0898)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8374 (7.8374)  time: 7.2193 (7.2193 -- 7.2193)  data: 6.6583 (6.6583 -- 6.6583)  max mem: 16413
[2023-09-05 09:49:42,319] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:49:42,319] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 09:49:42,320] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:49:42,320] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [293]  [ 20/160]  eta: 0:02:39  lr: 0.000000  min_lr: 0.000000  loss: 1.4523 (1.5487)  loss_scale: 16384.0000 (12483.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7887 (6.4806)  time: 0.8363 (0.5248 -- 2.7107)  data: 0.2905 (0.0005 -- 2.1858)  max mem: 16413
Epoch: [293]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.4290 (1.4967)  loss_scale: 16384.0000 (14385.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9162 (7.1356)  time: 0.9175 (0.5361 -- 2.6562)  data: 0.2806 (0.0004 -- 1.9357)  max mem: 16413
Epoch: [293]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.8230 (1.5818)  loss_scale: 16384.0000 (15041.0492)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1043 (7.0232)  time: 0.9069 (0.5267 -- 1.8404)  data: 0.2000 (0.0005 -- 1.2446)  max mem: 16413
Epoch: [293]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.5884 (1.5899)  loss_scale: 16384.0000 (15372.6420)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6429 (7.1735)  time: 0.8717 (0.5171 -- 2.5210)  data: 0.0332 (0.0002 -- 0.6418)  max mem: 16413
Epoch: [293]  [100/160]  eta: 0:00:58  lr: 0.000000  min_lr: 0.000000  loss: 1.3987 (1.5656)  loss_scale: 16384.0000 (15572.9109)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8865 (7.1254)  time: 0.9950 (0.5080 -- 4.1235)  data: 0.0018 (0.0002 -- 0.0116)  max mem: 16413
[2023-09-05 09:51:19,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=47000, skipped=267, lr=[2.7676876134059213e-08, 2.7676876134059213e-08, 3.075208459339913e-08, 3.075208459339913e-08, 3.416898288155458e-08, 3.416898288155458e-08, 3.796553653506065e-08, 3.796553653506065e-08, 4.218392948340072e-08, 4.218392948340072e-08, 4.6871032759334134e-08, 4.6871032759334134e-08, 5.2078925288149033e-08, 5.2078925288149033e-08, 5.786547254238781e-08, 5.786547254238781e-08, 6.429496949154201e-08, 6.429496949154201e-08, 7.143885499060223e-08, 7.143885499060223e-08, 7.93765055451136e-08, 7.93765055451136e-08, 8.819611727234844e-08, 8.819611727234844e-08, 9.799568585816493e-08, 9.799568585816493e-08, 1.0888409539796102e-07, 1.0888409539796102e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-09-05 09:51:19,961] [INFO] [timer.py:260:stop] epoch=0/micro_step=47000/global_step=47000, RunningAvgSamplesPerSec=17.754485988655503, CurrSamplesPerSec=22.894974506122942, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [293]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6574 (1.5678)  loss_scale: 16384.0000 (15706.9752)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6116 (7.2058)  time: 0.7974 (0.5163 -- 4.5733)  data: 0.0557 (0.0004 -- 1.0844)  max mem: 16413
[2023-09-05 09:51:35,845] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:51:35,846] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:51:35,846] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:51:35,846] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [293]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7314 (1.5879)  loss_scale: 16384.0000 (16151.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2209 (7.2252)  time: 0.8313 (0.5325 -- 2.6956)  data: 0.0267 (0.0004 -- 0.2228)  max mem: 16413
[2023-09-05 09:51:49,378] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47036
[2023-09-05 09:51:49,378] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47036
[2023-09-05 09:51:49,378] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:51:49,378] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:51:49,378] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [293]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5830 (1.5892)  loss_scale: 32768.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1095 (7.3057)  time: 0.7136 (0.4804 -- 2.4248)  data: 0.1274 (0.0002 -- 1.0539)  max mem: 16413
Epoch: [293] Total time: 0:02:24 (0.9007 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5830 (1.5500)  loss_scale: 32768.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1095 (7.3057)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1306 (0.1306)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5231 (2.5231 -- 2.5231)  data: 2.2994 (2.2994 -- 2.2994)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5634 (0.5048)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4360 (0.2131 -- 2.5231)  data: 0.2116 (0.0003 -- 2.2994)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4681 (0.5041)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2214 (0.1693 -- 0.3988)  data: 0.0118 (0.0001 -- 0.2054)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4681 (0.5490)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2022 (0.1326 -- 0.3988)  data: 0.0116 (0.0001 -- 0.2054)  max mem: 16413
Val: Total time: 0:00:07 (0.2938 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [294]  [  0/160]  eta: 0:16:39  lr: 0.000000  min_lr: 0.000000  loss: 1.9163 (1.9163)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5572 (5.5572)  time: 6.2479 (6.2479 -- 6.2479)  data: 5.6777 (5.6777 -- 5.6777)  max mem: 16413
Epoch: [294]  [ 20/160]  eta: 0:02:39  lr: 0.000000  min_lr: 0.000000  loss: 1.4188 (1.4867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0460 (6.5739)  time: 0.8829 (0.5212 -- 3.0630)  data: 0.0628 (0.0007 -- 0.7372)  max mem: 16413
Epoch: [294]  [ 40/160]  eta: 0:02:06  lr: 0.000000  min_lr: 0.000000  loss: 1.7166 (1.5270)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7235 (7.0340)  time: 0.9655 (0.5270 -- 3.6714)  data: 0.0394 (0.0004 -- 0.7559)  max mem: 16413
Epoch: [294]  [ 60/160]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 1.5770 (1.5469)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6717 (7.0699)  time: 0.7649 (0.5376 -- 2.7559)  data: 0.1606 (0.0002 -- 2.2187)  max mem: 16413
Epoch: [294]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.6459 (1.5666)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0565 (7.2267)  time: 0.9340 (0.5294 -- 3.4376)  data: 0.3447 (0.0004 -- 2.8990)  max mem: 16413
Epoch: [294]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.4899 (1.5623)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6009 (7.2948)  time: 0.9372 (0.5214 -- 4.7527)  data: 0.3962 (0.0003 -- 4.2503)  max mem: 16413
Epoch: [294]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6338 (1.5693)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2189 (7.4606)  time: 0.7841 (0.5229 -- 3.5908)  data: 0.2314 (0.0002 -- 3.0690)  max mem: 16413
[2023-09-05 09:53:56,017] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:53:56,017] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:53:56,018] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:53:56,018] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [294]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5201 (1.5701)  loss_scale: 32768.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4663 (7.4094)  time: 0.9872 (0.5030 -- 4.6737)  data: 0.4366 (0.0004 -- 4.1466)  max mem: 16413
[2023-09-05 09:54:13,454] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47186
[2023-09-05 09:54:13,454] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:54:13,454] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47186
[2023-09-05 09:54:13,454] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:54:13,455] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [294]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.3530 (1.5431)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0898 (7.3814)  time: 0.7242 (0.4944 -- 2.3252)  data: 0.2103 (0.0002 -- 1.7989)  max mem: 16413
Epoch: [294] Total time: 0:02:23 (0.8972 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.3530 (1.5566)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0898 (7.3814)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1306 (0.1306)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3722 (2.3722 -- 2.3722)  data: 2.1505 (2.1505 -- 2.1505)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5641 (0.5047)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4326 (0.2033 -- 2.3722)  data: 0.2099 (0.0008 -- 2.1505)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4677 (0.5038)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2242 (0.1706 -- 0.3800)  data: 0.0162 (0.0001 -- 0.1635)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4677 (0.5488)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2055 (0.1323 -- 0.3800)  data: 0.0155 (0.0001 -- 0.1635)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [295]  [  0/160]  eta: 0:18:03  lr: 0.000000  min_lr: 0.000000  loss: 1.4924 (1.4924)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7152 (5.7152)  time: 6.7696 (6.7696 -- 6.7696)  data: 6.2383 (6.2383 -- 6.2383)  max mem: 16413
Epoch: [295]  [ 20/160]  eta: 0:02:38  lr: 0.000000  min_lr: 0.000000  loss: 1.7062 (1.6441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3699 (7.4896)  time: 0.8467 (0.5257 -- 1.8083)  data: 0.2535 (0.0006 -- 1.2652)  max mem: 16413
Epoch: [295]  [ 40/160]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5970 (1.6547)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0529 (6.9999)  time: 0.8741 (0.5248 -- 2.6883)  data: 0.3168 (0.0003 -- 2.1533)  max mem: 16413
Epoch: [295]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6958 (1.6164)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9656 (6.8760)  time: 0.9280 (0.5330 -- 1.9730)  data: 0.3419 (0.0004 -- 1.4070)  max mem: 16413
Epoch: [295]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7801 (1.6380)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2528 (7.2245)  time: 0.8450 (0.5266 -- 3.6708)  data: 0.3000 (0.0001 -- 3.0991)  max mem: 16413
Epoch: [295]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.5082 (1.6146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0821 (7.2133)  time: 1.0113 (0.5236 -- 3.6721)  data: 0.3732 (0.0002 -- 3.1415)  max mem: 16413
[2023-09-05 09:56:19,536] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:56:19,536] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:56:19,537] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:56:19,538] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [295]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5129 (1.5933)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2682 (7.1756)  time: 0.7620 (0.5229 -- 2.8535)  data: 0.2107 (0.0003 -- 2.3312)  max mem: 16413
Epoch: [295]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5275 (1.5981)  loss_scale: 32768.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3600 (7.2141)  time: 0.9432 (0.5151 -- 4.5159)  data: 0.4000 (0.0005 -- 4.0013)  max mem: 16413
[2023-09-05 09:56:54,286] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47358
[2023-09-05 09:56:54,286] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:56:54,286] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47358
[2023-09-05 09:56:54,286] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-09-05 09:56:54,286] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [295]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6735 (1.6023)  loss_scale: 32768.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5398 (7.1586)  time: 0.7059 (0.4809 -- 3.9812)  data: 0.1933 (0.0002 -- 3.4675)  max mem: 16413
Epoch: [295] Total time: 0:02:24 (0.9036 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6735 (1.5534)  loss_scale: 32768.0000 (20787.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5398 (7.1586)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1307 (0.1307)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6285 (2.6285 -- 2.6285)  data: 2.3410 (2.3410 -- 2.3410)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5637 (0.5046)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4513 (0.1955 -- 2.6285)  data: 0.2302 (0.0005 -- 2.3410)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4679 (0.5036)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2156 (0.1691 -- 0.3988)  data: 0.0101 (0.0001 -- 0.1823)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4679 (0.5487)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.1993 (0.1355 -- 0.3988)  data: 0.0098 (0.0001 -- 0.1823)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [296]  [  0/160]  eta: 0:19:05  lr: 0.000000  min_lr: 0.000000  loss: 1.3207 (1.3207)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6464 (6.6464)  time: 7.1601 (7.1601 -- 7.1601)  data: 6.4429 (6.4429 -- 6.4429)  max mem: 16413
Epoch: [296]  [ 20/160]  eta: 0:02:47  lr: 0.000000  min_lr: 0.000000  loss: 1.4719 (1.5440)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6592 (7.3059)  time: 0.9017 (0.5274 -- 4.0709)  data: 0.3382 (0.0005 -- 3.5188)  max mem: 16413
Epoch: [296]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.6001 (1.5450)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0140 (7.7988)  time: 0.8646 (0.5339 -- 3.8187)  data: 0.3013 (0.0006 -- 3.2804)  max mem: 16413
Epoch: [296]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5309 (1.5302)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0219 (7.9121)  time: 0.8529 (0.5243 -- 2.8099)  data: 0.3057 (0.0003 -- 2.2726)  max mem: 16413
Epoch: [296]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.5678 (1.5366)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7039 (7.7928)  time: 0.9433 (0.5332 -- 3.0668)  data: 0.3305 (0.0004 -- 1.8383)  max mem: 16413
Epoch: [296]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.3254 (1.5126)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6800 (7.7229)  time: 0.8334 (0.5207 -- 2.9917)  data: 0.2081 (0.0002 -- 1.8577)  max mem: 16413
Epoch: [296]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6631 (1.5425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1331 (7.6419)  time: 0.8607 (0.5248 -- 2.4965)  data: 0.1627 (0.0004 -- 1.9683)  max mem: 16413
[2023-09-05 09:59:02,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:59:02,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 09:59:02,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 09:59:02,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [296]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.3924 (1.5316)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2862 (7.5663)  time: 0.9772 (0.5225 -- 4.4429)  data: 0.4328 (0.0003 -- 3.9279)  max mem: 16413
[2023-09-05 09:59:25,065] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47517
[2023-09-05 09:59:25,065] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:59:25,065] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47517
[2023-09-05 09:59:25,065] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 09:59:25,065] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [296]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4951 (1.5240)  loss_scale: 32768.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1174 (7.4747)  time: 0.7848 (0.4957 -- 4.2083)  data: 0.2651 (0.0002 -- 3.6766)  max mem: 16413
Epoch: [296] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4951 (1.5470)  loss_scale: 32768.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1174 (7.4747)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1307 (0.1307)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4101 (2.4101 -- 2.4101)  data: 2.1850 (2.1850 -- 2.1850)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5636 (0.5045)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4352 (0.1978 -- 2.4101)  data: 0.2142 (0.0008 -- 2.1850)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4682 (0.5035)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2275 (0.1683 -- 0.4104)  data: 0.0203 (0.0001 -- 0.2312)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4682 (0.5485)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2126 (0.1328 -- 0.4104)  data: 0.0198 (0.0001 -- 0.2312)  max mem: 16413
Val: Total time: 0:00:07 (0.2944 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [297]  [  0/160]  eta: 0:21:39  lr: 0.000000  min_lr: 0.000000  loss: 1.6383 (1.6383)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5567 (4.5567)  time: 8.1217 (8.1217 -- 8.1217)  data: 7.4613 (7.4613 -- 7.4613)  max mem: 16413
Epoch: [297]  [ 20/160]  eta: 0:02:51  lr: 0.000000  min_lr: 0.000000  loss: 1.6619 (1.7198)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7091 (7.2828)  time: 0.8819 (0.5284 -- 2.8853)  data: 0.3308 (0.0004 -- 2.3485)  max mem: 16413
Epoch: [297]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 1.5750 (1.6415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8987 (7.2743)  time: 0.9036 (0.5207 -- 3.7519)  data: 0.3367 (0.0004 -- 3.1815)  max mem: 16413
Epoch: [297]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.5416 (1.5999)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7934 (7.0841)  time: 0.8313 (0.5248 -- 2.5570)  data: 0.2805 (0.0003 -- 2.0308)  max mem: 16413
Epoch: [297]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.5706 (1.6028)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2398 (7.0352)  time: 0.8457 (0.5156 -- 2.5114)  data: 0.2994 (0.0003 -- 1.9656)  max mem: 16413
Epoch: [297]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.4362 (1.5774)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0566 (6.9844)  time: 0.8941 (0.5277 -- 2.4408)  data: 0.3440 (0.0005 -- 1.9030)  max mem: 16413
Epoch: [297]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7104 (1.5804)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8257 (7.1981)  time: 0.8653 (0.5245 -- 4.9468)  data: 0.3169 (0.0002 -- 4.4022)  max mem: 16413
[2023-09-05 10:01:33,486] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 10:01:33,486] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 10:01:33,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-09-05 10:01:33,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [297]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5293 (1.5849)  loss_scale: 32768.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0171 (7.1842)  time: 0.8880 (0.5378 -- 4.1235)  data: 0.3373 (0.0006 -- 3.6022)  max mem: 16413
[2023-09-05 10:01:57,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47678
[2023-09-05 10:01:57,166] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47678
[2023-09-05 10:01:57,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 10:01:57,166] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-09-05 10:01:57,166] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [297]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5571 (1.5840)  loss_scale: 32768.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0419 (7.2024)  time: 0.7705 (0.4883 -- 3.0749)  data: 0.2086 (0.0002 -- 2.5610)  max mem: 16413
Epoch: [297] Total time: 0:02:23 (0.8978 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5571 (1.5789)  loss_scale: 32768.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0419 (7.2024)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1307 (0.1307)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3841 (2.3841 -- 2.3841)  data: 2.1455 (2.1455 -- 2.1455)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5646 (0.5049)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4213 (0.1892 -- 2.3841)  data: 0.2113 (0.0006 -- 2.1455)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4683 (0.5035)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2239 (0.1684 -- 0.4164)  data: 0.0211 (0.0001 -- 0.2050)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4683 (0.5486)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2117 (0.1330 -- 0.4164)  data: 0.0209 (0.0001 -- 0.2050)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Epoch: [298]  [  0/160]  eta: 0:21:06  lr: 0.000000  min_lr: 0.000000  loss: 1.5488 (1.5488)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.2945 (13.2945)  time: 7.9152 (7.9152 -- 7.9152)  data: 5.5789 (5.5789 -- 5.5789)  max mem: 16413
Epoch: [298]  [ 20/160]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 1.5556 (1.6085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7864 (7.3056)  time: 0.8309 (0.5259 -- 2.5208)  data: 0.1596 (0.0002 -- 1.7650)  max mem: 16413
[2023-09-05 10:02:36,090] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47706
[2023-09-05 10:02:36,091] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 47706
[2023-09-05 10:02:36,091] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 10:02:36,091] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-09-05 10:02:36,091] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [298]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.6004 (1.6279)  loss_scale: 8192.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7140 (7.4229)  time: 0.8907 (0.5214 -- 2.9389)  data: 0.1847 (0.0005 -- 1.5148)  max mem: 16413
Epoch: [298]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.4162 (1.5993)  loss_scale: 8192.0000 (11683.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4120 (7.4347)  time: 0.8552 (0.5204 -- 3.7418)  data: 0.0208 (0.0004 -- 0.3897)  max mem: 16413
Epoch: [298]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7523 (1.6394)  loss_scale: 8192.0000 (10821.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0603 (7.4131)  time: 0.8493 (0.5146 -- 5.2969)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [298]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.5892 (1.6231)  loss_scale: 8192.0000 (10300.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0589 (7.3623)  time: 0.9987 (0.5163 -- 5.8319)  data: 0.0016 (0.0003 -- 0.0067)  max mem: 16413
Epoch: [298]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7272 (1.6427)  loss_scale: 8192.0000 (9952.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4035 (7.4214)  time: 0.7756 (0.5249 -- 2.1490)  data: 0.0021 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [298]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5267 (1.6255)  loss_scale: 8192.0000 (9702.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6578 (7.5766)  time: 0.9720 (0.5305 -- 4.1178)  data: 0.0022 (0.0003 -- 0.0143)  max mem: 16413
[2023-09-05 10:04:27,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 10:04:27,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-09-05 10:04:27,770] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-09-05 10:04:27,770] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [298]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6204 (1.6226)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3476 (7.4906)  time: 0.7875 (0.4940 -- 2.9926)  data: 0.0008 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [298] Total time: 0:02:24 (0.9010 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6204 (1.5929)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3476 (7.4906)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1307 (0.1307)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3917 (2.3917 -- 2.3917)  data: 2.1854 (2.1854 -- 2.1854)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5646 (0.5047)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 0.4214 (0.2023 -- 2.3917)  data: 0.2098 (0.0008 -- 2.1854)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4679 (0.5035)  acc1: 88.8889 (87.3016)  acc5: 100.0000 (98.4127)  time: 0.2230 (0.1713 -- 0.4079)  data: 0.0191 (0.0001 -- 0.2033)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4679 (0.5487)  acc1: 88.8889 (85.8921)  acc5: 100.0000 (97.9253)  time: 0.2092 (0.1324 -- 0.4079)  data: 0.0188 (0.0001 -- 0.2033)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.570
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 86.51%
Traceback (most recent call last):
  File "run_class_finetuning.py", line 927, in <module>
    main(opts, ds_init)
  File "run_class_finetuning.py", line 894, in main
    with open(
FileNotFoundError: [Errno 2] No such file or directory: '/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/log.txt'
configs/vit_b_k710.sh: line 43: 3406935 Killed                  OMP_NUM_THREADS=1 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node=${GPUS_PER_NODE} --master_port 12320 --nnodes=1 --node_rank=0 --master_addr=localhost run_class_finetuning.py --model vit_base_patch16_224 --data_set AI-City-Track-3 --nb_classes 16 --data_path ${DATA_PATH} --finetune ${MODEL_PATH} --log_dir ${OUTPUT_DIR} --output_dir ${OUTPUT_DIR} --batch_size 6 --input_size 224 --short_side_size 224 --save_ckpt_freq 20 --num_frames 16 --sampling_rate 4 --num_sample 2 --num_workers 8 --opt adamw --lr 1e-3 --drop_path 0.1 --head_drop_rate 0.0 --layer_decay 0.9 --opt_betas 0.9 0.999 --warmup_epochs 30 --epochs 300 --test_num_segment 5 --test_num_crop 3 --dist_eval --enable_deepspeed
