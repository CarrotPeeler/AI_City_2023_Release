[2023-08-31 11:02:48,423] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-31 11:02:48,458] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.001, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f5218354970>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00004688
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-08-31 11:02:53,786] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-31 11:02:53,786] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-31 11:02:53,823] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-31 11:02:53,823] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-31 11:02:53,928] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.38598012924194336 seconds
[2023-08-31 11:02:54,867] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-08-31 11:02:54,876] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-08-31 11:02:54,876] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-08-31 11:02:54,901] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-08-31 11:02:54,901] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-31 11:02:54,902] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-31 11:02:54,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 11:02:54,902] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-31 11:02:54,903] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-31 11:02:54,903] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-31 11:02:54,903] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-31 11:02:54,903] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f51cbd93400>
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-31 11:02:54,904] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-31 11:02:54,905] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.001, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-31 11:02:54,906] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-31 11:02:54,907] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.001, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:31:49  lr: 0.000000  min_lr: 0.000000  loss: 2.7734 (2.7734)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 11.9321 (11.9321 -- 11.9321)  data: 6.6000 (6.6000 -- 6.6000)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:44  lr: 0.000001  min_lr: 0.000000  loss: 2.7730 (2.7729)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5932 (1.6020)  time: 0.6362 (0.5005 -- 2.4496)  data: 0.0015 (0.0005 -- 0.0055)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:16  lr: 0.000002  min_lr: 0.000000  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4588 (1.5416)  time: 1.0946 (0.5081 -- 4.6190)  data: 0.0032 (0.0003 -- 0.0136)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:43  lr: 0.000004  min_lr: 0.000000  loss: 2.7724 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4178 (1.5246)  time: 0.8172 (0.5030 -- 4.1406)  data: 0.0014 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:22  lr: 0.000005  min_lr: 0.000000  loss: 2.7722 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4509 (1.5167)  time: 1.0070 (0.5046 -- 5.7658)  data: 0.0017 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:57  lr: 0.000006  min_lr: 0.000000  loss: 2.7720 (2.7724)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4788 (1.5105)  time: 0.6586 (0.5068 -- 2.7781)  data: 0.0023 (0.0002 -- 0.0076)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:38  lr: 0.000007  min_lr: 0.000000  loss: 2.7713 (2.7722)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4989 (1.5117)  time: 0.9920 (0.5094 -- 3.9085)  data: 0.0022 (0.0004 -- 0.0166)  max mem: 16413
[2023-08-31 11:04:56,156] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:04:56,156] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-08-31 11:04:56,158] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:04:56,158] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 2.7708 (2.7720)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4506 (1.5027)  time: 0.7486 (0.5111 -- 3.7460)  data: 0.0014 (0.0001 -- 0.0023)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.7698 (2.7718)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3672 (1.4909)  time: 0.7128 (0.4849 -- 3.1009)  data: 0.0008 (0.0001 -- 0.0023)  max mem: 16413
Epoch: [0] Total time: 0:02:24 (0.9047 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.7698 (2.7718)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3672 (1.4909)
Val:  [ 0/27]  eta: 0:01:15  loss: 2.7648 (2.7648)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.8024 (2.8024 -- 2.8024)  data: 2.4335 (2.4335 -- 2.4335)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7650 (2.7649)  acc1: 33.3333 (33.3333)  acc5: 77.7778 (80.8081)  time: 0.4577 (0.2070 -- 2.8024)  data: 0.2230 (0.0003 -- 2.4335)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7645 (2.7645)  acc1: 33.3333 (34.9206)  acc5: 77.7778 (82.5397)  time: 0.2163 (0.1693 -- 0.2768)  data: 0.0074 (0.0001 -- 0.0898)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7645 (2.7648)  acc1: 33.3333 (34.8548)  acc5: 85.7143 (80.9129)  time: 0.2011 (0.1663 -- 0.2768)  data: 0.0071 (0.0001 -- 0.0898)  max mem: 16413
Val: Total time: 0:00:08 (0.3021 s / it)
* Acc@1 33.195 Acc@5 81.950 loss 2.765
Accuracy of the network on the 482 val images: 33.20%
[2023-08-31 11:05:27,960] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-31 11:05:27,964] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:05:27,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:05:27,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:05:28,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:05:28,851] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 33.20%
Epoch: [1]  [  0/160]  eta: 0:20:59  lr: 0.000009  min_lr: 0.000000  loss: 2.7693 (2.7693)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6968 (1.6968)  time: 7.8716 (7.8716 -- 7.8716)  data: 7.3520 (7.3520 -- 7.3520)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:37  lr: 0.000011  min_lr: 0.000000  loss: 2.7683 (2.7679)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4957 (1.5208)  time: 0.7861 (0.5138 -- 2.9227)  data: 0.2404 (0.0004 -- 2.4173)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:02  lr: 0.000012  min_lr: 0.000000  loss: 2.7669 (2.7674)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5768 (1.5341)  time: 0.9213 (0.5058 -- 2.8041)  data: 0.2244 (0.0002 -- 2.2860)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:36  lr: 0.000013  min_lr: 0.000000  loss: 2.7617 (2.7656)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5483 (1.5419)  time: 0.8499 (0.5236 -- 2.5355)  data: 0.1956 (0.0005 -- 1.9644)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000000  loss: 2.7576 (2.7637)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5439 (1.5305)  time: 0.9319 (0.5246 -- 3.2068)  data: 0.1993 (0.0003 -- 2.0500)  max mem: 16413
[2023-08-31 11:06:59,632] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:06:59,632] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:06:59,632] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-08-31 11:06:59,632] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000000  loss: 2.7531 (2.7613)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5821 (1.5351)  time: 0.8343 (0.5121 -- 2.0076)  data: 0.1087 (0.0002 -- 1.4275)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:37  lr: 0.000016  min_lr: 0.000000  loss: 2.7412 (2.7583)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5286 (1.5338)  time: 0.8859 (0.5153 -- 2.1287)  data: 0.0868 (0.0004 -- 1.1455)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 2.7317 (2.7543)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6419 (1.5441)  time: 0.7950 (0.5160 -- 1.7300)  data: 0.1333 (0.0004 -- 1.2019)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.7261 (2.7512)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6011 (1.5514)  time: 0.7485 (0.4920 -- 3.7799)  data: 0.0008 (0.0002 -- 0.0073)  max mem: 16413
Epoch: [1] Total time: 0:02:22 (0.8901 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.7261 (2.7515)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6011 (1.5514)
Val:  [ 0/27]  eta: 0:01:05  loss: 2.6634 (2.6634)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.4133 (2.4133 -- 2.4133)  data: 2.1500 (2.1500 -- 2.1500)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.6793 (2.6791)  acc1: 33.3333 (36.3636)  acc5: 88.8889 (87.8788)  time: 0.4283 (0.1956 -- 2.4133)  data: 0.2098 (0.0008 -- 2.1500)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6691 (2.6739)  acc1: 44.4444 (40.7407)  acc5: 88.8889 (88.8889)  time: 0.2203 (0.1678 -- 0.3884)  data: 0.0163 (0.0001 -- 0.1652)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6747 (2.6779)  acc1: 44.4444 (40.6639)  acc5: 88.8889 (87.5519)  time: 0.2058 (0.1320 -- 0.3884)  data: 0.0160 (0.0001 -- 0.1652)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 41.701 Acc@5 86.722 loss 2.676
Accuracy of the network on the 482 val images: 41.70%
[2023-08-31 11:07:59,116] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:07:59,118] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:07:59,120] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:07:59,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:08:00,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:08:00,530] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 41.70%
Epoch: [2]  [  0/160]  eta: 0:23:12  lr: 0.000019  min_lr: 0.000000  loss: 2.7504 (2.7504)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5716 (1.5716)  time: 8.7024 (8.7024 -- 8.7024)  data: 8.1623 (8.1623 -- 8.1623)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:44  lr: 0.000020  min_lr: 0.000000  loss: 2.7133 (2.7169)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6110 (1.5803)  time: 0.7959 (0.5239 -- 4.6789)  data: 0.2536 (0.0004 -- 4.1542)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:06  lr: 0.000021  min_lr: 0.000001  loss: 2.6982 (2.7103)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6082 (1.5995)  time: 0.9365 (0.5200 -- 4.2438)  data: 0.3945 (0.0008 -- 3.7203)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:34  lr: 0.000022  min_lr: 0.000001  loss: 2.6873 (2.7022)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6106 (1.6211)  time: 0.7176 (0.5105 -- 2.0957)  data: 0.1722 (0.0002 -- 1.5547)  max mem: 16413
[2023-08-31 11:09:01,895] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:09:01,895] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-08-31 11:09:01,896] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:09:01,901] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.6800 (2.6970)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6739 (1.6298)  time: 0.9056 (0.5312 -- 2.4926)  data: 0.1725 (0.0003 -- 1.9195)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:56  lr: 0.000025  min_lr: 0.000001  loss: 2.6600 (2.6921)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7010 (1.6545)  time: 0.9536 (0.5177 -- 3.4736)  data: 0.0027 (0.0003 -- 0.0159)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 2.6545 (2.6869)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7258 (1.6790)  time: 0.8116 (0.5235 -- 3.3421)  data: 0.0022 (0.0004 -- 0.0092)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 2.6284 (2.6779)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7775 (1.7047)  time: 0.8705 (0.5201 -- 3.1167)  data: 0.0020 (0.0003 -- 0.0158)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 2.6048 (2.6699)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8229 (1.7190)  time: 0.6710 (0.4922 -- 2.9480)  data: 0.0008 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [2] Total time: 0:02:21 (0.8839 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 2.6048 (2.6687)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8229 (1.7190)
Val:  [ 0/27]  eta: 0:01:06  loss: 2.4295 (2.4295)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4454 (2.4454 -- 2.4454)  data: 2.1678 (2.1678 -- 2.1678)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.4913 (2.4812)  acc1: 44.4444 (39.3939)  acc5: 100.0000 (90.9091)  time: 0.4198 (0.1988 -- 2.4454)  data: 0.2012 (0.0007 -- 2.1678)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.4590 (2.4673)  acc1: 33.3333 (37.0370)  acc5: 88.8889 (91.0053)  time: 0.2158 (0.1691 -- 0.4119)  data: 0.0137 (0.0001 -- 0.2265)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.4590 (2.4778)  acc1: 33.3333 (36.0996)  acc5: 88.8889 (87.9668)  time: 0.2004 (0.1323 -- 0.4119)  data: 0.0132 (0.0001 -- 0.2265)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 39.834 Acc@5 88.174 loss 2.477
Accuracy of the network on the 482 val images: 39.83%
Max accuracy: 41.70%
Epoch: [3]  [  0/160]  eta: 0:16:20  lr: 0.000028  min_lr: 0.000001  loss: 2.5879 (2.5879)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2179 (2.2179)  time: 6.1303 (6.1303 -- 6.1303)  data: 5.3541 (5.3541 -- 5.3541)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:39  lr: 0.000029  min_lr: 0.000001  loss: 2.6229 (2.6226)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0108 (2.0081)  time: 0.8931 (0.5222 -- 3.7179)  data: 0.2275 (0.0005 -- 1.7857)  max mem: 16413
[2023-08-31 11:11:05,195] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:11:05,195] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-08-31 11:11:05,195] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:11:05,196] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:01  lr: 0.000031  min_lr: 0.000001  loss: 2.5916 (2.6033)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8906 (1.9391)  time: 0.8822 (0.5205 -- 3.4393)  data: 0.2450 (0.0004 -- 2.9097)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:36  lr: 0.000032  min_lr: 0.000001  loss: 2.5725 (2.5873)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0667 (1.9931)  time: 0.8693 (0.5274 -- 2.8462)  data: 0.2750 (0.0004 -- 2.3393)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:18  lr: 0.000033  min_lr: 0.000001  loss: 2.5628 (2.5800)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0703 (2.0425)  time: 1.0196 (0.5164 -- 4.3690)  data: 0.2469 (0.0003 -- 2.4597)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:57  lr: 0.000034  min_lr: 0.000001  loss: 2.5730 (2.5767)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0214 (2.0645)  time: 0.8487 (0.5056 -- 4.1985)  data: 0.0009 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000001  loss: 2.4899 (2.5644)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2990 (2.1563)  time: 0.7653 (0.5152 -- 2.5631)  data: 0.0019 (0.0002 -- 0.0059)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 2.5135 (2.5587)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2518 (2.1727)  time: 0.9240 (0.5268 -- 4.3974)  data: 0.0017 (0.0005 -- 0.0056)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 2.5227 (2.5529)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4611 (2.2150)  time: 0.6336 (0.4938 -- 2.6906)  data: 0.0009 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [3] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 2.5227 (2.5517)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4611 (2.2150)
Val:  [ 0/27]  eta: 0:01:08  loss: 2.1878 (2.1878)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.5487 (2.5487 -- 2.5487)  data: 2.3129 (2.3129 -- 2.3129)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.2746 (2.2614)  acc1: 33.3333 (39.3939)  acc5: 100.0000 (90.9091)  time: 0.4253 (0.1995 -- 2.5487)  data: 0.2112 (0.0005 -- 2.3129)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.2242 (2.2418)  acc1: 33.3333 (39.6825)  acc5: 88.8889 (89.9471)  time: 0.2174 (0.1693 -- 0.4396)  data: 0.0136 (0.0001 -- 0.2573)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.2384 (2.2622)  acc1: 33.3333 (38.1743)  acc5: 88.8889 (87.9668)  time: 0.2019 (0.1322 -- 0.4396)  data: 0.0133 (0.0001 -- 0.2573)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 39.834 Acc@5 87.344 loss 2.261
Accuracy of the network on the 482 val images: 39.83%
Max accuracy: 41.70%
[2023-08-31 11:13:05,490] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:13:05,491] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-08-31 11:13:05,494] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:13:05,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:14:53  lr: 0.000038  min_lr: 0.000001  loss: 2.3742 (2.3742)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.0786 (2.0786)  time: 5.5836 (5.5836 -- 5.5836)  data: 4.4884 (4.4884 -- 4.4884)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:40  lr: 0.000039  min_lr: 0.000001  loss: 2.4730 (2.4405)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3372 (2.5046)  time: 0.9219 (0.5140 -- 2.5153)  data: 0.2854 (0.0009 -- 1.9767)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:01:55  lr: 0.000040  min_lr: 0.000001  loss: 2.4832 (2.4558)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4207 (2.5182)  time: 0.7799 (0.5215 -- 3.1900)  data: 0.2356 (0.0007 -- 2.6711)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:37  lr: 0.000041  min_lr: 0.000001  loss: 2.3936 (2.4361)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7313 (2.6344)  time: 0.9903 (0.5310 -- 3.6572)  data: 0.4188 (0.0009 -- 3.1248)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:13  lr: 0.000042  min_lr: 0.000001  loss: 2.4373 (2.4293)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5132 (2.8124)  time: 0.7649 (0.5242 -- 2.1122)  data: 0.1264 (0.0007 -- 1.5736)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000001  loss: 2.4772 (2.4262)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2317 (2.9483)  time: 0.8979 (0.5247 -- 3.9869)  data: 0.3409 (0.0004 -- 3.4667)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.3864 (2.4163)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3004 (3.0155)  time: 0.9406 (0.5262 -- 4.2442)  data: 0.0080 (0.0003 -- 0.1238)  max mem: 16413
[2023-08-31 11:14:56,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:14:56,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:14:56,775] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-08-31 11:14:56,775] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.4446 (2.4187)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3376 (3.0539)  time: 0.9664 (0.5234 -- 4.1920)  data: 0.3754 (0.0004 -- 3.6550)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.4534 (2.4201)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5013 (3.0998)  time: 0.6110 (0.4961 -- 1.5155)  data: 0.0142 (0.0002 -- 0.2730)  max mem: 16413
Epoch: [4] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.4534 (2.4348)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5013 (3.0998)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.9627 (1.9627)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.5107 (2.5107 -- 2.5107)  data: 2.2940 (2.2940 -- 2.2940)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.0131 (2.0186)  acc1: 44.4444 (46.4646)  acc5: 100.0000 (93.9394)  time: 0.4329 (0.1993 -- 2.5107)  data: 0.2218 (0.0009 -- 2.2940)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.9734 (1.9897)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (93.1217)  time: 0.2131 (0.1681 -- 0.3346)  data: 0.0122 (0.0001 -- 0.1247)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0131 (2.0190)  acc1: 44.4444 (43.5685)  acc5: 88.8889 (91.7012)  time: 0.1978 (0.1322 -- 0.3346)  data: 0.0119 (0.0001 -- 0.1247)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 46.266 Acc@5 91.286 loss 2.005
Accuracy of the network on the 482 val images: 46.27%
[2023-08-31 11:15:29,246] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:15:29,248] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:15:29,248] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:15:29,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:15:30,607] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:15:30,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 46.27%
Epoch: [5]  [  0/160]  eta: 0:19:29  lr: 0.000047  min_lr: 0.000001  loss: 2.3115 (2.3115)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6342 (3.6342)  time: 7.3088 (7.3088 -- 7.3088)  data: 5.5977 (5.5977 -- 5.5977)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:46  lr: 0.000047  min_lr: 0.000001  loss: 2.2823 (2.3632)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6636 (4.0934)  time: 0.8839 (0.5257 -- 4.5248)  data: 0.1531 (0.0003 -- 2.1954)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000001  loss: 2.3140 (2.3371)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0236 (4.1529)  time: 0.9283 (0.5200 -- 4.5046)  data: 0.0014 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000001  loss: 2.3780 (2.3451)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6348 (4.0903)  time: 0.9044 (0.5160 -- 5.0593)  data: 0.0014 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:18  lr: 0.000047  min_lr: 0.000001  loss: 2.2980 (2.3376)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1741 (4.1866)  time: 0.8883 (0.5320 -- 4.0265)  data: 0.0013 (0.0002 -- 0.0034)  max mem: 16413
[2023-08-31 11:17:01,824] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:17:01,824] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:17:01,824] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-08-31 11:17:01,824] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000001  loss: 2.3526 (2.3400)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1553 (4.2147)  time: 0.7027 (0.5234 -- 2.6934)  data: 0.0017 (0.0004 -- 0.0058)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.2956 (2.3389)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8628 (4.2039)  time: 0.8286 (0.5272 -- 2.5703)  data: 0.0015 (0.0005 -- 0.0036)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.3241 (2.3385)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5138 (4.2651)  time: 0.8689 (0.5366 -- 2.8875)  data: 0.0018 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1827 (2.3261)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9975 (4.3155)  time: 0.6972 (0.4948 -- 2.4906)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [5] Total time: 0:02:20 (0.8803 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1827 (2.3293)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9975 (4.3155)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.7694 (1.7694)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.3483 (2.3483 -- 2.3483)  data: 2.1292 (2.1292 -- 2.1292)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.7804 (1.8090)  acc1: 44.4444 (46.4646)  acc5: 100.0000 (93.9394)  time: 0.4366 (0.1966 -- 2.3483)  data: 0.2234 (0.0007 -- 2.1292)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.7531 (1.7821)  acc1: 44.4444 (45.5026)  acc5: 100.0000 (94.1799)  time: 0.2238 (0.1691 -- 0.5546)  data: 0.0205 (0.0001 -- 0.3193)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7810 (1.8149)  acc1: 44.4444 (45.2282)  acc5: 88.8889 (93.7759)  time: 0.2089 (0.1331 -- 0.5546)  data: 0.0202 (0.0001 -- 0.3193)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 50.415 Acc@5 92.739 loss 1.794
Accuracy of the network on the 482 val images: 50.41%
[2023-08-31 11:17:59,269] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:17:59,271] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:17:59,271] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:17:59,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:18:00,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:18:00,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 50.41%
Epoch: [6]  [  0/160]  eta: 0:17:34  lr: 0.000047  min_lr: 0.000001  loss: 2.4900 (2.4900)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6360 (5.6360)  time: 6.5888 (6.5888 -- 6.5888)  data: 5.6723 (5.6723 -- 5.6723)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:37  lr: 0.000047  min_lr: 0.000001  loss: 2.3332 (2.4009)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3762 (4.5432)  time: 0.8531 (0.5237 -- 2.6209)  data: 0.1220 (0.0005 -- 0.9256)  max mem: 16413
[2023-08-31 11:18:41,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.1135100837097188e-06, 1.1135100837097188e-06, 1.4846801116129583e-06, 1.4846801116129583e-06, 1.979573482150611e-06, 1.979573482150611e-06, 2.639431309534148e-06, 2.639431309534148e-06, 3.5192417460455307e-06, 3.5192417460455307e-06, 4.692322328060707e-06, 4.692322328060707e-06, 6.25642977074761e-06, 6.25642977074761e-06, 8.341906360996815e-06, 8.341906360996815e-06, 1.1122541814662419e-05, 1.1122541814662419e-05, 1.4830055752883225e-05, 1.4830055752883225e-05, 1.9773407670510965e-05, 1.9773407670510965e-05, 2.6364543560681288e-05, 2.6364543560681288e-05, 3.515272474757505e-05, 3.515272474757505e-05, 4.68702996634334e-05, 4.68702996634334e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 11:18:41,223] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=16.595895761537832, CurrSamplesPerSec=22.539529314087215, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:01:59  lr: 0.000047  min_lr: 0.000001  loss: 2.2753 (2.3342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6809 (4.6700)  time: 0.8660 (0.5298 -- 3.9016)  data: 0.2150 (0.0006 -- 3.3603)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:37  lr: 0.000047  min_lr: 0.000001  loss: 2.2155 (2.3102)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7763 (4.7171)  time: 0.9253 (0.5280 -- 2.7177)  data: 0.2759 (0.0003 -- 2.1924)  max mem: 16413
[2023-08-31 11:19:02,529] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:19:02,530] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-08-31 11:19:02,533] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:19:02,571] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000001  loss: 2.2594 (2.3025)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3598 (4.7326)  time: 0.8624 (0.5156 -- 3.0312)  data: 0.1793 (0.0004 -- 2.4951)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.3019 (2.3025)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4086 (4.8699)  time: 0.8780 (0.5185 -- 3.6127)  data: 0.2886 (0.0004 -- 3.0927)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000001  loss: 2.1941 (2.2916)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0106 (4.8761)  time: 0.9508 (0.5176 -- 4.3844)  data: 0.3339 (0.0007 -- 3.8625)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.1975 (2.2808)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5491 (5.0698)  time: 0.8196 (0.5232 -- 2.3061)  data: 0.0881 (0.0002 -- 1.7321)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1935 (2.2718)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2293 (5.0589)  time: 0.7565 (0.4938 -- 2.4275)  data: 0.1621 (0.0002 -- 1.9334)  max mem: 16413
Epoch: [6] Total time: 0:02:23 (0.8969 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1935 (2.2666)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2293 (5.0589)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.6183 (1.6183)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.3747 (2.3747 -- 2.3747)  data: 2.1674 (2.1674 -- 2.1674)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.6298 (1.6735)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (93.9394)  time: 0.4318 (0.1998 -- 2.3747)  data: 0.2187 (0.0003 -- 2.1674)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6228 (1.6469)  acc1: 44.4444 (46.0317)  acc5: 100.0000 (94.1799)  time: 0.2171 (0.1681 -- 0.4361)  data: 0.0121 (0.0001 -- 0.2295)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6534 (1.6862)  acc1: 44.4444 (45.6432)  acc5: 88.8889 (93.3610)  time: 0.2028 (0.1326 -- 0.4361)  data: 0.0118 (0.0001 -- 0.2295)  max mem: 16413
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 51.452 Acc@5 93.154 loss 1.657
Accuracy of the network on the 482 val images: 51.45%
[2023-08-31 11:20:32,028] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:20:32,029] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:20:32,030] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:20:32,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:20:33,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:20:33,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 51.45%
Epoch: [7]  [  0/160]  eta: 0:18:28  lr: 0.000047  min_lr: 0.000001  loss: 1.8730 (1.8730)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2388 (4.2388)  time: 6.9254 (6.9254 -- 6.9254)  data: 6.3031 (6.3031 -- 6.3031)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:59  lr: 0.000047  min_lr: 0.000001  loss: 2.2713 (2.2707)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5958 (5.8888)  time: 0.9973 (0.5206 -- 3.7204)  data: 0.1590 (0.0003 -- 2.8986)  max mem: 16413
[2023-08-31 11:21:09,047] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:21:09,048] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-08-31 11:21:09,049] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:21:09,050] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000001  loss: 2.1249 (2.2341)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6913 (5.3373)  time: 0.7659 (0.5266 -- 2.7463)  data: 0.0015 (0.0004 -- 0.0039)  max mem: 16413
[2023-08-31 11:21:19,468] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1164
[2023-08-31 11:21:19,468] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1164
[2023-08-31 11:21:19,468] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-08-31 11:21:19,468] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-08-31 11:21:19,468] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
Epoch: [7]  [ 60/160]  eta: 0:01:36  lr: 0.000047  min_lr: 0.000001  loss: 2.3239 (2.2502)  loss_scale: 32768.0000 (39214.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2958 (5.2704)  time: 0.8353 (0.5296 -- 2.7997)  data: 0.0015 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000001  loss: 2.3100 (2.2669)  loss_scale: 32768.0000 (37622.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9392 (5.2278)  time: 0.8299 (0.5336 -- 2.3582)  data: 0.0014 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [7]  [100/160]  eta: 0:00:54  lr: 0.000047  min_lr: 0.000001  loss: 2.1266 (2.2466)  loss_scale: 32768.0000 (36661.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1644 (5.3382)  time: 0.8189 (0.5274 -- 2.2785)  data: 0.0015 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.2071 (2.2419)  loss_scale: 32768.0000 (36017.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4917 (5.4716)  time: 0.9236 (0.5340 -- 3.0239)  data: 0.0016 (0.0004 -- 0.0030)  max mem: 16413
[2023-08-31 11:22:25,304] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1243
[2023-08-31 11:22:25,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:22:25,304] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1243
[2023-08-31 11:22:25,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:22:25,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [7]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.3547 (2.2391)  loss_scale: 16384.0000 (33465.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8182 (5.5421)  time: 0.8382 (0.5333 -- 2.1229)  data: 0.1201 (0.0004 -- 1.5508)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.2370 (2.2423)  loss_scale: 16384.0000 (31436.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3727 (5.5808)  time: 0.6545 (0.4946 -- 1.7082)  data: 0.0817 (0.0002 -- 1.1904)  max mem: 16413
Epoch: [7] Total time: 0:02:19 (0.8730 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.2370 (2.2390)  loss_scale: 16384.0000 (31436.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3727 (5.5808)
Val:  [ 0/27]  eta: 0:01:11  loss: 1.5391 (1.5391)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.6650 (2.6650 -- 2.6650)  data: 2.4226 (2.4226 -- 2.4226)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5391 (1.5843)  acc1: 44.4444 (54.5455)  acc5: 100.0000 (94.9495)  time: 0.4397 (0.2007 -- 2.6650)  data: 0.2229 (0.0007 -- 2.4226)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5280 (1.5659)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (94.1799)  time: 0.2106 (0.1691 -- 0.2626)  data: 0.0056 (0.0001 -- 0.0796)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5394 (1.6019)  acc1: 55.5556 (53.5270)  acc5: 88.8889 (94.1909)  time: 0.1963 (0.1330 -- 0.2626)  data: 0.0053 (0.0001 -- 0.0796)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 58.506 Acc@5 93.361 loss 1.569
Accuracy of the network on the 482 val images: 58.51%
[2023-08-31 11:23:00,841] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:23:00,842] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:23:00,843] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:23:00,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:23:02,273] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:23:02,273] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 58.51%
Epoch: [8]  [  0/160]  eta: 0:20:15  lr: 0.000047  min_lr: 0.000001  loss: 2.0441 (2.0441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6384 (8.6384)  time: 7.5981 (7.5981 -- 7.5981)  data: 5.8540 (5.8540 -- 5.8540)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:45  lr: 0.000047  min_lr: 0.000001  loss: 2.0955 (2.1471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5826 (5.9525)  time: 0.8581 (0.5264 -- 3.2566)  data: 0.0863 (0.0006 -- 1.0087)  max mem: 16413
Epoch: [8]  [ 40/160]  eta: 0:02:07  lr: 0.000047  min_lr: 0.000001  loss: 2.1212 (2.1152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3609 (6.3025)  time: 0.9445 (0.5173 -- 5.3379)  data: 0.0013 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [8]  [ 60/160]  eta: 0:01:40  lr: 0.000047  min_lr: 0.000001  loss: 2.0701 (2.1449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3800 (6.2009)  time: 0.8965 (0.5205 -- 3.9479)  data: 0.0023 (0.0002 -- 0.0167)  max mem: 16413
Epoch: [8]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000001  loss: 2.1798 (2.1534)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4220 (6.1430)  time: 0.8130 (0.5276 -- 4.1374)  data: 0.0017 (0.0005 -- 0.0065)  max mem: 16413
[2023-08-31 11:24:30,672] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:24:30,672] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:24:30,673] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:24:30,673] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [8]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.2728 (2.1787)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1606 (6.0693)  time: 0.8476 (0.5266 -- 3.0446)  data: 0.0020 (0.0002 -- 0.0063)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.1736 (2.1871)  loss_scale: 32768.0000 (20310.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5416 (6.0753)  time: 0.7802 (0.5285 -- 1.7435)  data: 0.0016 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [8]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.3396 (2.2029)  loss_scale: 32768.0000 (22077.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7248 (6.1983)  time: 0.8558 (0.5287 -- 2.5100)  data: 0.0093 (0.0003 -- 0.1591)  max mem: 16413
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.2905 (2.2095)  loss_scale: 32768.0000 (23347.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7851 (6.1478)  time: 0.7482 (0.4952 -- 3.3463)  data: 0.2087 (0.0002 -- 2.8179)  max mem: 16413
Epoch: [8] Total time: 0:02:21 (0.8843 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.2905 (2.2016)  loss_scale: 32768.0000 (23347.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7851 (6.1478)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.4439 (1.4439)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3406 (2.3406 -- 2.3406)  data: 2.0807 (2.0807 -- 2.0807)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.4439 (1.4949)  acc1: 44.4444 (49.4949)  acc5: 100.0000 (93.9394)  time: 0.4088 (0.2029 -- 2.3406)  data: 0.1929 (0.0006 -- 2.0807)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4271 (1.4722)  acc1: 55.5556 (52.3810)  acc5: 100.0000 (93.6508)  time: 0.2147 (0.1705 -- 0.3770)  data: 0.0108 (0.0001 -- 0.1712)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5179 (1.5185)  acc1: 55.5556 (53.1120)  acc5: 88.8889 (93.7759)  time: 0.1991 (0.1329 -- 0.3770)  data: 0.0105 (0.0001 -- 0.1712)  max mem: 16413
Val: Total time: 0:00:07 (0.2823 s / it)
* Acc@1 57.676 Acc@5 93.154 loss 1.485
Accuracy of the network on the 482 val images: 57.68%
Max accuracy: 58.51%
Epoch: [9]  [  0/160]  eta: 0:17:57  lr: 0.000047  min_lr: 0.000001  loss: 2.0056 (2.0056)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1144 (4.1144)  time: 6.7323 (6.7323 -- 6.7323)  data: 6.1760 (6.1760 -- 6.1760)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:03:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1104 (2.1459)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0036 (6.4467)  time: 1.0181 (0.5251 -- 4.5200)  data: 0.0228 (0.0006 -- 0.4216)  max mem: 16413
Epoch: [9]  [ 40/160]  eta: 0:02:08  lr: 0.000047  min_lr: 0.000001  loss: 2.1699 (2.1655)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8904 (6.2476)  time: 0.8404 (0.5234 -- 3.7723)  data: 0.0012 (0.0004 -- 0.0031)  max mem: 16413
[2023-08-31 11:26:34,979] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:26:34,980] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 11:26:34,983] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:26:34,983] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 60/160]  eta: 0:01:43  lr: 0.000047  min_lr: 0.000001  loss: 2.1404 (2.1507)  loss_scale: 32768.0000 (33305.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0401 (6.1074)  time: 0.9658 (0.5157 -- 5.3751)  data: 0.0016 (0.0004 -- 0.0036)  max mem: 16413
[2023-08-31 11:26:35,506] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1501
[2023-08-31 11:26:35,506] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1501
[2023-08-31 11:26:35,506] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 11:26:35,506] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 11:26:35,506] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [ 80/160]  eta: 0:01:16  lr: 0.000047  min_lr: 0.000001  loss: 2.2259 (2.1551)  loss_scale: 32768.0000 (33172.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0160 (5.9035)  time: 0.7242 (0.5157 -- 2.7862)  data: 0.0016 (0.0002 -- 0.0092)  max mem: 16413
Epoch: [9]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000001  loss: 2.2242 (2.1609)  loss_scale: 32768.0000 (33092.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7940 (5.9232)  time: 0.9247 (0.5292 -- 3.3952)  data: 0.0016 (0.0005 -- 0.0037)  max mem: 16413
Epoch: [9]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.0861 (2.1575)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8051 (5.9227)  time: 0.7684 (0.5294 -- 3.4328)  data: 0.0015 (0.0005 -- 0.0040)  max mem: 16413
Epoch: [9]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.0267 (2.1390)  loss_scale: 32768.0000 (33000.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7459 (5.9347)  time: 0.9794 (0.5205 -- 3.7376)  data: 0.0020 (0.0003 -- 0.0169)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1165 (2.1407)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0630 (5.9589)  time: 0.6465 (0.4955 -- 2.3096)  data: 0.0006 (0.0002 -- 0.0014)  max mem: 16413
Epoch: [9] Total time: 0:02:23 (0.8972 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1165 (2.1476)  loss_scale: 32768.0000 (32972.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0630 (5.9589)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.3998 (1.3998)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4531 (2.4531 -- 2.4531)  data: 2.2081 (2.2081 -- 2.2081)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3998 (1.4366)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (96.9697)  time: 0.4163 (0.2043 -- 2.4531)  data: 0.2022 (0.0008 -- 2.2081)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3334 (1.4095)  acc1: 55.5556 (55.0265)  acc5: 100.0000 (96.2963)  time: 0.2204 (0.1702 -- 0.5748)  data: 0.0201 (0.0001 -- 0.3842)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4127 (1.4517)  acc1: 55.5556 (54.7718)  acc5: 88.8889 (94.6058)  time: 0.2061 (0.1328 -- 0.5748)  data: 0.0196 (0.0001 -- 0.3842)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 58.714 Acc@5 93.776 loss 1.424
Accuracy of the network on the 482 val images: 58.71%
[2023-08-31 11:28:03,168] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:28:03,169] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:28:03,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:28:03,170] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:28:04,323] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:28:04,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 58.71%
Epoch: [10]  [  0/160]  eta: 0:23:40  lr: 0.000047  min_lr: 0.000001  loss: 2.4177 (2.4177)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0459 (4.0459)  time: 8.8786 (8.8786 -- 8.8786)  data: 8.3639 (8.3639 -- 8.3639)  max mem: 16413
[2023-08-31 11:28:23,402] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1612
[2023-08-31 11:28:23,402] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:28:23,402] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1612
[2023-08-31 11:28:23,402] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 11:28:23,402] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [10]  [ 20/160]  eta: 0:02:53  lr: 0.000047  min_lr: 0.000001  loss: 2.0356 (2.1135)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4419 (5.8080)  time: 0.8584 (0.5164 -- 4.1350)  data: 0.2568 (0.0004 -- 3.6158)  max mem: 16413
Epoch: [10]  [ 40/160]  eta: 0:02:11  lr: 0.000047  min_lr: 0.000001  loss: 2.1834 (2.1651)  loss_scale: 16384.0000 (21179.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1012 (6.2666)  time: 0.9493 (0.5273 -- 4.3348)  data: 0.0204 (0.0003 -- 0.2881)  max mem: 16413
Epoch: [10]  [ 60/160]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000001  loss: 1.9358 (2.1020)  loss_scale: 16384.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3977 (6.3841)  time: 0.8546 (0.5200 -- 3.6685)  data: 0.0013 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [10]  [ 80/160]  eta: 0:01:18  lr: 0.000047  min_lr: 0.000001  loss: 2.0519 (2.0817)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3510 (6.3276)  time: 0.8906 (0.5144 -- 4.4870)  data: 0.0010 (0.0003 -- 0.0022)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:57  lr: 0.000047  min_lr: 0.000001  loss: 2.2688 (2.1087)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0124 (6.3974)  time: 0.8343 (0.5280 -- 3.7161)  data: 0.0016 (0.0004 -- 0.0051)  max mem: 16413
Epoch: [10]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000001  loss: 2.0778 (2.1072)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0563 (6.3541)  time: 0.8163 (0.5428 -- 3.1622)  data: 0.0018 (0.0002 -- 0.0053)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.2099 (2.1261)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3108 (6.5076)  time: 0.7923 (0.5270 -- 3.5116)  data: 0.0014 (0.0003 -- 0.0043)  max mem: 16413
[2023-08-31 11:30:13,700] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:30:13,701] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:30:13,703] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:30:13,703] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1819 (2.1383)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3680 (6.6403)  time: 0.6421 (0.4942 -- 2.6258)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [10] Total time: 0:02:21 (0.8819 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1819 (2.1394)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3680 (6.6403)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.3736 (1.3736)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3712 (2.3712 -- 2.3712)  data: 2.1722 (2.1722 -- 2.1722)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3733 (1.3873)  acc1: 44.4444 (54.5455)  acc5: 100.0000 (95.9596)  time: 0.4279 (0.2006 -- 2.3712)  data: 0.2169 (0.0011 -- 2.1722)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3226 (1.3685)  acc1: 55.5556 (56.0847)  acc5: 100.0000 (95.2381)  time: 0.2223 (0.1695 -- 0.4114)  data: 0.0187 (0.0001 -- 0.2020)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3733 (1.4158)  acc1: 55.5556 (56.0166)  acc5: 88.8889 (94.1909)  time: 0.2063 (0.1332 -- 0.4114)  data: 0.0183 (0.0001 -- 0.2020)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 59.336 Acc@5 93.568 loss 1.384
Accuracy of the network on the 482 val images: 59.34%
[2023-08-31 11:30:33,281] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:30:33,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:30:33,283] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:30:33,283] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:30:34,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:30:34,773] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.34%
Epoch: [11]  [  0/160]  eta: 0:18:16  lr: 0.000047  min_lr: 0.000001  loss: 2.3244 (2.3244)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9607 (4.9607)  time: 6.8517 (6.8517 -- 6.8517)  data: 5.7139 (5.7139 -- 5.7139)  max mem: 16413
Epoch: [11]  [ 20/160]  eta: 0:02:40  lr: 0.000047  min_lr: 0.000001  loss: 2.1664 (2.2285)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2375 (6.6089)  time: 0.8615 (0.5312 -- 1.9337)  data: 0.1470 (0.0002 -- 1.3977)  max mem: 16413
[2023-08-31 11:30:59,940] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1782
[2023-08-31 11:30:59,941] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:30:59,941] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1782
[2023-08-31 11:30:59,941] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:30:59,942] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [ 40/160]  eta: 0:02:03  lr: 0.000047  min_lr: 0.000001  loss: 2.0774 (2.1696)  loss_scale: 16384.0000 (25175.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0559 (6.6274)  time: 0.9006 (0.5171 -- 4.0071)  data: 0.0025 (0.0006 -- 0.0168)  max mem: 16413
Epoch: [11]  [ 60/160]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000001  loss: 2.2175 (2.1786)  loss_scale: 16384.0000 (22292.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0145 (6.9150)  time: 0.9878 (0.5228 -- 4.9412)  data: 0.0012 (0.0005 -- 0.0023)  max mem: 16413
Epoch: [11]  [ 80/160]  eta: 0:01:15  lr: 0.000047  min_lr: 0.000001  loss: 2.2194 (2.1794)  loss_scale: 16384.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6738 (7.0432)  time: 0.7550 (0.5143 -- 2.9261)  data: 0.0013 (0.0001 -- 0.0060)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.1229 (2.1627)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0332 (7.0977)  time: 0.8706 (0.5275 -- 3.2222)  data: 0.0020 (0.0002 -- 0.0150)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.1337 (2.1573)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3969 (7.0076)  time: 0.8362 (0.5320 -- 3.5039)  data: 0.0046 (0.0003 -- 0.0640)  max mem: 16413
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.0487 (2.1349)  loss_scale: 16384.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3371 (6.9828)  time: 0.9396 (0.5198 -- 4.4335)  data: 0.0014 (0.0002 -- 0.0038)  max mem: 16413
[2023-08-31 11:32:53,578] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:32:53,578] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:32:53,578] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:32:53,578] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 1.9643 (2.1230)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8874 (6.9409)  time: 0.7183 (0.4948 -- 2.7870)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [11] Total time: 0:02:23 (0.8984 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 1.9643 (2.1033)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8874 (6.9409)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.2569 (1.2569)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.5176 (2.5176 -- 2.5176)  data: 2.2862 (2.2862 -- 2.2862)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2569 (1.3125)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (96.9697)  time: 0.4340 (0.2004 -- 2.5176)  data: 0.2195 (0.0008 -- 2.2862)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2264 (1.2838)  acc1: 55.5556 (57.1429)  acc5: 100.0000 (96.2963)  time: 0.2168 (0.1693 -- 0.3839)  data: 0.0165 (0.0001 -- 0.1985)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3252 (1.3395)  acc1: 55.5556 (57.2614)  acc5: 100.0000 (95.4357)  time: 0.2017 (0.1326 -- 0.3839)  data: 0.0161 (0.0001 -- 0.1985)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 60.373 Acc@5 94.191 loss 1.306
Accuracy of the network on the 482 val images: 60.37%
[2023-08-31 11:33:06,368] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:33:06,370] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:33:06,370] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:33:06,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:33:07,892] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:33:07,893] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 60.37%
Epoch: [12]  [  0/160]  eta: 0:19:49  lr: 0.000047  min_lr: 0.000001  loss: 1.5142 (1.5142)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0166 (7.0166)  time: 7.4326 (7.4326 -- 7.4326)  data: 4.8873 (4.8873 -- 4.8873)  max mem: 16413
Epoch: [12]  [ 20/160]  eta: 0:02:35  lr: 0.000047  min_lr: 0.000001  loss: 1.9379 (1.9373)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6117 (6.9809)  time: 0.7937 (0.5240 -- 1.8340)  data: 0.1705 (0.0006 -- 1.0029)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:01:57  lr: 0.000047  min_lr: 0.000001  loss: 2.0123 (2.0012)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0993 (7.1647)  time: 0.8482 (0.5281 -- 2.1178)  data: 0.2221 (0.0004 -- 1.5329)  max mem: 16413
Epoch: [12]  [ 60/160]  eta: 0:01:41  lr: 0.000047  min_lr: 0.000001  loss: 2.0658 (2.0119)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8830 (6.8719)  time: 1.0751 (0.5278 -- 3.9806)  data: 0.4862 (0.0003 -- 3.4656)  max mem: 16413
[2023-08-31 11:34:23,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=5, lr=[1.1095727809336618e-06, 1.1095727809336618e-06, 1.4794303745782159e-06, 1.4794303745782159e-06, 1.9725738327709545e-06, 1.9725738327709545e-06, 2.630098443694606e-06, 2.630098443694606e-06, 3.5067979249261413e-06, 3.5067979249261413e-06, 4.675730566568188e-06, 4.675730566568188e-06, 6.234307422090918e-06, 6.234307422090918e-06, 8.312409896121224e-06, 8.312409896121224e-06, 1.1083213194828298e-05, 1.1083213194828298e-05, 1.4777617593104396e-05, 1.4777617593104396e-05, 1.9703490124139195e-05, 1.9703490124139195e-05, 2.6271320165518927e-05, 2.6271320165518927e-05, 3.502842688735857e-05, 3.502842688735857e-05, 4.670456918314476e-05, 4.670456918314476e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 11:34:23,174] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=16.101998218378476, CurrSamplesPerSec=22.076155637722543, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:14  lr: 0.000047  min_lr: 0.000001  loss: 2.0507 (2.0197)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8958 (6.9859)  time: 0.7022 (0.5227 -- 2.2636)  data: 0.1542 (0.0002 -- 1.7477)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.3129 (2.0696)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9786 (7.0643)  time: 0.9440 (0.5240 -- 3.3834)  data: 0.4020 (0.0006 -- 2.8585)  max mem: 16413
[2023-08-31 11:34:57,012] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:34:57,012] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 11:34:57,012] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:34:57,013] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 11:34:57,571] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2040
[2023-08-31 11:34:57,571] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2040
[2023-08-31 11:34:57,571] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 11:34:57,571] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 11:34:57,571] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [12]  [120/160]  eta: 0:00:36  lr: 0.000047  min_lr: 0.000001  loss: 2.1790 (2.0877)  loss_scale: 32768.0000 (33038.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8277 (7.1358)  time: 0.7480 (0.5197 -- 2.7072)  data: 0.1975 (0.0003 -- 2.1731)  max mem: 16413
[2023-08-31 11:34:58,639] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2042
[2023-08-31 11:34:58,639] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2042
[2023-08-31 11:34:58,639] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:34:58,639] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:34:58,639] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [12]  [140/160]  eta: 0:00:17  lr: 0.000047  min_lr: 0.000001  loss: 1.8990 (2.0618)  loss_scale: 16384.0000 (30792.6241)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2201 (6.9300)  time: 0.8547 (0.5200 -- 3.7641)  data: 0.2570 (0.0005 -- 2.4936)  max mem: 16413
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1904 (2.0724)  loss_scale: 16384.0000 (29081.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5085 (6.9872)  time: 0.7320 (0.4976 -- 3.4662)  data: 0.0694 (0.0002 -- 1.3741)  max mem: 16413
Epoch: [12] Total time: 0:02:20 (0.8805 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1904 (2.0793)  loss_scale: 16384.0000 (29081.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5085 (6.9872)
Val:  [ 0/27]  eta: 0:01:11  loss: 1.2389 (1.2389)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.6609 (2.6609 -- 2.6609)  data: 2.4315 (2.4315 -- 2.4315)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2342 (1.2906)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (96.9697)  time: 0.4606 (0.1930 -- 2.6609)  data: 0.2494 (0.0005 -- 2.4315)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2240 (1.2616)  acc1: 55.5556 (58.7302)  acc5: 100.0000 (96.2963)  time: 0.2170 (0.1710 -- 0.5049)  data: 0.0157 (0.0001 -- 0.2986)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2814 (1.3042)  acc1: 55.5556 (58.9212)  acc5: 100.0000 (95.4357)  time: 0.2041 (0.1370 -- 0.5049)  data: 0.0153 (0.0001 -- 0.2986)  max mem: 16413
Val: Total time: 0:00:07 (0.2960 s / it)
* Acc@1 62.863 Acc@5 94.813 loss 1.269
Accuracy of the network on the 482 val images: 62.86%
[2023-08-31 11:35:36,763] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:35:36,765] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:35:36,765] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:35:36,765] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:35:38,205] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:35:38,206] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 62.86%
Epoch: [13]  [  0/160]  eta: 0:20:24  lr: 0.000047  min_lr: 0.000001  loss: 1.7206 (1.7206)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4557 (6.4557)  time: 7.6504 (7.6504 -- 7.6504)  data: 6.8030 (6.8030 -- 6.8030)  max mem: 16413
Epoch: [13]  [ 20/160]  eta: 0:02:42  lr: 0.000047  min_lr: 0.000001  loss: 2.1541 (2.1369)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2082 (7.0198)  time: 0.8372 (0.5286 -- 3.8797)  data: 0.0235 (0.0003 -- 0.4398)  max mem: 16413
Epoch: [13]  [ 40/160]  eta: 0:02:06  lr: 0.000047  min_lr: 0.000001  loss: 2.1045 (2.1524)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8684 (7.2865)  time: 0.9412 (0.5230 -- 3.8850)  data: 0.0023 (0.0002 -- 0.0073)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000001  loss: 2.0640 (2.1288)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4000 (7.0783)  time: 0.8694 (0.5294 -- 3.7127)  data: 0.0015 (0.0006 -- 0.0036)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:19  lr: 0.000047  min_lr: 0.000001  loss: 2.0798 (2.1299)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3010 (7.1940)  time: 0.9977 (0.5275 -- 4.0647)  data: 0.0281 (0.0004 -- 0.5162)  max mem: 16413
[2023-08-31 11:37:06,383] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:37:06,383] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:37:06,383] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:37:06,383] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:37:11,364] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2178
[2023-08-31 11:37:11,364] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:37:11,364] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2178
[2023-08-31 11:37:11,365] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:37:11,365] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 1.9311 (2.1132)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7255 (7.1964)  time: 0.6859 (0.5038 -- 2.2315)  data: 0.0010 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000001  loss: 2.0968 (2.1002)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2511 (7.3508)  time: 0.9128 (0.5334 -- 3.3128)  data: 0.0016 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.0596 (2.0800)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9688 (7.3360)  time: 0.8598 (0.5164 -- 4.8333)  data: 0.0015 (0.0002 -- 0.0069)  max mem: 16413
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 1.9669 (2.0638)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5955 (7.5025)  time: 0.7091 (0.4940 -- 3.1849)  data: 0.0005 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [13] Total time: 0:02:23 (0.8963 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 1.9669 (2.0713)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5955 (7.5025)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.1899 (1.1899)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4026 (2.4026 -- 2.4026)  data: 2.1839 (2.1839 -- 2.1839)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1383 (1.2381)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (96.9697)  time: 0.4264 (0.2053 -- 2.4026)  data: 0.2153 (0.0007 -- 2.1839)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1102 (1.2067)  acc1: 55.5556 (58.2011)  acc5: 100.0000 (96.2963)  time: 0.2260 (0.1701 -- 0.4700)  data: 0.0242 (0.0001 -- 0.2953)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1777 (1.2462)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (95.0207)  time: 0.2114 (0.1323 -- 0.4700)  data: 0.0238 (0.0001 -- 0.2953)  max mem: 16413
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 62.656 Acc@5 94.398 loss 1.211
Accuracy of the network on the 482 val images: 62.66%
Max accuracy: 62.86%
Epoch: [14]  [  0/160]  eta: 0:23:46  lr: 0.000047  min_lr: 0.000001  loss: 2.0189 (2.0189)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5597 (5.5597)  time: 8.9183 (8.9183 -- 8.9183)  data: 8.3990 (8.3990 -- 8.3990)  max mem: 16413
Epoch: [14]  [ 20/160]  eta: 0:02:44  lr: 0.000047  min_lr: 0.000001  loss: 2.0312 (2.0197)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4170 (8.1298)  time: 0.7892 (0.5259 -- 3.0218)  data: 0.1662 (0.0003 -- 2.4038)  max mem: 16413
Epoch: [14]  [ 40/160]  eta: 0:02:08  lr: 0.000047  min_lr: 0.000001  loss: 2.0057 (2.0143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8321 (8.0284)  time: 0.9628 (0.5198 -- 4.4169)  data: 0.0398 (0.0003 -- 0.7672)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:39  lr: 0.000047  min_lr: 0.000001  loss: 2.0030 (2.0304)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7509 (8.1318)  time: 0.8413 (0.5325 -- 5.4001)  data: 0.1795 (0.0003 -- 2.9434)  max mem: 16413
[2023-08-31 11:39:16,719] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:39:16,720] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:39:16,720] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:39:16,721] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 80/160]  eta: 0:01:18  lr: 0.000047  min_lr: 0.000001  loss: 2.0587 (2.0392)  loss_scale: 32768.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0419 (7.9384)  time: 0.9381 (0.5173 -- 3.2803)  data: 0.0090 (0.0004 -- 0.1591)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:55  lr: 0.000047  min_lr: 0.000001  loss: 2.0444 (2.0412)  loss_scale: 32768.0000 (21899.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9044 (8.0258)  time: 0.6955 (0.5262 -- 2.5345)  data: 0.0017 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000001  loss: 1.9021 (2.0211)  loss_scale: 32768.0000 (23695.8678)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4699 (7.9504)  time: 0.9844 (0.5225 -- 3.4949)  data: 0.1956 (0.0005 -- 2.9895)  max mem: 16413
[2023-08-31 11:40:04,332] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2363
[2023-08-31 11:40:04,332] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2363
[2023-08-31 11:40:04,332] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:40:04,333] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 11:40:04,333] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.1039 (2.0251)  loss_scale: 16384.0000 (22891.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5452 (7.9512)  time: 0.8919 (0.5177 -- 4.2057)  data: 0.0208 (0.0004 -- 0.3855)  max mem: 16413
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 1.9248 (2.0176)  loss_scale: 16384.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3524 (7.8419)  time: 0.5790 (0.4952 -- 1.3905)  data: 0.0174 (0.0001 -- 0.3361)  max mem: 16413
Epoch: [14] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 1.9248 (2.0527)  loss_scale: 16384.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3524 (7.8419)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.1568 (1.1568)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5043 (2.5043 -- 2.5043)  data: 2.2824 (2.2824 -- 2.2824)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1568 (1.2393)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (93.9394)  time: 0.4533 (0.2034 -- 2.5043)  data: 0.2359 (0.0006 -- 2.2824)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0818 (1.1825)  acc1: 55.5556 (60.3175)  acc5: 100.0000 (95.2381)  time: 0.2205 (0.1692 -- 0.5262)  data: 0.0174 (0.0001 -- 0.2789)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1879 (1.2396)  acc1: 55.5556 (59.3361)  acc5: 100.0000 (95.0207)  time: 0.2052 (0.1326 -- 0.5262)  data: 0.0164 (0.0001 -- 0.2789)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 62.033 Acc@5 94.191 loss 1.200
Accuracy of the network on the 482 val images: 62.03%
Max accuracy: 62.86%
Epoch: [15]  [  0/160]  eta: 0:18:00  lr: 0.000047  min_lr: 0.000001  loss: 2.0092 (2.0092)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5839 (8.5839)  time: 6.7525 (6.7525 -- 6.7525)  data: 6.2197 (6.2197 -- 6.2197)  max mem: 16413
Epoch: [15]  [ 20/160]  eta: 0:02:48  lr: 0.000047  min_lr: 0.000001  loss: 2.1169 (2.0481)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7310 (8.0856)  time: 0.9256 (0.5278 -- 3.1045)  data: 0.3013 (0.0003 -- 2.5758)  max mem: 16413
Epoch: [15]  [ 40/160]  eta: 0:02:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1273 (2.0476)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3045 (7.8739)  time: 0.7887 (0.5209 -- 2.5826)  data: 0.2430 (0.0003 -- 2.0508)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:37  lr: 0.000047  min_lr: 0.000001  loss: 2.0495 (2.0557)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4969 (7.8947)  time: 0.9267 (0.5256 -- 3.2077)  data: 0.3839 (0.0005 -- 2.6551)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:17  lr: 0.000047  min_lr: 0.000001  loss: 1.9523 (2.0439)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0437 (7.8379)  time: 0.9642 (0.5223 -- 4.2542)  data: 0.4235 (0.0003 -- 3.7445)  max mem: 16413
[2023-08-31 11:42:08,573] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:42:08,573] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:42:08,574] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:42:08,574] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [100/160]  eta: 0:00:56  lr: 0.000047  min_lr: 0.000001  loss: 2.1112 (2.0504)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7962 (7.7853)  time: 0.8057 (0.5252 -- 4.2496)  data: 0.2634 (0.0002 -- 3.7302)  max mem: 16413
Epoch: [15]  [120/160]  eta: 0:00:37  lr: 0.000047  min_lr: 0.000001  loss: 1.9998 (2.0390)  loss_scale: 32768.0000 (20310.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3513 (7.8539)  time: 0.9235 (0.5228 -- 3.7539)  data: 0.3762 (0.0002 -- 3.2391)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000047  min_lr: 0.000001  loss: 2.1433 (2.0545)  loss_scale: 32768.0000 (22077.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6831 (7.8239)  time: 0.8729 (0.5130 -- 5.2363)  data: 0.3255 (0.0003 -- 4.7219)  max mem: 16413
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000001  loss: 2.1433 (2.0705)  loss_scale: 32768.0000 (23347.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5421 (7.8067)  time: 0.6487 (0.4950 -- 2.1206)  data: 0.1260 (0.0001 -- 1.5880)  max mem: 16413
Epoch: [15] Total time: 0:02:23 (0.8960 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000001  loss: 2.1433 (2.0408)  loss_scale: 32768.0000 (23347.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5421 (7.8067)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.1835 (1.1835)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4678 (2.4678 -- 2.4678)  data: 2.2611 (2.2611 -- 2.2611)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1320 (1.1865)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (94.9495)  time: 0.4176 (0.2020 -- 2.4678)  data: 0.2066 (0.0005 -- 2.2611)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0292 (1.1384)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (95.2381)  time: 0.2170 (0.1693 -- 0.4874)  data: 0.0160 (0.0001 -- 0.3055)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1462 (1.1865)  acc1: 55.5556 (61.4108)  acc5: 100.0000 (95.4357)  time: 0.2016 (0.1327 -- 0.4874)  data: 0.0158 (0.0001 -- 0.3055)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 63.693 Acc@5 94.813 loss 1.155
Accuracy of the network on the 482 val images: 63.69%
[2023-08-31 11:43:10,682] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:43:10,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:43:10,684] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:43:10,684] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:43:12,224] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:43:12,224] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.69%
Epoch: [16]  [  0/160]  eta: 0:21:13  lr: 0.000047  min_lr: 0.000001  loss: 2.0382 (2.0382)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0950 (11.0950)  time: 7.9617 (7.9617 -- 7.9617)  data: 7.4249 (7.4249 -- 7.4249)  max mem: 16413
[2023-08-31 11:43:34,790] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2576
[2023-08-31 11:43:34,790] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:43:34,790] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2576
[2023-08-31 11:43:34,790] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:43:34,790] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 20/160]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000001  loss: 1.9397 (2.0494)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7332 (7.6601)  time: 0.8346 (0.5220 -- 3.4762)  data: 0.2667 (0.0004 -- 2.7980)  max mem: 16413
Epoch: [16]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000001  loss: 2.0594 (2.0296)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0649 (7.7192)  time: 0.9131 (0.5204 -- 3.2815)  data: 0.3621 (0.0002 -- 2.7556)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000001  loss: 1.9680 (2.0170)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0517 (7.6009)  time: 0.7961 (0.5244 -- 2.6400)  data: 0.1977 (0.0003 -- 2.1240)  max mem: 16413
Epoch: [16]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000001  loss: 1.9536 (2.0208)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0407 (8.0332)  time: 0.9386 (0.5207 -- 3.2081)  data: 0.2132 (0.0005 -- 2.6903)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.1041 (2.0221)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6701 (8.2384)  time: 0.8048 (0.5277 -- 2.7373)  data: 0.1902 (0.0007 -- 2.2111)  max mem: 16413
Epoch: [16]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.1005 (2.0414)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2018 (8.2145)  time: 0.8435 (0.5329 -- 3.5746)  data: 0.0987 (0.0004 -- 0.9841)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.1058 (2.0441)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4057 (8.0976)  time: 0.9568 (0.5190 -- 4.3159)  data: 0.0634 (0.0004 -- 1.0761)  max mem: 16413
[2023-08-31 11:45:24,822] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:45:24,822] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:45:24,825] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:45:24,826] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.1303 (2.0545)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6609 (8.0406)  time: 0.6468 (0.4953 -- 2.7311)  data: 0.0007 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [16] Total time: 0:02:22 (0.8886 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.1303 (2.0500)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6609 (8.0406)
Val:  [ 0/27]  eta: 0:01:07  loss: 1.0947 (1.0947)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5072 (2.5072 -- 2.5072)  data: 2.2762 (2.2762 -- 2.2762)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0707 (1.1554)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (94.9495)  time: 0.4481 (0.2000 -- 2.5072)  data: 0.2327 (0.0006 -- 2.2762)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9503 (1.1132)  acc1: 55.5556 (62.9630)  acc5: 100.0000 (95.2381)  time: 0.2191 (0.1689 -- 0.5185)  data: 0.0166 (0.0001 -- 0.2740)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1035 (1.1544)  acc1: 55.5556 (63.0705)  acc5: 100.0000 (95.0207)  time: 0.2050 (0.1325 -- 0.5185)  data: 0.0163 (0.0001 -- 0.2740)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 65.560 Acc@5 94.813 loss 1.117
Accuracy of the network on the 482 val images: 65.56%
[2023-08-31 11:45:42,282] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:45:42,284] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:45:42,284] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:45:42,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:45:43,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:45:43,506] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.56%
Epoch: [17]  [  0/160]  eta: 0:21:57  lr: 0.000046  min_lr: 0.000001  loss: 2.4969 (2.4969)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2899 (5.2899)  time: 8.2353 (8.2353 -- 8.2353)  data: 4.4717 (4.4717 -- 4.4717)  max mem: 16413
Epoch: [17]  [ 20/160]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000001  loss: 1.8226 (1.9490)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8034 (7.7434)  time: 0.8234 (0.5253 -- 3.2841)  data: 0.0754 (0.0006 -- 1.4563)  max mem: 16413
Epoch: [17]  [ 40/160]  eta: 0:02:03  lr: 0.000046  min_lr: 0.000001  loss: 1.8831 (1.9482)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9100 (7.7200)  time: 0.8798 (0.5364 -- 2.1102)  data: 0.1234 (0.0005 -- 1.5073)  max mem: 16413
[2023-08-31 11:46:38,007] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2776
[2023-08-31 11:46:38,010] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2776
[2023-08-31 11:46:38,049] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:46:38,049] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:46:38,049] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [17]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000001  loss: 1.9158 (1.9707)  loss_scale: 32768.0000 (31425.0492)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7627 (7.5285)  time: 0.9056 (0.5243 -- 2.5480)  data: 0.3652 (0.0004 -- 2.0328)  max mem: 16413
Epoch: [17]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000001  loss: 1.9284 (1.9702)  loss_scale: 16384.0000 (27711.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2180 (7.7215)  time: 0.8969 (0.5228 -- 3.5439)  data: 0.1947 (0.0003 -- 1.9978)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 1.9994 (1.9835)  loss_scale: 16384.0000 (25468.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6099 (7.8561)  time: 0.7519 (0.5290 -- 3.0846)  data: 0.0357 (0.0004 -- 0.6480)  max mem: 16413
[2023-08-31 11:47:17,436] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2821
[2023-08-31 11:47:17,436] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2821
[2023-08-31 11:47:17,436] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 11:47:17,436] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 11:47:17,437] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [17]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 1.8185 (1.9756)  loss_scale: 8192.0000 (22612.6281)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3433 (7.7706)  time: 0.9317 (0.5196 -- 3.8627)  data: 0.0013 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 1.8149 (1.9542)  loss_scale: 8192.0000 (20567.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5298 (7.6874)  time: 0.8751 (0.5293 -- 3.6236)  data: 0.0014 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 1.8203 (1.9499)  loss_scale: 8192.0000 (19097.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2041 (7.6390)  time: 0.6981 (0.4968 -- 2.1499)  data: 0.0009 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [17] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 1.8203 (1.9800)  loss_scale: 8192.0000 (19097.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2041 (7.6390)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.0711 (1.0711)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4180 (2.4180 -- 2.4180)  data: 2.2140 (2.2140 -- 2.2140)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0711 (1.1614)  acc1: 66.6667 (59.5960)  acc5: 100.0000 (94.9495)  time: 0.4386 (0.2015 -- 2.4180)  data: 0.2293 (0.0005 -- 2.2140)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0053 (1.1083)  acc1: 66.6667 (61.3757)  acc5: 100.0000 (95.7672)  time: 0.2253 (0.1697 -- 0.5421)  data: 0.0235 (0.0001 -- 0.3000)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1438 (1.1637)  acc1: 66.6667 (58.9212)  acc5: 100.0000 (95.4357)  time: 0.2126 (0.1318 -- 0.5421)  data: 0.0233 (0.0001 -- 0.3000)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 63.900 Acc@5 95.021 loss 1.124
Accuracy of the network on the 482 val images: 63.90%
Max accuracy: 65.56%
Epoch: [18]  [  0/160]  eta: 0:20:41  lr: 0.000046  min_lr: 0.000001  loss: 2.0443 (2.0443)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9064 (7.9064)  time: 7.7580 (7.7580 -- 7.7580)  data: 7.1835 (7.1835 -- 7.1835)  max mem: 16413
Epoch: [18]  [ 20/160]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000001  loss: 1.9892 (2.0170)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8961 (8.3723)  time: 0.8679 (0.5240 -- 3.8323)  data: 0.2760 (0.0002 -- 3.2829)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:05  lr: 0.000046  min_lr: 0.000001  loss: 2.0565 (2.0148)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7485 (8.1064)  time: 0.8873 (0.5181 -- 2.8310)  data: 0.1450 (0.0005 -- 2.2595)  max mem: 16413
Epoch: [18]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000001  loss: 2.1972 (2.0690)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1991 (7.9357)  time: 0.8850 (0.5266 -- 3.5396)  data: 0.0763 (0.0003 -- 1.4830)  max mem: 16413
[2023-08-31 11:49:22,332] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:49:22,332] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:49:22,332] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 11:49:22,332] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [18]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 1.9362 (2.0299)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4319 (7.9449)  time: 0.7873 (0.5176 -- 3.7849)  data: 0.0364 (0.0003 -- 0.7013)  max mem: 16413
Epoch: [18]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000001  loss: 2.1105 (2.0360)  loss_scale: 16384.0000 (10706.3762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8355 (7.9631)  time: 0.9409 (0.5131 -- 4.6477)  data: 0.0484 (0.0007 -- 0.9420)  max mem: 16413
[2023-08-31 11:50:03,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=12, lr=[1.1000414583904577e-06, 1.1000414583904577e-06, 1.4667219445206102e-06, 1.4667219445206102e-06, 1.9556292593608135e-06, 1.9556292593608135e-06, 2.6075056791477513e-06, 2.6075056791477513e-06, 3.4766742388636687e-06, 3.4766742388636687e-06, 4.635565651818225e-06, 4.635565651818225e-06, 6.1807542024243e-06, 6.1807542024243e-06, 8.2410056032324e-06, 8.2410056032324e-06, 1.0988007470976532e-05, 1.0988007470976532e-05, 1.465067662796871e-05, 1.465067662796871e-05, 1.953423550395828e-05, 1.953423550395828e-05, 2.604564733861104e-05, 2.604564733861104e-05, 3.472752978481472e-05, 3.472752978481472e-05, 4.630337304641963e-05, 4.630337304641963e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 11:50:03,198] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=16.44702531496611, CurrSamplesPerSec=22.396666552454676, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-08-31 11:50:03,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3000
[2023-08-31 11:50:03,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3000
[2023-08-31 11:50:03,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 11:50:03,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 11:50:03,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [18]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.1550 (2.0500)  loss_scale: 16384.0000 (11577.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4410 (8.0370)  time: 0.7801 (0.5205 -- 3.0481)  data: 0.0016 (0.0003 -- 0.0058)  max mem: 16413
Epoch: [18]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 1.9410 (2.0511)  loss_scale: 8192.0000 (11096.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8836 (7.9361)  time: 0.8252 (0.5256 -- 2.4288)  data: 0.0341 (0.0001 -- 0.6573)  max mem: 16413
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 1.9314 (2.0448)  loss_scale: 8192.0000 (10752.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0561 (7.8687)  time: 0.7701 (0.4954 -- 3.3570)  data: 0.2544 (0.0002 -- 2.8296)  max mem: 16413
Epoch: [18] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 1.9314 (2.0122)  loss_scale: 8192.0000 (10752.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0561 (7.8687)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.0525 (1.0525)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2835 (2.2835 -- 2.2835)  data: 2.0530 (2.0530 -- 2.0530)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9798 (1.1301)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (94.9495)  time: 0.4291 (0.2017 -- 2.2835)  data: 0.2130 (0.0004 -- 2.0530)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9323 (1.0888)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (95.7672)  time: 0.2324 (0.1692 -- 0.5098)  data: 0.0279 (0.0001 -- 0.2818)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1022 (1.1400)  acc1: 55.5556 (62.2407)  acc5: 100.0000 (95.0207)  time: 0.2171 (0.1330 -- 0.5098)  data: 0.0277 (0.0001 -- 0.2818)  max mem: 16413
Val: Total time: 0:00:07 (0.2932 s / it)
* Acc@1 65.975 Acc@5 95.021 loss 1.096
Accuracy of the network on the 482 val images: 65.98%
[2023-08-31 11:50:42,999] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:50:43,001] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:50:43,001] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:50:43,001] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:50:44,484] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:50:44,484] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.98%
Epoch: [19]  [  0/160]  eta: 0:16:39  lr: 0.000046  min_lr: 0.000001  loss: 1.6288 (1.6288)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7039 (7.7039)  time: 6.2456 (6.2456 -- 6.2456)  data: 5.7168 (5.7168 -- 5.7168)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:44  lr: 0.000046  min_lr: 0.000001  loss: 1.9239 (1.8789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3904 (6.7753)  time: 0.9249 (0.5246 -- 3.7328)  data: 0.1191 (0.0005 -- 1.9038)  max mem: 16413
Epoch: [19]  [ 40/160]  eta: 0:01:57  lr: 0.000046  min_lr: 0.000001  loss: 1.9045 (1.9223)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8372 (7.5804)  time: 0.7742 (0.5303 -- 4.2055)  data: 0.0017 (0.0005 -- 0.0076)  max mem: 16413
Epoch: [19]  [ 60/160]  eta: 0:01:34  lr: 0.000046  min_lr: 0.000001  loss: 2.0996 (1.9907)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8192 (7.6043)  time: 0.8840 (0.5362 -- 2.8373)  data: 0.1765 (0.0007 -- 2.2809)  max mem: 16413
Epoch: [19]  [ 80/160]  eta: 0:01:14  lr: 0.000046  min_lr: 0.000001  loss: 1.9107 (1.9653)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6332 (7.9093)  time: 0.8773 (0.5324 -- 3.0466)  data: 0.0938 (0.0004 -- 1.8527)  max mem: 16413
[2023-08-31 11:52:08,062] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:52:08,063] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 11:52:08,064] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:52:08,064] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [19]  [100/160]  eta: 0:00:54  lr: 0.000046  min_lr: 0.000001  loss: 1.8169 (1.9578)  loss_scale: 16384.0000 (9165.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4748 (7.9442)  time: 0.8542 (0.5219 -- 3.5241)  data: 0.2574 (0.0004 -- 3.0094)  max mem: 16413
Epoch: [19]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.0454 (1.9697)  loss_scale: 16384.0000 (10358.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6604 (8.0802)  time: 0.9013 (0.5303 -- 2.5837)  data: 0.1605 (0.0003 -- 1.5927)  max mem: 16413
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.0801 (1.9775)  loss_scale: 16384.0000 (11213.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9304 (8.1752)  time: 0.9407 (0.5314 -- 2.6410)  data: 0.0015 (0.0005 -- 0.0034)  max mem: 16413
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0509 (1.9745)  loss_scale: 16384.0000 (11827.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0073 (8.0433)  time: 0.6582 (0.4982 -- 1.8509)  data: 0.0015 (0.0001 -- 0.0147)  max mem: 16413
Epoch: [19] Total time: 0:02:21 (0.8821 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0509 (1.9908)  loss_scale: 16384.0000 (11827.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0073 (8.0433)
[2023-08-31 11:53:05,623] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-08-31 11:53:05,625] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-08-31 11:53:05,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-08-31 11:53:05,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-08-31 11:53:06,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-08-31 11:53:06,719] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 1.0218 (1.0218)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3818 (2.3818 -- 2.3818)  data: 2.1314 (2.1314 -- 2.1314)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0218 (1.1235)  acc1: 55.5556 (62.6263)  acc5: 100.0000 (94.9495)  time: 0.4334 (0.1997 -- 2.3818)  data: 0.2144 (0.0006 -- 2.1314)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9454 (1.0694)  acc1: 66.6667 (63.4921)  acc5: 100.0000 (95.7672)  time: 0.2260 (0.1688 -- 0.4299)  data: 0.0206 (0.0001 -- 0.1973)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1430 (1.1346)  acc1: 55.5556 (60.5809)  acc5: 100.0000 (95.0207)  time: 0.2087 (0.1325 -- 0.4299)  data: 0.0193 (0.0001 -- 0.1973)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 65.145 Acc@5 94.191 loss 1.081
Accuracy of the network on the 482 val images: 65.15%
Max accuracy: 65.98%
Epoch: [20]  [  0/160]  eta: 0:18:15  lr: 0.000046  min_lr: 0.000001  loss: 2.3310 (2.3310)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1901 (5.1901)  time: 6.8439 (6.8439 -- 6.8439)  data: 4.9227 (4.9227 -- 4.9227)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:47  lr: 0.000046  min_lr: 0.000001  loss: 1.9509 (1.9750)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6562 (8.0192)  time: 0.9175 (0.5262 -- 4.6551)  data: 0.0015 (0.0002 -- 0.0056)  max mem: 16413
Epoch: [20]  [ 40/160]  eta: 0:02:07  lr: 0.000046  min_lr: 0.000001  loss: 2.1131 (2.0187)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9386 (7.6986)  time: 0.9109 (0.5168 -- 4.5934)  data: 0.0015 (0.0002 -- 0.0033)  max mem: 16413
[2023-08-31 11:54:12,563] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:54:12,563] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:54:12,567] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:54:12,567] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [ 60/160]  eta: 0:01:37  lr: 0.000046  min_lr: 0.000001  loss: 2.0254 (2.0222)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9587 (7.6294)  time: 0.8072 (0.5226 -- 2.4363)  data: 0.0928 (0.0005 -- 1.0023)  max mem: 16413
[2023-08-31 11:54:29,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3279
[2023-08-31 11:54:29,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3279
[2023-08-31 11:54:29,525] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:54:29,525] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:54:29,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [20]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 1.9933 (2.0052)  loss_scale: 32768.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1014 (7.5483)  time: 0.8599 (0.5317 -- 2.4898)  data: 0.0827 (0.0006 -- 1.2392)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 1.9695 (2.0000)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1191 (7.5258)  time: 0.8706 (0.5405 -- 2.6965)  data: 0.0204 (0.0008 -- 0.3663)  max mem: 16413
Epoch: [20]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.0081 (1.9950)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8499 (7.5950)  time: 0.8751 (0.5409 -- 4.6502)  data: 0.0019 (0.0003 -- 0.0060)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 1.9375 (1.9862)  loss_scale: 16384.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0856 (7.6509)  time: 0.8697 (0.5119 -- 3.4756)  data: 0.0014 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0405 (1.9949)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1627 (7.6726)  time: 0.6715 (0.4960 -- 2.2348)  data: 0.0010 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [20] Total time: 0:02:22 (0.8877 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0405 (1.9921)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1627 (7.6726)
Val:  [ 0/27]  eta: 0:01:10  loss: 1.0498 (1.0498)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5991 (2.5991 -- 2.5991)  data: 2.3337 (2.3337 -- 2.3337)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0498 (1.0889)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (94.9495)  time: 0.4440 (0.1959 -- 2.5991)  data: 0.2213 (0.0008 -- 2.3337)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8447 (1.0441)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (95.7672)  time: 0.2101 (0.1691 -- 0.3459)  data: 0.0052 (0.0001 -- 0.0899)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0226 (1.0837)  acc1: 66.6667 (63.4855)  acc5: 100.0000 (94.6058)  time: 0.1929 (0.1329 -- 0.3459)  data: 0.0048 (0.0001 -- 0.0899)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 67.220 Acc@5 94.398 loss 1.035
Accuracy of the network on the 482 val images: 67.22%
[2023-08-31 11:55:44,500] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:55:44,502] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:55:44,502] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:55:44,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:55:45,992] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:55:45,992] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.22%
Epoch: [21]  [  0/160]  eta: 0:23:58  lr: 0.000046  min_lr: 0.000001  loss: 2.1255 (2.1255)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5727 (10.5727)  time: 8.9913 (8.9913 -- 8.9913)  data: 8.4506 (8.4506 -- 8.4506)  max mem: 16413
Epoch: [21]  [ 20/160]  eta: 0:02:50  lr: 0.000046  min_lr: 0.000001  loss: 2.0267 (2.0293)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3223 (7.7085)  time: 0.8283 (0.5171 -- 3.5172)  data: 0.2842 (0.0004 -- 2.9809)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000001  loss: 1.8932 (1.9940)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7696 (7.6605)  time: 0.9384 (0.5238 -- 4.1837)  data: 0.3283 (0.0003 -- 3.6669)  max mem: 16413
[2023-08-31 11:56:35,960] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:56:35,960] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:56:35,961] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:56:35,961] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [21]  [ 60/160]  eta: 0:01:36  lr: 0.000046  min_lr: 0.000001  loss: 2.1837 (2.0109)  loss_scale: 32768.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2658 (7.6273)  time: 0.7146 (0.5298 -- 2.5233)  data: 0.0540 (0.0003 -- 1.0454)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000001  loss: 1.8887 (1.9856)  loss_scale: 32768.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1162 (7.7510)  time: 0.9638 (0.5183 -- 4.4780)  data: 0.1037 (0.0003 -- 2.0436)  max mem: 16413
[2023-08-31 11:57:18,229] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3460
[2023-08-31 11:57:18,229] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3460
[2023-08-31 11:57:18,230] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:57:18,230] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 11:57:18,230] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [21]  [100/160]  eta: 0:00:54  lr: 0.000046  min_lr: 0.000001  loss: 1.9918 (1.9836)  loss_scale: 32768.0000 (24819.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5279 (7.8048)  time: 0.7163 (0.5345 -- 2.5525)  data: 0.1545 (0.0005 -- 2.0248)  max mem: 16413
Epoch: [21]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 1.9165 (1.9795)  loss_scale: 16384.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1478 (7.8498)  time: 0.9908 (0.5366 -- 3.9840)  data: 0.4041 (0.0009 -- 3.4513)  max mem: 16413
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 1.7846 (1.9691)  loss_scale: 16384.0000 (22426.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7667 (8.0342)  time: 0.7971 (0.5242 -- 3.1602)  data: 0.0351 (0.0004 -- 0.6777)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0292 (1.9726)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7553 (8.0145)  time: 0.6507 (0.4962 -- 2.2758)  data: 0.0431 (0.0002 -- 0.8499)  max mem: 16413
Epoch: [21] Total time: 0:02:20 (0.8780 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0292 (1.9717)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7553 (8.0145)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.9069 (0.9069)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5553 (2.5553 -- 2.5553)  data: 2.3345 (2.3345 -- 2.3345)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9069 (1.0519)  acc1: 55.5556 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4404 (0.1939 -- 2.5553)  data: 0.2252 (0.0006 -- 2.3345)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8649 (1.0115)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.2963)  time: 0.2180 (0.1694 -- 0.3997)  data: 0.0144 (0.0001 -- 0.1415)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0423 (1.0514)  acc1: 55.5556 (63.4855)  acc5: 100.0000 (95.8506)  time: 0.2033 (0.1331 -- 0.3997)  data: 0.0140 (0.0001 -- 0.1415)  max mem: 16413
Val: Total time: 0:00:07 (0.2926 s / it)
* Acc@1 67.635 Acc@5 95.021 loss 1.009
Accuracy of the network on the 482 val images: 67.63%
[2023-08-31 11:58:14,381] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 11:58:14,383] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 11:58:14,383] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 11:58:14,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 11:58:15,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 11:58:15,879] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.63%
Epoch: [22]  [  0/160]  eta: 0:21:53  lr: 0.000046  min_lr: 0.000001  loss: 1.8344 (1.8344)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8542 (5.8542)  time: 8.2120 (8.2120 -- 8.2120)  data: 7.6824 (7.6824 -- 7.6824)  max mem: 16413
Epoch: [22]  [ 20/160]  eta: 0:02:54  lr: 0.000046  min_lr: 0.000001  loss: 2.0184 (1.9215)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8719 (7.6376)  time: 0.8959 (0.5248 -- 2.9618)  data: 0.2237 (0.0003 -- 2.4295)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000001  loss: 2.0777 (1.9910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0700 (8.4499)  time: 0.7854 (0.5236 -- 3.5574)  data: 0.0023 (0.0005 -- 0.0165)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:39  lr: 0.000046  min_lr: 0.000001  loss: 2.2072 (2.0450)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7083 (8.3301)  time: 0.9297 (0.5273 -- 3.1091)  data: 0.1161 (0.0002 -- 1.6999)  max mem: 16413
[2023-08-31 11:59:24,204] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:59:24,204] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 11:59:24,205] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:59:24,205] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 11:59:25,308] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3591
[2023-08-31 11:59:25,308] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3591
[2023-08-31 11:59:25,309] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:59:25,309] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 11:59:25,309] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [ 80/160]  eta: 0:01:15  lr: 0.000046  min_lr: 0.000001  loss: 1.9998 (2.0265)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7751 (8.2517)  time: 0.8105 (0.5158 -- 2.1028)  data: 0.1097 (0.0004 -- 0.8088)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:55  lr: 0.000046  min_lr: 0.000001  loss: 2.0656 (2.0223)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3300 (8.1134)  time: 0.8340 (0.5217 -- 2.6055)  data: 0.1336 (0.0001 -- 2.0763)  max mem: 16413
Epoch: [22]  [120/160]  eta: 0:00:37  lr: 0.000046  min_lr: 0.000001  loss: 2.0036 (2.0160)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6091 (8.0205)  time: 0.9913 (0.5294 -- 4.2942)  data: 0.3016 (0.0004 -- 3.7422)  max mem: 16413
Epoch: [22]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 1.9396 (2.0155)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9146 (8.0052)  time: 0.8256 (0.5336 -- 3.0966)  data: 0.1041 (0.0002 -- 1.3030)  max mem: 16413
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0263 (2.0104)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5557 (7.9055)  time: 0.6607 (0.4965 -- 2.4803)  data: 0.0008 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [22] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0263 (1.9827)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5557 (7.9055)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.9332 (0.9332)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.6401 (2.6401 -- 2.6401)  data: 2.3877 (2.3877 -- 2.3877)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9332 (1.0584)  acc1: 55.5556 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4543 (0.1993 -- 2.6401)  data: 0.2344 (0.0006 -- 2.3877)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9420 (1.0149)  acc1: 66.6667 (62.9630)  acc5: 100.0000 (96.2963)  time: 0.2152 (0.1702 -- 0.3759)  data: 0.0118 (0.0001 -- 0.1681)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0658 (1.0541)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (95.0207)  time: 0.1982 (0.1328 -- 0.3759)  data: 0.0115 (0.0001 -- 0.1681)  max mem: 16413
Val: Total time: 0:00:07 (0.2937 s / it)
* Acc@1 66.390 Acc@5 95.021 loss 1.005
Accuracy of the network on the 482 val images: 66.39%
Max accuracy: 67.63%
Epoch: [23]  [  0/160]  eta: 0:24:39  lr: 0.000046  min_lr: 0.000001  loss: 2.2283 (2.2283)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7901 (10.7901)  time: 9.2471 (9.2471 -- 9.2471)  data: 8.7088 (8.7088 -- 8.7088)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:49  lr: 0.000046  min_lr: 0.000001  loss: 1.9415 (1.9586)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6645 (7.8811)  time: 0.8068 (0.5166 -- 5.3637)  data: 0.2623 (0.0004 -- 4.8337)  max mem: 16413
[2023-08-31 12:01:28,544] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:01:28,544] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:01:28,547] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:01:28,547] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [23]  [ 40/160]  eta: 0:02:04  lr: 0.000046  min_lr: 0.000001  loss: 1.8986 (1.9531)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9987 (8.0980)  time: 0.8515 (0.5250 -- 3.1597)  data: 0.3009 (0.0004 -- 2.6017)  max mem: 16413
[2023-08-31 12:01:34,208] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3725
[2023-08-31 12:01:34,208] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3725
[2023-08-31 12:01:34,208] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:01:34,208] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:01:34,208] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [23]  [ 60/160]  eta: 0:01:43  lr: 0.000046  min_lr: 0.000001  loss: 2.0984 (2.0229)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4605 (7.8896)  time: 1.0425 (0.5084 -- 4.0533)  data: 0.4986 (0.0003 -- 3.5262)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:18  lr: 0.000046  min_lr: 0.000001  loss: 2.0838 (2.0214)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6904 (7.9256)  time: 0.7864 (0.5247 -- 3.5421)  data: 0.2451 (0.0002 -- 3.0292)  max mem: 16413
Epoch: [23]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000001  loss: 2.0251 (2.0155)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6243 (7.9321)  time: 0.9253 (0.5219 -- 3.1146)  data: 0.3744 (0.0003 -- 2.5677)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 2.0152 (2.0190)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0880 (7.9375)  time: 0.6887 (0.5248 -- 1.7984)  data: 0.1332 (0.0004 -- 1.2755)  max mem: 16413
Epoch: [23]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 1.9177 (2.0123)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3095 (8.0350)  time: 0.8761 (0.5218 -- 2.3145)  data: 0.2436 (0.0003 -- 1.7691)  max mem: 16413
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 1.9009 (2.0101)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5342 (8.0488)  time: 0.7523 (0.4978 -- 3.0213)  data: 0.1010 (0.0002 -- 1.4489)  max mem: 16413
Epoch: [23] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 1.9009 (2.0075)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5342 (8.0488)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.9744 (0.9744)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2882 (2.2882 -- 2.2882)  data: 2.0666 (2.0666 -- 2.0666)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9744 (1.0586)  acc1: 55.5556 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4101 (0.2000 -- 2.2882)  data: 0.1949 (0.0007 -- 2.0666)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9462 (1.0030)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (96.2963)  time: 0.2283 (0.1687 -- 0.5808)  data: 0.0238 (0.0001 -- 0.3941)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0353 (1.0491)  acc1: 55.5556 (63.9004)  acc5: 100.0000 (95.4357)  time: 0.2121 (0.1325 -- 0.5808)  data: 0.0235 (0.0001 -- 0.3941)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 67.635 Acc@5 94.813 loss 1.010
Accuracy of the network on the 482 val images: 67.63%
Max accuracy: 67.63%
Epoch: [24]  [  0/160]  eta: 0:20:08  lr: 0.000046  min_lr: 0.000001  loss: 1.6779 (1.6779)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2565 (7.2565)  time: 7.5528 (7.5528 -- 7.5528)  data: 5.8148 (5.8148 -- 5.8148)  max mem: 16413
[2023-08-31 12:03:35,413] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:03:35,414] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:03:35,418] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:03:35,419] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [24]  [ 20/160]  eta: 0:02:46  lr: 0.000046  min_lr: 0.000001  loss: 1.9758 (1.9179)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1800 (8.4730)  time: 0.8739 (0.5259 -- 4.0614)  data: 0.0946 (0.0003 -- 0.9422)  max mem: 16413
[2023-08-31 12:03:56,830] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3878
[2023-08-31 12:03:56,831] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:03:56,830] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3878
[2023-08-31 12:03:56,832] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:03:56,832] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [24]  [ 40/160]  eta: 0:02:02  lr: 0.000046  min_lr: 0.000001  loss: 1.8553 (1.8887)  loss_scale: 32768.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1487 (7.7379)  time: 0.8379 (0.5241 -- 2.4904)  data: 0.0780 (0.0006 -- 1.0214)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000001  loss: 1.8452 (1.8985)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7386 (7.7306)  time: 0.9752 (0.5249 -- 3.2888)  data: 0.2022 (0.0003 -- 2.7824)  max mem: 16413
Epoch: [24]  [ 80/160]  eta: 0:01:17  lr: 0.000046  min_lr: 0.000001  loss: 1.9608 (1.9277)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6686 (8.1578)  time: 0.8728 (0.5228 -- 3.9856)  data: 0.3250 (0.0002 -- 3.4732)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:57  lr: 0.000046  min_lr: 0.000001  loss: 1.6319 (1.8855)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8988 (8.3600)  time: 0.9010 (0.5311 -- 3.5538)  data: 0.3516 (0.0003 -- 3.0278)  max mem: 16413
Epoch: [24]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 1.9007 (1.8871)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0469 (8.3876)  time: 0.7576 (0.5288 -- 3.0734)  data: 0.2021 (0.0002 -- 2.5131)  max mem: 16413
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.0829 (1.9071)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6128 (8.3533)  time: 0.8978 (0.5296 -- 2.9166)  data: 0.3437 (0.0004 -- 2.3809)  max mem: 16413
[2023-08-31 12:05:39,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=18, lr=[1.0850126715216115e-06, 1.0850126715216115e-06, 1.4466835620288151e-06, 1.4466835620288151e-06, 1.9289114160384203e-06, 1.9289114160384203e-06, 2.571881888051227e-06, 2.571881888051227e-06, 3.4291758507349696e-06, 3.4291758507349696e-06, 4.572234467646626e-06, 4.572234467646626e-06, 6.096312623528834e-06, 6.096312623528834e-06, 8.12841683137178e-06, 8.12841683137178e-06, 1.0837889108495706e-05, 1.0837889108495706e-05, 1.4450518811327607e-05, 1.4450518811327607e-05, 1.9267358415103478e-05, 1.9267358415103478e-05, 2.568981122013797e-05, 2.568981122013797e-05, 3.425308162685062e-05, 3.425308162685062e-05, 4.56707755024675e-05, 4.56707755024675e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 12:05:39,559] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=16.745418172343165, CurrSamplesPerSec=24.702382600040636, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0480 (1.9178)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7452 (8.3036)  time: 0.7625 (0.4938 -- 2.9166)  data: 0.2393 (0.0002 -- 2.3809)  max mem: 16413
Epoch: [24] Total time: 0:02:22 (0.8890 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0480 (1.9247)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7452 (8.3036)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.9332 (0.9332)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4067 (2.4067 -- 2.4067)  data: 2.1719 (2.1719 -- 2.1719)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9332 (1.0165)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4209 (0.1948 -- 2.4067)  data: 0.2099 (0.0007 -- 2.1719)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9205 (0.9832)  acc1: 66.6667 (66.1376)  acc5: 100.0000 (96.2963)  time: 0.2223 (0.1691 -- 0.4707)  data: 0.0218 (0.0001 -- 0.2953)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9796 (1.0256)  acc1: 66.6667 (64.7303)  acc5: 100.0000 (95.4357)  time: 0.2096 (0.1334 -- 0.4707)  data: 0.0215 (0.0001 -- 0.2953)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 67.635 Acc@5 95.021 loss 0.983
Accuracy of the network on the 482 val images: 67.63%
Max accuracy: 67.63%
Epoch: [25]  [  0/160]  eta: 0:19:38  lr: 0.000046  min_lr: 0.000001  loss: 2.1074 (2.1074)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4644 (5.4644)  time: 7.3686 (7.3686 -- 7.3686)  data: 6.8190 (6.8190 -- 6.8190)  max mem: 16413
[2023-08-31 12:05:59,551] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:05:59,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:05:59,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:05:59,553] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [25]  [ 20/160]  eta: 0:03:04  lr: 0.000046  min_lr: 0.000001  loss: 1.9819 (2.0507)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2305 (8.1990)  time: 1.0175 (0.5307 -- 3.5491)  data: 0.2482 (0.0004 -- 2.0494)  max mem: 16413
Epoch: [25]  [ 40/160]  eta: 0:02:07  lr: 0.000046  min_lr: 0.000001  loss: 1.9627 (1.9930)  loss_scale: 32768.0000 (29970.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7132 (7.8374)  time: 0.7945 (0.5164 -- 3.0466)  data: 0.0016 (0.0002 -- 0.0031)  max mem: 16413
[2023-08-31 12:06:44,768] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4055
[2023-08-31 12:06:44,768] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4055
[2023-08-31 12:06:44,768] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:06:44,769] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:06:44,769] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [ 60/160]  eta: 0:01:41  lr: 0.000046  min_lr: 0.000001  loss: 2.1267 (2.0387)  loss_scale: 32768.0000 (29276.3279)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6573 (7.8973)  time: 0.9056 (0.5099 -- 3.8033)  data: 0.0018 (0.0003 -- 0.0063)  max mem: 16413
Epoch: [25]  [ 80/160]  eta: 0:01:16  lr: 0.000046  min_lr: 0.000001  loss: 1.8565 (2.0128)  loss_scale: 16384.0000 (26093.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1164 (7.7361)  time: 0.7988 (0.5186 -- 3.6914)  data: 0.0016 (0.0006 -- 0.0048)  max mem: 16413
Epoch: [25]  [100/160]  eta: 0:00:56  lr: 0.000046  min_lr: 0.000001  loss: 1.9016 (1.9982)  loss_scale: 16384.0000 (24170.4554)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9064 (7.6366)  time: 0.9058 (0.5334 -- 3.7514)  data: 0.0017 (0.0003 -- 0.0124)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000046  min_lr: 0.000001  loss: 1.9381 (1.9814)  loss_scale: 16384.0000 (22883.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7578 (7.5616)  time: 0.7773 (0.5297 -- 2.4357)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
[2023-08-31 12:07:44,710] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4125
[2023-08-31 12:07:44,710] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4125
[2023-08-31 12:07:44,710] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:07:44,710] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:07:44,710] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [25]  [140/160]  eta: 0:00:18  lr: 0.000046  min_lr: 0.000001  loss: 2.0426 (1.9832)  loss_scale: 8192.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6747 (7.6778)  time: 0.8173 (0.5267 -- 3.4871)  data: 0.0014 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000001  loss: 2.0339 (1.9766)  loss_scale: 8192.0000 (19507.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1494 (7.7447)  time: 0.7238 (0.4961 -- 2.7753)  data: 0.0064 (0.0002 -- 0.1093)  max mem: 16413
Epoch: [25] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000001  loss: 2.0339 (1.9776)  loss_scale: 8192.0000 (19507.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1494 (7.7447)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.7796 (0.7796)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4731 (2.4731 -- 2.4731)  data: 2.2190 (2.2190 -- 2.2190)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7796 (1.0085)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (94.9495)  time: 0.4294 (0.1783 -- 2.4731)  data: 0.2158 (0.0004 -- 2.2190)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9079 (0.9607)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (95.7672)  time: 0.2202 (0.1685 -- 0.3745)  data: 0.0177 (0.0001 -- 0.1966)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9634 (1.0124)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (94.6058)  time: 0.2063 (0.1320 -- 0.3745)  data: 0.0171 (0.0001 -- 0.1966)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 68.050 Acc@5 95.021 loss 0.969
Accuracy of the network on the 482 val images: 68.05%
[2023-08-31 12:08:16,936] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:08:16,938] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:08:16,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:08:16,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:08:18,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:08:18,349] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.05%
Epoch: [26]  [  0/160]  eta: 0:21:44  lr: 0.000046  min_lr: 0.000001  loss: 1.1741 (1.1741)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7074 (6.7074)  time: 8.1553 (8.1553 -- 8.1553)  data: 5.9750 (5.9750 -- 5.9750)  max mem: 16413
Epoch: [26]  [ 20/160]  eta: 0:02:42  lr: 0.000046  min_lr: 0.000001  loss: 2.0968 (2.0647)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6035 (8.2280)  time: 0.8083 (0.5330 -- 2.7311)  data: 0.0442 (0.0008 -- 0.5635)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:09  lr: 0.000046  min_lr: 0.000001  loss: 1.9248 (2.0238)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6209 (7.6842)  time: 1.0045 (0.5221 -- 3.1912)  data: 0.0369 (0.0002 -- 0.7114)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:40  lr: 0.000046  min_lr: 0.000001  loss: 2.0736 (2.0213)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7035 (7.4883)  time: 0.8464 (0.5438 -- 4.0467)  data: 0.0020 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000001  loss: 1.9342 (2.0032)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8660 (7.5379)  time: 0.9306 (0.5327 -- 3.3261)  data: 0.0013 (0.0003 -- 0.0027)  max mem: 16413
[2023-08-31 12:09:48,403] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:09:48,403] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 12:09:48,404] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:09:48,404] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [26]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000001  loss: 2.0280 (2.0003)  loss_scale: 8192.0000 (8759.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9944 (7.6301)  time: 0.7785 (0.5253 -- 2.7912)  data: 0.0017 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [26]  [120/160]  eta: 0:00:38  lr: 0.000045  min_lr: 0.000001  loss: 2.1213 (1.9999)  loss_scale: 16384.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3655 (7.6575)  time: 0.9844 (0.5377 -- 3.8082)  data: 0.0015 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 2.0202 (1.9908)  loss_scale: 16384.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9616 (7.6485)  time: 0.6802 (0.5022 -- 2.3305)  data: 0.0017 (0.0004 -- 0.0056)  max mem: 16413
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0616 (1.9935)  loss_scale: 16384.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2689 (7.7036)  time: 0.7376 (0.4960 -- 3.5665)  data: 0.0010 (0.0002 -- 0.0056)  max mem: 16413
Epoch: [26] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0616 (1.9745)  loss_scale: 16384.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2689 (7.7036)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.8921 (0.8921)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4722 (2.4722 -- 2.4722)  data: 2.2625 (2.2625 -- 2.2625)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7525 (1.0056)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (94.9495)  time: 0.4307 (0.2032 -- 2.4722)  data: 0.2170 (0.0003 -- 2.2625)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7525 (0.9587)  acc1: 66.6667 (69.3122)  acc5: 100.0000 (95.2381)  time: 0.2173 (0.1688 -- 0.3570)  data: 0.0127 (0.0001 -- 0.1258)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9235 (0.9967)  acc1: 66.6667 (68.4647)  acc5: 100.0000 (94.6058)  time: 0.2025 (0.1332 -- 0.3570)  data: 0.0122 (0.0001 -- 0.1258)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 70.954 Acc@5 94.813 loss 0.957
Accuracy of the network on the 482 val images: 70.95%
[2023-08-31 12:10:49,301] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:10:49,302] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:10:49,302] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:10:49,303] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:10:50,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:10:50,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.95%
Epoch: [27]  [  0/160]  eta: 0:22:13  lr: 0.000045  min_lr: 0.000001  loss: 2.0205 (2.0205)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1189 (5.1189)  time: 8.3368 (8.3368 -- 8.3368)  data: 4.3341 (4.3341 -- 4.3341)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:50  lr: 0.000045  min_lr: 0.000001  loss: 1.8695 (1.8893)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8980 (8.2317)  time: 0.8607 (0.5276 -- 3.8184)  data: 0.0016 (0.0004 -- 0.0060)  max mem: 16413
[2023-08-31 12:11:20,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4344
[2023-08-31 12:11:20,477] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:11:20,477] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4344
[2023-08-31 12:11:20,477] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:11:20,477] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [27]  [ 40/160]  eta: 0:02:10  lr: 0.000045  min_lr: 0.000001  loss: 1.8485 (1.8887)  loss_scale: 8192.0000 (12987.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9453 (7.6225)  time: 0.9516 (0.5259 -- 4.1448)  data: 0.0440 (0.0003 -- 0.8600)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000001  loss: 2.0591 (1.9081)  loss_scale: 8192.0000 (11415.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8462 (7.7951)  time: 0.8211 (0.5256 -- 3.5263)  data: 0.0013 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:17  lr: 0.000045  min_lr: 0.000001  loss: 1.9784 (1.9211)  loss_scale: 8192.0000 (10619.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5099 (7.7159)  time: 0.8748 (0.5164 -- 4.0112)  data: 0.0018 (0.0003 -- 0.0058)  max mem: 16413
Epoch: [27]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000001  loss: 2.0862 (1.9503)  loss_scale: 8192.0000 (10138.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7812 (7.7684)  time: 0.7215 (0.5242 -- 2.5750)  data: 0.0573 (0.0004 -- 0.9895)  max mem: 16413
Epoch: [27]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 1.9146 (1.9495)  loss_scale: 8192.0000 (9816.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5165 (8.0361)  time: 0.8871 (0.5273 -- 2.8755)  data: 0.0938 (0.0003 -- 1.0830)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 1.9842 (1.9456)  loss_scale: 8192.0000 (9586.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7994 (7.9585)  time: 0.8642 (0.5360 -- 2.9899)  data: 0.0806 (0.0001 -- 1.0629)  max mem: 16413
[2023-08-31 12:13:09,765] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:13:09,765] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 12:13:09,765] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:13:09,766] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0032 (1.9535)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5843 (8.0038)  time: 0.7282 (0.4944 -- 2.6747)  data: 0.0721 (0.0002 -- 1.4228)  max mem: 16413
Epoch: [27] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0032 (1.9599)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5843 (8.0038)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7510 (0.7510)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2755 (2.2755 -- 2.2755)  data: 2.0631 (2.0631 -- 2.0631)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.8167 (1.0445)  acc1: 66.6667 (68.6869)  acc5: 100.0000 (93.9394)  time: 0.4111 (0.2026 -- 2.2755)  data: 0.1969 (0.0005 -- 2.0631)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8634 (0.9893)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.2381)  time: 0.2250 (0.1708 -- 0.5033)  data: 0.0211 (0.0001 -- 0.3163)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9732 (1.0383)  acc1: 55.5556 (63.4855)  acc5: 100.0000 (94.6058)  time: 0.2086 (0.1323 -- 0.5033)  data: 0.0207 (0.0001 -- 0.3163)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 67.635 Acc@5 95.228 loss 0.988
Accuracy of the network on the 482 val images: 67.63%
Max accuracy: 70.95%
Epoch: [28]  [  0/160]  eta: 0:17:06  lr: 0.000045  min_lr: 0.000001  loss: 2.0064 (2.0064)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7361 (9.7361)  time: 6.4178 (6.4178 -- 6.4178)  data: 5.4205 (5.4205 -- 5.4205)  max mem: 16413
Epoch: [28]  [ 20/160]  eta: 0:02:44  lr: 0.000045  min_lr: 0.000001  loss: 1.8143 (1.8169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8681 (7.7604)  time: 0.9142 (0.5268 -- 3.0026)  data: 0.2449 (0.0010 -- 2.4563)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:02:09  lr: 0.000045  min_lr: 0.000001  loss: 2.0817 (1.8955)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2497 (7.8983)  time: 0.9827 (0.5134 -- 4.0191)  data: 0.0604 (0.0004 -- 0.6093)  max mem: 16413
[2023-08-31 12:14:17,213] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4536
[2023-08-31 12:14:17,213] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:14:17,213] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 12:14:17,213] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4536
[2023-08-31 12:14:17,213] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [28]  [ 60/160]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000001  loss: 1.8207 (1.8826)  loss_scale: 16384.0000 (15712.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4885 (7.7258)  time: 0.7223 (0.5265 -- 2.8276)  data: 0.0664 (0.0001 -- 1.2941)  max mem: 16413
Epoch: [28]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000001  loss: 1.9194 (1.8966)  loss_scale: 8192.0000 (13855.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0353 (7.8790)  time: 0.9413 (0.5303 -- 3.6581)  data: 0.3490 (0.0007 -- 3.1110)  max mem: 16413
Epoch: [28]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000001  loss: 2.0004 (1.9254)  loss_scale: 8192.0000 (12734.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2755 (8.3101)  time: 0.8442 (0.5273 -- 4.5576)  data: 0.0016 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 1.8741 (1.9206)  loss_scale: 8192.0000 (11983.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4768 (8.3320)  time: 0.8625 (0.5321 -- 3.3659)  data: 0.0016 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 1.9794 (1.9199)  loss_scale: 8192.0000 (11445.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9303 (8.4393)  time: 0.8305 (0.5151 -- 3.8504)  data: 0.0105 (0.0001 -- 0.1751)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9239 (1.9185)  loss_scale: 8192.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5080 (8.4912)  time: 0.6902 (0.4941 -- 3.5204)  data: 0.0005 (0.0002 -- 0.0011)  max mem: 16413
Epoch: [28] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9239 (1.9408)  loss_scale: 8192.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5080 (8.4912)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.7476 (0.7476)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4400 (2.4400 -- 2.4400)  data: 2.1985 (2.1985 -- 2.1985)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8129 (1.0298)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4162 (0.2001 -- 2.4400)  data: 0.2010 (0.0007 -- 2.1985)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8167 (0.9591)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (96.2963)  time: 0.2175 (0.1701 -- 0.4108)  data: 0.0122 (0.0001 -- 0.2288)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8928 (1.0038)  acc1: 66.6667 (64.7303)  acc5: 100.0000 (95.4357)  time: 0.2026 (0.1329 -- 0.4108)  data: 0.0119 (0.0001 -- 0.2288)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 70.332 Acc@5 95.851 loss 0.949
Accuracy of the network on the 482 val images: 70.33%
Max accuracy: 70.95%
Epoch: [29]  [  0/160]  eta: 0:19:48  lr: 0.000045  min_lr: 0.000001  loss: 2.5770 (2.5770)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3052 (6.3052)  time: 7.4284 (7.4284 -- 7.4284)  data: 6.8879 (6.8879 -- 6.8879)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:37  lr: 0.000045  min_lr: 0.000001  loss: 2.1284 (2.0978)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4825 (8.1978)  time: 0.8124 (0.5252 -- 1.7168)  data: 0.2054 (0.0003 -- 1.1702)  max mem: 16413
[2023-08-31 12:16:18,635] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:16:18,635] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:16:18,636] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 12:16:18,636] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [29]  [ 40/160]  eta: 0:02:06  lr: 0.000045  min_lr: 0.000001  loss: 1.8199 (1.9940)  loss_scale: 16384.0000 (11388.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9102 (7.8765)  time: 0.9721 (0.5295 -- 2.6888)  data: 0.3803 (0.0003 -- 2.1569)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:36  lr: 0.000045  min_lr: 0.000001  loss: 1.8189 (2.0031)  loss_scale: 16384.0000 (13026.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8359 (8.0108)  time: 0.7951 (0.5323 -- 3.4618)  data: 0.2383 (0.0003 -- 2.9120)  max mem: 16413
Epoch: [29]  [ 80/160]  eta: 0:01:16  lr: 0.000045  min_lr: 0.000001  loss: 1.9054 (1.9827)  loss_scale: 16384.0000 (13855.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2794 (8.1616)  time: 0.9069 (0.5358 -- 3.6093)  data: 0.3546 (0.0007 -- 3.0831)  max mem: 16413
Epoch: [29]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000001  loss: 1.8701 (1.9627)  loss_scale: 16384.0000 (14356.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3956 (8.0711)  time: 0.8863 (0.5341 -- 4.1697)  data: 0.3293 (0.0004 -- 3.6213)  max mem: 16413
Epoch: [29]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000001  loss: 1.9047 (1.9534)  loss_scale: 16384.0000 (14691.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1566 (8.1093)  time: 0.8629 (0.5270 -- 2.9515)  data: 0.3153 (0.0007 -- 2.3926)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 1.8587 (1.9402)  loss_scale: 16384.0000 (14931.5177)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4742 (8.1540)  time: 0.8013 (0.5246 -- 3.7269)  data: 0.0599 (0.0005 -- 1.1624)  max mem: 16413
[2023-08-31 12:18:09,347] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:18:09,347] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:18:09,347] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:18:09,348] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0264 (1.9495)  loss_scale: 16384.0000 (15820.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1264 (8.0978)  time: 0.7973 (0.4953 -- 3.2338)  data: 0.0008 (0.0001 -- 0.0036)  max mem: 16413
Epoch: [29] Total time: 0:02:23 (0.8975 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0264 (1.9287)  loss_scale: 16384.0000 (15820.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1264 (8.0978)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.7763 (0.7763)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.6822 (2.6822 -- 2.6822)  data: 2.4385 (2.4385 -- 2.4385)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7763 (0.9637)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (94.9495)  time: 0.4432 (0.1977 -- 2.6822)  data: 0.2290 (0.0007 -- 2.4385)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8247 (0.9155)  acc1: 77.7778 (71.9577)  acc5: 100.0000 (95.7672)  time: 0.2104 (0.1697 -- 0.3013)  data: 0.0098 (0.0001 -- 0.1139)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8750 (0.9547)  acc1: 66.6667 (70.1245)  acc5: 100.0000 (95.4357)  time: 0.1949 (0.1320 -- 0.3013)  data: 0.0095 (0.0001 -- 0.1139)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 72.614 Acc@5 96.058 loss 0.908
Accuracy of the network on the 482 val images: 72.61%
[2023-08-31 12:18:21,611] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:18:21,612] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:18:21,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:18:21,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:18:23,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:18:23,004] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 72.61%
Epoch: [30]  [  0/160]  eta: 0:16:06  lr: 0.000045  min_lr: 0.000001  loss: 1.7135 (1.7135)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6523 (7.6523)  time: 6.0424 (6.0424 -- 6.0424)  data: 5.5191 (5.5191 -- 5.5191)  max mem: 16413
[2023-08-31 12:18:35,081] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4806
[2023-08-31 12:18:35,081] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4806
[2023-08-31 12:18:35,081] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:18:35,081] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:18:35,082] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [ 20/160]  eta: 0:03:04  lr: 0.000045  min_lr: 0.000001  loss: 1.7780 (1.8751)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0523 (7.8316)  time: 1.0815 (0.5184 -- 5.2220)  data: 0.4565 (0.0001 -- 4.7015)  max mem: 16413
Epoch: [30]  [ 40/160]  eta: 0:02:05  lr: 0.000045  min_lr: 0.000001  loss: 1.9867 (1.9006)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8787 (7.8257)  time: 0.7671 (0.5128 -- 4.4388)  data: 0.2256 (0.0003 -- 3.9362)  max mem: 16413
Epoch: [30]  [ 60/160]  eta: 0:01:44  lr: 0.000045  min_lr: 0.000001  loss: 1.8932 (1.8757)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9229 (7.9821)  time: 1.0364 (0.5084 -- 4.6662)  data: 0.1479 (0.0004 -- 1.9057)  max mem: 16413
Epoch: [30]  [ 80/160]  eta: 0:01:17  lr: 0.000045  min_lr: 0.000001  loss: 1.6709 (1.8402)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4603 (8.1650)  time: 0.7371 (0.5070 -- 3.0617)  data: 0.0015 (0.0002 -- 0.0108)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:57  lr: 0.000045  min_lr: 0.000001  loss: 1.9653 (1.8643)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1675 (7.9947)  time: 0.9071 (0.5243 -- 4.3374)  data: 0.0018 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000045  min_lr: 0.000001  loss: 1.9705 (1.8786)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3526 (8.2071)  time: 0.8654 (0.5292 -- 2.6480)  data: 0.0014 (0.0003 -- 0.0037)  max mem: 16413
[2023-08-31 12:20:28,166] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:20:28,167] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:20:28,169] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:20:28,170] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:20:30,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4938
[2023-08-31 12:20:30,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4938
[2023-08-31 12:20:30,158] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:20:30,158] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:20:30,158] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 1.9942 (1.8978)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7446 (8.1063)  time: 0.8147 (0.5168 -- 2.5574)  data: 0.0157 (0.0004 -- 0.2811)  max mem: 16413
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9467 (1.9075)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0716 (8.1060)  time: 0.8047 (0.4952 -- 2.5574)  data: 0.0010 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [30] Total time: 0:02:23 (0.8987 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9467 (1.8960)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0716 (8.1060)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.6751 (0.6751)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.6307 (2.6307 -- 2.6307)  data: 2.4160 (2.4160 -- 2.4160)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7072 (0.9582)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (95.9596)  time: 0.4432 (0.1942 -- 2.6307)  data: 0.2296 (0.0005 -- 2.4160)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8014 (0.9137)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (95.7672)  time: 0.2145 (0.1699 -- 0.3207)  data: 0.0119 (0.0001 -- 0.1259)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8853 (0.9629)  acc1: 66.6667 (70.1245)  acc5: 100.0000 (95.4357)  time: 0.1996 (0.1326 -- 0.3207)  data: 0.0116 (0.0001 -- 0.1259)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 74.066 Acc@5 96.058 loss 0.906
Accuracy of the network on the 482 val images: 74.07%
[2023-08-31 12:20:54,709] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:20:54,711] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:20:54,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:20:54,711] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:20:55,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:20:55,853] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 74.07%
Epoch: [31]  [  0/160]  eta: 0:19:00  lr: 0.000045  min_lr: 0.000001  loss: 1.1969 (1.1969)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2497 (8.2497)  time: 7.1300 (7.1300 -- 7.1300)  data: 6.6047 (6.6047 -- 6.6047)  max mem: 16413
Epoch: [31]  [ 20/160]  eta: 0:02:48  lr: 0.000045  min_lr: 0.000001  loss: 2.0184 (1.8786)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8122 (8.5894)  time: 0.9063 (0.5238 -- 2.7910)  data: 0.0789 (0.0003 -- 1.2474)  max mem: 16413
[2023-08-31 12:21:36,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=24, lr=[1.0646386668957208e-06, 1.0646386668957208e-06, 1.4195182225276278e-06, 1.4195182225276278e-06, 1.8926909633701704e-06, 1.8926909633701704e-06, 2.523587951160227e-06, 2.523587951160227e-06, 3.364783934880303e-06, 3.364783934880303e-06, 4.486378579840404e-06, 4.486378579840404e-06, 5.9818381064538724e-06, 5.9818381064538724e-06, 7.975784141938495e-06, 7.975784141938495e-06, 1.0634378855917995e-05, 1.0634378855917995e-05, 1.4179171807890659e-05, 1.4179171807890659e-05, 1.890556241052088e-05, 1.890556241052088e-05, 2.5207416547361173e-05, 2.5207416547361173e-05, 3.3609888729814896e-05, 3.3609888729814896e-05, 4.481318497308653e-05, 4.481318497308653e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 12:21:36,843] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=16.56086070539386, CurrSamplesPerSec=21.974193283617602, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:02:01  lr: 0.000045  min_lr: 0.000001  loss: 1.9895 (1.9426)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7016 (8.1025)  time: 0.8171 (0.5296 -- 2.4845)  data: 0.0352 (0.0005 -- 0.4482)  max mem: 16413
Epoch: [31]  [ 60/160]  eta: 0:01:40  lr: 0.000045  min_lr: 0.000001  loss: 1.8399 (1.9291)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2146 (7.9570)  time: 0.9904 (0.5251 -- 4.7997)  data: 0.0018 (0.0004 -- 0.0094)  max mem: 16413
Epoch: [31]  [ 80/160]  eta: 0:01:15  lr: 0.000045  min_lr: 0.000001  loss: 1.9887 (1.9280)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5785 (8.1181)  time: 0.7711 (0.5140 -- 2.7879)  data: 0.0012 (0.0003 -- 0.0020)  max mem: 16413
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000045  min_lr: 0.000001  loss: 2.0799 (1.9536)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3895 (8.0229)  time: 0.8206 (0.5246 -- 2.8450)  data: 0.0017 (0.0002 -- 0.0061)  max mem: 16413
[2023-08-31 12:22:35,015] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:22:35,015] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:22:35,015] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:22:35,015] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:22:36,710] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5070
[2023-08-31 12:22:36,710] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5070
[2023-08-31 12:22:36,710] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:22:36,710] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:22:36,710] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 1.9124 (1.9549)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6784 (7.9506)  time: 0.7978 (0.5323 -- 2.3558)  data: 0.0016 (0.0005 -- 0.0049)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:17  lr: 0.000045  min_lr: 0.000001  loss: 1.7321 (1.9370)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2552 (7.9476)  time: 0.8719 (0.5330 -- 2.0997)  data: 0.0012 (0.0005 -- 0.0025)  max mem: 16413
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 2.0214 (1.9348)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5576 (7.9041)  time: 0.7455 (0.4973 -- 2.8057)  data: 0.0200 (0.0002 -- 0.3846)  max mem: 16413
Epoch: [31] Total time: 0:02:20 (0.8812 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 2.0214 (1.9362)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5576 (7.9041)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.8036 (0.8036)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4561 (2.4561 -- 2.4561)  data: 2.1932 (2.1932 -- 2.1932)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8036 (0.9698)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (94.9495)  time: 0.4314 (0.2021 -- 2.4561)  data: 0.2058 (0.0006 -- 2.1932)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7856 (0.9250)  acc1: 66.6667 (68.2540)  acc5: 100.0000 (95.2381)  time: 0.2220 (0.1700 -- 0.3975)  data: 0.0142 (0.0001 -- 0.2100)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9314 (0.9634)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (94.6058)  time: 0.2056 (0.1324 -- 0.3975)  data: 0.0139 (0.0001 -- 0.2100)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 71.784 Acc@5 95.643 loss 0.909
Accuracy of the network on the 482 val images: 71.78%
Max accuracy: 74.07%
Epoch: [32]  [  0/160]  eta: 0:24:07  lr: 0.000045  min_lr: 0.000001  loss: 2.3773 (2.3773)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8743 (5.8743)  time: 9.0472 (9.0472 -- 9.0472)  data: 7.6006 (7.6006 -- 7.6006)  max mem: 16413
Epoch: [32]  [ 20/160]  eta: 0:02:58  lr: 0.000045  min_lr: 0.000001  loss: 1.8808 (1.9103)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2041 (7.6743)  time: 0.8842 (0.5142 -- 4.4061)  data: 0.1648 (0.0004 -- 1.9404)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:06  lr: 0.000045  min_lr: 0.000001  loss: 1.9628 (1.9549)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7489 (7.8374)  time: 0.8222 (0.5191 -- 3.9187)  data: 0.0413 (0.0001 -- 0.5674)  max mem: 16413
Epoch: [32]  [ 60/160]  eta: 0:01:42  lr: 0.000045  min_lr: 0.000001  loss: 1.8966 (1.9373)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4352 (7.7747)  time: 0.9553 (0.5147 -- 5.6410)  data: 0.0609 (0.0002 -- 1.1967)  max mem: 16413
[2023-08-31 12:24:40,308] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:24:40,308] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:24:40,308] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:24:40,308] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [32]  [ 80/160]  eta: 0:01:18  lr: 0.000045  min_lr: 0.000001  loss: 1.9200 (1.9255)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5255 (7.7902)  time: 0.8639 (0.5230 -- 3.9798)  data: 0.0016 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:56  lr: 0.000045  min_lr: 0.000001  loss: 2.0839 (1.9453)  loss_scale: 32768.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1787 (8.0518)  time: 0.8005 (0.5165 -- 2.8814)  data: 0.0248 (0.0004 -- 0.3679)  max mem: 16413
[2023-08-31 12:25:03,026] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5224
[2023-08-31 12:25:03,026] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5224
[2023-08-31 12:25:03,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:25:03,027] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 12:25:03,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [32]  [120/160]  eta: 0:00:36  lr: 0.000045  min_lr: 0.000001  loss: 2.0964 (1.9655)  loss_scale: 16384.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4936 (8.1371)  time: 0.7995 (0.5215 -- 2.1020)  data: 0.2151 (0.0002 -- 1.5646)  max mem: 16413
Epoch: [32]  [140/160]  eta: 0:00:18  lr: 0.000045  min_lr: 0.000001  loss: 1.8344 (1.9616)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7809 (8.2022)  time: 0.8957 (0.5297 -- 2.4065)  data: 0.2974 (0.0005 -- 1.8888)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000001  loss: 1.9997 (1.9577)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2242 (8.1486)  time: 0.7139 (0.4951 -- 2.5145)  data: 0.1770 (0.0002 -- 2.0203)  max mem: 16413
Epoch: [32] Total time: 0:02:23 (0.8953 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000001  loss: 1.9997 (1.9242)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2242 (8.1486)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.7131 (0.7131)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4966 (2.4966 -- 2.4966)  data: 2.2841 (2.2841 -- 2.2841)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7131 (0.9428)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4341 (0.1872 -- 2.4966)  data: 0.2256 (0.0006 -- 2.2841)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6906 (0.8976)  acc1: 77.7778 (68.7831)  acc5: 100.0000 (95.7672)  time: 0.2264 (0.1691 -- 0.5140)  data: 0.0270 (0.0001 -- 0.3400)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8320 (0.9434)  acc1: 66.6667 (66.8050)  acc5: 100.0000 (95.0207)  time: 0.2125 (0.1323 -- 0.5140)  data: 0.0267 (0.0001 -- 0.3400)  max mem: 16413
Val: Total time: 0:00:08 (0.2965 s / it)
* Acc@1 71.784 Acc@5 95.436 loss 0.883
Accuracy of the network on the 482 val images: 71.78%
Max accuracy: 74.07%
Epoch: [33]  [  0/160]  eta: 0:21:27  lr: 0.000045  min_lr: 0.000001  loss: 2.1372 (2.1372)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7535 (7.7535)  time: 8.0493 (8.0493 -- 8.0493)  data: 5.1605 (5.1605 -- 5.1605)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:47  lr: 0.000045  min_lr: 0.000001  loss: 2.0066 (1.9585)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2730 (8.1721)  time: 0.8503 (0.5179 -- 3.3446)  data: 0.0026 (0.0004 -- 0.0154)  max mem: 16413
Epoch: [33]  [ 40/160]  eta: 0:02:05  lr: 0.000044  min_lr: 0.000001  loss: 1.8530 (1.9224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3283 (8.0111)  time: 0.8908 (0.5128 -- 3.5949)  data: 0.0291 (0.0004 -- 0.5528)  max mem: 16413
Epoch: [33]  [ 60/160]  eta: 0:01:37  lr: 0.000044  min_lr: 0.000001  loss: 2.1049 (1.9503)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8917 (8.2010)  time: 0.8281 (0.5314 -- 2.6063)  data: 0.0961 (0.0003 -- 1.3809)  max mem: 16413
[2023-08-31 12:27:08,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:27:08,303] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:27:08,343] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:27:08,343] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000001  loss: 1.9240 (1.9480)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4475 (8.1380)  time: 0.9244 (0.5339 -- 3.6991)  data: 0.0015 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [33]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000001  loss: 1.7992 (1.9366)  loss_scale: 32768.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9234 (8.1788)  time: 0.8752 (0.5144 -- 4.4830)  data: 0.0017 (0.0004 -- 0.0051)  max mem: 16413
[2023-08-31 12:27:31,942] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5381
[2023-08-31 12:27:31,942] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5381
[2023-08-31 12:27:31,943] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:27:31,943] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:27:31,943] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [120/160]  eta: 0:00:37  lr: 0.000044  min_lr: 0.000001  loss: 1.9543 (1.9292)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1231 (8.2850)  time: 0.8971 (0.5099 -- 4.6776)  data: 0.0013 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 1.7785 (1.9097)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7987 (8.2183)  time: 0.6807 (0.5339 -- 2.2254)  data: 0.0019 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8895 (1.9012)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5696 (8.3426)  time: 0.7255 (0.4961 -- 2.2796)  data: 0.1959 (0.0002 -- 1.7581)  max mem: 16413
Epoch: [33] Total time: 0:02:20 (0.8810 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.8895 (1.8913)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5696 (8.3426)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.6648 (0.6648)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5821 (2.5821 -- 2.5821)  data: 2.3335 (2.3335 -- 2.3335)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6648 (0.9514)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4404 (0.1949 -- 2.5821)  data: 0.2256 (0.0007 -- 2.3335)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7451 (0.8842)  acc1: 77.7778 (71.4286)  acc5: 100.0000 (96.2963)  time: 0.2171 (0.1712 -- 0.3486)  data: 0.0142 (0.0001 -- 0.1377)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8149 (0.9311)  acc1: 66.6667 (68.8797)  acc5: 100.0000 (95.8506)  time: 0.2032 (0.1336 -- 0.3486)  data: 0.0139 (0.0001 -- 0.1377)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 73.444 Acc@5 96.058 loss 0.878
Accuracy of the network on the 482 val images: 73.44%
Max accuracy: 74.07%
Epoch: [34]  [  0/160]  eta: 0:18:40  lr: 0.000044  min_lr: 0.000001  loss: 1.7556 (1.7556)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9950 (9.9950)  time: 7.0020 (7.0020 -- 7.0020)  data: 5.2040 (5.2040 -- 5.2040)  max mem: 16413
[2023-08-31 12:28:43,560] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5455
[2023-08-31 12:28:43,561] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:28:43,561] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5455
[2023-08-31 12:28:43,561] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:28:43,561] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [34]  [ 20/160]  eta: 0:02:54  lr: 0.000044  min_lr: 0.000001  loss: 1.9495 (1.9396)  loss_scale: 16384.0000 (14043.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8571 (9.3150)  time: 0.9564 (0.5141 -- 3.2352)  data: 0.0616 (0.0005 -- 0.7064)  max mem: 16413
Epoch: [34]  [ 40/160]  eta: 0:02:08  lr: 0.000044  min_lr: 0.000001  loss: 1.9143 (1.9192)  loss_scale: 8192.0000 (11189.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8179 (8.7043)  time: 0.8823 (0.5134 -- 4.3664)  data: 0.0012 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [34]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000001  loss: 1.9686 (1.9045)  loss_scale: 8192.0000 (10206.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9023 (8.2371)  time: 0.8027 (0.5301 -- 2.0903)  data: 0.0018 (0.0005 -- 0.0059)  max mem: 16413
Epoch: [34]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000001  loss: 1.9141 (1.9253)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9774 (8.2807)  time: 0.8802 (0.5181 -- 2.3834)  data: 0.0952 (0.0002 -- 1.8674)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000001  loss: 1.9276 (1.9183)  loss_scale: 8192.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9926 (8.1374)  time: 0.9104 (0.5214 -- 3.4129)  data: 0.3619 (0.0002 -- 2.9029)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 1.7859 (1.8860)  loss_scale: 8192.0000 (9207.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4660 (8.1325)  time: 0.8111 (0.5185 -- 2.1896)  data: 0.1611 (0.0003 -- 1.4995)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 1.9024 (1.8780)  loss_scale: 8192.0000 (9063.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3159 (8.1364)  time: 0.8691 (0.5243 -- 1.8344)  data: 0.1314 (0.0003 -- 1.0841)  max mem: 16413
[2023-08-31 12:30:36,390] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:30:36,390] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:30:36,391] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 12:30:36,391] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.9894 (1.8833)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9209 (8.2066)  time: 0.6654 (0.4968 -- 1.8233)  data: 0.0366 (0.0003 -- 0.7035)  max mem: 16413
Epoch: [34] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.9894 (1.9153)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9209 (8.2066)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.6368 (0.6368)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2898 (2.2898 -- 2.2898)  data: 2.0833 (2.0833 -- 2.0833)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6412 (0.9372)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (95.9596)  time: 0.4228 (0.1984 -- 2.2898)  data: 0.2096 (0.0004 -- 2.0833)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7386 (0.8779)  acc1: 66.6667 (71.4286)  acc5: 100.0000 (96.2963)  time: 0.2287 (0.1693 -- 0.4592)  data: 0.0248 (0.0001 -- 0.2708)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8315 (0.9207)  acc1: 66.6667 (69.7095)  acc5: 100.0000 (95.8506)  time: 0.2131 (0.1334 -- 0.4592)  data: 0.0246 (0.0001 -- 0.2708)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 73.859 Acc@5 96.266 loss 0.861
Accuracy of the network on the 482 val images: 73.86%
Max accuracy: 74.07%
[2023-08-31 12:31:01,441] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5600
[2023-08-31 12:31:01,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:31:01,441] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5600
[2023-08-31 12:31:01,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 12:31:01,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [35]  [  0/160]  eta: 0:19:49  lr: 0.000044  min_lr: 0.000001  loss: 1.5603 (1.5603)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6154 (8.6154)  time: 7.4346 (7.4346 -- 7.4346)  data: 6.9149 (6.9149 -- 6.9149)  max mem: 16413
Epoch: [35]  [ 20/160]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000001  loss: 1.8069 (1.7765)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0535 (7.6500)  time: 0.8254 (0.5304 -- 4.3791)  data: 0.1960 (0.0006 -- 2.9825)  max mem: 16413
Epoch: [35]  [ 40/160]  eta: 0:02:06  lr: 0.000044  min_lr: 0.000001  loss: 2.0201 (1.9023)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4333 (8.3912)  time: 0.9576 (0.5276 -- 2.5679)  data: 0.2056 (0.0003 -- 1.9913)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:40  lr: 0.000044  min_lr: 0.000001  loss: 1.9005 (1.8880)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7162 (8.2891)  time: 0.9032 (0.5184 -- 3.4286)  data: 0.1454 (0.0001 -- 2.8794)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000001  loss: 1.6434 (1.8737)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2419 (8.4176)  time: 0.7503 (0.5320 -- 2.6626)  data: 0.0023 (0.0001 -- 0.0150)  max mem: 16413
Epoch: [35]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000001  loss: 1.9977 (1.8937)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5956 (8.4141)  time: 0.9307 (0.5325 -- 4.2184)  data: 0.0016 (0.0002 -- 0.0035)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 1.8945 (1.8839)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5724 (8.3080)  time: 0.8388 (0.5284 -- 3.0500)  data: 0.0836 (0.0004 -- 0.9753)  max mem: 16413
[2023-08-31 12:32:54,013] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:32:54,013] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:32:54,014] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 12:32:54,014] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [35]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 1.9640 (1.8938)  loss_scale: 16384.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4343 (8.3403)  time: 0.9676 (0.5160 -- 3.8886)  data: 0.0015 (0.0001 -- 0.0079)  max mem: 16413
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8751 (1.9032)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8132 (8.3836)  time: 0.6208 (0.4957 -- 1.7252)  data: 0.0006 (0.0001 -- 0.0013)  max mem: 16413
Epoch: [35] Total time: 0:02:22 (0.8925 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.8751 (1.8882)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8132 (8.3836)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5892 (0.5892)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3984 (2.3984 -- 2.3984)  data: 2.1677 (2.1677 -- 2.1677)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7573 (0.9604)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (93.9394)  time: 0.4288 (0.2061 -- 2.3984)  data: 0.2104 (0.0007 -- 2.1677)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7447 (0.8811)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (95.2381)  time: 0.2226 (0.1708 -- 0.3509)  data: 0.0144 (0.0001 -- 0.1369)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8342 (0.9328)  acc1: 66.6667 (70.5394)  acc5: 100.0000 (95.0207)  time: 0.2062 (0.1326 -- 0.3509)  data: 0.0136 (0.0001 -- 0.1369)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 75.104 Acc@5 95.851 loss 0.875
Accuracy of the network on the 482 val images: 75.10%
[2023-08-31 12:33:24,656] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:33:24,658] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:33:24,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:33:24,658] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:33:26,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:33:26,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.10%
Epoch: [36]  [  0/160]  eta: 0:20:50  lr: 0.000044  min_lr: 0.000001  loss: 1.9729 (1.9729)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3183 (7.3183)  time: 7.8162 (7.8162 -- 7.8162)  data: 7.2884 (7.2884 -- 7.2884)  max mem: 16413
Epoch: [36]  [ 20/160]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000001  loss: 1.6669 (1.7203)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3632 (8.2283)  time: 0.8067 (0.5176 -- 2.9777)  data: 0.1941 (0.0003 -- 2.4215)  max mem: 16413
Epoch: [36]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8963 (1.8069)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6042 (8.3017)  time: 0.8604 (0.5361 -- 3.9641)  data: 0.2678 (0.0004 -- 3.4106)  max mem: 16413
Epoch: [36]  [ 60/160]  eta: 0:01:38  lr: 0.000044  min_lr: 0.000001  loss: 1.7771 (1.8040)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0512 (8.2443)  time: 0.9381 (0.5299 -- 4.2773)  data: 0.0016 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000001  loss: 2.0126 (1.8512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0502 (8.0123)  time: 0.8326 (0.5189 -- 3.6537)  data: 0.0013 (0.0003 -- 0.0024)  max mem: 16413
[2023-08-31 12:35:01,617] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:35:01,618] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:35:01,619] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:35:01,619] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [100/160]  eta: 0:00:57  lr: 0.000044  min_lr: 0.000001  loss: 2.0801 (1.8864)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2633 (8.1272)  time: 1.0269 (0.5170 -- 4.5015)  data: 0.0012 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 2.1480 (1.9270)  loss_scale: 32768.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8830 (8.2753)  time: 0.7269 (0.5170 -- 3.1457)  data: 0.0013 (0.0001 -- 0.0030)  max mem: 16413
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 1.6652 (1.9166)  loss_scale: 32768.0000 (21496.7376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3518 (8.2042)  time: 0.9872 (0.5243 -- 3.8436)  data: 0.0014 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 2.0123 (1.9274)  loss_scale: 32768.0000 (22835.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6315 (8.1236)  time: 0.6251 (0.4955 -- 2.6726)  data: 0.0005 (0.0002 -- 0.0015)  max mem: 16413
Epoch: [36] Total time: 0:02:23 (0.8960 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 2.0123 (1.9069)  loss_scale: 32768.0000 (22835.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6315 (8.1236)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.5759 (0.5759)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3650 (2.3650 -- 2.3650)  data: 2.1454 (2.1454 -- 2.1454)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6700 (0.9437)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (94.9495)  time: 0.4276 (0.2006 -- 2.3650)  data: 0.2170 (0.0003 -- 2.1454)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7373 (0.8720)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (95.7672)  time: 0.2308 (0.1694 -- 0.5383)  data: 0.0303 (0.0001 -- 0.3610)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8608 (0.9213)  acc1: 77.7778 (71.7842)  acc5: 100.0000 (95.4357)  time: 0.2154 (0.1324 -- 0.5383)  data: 0.0300 (0.0001 -- 0.3610)  max mem: 16413
Val: Total time: 0:00:07 (0.2950 s / it)
* Acc@1 75.726 Acc@5 96.058 loss 0.870
Accuracy of the network on the 482 val images: 75.73%
[2023-08-31 12:35:57,405] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:35:57,407] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:35:57,407] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:35:57,407] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:35:58,850] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:35:58,851] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.73%
Epoch: [37]  [  0/160]  eta: 0:18:21  lr: 0.000044  min_lr: 0.000001  loss: 2.3096 (2.3096)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9672 (8.9672)  time: 6.8814 (6.8814 -- 6.8814)  data: 5.9051 (5.9051 -- 5.9051)  max mem: 16413
[2023-08-31 12:36:23,908] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5940
[2023-08-31 12:36:23,908] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:36:23,908] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5940
[2023-08-31 12:36:23,909] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:36:23,909] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [37]  [ 20/160]  eta: 0:02:47  lr: 0.000044  min_lr: 0.000001  loss: 1.9541 (1.9642)  loss_scale: 32768.0000 (31987.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6774 (7.1368)  time: 0.9086 (0.5125 -- 2.7841)  data: 0.1562 (0.0004 -- 1.1932)  max mem: 16413
Epoch: [37]  [ 40/160]  eta: 0:02:03  lr: 0.000044  min_lr: 0.000001  loss: 1.8379 (1.9270)  loss_scale: 16384.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5766 (7.1697)  time: 0.8618 (0.5192 -- 3.0270)  data: 0.1005 (0.0004 -- 1.5315)  max mem: 16413
Epoch: [37]  [ 60/160]  eta: 0:01:39  lr: 0.000044  min_lr: 0.000001  loss: 1.9880 (1.9431)  loss_scale: 16384.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5971 (7.6448)  time: 0.9229 (0.5174 -- 4.0013)  data: 0.3671 (0.0003 -- 3.2069)  max mem: 16413
[2023-08-31 12:37:16,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=30, lr=[1.0391258399005154e-06, 1.0391258399005154e-06, 1.3855011198673537e-06, 1.3855011198673537e-06, 1.847334826489805e-06, 1.847334826489805e-06, 2.463113101986407e-06, 2.463113101986407e-06, 3.2841508026485425e-06, 3.2841508026485425e-06, 4.378867736864724e-06, 4.378867736864724e-06, 5.838490315819631e-06, 5.838490315819631e-06, 7.784653754426174e-06, 7.784653754426174e-06, 1.03795383392349e-05, 1.03795383392349e-05, 1.38393844523132e-05, 1.38393844523132e-05, 1.8452512603084266e-05, 1.8452512603084266e-05, 2.4603350137445687e-05, 2.4603350137445687e-05, 3.2804466849927585e-05, 3.2804466849927585e-05, 4.373928913323678e-05, 4.373928913323678e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 12:37:16,032] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=16.45205796793254, CurrSamplesPerSec=22.078654110364212, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:16  lr: 0.000044  min_lr: 0.000001  loss: 1.9078 (1.9246)  loss_scale: 16384.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0536 (7.7214)  time: 0.8475 (0.5069 -- 4.1229)  data: 0.3073 (0.0004 -- 3.6057)  max mem: 16413
Epoch: [37]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000001  loss: 1.9501 (1.9305)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8457 (7.7798)  time: 0.8549 (0.5211 -- 2.7358)  data: 0.3123 (0.0006 -- 2.2231)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:36  lr: 0.000044  min_lr: 0.000001  loss: 1.9201 (1.9169)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7874 (8.1650)  time: 0.8229 (0.5205 -- 3.2690)  data: 0.2706 (0.0003 -- 2.7398)  max mem: 16413
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000044  min_lr: 0.000001  loss: 1.8122 (1.9004)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4350 (8.0747)  time: 0.8853 (0.5371 -- 2.9268)  data: 0.3028 (0.0005 -- 1.5964)  max mem: 16413
[2023-08-31 12:38:15,550] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:38:15,551] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:38:15,552] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:38:15,552] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000001  loss: 1.8618 (1.8993)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2506 (8.1012)  time: 0.6727 (0.4953 -- 3.2253)  data: 0.0280 (0.0002 -- 0.5440)  max mem: 16413
Epoch: [37] Total time: 0:02:21 (0.8869 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000001  loss: 1.8618 (1.9098)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2506 (8.1012)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.5152 (0.5152)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4593 (2.4593 -- 2.4593)  data: 2.2444 (2.2444 -- 2.2444)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6269 (0.8806)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (94.9495)  time: 0.4185 (0.1977 -- 2.4593)  data: 0.2086 (0.0008 -- 2.2444)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7122 (0.8376)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (95.7672)  time: 0.2200 (0.1691 -- 0.3638)  data: 0.0185 (0.0001 -- 0.1754)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8128 (0.8889)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.4357)  time: 0.2054 (0.1327 -- 0.3638)  data: 0.0182 (0.0001 -- 0.1754)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 76.349 Acc@5 96.266 loss 0.837
Accuracy of the network on the 482 val images: 76.35%
[2023-08-31 12:38:28,599] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:38:28,601] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:38:28,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:38:28,601] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:38:30,008] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:38:30,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.35%
Epoch: [38]  [  0/160]  eta: 0:16:35  lr: 0.000044  min_lr: 0.000001  loss: 1.9838 (1.9838)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3947 (9.3947)  time: 6.2207 (6.2207 -- 6.2207)  data: 5.6979 (5.6979 -- 5.6979)  max mem: 16413
[2023-08-31 12:38:40,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6086
[2023-08-31 12:38:40,294] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:38:40,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6086
[2023-08-31 12:38:40,294] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 12:38:40,295] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [38]  [ 20/160]  eta: 0:02:39  lr: 0.000044  min_lr: 0.000001  loss: 2.0407 (1.9266)  loss_scale: 16384.0000 (21065.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6437 (7.5220)  time: 0.8856 (0.5289 -- 2.8360)  data: 0.2052 (0.0004 -- 2.2957)  max mem: 16413
Epoch: [38]  [ 40/160]  eta: 0:02:00  lr: 0.000044  min_lr: 0.000001  loss: 1.9120 (1.9230)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6163 (8.1926)  time: 0.8546 (0.5230 -- 3.1423)  data: 0.2974 (0.0002 -- 2.6147)  max mem: 16413
Epoch: [38]  [ 60/160]  eta: 0:01:37  lr: 0.000044  min_lr: 0.000001  loss: 1.8036 (1.8967)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0124 (8.2546)  time: 0.9096 (0.5166 -- 5.4835)  data: 0.3147 (0.0004 -- 4.9666)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:15  lr: 0.000044  min_lr: 0.000001  loss: 1.8485 (1.9026)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9521 (8.1809)  time: 0.8467 (0.5278 -- 3.5794)  data: 0.1549 (0.0002 -- 1.8477)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:56  lr: 0.000044  min_lr: 0.000001  loss: 2.0067 (1.9312)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6919 (8.1556)  time: 0.9088 (0.5255 -- 3.1552)  data: 0.2602 (0.0003 -- 2.5413)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000001  loss: 1.9732 (1.9440)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4146 (8.2771)  time: 0.8897 (0.5170 -- 4.1701)  data: 0.3493 (0.0003 -- 3.6481)  max mem: 16413
[2023-08-31 12:40:36,169] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:40:36,169] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:40:36,171] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:40:36,171] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [38]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 1.7903 (1.9345)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9181 (8.4923)  time: 0.8778 (0.5109 -- 4.0122)  data: 0.3402 (0.0003 -- 3.5027)  max mem: 16413
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9506 (1.9326)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7018 (8.3886)  time: 0.6389 (0.4947 -- 2.6606)  data: 0.1216 (0.0002 -- 2.1248)  max mem: 16413
Epoch: [38] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9506 (1.9073)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7018 (8.3886)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6096 (0.6096)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4544 (2.4544 -- 2.4544)  data: 2.1843 (2.1843 -- 2.1843)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6196 (0.8917)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (96.9697)  time: 0.4301 (0.1960 -- 2.4544)  data: 0.2116 (0.0005 -- 2.1843)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7206 (0.8311)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.8254)  time: 0.2175 (0.1687 -- 0.3565)  data: 0.0145 (0.0001 -- 0.1427)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7711 (0.8813)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (96.2656)  time: 0.2017 (0.1329 -- 0.3565)  data: 0.0142 (0.0001 -- 0.1427)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 78.216 Acc@5 96.473 loss 0.836
Accuracy of the network on the 482 val images: 78.22%
[2023-08-31 12:40:59,779] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:40:59,781] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:40:59,781] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:40:59,781] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:41:01,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:41:01,276] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 78.22%
Epoch: [39]  [  0/160]  eta: 0:19:06  lr: 0.000043  min_lr: 0.000001  loss: 1.9802 (1.9802)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1711 (7.1711)  time: 7.1626 (7.1626 -- 7.1626)  data: 4.0075 (4.0075 -- 4.0075)  max mem: 16413
[2023-08-31 12:41:18,072] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6253
[2023-08-31 12:41:18,072] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6253
[2023-08-31 12:41:18,072] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:41:18,072] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:41:18,072] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [39]  [ 20/160]  eta: 0:02:30  lr: 0.000043  min_lr: 0.000001  loss: 1.9152 (1.9185)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4899 (7.7942)  time: 0.7691 (0.5117 -- 2.3666)  data: 0.0126 (0.0003 -- 0.2220)  max mem: 16413
Epoch: [39]  [ 40/160]  eta: 0:01:58  lr: 0.000043  min_lr: 0.000001  loss: 1.8193 (1.9124)  loss_scale: 16384.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3648 (7.6525)  time: 0.9035 (0.5279 -- 2.6438)  data: 0.0016 (0.0008 -- 0.0025)  max mem: 16413
Epoch: [39]  [ 60/160]  eta: 0:01:34  lr: 0.000043  min_lr: 0.000001  loss: 2.0115 (1.9310)  loss_scale: 16384.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8855 (7.9122)  time: 0.8531 (0.5346 -- 2.9405)  data: 0.0019 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:14  lr: 0.000043  min_lr: 0.000001  loss: 2.0581 (1.9600)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4716 (7.9453)  time: 0.8972 (0.5182 -- 2.9141)  data: 0.0015 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000001  loss: 1.8764 (1.9496)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2789 (8.1499)  time: 0.9478 (0.5234 -- 3.1598)  data: 0.0666 (0.0003 -- 1.2918)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000001  loss: 1.8450 (1.9353)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8130 (8.1678)  time: 0.7544 (0.5350 -- 1.6042)  data: 0.0552 (0.0002 -- 1.0777)  max mem: 16413
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 1.8924 (1.9341)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1654 (8.2240)  time: 0.9721 (0.5368 -- 3.6061)  data: 0.0181 (0.0004 -- 0.3153)  max mem: 16413
[2023-08-31 12:43:11,569] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:43:11,570] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:43:11,570] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:43:11,570] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:43:20,565] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6396
[2023-08-31 12:43:20,565] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:43:20,565] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 12:43:20,565] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6396
[2023-08-31 12:43:20,565] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9112 (1.9190)  loss_scale: 32768.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3480 (8.1530)  time: 0.6084 (0.4850 -- 1.8626)  data: 0.0013 (0.0002 -- 0.0087)  max mem: 16413
Epoch: [39] Total time: 0:02:20 (0.8799 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.9112 (1.9090)  loss_scale: 32768.0000 (19148.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3480 (8.1530)
[2023-08-31 12:43:22,067] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-08-31 12:43:22,069] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-08-31 12:43:22,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-08-31 12:43:22,069] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-08-31 12:43:22,945] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-08-31 12:43:22,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.4879 (0.4879)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4924 (2.4924 -- 2.4924)  data: 2.2681 (2.2681 -- 2.2681)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5968 (0.8810)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (94.9495)  time: 0.4262 (0.2027 -- 2.4924)  data: 0.2131 (0.0003 -- 2.2681)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7113 (0.8319)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (95.7672)  time: 0.2181 (0.1696 -- 0.4009)  data: 0.0146 (0.0001 -- 0.2123)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7804 (0.8860)  acc1: 77.7778 (72.1992)  acc5: 100.0000 (95.4357)  time: 0.2022 (0.1335 -- 0.4009)  data: 0.0143 (0.0001 -- 0.2123)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 75.311 Acc@5 96.058 loss 0.836
Accuracy of the network on the 482 val images: 75.31%
Max accuracy: 78.22%
Epoch: [40]  [  0/160]  eta: 0:17:52  lr: 0.000043  min_lr: 0.000001  loss: 1.4840 (1.4840)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6155 (10.6155)  time: 6.7050 (6.7050 -- 6.7050)  data: 4.1954 (4.1954 -- 4.1954)  max mem: 16413
Epoch: [40]  [ 20/160]  eta: 0:02:40  lr: 0.000043  min_lr: 0.000001  loss: 1.9083 (1.8795)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0017 (8.1695)  time: 0.8657 (0.5192 -- 4.1947)  data: 0.0095 (0.0003 -- 0.1078)  max mem: 16413
Epoch: [40]  [ 40/160]  eta: 0:02:00  lr: 0.000043  min_lr: 0.000001  loss: 1.9157 (1.8985)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0370 (8.2828)  time: 0.8610 (0.5231 -- 2.5431)  data: 0.0016 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:37  lr: 0.000043  min_lr: 0.000001  loss: 1.7741 (1.8668)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3569 (8.3387)  time: 0.9192 (0.5195 -- 3.3672)  data: 0.0012 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:16  lr: 0.000043  min_lr: 0.000001  loss: 1.8403 (1.8554)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4543 (8.3502)  time: 0.8990 (0.5254 -- 2.4417)  data: 0.0024 (0.0004 -- 0.0169)  max mem: 16413
Epoch: [40]  [100/160]  eta: 0:00:55  lr: 0.000043  min_lr: 0.000001  loss: 2.0021 (1.8637)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0007 (8.4541)  time: 0.7850 (0.5301 -- 2.3878)  data: 0.0422 (0.0003 -- 0.8136)  max mem: 16413
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000043  min_lr: 0.000001  loss: 1.9712 (1.8805)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1515 (8.5099)  time: 0.8955 (0.5355 -- 2.7096)  data: 0.0477 (0.0002 -- 0.9191)  max mem: 16413
[2023-08-31 12:45:26,867] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:45:26,867] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:45:26,868] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:45:26,868] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 2.0165 (1.8961)  loss_scale: 32768.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4219 (8.3230)  time: 0.8423 (0.5222 -- 3.7559)  data: 0.0013 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.8366 (1.8792)  loss_scale: 32768.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0062 (8.3518)  time: 0.7155 (0.4942 -- 1.5625)  data: 0.0009 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [40] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.8366 (1.8822)  loss_scale: 32768.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0062 (8.3518)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5009 (0.5009)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3900 (2.3900 -- 2.3900)  data: 2.1502 (2.1502 -- 2.1502)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6144 (0.8878)  acc1: 66.6667 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4365 (0.2013 -- 2.3900)  data: 0.2185 (0.0006 -- 2.1502)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6297 (0.8165)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2294 (0.1695 -- 0.4833)  data: 0.0267 (0.0001 -- 0.2780)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7521 (0.8664)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.4357)  time: 0.2133 (0.1323 -- 0.4833)  data: 0.0265 (0.0001 -- 0.2780)  max mem: 16413
Val: Total time: 0:00:07 (0.2948 s / it)
* Acc@1 78.216 Acc@5 96.473 loss 0.818
Accuracy of the network on the 482 val images: 78.22%
Max accuracy: 78.22%
Epoch: [41]  [  0/160]  eta: 0:21:30  lr: 0.000043  min_lr: 0.000001  loss: 2.2975 (2.2975)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8020 (6.8020)  time: 8.0642 (8.0642 -- 8.0642)  data: 7.5154 (7.5154 -- 7.5154)  max mem: 16413
Epoch: [41]  [ 20/160]  eta: 0:02:44  lr: 0.000043  min_lr: 0.000001  loss: 1.8701 (1.9119)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6897 (8.2483)  time: 0.8321 (0.5300 -- 3.6392)  data: 0.0629 (0.0002 -- 0.7158)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:04  lr: 0.000043  min_lr: 0.000001  loss: 1.8758 (1.9272)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5407 (8.2161)  time: 0.8939 (0.5245 -- 3.1252)  data: 0.2641 (0.0005 -- 1.8157)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:36  lr: 0.000043  min_lr: 0.000001  loss: 1.8210 (1.9165)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7197 (7.8527)  time: 0.8103 (0.5120 -- 2.6764)  data: 0.0953 (0.0004 -- 0.9522)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000001  loss: 1.9709 (1.9086)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9642 (7.7525)  time: 0.9018 (0.5348 -- 3.9336)  data: 0.0337 (0.0004 -- 0.5647)  max mem: 16413
[2023-08-31 12:47:29,404] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:47:29,404] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-31 12:47:29,407] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:47:29,407] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [41]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000001  loss: 1.8836 (1.8877)  loss_scale: 32768.0000 (35363.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0747 (7.6582)  time: 0.8936 (0.5223 -- 3.3790)  data: 0.1013 (0.0001 -- 1.1038)  max mem: 16413
[2023-08-31 12:47:40,403] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6666
[2023-08-31 12:47:40,403] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 12:47:40,403] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6666
[2023-08-31 12:47:40,403] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-31 12:47:40,403] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
[2023-08-31 12:47:50,333] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6678
[2023-08-31 12:47:50,333] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6678
[2023-08-31 12:47:50,333] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:47:50,333] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:47:50,333] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [41]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000001  loss: 1.8219 (1.8759)  loss_scale: 32768.0000 (35882.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9111 (7.7337)  time: 0.8733 (0.5086 -- 3.4552)  data: 0.0014 (0.0006 -- 0.0041)  max mem: 16413
Epoch: [41]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 1.8509 (1.8675)  loss_scale: 16384.0000 (33116.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9582 (7.8909)  time: 0.8885 (0.5233 -- 3.8282)  data: 0.0022 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.8796 (1.8783)  loss_scale: 16384.0000 (31129.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9710 (7.9420)  time: 0.6320 (0.4956 -- 1.7362)  data: 0.0130 (0.0002 -- 0.2411)  max mem: 16413
Epoch: [41] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.8796 (1.8783)  loss_scale: 16384.0000 (31129.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9710 (7.9420)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.5106 (0.5106)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5624 (2.5624 -- 2.5624)  data: 2.3264 (2.3264 -- 2.3264)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5634 (0.8399)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4222 (0.1955 -- 2.5624)  data: 0.2123 (0.0003 -- 2.3264)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6261 (0.7943)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2186 (0.1686 -- 0.4878)  data: 0.0163 (0.0001 -- 0.3136)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7088 (0.8403)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.8506)  time: 0.2048 (0.1317 -- 0.4878)  data: 0.0161 (0.0001 -- 0.3136)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 79.461 Acc@5 96.473 loss 0.793
Accuracy of the network on the 482 val images: 79.46%
[2023-08-31 12:48:30,655] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 12:48:30,657] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 12:48:30,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 12:48:30,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 12:48:31,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 12:48:31,852] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.46%
Epoch: [42]  [  0/160]  eta: 0:21:26  lr: 0.000043  min_lr: 0.000001  loss: 2.1427 (2.1427)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2931 (9.2931)  time: 8.0396 (8.0396 -- 8.0396)  data: 6.7086 (6.7086 -- 6.7086)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:46  lr: 0.000043  min_lr: 0.000001  loss: 1.9611 (1.9226)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5156 (8.1232)  time: 0.8478 (0.5233 -- 3.9480)  data: 0.0744 (0.0004 -- 1.4670)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:06  lr: 0.000043  min_lr: 0.000001  loss: 1.9191 (1.8676)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9704 (8.4302)  time: 0.9134 (0.5321 -- 4.8187)  data: 0.0010 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [42]  [ 60/160]  eta: 0:01:37  lr: 0.000043  min_lr: 0.000001  loss: 1.8990 (1.8735)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0835 (8.3429)  time: 0.7970 (0.5335 -- 1.9678)  data: 0.0055 (0.0005 -- 0.0764)  max mem: 16413
Epoch: [42]  [ 80/160]  eta: 0:01:16  lr: 0.000043  min_lr: 0.000001  loss: 1.8926 (1.8910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7757 (8.2989)  time: 0.9107 (0.5301 -- 2.1512)  data: 0.0821 (0.0003 -- 1.6138)  max mem: 16413
[2023-08-31 12:49:54,326] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:49:54,327] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:49:54,368] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:49:54,368] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [42]  [100/160]  eta: 0:00:56  lr: 0.000043  min_lr: 0.000001  loss: 1.7908 (1.8803)  loss_scale: 32768.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6775 (8.1390)  time: 0.8564 (0.5404 -- 2.8937)  data: 0.0923 (0.0007 -- 1.1308)  max mem: 16413
[2023-08-31 12:50:17,246] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6832
[2023-08-31 12:50:17,246] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6832
[2023-08-31 12:50:17,247] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:50:17,247] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:50:17,247] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [42]  [120/160]  eta: 0:00:37  lr: 0.000043  min_lr: 0.000001  loss: 1.7234 (1.8609)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7082 (7.9597)  time: 0.8828 (0.5290 -- 2.9726)  data: 0.0013 (0.0002 -- 0.0047)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000043  min_lr: 0.000001  loss: 1.6845 (1.8465)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3267 (8.0300)  time: 0.8349 (0.5316 -- 3.4026)  data: 0.0015 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000001  loss: 1.8903 (1.8414)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6859 (8.0054)  time: 0.7150 (0.4935 -- 2.0893)  data: 0.1309 (0.0002 -- 1.5705)  max mem: 16413
Epoch: [42] Total time: 0:02:22 (0.8919 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000001  loss: 1.8903 (1.8589)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6859 (8.0054)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.5152 (0.5152)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.6137 (2.6137 -- 2.6137)  data: 2.3692 (2.3692 -- 2.3692)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5617 (0.8738)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4371 (0.2066 -- 2.6137)  data: 0.2166 (0.0004 -- 2.3692)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5878 (0.7795)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2149 (0.1683 -- 0.3739)  data: 0.0105 (0.0001 -- 0.1946)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7271 (0.8305)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.8506)  time: 0.1995 (0.1328 -- 0.3739)  data: 0.0101 (0.0001 -- 0.1946)  max mem: 16413
Val: Total time: 0:00:07 (0.2926 s / it)
* Acc@1 78.423 Acc@5 96.888 loss 0.790
Accuracy of the network on the 482 val images: 78.42%
Max accuracy: 79.46%
Epoch: [43]  [  0/160]  eta: 0:18:07  lr: 0.000043  min_lr: 0.000001  loss: 2.0595 (2.0595)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2615 (7.2615)  time: 6.7999 (6.7999 -- 6.7999)  data: 5.7814 (5.7814 -- 5.7814)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:44  lr: 0.000043  min_lr: 0.000001  loss: 1.9150 (1.8961)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6023 (7.9447)  time: 0.8946 (0.5261 -- 3.5604)  data: 0.2443 (0.0001 -- 3.0145)  max mem: 16413
Epoch: [43]  [ 40/160]  eta: 0:01:59  lr: 0.000043  min_lr: 0.000001  loss: 1.8445 (1.8305)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0729 (8.2362)  time: 0.8014 (0.5183 -- 2.7918)  data: 0.0382 (0.0007 -- 0.6587)  max mem: 16413
Epoch: [43]  [ 60/160]  eta: 0:01:38  lr: 0.000043  min_lr: 0.000001  loss: 1.8193 (1.8491)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7985 (8.3091)  time: 0.9564 (0.5223 -- 4.0416)  data: 0.0015 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [43]  [ 80/160]  eta: 0:01:15  lr: 0.000043  min_lr: 0.000001  loss: 1.8823 (1.8323)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3074 (8.3819)  time: 0.8094 (0.5161 -- 2.6064)  data: 0.0590 (0.0002 -- 0.7501)  max mem: 16413
[2023-08-31 12:52:19,540] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:52:19,540] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:52:19,544] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:52:19,544] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [43]  [100/160]  eta: 0:00:55  lr: 0.000042  min_lr: 0.000001  loss: 1.8438 (1.8437)  loss_scale: 32768.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0393 (8.2867)  time: 0.8362 (0.5273 -- 2.7327)  data: 0.2717 (0.0002 -- 2.2091)  max mem: 16413
[2023-08-31 12:52:52,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=36, lr=[1.0087326438894925e-06, 1.0087326438894925e-06, 1.3449768585193233e-06, 1.3449768585193233e-06, 1.7933024780257643e-06, 1.7933024780257643e-06, 2.3910699707010193e-06, 2.3910699707010193e-06, 3.1880932942680254e-06, 3.1880932942680254e-06, 4.2507910590240344e-06, 4.2507910590240344e-06, 5.667721412032045e-06, 5.667721412032045e-06, 7.556961882709394e-06, 7.556961882709394e-06, 1.0075949176945859e-05, 1.0075949176945859e-05, 1.3434598902594478e-05, 1.3434598902594478e-05, 1.7912798536792636e-05, 1.7912798536792636e-05, 2.3883731382390183e-05, 2.3883731382390183e-05, 3.1844975176520246e-05, 3.1844975176520246e-05, 4.245996690202699e-05, 4.245996690202699e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 12:52:52,640] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=16.42669546659428, CurrSamplesPerSec=21.686160305156037, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000001  loss: 1.6887 (1.8227)  loss_scale: 32768.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3518 (8.3596)  time: 0.8960 (0.5330 -- 3.0790)  data: 0.2066 (0.0003 -- 2.5489)  max mem: 16413
[2023-08-31 12:52:58,816] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7003
[2023-08-31 12:52:58,816] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:52:58,816] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7003
[2023-08-31 12:52:58,816] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 12:52:58,816] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 1.7878 (1.8308)  loss_scale: 16384.0000 (21264.3404)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1636 (8.4030)  time: 0.8612 (0.5129 -- 4.5499)  data: 0.3149 (0.0004 -- 4.0301)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.8993 (1.8335)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3197 (8.3351)  time: 0.7036 (0.4936 -- 3.2196)  data: 0.1448 (0.0002 -- 2.6764)  max mem: 16413
Epoch: [43] Total time: 0:02:21 (0.8840 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.8993 (1.8573)  loss_scale: 16384.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3197 (8.3351)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.4932 (0.4932)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4031 (2.4031 -- 2.4031)  data: 2.1579 (2.1579 -- 2.1579)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6865 (0.8683)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4121 (0.2032 -- 2.4031)  data: 0.1973 (0.0008 -- 2.1579)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6672 (0.7870)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.2963)  time: 0.2201 (0.1690 -- 0.4023)  data: 0.0177 (0.0001 -- 0.1915)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7362 (0.8393)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.4357)  time: 0.2060 (0.1330 -- 0.4023)  data: 0.0174 (0.0001 -- 0.1915)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 78.838 Acc@5 96.473 loss 0.790
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 79.46%
Epoch: [44]  [  0/160]  eta: 0:22:12  lr: 0.000042  min_lr: 0.000001  loss: 1.6798 (1.6798)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3317 (8.3317)  time: 8.3302 (8.3302 -- 8.3302)  data: 7.7813 (7.7813 -- 7.7813)  max mem: 16413
Epoch: [44]  [ 20/160]  eta: 0:02:44  lr: 0.000042  min_lr: 0.000001  loss: 1.8731 (1.7898)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1814 (8.4214)  time: 0.8173 (0.5291 -- 3.3597)  data: 0.2074 (0.0008 -- 2.8008)  max mem: 16413
Epoch: [44]  [ 40/160]  eta: 0:02:06  lr: 0.000042  min_lr: 0.000001  loss: 1.7333 (1.7674)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2100 (8.1398)  time: 0.9190 (0.5205 -- 2.8317)  data: 0.2982 (0.0002 -- 2.3018)  max mem: 16413
Epoch: [44]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000001  loss: 1.9009 (1.8141)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9265 (8.1198)  time: 0.8710 (0.5229 -- 2.8932)  data: 0.2269 (0.0002 -- 2.3403)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:16  lr: 0.000042  min_lr: 0.000001  loss: 1.8178 (1.8276)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1366 (8.2112)  time: 0.8677 (0.5246 -- 4.0663)  data: 0.2794 (0.0003 -- 3.5380)  max mem: 16413
[2023-08-31 12:55:01,094] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:55:01,094] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 12:55:01,098] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:55:01,098] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000001  loss: 1.7284 (1.8128)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6756 (7.9749)  time: 0.8840 (0.5142 -- 2.4740)  data: 0.1540 (0.0003 -- 1.9505)  max mem: 16413
[2023-08-31 12:55:20,436] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7155
[2023-08-31 12:55:20,436] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7155
[2023-08-31 12:55:20,436] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:55:20,436] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 12:55:20,436] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [44]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000001  loss: 1.8972 (1.8255)  loss_scale: 32768.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2405 (8.1019)  time: 0.7965 (0.4996 -- 3.5140)  data: 0.0019 (0.0005 -- 0.0095)  max mem: 16413
Epoch: [44]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 1.8230 (1.8373)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1827 (8.1890)  time: 1.0401 (0.5052 -- 5.0748)  data: 0.0009 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 2.0051 (1.8469)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1218 (8.2614)  time: 0.5559 (0.4952 -- 1.1341)  data: 0.0008 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [44] Total time: 0:02:22 (0.8928 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 2.0051 (1.8673)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1218 (8.2614)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3792 (0.3792)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5222 (2.5222 -- 2.5222)  data: 2.2910 (2.2910 -- 2.2910)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5541 (0.8459)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4268 (0.1974 -- 2.5222)  data: 0.2102 (0.0007 -- 2.2910)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6099 (0.7785)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (95.7672)  time: 0.2172 (0.1691 -- 0.3703)  data: 0.0107 (0.0001 -- 0.1903)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7195 (0.8395)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.0207)  time: 0.1994 (0.1326 -- 0.3703)  data: 0.0100 (0.0001 -- 0.1903)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 78.008 Acc@5 96.266 loss 0.786
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.46%
Epoch: [45]  [  0/160]  eta: 0:20:13  lr: 0.000042  min_lr: 0.000001  loss: 2.0407 (2.0407)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3135 (8.3135)  time: 7.5866 (7.5866 -- 7.5866)  data: 5.0915 (5.0915 -- 5.0915)  max mem: 16413
[2023-08-31 12:56:24,687] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7218
[2023-08-31 12:56:24,687] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7218
[2023-08-31 12:56:24,687] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:56:24,687] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 12:56:24,687] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [45]  [ 20/160]  eta: 0:02:35  lr: 0.000042  min_lr: 0.000001  loss: 1.8273 (1.8589)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7216 (8.2925)  time: 0.7860 (0.5356 -- 2.5479)  data: 0.0087 (0.0011 -- 0.1358)  max mem: 16413
Epoch: [45]  [ 40/160]  eta: 0:01:57  lr: 0.000042  min_lr: 0.000001  loss: 1.9838 (1.8884)  loss_scale: 8192.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9224 (8.9149)  time: 0.8335 (0.5290 -- 2.5904)  data: 0.1200 (0.0004 -- 1.6034)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:35  lr: 0.000042  min_lr: 0.000001  loss: 1.8782 (1.8710)  loss_scale: 8192.0000 (10609.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6299 (9.5301)  time: 0.9164 (0.5420 -- 2.4860)  data: 0.0908 (0.0002 -- 1.4859)  max mem: 16413
Epoch: [45]  [ 80/160]  eta: 0:01:14  lr: 0.000042  min_lr: 0.000001  loss: 1.9023 (1.8541)  loss_scale: 8192.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9693 (9.3099)  time: 0.8809 (0.5241 -- 3.1743)  data: 0.2966 (0.0004 -- 2.6402)  max mem: 16413
Epoch: [45]  [100/160]  eta: 0:00:57  lr: 0.000042  min_lr: 0.000001  loss: 1.9927 (1.8813)  loss_scale: 8192.0000 (9651.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6207 (9.2142)  time: 1.0395 (0.5225 -- 4.3738)  data: 0.0016 (0.0005 -- 0.0062)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000001  loss: 1.8594 (1.8800)  loss_scale: 8192.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9687 (9.2217)  time: 0.8307 (0.5172 -- 4.3563)  data: 0.0010 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 1.8818 (1.8711)  loss_scale: 8192.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7663 (9.0350)  time: 0.7840 (0.5199 -- 3.0415)  data: 0.0015 (0.0004 -- 0.0046)  max mem: 16413
[2023-08-31 12:58:17,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:58:17,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 12:58:17,434] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 12:58:17,434] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.9903 (1.8691)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2189 (8.9551)  time: 0.6606 (0.4965 -- 2.5941)  data: 0.0010 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [45] Total time: 0:02:21 (0.8856 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.9903 (1.8359)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2189 (8.9551)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.4108 (0.4108)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5708 (2.5708 -- 2.5708)  data: 2.3463 (2.3463 -- 2.3463)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5205 (0.7997)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4384 (0.1996 -- 2.5708)  data: 0.2219 (0.0004 -- 2.3463)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6110 (0.7534)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2178 (0.1689 -- 0.3982)  data: 0.0155 (0.0001 -- 0.2122)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6598 (0.7998)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (96.2656)  time: 0.2037 (0.1330 -- 0.3982)  data: 0.0152 (0.0001 -- 0.2122)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 78.838 Acc@5 96.888 loss 0.757
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 79.46%
Epoch: [46]  [  0/160]  eta: 0:17:37  lr: 0.000042  min_lr: 0.000001  loss: 2.2763 (2.2763)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3890 (7.3890)  time: 6.6073 (6.6073 -- 6.6073)  data: 6.0614 (6.0614 -- 6.0614)  max mem: 16413
Epoch: [46]  [ 20/160]  eta: 0:02:47  lr: 0.000042  min_lr: 0.000001  loss: 1.7598 (1.8512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4490 (7.5654)  time: 0.9256 (0.5288 -- 3.8280)  data: 0.3632 (0.0002 -- 3.3028)  max mem: 16413
Epoch: [46]  [ 40/160]  eta: 0:02:03  lr: 0.000042  min_lr: 0.000001  loss: 1.7446 (1.8533)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8466 (8.1650)  time: 0.8478 (0.5340 -- 2.9388)  data: 0.2941 (0.0002 -- 2.3877)  max mem: 16413
Epoch: [46]  [ 60/160]  eta: 0:01:39  lr: 0.000042  min_lr: 0.000001  loss: 1.9748 (1.8746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1928 (8.5128)  time: 0.9323 (0.5211 -- 3.2858)  data: 0.2510 (0.0004 -- 2.3603)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000001  loss: 1.7858 (1.8450)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6570 (8.3333)  time: 0.7621 (0.5188 -- 2.4460)  data: 0.1148 (0.0001 -- 1.2455)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:55  lr: 0.000042  min_lr: 0.000001  loss: 1.7305 (1.8266)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2430 (8.2599)  time: 0.9007 (0.5384 -- 2.1719)  data: 0.1680 (0.0005 -- 1.6416)  max mem: 16413
[2023-08-31 13:00:19,851] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:00:19,851] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:00:19,852] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:00:19,852] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [46]  [120/160]  eta: 0:00:36  lr: 0.000042  min_lr: 0.000001  loss: 2.1142 (1.8573)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4819 (8.2328)  time: 0.8249 (0.5155 -- 3.2563)  data: 0.2214 (0.0004 -- 2.7232)  max mem: 16413
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 1.7684 (1.8603)  loss_scale: 32768.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4302 (8.2190)  time: 0.9104 (0.5152 -- 3.0642)  data: 0.1242 (0.0003 -- 1.5278)  max mem: 16413
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000001  loss: 1.9366 (1.8627)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4486 (8.2332)  time: 0.7264 (0.4970 -- 3.3531)  data: 0.0044 (0.0002 -- 0.0738)  max mem: 16413
Epoch: [46] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000001  loss: 1.9366 (1.8653)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4486 (8.2332)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5385 (0.5385)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3926 (2.3926 -- 2.3926)  data: 2.1786 (2.1786 -- 2.1786)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5880 (0.8019)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4251 (0.1964 -- 2.3926)  data: 0.2146 (0.0004 -- 2.1786)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5880 (0.7444)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2204 (0.1699 -- 0.3801)  data: 0.0187 (0.0001 -- 0.1706)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6723 (0.7936)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.4357)  time: 0.2058 (0.1325 -- 0.3801)  data: 0.0183 (0.0001 -- 0.1706)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 80.290 Acc@5 96.473 loss 0.761
Accuracy of the network on the 482 val images: 80.29%
[2023-08-31 13:01:02,585] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 13:01:02,587] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 13:01:02,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 13:01:02,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 13:01:03,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 13:01:03,859] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.29%
Epoch: [47]  [  0/160]  eta: 0:20:49  lr: 0.000042  min_lr: 0.000001  loss: 1.7052 (1.7052)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9995 (10.9995)  time: 7.8093 (7.8093 -- 7.8093)  data: 5.7405 (5.7405 -- 5.7405)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:40  lr: 0.000042  min_lr: 0.000001  loss: 1.9030 (1.8799)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3747 (8.5064)  time: 0.8139 (0.5259 -- 3.2765)  data: 0.2665 (0.0002 -- 2.7492)  max mem: 16413
[2023-08-31 13:01:32,628] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7544
[2023-08-31 13:01:32,628] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7544
[2023-08-31 13:01:32,628] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:01:32,628] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:01:32,628] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [ 40/160]  eta: 0:02:04  lr: 0.000042  min_lr: 0.000001  loss: 1.8382 (1.8360)  loss_scale: 16384.0000 (25974.6341)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7053 (8.4902)  time: 0.9147 (0.5205 -- 2.9817)  data: 0.2307 (0.0004 -- 2.4804)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:41  lr: 0.000042  min_lr: 0.000001  loss: 1.7957 (1.8259)  loss_scale: 16384.0000 (22830.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2459 (8.2598)  time: 0.9801 (0.5133 -- 4.3682)  data: 0.0015 (0.0001 -- 0.0030)  max mem: 16413
Epoch: [47]  [ 80/160]  eta: 0:01:15  lr: 0.000042  min_lr: 0.000001  loss: 1.7465 (1.8218)  loss_scale: 16384.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0003 (8.0496)  time: 0.7432 (0.5212 -- 2.6470)  data: 0.0610 (0.0001 -- 1.1820)  max mem: 16413
Epoch: [47]  [100/160]  eta: 0:00:56  lr: 0.000042  min_lr: 0.000001  loss: 1.7883 (1.8197)  loss_scale: 16384.0000 (20277.2277)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9816 (8.0738)  time: 0.9087 (0.5336 -- 2.9218)  data: 0.1198 (0.0002 -- 2.3716)  max mem: 16413
Epoch: [47]  [120/160]  eta: 0:00:37  lr: 0.000042  min_lr: 0.000001  loss: 1.9198 (1.8339)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9115 (8.1710)  time: 0.9023 (0.5369 -- 3.7311)  data: 0.3501 (0.0003 -- 3.2070)  max mem: 16413
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000042  min_lr: 0.000001  loss: 1.8862 (1.8394)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2957 (8.2543)  time: 0.8034 (0.5281 -- 2.3987)  data: 0.1733 (0.0008 -- 1.8553)  max mem: 16413
[2023-08-31 13:03:22,803] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:03:22,803] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:03:22,804] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:03:22,804] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.8802 (1.8258)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0073 (8.3545)  time: 0.6672 (0.4973 -- 1.8893)  data: 0.1332 (0.0002 -- 1.3642)  max mem: 16413
Epoch: [47] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.8802 (1.8378)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0073 (8.3545)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.4863 (0.4863)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2249 (2.2249 -- 2.2249)  data: 1.9887 (1.9887 -- 1.9887)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6206 (0.8314)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4053 (0.2042 -- 2.2249)  data: 0.1899 (0.0009 -- 1.9887)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6206 (0.7557)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.8254)  time: 0.2250 (0.1687 -- 0.4890)  data: 0.0174 (0.0001 -- 0.2449)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6946 (0.8137)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.8506)  time: 0.2106 (0.1330 -- 0.4890)  data: 0.0171 (0.0001 -- 0.2449)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 77.801 Acc@5 96.473 loss 0.766
Accuracy of the network on the 482 val images: 77.80%
Max accuracy: 80.29%
Epoch: [48]  [  0/160]  eta: 0:20:03  lr: 0.000041  min_lr: 0.000001  loss: 1.6638 (1.6638)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0279 (10.0279)  time: 7.5211 (7.5211 -- 7.5211)  data: 6.0700 (6.0700 -- 6.0700)  max mem: 16413
[2023-08-31 13:03:51,529] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7694
[2023-08-31 13:03:51,529] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:03:51,529] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7694
[2023-08-31 13:03:51,529] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:03:51,529] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [48]  [ 20/160]  eta: 0:02:43  lr: 0.000041  min_lr: 0.000001  loss: 1.8913 (1.9253)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3060 (9.1417)  time: 0.8466 (0.5246 -- 3.7179)  data: 0.2621 (0.0004 -- 3.1633)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:02  lr: 0.000041  min_lr: 0.000001  loss: 1.6499 (1.8259)  loss_scale: 16384.0000 (21978.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0680 (8.9706)  time: 0.8634 (0.5234 -- 3.2523)  data: 0.1217 (0.0007 -- 2.4020)  max mem: 16413
Epoch: [48]  [ 60/160]  eta: 0:01:42  lr: 0.000041  min_lr: 0.000001  loss: 1.8703 (1.8394)  loss_scale: 16384.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2512 (8.9444)  time: 1.0271 (0.5257 -- 5.5843)  data: 0.0013 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:16  lr: 0.000041  min_lr: 0.000001  loss: 1.8459 (1.8459)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6066 (8.8176)  time: 0.7687 (0.5157 -- 3.7072)  data: 0.0015 (0.0001 -- 0.0030)  max mem: 16413
Epoch: [48]  [100/160]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000001  loss: 1.8100 (1.8314)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0896 (8.8614)  time: 0.8634 (0.5398 -- 3.7900)  data: 0.0027 (0.0003 -- 0.0066)  max mem: 16413
Epoch: [48]  [120/160]  eta: 0:00:36  lr: 0.000041  min_lr: 0.000001  loss: 1.8458 (1.8418)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5051 (8.8166)  time: 0.7713 (0.5277 -- 2.7188)  data: 0.0018 (0.0004 -- 0.0065)  max mem: 16413
Epoch: [48]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000001  loss: 1.8805 (1.8476)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6019 (8.7063)  time: 0.8691 (0.5443 -- 2.2352)  data: 0.1375 (0.0002 -- 1.7036)  max mem: 16413
[2023-08-31 13:05:44,040] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:05:44,041] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:05:44,041] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:05:44,041] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:05:45,163] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7825
[2023-08-31 13:05:45,163] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7825
[2023-08-31 13:05:45,163] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:05:45,163] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:05:45,163] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.9667 (1.8383)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3997 (8.6261)  time: 0.8069 (0.4963 -- 2.4711)  data: 0.0507 (0.0002 -- 0.9966)  max mem: 16413
Epoch: [48] Total time: 0:02:22 (0.8900 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.9667 (1.8286)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3997 (8.6261)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3684 (0.3684)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3836 (2.3836 -- 2.3836)  data: 2.1746 (2.1746 -- 2.1746)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5276 (0.7996)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4232 (0.2094 -- 2.3836)  data: 0.2043 (0.0006 -- 2.1746)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5276 (0.7313)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2205 (0.1703 -- 0.3931)  data: 0.0141 (0.0001 -- 0.2061)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7370 (0.7886)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.4357)  time: 0.2039 (0.1332 -- 0.3931)  data: 0.0132 (0.0001 -- 0.2061)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 80.083 Acc@5 96.266 loss 0.747
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.29%
Epoch: [49]  [  0/160]  eta: 0:18:16  lr: 0.000041  min_lr: 0.000001  loss: 2.3827 (2.3827)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2754 (10.2754)  time: 6.8537 (6.8537 -- 6.8537)  data: 6.1743 (6.1743 -- 6.1743)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:43  lr: 0.000041  min_lr: 0.000001  loss: 1.8628 (1.9003)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7309 (8.4927)  time: 0.8848 (0.5257 -- 3.2258)  data: 0.3448 (0.0004 -- 2.7058)  max mem: 16413
Epoch: [49]  [ 40/160]  eta: 0:02:05  lr: 0.000041  min_lr: 0.000001  loss: 1.8592 (1.9082)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5507 (8.1674)  time: 0.9233 (0.5281 -- 3.2596)  data: 0.3778 (0.0004 -- 2.7452)  max mem: 16413
[2023-08-31 13:07:01,738] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7896
[2023-08-31 13:07:01,738] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:07:01,738] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7896
[2023-08-31 13:07:01,738] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:07:01,738] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [49]  [ 60/160]  eta: 0:01:38  lr: 0.000041  min_lr: 0.000001  loss: 1.8080 (1.8926)  loss_scale: 16384.0000 (15712.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8655 (7.9588)  time: 0.8554 (0.5301 -- 3.9076)  data: 0.2893 (0.0006 -- 3.3852)  max mem: 16413
Epoch: [49]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000001  loss: 1.9521 (1.8900)  loss_scale: 8192.0000 (13855.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5874 (8.0580)  time: 0.9348 (0.5283 -- 3.6895)  data: 0.1299 (0.0004 -- 1.8165)  max mem: 16413
Epoch: [49]  [100/160]  eta: 0:00:57  lr: 0.000041  min_lr: 0.000001  loss: 1.9612 (1.8858)  loss_scale: 8192.0000 (12734.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5017 (8.0888)  time: 0.8778 (0.5134 -- 2.6286)  data: 0.1691 (0.0001 -- 2.0958)  max mem: 16413
Epoch: [49]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000001  loss: 2.0175 (1.8855)  loss_scale: 8192.0000 (11983.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9496 (8.1123)  time: 0.8293 (0.5304 -- 2.8919)  data: 0.2102 (0.0003 -- 2.3797)  max mem: 16413
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000001  loss: 1.8406 (1.8701)  loss_scale: 8192.0000 (11445.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1691 (8.3707)  time: 0.8195 (0.5282 -- 4.2192)  data: 0.2727 (0.0002 -- 3.7158)  max mem: 16413
[2023-08-31 13:08:26,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=43, lr=[9.737669719641908e-07, 9.737669719641908e-07, 1.2983559626189209e-06, 1.2983559626189209e-06, 1.7311412834918947e-06, 1.7311412834918947e-06, 2.308188377989193e-06, 2.308188377989193e-06, 3.0775845039855904e-06, 3.0775845039855904e-06, 4.103446005314121e-06, 4.103446005314121e-06, 5.471261340418828e-06, 5.471261340418828e-06, 7.295015120558436e-06, 7.295015120558436e-06, 9.726686827411249e-06, 9.726686827411249e-06, 1.2968915769881664e-05, 1.2968915769881664e-05, 1.7291887693175552e-05, 1.7291887693175552e-05, 2.3055850257567405e-05, 2.3055850257567405e-05, 3.0741133676756536e-05, 3.0741133676756536e-05, 4.0988178235675386e-05, 4.0988178235675386e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 13:08:26,997] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=16.555471922136938, CurrSamplesPerSec=24.88040839509566, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.7231 (1.8533)  loss_scale: 8192.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9538 (8.3608)  time: 0.7187 (0.4953 -- 3.6128)  data: 0.1627 (0.0002 -- 3.0896)  max mem: 16413
Epoch: [49] Total time: 0:02:23 (0.8950 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.7231 (1.8622)  loss_scale: 8192.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9538 (8.3608)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.4919 (0.4919)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5623 (2.5623 -- 2.5623)  data: 2.3072 (2.3072 -- 2.3072)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5991 (0.7937)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4407 (0.2012 -- 2.5623)  data: 0.2258 (0.0006 -- 2.3072)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5991 (0.7186)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2110 (0.1686 -- 0.4072)  data: 0.0094 (0.0001 -- 0.1691)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6540 (0.7716)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.4357)  time: 0.1974 (0.1328 -- 0.4072)  data: 0.0092 (0.0001 -- 0.1691)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 81.120 Acc@5 96.058 loss 0.741
Accuracy of the network on the 482 val images: 81.12%
[2023-08-31 13:08:34,927] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 13:08:34,929] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 13:08:34,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 13:08:34,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 13:08:36,513] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 13:08:36,514] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.12%
Epoch: [50]  [  0/160]  eta: 0:19:55  lr: 0.000041  min_lr: 0.000001  loss: 2.0517 (2.0517)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4373 (7.4373)  time: 7.4699 (7.4699 -- 7.4699)  data: 5.9686 (5.9686 -- 5.9686)  max mem: 16413
Epoch: [50]  [ 20/160]  eta: 0:02:39  lr: 0.000041  min_lr: 0.000001  loss: 1.6577 (1.6861)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0973 (7.4332)  time: 0.8231 (0.5195 -- 3.2818)  data: 0.1980 (0.0003 -- 1.3955)  max mem: 16413
[2023-08-31 13:09:04,989] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:09:04,989] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:09:04,990] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:09:04,990] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [50]  [ 40/160]  eta: 0:02:01  lr: 0.000041  min_lr: 0.000001  loss: 1.6667 (1.7086)  loss_scale: 16384.0000 (11388.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2785 (7.6915)  time: 0.8774 (0.5238 -- 2.8147)  data: 0.2403 (0.0007 -- 2.2966)  max mem: 16413
Epoch: [50]  [ 60/160]  eta: 0:01:39  lr: 0.000041  min_lr: 0.000001  loss: 1.8366 (1.7317)  loss_scale: 16384.0000 (13026.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7515 (8.3259)  time: 0.9723 (0.5275 -- 3.8344)  data: 0.3129 (0.0005 -- 3.2933)  max mem: 16413
Epoch: [50]  [ 80/160]  eta: 0:01:15  lr: 0.000041  min_lr: 0.000001  loss: 1.9860 (1.7616)  loss_scale: 16384.0000 (13855.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8168 (8.3782)  time: 0.7981 (0.5240 -- 2.4703)  data: 0.0045 (0.0003 -- 0.0510)  max mem: 16413
Epoch: [50]  [100/160]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000001  loss: 1.8084 (1.7753)  loss_scale: 16384.0000 (14356.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0832 (8.4614)  time: 0.9190 (0.5268 -- 5.0965)  data: 0.3142 (0.0003 -- 4.5708)  max mem: 16413
Epoch: [50]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000001  loss: 1.7376 (1.7790)  loss_scale: 16384.0000 (14691.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5974 (8.5500)  time: 0.8807 (0.5028 -- 4.6801)  data: 0.3357 (0.0004 -- 4.1710)  max mem: 16413
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000001  loss: 1.8393 (1.7952)  loss_scale: 16384.0000 (14931.5177)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2859 (8.6594)  time: 0.8341 (0.5285 -- 3.8122)  data: 0.2897 (0.0002 -- 3.2584)  max mem: 16413
[2023-08-31 13:10:55,797] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:10:55,797] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:10:55,797] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:10:55,798] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:10:57,791] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8157
[2023-08-31 13:10:57,791] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8157
[2023-08-31 13:10:57,791] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:10:57,791] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:10:57,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000001  loss: 1.7961 (1.7949)  loss_scale: 16384.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8041 (8.7874)  time: 0.6617 (0.4860 -- 3.3961)  data: 0.1436 (0.0002 -- 2.8567)  max mem: 16413
Epoch: [50] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000001  loss: 1.7961 (1.8266)  loss_scale: 16384.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8041 (8.7874)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.4044 (0.4044)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5670 (2.5670 -- 2.5670)  data: 2.3096 (2.3096 -- 2.3096)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5249 (0.7776)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4492 (0.1911 -- 2.5670)  data: 0.2359 (0.0006 -- 2.3096)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5772 (0.7269)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2158 (0.1690 -- 0.4771)  data: 0.0144 (0.0001 -- 0.2743)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6374 (0.7802)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2013 (0.1329 -- 0.4771)  data: 0.0141 (0.0001 -- 0.2743)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 78.838 Acc@5 97.095 loss 0.745
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 81.12%
Epoch: [51]  [  0/160]  eta: 0:19:34  lr: 0.000041  min_lr: 0.000001  loss: 2.0507 (2.0507)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8069 (7.8069)  time: 7.3422 (7.3422 -- 7.3422)  data: 6.6249 (6.6249 -- 6.6249)  max mem: 16413
[2023-08-31 13:11:30,531] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8178
[2023-08-31 13:11:30,531] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8178
[2023-08-31 13:11:30,531] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:11:30,531] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:11:30,531] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [51]  [ 20/160]  eta: 0:02:46  lr: 0.000041  min_lr: 0.000001  loss: 1.7997 (1.8206)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7540 (8.7663)  time: 0.8823 (0.5204 -- 2.7392)  data: 0.0798 (0.0002 -- 1.5607)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:01:58  lr: 0.000041  min_lr: 0.000001  loss: 1.6891 (1.8063)  loss_scale: 8192.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8245 (8.3697)  time: 0.7812 (0.5265 -- 3.2331)  data: 0.0194 (0.0006 -- 0.3518)  max mem: 16413
Epoch: [51]  [ 60/160]  eta: 0:01:34  lr: 0.000041  min_lr: 0.000001  loss: 1.7012 (1.8068)  loss_scale: 8192.0000 (10609.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7415 (8.5784)  time: 0.8531 (0.5193 -- 2.2250)  data: 0.0781 (0.0002 -- 0.9329)  max mem: 16413
Epoch: [51]  [ 80/160]  eta: 0:01:17  lr: 0.000041  min_lr: 0.000001  loss: 1.7599 (1.8114)  loss_scale: 8192.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0983 (8.4864)  time: 1.0621 (0.5261 -- 5.6557)  data: 0.5208 (0.0003 -- 5.1285)  max mem: 16413
Epoch: [51]  [100/160]  eta: 0:00:56  lr: 0.000041  min_lr: 0.000001  loss: 1.7977 (1.8192)  loss_scale: 8192.0000 (9651.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5629 (8.3199)  time: 0.7723 (0.5351 -- 3.0289)  data: 0.2219 (0.0003 -- 2.4904)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:37  lr: 0.000041  min_lr: 0.000001  loss: 1.8976 (1.8338)  loss_scale: 8192.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4025 (8.2865)  time: 0.8948 (0.5145 -- 4.4996)  data: 0.3486 (0.0004 -- 3.9625)  max mem: 16413
Epoch: [51]  [140/160]  eta: 0:00:18  lr: 0.000041  min_lr: 0.000001  loss: 1.7875 (1.8383)  loss_scale: 8192.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3379 (8.4492)  time: 0.7527 (0.5289 -- 3.1651)  data: 0.2033 (0.0008 -- 2.6324)  max mem: 16413
[2023-08-31 13:13:19,756] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:13:19,756] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:13:19,757] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:13:19,758] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.8777 (1.8405)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8695 (8.4190)  time: 0.7132 (0.4963 -- 2.4918)  data: 0.1559 (0.0002 -- 1.9781)  max mem: 16413
Epoch: [51] Total time: 0:02:21 (0.8815 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.8777 (1.8417)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8695 (8.4190)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3800 (0.3800)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3828 (2.3828 -- 2.3828)  data: 2.1766 (2.1766 -- 2.1766)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5426 (0.7901)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4458 (0.2001 -- 2.3828)  data: 0.2367 (0.0004 -- 2.1766)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5322 (0.7113)  acc1: 88.8889 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2225 (0.1697 -- 0.6398)  data: 0.0215 (0.0001 -- 0.4187)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6328 (0.7767)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2085 (0.1325 -- 0.6398)  data: 0.0212 (0.0001 -- 0.4187)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 80.290 Acc@5 96.473 loss 0.737
Accuracy of the network on the 482 val images: 80.29%
Max accuracy: 81.12%
Epoch: [52]  [  0/160]  eta: 0:19:21  lr: 0.000040  min_lr: 0.000001  loss: 1.7241 (1.7241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9129 (9.9129)  time: 7.2624 (7.2624 -- 7.2624)  data: 6.7057 (6.7057 -- 6.7057)  max mem: 16413
Epoch: [52]  [ 20/160]  eta: 0:02:31  lr: 0.000040  min_lr: 0.000001  loss: 1.5978 (1.6971)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9620 (8.6177)  time: 0.7707 (0.5450 -- 1.9541)  data: 0.0429 (0.0003 -- 0.6923)  max mem: 16413
Epoch: [52]  [ 40/160]  eta: 0:02:01  lr: 0.000040  min_lr: 0.000001  loss: 1.7842 (1.7371)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2231 (8.4000)  time: 0.9346 (0.5197 -- 3.2666)  data: 0.0786 (0.0002 -- 1.0813)  max mem: 16413
Epoch: [52]  [ 60/160]  eta: 0:01:35  lr: 0.000040  min_lr: 0.000001  loss: 1.9263 (1.8108)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4229 (8.7616)  time: 0.8416 (0.5249 -- 2.8556)  data: 0.0016 (0.0003 -- 0.0054)  max mem: 16413
Epoch: [52]  [ 80/160]  eta: 0:01:16  lr: 0.000040  min_lr: 0.000001  loss: 1.8315 (1.8190)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0716 (8.7386)  time: 0.9698 (0.5255 -- 4.1446)  data: 0.0014 (0.0004 -- 0.0054)  max mem: 16413
Epoch: [52]  [100/160]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000001  loss: 1.8673 (1.8274)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1635 (8.5785)  time: 0.8889 (0.5246 -- 3.8551)  data: 0.0016 (0.0002 -- 0.0054)  max mem: 16413
[2023-08-31 13:15:16,456] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8425
[2023-08-31 13:15:16,456] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8425
[2023-08-31 13:15:16,456] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:15:16,456] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:15:16,456] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [52]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000001  loss: 1.8421 (1.8349)  loss_scale: 8192.0000 (15300.7603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9539 (8.5483)  time: 0.8737 (0.5154 -- 3.4684)  data: 0.0012 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000001  loss: 1.8521 (1.8361)  loss_scale: 8192.0000 (14292.4255)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7910 (8.4964)  time: 0.9141 (0.5116 -- 3.2020)  data: 0.0013 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.8791 (1.8360)  loss_scale: 8192.0000 (13568.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4971 (8.4109)  time: 0.6414 (0.4950 -- 2.6083)  data: 0.0009 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [52] Total time: 0:02:23 (0.8967 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.8791 (1.8517)  loss_scale: 8192.0000 (13568.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4971 (8.4109)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3765 (0.3765)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3493 (2.3493 -- 2.3493)  data: 2.0851 (2.0851 -- 2.0851)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5176 (0.7656)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4162 (0.1931 -- 2.3493)  data: 0.1954 (0.0005 -- 2.0851)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5437 (0.6994)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2240 (0.1692 -- 0.4840)  data: 0.0186 (0.0001 -- 0.3047)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6293 (0.7658)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2067 (0.1332 -- 0.4840)  data: 0.0183 (0.0001 -- 0.3047)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 80.705 Acc@5 96.680 loss 0.730
Accuracy of the network on the 482 val images: 80.71%
Max accuracy: 81.12%
Epoch: [53]  [  0/160]  eta: 0:20:54  lr: 0.000040  min_lr: 0.000001  loss: 2.0296 (2.0296)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8409 (4.8409)  time: 7.8420 (7.8420 -- 7.8420)  data: 7.3097 (7.3097 -- 7.3097)  max mem: 16413
Epoch: [53]  [ 20/160]  eta: 0:02:37  lr: 0.000040  min_lr: 0.000001  loss: 1.8325 (1.8043)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0788 (8.7492)  time: 0.7881 (0.5256 -- 3.1792)  data: 0.2358 (0.0001 -- 2.6305)  max mem: 16413
Epoch: [53]  [ 40/160]  eta: 0:02:10  lr: 0.000040  min_lr: 0.000001  loss: 1.7573 (1.7792)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1231 (8.7399)  time: 1.0430 (0.5247 -- 4.6950)  data: 0.4980 (0.0007 -- 4.1583)  max mem: 16413
Epoch: [53]  [ 60/160]  eta: 0:01:38  lr: 0.000040  min_lr: 0.000001  loss: 2.0136 (1.8599)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1642 (8.8674)  time: 0.7738 (0.5214 -- 3.3447)  data: 0.2239 (0.0003 -- 2.8282)  max mem: 16413
[2023-08-31 13:17:17,048] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:17:17,048] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:17:17,048] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:17:17,048] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [53]  [ 80/160]  eta: 0:01:13  lr: 0.000040  min_lr: 0.000001  loss: 1.7329 (1.8326)  loss_scale: 8192.0000 (8899.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3260 (8.6241)  time: 0.7427 (0.5386 -- 2.6311)  data: 0.1757 (0.0005 -- 2.1048)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000001  loss: 1.9584 (1.8613)  loss_scale: 16384.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7860 (8.6887)  time: 1.0133 (0.5160 -- 4.5923)  data: 0.1087 (0.0003 -- 1.3640)  max mem: 16413
Epoch: [53]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000001  loss: 1.8108 (1.8561)  loss_scale: 16384.0000 (11374.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8067 (8.4805)  time: 0.8507 (0.5215 -- 4.3226)  data: 0.0012 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000001  loss: 1.7627 (1.8451)  loss_scale: 16384.0000 (12084.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1876 (8.4991)  time: 0.8532 (0.5268 -- 4.5910)  data: 0.0015 (0.0005 -- 0.0034)  max mem: 16413
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.9070 (1.8422)  loss_scale: 16384.0000 (12595.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1113 (8.6051)  time: 0.7391 (0.4970 -- 4.0382)  data: 0.0007 (0.0001 -- 0.0023)  max mem: 16413
Epoch: [53] Total time: 0:02:23 (0.8962 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.9070 (1.8184)  loss_scale: 16384.0000 (12595.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1113 (8.6051)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5280 (0.5280)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4213 (2.4213 -- 2.4213)  data: 2.1579 (2.1579 -- 2.1579)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5280 (0.8055)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4516 (0.2034 -- 2.4213)  data: 0.2268 (0.0008 -- 2.1579)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5119 (0.7146)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2251 (0.1689 -- 0.5752)  data: 0.0208 (0.0001 -- 0.3148)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6327 (0.7716)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (97.0954)  time: 0.2069 (0.1323 -- 0.5752)  data: 0.0199 (0.0001 -- 0.3148)  max mem: 16413
Val: Total time: 0:00:07 (0.2943 s / it)
* Acc@1 79.046 Acc@5 96.888 loss 0.745
Accuracy of the network on the 482 val images: 79.05%
Max accuracy: 81.12%
Epoch: [54]  [  0/160]  eta: 0:20:31  lr: 0.000040  min_lr: 0.000001  loss: 2.3500 (2.3500)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1662 (7.1662)  time: 7.6963 (7.6963 -- 7.6963)  data: 6.5133 (6.5133 -- 6.5133)  max mem: 16413
Epoch: [54]  [ 20/160]  eta: 0:02:42  lr: 0.000040  min_lr: 0.000001  loss: 1.8296 (1.7800)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1954 (8.5805)  time: 0.8310 (0.5199 -- 2.9612)  data: 0.1691 (0.0007 -- 1.5264)  max mem: 16413
Epoch: [54]  [ 40/160]  eta: 0:02:02  lr: 0.000040  min_lr: 0.000001  loss: 1.7530 (1.7907)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5008 (8.8882)  time: 0.8848 (0.5276 -- 3.2200)  data: 0.2536 (0.0003 -- 2.6572)  max mem: 16413
[2023-08-31 13:19:22,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:19:22,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:19:22,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:19:22,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [54]  [ 60/160]  eta: 0:01:40  lr: 0.000040  min_lr: 0.000001  loss: 1.7976 (1.7864)  loss_scale: 32768.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4188 (9.0504)  time: 0.9779 (0.5169 -- 5.2460)  data: 0.4357 (0.0003 -- 4.7237)  max mem: 16413
[2023-08-31 13:19:46,116] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8707
[2023-08-31 13:19:46,117] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:19:46,117] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8707
[2023-08-31 13:19:46,117] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:19:46,117] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [54]  [ 80/160]  eta: 0:01:16  lr: 0.000040  min_lr: 0.000001  loss: 1.9469 (1.8251)  loss_scale: 16384.0000 (21440.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0329 (9.0282)  time: 0.8151 (0.5184 -- 3.0869)  data: 0.2723 (0.0001 -- 2.5407)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:56  lr: 0.000040  min_lr: 0.000001  loss: 1.8733 (1.8258)  loss_scale: 16384.0000 (20439.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4854 (8.9616)  time: 0.9032 (0.5276 -- 4.1237)  data: 0.3518 (0.0003 -- 3.5955)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:37  lr: 0.000040  min_lr: 0.000001  loss: 1.9131 (1.8295)  loss_scale: 16384.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8144 (8.9013)  time: 0.8314 (0.5208 -- 3.0395)  data: 0.2846 (0.0003 -- 2.4644)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000040  min_lr: 0.000001  loss: 1.8313 (1.8287)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6631 (8.8461)  time: 0.7978 (0.5242 -- 3.0364)  data: 0.2469 (0.0004 -- 2.4823)  max mem: 16413
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000001  loss: 1.8698 (1.8245)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7142 (8.8247)  time: 0.7015 (0.4948 -- 3.9079)  data: 0.1883 (0.0002 -- 3.3727)  max mem: 16413
Epoch: [54] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000001  loss: 1.8698 (1.8207)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7142 (8.8247)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.4586 (0.4586)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4253 (2.4253 -- 2.4253)  data: 2.2155 (2.2155 -- 2.2155)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5366 (0.7714)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4271 (0.1993 -- 2.4253)  data: 0.2112 (0.0006 -- 2.2155)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5406 (0.7040)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2162 (0.1694 -- 0.3172)  data: 0.0106 (0.0001 -- 0.1009)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6164 (0.7626)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2005 (0.1330 -- 0.3172)  data: 0.0103 (0.0001 -- 0.1009)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 80.498 Acc@5 97.095 loss 0.736
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 81.12%
Epoch: [55]  [  0/160]  eta: 0:22:33  lr: 0.000040  min_lr: 0.000001  loss: 1.4587 (1.4587)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1076 (12.1076)  time: 8.4585 (8.4585 -- 8.4585)  data: 6.0434 (6.0434 -- 6.0434)  max mem: 16413
Epoch: [55]  [ 20/160]  eta: 0:02:49  lr: 0.000040  min_lr: 0.000001  loss: 1.8125 (1.9074)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4885 (9.6306)  time: 0.8517 (0.5063 -- 3.7159)  data: 0.0971 (0.0001 -- 1.4635)  max mem: 16413
[2023-08-31 13:21:45,884] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:21:45,884] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:21:45,884] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:21:45,884] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:21:50,717] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8840
[2023-08-31 13:21:50,717] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8840
[2023-08-31 13:21:50,718] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:21:50,718] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:21:50,718] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [55]  [ 40/160]  eta: 0:02:04  lr: 0.000040  min_lr: 0.000001  loss: 1.9280 (1.9318)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6393 (8.7099)  time: 0.8545 (0.5202 -- 3.2016)  data: 0.1291 (0.0004 -- 0.9350)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:37  lr: 0.000040  min_lr: 0.000001  loss: 1.8703 (1.8991)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4475 (8.6665)  time: 0.8354 (0.5206 -- 2.4879)  data: 0.1798 (0.0004 -- 1.2878)  max mem: 16413
Epoch: [55]  [ 80/160]  eta: 0:01:15  lr: 0.000040  min_lr: 0.000001  loss: 1.8440 (1.8653)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3462 (8.6573)  time: 0.8523 (0.5329 -- 2.5512)  data: 0.1996 (0.0005 -- 2.0013)  max mem: 16413
Epoch: [55]  [100/160]  eta: 0:00:55  lr: 0.000040  min_lr: 0.000001  loss: 1.8780 (1.8538)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1838 (8.6863)  time: 0.8458 (0.5501 -- 2.7814)  data: 0.2188 (0.0002 -- 2.2484)  max mem: 16413
[2023-08-31 13:22:41,986] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8901
[2023-08-31 13:22:41,986] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:22:41,986] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8901
[2023-08-31 13:22:41,986] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:22:41,987] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [55]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000001  loss: 1.8905 (1.8526)  loss_scale: 8192.0000 (15571.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6720 (8.6168)  time: 0.8874 (0.5316 -- 2.6305)  data: 0.0015 (0.0006 -- 0.0035)  max mem: 16413
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000001  loss: 1.9894 (1.8664)  loss_scale: 8192.0000 (14524.8227)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7712 (8.5737)  time: 0.8892 (0.5272 -- 4.7627)  data: 0.0015 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.8679 (1.8760)  loss_scale: 8192.0000 (13772.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5256 (8.7054)  time: 0.7076 (0.4954 -- 2.3450)  data: 0.0011 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [55] Total time: 0:02:22 (0.8902 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.8679 (1.8460)  loss_scale: 8192.0000 (13772.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5256 (8.7054)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4819 (0.4819)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2954 (2.2954 -- 2.2954)  data: 2.0623 (2.0623 -- 2.0623)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5402 (0.7757)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4246 (0.2016 -- 2.2954)  data: 0.2063 (0.0009 -- 2.0623)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5402 (0.7051)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.8254)  time: 0.2286 (0.1692 -- 0.4528)  data: 0.0238 (0.0001 -- 0.2656)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5935 (0.7715)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (96.2656)  time: 0.2110 (0.1336 -- 0.4528)  data: 0.0235 (0.0001 -- 0.2656)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 79.668 Acc@5 96.473 loss 0.737
Accuracy of the network on the 482 val images: 79.67%
Max accuracy: 81.12%
Epoch: [56]  [  0/160]  eta: 0:18:09  lr: 0.000039  min_lr: 0.000001  loss: 2.0154 (2.0154)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7019 (13.7019)  time: 6.8119 (6.8119 -- 6.8119)  data: 6.2695 (6.2695 -- 6.2695)  max mem: 16413
Epoch: [56]  [ 20/160]  eta: 0:02:40  lr: 0.000039  min_lr: 0.000001  loss: 1.8946 (1.9271)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2989 (9.6798)  time: 0.8649 (0.5303 -- 3.2844)  data: 0.2373 (0.0007 -- 2.7395)  max mem: 16413
[2023-08-31 13:24:18,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[9.345830379155042e-07, 9.345830379155042e-07, 1.2461107172206725e-06, 1.2461107172206725e-06, 1.6614809562942299e-06, 1.6614809562942299e-06, 2.2153079417256397e-06, 2.2153079417256397e-06, 2.953743922300853e-06, 2.953743922300853e-06, 3.938325229734471e-06, 3.938325229734471e-06, 5.251100306312628e-06, 5.251100306312628e-06, 7.001467075083503e-06, 7.001467075083503e-06, 9.33528943344467e-06, 9.33528943344467e-06, 1.2447052577926228e-05, 1.2447052577926228e-05, 1.659607010390164e-05, 1.659607010390164e-05, 2.212809347186885e-05, 2.212809347186885e-05, 2.950412462915847e-05, 2.950412462915847e-05, 3.9338832838877956e-05, 3.9338832838877956e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 13:24:18,087] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=16.699137927288614, CurrSamplesPerSec=21.97638085790222, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:00  lr: 0.000039  min_lr: 0.000001  loss: 1.6846 (1.8237)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2399 (8.9526)  time: 0.8457 (0.5189 -- 2.4477)  data: 0.2961 (0.0005 -- 1.9266)  max mem: 16413
Epoch: [56]  [ 60/160]  eta: 0:01:37  lr: 0.000039  min_lr: 0.000001  loss: 1.7437 (1.8017)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8173 (8.8487)  time: 0.9151 (0.5295 -- 3.1943)  data: 0.0075 (0.0004 -- 0.1221)  max mem: 16413
[2023-08-31 13:24:45,074] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:24:45,075] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:24:45,078] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:24:45,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [56]  [ 80/160]  eta: 0:01:15  lr: 0.000039  min_lr: 0.000001  loss: 1.8766 (1.8233)  loss_scale: 16384.0000 (9304.4938)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4731 (9.0057)  time: 0.8555 (0.5291 -- 2.3736)  data: 0.1402 (0.0004 -- 1.8560)  max mem: 16413
[2023-08-31 13:25:11,595] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9060
[2023-08-31 13:25:11,595] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9060
[2023-08-31 13:25:11,595] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:25:11,595] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:25:11,595] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [56]  [100/160]  eta: 0:00:55  lr: 0.000039  min_lr: 0.000001  loss: 1.7146 (1.7805)  loss_scale: 16384.0000 (10625.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3743 (8.8716)  time: 0.8359 (0.5136 -- 2.6406)  data: 0.0376 (0.0004 -- 0.7115)  max mem: 16413
Epoch: [56]  [120/160]  eta: 0:00:36  lr: 0.000039  min_lr: 0.000001  loss: 1.9153 (1.7944)  loss_scale: 8192.0000 (10223.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6765 (8.9168)  time: 0.8498 (0.5383 -- 2.3681)  data: 0.0017 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000001  loss: 1.7874 (1.7925)  loss_scale: 8192.0000 (9934.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6596 (8.8008)  time: 0.8992 (0.5387 -- 3.8235)  data: 0.0017 (0.0003 -- 0.0079)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.9053 (1.8039)  loss_scale: 8192.0000 (9728.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2306 (8.8246)  time: 0.6914 (0.4967 -- 2.6115)  data: 0.0009 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [56] Total time: 0:02:21 (0.8841 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.9053 (1.7988)  loss_scale: 8192.0000 (9728.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2306 (8.8246)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3081 (0.3081)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4795 (2.4795 -- 2.4795)  data: 2.2481 (2.2481 -- 2.2481)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5070 (0.7581)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4194 (0.1997 -- 2.4795)  data: 0.2055 (0.0009 -- 2.2481)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5070 (0.6732)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2173 (0.1695 -- 0.4796)  data: 0.0159 (0.0001 -- 0.3021)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5950 (0.7450)  acc1: 85.7143 (79.6681)  acc5: 100.0000 (95.4357)  time: 0.2019 (0.1332 -- 0.4796)  data: 0.0156 (0.0001 -- 0.3021)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 81.328 Acc@5 95.851 loss 0.715
Accuracy of the network on the 482 val images: 81.33%
[2023-08-31 13:26:07,688] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 13:26:07,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 13:26:07,690] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 13:26:07,690] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 13:26:09,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 13:26:09,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 81.33%
Epoch: [57]  [  0/160]  eta: 0:20:06  lr: 0.000039  min_lr: 0.000001  loss: 2.1044 (2.1044)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1699 (9.1699)  time: 7.5436 (7.5436 -- 7.5436)  data: 7.0003 (7.0003 -- 7.0003)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:46  lr: 0.000039  min_lr: 0.000001  loss: 1.9219 (1.9027)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1845 (8.8207)  time: 0.8687 (0.5225 -- 2.4333)  data: 0.3251 (0.0005 -- 1.8896)  max mem: 16413
Epoch: [57]  [ 40/160]  eta: 0:02:14  lr: 0.000039  min_lr: 0.000001  loss: 1.5344 (1.7727)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9688 (8.4321)  time: 1.0541 (0.5139 -- 5.2997)  data: 0.5165 (0.0002 -- 4.7845)  max mem: 16413
Epoch: [57]  [ 60/160]  eta: 0:01:36  lr: 0.000039  min_lr: 0.000001  loss: 1.7149 (1.7682)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2660 (8.1213)  time: 0.6356 (0.5088 -- 1.8320)  data: 0.0903 (0.0002 -- 1.2767)  max mem: 16413
[2023-08-31 13:27:15,801] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:27:15,801] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:27:15,802] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:27:15,802] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [57]  [ 80/160]  eta: 0:01:16  lr: 0.000039  min_lr: 0.000001  loss: 1.7505 (1.7695)  loss_scale: 16384.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8619 (8.3157)  time: 0.9617 (0.5288 -- 3.0970)  data: 0.3897 (0.0006 -- 2.5790)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:56  lr: 0.000039  min_lr: 0.000001  loss: 1.8802 (1.7868)  loss_scale: 16384.0000 (10787.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2503 (8.3916)  time: 0.8633 (0.5246 -- 3.6422)  data: 0.3180 (0.0005 -- 3.1198)  max mem: 16413
Epoch: [57]  [120/160]  eta: 0:00:38  lr: 0.000039  min_lr: 0.000001  loss: 1.8523 (1.7840)  loss_scale: 16384.0000 (11712.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0767 (8.5246)  time: 1.0053 (0.5332 -- 4.2266)  data: 0.4595 (0.0004 -- 3.7044)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000001  loss: 1.9223 (1.8004)  loss_scale: 16384.0000 (12375.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8989 (8.5175)  time: 0.7524 (0.5192 -- 3.4150)  data: 0.2120 (0.0002 -- 2.8795)  max mem: 16413
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.8799 (1.8077)  loss_scale: 16384.0000 (12851.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2034 (8.5694)  time: 0.6794 (0.4953 -- 2.3065)  data: 0.1470 (0.0002 -- 1.7896)  max mem: 16413
Epoch: [57] Total time: 0:02:23 (0.8964 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.8799 (1.8389)  loss_scale: 16384.0000 (12851.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2034 (8.5694)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.3451 (0.3451)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5973 (2.5973 -- 2.5973)  data: 2.3678 (2.3678 -- 2.3678)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5454 (0.7788)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (95.9596)  time: 0.4632 (0.2003 -- 2.5973)  data: 0.2510 (0.0004 -- 2.3678)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5055 (0.6747)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (96.2963)  time: 0.2180 (0.1701 -- 0.5864)  data: 0.0198 (0.0001 -- 0.3712)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5860 (0.7468)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.4357)  time: 0.2039 (0.1329 -- 0.5864)  data: 0.0195 (0.0001 -- 0.3712)  max mem: 16413
Val: Total time: 0:00:07 (0.2941 s / it)
* Acc@1 80.705 Acc@5 96.058 loss 0.709
Accuracy of the network on the 482 val images: 80.71%
Max accuracy: 81.33%
Epoch: [58]  [  0/160]  eta: 0:24:00  lr: 0.000039  min_lr: 0.000001  loss: 1.7793 (1.7793)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1672 (9.1672)  time: 9.0033 (9.0033 -- 9.0033)  data: 5.9351 (5.9351 -- 5.9351)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:47  lr: 0.000039  min_lr: 0.000001  loss: 1.6177 (1.6677)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6914 (8.2798)  time: 0.8047 (0.5340 -- 3.7395)  data: 0.0022 (0.0004 -- 0.0144)  max mem: 16413
[2023-08-31 13:29:21,715] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:29:21,715] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:29:21,716] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:29:21,716] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [58]  [ 40/160]  eta: 0:02:11  lr: 0.000039  min_lr: 0.000001  loss: 1.8585 (1.7408)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2482 (8.4635)  time: 0.9915 (0.5220 -- 4.7898)  data: 0.0082 (0.0003 -- 0.1372)  max mem: 16413
[2023-08-31 13:29:41,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9337
[2023-08-31 13:29:41,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:29:41,999] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9337
[2023-08-31 13:29:41,999] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:29:41,999] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [ 60/160]  eta: 0:01:43  lr: 0.000039  min_lr: 0.000001  loss: 1.8443 (1.7579)  loss_scale: 32768.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8065 (8.4299)  time: 0.9028 (0.4958 -- 6.1874)  data: 0.0014 (0.0004 -- 0.0033)  max mem: 16413
[2023-08-31 13:29:47,817] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9344
[2023-08-31 13:29:47,817] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9344
[2023-08-31 13:29:47,858] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:29:47,858] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:29:47,858] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [58]  [ 80/160]  eta: 0:01:20  lr: 0.000039  min_lr: 0.000001  loss: 1.7711 (1.7614)  loss_scale: 8192.0000 (18710.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0846 (8.3980)  time: 0.9303 (0.5238 -- 3.7861)  data: 0.0010 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:58  lr: 0.000039  min_lr: 0.000001  loss: 2.0245 (1.8040)  loss_scale: 8192.0000 (16627.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0657 (8.5525)  time: 0.8110 (0.5095 -- 3.6409)  data: 0.0010 (0.0003 -- 0.0020)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:37  lr: 0.000039  min_lr: 0.000001  loss: 1.9288 (1.8404)  loss_scale: 8192.0000 (15233.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6540 (8.5380)  time: 0.8344 (0.5237 -- 3.0505)  data: 0.0019 (0.0003 -- 0.0061)  max mem: 16413
Epoch: [58]  [140/160]  eta: 0:00:18  lr: 0.000039  min_lr: 0.000001  loss: 1.9000 (1.8469)  loss_scale: 8192.0000 (14234.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2198 (8.4534)  time: 0.7762 (0.5200 -- 2.7463)  data: 0.0015 (0.0005 -- 0.0024)  max mem: 16413
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000001  loss: 1.8916 (1.8475)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0311 (8.4107)  time: 0.7003 (0.4942 -- 3.3309)  data: 0.0008 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [58] Total time: 0:02:23 (0.8968 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000001  loss: 1.8916 (1.8445)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0311 (8.4107)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3595 (0.3595)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4184 (2.4184 -- 2.4184)  data: 2.1836 (2.1836 -- 2.1836)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5320 (0.7492)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4524 (0.1978 -- 2.4184)  data: 0.2412 (0.0006 -- 2.1836)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5033 (0.6720)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2239 (0.1690 -- 0.6589)  data: 0.0238 (0.0001 -- 0.4592)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6129 (0.7446)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (96.2656)  time: 0.2094 (0.1334 -- 0.6589)  data: 0.0235 (0.0001 -- 0.4592)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 80.913 Acc@5 96.473 loss 0.708
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 81.33%
Epoch: [59]  [  0/160]  eta: 0:20:58  lr: 0.000039  min_lr: 0.000001  loss: 1.6796 (1.6796)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9727 (8.9727)  time: 7.8641 (7.8641 -- 7.8641)  data: 6.4430 (6.4430 -- 6.4430)  max mem: 16413
Epoch: [59]  [ 20/160]  eta: 0:02:39  lr: 0.000039  min_lr: 0.000001  loss: 1.6803 (1.7902)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8850 (8.0751)  time: 0.8056 (0.5223 -- 3.6430)  data: 0.0666 (0.0006 -- 1.3015)  max mem: 16413
[2023-08-31 13:31:48,589] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:31:48,589] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:31:48,595] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:31:48,596] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [59]  [ 40/160]  eta: 0:02:04  lr: 0.000038  min_lr: 0.000001  loss: 1.9546 (1.8621)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8126 (8.1983)  time: 0.9270 (0.5284 -- 3.7694)  data: 0.2323 (0.0003 -- 2.0723)  max mem: 16413
Epoch: [59]  [ 60/160]  eta: 0:01:37  lr: 0.000038  min_lr: 0.000001  loss: 1.8702 (1.8541)  loss_scale: 16384.0000 (11952.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8334 (8.3367)  time: 0.8466 (0.5229 -- 3.0483)  data: 0.0011 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:16  lr: 0.000038  min_lr: 0.000001  loss: 1.6198 (1.7874)  loss_scale: 16384.0000 (13046.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4428 (8.3204)  time: 0.8869 (0.5160 -- 5.0241)  data: 0.0016 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:00:57  lr: 0.000038  min_lr: 0.000001  loss: 1.9290 (1.8249)  loss_scale: 16384.0000 (13707.4059)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2050 (8.3536)  time: 0.9701 (0.5188 -- 4.2600)  data: 0.0012 (0.0004 -- 0.0027)  max mem: 16413
[2023-08-31 13:32:52,248] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9545
[2023-08-31 13:32:52,248] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9545
[2023-08-31 13:32:52,249] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:32:52,249] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:32:52,249] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000001  loss: 1.8770 (1.8129)  loss_scale: 8192.0000 (13066.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5331 (8.4675)  time: 0.7324 (0.5179 -- 3.2529)  data: 0.0016 (0.0001 -- 0.0066)  max mem: 16413
Epoch: [59]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000001  loss: 1.7402 (1.8129)  loss_scale: 8192.0000 (12375.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5961 (8.4942)  time: 0.7925 (0.5219 -- 2.4305)  data: 0.0016 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.7017 (1.8062)  loss_scale: 8192.0000 (11878.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5859 (8.5517)  time: 0.7291 (0.4962 -- 1.9254)  data: 0.1210 (0.0002 -- 1.3848)  max mem: 16413
Epoch: [59] Total time: 0:02:21 (0.8820 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.7017 (1.7925)  loss_scale: 8192.0000 (11878.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5859 (8.5517)
[2023-08-31 13:33:33,100] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-08-31 13:33:33,103] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-08-31 13:33:33,105] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-08-31 13:33:33,105] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-08-31 13:33:34,252] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-08-31 13:33:34,253] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3465 (0.3465)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2745 (2.2745 -- 2.2745)  data: 2.0276 (2.0276 -- 2.0276)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5079 (0.7647)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4511 (0.2027 -- 2.2745)  data: 0.2272 (0.0007 -- 2.0276)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5484 (0.6845)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2308 (0.1697 -- 0.5533)  data: 0.0237 (0.0001 -- 0.3414)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6461 (0.7491)  acc1: 85.7143 (80.9129)  acc5: 100.0000 (95.8506)  time: 0.2125 (0.1330 -- 0.5533)  data: 0.0230 (0.0001 -- 0.3414)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 82.365 Acc@5 96.266 loss 0.709
Accuracy of the network on the 482 val images: 82.37%
[2023-08-31 13:33:42,134] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 13:33:42,136] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 13:33:42,136] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 13:33:42,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 13:33:43,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 13:33:43,299] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [60]  [  0/160]  eta: 0:23:19  lr: 0.000038  min_lr: 0.000001  loss: 2.0748 (2.0748)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9095 (11.9095)  time: 8.7447 (8.7447 -- 8.7447)  data: 8.2061 (8.2061 -- 8.2061)  max mem: 16413
Epoch: [60]  [ 20/160]  eta: 0:02:39  lr: 0.000038  min_lr: 0.000001  loss: 1.6532 (1.7438)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9814 (9.2461)  time: 0.7592 (0.5311 -- 2.3280)  data: 0.2120 (0.0003 -- 1.8147)  max mem: 16413
Epoch: [60]  [ 40/160]  eta: 0:02:02  lr: 0.000038  min_lr: 0.000001  loss: 1.7979 (1.7756)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7975 (8.6791)  time: 0.8948 (0.5297 -- 2.8423)  data: 0.2805 (0.0003 -- 1.8227)  max mem: 16413
Epoch: [60]  [ 60/160]  eta: 0:01:35  lr: 0.000038  min_lr: 0.000001  loss: 1.8506 (1.7903)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5943 (8.8155)  time: 0.8300 (0.5200 -- 2.7349)  data: 0.1404 (0.0003 -- 2.2161)  max mem: 16413
[2023-08-31 13:34:55,015] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:34:55,015] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:34:55,015] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:34:55,015] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [60]  [ 80/160]  eta: 0:01:15  lr: 0.000038  min_lr: 0.000001  loss: 1.8379 (1.8169)  loss_scale: 8192.0000 (8899.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1583 (8.8438)  time: 0.8806 (0.5334 -- 3.0289)  data: 0.0015 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000001  loss: 1.7790 (1.8215)  loss_scale: 16384.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6942 (8.9179)  time: 0.9123 (0.5275 -- 3.3830)  data: 0.0194 (0.0002 -- 0.3529)  max mem: 16413
Epoch: [60]  [120/160]  eta: 0:00:36  lr: 0.000038  min_lr: 0.000001  loss: 1.8805 (1.8213)  loss_scale: 16384.0000 (11374.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4700 (8.8437)  time: 0.8417 (0.5219 -- 2.6162)  data: 0.1062 (0.0003 -- 2.0790)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000001  loss: 1.8556 (1.8368)  loss_scale: 16384.0000 (12084.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3916 (8.9238)  time: 0.9266 (0.5338 -- 2.4284)  data: 0.0358 (0.0003 -- 0.6315)  max mem: 16413
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.7029 (1.8318)  loss_scale: 16384.0000 (12595.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2825 (8.8507)  time: 0.6169 (0.4950 -- 2.3515)  data: 0.0058 (0.0003 -- 0.1029)  max mem: 16413
Epoch: [60] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.7029 (1.8163)  loss_scale: 16384.0000 (12595.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2825 (8.8507)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.2973 (0.2973)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6337 (2.6337 -- 2.6337)  data: 2.3665 (2.3665 -- 2.3665)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5478 (0.7559)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4295 (0.1973 -- 2.6337)  data: 0.2161 (0.0006 -- 2.3665)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5346 (0.6701)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (95.7672)  time: 0.2104 (0.1687 -- 0.3919)  data: 0.0115 (0.0001 -- 0.2159)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5560 (0.7448)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.4357)  time: 0.1970 (0.1333 -- 0.3919)  data: 0.0112 (0.0001 -- 0.2159)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 80.913 Acc@5 96.058 loss 0.701
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 82.37%
Epoch: [61]  [  0/160]  eta: 0:18:43  lr: 0.000038  min_lr: 0.000001  loss: 2.0788 (2.0788)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7547 (6.7547)  time: 7.0206 (7.0206 -- 7.0206)  data: 6.4849 (6.4849 -- 6.4849)  max mem: 16413
Epoch: [61]  [ 20/160]  eta: 0:02:54  lr: 0.000038  min_lr: 0.000001  loss: 1.7379 (1.7590)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1700 (8.6414)  time: 0.9568 (0.5301 -- 5.4278)  data: 0.1148 (0.0004 -- 2.2597)  max mem: 16413
Epoch: [61]  [ 40/160]  eta: 0:02:11  lr: 0.000038  min_lr: 0.000001  loss: 1.7591 (1.7567)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5569 (8.5125)  time: 0.9354 (0.5275 -- 3.7824)  data: 0.0013 (0.0004 -- 0.0026)  max mem: 16413
[2023-08-31 13:36:58,580] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:36:58,580] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:36:58,580] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:36:58,581] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:37:13,968] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9820
[2023-08-31 13:37:13,969] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:37:13,969] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9820
[2023-08-31 13:37:13,969] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:37:13,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [61]  [ 60/160]  eta: 0:01:40  lr: 0.000038  min_lr: 0.000001  loss: 1.7365 (1.7699)  loss_scale: 32768.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2321 (8.6048)  time: 0.8243 (0.5105 -- 4.3718)  data: 0.0014 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [61]  [ 80/160]  eta: 0:01:19  lr: 0.000038  min_lr: 0.000001  loss: 1.6826 (1.7506)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9212 (8.4689)  time: 0.9490 (0.5198 -- 4.0660)  data: 0.0015 (0.0004 -- 0.0063)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:56  lr: 0.000038  min_lr: 0.000001  loss: 1.8069 (1.7705)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3480 (8.6123)  time: 0.7668 (0.5135 -- 2.9076)  data: 0.0022 (0.0008 -- 0.0144)  max mem: 16413
Epoch: [61]  [120/160]  eta: 0:00:37  lr: 0.000038  min_lr: 0.000001  loss: 1.7262 (1.7778)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3839 (8.5757)  time: 0.9487 (0.5169 -- 4.4360)  data: 0.0020 (0.0002 -- 0.0153)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000038  min_lr: 0.000001  loss: 1.7861 (1.7846)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1797 (8.5625)  time: 0.8053 (0.5245 -- 3.0467)  data: 0.0014 (0.0008 -- 0.0023)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000001  loss: 1.8573 (1.7986)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1775 (8.5800)  time: 0.6542 (0.4943 -- 2.6921)  data: 0.0009 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [61] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000001  loss: 1.8573 (1.8172)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1775 (8.5800)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3225 (0.3225)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3488 (2.3488 -- 2.3488)  data: 2.1368 (2.1368 -- 2.1368)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4248 (0.7260)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (94.9495)  time: 0.4288 (0.2016 -- 2.3488)  data: 0.2145 (0.0006 -- 2.1368)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5515 (0.6588)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (95.2381)  time: 0.2293 (0.1698 -- 0.5121)  data: 0.0280 (0.0001 -- 0.3350)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6296 (0.7251)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (95.0207)  time: 0.2151 (0.1333 -- 0.5121)  data: 0.0277 (0.0001 -- 0.3350)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 82.988 Acc@5 96.058 loss 0.690
Accuracy of the network on the 482 val images: 82.99%
[2023-08-31 13:38:43,860] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 13:38:43,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 13:38:43,862] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 13:38:43,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 13:38:45,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 13:38:45,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.99%
Epoch: [62]  [  0/160]  eta: 0:21:16  lr: 0.000038  min_lr: 0.000001  loss: 1.6279 (1.6279)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6860 (5.6860)  time: 7.9775 (7.9775 -- 7.9775)  data: 7.4341 (7.4341 -- 7.4341)  max mem: 16413
Epoch: [62]  [ 20/160]  eta: 0:02:37  lr: 0.000038  min_lr: 0.000001  loss: 1.8878 (1.8533)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2241 (8.7144)  time: 0.7856 (0.5373 -- 2.9057)  data: 0.2391 (0.0004 -- 2.3745)  max mem: 16413
[2023-08-31 13:39:16,907] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:39:16,907] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:39:16,909] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:39:16,910] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:39:23,017] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9954
[2023-08-31 13:39:23,017] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9954
[2023-08-31 13:39:23,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:39:23,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:39:23,017] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [62]  [ 40/160]  eta: 0:02:06  lr: 0.000038  min_lr: 0.000001  loss: 1.7280 (1.7993)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6064 (9.2081)  time: 0.9829 (0.5337 -- 3.9016)  data: 0.4374 (0.0002 -- 3.3731)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:39  lr: 0.000038  min_lr: 0.000001  loss: 1.8919 (1.8340)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3366 (9.1308)  time: 0.8691 (0.5249 -- 3.8667)  data: 0.3220 (0.0004 -- 3.3485)  max mem: 16413
[2023-08-31 13:40:01,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=55, lr=[8.915777879211351e-07, 8.915777879211351e-07, 1.1887703838948467e-06, 1.1887703838948467e-06, 1.5850271785264623e-06, 1.5850271785264623e-06, 2.1133695713686163e-06, 2.1133695713686163e-06, 2.8178260951581553e-06, 2.8178260951581553e-06, 3.7571014602108737e-06, 3.7571014602108737e-06, 5.009468613614498e-06, 5.009468613614498e-06, 6.679291484819331e-06, 6.679291484819331e-06, 8.905721979759108e-06, 8.905721979759108e-06, 1.1874295973012144e-05, 1.1874295973012144e-05, 1.5832394630682858e-05, 1.5832394630682858e-05, 2.1109859507577145e-05, 2.1109859507577145e-05, 2.814647934343619e-05, 2.814647934343619e-05, 3.752863912458159e-05, 3.752863912458159e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 13:40:01,892] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=16.6677139881553, CurrSamplesPerSec=21.3861802125036, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:20  lr: 0.000038  min_lr: 0.000001  loss: 1.7234 (1.8105)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2792 (8.8068)  time: 1.0320 (0.5107 -- 4.8417)  data: 0.1753 (0.0002 -- 1.7423)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:57  lr: 0.000037  min_lr: 0.000001  loss: 1.7193 (1.8023)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2420 (8.7425)  time: 0.7802 (0.5066 -- 4.5033)  data: 0.0013 (0.0002 -- 0.0055)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000001  loss: 1.9292 (1.8196)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2295 (8.9615)  time: 0.8677 (0.5180 -- 4.0950)  data: 0.0017 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000001  loss: 1.8032 (1.8180)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9070 (8.8300)  time: 0.7675 (0.5134 -- 4.2041)  data: 0.0012 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.9474 (1.8152)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8188 (8.7285)  time: 0.6213 (0.4952 -- 1.5938)  data: 0.0009 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [62] Total time: 0:02:21 (0.8852 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.9474 (1.7900)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8188 (8.7285)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2530 (0.2530)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3868 (2.3868 -- 2.3868)  data: 2.1817 (2.1817 -- 2.1817)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4932 (0.7425)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (95.9596)  time: 0.4377 (0.2092 -- 2.3868)  data: 0.2211 (0.0010 -- 2.1817)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5410 (0.6629)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.2963)  time: 0.2259 (0.1690 -- 0.4586)  data: 0.0185 (0.0001 -- 0.2316)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5831 (0.7289)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2093 (0.1333 -- 0.4586)  data: 0.0180 (0.0001 -- 0.2316)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 82.988 Acc@5 96.266 loss 0.688
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [63]  [  0/160]  eta: 0:20:55  lr: 0.000037  min_lr: 0.000001  loss: 2.0236 (2.0236)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3844 (6.3844)  time: 7.8448 (7.8448 -- 7.8448)  data: 6.0303 (6.0303 -- 6.0303)  max mem: 16413
[2023-08-31 13:41:24,434] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:41:24,434] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:41:24,435] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:41:24,435] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [63]  [ 20/160]  eta: 0:03:01  lr: 0.000037  min_lr: 0.000001  loss: 1.8187 (1.7270)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8245 (7.9404)  time: 0.9711 (0.5188 -- 4.7865)  data: 0.0082 (0.0004 -- 0.1274)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:02:03  lr: 0.000037  min_lr: 0.000001  loss: 1.8628 (1.7873)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9132 (8.1875)  time: 0.7496 (0.5153 -- 2.9917)  data: 0.0016 (0.0003 -- 0.0051)  max mem: 16413
[2023-08-31 13:42:02,287] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10123
[2023-08-31 13:42:02,288] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10123
[2023-08-31 13:42:02,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:42:02,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:42:02,288] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [63]  [ 60/160]  eta: 0:01:38  lr: 0.000037  min_lr: 0.000001  loss: 1.8387 (1.7875)  loss_scale: 16384.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0201 (8.2108)  time: 0.8818 (0.5278 -- 4.1344)  data: 0.0017 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [63]  [ 80/160]  eta: 0:01:15  lr: 0.000037  min_lr: 0.000001  loss: 1.7500 (1.7848)  loss_scale: 16384.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5921 (8.4397)  time: 0.8124 (0.5150 -- 3.6667)  data: 0.0013 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [63]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000001  loss: 1.8060 (1.8043)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3261 (8.5072)  time: 0.9589 (0.5312 -- 3.4417)  data: 0.1367 (0.0002 -- 1.7703)  max mem: 16413
Epoch: [63]  [120/160]  eta: 0:00:37  lr: 0.000037  min_lr: 0.000001  loss: 1.8297 (1.8135)  loss_scale: 16384.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8863 (8.4455)  time: 0.8891 (0.5135 -- 4.6755)  data: 0.0155 (0.0004 -- 0.2885)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000001  loss: 1.8618 (1.8175)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9107 (8.4921)  time: 0.7925 (0.5223 -- 3.6263)  data: 0.0013 (0.0005 -- 0.0024)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.7921 (1.8104)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0540 (8.6272)  time: 0.6940 (0.4963 -- 2.5999)  data: 0.0013 (0.0001 -- 0.0109)  max mem: 16413
Epoch: [63] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.7921 (1.8190)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0540 (8.6272)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3177 (0.3177)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3659 (2.3659 -- 2.3659)  data: 2.0949 (2.0949 -- 2.0949)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4529 (0.7303)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4457 (0.1928 -- 2.3659)  data: 0.2193 (0.0006 -- 2.0949)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4887 (0.6599)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2307 (0.1694 -- 0.5517)  data: 0.0226 (0.0001 -- 0.2933)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5676 (0.7293)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.0207)  time: 0.2129 (0.1326 -- 0.5517)  data: 0.0216 (0.0001 -- 0.2933)  max mem: 16413
Val: Total time: 0:00:07 (0.2951 s / it)
* Acc@1 81.120 Acc@5 96.058 loss 0.696
Accuracy of the network on the 482 val images: 81.12%
Max accuracy: 82.99%
Epoch: [64]  [  0/160]  eta: 0:18:18  lr: 0.000037  min_lr: 0.000001  loss: 1.5892 (1.5892)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1901 (10.1901)  time: 6.8629 (6.8629 -- 6.8629)  data: 6.3276 (6.3276 -- 6.3276)  max mem: 16413
[2023-08-31 13:44:01,684] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:44:01,684] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:44:01,688] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:44:01,689] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [64]  [ 20/160]  eta: 0:02:41  lr: 0.000037  min_lr: 0.000001  loss: 1.8658 (1.8836)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0817 (8.7654)  time: 0.8668 (0.5357 -- 3.7059)  data: 0.2850 (0.0005 -- 3.1801)  max mem: 16413
[2023-08-31 13:44:19,510] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10272
[2023-08-31 13:44:19,510] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10272
[2023-08-31 13:44:19,510] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:44:19,510] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:44:19,510] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [64]  [ 40/160]  eta: 0:02:02  lr: 0.000037  min_lr: 0.000001  loss: 1.7879 (1.8457)  loss_scale: 32768.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4778 (8.2762)  time: 0.8773 (0.5322 -- 1.9278)  data: 0.2317 (0.0008 -- 1.3848)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:37  lr: 0.000037  min_lr: 0.000001  loss: 1.7692 (1.8024)  loss_scale: 16384.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8148 (8.3279)  time: 0.8888 (0.5241 -- 2.5215)  data: 0.1868 (0.0005 -- 1.9838)  max mem: 16413
Epoch: [64]  [ 80/160]  eta: 0:01:16  lr: 0.000037  min_lr: 0.000001  loss: 1.7874 (1.7951)  loss_scale: 16384.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5493 (8.5031)  time: 0.8952 (0.5390 -- 4.3601)  data: 0.0127 (0.0003 -- 0.2189)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:55  lr: 0.000037  min_lr: 0.000001  loss: 1.9039 (1.8163)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9378 (8.9287)  time: 0.8328 (0.5209 -- 3.4288)  data: 0.0936 (0.0002 -- 1.1944)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000001  loss: 1.9567 (1.8345)  loss_scale: 16384.0000 (19092.0992)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4378 (8.7941)  time: 0.7866 (0.5272 -- 2.2614)  data: 0.0643 (0.0002 -- 1.2556)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000037  min_lr: 0.000001  loss: 1.6897 (1.8178)  loss_scale: 16384.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1866 (8.6093)  time: 0.8949 (0.5291 -- 2.5052)  data: 0.0725 (0.0002 -- 1.4216)  max mem: 16413
[2023-08-31 13:46:02,708] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10394
[2023-08-31 13:46:02,709] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:46:02,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 13:46:02,709] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10394
[2023-08-31 13:46:02,709] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000001  loss: 1.8151 (1.8121)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7274 (8.6018)  time: 0.6442 (0.4963 -- 1.6683)  data: 0.0141 (0.0002 -- 0.2624)  max mem: 16413
Epoch: [64] Total time: 0:02:20 (0.8756 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000001  loss: 1.8151 (1.8130)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7274 (8.6018)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3191 (0.3191)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3166 (2.3166 -- 2.3166)  data: 2.1021 (2.1021 -- 2.1021)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4846 (0.7296)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (95.9596)  time: 0.4356 (0.2007 -- 2.3166)  data: 0.2254 (0.0006 -- 2.1021)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5509 (0.6642)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (95.7672)  time: 0.2307 (0.1691 -- 0.5839)  data: 0.0314 (0.0001 -- 0.3692)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5736 (0.7180)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (95.0207)  time: 0.2165 (0.1331 -- 0.5839)  data: 0.0312 (0.0001 -- 0.3692)  max mem: 16413
Val: Total time: 0:00:07 (0.2932 s / it)
* Acc@1 83.610 Acc@5 96.266 loss 0.685
Accuracy of the network on the 482 val images: 83.61%
[2023-08-31 13:46:13,126] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 13:46:13,127] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 13:46:13,127] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 13:46:13,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 13:46:14,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 13:46:14,544] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 83.61%
Epoch: [65]  [  0/160]  eta: 0:20:28  lr: 0.000037  min_lr: 0.000001  loss: 1.7016 (1.7016)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7961 (8.7961)  time: 7.6770 (7.6770 -- 7.6770)  data: 4.6846 (4.6846 -- 4.6846)  max mem: 16413
Epoch: [65]  [ 20/160]  eta: 0:02:37  lr: 0.000037  min_lr: 0.000001  loss: 1.6641 (1.7594)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9449 (8.4823)  time: 0.7948 (0.5196 -- 4.1890)  data: 0.0017 (0.0007 -- 0.0030)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:01  lr: 0.000037  min_lr: 0.000001  loss: 1.5106 (1.6545)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1941 (8.3552)  time: 0.9007 (0.5313 -- 3.1799)  data: 0.0016 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [65]  [ 60/160]  eta: 0:01:38  lr: 0.000037  min_lr: 0.000001  loss: 1.7119 (1.6657)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8361 (8.3730)  time: 0.9145 (0.5264 -- 2.5901)  data: 0.0011 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [65]  [ 80/160]  eta: 0:01:15  lr: 0.000037  min_lr: 0.000001  loss: 1.9227 (1.7102)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9160 (8.5599)  time: 0.8076 (0.5095 -- 2.8785)  data: 0.0019 (0.0003 -- 0.0088)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:00:56  lr: 0.000037  min_lr: 0.000001  loss: 1.7255 (1.7097)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7296 (8.5123)  time: 0.9849 (0.5189 -- 3.6856)  data: 0.0016 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [65]  [120/160]  eta: 0:00:36  lr: 0.000037  min_lr: 0.000001  loss: 1.8013 (1.7325)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2153 (8.5280)  time: 0.7820 (0.5210 -- 3.4733)  data: 0.0022 (0.0003 -- 0.0135)  max mem: 16413
[2023-08-31 13:48:07,645] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:48:07,645] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:48:07,645] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:48:07,645] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [65]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 1.6523 (1.7308)  loss_scale: 16384.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0638 (8.6830)  time: 0.7942 (0.5347 -- 1.7050)  data: 0.0016 (0.0005 -- 0.0033)  max mem: 16413
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.8675 (1.7449)  loss_scale: 16384.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7423 (8.6182)  time: 0.7617 (0.4960 -- 2.1100)  data: 0.0009 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [65] Total time: 0:02:21 (0.8832 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.8675 (1.7829)  loss_scale: 16384.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7423 (8.6182)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2962 (0.2962)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5010 (2.5010 -- 2.5010)  data: 2.2529 (2.2529 -- 2.2529)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4690 (0.7139)  acc1: 77.7778 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4475 (0.2062 -- 2.5010)  data: 0.2290 (0.0005 -- 2.2529)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4996 (0.6260)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2219 (0.1694 -- 0.4856)  data: 0.0176 (0.0001 -- 0.2569)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5140 (0.6955)  acc1: 85.7143 (82.5726)  acc5: 100.0000 (95.8506)  time: 0.2067 (0.1327 -- 0.4856)  data: 0.0173 (0.0001 -- 0.2569)  max mem: 16413
Val: Total time: 0:00:07 (0.2934 s / it)
* Acc@1 84.232 Acc@5 96.473 loss 0.665
Accuracy of the network on the 482 val images: 84.23%
[2023-08-31 13:48:43,789] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 13:48:43,790] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 13:48:43,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 13:48:43,791] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 13:48:45,190] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 13:48:45,190] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.23%
Epoch: [66]  [  0/160]  eta: 0:17:31  lr: 0.000036  min_lr: 0.000001  loss: 1.1502 (1.1502)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7589 (10.7589)  time: 6.5696 (6.5696 -- 6.5696)  data: 5.8114 (5.8114 -- 5.8114)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:35  lr: 0.000036  min_lr: 0.000001  loss: 1.7992 (1.7667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4353 (8.3746)  time: 0.8378 (0.5291 -- 3.6449)  data: 0.2628 (0.0007 -- 3.0838)  max mem: 16413
Epoch: [66]  [ 40/160]  eta: 0:01:58  lr: 0.000036  min_lr: 0.000001  loss: 1.7201 (1.7467)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3017 (8.3333)  time: 0.8538 (0.5224 -- 1.9377)  data: 0.2231 (0.0002 -- 1.3863)  max mem: 16413
[2023-08-31 13:49:31,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10605
[2023-08-31 13:49:31,932] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 13:49:31,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 13:49:31,932] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10605
[2023-08-31 13:49:31,933] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [66]  [ 60/160]  eta: 0:01:37  lr: 0.000036  min_lr: 0.000001  loss: 1.6494 (1.7412)  loss_scale: 8192.0000 (14235.2787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2096 (8.2936)  time: 0.9391 (0.5351 -- 3.7192)  data: 0.2767 (0.0008 -- 3.2082)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:14  lr: 0.000036  min_lr: 0.000001  loss: 1.9998 (1.7917)  loss_scale: 8192.0000 (12743.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6244 (8.6089)  time: 0.8014 (0.5233 -- 1.9146)  data: 0.2533 (0.0004 -- 1.4019)  max mem: 16413
Epoch: [66]  [100/160]  eta: 0:00:55  lr: 0.000036  min_lr: 0.000001  loss: 1.6524 (1.7824)  loss_scale: 8192.0000 (11841.9010)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6006 (8.5094)  time: 0.9015 (0.5260 -- 3.0232)  data: 0.3487 (0.0008 -- 2.5042)  max mem: 16413
Epoch: [66]  [120/160]  eta: 0:00:36  lr: 0.000036  min_lr: 0.000001  loss: 1.9360 (1.8018)  loss_scale: 8192.0000 (11238.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6391 (8.5051)  time: 0.8684 (0.5262 -- 4.0179)  data: 0.3196 (0.0007 -- 3.4854)  max mem: 16413
Epoch: [66]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 1.8176 (1.7957)  loss_scale: 8192.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4145 (8.5855)  time: 0.9630 (0.5122 -- 3.6388)  data: 0.4224 (0.0004 -- 3.0846)  max mem: 16413
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.6892 (1.7820)  loss_scale: 8192.0000 (10496.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1536 (8.5961)  time: 0.5780 (0.4944 -- 1.6236)  data: 0.0557 (0.0002 -- 1.1003)  max mem: 16413
Epoch: [66] Total time: 0:02:20 (0.8810 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.6892 (1.8102)  loss_scale: 8192.0000 (10496.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1536 (8.5961)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.2867 (0.2867)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7023 (2.7023 -- 2.7023)  data: 2.4896 (2.4896 -- 2.4896)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4944 (0.7334)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (95.9596)  time: 0.4506 (0.2033 -- 2.7023)  data: 0.2298 (0.0006 -- 2.4896)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5156 (0.6474)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2086 (0.1690 -- 0.2405)  data: 0.0021 (0.0001 -- 0.0215)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5525 (0.7133)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (95.4357)  time: 0.1916 (0.1321 -- 0.2405)  data: 0.0018 (0.0001 -- 0.0215)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 82.573 Acc@5 95.851 loss 0.677
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.23%
Epoch: [67]  [  0/160]  eta: 0:18:14  lr: 0.000036  min_lr: 0.000001  loss: 1.8843 (1.8843)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5537 (4.5537)  time: 6.8427 (6.8427 -- 6.8427)  data: 6.1378 (6.1378 -- 6.1378)  max mem: 16413
[2023-08-31 13:51:32,898] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:51:32,899] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 13:51:32,904] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:51:32,904] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [67]  [ 20/160]  eta: 0:02:42  lr: 0.000036  min_lr: 0.000001  loss: 1.7268 (1.8381)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9954 (8.1010)  time: 0.8774 (0.5335 -- 3.9884)  data: 0.3178 (0.0008 -- 3.4561)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:02:09  lr: 0.000036  min_lr: 0.000001  loss: 1.6588 (1.7775)  loss_scale: 16384.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5150 (7.8504)  time: 0.9870 (0.5236 -- 3.9408)  data: 0.4411 (0.0004 -- 3.3854)  max mem: 16413
Epoch: [67]  [ 60/160]  eta: 0:01:36  lr: 0.000036  min_lr: 0.000001  loss: 1.8502 (1.7786)  loss_scale: 16384.0000 (14503.8689)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8030 (7.8947)  time: 0.7468 (0.5384 -- 3.1299)  data: 0.1834 (0.0004 -- 2.5713)  max mem: 16413
Epoch: [67]  [ 80/160]  eta: 0:01:18  lr: 0.000036  min_lr: 0.000001  loss: 1.8441 (1.7913)  loss_scale: 16384.0000 (14968.0988)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5399 (8.2632)  time: 0.9995 (0.5174 -- 4.5186)  data: 0.4587 (0.0004 -- 3.9721)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:57  lr: 0.000036  min_lr: 0.000001  loss: 1.8189 (1.8058)  loss_scale: 16384.0000 (15248.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9149 (8.5520)  time: 0.8461 (0.5195 -- 3.7168)  data: 0.2998 (0.0003 -- 3.1861)  max mem: 16413
Epoch: [67]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000001  loss: 1.5753 (1.7865)  loss_scale: 16384.0000 (15436.1653)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1697 (8.4558)  time: 0.9151 (0.5324 -- 3.8905)  data: 0.3668 (0.0003 -- 3.3444)  max mem: 16413
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 1.8037 (1.7835)  loss_scale: 16384.0000 (15570.6099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4276 (8.4072)  time: 0.7730 (0.5300 -- 2.9233)  data: 0.2267 (0.0006 -- 2.3685)  max mem: 16413
[2023-08-31 13:53:24,922] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:53:24,922] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:53:24,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:53:24,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:53:35,551] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10877
[2023-08-31 13:53:35,551] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:53:35,551] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10877
[2023-08-31 13:53:35,551] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 13:53:35,551] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000001  loss: 1.9242 (1.7904)  loss_scale: 32768.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0279 (8.3963)  time: 0.6650 (0.4839 -- 3.4153)  data: 0.0409 (0.0002 -- 0.8035)  max mem: 16413
Epoch: [67] Total time: 0:02:22 (0.8908 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000001  loss: 1.9242 (1.7884)  loss_scale: 32768.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0279 (8.3963)
Val:  [ 0/27]  eta: 0:01:13  loss: 0.3329 (0.3329)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7090 (2.7090 -- 2.7090)  data: 2.5100 (2.5100 -- 2.5100)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4186 (0.7190)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4551 (0.2072 -- 2.7090)  data: 0.2372 (0.0008 -- 2.5100)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5126 (0.6434)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (95.7672)  time: 0.2113 (0.1690 -- 0.3292)  data: 0.0061 (0.0001 -- 0.0909)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5550 (0.7126)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.0207)  time: 0.1940 (0.1336 -- 0.3292)  data: 0.0058 (0.0001 -- 0.0909)  max mem: 16413
Val: Total time: 0:00:07 (0.2933 s / it)
* Acc@1 82.573 Acc@5 96.058 loss 0.671
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.23%
Epoch: [68]  [  0/160]  eta: 0:19:44  lr: 0.000036  min_lr: 0.000001  loss: 2.1260 (2.1260)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8231 (6.8231)  time: 7.4019 (7.4019 -- 7.4019)  data: 6.8552 (6.8552 -- 6.8552)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:45  lr: 0.000036  min_lr: 0.000001  loss: 1.6164 (1.6948)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1878 (8.5556)  time: 0.8747 (0.5273 -- 3.6948)  data: 0.2060 (0.0005 -- 2.1452)  max mem: 16413
Epoch: [68]  [ 40/160]  eta: 0:02:10  lr: 0.000036  min_lr: 0.000001  loss: 1.7507 (1.7067)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3504 (8.6993)  time: 0.9900 (0.5249 -- 4.0695)  data: 0.0255 (0.0002 -- 0.4790)  max mem: 16413
Epoch: [68]  [ 60/160]  eta: 0:01:40  lr: 0.000036  min_lr: 0.000001  loss: 1.7052 (1.7062)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2553 (8.4039)  time: 0.8162 (0.5209 -- 4.5410)  data: 0.0014 (0.0003 -- 0.0065)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:17  lr: 0.000036  min_lr: 0.000001  loss: 1.8921 (1.7648)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9871 (8.5360)  time: 0.8544 (0.5292 -- 3.0156)  data: 0.0013 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:56  lr: 0.000036  min_lr: 0.000001  loss: 1.7656 (1.7691)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3917 (8.5821)  time: 0.8349 (0.5263 -- 2.4773)  data: 0.0015 (0.0003 -- 0.0038)  max mem: 16413
[2023-08-31 13:55:36,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=60, lr=[8.451868793498704e-07, 8.451868793498704e-07, 1.1269158391331605e-06, 1.1269158391331605e-06, 1.5025544521775474e-06, 1.5025544521775474e-06, 2.0034059362367297e-06, 2.0034059362367297e-06, 2.6712079149823063e-06, 2.6712079149823063e-06, 3.561610553309742e-06, 3.561610553309742e-06, 4.748814071079656e-06, 4.748814071079656e-06, 6.3317520947728745e-06, 6.3317520947728745e-06, 8.442336126363833e-06, 8.442336126363833e-06, 1.125644816848511e-05, 1.125644816848511e-05, 1.5008597557980148e-05, 1.5008597557980148e-05, 2.0011463410640196e-05, 2.0011463410640196e-05, 2.668195121418693e-05, 2.668195121418693e-05, 3.557593495224924e-05, 3.557593495224924e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 13:55:36,758] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=16.6717973251037, CurrSamplesPerSec=22.107213620373635, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000001  loss: 1.8278 (1.7781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8820 (8.4891)  time: 0.8993 (0.5242 -- 2.9183)  data: 0.0018 (0.0004 -- 0.0056)  max mem: 16413
[2023-08-31 13:55:42,483] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:55:42,483] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:55:42,486] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:55:42,486] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:55:49,490] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11015
[2023-08-31 13:55:49,490] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11015
[2023-08-31 13:55:49,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:55:49,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:55:49,491] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000036  min_lr: 0.000001  loss: 1.9553 (1.8095)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7569 (8.5317)  time: 0.8349 (0.5305 -- 2.4484)  data: 0.0368 (0.0003 -- 0.7037)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.7943 (1.8080)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6268 (8.5047)  time: 0.6617 (0.4951 -- 1.9425)  data: 0.0721 (0.0001 -- 1.4298)  max mem: 16413
Epoch: [68] Total time: 0:02:22 (0.8888 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.7943 (1.8044)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6268 (8.5047)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.3421 (0.3421)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5773 (2.5773 -- 2.5773)  data: 2.3378 (2.3378 -- 2.3378)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4230 (0.7316)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4409 (0.1981 -- 2.5773)  data: 0.2247 (0.0004 -- 2.3378)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5406 (0.6543)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (95.7672)  time: 0.2201 (0.1681 -- 0.4287)  data: 0.0191 (0.0001 -- 0.2464)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5821 (0.7212)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (95.0207)  time: 0.2025 (0.1323 -- 0.4287)  data: 0.0181 (0.0001 -- 0.2464)  max mem: 16413
Val: Total time: 0:00:07 (0.2949 s / it)
* Acc@1 83.195 Acc@5 95.851 loss 0.689
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 84.23%
Epoch: [69]  [  0/160]  eta: 0:19:20  lr: 0.000035  min_lr: 0.000001  loss: 2.3711 (2.3711)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7451 (6.7451)  time: 7.2559 (7.2559 -- 7.2559)  data: 5.3169 (5.3169 -- 5.3169)  max mem: 16413
Epoch: [69]  [ 20/160]  eta: 0:02:51  lr: 0.000035  min_lr: 0.000001  loss: 1.8435 (1.8589)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0895 (8.6705)  time: 0.9266 (0.5162 -- 5.0418)  data: 0.3387 (0.0006 -- 4.5025)  max mem: 16413
Epoch: [69]  [ 40/160]  eta: 0:02:07  lr: 0.000035  min_lr: 0.000001  loss: 1.7862 (1.8318)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5250 (8.7973)  time: 0.8874 (0.5263 -- 3.8674)  data: 0.2372 (0.0004 -- 3.3298)  max mem: 16413
Epoch: [69]  [ 60/160]  eta: 0:01:36  lr: 0.000035  min_lr: 0.000001  loss: 1.9536 (1.8698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4068 (8.6912)  time: 0.7799 (0.5246 -- 2.9611)  data: 0.2119 (0.0003 -- 2.2965)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:18  lr: 0.000035  min_lr: 0.000001  loss: 1.8894 (1.8623)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6956 (8.9782)  time: 1.0137 (0.5136 -- 4.6213)  data: 0.1726 (0.0006 -- 2.2108)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000001  loss: 1.9422 (1.8518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6057 (8.9788)  time: 0.7701 (0.5227 -- 2.9480)  data: 0.0015 (0.0004 -- 0.0033)  max mem: 16413
[2023-08-31 13:57:55,441] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:57:55,441] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:57:55,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:57:55,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:57:56,504] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11146
[2023-08-31 13:57:56,504] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11146
[2023-08-31 13:57:56,504] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:57:56,504] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 13:57:56,504] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [69]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000001  loss: 1.8939 (1.8584)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8744 (8.7726)  time: 0.9282 (0.5137 -- 4.2515)  data: 0.0014 (0.0006 -- 0.0051)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000001  loss: 1.7834 (1.8480)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4508 (8.6734)  time: 0.8431 (0.5230 -- 3.9198)  data: 0.0010 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.6692 (1.8283)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0473 (8.5903)  time: 0.6677 (0.4944 -- 2.0116)  data: 0.0014 (0.0002 -- 0.0136)  max mem: 16413
Epoch: [69] Total time: 0:02:23 (0.8941 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.6692 (1.8075)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0473 (8.5903)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2857 (0.2857)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2830 (2.2830 -- 2.2830)  data: 2.0404 (2.0404 -- 2.0404)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4417 (0.7024)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4248 (0.1940 -- 2.2830)  data: 0.2121 (0.0005 -- 2.0404)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4983 (0.6265)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2257 (0.1689 -- 0.4828)  data: 0.0271 (0.0001 -- 0.2823)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5485 (0.6943)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (95.8506)  time: 0.2124 (0.1334 -- 0.4828)  data: 0.0269 (0.0001 -- 0.2823)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 83.817 Acc@5 96.266 loss 0.668
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.23%
Epoch: [70]  [  0/160]  eta: 0:17:55  lr: 0.000035  min_lr: 0.000001  loss: 1.7519 (1.7519)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8643 (6.8643)  time: 6.7223 (6.7223 -- 6.7223)  data: 6.1577 (6.1577 -- 6.1577)  max mem: 16413
Epoch: [70]  [ 20/160]  eta: 0:02:44  lr: 0.000035  min_lr: 0.000001  loss: 1.7458 (1.7676)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5686 (9.2107)  time: 0.8984 (0.5255 -- 3.5798)  data: 0.2767 (0.0005 -- 3.0555)  max mem: 16413
Epoch: [70]  [ 40/160]  eta: 0:01:56  lr: 0.000035  min_lr: 0.000001  loss: 1.6618 (1.7377)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8103 (9.3402)  time: 0.7587 (0.5298 -- 2.6801)  data: 0.1947 (0.0005 -- 2.0772)  max mem: 16413
Epoch: [70]  [ 60/160]  eta: 0:01:35  lr: 0.000035  min_lr: 0.000001  loss: 1.6775 (1.7249)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2510 (9.2383)  time: 0.9244 (0.5425 -- 2.3918)  data: 0.1264 (0.0006 -- 1.8452)  max mem: 16413
[2023-08-31 13:59:59,073] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:59:59,074] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 13:59:59,074] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 13:59:59,075] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [70]  [ 80/160]  eta: 0:01:18  lr: 0.000035  min_lr: 0.000001  loss: 1.9028 (1.7754)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3835 (9.1279)  time: 1.0387 (0.5267 -- 4.7515)  data: 0.0015 (0.0004 -- 0.0046)  max mem: 16413
[2023-08-31 14:00:20,554] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11300
[2023-08-31 14:00:20,554] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11300
[2023-08-31 14:00:20,554] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:00:20,554] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:00:20,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [70]  [100/160]  eta: 0:00:56  lr: 0.000035  min_lr: 0.000001  loss: 1.9448 (1.8061)  loss_scale: 32768.0000 (20439.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5893 (8.9800)  time: 0.7843 (0.5019 -- 4.0569)  data: 0.0015 (0.0004 -- 0.0054)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:36  lr: 0.000035  min_lr: 0.000001  loss: 1.8493 (1.8092)  loss_scale: 16384.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9283 (8.8374)  time: 0.7991 (0.5246 -- 2.9493)  data: 0.1334 (0.0003 -- 2.4002)  max mem: 16413
Epoch: [70]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000001  loss: 1.8670 (1.8136)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4694 (8.7448)  time: 0.8506 (0.5341 -- 3.1620)  data: 0.0984 (0.0006 -- 1.9327)  max mem: 16413
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.9068 (1.8184)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3781 (8.7539)  time: 0.6869 (0.4954 -- 3.2069)  data: 0.0269 (0.0002 -- 0.5263)  max mem: 16413
Epoch: [70] Total time: 0:02:21 (0.8815 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.9068 (1.7963)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3781 (8.7539)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2375 (0.2375)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6053 (2.6053 -- 2.6053)  data: 2.3533 (2.3533 -- 2.3533)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4456 (0.7094)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4338 (0.1882 -- 2.6053)  data: 0.2246 (0.0006 -- 2.3533)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4530 (0.6277)  acc1: 77.7778 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2086 (0.1700 -- 0.3102)  data: 0.0108 (0.0001 -- 0.1005)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5247 (0.6996)  acc1: 85.7143 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.1965 (0.1323 -- 0.3102)  data: 0.0105 (0.0001 -- 0.1005)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 83.402 Acc@5 96.680 loss 0.663
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.23%
Epoch: [71]  [  0/160]  eta: 0:21:04  lr: 0.000035  min_lr: 0.000001  loss: 1.2786 (1.2786)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0506 (10.0506)  time: 7.9042 (7.9042 -- 7.9042)  data: 6.2218 (6.2218 -- 6.2218)  max mem: 16413
Epoch: [71]  [ 20/160]  eta: 0:02:46  lr: 0.000035  min_lr: 0.000001  loss: 1.8079 (1.7234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8526 (9.5019)  time: 0.8570 (0.5297 -- 2.9642)  data: 0.3023 (0.0003 -- 2.3524)  max mem: 16413
Epoch: [71]  [ 40/160]  eta: 0:02:06  lr: 0.000035  min_lr: 0.000001  loss: 1.6752 (1.7139)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5366 (8.8796)  time: 0.9110 (0.5168 -- 4.1372)  data: 0.0563 (0.0004 -- 1.0870)  max mem: 16413
Epoch: [71]  [ 60/160]  eta: 0:01:40  lr: 0.000035  min_lr: 0.000001  loss: 1.9076 (1.7520)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2788 (9.1722)  time: 0.9015 (0.5378 -- 4.5127)  data: 0.0041 (0.0004 -- 0.0386)  max mem: 16413
[2023-08-31 14:02:21,024] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:02:21,025] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:02:21,027] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:02:21,066] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [71]  [ 80/160]  eta: 0:01:15  lr: 0.000035  min_lr: 0.000001  loss: 1.4980 (1.7190)  loss_scale: 32768.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0381 (8.7480)  time: 0.7707 (0.5395 -- 2.6306)  data: 0.2077 (0.0006 -- 2.1014)  max mem: 16413
Epoch: [71]  [100/160]  eta: 0:00:55  lr: 0.000035  min_lr: 0.000001  loss: 1.8476 (1.7291)  loss_scale: 32768.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8662 (8.6857)  time: 0.8394 (0.5212 -- 2.9842)  data: 0.2882 (0.0003 -- 2.4153)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000035  min_lr: 0.000001  loss: 1.7943 (1.7481)  loss_scale: 32768.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8802 (8.6245)  time: 1.0166 (0.5330 -- 3.9847)  data: 0.4559 (0.0007 -- 3.4417)  max mem: 16413
[2023-08-31 14:03:11,756] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11486
[2023-08-31 14:03:11,756] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11486
[2023-08-31 14:03:11,757] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:03:11,757] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:03:11,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000035  min_lr: 0.000001  loss: 2.0604 (1.7780)  loss_scale: 16384.0000 (23007.3191)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8751 (8.5947)  time: 0.7622 (0.5206 -- 4.3758)  data: 0.2190 (0.0003 -- 3.8385)  max mem: 16413
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000001  loss: 1.8671 (1.7933)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4808 (8.4981)  time: 0.6822 (0.4948 -- 3.3806)  data: 0.1504 (0.0002 -- 2.8167)  max mem: 16413
Epoch: [71] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000001  loss: 1.8671 (1.7701)  loss_scale: 16384.0000 (22220.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4808 (8.4981)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.3235 (0.3235)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4987 (2.4987 -- 2.4987)  data: 2.2713 (2.2713 -- 2.2713)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4624 (0.7360)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4383 (0.1970 -- 2.4987)  data: 0.2224 (0.0006 -- 2.2713)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4849 (0.6426)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (95.7672)  time: 0.2179 (0.1692 -- 0.4059)  data: 0.0146 (0.0001 -- 0.1623)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5579 (0.7012)  acc1: 85.7143 (81.3278)  acc5: 100.0000 (94.6058)  time: 0.2037 (0.1330 -- 0.4059)  data: 0.0143 (0.0001 -- 0.1623)  max mem: 16413
Val: Total time: 0:00:07 (0.2906 s / it)
* Acc@1 82.573 Acc@5 96.266 loss 0.672
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.23%
Epoch: [72]  [  0/160]  eta: 0:18:12  lr: 0.000035  min_lr: 0.000001  loss: 1.4149 (1.4149)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6320 (5.6320)  time: 6.8268 (6.8268 -- 6.8268)  data: 5.5783 (5.5783 -- 5.5783)  max mem: 16413
Epoch: [72]  [ 20/160]  eta: 0:02:40  lr: 0.000034  min_lr: 0.000001  loss: 1.7324 (1.7264)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4015 (8.9230)  time: 0.8613 (0.5299 -- 2.8550)  data: 0.3121 (0.0004 -- 2.3303)  max mem: 16413
Epoch: [72]  [ 40/160]  eta: 0:02:00  lr: 0.000034  min_lr: 0.000001  loss: 1.7877 (1.7545)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3981 (9.0348)  time: 0.8607 (0.5295 -- 3.3606)  data: 0.2818 (0.0004 -- 2.8455)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:36  lr: 0.000034  min_lr: 0.000001  loss: 1.6586 (1.7274)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9705 (9.0381)  time: 0.8718 (0.5181 -- 2.9134)  data: 0.2616 (0.0004 -- 2.3774)  max mem: 16413
Epoch: [72]  [ 80/160]  eta: 0:01:17  lr: 0.000034  min_lr: 0.000001  loss: 1.7511 (1.7435)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6298 (8.8287)  time: 0.9911 (0.5258 -- 5.0505)  data: 0.0014 (0.0004 -- 0.0053)  max mem: 16413
[2023-08-31 14:05:13,545] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:05:13,546] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:05:13,546] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:05:13,547] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [72]  [100/160]  eta: 0:00:54  lr: 0.000034  min_lr: 0.000001  loss: 1.6576 (1.7412)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6063 (8.5146)  time: 0.6768 (0.5395 -- 2.5440)  data: 0.0016 (0.0002 -- 0.0035)  max mem: 16413
[2023-08-31 14:05:26,882] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11630
[2023-08-31 14:05:26,882] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11630
[2023-08-31 14:05:26,882] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:05:26,882] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:05:26,882] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [72]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000001  loss: 1.7370 (1.7374)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9150 (8.4294)  time: 0.9441 (0.5222 -- 2.7757)  data: 0.2081 (0.0004 -- 2.2715)  max mem: 16413
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000001  loss: 1.8090 (1.7455)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2213 (8.4138)  time: 0.8928 (0.5365 -- 3.3311)  data: 0.1527 (0.0005 -- 1.4269)  max mem: 16413
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.6804 (1.7432)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0651 (8.3936)  time: 0.6739 (0.4935 -- 1.9736)  data: 0.0435 (0.0001 -- 0.5532)  max mem: 16413
Epoch: [72] Total time: 0:02:21 (0.8860 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.6804 (1.7633)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0651 (8.3936)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2927 (0.2927)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4410 (2.4410 -- 2.4410)  data: 2.2328 (2.2328 -- 2.2328)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4662 (0.7169)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4198 (0.1963 -- 2.4410)  data: 0.2040 (0.0006 -- 2.2328)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4804 (0.6388)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2184 (0.1685 -- 0.4755)  data: 0.0151 (0.0001 -- 0.2887)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6034 (0.6974)  acc1: 85.7143 (80.4979)  acc5: 100.0000 (95.4357)  time: 0.2020 (0.1324 -- 0.4755)  data: 0.0148 (0.0001 -- 0.2887)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 82.573 Acc@5 96.473 loss 0.671
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.23%
Epoch: [73]  [  0/160]  eta: 0:19:15  lr: 0.000034  min_lr: 0.000001  loss: 2.2723 (2.2723)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4382 (8.4382)  time: 7.2218 (7.2218 -- 7.2218)  data: 6.3062 (6.3062 -- 6.3062)  max mem: 16413
Epoch: [73]  [ 20/160]  eta: 0:02:50  lr: 0.000034  min_lr: 0.000001  loss: 1.8402 (1.8109)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0503 (8.1814)  time: 0.9184 (0.5240 -- 2.6163)  data: 0.0168 (0.0004 -- 0.3028)  max mem: 16413
[2023-08-31 14:06:45,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11705
[2023-08-31 14:06:45,500] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11705
[2023-08-31 14:06:45,500] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:06:45,500] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:06:45,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [73]  [ 40/160]  eta: 0:02:11  lr: 0.000034  min_lr: 0.000001  loss: 1.6768 (1.7241)  loss_scale: 8192.0000 (13187.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8780 (8.2233)  time: 0.9598 (0.5111 -- 4.0687)  data: 0.0015 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:41  lr: 0.000034  min_lr: 0.000001  loss: 1.7391 (1.7480)  loss_scale: 8192.0000 (11549.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5500 (8.5297)  time: 0.8435 (0.5138 -- 4.9772)  data: 0.0011 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [73]  [ 80/160]  eta: 0:01:17  lr: 0.000034  min_lr: 0.000001  loss: 1.7195 (1.7404)  loss_scale: 8192.0000 (10720.3951)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2257 (8.5229)  time: 0.8630 (0.5070 -- 4.5094)  data: 0.0009 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:56  lr: 0.000034  min_lr: 0.000001  loss: 1.8303 (1.7482)  loss_scale: 8192.0000 (10219.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6011 (8.3538)  time: 0.7956 (0.5209 -- 3.2706)  data: 0.0016 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:37  lr: 0.000034  min_lr: 0.000001  loss: 1.7508 (1.7408)  loss_scale: 8192.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4263 (8.2915)  time: 0.8630 (0.5280 -- 3.2809)  data: 0.0017 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000034  min_lr: 0.000001  loss: 1.6803 (1.7445)  loss_scale: 8192.0000 (9644.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8427 (8.4181)  time: 0.9905 (0.5056 -- 5.0008)  data: 0.0368 (0.0004 -- 0.7107)  max mem: 16413
[2023-08-31 14:08:35,454] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:08:35,454] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:08:35,454] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:08:35,455] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.7160 (1.7451)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5821 (8.3125)  time: 0.6087 (0.4942 -- 2.3961)  data: 0.0009 (0.0002 -- 0.0085)  max mem: 16413
Epoch: [73] Total time: 0:02:23 (0.8973 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.7160 (1.7856)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5821 (8.3125)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2525 (0.2525)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4564 (2.4564 -- 2.4564)  data: 2.2193 (2.2193 -- 2.2193)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5159 (0.7012)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4215 (0.1969 -- 2.4564)  data: 0.2031 (0.0007 -- 2.2193)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5135 (0.6338)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2160 (0.1690 -- 0.4085)  data: 0.0116 (0.0001 -- 0.2144)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6527 (0.6873)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (95.8506)  time: 0.1992 (0.1334 -- 0.4085)  data: 0.0111 (0.0001 -- 0.2144)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 82.573 Acc@5 96.680 loss 0.653
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.23%
Epoch: [74]  [  0/160]  eta: 0:19:41  lr: 0.000034  min_lr: 0.000001  loss: 1.6764 (1.6764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5242 (5.5242)  time: 7.3844 (7.3844 -- 7.3844)  data: 6.8208 (6.8208 -- 6.8208)  max mem: 16413
Epoch: [74]  [ 20/160]  eta: 0:02:37  lr: 0.000034  min_lr: 0.000001  loss: 1.7432 (1.7208)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5926 (8.3408)  time: 0.8098 (0.5282 -- 1.8955)  data: 0.2302 (0.0005 -- 1.3813)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:02:02  lr: 0.000034  min_lr: 0.000001  loss: 1.7038 (1.6927)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2436 (8.3642)  time: 0.9104 (0.5425 -- 2.7577)  data: 0.0953 (0.0002 -- 1.5519)  max mem: 16413
[2023-08-31 14:09:41,985] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11895
[2023-08-31 14:09:41,985] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11895
[2023-08-31 14:09:41,986] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:09:41,986] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:09:41,986] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [74]  [ 60/160]  eta: 0:01:37  lr: 0.000034  min_lr: 0.000001  loss: 1.8277 (1.7416)  loss_scale: 16384.0000 (15578.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7909 (8.4843)  time: 0.8890 (0.5130 -- 6.1089)  data: 0.0044 (0.0003 -- 0.0343)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:15  lr: 0.000034  min_lr: 0.000001  loss: 1.9299 (1.7714)  loss_scale: 8192.0000 (13754.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0860 (8.4795)  time: 0.8656 (0.5230 -- 3.1284)  data: 0.0019 (0.0005 -- 0.0051)  max mem: 16413
Epoch: [74]  [100/160]  eta: 0:00:55  lr: 0.000034  min_lr: 0.000001  loss: 1.8010 (1.7784)  loss_scale: 8192.0000 (12652.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4907 (8.5007)  time: 0.8259 (0.5192 -- 2.6109)  data: 0.0017 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000034  min_lr: 0.000001  loss: 1.9355 (1.7906)  loss_scale: 8192.0000 (11915.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9202 (8.3820)  time: 0.8655 (0.5239 -- 3.1848)  data: 0.0105 (0.0004 -- 0.1777)  max mem: 16413
Epoch: [74]  [140/160]  eta: 0:00:17  lr: 0.000034  min_lr: 0.000001  loss: 1.9022 (1.8046)  loss_scale: 8192.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9058 (8.4213)  time: 0.7489 (0.5244 -- 3.5661)  data: 0.0255 (0.0003 -- 0.4663)  max mem: 16413
[2023-08-31 14:11:06,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=67, lr=[7.958802674087248e-07, 7.958802674087248e-07, 1.0611736898782997e-06, 1.0611736898782997e-06, 1.4148982531710663e-06, 1.4148982531710663e-06, 1.8865310042280885e-06, 1.8865310042280885e-06, 2.515374672304118e-06, 2.515374672304118e-06, 3.3538328964054906e-06, 3.3538328964054906e-06, 4.471777195207321e-06, 4.471777195207321e-06, 5.962369593609761e-06, 5.962369593609761e-06, 7.949826124813014e-06, 7.949826124813014e-06, 1.0599768166417353e-05, 1.0599768166417353e-05, 1.4133024221889804e-05, 1.4133024221889804e-05, 1.8844032295853073e-05, 1.8844032295853073e-05, 2.512537639447076e-05, 2.512537639447076e-05, 3.350050185929435e-05, 3.350050185929435e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 14:11:06,617] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=16.66189726433468, CurrSamplesPerSec=24.47839717221275, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000001  loss: 1.6218 (1.7952)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8306 (8.3961)  time: 0.7800 (0.4961 -- 2.6407)  data: 0.0948 (0.0001 -- 1.6998)  max mem: 16413
Epoch: [74] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000001  loss: 1.6218 (1.7961)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8306 (8.3961)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.2540 (0.2540)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6456 (2.6456 -- 2.6456)  data: 2.4390 (2.4390 -- 2.4390)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4574 (0.7083)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4465 (0.2008 -- 2.6456)  data: 0.2339 (0.0006 -- 2.4390)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4820 (0.6317)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (95.7672)  time: 0.2139 (0.1686 -- 0.3430)  data: 0.0125 (0.0001 -- 0.1210)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5367 (0.6913)  acc1: 85.7143 (81.3278)  acc5: 100.0000 (95.0207)  time: 0.1984 (0.1325 -- 0.3430)  data: 0.0121 (0.0001 -- 0.1210)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 83.402 Acc@5 96.473 loss 0.659
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.23%
Epoch: [75]  [  0/160]  eta: 0:22:06  lr: 0.000033  min_lr: 0.000001  loss: 1.9071 (1.9071)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6650 (4.6650)  time: 8.2930 (8.2930 -- 8.2930)  data: 4.8441 (4.8441 -- 4.8441)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:02:52  lr: 0.000033  min_lr: 0.000001  loss: 1.7790 (1.7116)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3666 (8.7705)  time: 0.8788 (0.5188 -- 3.2334)  data: 0.0340 (0.0004 -- 0.6477)  max mem: 16413
[2023-08-31 14:11:43,523] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:11:43,523] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:11:43,523] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:11:43,523] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [75]  [ 40/160]  eta: 0:02:07  lr: 0.000033  min_lr: 0.000001  loss: 1.9074 (1.7741)  loss_scale: 16384.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9877 (9.0634)  time: 0.8761 (0.5206 -- 3.0404)  data: 0.1185 (0.0002 -- 1.3923)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:41  lr: 0.000033  min_lr: 0.000001  loss: 1.8660 (1.7737)  loss_scale: 16384.0000 (13160.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0083 (9.0656)  time: 0.9151 (0.5314 -- 3.3203)  data: 0.0015 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [75]  [ 80/160]  eta: 0:01:19  lr: 0.000033  min_lr: 0.000001  loss: 1.7305 (1.7708)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6949 (9.1099)  time: 0.9314 (0.5230 -- 3.7296)  data: 0.0011 (0.0005 -- 0.0040)  max mem: 16413
Epoch: [75]  [100/160]  eta: 0:00:57  lr: 0.000033  min_lr: 0.000001  loss: 1.6961 (1.7454)  loss_scale: 16384.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8207 (8.9744)  time: 0.8393 (0.5121 -- 5.1146)  data: 0.0016 (0.0003 -- 0.0069)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000001  loss: 1.8065 (1.7431)  loss_scale: 16384.0000 (14759.1405)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0223 (8.8937)  time: 0.8577 (0.5370 -- 3.1396)  data: 0.0017 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 1.7847 (1.7439)  loss_scale: 16384.0000 (14989.6170)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4281 (8.9479)  time: 0.8533 (0.5245 -- 4.0784)  data: 0.0015 (0.0008 -- 0.0032)  max mem: 16413
[2023-08-31 14:13:35,643] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:13:35,643] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:13:35,643] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:13:35,643] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.6991 (1.7384)  loss_scale: 16384.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3253 (8.8614)  time: 0.6874 (0.4951 -- 3.0608)  data: 0.0007 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [75] Total time: 0:02:24 (0.9036 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.6991 (1.7493)  loss_scale: 16384.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3253 (8.8614)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2366 (0.2366)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4874 (2.4874 -- 2.4874)  data: 2.2712 (2.2712 -- 2.2712)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4584 (0.7083)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (95.9596)  time: 0.4395 (0.2052 -- 2.4874)  data: 0.2235 (0.0005 -- 2.2712)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4958 (0.6307)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (95.7672)  time: 0.2161 (0.1709 -- 0.3790)  data: 0.0109 (0.0001 -- 0.1696)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5499 (0.6792)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (95.8506)  time: 0.1981 (0.1335 -- 0.3790)  data: 0.0106 (0.0001 -- 0.1696)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 83.402 Acc@5 96.680 loss 0.644
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.23%
Epoch: [76]  [  0/160]  eta: 0:17:52  lr: 0.000033  min_lr: 0.000001  loss: 1.9152 (1.9152)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7943 (11.7943)  time: 6.7014 (6.7014 -- 6.7014)  data: 4.2696 (4.2696 -- 4.2696)  max mem: 16413
[2023-08-31 14:14:01,743] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12169
[2023-08-31 14:14:01,743] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12169
[2023-08-31 14:14:01,744] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:14:01,744] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:14:01,744] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [76]  [ 20/160]  eta: 0:02:47  lr: 0.000033  min_lr: 0.000001  loss: 1.9128 (1.8270)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3455 (8.7963)  time: 0.9235 (0.5195 -- 2.9861)  data: 0.2809 (0.0008 -- 2.4581)  max mem: 16413
Epoch: [76]  [ 40/160]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000001  loss: 1.4980 (1.6827)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1912 (8.9239)  time: 0.8902 (0.5171 -- 3.3063)  data: 0.1221 (0.0004 -- 2.4105)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:38  lr: 0.000033  min_lr: 0.000001  loss: 1.7760 (1.7280)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8847 (8.7356)  time: 0.8599 (0.5277 -- 3.1039)  data: 0.0012 (0.0005 -- 0.0029)  max mem: 16413
Epoch: [76]  [ 80/160]  eta: 0:01:14  lr: 0.000033  min_lr: 0.000001  loss: 1.8911 (1.7614)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4557 (8.5591)  time: 0.7674 (0.5297 -- 2.6538)  data: 0.0017 (0.0005 -- 0.0054)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000001  loss: 1.8237 (1.7733)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9694 (8.7484)  time: 0.9628 (0.5290 -- 2.9730)  data: 0.0019 (0.0010 -- 0.0034)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000001  loss: 1.8769 (1.7867)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3465 (8.9600)  time: 0.9485 (0.5281 -- 4.2445)  data: 0.0015 (0.0004 -- 0.0031)  max mem: 16413
[2023-08-31 14:15:54,625] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:15:54,625] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:15:54,625] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:15:54,625] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:15:55,734] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12300
[2023-08-31 14:15:55,734] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12300
[2023-08-31 14:15:55,734] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:15:55,734] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:15:55,734] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [76]  [140/160]  eta: 0:00:18  lr: 0.000033  min_lr: 0.000001  loss: 1.8658 (1.7905)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6694 (8.9280)  time: 0.7512 (0.5260 -- 2.1633)  data: 0.0273 (0.0003 -- 0.5095)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000001  loss: 1.6484 (1.7761)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1434 (8.9393)  time: 0.6481 (0.4967 -- 1.2610)  data: 0.0648 (0.0002 -- 0.7246)  max mem: 16413
Epoch: [76] Total time: 0:02:21 (0.8827 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000001  loss: 1.6484 (1.7787)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1434 (8.9393)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2851 (0.2851)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3821 (2.3821 -- 2.3821)  data: 2.1672 (2.1672 -- 2.1672)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3998 (0.7035)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4360 (0.2000 -- 2.3821)  data: 0.2199 (0.0007 -- 2.1672)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4738 (0.6215)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2268 (0.1691 -- 0.4545)  data: 0.0196 (0.0001 -- 0.2318)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5388 (0.6743)  acc1: 85.7143 (81.7427)  acc5: 100.0000 (95.4357)  time: 0.2115 (0.1326 -- 0.4545)  data: 0.0190 (0.0001 -- 0.2318)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 83.195 Acc@5 96.888 loss 0.653
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 84.23%
Epoch: [77]  [  0/160]  eta: 0:19:14  lr: 0.000033  min_lr: 0.000001  loss: 1.9601 (1.9601)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7936 (7.7936)  time: 7.2173 (7.2173 -- 7.2173)  data: 6.6783 (6.6783 -- 6.6783)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:43  lr: 0.000033  min_lr: 0.000001  loss: 1.8240 (1.8323)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1243 (8.3575)  time: 0.8662 (0.5300 -- 4.3031)  data: 0.3146 (0.0006 -- 3.7847)  max mem: 16413
Epoch: [77]  [ 40/160]  eta: 0:02:05  lr: 0.000033  min_lr: 0.000001  loss: 1.8431 (1.7923)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0913 (8.2296)  time: 0.9095 (0.5286 -- 2.9052)  data: 0.3689 (0.0005 -- 2.3620)  max mem: 16413
Epoch: [77]  [ 60/160]  eta: 0:01:36  lr: 0.000033  min_lr: 0.000001  loss: 1.6061 (1.7260)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6220 (8.2328)  time: 0.8071 (0.5164 -- 3.0710)  data: 0.2405 (0.0004 -- 2.4304)  max mem: 16413
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000033  min_lr: 0.000001  loss: 1.6310 (1.7273)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4878 (8.4576)  time: 0.9353 (0.5355 -- 4.0238)  data: 0.1913 (0.0005 -- 1.5681)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:56  lr: 0.000033  min_lr: 0.000001  loss: 1.5618 (1.7209)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6977 (8.7865)  time: 0.8487 (0.5310 -- 3.5846)  data: 0.0169 (0.0002 -- 0.2949)  max mem: 16413
[2023-08-31 14:17:56,683] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:17:56,684] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:17:56,685] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:17:56,686] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000033  min_lr: 0.000001  loss: 1.6212 (1.7117)  loss_scale: 32768.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9440 (8.9151)  time: 0.8287 (0.5448 -- 2.7460)  data: 0.1336 (0.0008 -- 2.2069)  max mem: 16413
[2023-08-31 14:18:14,645] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12450
[2023-08-31 14:18:14,645] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12450
[2023-08-31 14:18:14,646] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:18:14,646] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:18:14,646] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [77]  [140/160]  eta: 0:00:17  lr: 0.000033  min_lr: 0.000001  loss: 1.7560 (1.7153)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8451 (8.8742)  time: 0.7880 (0.5434 -- 2.3838)  data: 0.0913 (0.0009 -- 1.0612)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.8322 (1.7324)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2714 (8.7909)  time: 0.8267 (0.4948 -- 2.8487)  data: 0.2104 (0.0002 -- 2.3053)  max mem: 16413
Epoch: [77] Total time: 0:02:22 (0.8931 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.8322 (1.7691)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2714 (8.7909)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2346 (0.2346)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3571 (2.3571 -- 2.3571)  data: 2.1428 (2.1428 -- 2.1428)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4141 (0.7133)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4218 (0.1998 -- 2.3571)  data: 0.2074 (0.0009 -- 2.1428)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5050 (0.6346)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2247 (0.1689 -- 0.4559)  data: 0.0210 (0.0001 -- 0.2770)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5302 (0.6848)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (95.4357)  time: 0.2102 (0.1325 -- 0.4559)  data: 0.0205 (0.0001 -- 0.2770)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 83.402 Acc@5 96.680 loss 0.649
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.23%
Epoch: [78]  [  0/160]  eta: 0:20:59  lr: 0.000032  min_lr: 0.000001  loss: 1.5743 (1.5743)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1142 (5.1142)  time: 7.8749 (7.8749 -- 7.8749)  data: 7.3327 (7.3327 -- 7.3327)  max mem: 16413
Epoch: [78]  [ 20/160]  eta: 0:02:36  lr: 0.000032  min_lr: 0.000001  loss: 1.6374 (1.6902)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5808 (8.8271)  time: 0.7821 (0.5254 -- 2.9581)  data: 0.1259 (0.0002 -- 1.7151)  max mem: 16413
Epoch: [78]  [ 40/160]  eta: 0:02:05  lr: 0.000032  min_lr: 0.000001  loss: 1.9556 (1.7951)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7082 (8.8308)  time: 0.9760 (0.5267 -- 4.2503)  data: 0.2336 (0.0003 -- 1.6992)  max mem: 16413
[2023-08-31 14:19:32,240] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12524
[2023-08-31 14:19:32,240] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:19:32,240] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12524
[2023-08-31 14:19:32,243] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:19:32,256] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [78]  [ 60/160]  eta: 0:01:36  lr: 0.000032  min_lr: 0.000001  loss: 1.8189 (1.7981)  loss_scale: 8192.0000 (14100.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3144 (8.9095)  time: 0.7860 (0.5135 -- 3.0735)  data: 0.0363 (0.0003 -- 0.6709)  max mem: 16413
Epoch: [78]  [ 80/160]  eta: 0:01:14  lr: 0.000032  min_lr: 0.000001  loss: 1.9276 (1.8096)  loss_scale: 8192.0000 (12641.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6356 (8.9589)  time: 0.8400 (0.5276 -- 2.4450)  data: 0.1171 (0.0005 -- 1.3789)  max mem: 16413
Epoch: [78]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 1.8232 (1.8062)  loss_scale: 8192.0000 (11760.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3450 (8.9523)  time: 0.8515 (0.5277 -- 4.3314)  data: 0.3005 (0.0003 -- 3.7913)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:36  lr: 0.000032  min_lr: 0.000001  loss: 1.8050 (1.8079)  loss_scale: 8192.0000 (11170.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6297 (8.8177)  time: 0.9020 (0.5390 -- 2.7809)  data: 0.2763 (0.0004 -- 2.2531)  max mem: 16413
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.7131 (1.7895)  loss_scale: 8192.0000 (10748.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7935 (8.8182)  time: 0.9787 (0.5218 -- 3.5844)  data: 0.4083 (0.0003 -- 3.0571)  max mem: 16413
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.7656 (1.7816)  loss_scale: 8192.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6860 (8.8177)  time: 0.6115 (0.4950 -- 1.6915)  data: 0.0572 (0.0002 -- 1.1300)  max mem: 16413
Epoch: [78] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.7656 (1.7853)  loss_scale: 8192.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6860 (8.8177)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2573 (0.2573)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4406 (2.4406 -- 2.4406)  data: 2.2103 (2.2103 -- 2.2103)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4944 (0.6960)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4443 (0.2036 -- 2.4406)  data: 0.2253 (0.0009 -- 2.2103)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5024 (0.6183)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (95.7672)  time: 0.2255 (0.1692 -- 0.4701)  data: 0.0206 (0.0001 -- 0.2478)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5223 (0.6716)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (95.0207)  time: 0.2089 (0.1328 -- 0.4701)  data: 0.0198 (0.0001 -- 0.2478)  max mem: 16413
Val: Total time: 0:00:07 (0.2938 s / it)
* Acc@1 83.402 Acc@5 96.888 loss 0.648
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.23%
Epoch: [79]  [  0/160]  eta: 0:24:26  lr: 0.000032  min_lr: 0.000001  loss: 1.8843 (1.8843)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5524 (6.5524)  time: 9.1660 (9.1660 -- 9.1660)  data: 8.6314 (8.6314 -- 8.6314)  max mem: 16413
[2023-08-31 14:21:35,697] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:21:35,698] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:21:35,698] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:21:35,698] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [79]  [ 20/160]  eta: 0:02:43  lr: 0.000032  min_lr: 0.000001  loss: 1.7670 (1.8113)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9096 (8.1501)  time: 0.7674 (0.5308 -- 3.0391)  data: 0.2101 (0.0004 -- 2.4983)  max mem: 16413
[2023-08-31 14:21:49,911] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12668
[2023-08-31 14:21:49,911] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12668
[2023-08-31 14:21:49,911] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:21:49,911] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:21:49,911] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [79]  [ 40/160]  eta: 0:02:04  lr: 0.000032  min_lr: 0.000001  loss: 1.8875 (1.8569)  loss_scale: 8192.0000 (11189.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3051 (8.4505)  time: 0.9020 (0.5205 -- 3.7369)  data: 0.0581 (0.0003 -- 0.9095)  max mem: 16413
Epoch: [79]  [ 60/160]  eta: 0:01:38  lr: 0.000032  min_lr: 0.000001  loss: 1.8932 (1.8634)  loss_scale: 8192.0000 (10206.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5711 (8.6306)  time: 0.8779 (0.5223 -- 4.2072)  data: 0.0012 (0.0004 -- 0.0029)  max mem: 16413
Epoch: [79]  [ 80/160]  eta: 0:01:14  lr: 0.000032  min_lr: 0.000001  loss: 1.8896 (1.8466)  loss_scale: 8192.0000 (9709.0370)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1915 (8.8751)  time: 0.7887 (0.5168 -- 3.2630)  data: 0.0019 (0.0002 -- 0.0060)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:57  lr: 0.000032  min_lr: 0.000001  loss: 1.8443 (1.8416)  loss_scale: 8192.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5392 (8.8208)  time: 1.0103 (0.5236 -- 4.9200)  data: 0.0014 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:37  lr: 0.000032  min_lr: 0.000001  loss: 1.5222 (1.7925)  loss_scale: 8192.0000 (9207.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0329 (8.7864)  time: 0.7926 (0.5116 -- 4.2487)  data: 0.0015 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000032  min_lr: 0.000001  loss: 1.7647 (1.7796)  loss_scale: 8192.0000 (9063.4894)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6235 (8.7690)  time: 0.8519 (0.5154 -- 4.2952)  data: 0.0013 (0.0003 -- 0.0055)  max mem: 16413
[2023-08-31 14:23:38,230] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:23:38,230] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:23:38,232] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:23:38,232] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000001  loss: 1.5266 (1.7596)  loss_scale: 8192.0000 (9113.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5569 (8.7048)  time: 0.6973 (0.4941 -- 3.2554)  data: 0.0009 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [79] Total time: 0:02:22 (0.8902 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000001  loss: 1.5266 (1.7771)  loss_scale: 8192.0000 (9113.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5569 (8.7048)
[2023-08-31 14:23:39,249] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-08-31 14:23:39,251] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-08-31 14:23:39,251] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-08-31 14:23:39,251] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-08-31 14:23:40,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-08-31 14:23:40,222] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2638 (0.2638)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5145 (2.5145 -- 2.5145)  data: 2.2988 (2.2988 -- 2.2988)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4408 (0.7036)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4377 (0.2027 -- 2.5145)  data: 0.2173 (0.0007 -- 2.2988)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4408 (0.6239)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2176 (0.1700 -- 0.2751)  data: 0.0092 (0.0001 -- 0.0679)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5283 (0.6738)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (97.0954)  time: 0.2000 (0.1338 -- 0.2751)  data: 0.0082 (0.0001 -- 0.0679)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.646
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.23%
Epoch: [80]  [  0/160]  eta: 0:21:38  lr: 0.000032  min_lr: 0.000001  loss: 2.0339 (2.0339)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.9304 (12.9304)  time: 8.1136 (8.1136 -- 8.1136)  data: 6.2242 (6.2242 -- 6.2242)  max mem: 16413
Epoch: [80]  [ 20/160]  eta: 0:02:46  lr: 0.000032  min_lr: 0.000001  loss: 1.7471 (1.7971)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8301 (9.0214)  time: 0.8402 (0.5215 -- 4.0013)  data: 0.0347 (0.0005 -- 0.3963)  max mem: 16413
Epoch: [80]  [ 40/160]  eta: 0:02:02  lr: 0.000032  min_lr: 0.000001  loss: 1.6853 (1.7606)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9014 (9.2202)  time: 0.8510 (0.5241 -- 3.0269)  data: 0.0149 (0.0002 -- 0.2583)  max mem: 16413
Epoch: [80]  [ 60/160]  eta: 0:01:38  lr: 0.000032  min_lr: 0.000001  loss: 1.6572 (1.7603)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7769 (8.9614)  time: 0.8968 (0.5164 -- 2.3391)  data: 0.0138 (0.0007 -- 0.2439)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:16  lr: 0.000032  min_lr: 0.000001  loss: 1.8013 (1.7772)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1579 (8.9050)  time: 0.8595 (0.5343 -- 2.3824)  data: 0.1436 (0.0005 -- 1.8334)  max mem: 16413
Epoch: [80]  [100/160]  eta: 0:00:55  lr: 0.000032  min_lr: 0.000001  loss: 1.8735 (1.7865)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5183 (8.7507)  time: 0.7989 (0.5383 -- 3.2946)  data: 0.2133 (0.0003 -- 2.7740)  max mem: 16413
[2023-08-31 14:25:24,906] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12904
[2023-08-31 14:25:24,906] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12904
[2023-08-31 14:25:24,906] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:25:24,906] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 14:25:24,906] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.8213 (1.8018)  loss_scale: 8192.0000 (15233.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3135 (8.8523)  time: 0.8649 (0.5335 -- 2.7668)  data: 0.2234 (0.0006 -- 2.2463)  max mem: 16413
Epoch: [80]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.4698 (1.7717)  loss_scale: 8192.0000 (14234.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0546 (8.8971)  time: 0.9187 (0.5243 -- 3.9862)  data: 0.0556 (0.0004 -- 0.6697)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.8370 (1.7851)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7111 (9.0806)  time: 0.7037 (0.4948 -- 3.4304)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [80] Total time: 0:02:22 (0.8893 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.8370 (1.7824)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7111 (9.0806)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.1970 (0.1970)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6739 (2.6739 -- 2.6739)  data: 2.4180 (2.4180 -- 2.4180)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3978 (0.7041)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4361 (0.2028 -- 2.6739)  data: 0.2208 (0.0004 -- 2.4180)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3980 (0.6119)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.2963)  time: 0.2113 (0.1689 -- 0.3610)  data: 0.0097 (0.0001 -- 0.1810)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5135 (0.6640)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.2656)  time: 0.1969 (0.1327 -- 0.3610)  data: 0.0094 (0.0001 -- 0.1810)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.639
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.23%
Epoch: [81]  [  0/160]  eta: 0:22:22  lr: 0.000031  min_lr: 0.000001  loss: 2.0991 (2.0991)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1321 (8.1321)  time: 8.3888 (8.3888 -- 8.3888)  data: 7.8417 (7.8417 -- 7.8417)  max mem: 16413
Epoch: [81]  [ 20/160]  eta: 0:02:44  lr: 0.000031  min_lr: 0.000001  loss: 1.8651 (1.8296)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1689 (8.5387)  time: 0.8159 (0.5258 -- 2.1445)  data: 0.1043 (0.0006 -- 1.5167)  max mem: 16413
[2023-08-31 14:26:59,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=73, lr=[7.441574443416659e-07, 7.441574443416659e-07, 9.922099257888878e-07, 9.922099257888878e-07, 1.3229465677185172e-06, 1.3229465677185172e-06, 1.763928756958023e-06, 1.763928756958023e-06, 2.3519050092773636e-06, 2.3519050092773636e-06, 3.135873345703152e-06, 3.135873345703152e-06, 4.181164460937536e-06, 4.181164460937536e-06, 5.574885947916714e-06, 5.574885947916714e-06, 7.433181263888952e-06, 7.433181263888952e-06, 9.910908351851937e-06, 9.910908351851937e-06, 1.3214544469135915e-05, 1.3214544469135915e-05, 1.7619392625514554e-05, 1.7619392625514554e-05, 2.349252350068607e-05, 2.349252350068607e-05, 3.132336466758143e-05, 3.132336466758143e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 14:26:59,575] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=16.626617232276672, CurrSamplesPerSec=23.173643778474034, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:02  lr: 0.000031  min_lr: 0.000001  loss: 1.4831 (1.7366)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4441 (8.8575)  time: 0.8531 (0.5236 -- 3.0005)  data: 0.0216 (0.0002 -- 0.4032)  max mem: 16413
Epoch: [81]  [ 60/160]  eta: 0:01:40  lr: 0.000031  min_lr: 0.000001  loss: 1.9179 (1.7945)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0044 (8.7197)  time: 0.9917 (0.5179 -- 5.6344)  data: 0.3374 (0.0004 -- 5.1151)  max mem: 16413
[2023-08-31 14:27:30,531] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:27:30,531] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:27:30,531] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:27:30,531] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [81]  [ 80/160]  eta: 0:01:17  lr: 0.000031  min_lr: 0.000001  loss: 1.7894 (1.7767)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2288 (8.6349)  time: 0.8670 (0.5289 -- 3.5464)  data: 0.3241 (0.0002 -- 2.9779)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:56  lr: 0.000031  min_lr: 0.000001  loss: 1.7977 (1.7748)  loss_scale: 16384.0000 (10463.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8983 (8.5739)  time: 0.7846 (0.5280 -- 5.1835)  data: 0.2353 (0.0003 -- 4.6787)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.5570 (1.7562)  loss_scale: 16384.0000 (11441.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2227 (8.5217)  time: 0.7443 (0.5344 -- 2.1635)  data: 0.1294 (0.0005 -- 1.5988)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.8118 (1.7613)  loss_scale: 16384.0000 (12142.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2041 (8.5212)  time: 0.9633 (0.5203 -- 3.1085)  data: 0.2799 (0.0008 -- 2.5485)  max mem: 16413
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.7023 (1.7453)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6898 (8.4786)  time: 0.7297 (0.4948 -- 3.6119)  data: 0.0418 (0.0002 -- 0.8246)  max mem: 16413
Epoch: [81] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.7023 (1.7430)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6898 (8.4786)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2422 (0.2422)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5618 (2.5618 -- 2.5618)  data: 2.3442 (2.3442 -- 2.3442)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4305 (0.7096)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4422 (0.1994 -- 2.5618)  data: 0.2286 (0.0006 -- 2.3442)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4305 (0.6177)  acc1: 88.8889 (86.7725)  acc5: 100.0000 (96.8254)  time: 0.2157 (0.1691 -- 0.3798)  data: 0.0122 (0.0001 -- 0.1614)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5144 (0.6778)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (96.2656)  time: 0.2006 (0.1329 -- 0.3798)  data: 0.0120 (0.0001 -- 0.1614)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 83.817 Acc@5 96.888 loss 0.657
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.23%
Epoch: [82]  [  0/160]  eta: 0:21:09  lr: 0.000031  min_lr: 0.000001  loss: 2.0422 (2.0422)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1179 (9.1179)  time: 7.9337 (7.9337 -- 7.9337)  data: 6.4406 (6.4406 -- 6.4406)  max mem: 16413
Epoch: [82]  [ 20/160]  eta: 0:02:39  lr: 0.000031  min_lr: 0.000001  loss: 1.8110 (1.8303)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0002 (9.3198)  time: 0.7960 (0.5331 -- 2.4546)  data: 0.1226 (0.0004 -- 0.7484)  max mem: 16413
Epoch: [82]  [ 40/160]  eta: 0:02:12  lr: 0.000031  min_lr: 0.000001  loss: 1.7374 (1.7952)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3185 (9.2927)  time: 1.0688 (0.5208 -- 4.5339)  data: 0.5243 (0.0005 -- 4.0287)  max mem: 16413
[2023-08-31 14:29:34,844] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:29:34,844] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:29:34,845] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:29:34,845] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [82]  [ 60/160]  eta: 0:01:38  lr: 0.000031  min_lr: 0.000001  loss: 1.8011 (1.7804)  loss_scale: 32768.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8257 (9.2678)  time: 0.7371 (0.5237 -- 2.6046)  data: 0.1953 (0.0003 -- 2.0331)  max mem: 16413
[2023-08-31 14:29:50,221] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13182
[2023-08-31 14:29:50,221] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13182
[2023-08-31 14:29:50,221] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:29:50,221] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:29:50,221] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [82]  [ 80/160]  eta: 0:01:13  lr: 0.000031  min_lr: 0.000001  loss: 1.8335 (1.8038)  loss_scale: 16384.0000 (20631.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1785 (9.1348)  time: 0.7142 (0.5296 -- 3.3334)  data: 0.1550 (0.0005 -- 2.7933)  max mem: 16413
Epoch: [82]  [100/160]  eta: 0:00:55  lr: 0.000031  min_lr: 0.000001  loss: 1.9760 (1.8237)  loss_scale: 16384.0000 (19790.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6791 (8.9074)  time: 0.9521 (0.5326 -- 4.1405)  data: 0.0040 (0.0002 -- 0.0491)  max mem: 16413
[2023-08-31 14:30:24,579] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13224
[2023-08-31 14:30:24,579] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13224
[2023-08-31 14:30:24,579] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:30:24,579] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:30:24,579] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [82]  [120/160]  eta: 0:00:36  lr: 0.000031  min_lr: 0.000001  loss: 1.7009 (1.8014)  loss_scale: 8192.0000 (18076.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7830 (8.8845)  time: 0.8271 (0.5334 -- 2.4979)  data: 0.0864 (0.0005 -- 1.5739)  max mem: 16413
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000031  min_lr: 0.000001  loss: 1.6081 (1.7827)  loss_scale: 8192.0000 (16674.4965)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7171 (8.7985)  time: 0.9693 (0.5206 -- 2.5687)  data: 0.0852 (0.0007 -- 1.6484)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000001  loss: 1.6430 (1.7700)  loss_scale: 8192.0000 (15667.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7083 (8.8701)  time: 0.6611 (0.4948 -- 2.2642)  data: 0.0008 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [82] Total time: 0:02:20 (0.8806 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000001  loss: 1.6430 (1.7704)  loss_scale: 8192.0000 (15667.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7083 (8.8701)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2314 (0.2314)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4597 (2.4597 -- 2.4597)  data: 2.2216 (2.2216 -- 2.2216)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4371 (0.7087)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4495 (0.1900 -- 2.4597)  data: 0.2329 (0.0006 -- 2.2216)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4371 (0.6139)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.2963)  time: 0.2241 (0.1680 -- 0.5675)  data: 0.0211 (0.0001 -- 0.3305)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4948 (0.6679)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (95.8506)  time: 0.2088 (0.1330 -- 0.5675)  data: 0.0208 (0.0001 -- 0.3305)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 82.988 Acc@5 96.888 loss 0.644
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.23%
Epoch: [83]  [  0/160]  eta: 0:21:09  lr: 0.000031  min_lr: 0.000001  loss: 1.8219 (1.8219)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9238 (6.9238)  time: 7.9353 (7.9353 -- 7.9353)  data: 7.4018 (7.4018 -- 7.4018)  max mem: 16413
Epoch: [83]  [ 20/160]  eta: 0:02:39  lr: 0.000031  min_lr: 0.000001  loss: 1.6687 (1.7557)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5937 (9.0658)  time: 0.7985 (0.5322 -- 3.0803)  data: 0.2512 (0.0002 -- 2.5293)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:01:59  lr: 0.000031  min_lr: 0.000001  loss: 1.6121 (1.6863)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8874 (8.8346)  time: 0.8391 (0.5280 -- 3.0391)  data: 0.1469 (0.0004 -- 1.5200)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:40  lr: 0.000031  min_lr: 0.000001  loss: 1.8550 (1.7351)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5141 (8.7325)  time: 1.0157 (0.5258 -- 4.1223)  data: 0.0019 (0.0002 -- 0.0068)  max mem: 16413
[2023-08-31 14:32:27,710] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:32:27,710] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:32:27,711] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:32:27,711] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [83]  [ 80/160]  eta: 0:01:16  lr: 0.000031  min_lr: 0.000001  loss: 1.8253 (1.7419)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6320 (8.6379)  time: 0.8448 (0.5151 -- 4.8086)  data: 0.0015 (0.0003 -- 0.0086)  max mem: 16413
Epoch: [83]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000001  loss: 1.9685 (1.7850)  loss_scale: 16384.0000 (10463.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3991 (8.6226)  time: 0.8606 (0.5250 -- 3.2167)  data: 0.0018 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [83]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 1.5363 (1.7590)  loss_scale: 16384.0000 (11441.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1743 (8.6548)  time: 0.7935 (0.5241 -- 4.0513)  data: 0.0018 (0.0005 -- 0.0068)  max mem: 16413
Epoch: [83]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.5429 (1.7385)  loss_scale: 16384.0000 (12142.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0916 (8.7029)  time: 1.0074 (0.5293 -- 4.1824)  data: 0.0019 (0.0005 -- 0.0067)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.9140 (1.7528)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7206 (8.7528)  time: 0.6268 (0.4940 -- 2.7272)  data: 0.0007 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [83] Total time: 0:02:23 (0.8947 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.9140 (1.7737)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7206 (8.7528)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2543 (0.2543)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3628 (2.3628 -- 2.3628)  data: 2.1493 (2.1493 -- 2.1493)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5027 (0.6919)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4226 (0.2037 -- 2.3628)  data: 0.2061 (0.0009 -- 2.1493)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5027 (0.6148)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2296 (0.1687 -- 0.5650)  data: 0.0248 (0.0001 -- 0.3755)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5537 (0.6684)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2131 (0.1327 -- 0.5650)  data: 0.0238 (0.0001 -- 0.3755)  max mem: 16413
Val: Total time: 0:00:07 (0.2940 s / it)
* Acc@1 81.950 Acc@5 96.888 loss 0.649
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 84.23%
Epoch: [84]  [  0/160]  eta: 0:20:59  lr: 0.000030  min_lr: 0.000001  loss: 1.9868 (1.9868)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1604 (7.1604)  time: 7.8741 (7.8741 -- 7.8741)  data: 7.3704 (7.3704 -- 7.3704)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:53  lr: 0.000030  min_lr: 0.000001  loss: 1.5773 (1.6360)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0219 (8.1035)  time: 0.9047 (0.5349 -- 4.0313)  data: 0.3499 (0.0004 -- 3.5185)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:06  lr: 0.000030  min_lr: 0.000001  loss: 1.7798 (1.7107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8854 (8.1869)  time: 0.8678 (0.5333 -- 3.3645)  data: 0.3180 (0.0007 -- 2.8162)  max mem: 16413
[2023-08-31 14:34:32,879] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:34:32,879] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:34:32,880] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:34:32,880] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [84]  [ 60/160]  eta: 0:01:39  lr: 0.000030  min_lr: 0.000001  loss: 1.8202 (1.7550)  loss_scale: 32768.0000 (21755.8033)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2023 (8.2394)  time: 0.8632 (0.5183 -- 4.1064)  data: 0.3116 (0.0005 -- 3.5704)  max mem: 16413
[2023-08-31 14:34:51,288] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13503
[2023-08-31 14:34:51,288] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13503
[2023-08-31 14:34:51,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:34:51,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:34:51,288] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [84]  [ 80/160]  eta: 0:01:17  lr: 0.000030  min_lr: 0.000001  loss: 1.6107 (1.7376)  loss_scale: 16384.0000 (20833.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3051 (8.3299)  time: 0.8958 (0.5333 -- 4.2974)  data: 0.3456 (0.0006 -- 3.7878)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:56  lr: 0.000030  min_lr: 0.000001  loss: 1.6311 (1.7228)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7518 (8.1416)  time: 0.7950 (0.5315 -- 3.2966)  data: 0.2477 (0.0004 -- 2.7691)  max mem: 16413
Epoch: [84]  [120/160]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000001  loss: 1.8660 (1.7382)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9768 (8.1518)  time: 0.9115 (0.5322 -- 3.5844)  data: 0.3524 (0.0004 -- 3.0496)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.6706 (1.7370)  loss_scale: 16384.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0978 (8.3877)  time: 0.7725 (0.5211 -- 2.1036)  data: 0.1529 (0.0006 -- 1.5842)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.7312 (1.7280)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3546 (8.3055)  time: 0.7625 (0.4947 -- 2.8112)  data: 0.1818 (0.0002 -- 2.2608)  max mem: 16413
Epoch: [84] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.7312 (1.7611)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3546 (8.3055)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2319 (0.2319)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3698 (2.3698 -- 2.3698)  data: 2.1338 (2.1338 -- 2.1338)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4854 (0.7100)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4164 (0.2108 -- 2.3698)  data: 0.1969 (0.0007 -- 2.1338)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4854 (0.6114)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2215 (0.1695 -- 0.4793)  data: 0.0165 (0.0001 -- 0.2944)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5051 (0.6652)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (95.8506)  time: 0.2034 (0.1330 -- 0.4793)  data: 0.0160 (0.0001 -- 0.2944)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 83.402 Acc@5 97.095 loss 0.643
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.23%
Epoch: [85]  [  0/160]  eta: 0:22:40  lr: 0.000030  min_lr: 0.000001  loss: 1.5064 (1.5064)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0428 (8.0428)  time: 8.5025 (8.5025 -- 8.5025)  data: 6.6604 (6.6604 -- 6.6604)  max mem: 16413
Epoch: [85]  [ 20/160]  eta: 0:02:39  lr: 0.000030  min_lr: 0.000001  loss: 1.7624 (1.7267)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1265 (8.5107)  time: 0.7709 (0.5341 -- 2.2709)  data: 0.0350 (0.0004 -- 0.6740)  max mem: 16413
[2023-08-31 14:36:54,281] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:36:54,281] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:36:54,282] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:36:54,282] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [85]  [ 40/160]  eta: 0:02:03  lr: 0.000030  min_lr: 0.000001  loss: 1.7924 (1.7200)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7648 (8.1432)  time: 0.9103 (0.5169 -- 2.5710)  data: 0.0089 (0.0003 -- 0.1482)  max mem: 16413
[2023-08-31 14:37:06,017] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13647
[2023-08-31 14:37:06,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:37:06,017] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13647
[2023-08-31 14:37:06,017] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 14:37:06,017] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [85]  [ 60/160]  eta: 0:01:35  lr: 0.000030  min_lr: 0.000001  loss: 1.4994 (1.6678)  loss_scale: 16384.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0840 (7.9559)  time: 0.8168 (0.5109 -- 2.3197)  data: 0.0098 (0.0004 -- 0.1628)  max mem: 16413
Epoch: [85]  [ 80/160]  eta: 0:01:15  lr: 0.000030  min_lr: 0.000001  loss: 1.8802 (1.7164)  loss_scale: 16384.0000 (19418.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2959 (7.8940)  time: 0.9240 (0.5348 -- 3.6572)  data: 0.0771 (0.0008 -- 0.7682)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:55  lr: 0.000030  min_lr: 0.000001  loss: 1.6071 (1.6918)  loss_scale: 16384.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6567 (8.0280)  time: 0.8540 (0.5294 -- 2.3330)  data: 0.1448 (0.0003 -- 1.5824)  max mem: 16413
Epoch: [85]  [120/160]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000001  loss: 1.6917 (1.7016)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5830 (8.2449)  time: 0.8823 (0.5203 -- 3.1205)  data: 0.0563 (0.0003 -- 1.1036)  max mem: 16413
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000030  min_lr: 0.000001  loss: 1.6720 (1.7118)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7814 (8.3723)  time: 0.7952 (0.5347 -- 2.5350)  data: 0.1930 (0.0002 -- 1.9852)  max mem: 16413
[2023-08-31 14:38:39,099] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13753
[2023-08-31 14:38:39,099] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13753
[2023-08-31 14:38:39,099] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:38:39,099] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:38:39,099] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000001  loss: 1.8569 (1.7289)  loss_scale: 16384.0000 (17561.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5955 (8.4179)  time: 0.8239 (0.4834 -- 3.4553)  data: 0.1127 (0.0002 -- 1.3102)  max mem: 16413
Epoch: [85] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000001  loss: 1.8569 (1.7665)  loss_scale: 16384.0000 (17561.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5955 (8.4179)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2156 (0.2156)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4629 (2.4629 -- 2.4629)  data: 2.2110 (2.2110 -- 2.2110)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4788 (0.7122)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4362 (0.1987 -- 2.4629)  data: 0.2150 (0.0006 -- 2.2110)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4862 (0.6296)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2194 (0.1696 -- 0.3864)  data: 0.0132 (0.0001 -- 0.1451)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5378 (0.6800)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.2656)  time: 0.2044 (0.1327 -- 0.3864)  data: 0.0128 (0.0001 -- 0.1451)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.659
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.23%
Epoch: [86]  [  0/160]  eta: 0:18:14  lr: 0.000030  min_lr: 0.000001  loss: 1.9175 (1.9175)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1621 (7.1621)  time: 6.8430 (6.8430 -- 6.8430)  data: 6.2979 (6.2979 -- 6.2979)  max mem: 16413
Epoch: [86]  [ 20/160]  eta: 0:02:48  lr: 0.000030  min_lr: 0.000001  loss: 1.8606 (1.8042)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7695 (8.7487)  time: 0.9247 (0.5083 -- 2.8831)  data: 0.2014 (0.0003 -- 2.3584)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:05  lr: 0.000030  min_lr: 0.000001  loss: 1.6626 (1.7423)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3428 (9.3805)  time: 0.8812 (0.5180 -- 3.2656)  data: 0.1158 (0.0004 -- 1.3180)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:41  lr: 0.000029  min_lr: 0.000001  loss: 1.6910 (1.7378)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9592 (9.4801)  time: 0.9387 (0.5274 -- 3.6261)  data: 0.0011 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [86]  [ 80/160]  eta: 0:01:15  lr: 0.000029  min_lr: 0.000001  loss: 1.8588 (1.7672)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3257 (9.2988)  time: 0.7437 (0.5412 -- 2.4569)  data: 0.0350 (0.0005 -- 0.2915)  max mem: 16413
Epoch: [86]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000001  loss: 1.7399 (1.7569)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1828 (9.1224)  time: 0.8270 (0.5335 -- 3.1105)  data: 0.1395 (0.0004 -- 2.3045)  max mem: 16413
Epoch: [86]  [120/160]  eta: 0:00:38  lr: 0.000029  min_lr: 0.000001  loss: 1.8772 (1.7788)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1487 (9.0642)  time: 1.1074 (0.5307 -- 4.0026)  data: 0.5627 (0.0004 -- 3.4587)  max mem: 16413
[2023-08-31 14:40:47,310] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:40:47,310] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:40:47,310] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:40:47,311] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.6890 (1.7663)  loss_scale: 16384.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7287 (8.9991)  time: 0.8237 (0.5161 -- 3.5426)  data: 0.2819 (0.0003 -- 3.0100)  max mem: 16413
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.7794 (1.7589)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9280 (8.8607)  time: 0.6787 (0.4938 -- 3.8586)  data: 0.1673 (0.0002 -- 3.3360)  max mem: 16413
Epoch: [86] Total time: 0:02:24 (0.9053 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.7794 (1.7279)  loss_scale: 16384.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9280 (8.8607)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2030 (0.2030)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4967 (2.4967 -- 2.4967)  data: 2.2541 (2.2541 -- 2.2541)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4634 (0.7223)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (96.9697)  time: 0.4510 (0.1944 -- 2.4967)  data: 0.2340 (0.0006 -- 2.2541)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4634 (0.6206)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.2963)  time: 0.2199 (0.1708 -- 0.5751)  data: 0.0195 (0.0001 -- 0.3073)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4919 (0.6832)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (95.4357)  time: 0.2052 (0.1325 -- 0.5751)  data: 0.0190 (0.0001 -- 0.3073)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 84.440 Acc@5 96.473 loss 0.656
Accuracy of the network on the 482 val images: 84.44%
[2023-08-31 14:41:23,649] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 14:41:23,651] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 14:41:23,651] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 14:41:23,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 14:41:25,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 14:41:25,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 84.44%
Epoch: [87]  [  0/160]  eta: 0:22:01  lr: 0.000029  min_lr: 0.000001  loss: 1.9013 (1.9013)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9751 (8.9751)  time: 8.2624 (8.2624 -- 8.2624)  data: 7.7367 (7.7367 -- 7.7367)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:45  lr: 0.000029  min_lr: 0.000001  loss: 1.6088 (1.6985)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7355 (8.7850)  time: 0.8271 (0.5232 -- 3.4757)  data: 0.2608 (0.0001 -- 2.9530)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:01  lr: 0.000029  min_lr: 0.000001  loss: 1.7347 (1.6911)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7882 (8.3497)  time: 0.8418 (0.5328 -- 2.7660)  data: 0.2907 (0.0003 -- 2.2067)  max mem: 16413
[2023-08-31 14:42:10,808] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13966
[2023-08-31 14:42:10,809] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13966
[2023-08-31 14:42:10,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:42:10,809] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 14:42:10,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [87]  [ 60/160]  eta: 0:01:35  lr: 0.000029  min_lr: 0.000001  loss: 1.6977 (1.6957)  loss_scale: 8192.0000 (14369.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8172 (8.5535)  time: 0.8355 (0.5212 -- 2.8960)  data: 0.1876 (0.0003 -- 1.3896)  max mem: 16413
[2023-08-31 14:42:39,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=79, lr=[6.905423794084148e-07, 6.905423794084148e-07, 9.207231725445531e-07, 9.207231725445531e-07, 1.2276308967260707e-06, 1.2276308967260707e-06, 1.636841195634761e-06, 1.636841195634761e-06, 2.1824549275130148e-06, 2.1824549275130148e-06, 2.909939903350686e-06, 2.909939903350686e-06, 3.8799198711342485e-06, 3.8799198711342485e-06, 5.173226494845664e-06, 5.173226494845664e-06, 6.897635326460885e-06, 6.897635326460885e-06, 9.196847101947847e-06, 9.196847101947847e-06, 1.226246280259713e-05, 1.226246280259713e-05, 1.634995040346284e-05, 1.634995040346284e-05, 2.1799933871283787e-05, 2.1799933871283787e-05, 2.906657849504505e-05, 2.906657849504505e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 14:42:39,197] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=16.712804522637636, CurrSamplesPerSec=23.553923052840393, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:15  lr: 0.000029  min_lr: 0.000001  loss: 2.0511 (1.7716)  loss_scale: 8192.0000 (12844.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9317 (8.4576)  time: 0.9087 (0.5280 -- 3.4788)  data: 0.2458 (0.0007 -- 2.9625)  max mem: 16413
Epoch: [87]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000001  loss: 1.6870 (1.7527)  loss_scale: 8192.0000 (11923.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6738 (8.5861)  time: 0.8090 (0.5207 -- 3.0464)  data: 0.0013 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [87]  [120/160]  eta: 0:00:35  lr: 0.000029  min_lr: 0.000001  loss: 1.7440 (1.7588)  loss_scale: 8192.0000 (11306.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3480 (8.5555)  time: 0.7886 (0.5210 -- 2.6424)  data: 0.0256 (0.0004 -- 0.4658)  max mem: 16413
Epoch: [87]  [140/160]  eta: 0:00:17  lr: 0.000029  min_lr: 0.000001  loss: 1.7938 (1.7729)  loss_scale: 8192.0000 (10864.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6324 (8.5541)  time: 0.8679 (0.5192 -- 2.6326)  data: 0.1919 (0.0003 -- 2.1048)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.6857 (1.7733)  loss_scale: 8192.0000 (10547.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2115 (8.5049)  time: 0.7955 (0.4957 -- 1.7802)  data: 0.1863 (0.0002 -- 1.2606)  max mem: 16413
Epoch: [87] Total time: 0:02:20 (0.8801 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.6857 (1.7443)  loss_scale: 8192.0000 (10547.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2115 (8.5049)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2215 (0.2215)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3637 (2.3637 -- 2.3637)  data: 2.1097 (2.1097 -- 2.1097)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3944 (0.7093)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4426 (0.1969 -- 2.3637)  data: 0.2269 (0.0006 -- 2.1097)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4422 (0.6078)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.2963)  time: 0.2273 (0.1700 -- 0.6041)  data: 0.0244 (0.0001 -- 0.3776)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4934 (0.6626)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (95.4357)  time: 0.2140 (0.1332 -- 0.6041)  data: 0.0241 (0.0001 -- 0.3776)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 84.025 Acc@5 96.680 loss 0.638
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.44%
Epoch: [88]  [  0/160]  eta: 0:16:56  lr: 0.000029  min_lr: 0.000001  loss: 1.9046 (1.9046)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5987 (4.5987)  time: 6.3514 (6.3514 -- 6.3514)  data: 5.0592 (5.0592 -- 5.0592)  max mem: 16413
[2023-08-31 14:44:12,997] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:44:12,997] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:44:12,997] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:44:12,997] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [88]  [ 20/160]  eta: 0:02:41  lr: 0.000029  min_lr: 0.000001  loss: 1.8817 (1.8406)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5478 (9.0160)  time: 0.8938 (0.5383 -- 2.9006)  data: 0.3387 (0.0008 -- 2.3412)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:01:56  lr: 0.000029  min_lr: 0.000001  loss: 1.6682 (1.7126)  loss_scale: 16384.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8815 (8.8571)  time: 0.7835 (0.5397 -- 1.4804)  data: 0.1106 (0.0006 -- 0.7598)  max mem: 16413
Epoch: [88]  [ 60/160]  eta: 0:01:35  lr: 0.000029  min_lr: 0.000001  loss: 1.8333 (1.7300)  loss_scale: 16384.0000 (14369.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9512 (8.6191)  time: 0.9212 (0.5303 -- 2.5626)  data: 0.2482 (0.0002 -- 2.0391)  max mem: 16413
Epoch: [88]  [ 80/160]  eta: 0:01:14  lr: 0.000029  min_lr: 0.000001  loss: 1.7848 (1.7292)  loss_scale: 16384.0000 (14866.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8730 (8.5098)  time: 0.8634 (0.5195 -- 3.0905)  data: 0.0458 (0.0003 -- 0.5787)  max mem: 16413
Epoch: [88]  [100/160]  eta: 0:00:55  lr: 0.000029  min_lr: 0.000001  loss: 1.8278 (1.7373)  loss_scale: 16384.0000 (15167.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8128 (8.5543)  time: 0.9337 (0.5211 -- 3.5464)  data: 0.1509 (0.0007 -- 2.9891)  max mem: 16413
[2023-08-31 14:45:40,745] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14193
[2023-08-31 14:45:40,745] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14193
[2023-08-31 14:45:40,745] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:45:40,745] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 14:45:40,746] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [88]  [120/160]  eta: 0:00:37  lr: 0.000029  min_lr: 0.000001  loss: 1.7948 (1.7491)  loss_scale: 16384.0000 (14826.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4892 (8.5969)  time: 0.9353 (0.5105 -- 3.4459)  data: 0.3978 (0.0003 -- 2.9321)  max mem: 16413
Epoch: [88]  [140/160]  eta: 0:00:18  lr: 0.000029  min_lr: 0.000001  loss: 1.6621 (1.7320)  loss_scale: 8192.0000 (13885.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5762 (8.6458)  time: 0.8502 (0.5098 -- 3.6856)  data: 0.3108 (0.0003 -- 3.1651)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000001  loss: 1.7148 (1.7289)  loss_scale: 8192.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6063 (8.6836)  time: 0.6728 (0.4962 -- 2.5214)  data: 0.1579 (0.0002 -- 1.9850)  max mem: 16413
Epoch: [88] Total time: 0:02:22 (0.8932 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000001  loss: 1.7148 (1.7423)  loss_scale: 8192.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6063 (8.6836)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2642 (0.2642)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4341 (2.4341 -- 2.4341)  data: 2.1774 (2.1774 -- 2.1774)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3929 (0.7023)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4398 (0.1867 -- 2.4341)  data: 0.2236 (0.0005 -- 2.1774)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3929 (0.6121)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2234 (0.1695 -- 0.5054)  data: 0.0167 (0.0001 -- 0.2745)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5005 (0.6650)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (95.8506)  time: 0.2088 (0.1327 -- 0.5054)  data: 0.0165 (0.0001 -- 0.2745)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.654
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 84.44%
Epoch: [89]  [  0/160]  eta: 0:20:53  lr: 0.000029  min_lr: 0.000001  loss: 1.9388 (1.9388)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5317 (11.5317)  time: 7.8355 (7.8355 -- 7.8355)  data: 7.2885 (7.2885 -- 7.2885)  max mem: 16413
Epoch: [89]  [ 20/160]  eta: 0:02:47  lr: 0.000028  min_lr: 0.000001  loss: 1.6890 (1.7613)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5925 (9.2706)  time: 0.8665 (0.5151 -- 4.1275)  data: 0.3242 (0.0007 -- 3.5903)  max mem: 16413
Epoch: [89]  [ 40/160]  eta: 0:02:07  lr: 0.000028  min_lr: 0.000001  loss: 1.8512 (1.7722)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9464 (9.3478)  time: 0.9171 (0.5303 -- 3.3768)  data: 0.3636 (0.0005 -- 2.8481)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:35  lr: 0.000028  min_lr: 0.000001  loss: 1.7185 (1.7765)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3554 (8.8931)  time: 0.7448 (0.5151 -- 1.6913)  data: 0.1424 (0.0004 -- 0.9141)  max mem: 16413
Epoch: [89]  [ 80/160]  eta: 0:01:15  lr: 0.000028  min_lr: 0.000001  loss: 1.7353 (1.7734)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1797 (9.2244)  time: 0.9141 (0.5272 -- 3.8192)  data: 0.3381 (0.0003 -- 3.3080)  max mem: 16413
[2023-08-31 14:47:42,371] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:47:42,371] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 14:47:42,374] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:47:42,374] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [89]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000001  loss: 1.6480 (1.7627)  loss_scale: 16384.0000 (9733.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7703 (9.0883)  time: 0.8289 (0.5250 -- 2.2148)  data: 0.2761 (0.0004 -- 1.6929)  max mem: 16413
Epoch: [89]  [120/160]  eta: 0:00:35  lr: 0.000028  min_lr: 0.000001  loss: 1.6191 (1.7346)  loss_scale: 16384.0000 (10832.3967)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2176 (9.1565)  time: 0.7815 (0.5224 -- 2.5185)  data: 0.1791 (0.0003 -- 1.9735)  max mem: 16413
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000001  loss: 1.9309 (1.7654)  loss_scale: 16384.0000 (11619.8582)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4232 (8.9905)  time: 1.0390 (0.5233 -- 4.4514)  data: 0.4545 (0.0007 -- 3.9164)  max mem: 16413
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.7234 (1.7603)  loss_scale: 16384.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4084 (8.9781)  time: 0.5602 (0.4952 -- 0.9199)  data: 0.0106 (0.0002 -- 0.1949)  max mem: 16413
Epoch: [89] Total time: 0:02:20 (0.8775 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.7234 (1.7881)  loss_scale: 16384.0000 (12185.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4084 (8.9781)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2414 (0.2414)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6180 (2.6180 -- 2.6180)  data: 2.3903 (2.3903 -- 2.3903)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4084 (0.7099)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4518 (0.1954 -- 2.6180)  data: 0.2356 (0.0003 -- 2.3903)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4609 (0.6192)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2156 (0.1691 -- 0.4258)  data: 0.0122 (0.0001 -- 0.1924)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5187 (0.6705)  acc1: 85.7143 (81.3278)  acc5: 100.0000 (95.4357)  time: 0.1999 (0.1332 -- 0.4258)  data: 0.0119 (0.0001 -- 0.1924)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 82.365 Acc@5 96.680 loss 0.658
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 84.44%
Epoch: [90]  [  0/160]  eta: 0:19:08  lr: 0.000028  min_lr: 0.000001  loss: 2.1246 (2.1246)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4911 (11.4911)  time: 7.1755 (7.1755 -- 7.1755)  data: 6.6427 (6.6427 -- 6.6427)  max mem: 16413
Epoch: [90]  [ 20/160]  eta: 0:02:41  lr: 0.000028  min_lr: 0.000001  loss: 1.7808 (1.7828)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8420 (8.8197)  time: 0.8549 (0.5252 -- 2.4289)  data: 0.2301 (0.0002 -- 1.8987)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:03  lr: 0.000028  min_lr: 0.000001  loss: 1.9207 (1.8455)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5011 (8.7000)  time: 0.8911 (0.5322 -- 3.1027)  data: 0.2938 (0.0003 -- 2.2395)  max mem: 16413
[2023-08-31 14:49:44,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:49:44,666] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:49:44,665] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:49:44,666] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [90]  [ 60/160]  eta: 0:01:38  lr: 0.000028  min_lr: 0.000001  loss: 1.8647 (1.8667)  loss_scale: 32768.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5021 (8.7174)  time: 0.8874 (0.5185 -- 3.0184)  data: 0.1857 (0.0005 -- 2.5010)  max mem: 16413
[2023-08-31 14:49:58,483] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14466
[2023-08-31 14:49:58,484] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:49:58,484] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 14:49:58,484] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14466
[2023-08-31 14:49:58,484] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [90]  [ 80/160]  eta: 0:01:15  lr: 0.000028  min_lr: 0.000001  loss: 1.8599 (1.8640)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0589 (8.8018)  time: 0.8161 (0.5191 -- 2.7368)  data: 0.2122 (0.0002 -- 2.2147)  max mem: 16413
Epoch: [90]  [100/160]  eta: 0:00:56  lr: 0.000028  min_lr: 0.000001  loss: 1.8081 (1.8401)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1577 (8.7193)  time: 0.9282 (0.5373 -- 2.9480)  data: 0.2105 (0.0003 -- 2.4104)  max mem: 16413
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000028  min_lr: 0.000001  loss: 1.6203 (1.8149)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6994 (8.8021)  time: 0.8342 (0.5290 -- 3.1808)  data: 0.2402 (0.0004 -- 2.6603)  max mem: 16413
Epoch: [90]  [140/160]  eta: 0:00:18  lr: 0.000028  min_lr: 0.000001  loss: 1.7378 (1.8056)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0018 (8.7962)  time: 0.9105 (0.5211 -- 3.9178)  data: 0.3473 (0.0004 -- 3.3788)  max mem: 16413
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000001  loss: 1.9501 (1.8089)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7631 (8.8941)  time: 0.5582 (0.4949 -- 1.1853)  data: 0.0341 (0.0002 -- 0.6667)  max mem: 16413
Epoch: [90] Total time: 0:02:20 (0.8766 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000001  loss: 1.9501 (1.7907)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7631 (8.8941)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2051 (0.2051)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5092 (2.5092 -- 2.5092)  data: 2.2728 (2.2728 -- 2.2728)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4270 (0.7174)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (96.9697)  time: 0.4526 (0.1994 -- 2.5092)  data: 0.2373 (0.0005 -- 2.2728)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4653 (0.6273)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2213 (0.1692 -- 0.5652)  data: 0.0171 (0.0001 -- 0.3301)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5829 (0.6813)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.2057 (0.1332 -- 0.5652)  data: 0.0169 (0.0001 -- 0.3301)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.663
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.44%
Epoch: [91]  [  0/160]  eta: 0:21:44  lr: 0.000028  min_lr: 0.000001  loss: 2.1295 (2.1295)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8345 (9.8345)  time: 8.1561 (8.1561 -- 8.1561)  data: 7.6141 (7.6141 -- 7.6141)  max mem: 16413
Epoch: [91]  [ 20/160]  eta: 0:02:46  lr: 0.000028  min_lr: 0.000001  loss: 1.7701 (1.7004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0088 (8.7195)  time: 0.8445 (0.5263 -- 5.4855)  data: 0.1981 (0.0002 -- 3.4287)  max mem: 16413
[2023-08-31 14:51:58,713] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:51:58,713] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:51:58,714] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:51:58,714] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:52:03,614] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14599
[2023-08-31 14:52:03,614] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14599
[2023-08-31 14:52:03,615] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:52:03,615] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:52:03,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [91]  [ 40/160]  eta: 0:02:08  lr: 0.000028  min_lr: 0.000001  loss: 1.8266 (1.7222)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6364 (8.7457)  time: 0.9389 (0.5226 -- 3.8575)  data: 0.0730 (0.0005 -- 1.4278)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:36  lr: 0.000028  min_lr: 0.000001  loss: 1.9830 (1.7890)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7656 (8.5912)  time: 0.7558 (0.5319 -- 3.2702)  data: 0.0015 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [91]  [ 80/160]  eta: 0:01:16  lr: 0.000028  min_lr: 0.000001  loss: 1.5972 (1.7593)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9637 (8.7033)  time: 0.9477 (0.5321 -- 3.4536)  data: 0.0016 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [91]  [100/160]  eta: 0:00:55  lr: 0.000028  min_lr: 0.000001  loss: 1.6886 (1.7551)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9713 (8.7984)  time: 0.8053 (0.5244 -- 3.9652)  data: 0.0019 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [91]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000001  loss: 1.4660 (1.7245)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1991 (8.7287)  time: 0.9585 (0.5154 -- 4.7384)  data: 0.0023 (0.0006 -- 0.0151)  max mem: 16413
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 1.5786 (1.7092)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3587 (8.9106)  time: 0.8032 (0.5306 -- 3.1703)  data: 0.0013 (0.0006 -- 0.0030)  max mem: 16413
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.4406 (1.6929)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3839 (8.9498)  time: 0.6968 (0.4942 -- 1.6861)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [91] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.4406 (1.7211)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3839 (8.9498)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2338 (0.2338)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3707 (2.3707 -- 2.3707)  data: 2.1359 (2.1359 -- 2.1359)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4151 (0.6782)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4106 (0.2037 -- 2.3707)  data: 0.1968 (0.0010 -- 2.1359)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4877 (0.5928)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2201 (0.1695 -- 0.5001)  data: 0.0170 (0.0001 -- 0.3080)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5436 (0.6472)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.6805)  time: 0.2039 (0.1326 -- 0.5001)  data: 0.0159 (0.0001 -- 0.3080)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 83.817 Acc@5 97.303 loss 0.638
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.44%
Epoch: [92]  [  0/160]  eta: 0:18:30  lr: 0.000027  min_lr: 0.000001  loss: 1.7684 (1.7684)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2040 (9.2040)  time: 6.9416 (6.9416 -- 6.9416)  data: 6.3630 (6.3630 -- 6.3630)  max mem: 16413
[2023-08-31 14:54:05,725] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:54:05,726] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:54:05,726] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:54:05,726] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:54:08,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14733
[2023-08-31 14:54:08,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14733
[2023-08-31 14:54:08,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:54:08,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:54:08,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [92]  [ 20/160]  eta: 0:02:53  lr: 0.000027  min_lr: 0.000001  loss: 1.8719 (1.7762)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6687 (8.4324)  time: 0.9515 (0.5207 -- 3.3283)  data: 0.0959 (0.0006 -- 1.3494)  max mem: 16413
Epoch: [92]  [ 40/160]  eta: 0:02:08  lr: 0.000027  min_lr: 0.000001  loss: 1.6522 (1.7144)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8651 (8.3171)  time: 0.8977 (0.5232 -- 3.3469)  data: 0.0878 (0.0002 -- 1.7256)  max mem: 16413
Epoch: [92]  [ 60/160]  eta: 0:01:42  lr: 0.000027  min_lr: 0.000001  loss: 1.5331 (1.7018)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2747 (8.3215)  time: 0.9433 (0.5323 -- 3.6413)  data: 0.0750 (0.0004 -- 0.9386)  max mem: 16413
Epoch: [92]  [ 80/160]  eta: 0:01:17  lr: 0.000027  min_lr: 0.000001  loss: 1.8852 (1.7522)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4844 (8.4589)  time: 0.7608 (0.5140 -- 3.1103)  data: 0.0017 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [92]  [100/160]  eta: 0:00:57  lr: 0.000027  min_lr: 0.000001  loss: 1.7415 (1.7401)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5628 (8.5055)  time: 0.8987 (0.5188 -- 3.8166)  data: 0.0022 (0.0003 -- 0.0160)  max mem: 16413
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000027  min_lr: 0.000001  loss: 1.6545 (1.7249)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5705 (8.6476)  time: 0.7393 (0.5389 -- 2.5473)  data: 0.0304 (0.0003 -- 0.5642)  max mem: 16413
Epoch: [92]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 1.7162 (1.7265)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6113 (8.7231)  time: 0.9067 (0.5282 -- 2.4714)  data: 0.2274 (0.0001 -- 1.9441)  max mem: 16413
[2023-08-31 14:56:03,496] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:56:03,497] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:56:03,497] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:56:03,497] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:56:04,025] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14863
[2023-08-31 14:56:04,025] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:56:04,025] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14863
[2023-08-31 14:56:04,025] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 14:56:04,025] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.6547 (1.7283)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8630 (8.5782)  time: 0.7190 (0.4962 -- 2.4880)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [92] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.6547 (1.7407)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8630 (8.5782)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2224 (0.2224)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3575 (2.3575 -- 2.3575)  data: 2.1184 (2.1184 -- 2.1184)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4020 (0.6886)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4305 (0.1946 -- 2.3575)  data: 0.2103 (0.0004 -- 2.1184)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4020 (0.6038)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2239 (0.1692 -- 0.3867)  data: 0.0195 (0.0001 -- 0.1920)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4979 (0.6657)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.2656)  time: 0.2082 (0.1325 -- 0.3867)  data: 0.0184 (0.0001 -- 0.1920)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 83.817 Acc@5 97.095 loss 0.637
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.44%
Epoch: [93]  [  0/160]  eta: 0:19:51  lr: 0.000027  min_lr: 0.000001  loss: 1.2444 (1.2444)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2338 (4.2338)  time: 7.4479 (7.4479 -- 7.4479)  data: 5.5268 (5.5268 -- 5.5268)  max mem: 16413
Epoch: [93]  [ 20/160]  eta: 0:02:44  lr: 0.000027  min_lr: 0.000001  loss: 1.7527 (1.7164)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1089 (8.4751)  time: 0.8606 (0.5308 -- 4.9091)  data: 0.0565 (0.0005 -- 0.6723)  max mem: 16413
Epoch: [93]  [ 40/160]  eta: 0:02:07  lr: 0.000027  min_lr: 0.000001  loss: 1.5654 (1.6927)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3910 (8.5469)  time: 0.9433 (0.5179 -- 4.8417)  data: 0.0300 (0.0003 -- 0.5743)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:40  lr: 0.000027  min_lr: 0.000001  loss: 1.7056 (1.6951)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7289 (8.5828)  time: 0.8977 (0.5196 -- 4.1355)  data: 0.0834 (0.0003 -- 1.2661)  max mem: 16413
Epoch: [93]  [ 80/160]  eta: 0:01:17  lr: 0.000027  min_lr: 0.000001  loss: 1.8205 (1.7068)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1791 (8.5798)  time: 0.8658 (0.5068 -- 2.6918)  data: 0.0012 (0.0001 -- 0.0026)  max mem: 16413
Epoch: [93]  [100/160]  eta: 0:00:57  lr: 0.000027  min_lr: 0.000001  loss: 1.6690 (1.7066)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2316 (8.7821)  time: 0.9209 (0.5138 -- 3.0278)  data: 0.0020 (0.0004 -- 0.0056)  max mem: 16413
[2023-08-31 14:58:08,186] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:58:08,186] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:58:08,190] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 14:58:08,191] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 14:58:13,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=84, lr=[6.355782109029989e-07, 6.355782109029989e-07, 8.474376145373319e-07, 8.474376145373319e-07, 1.129916819383109e-06, 1.129916819383109e-06, 1.5065557591774788e-06, 1.5065557591774788e-06, 2.008741012236638e-06, 2.008741012236638e-06, 2.678321349648851e-06, 2.678321349648851e-06, 3.571095132865135e-06, 3.571095132865135e-06, 4.761460177153514e-06, 4.761460177153514e-06, 6.348613569538018e-06, 6.348613569538018e-06, 8.464818092717356e-06, 8.464818092717356e-06, 1.1286424123623142e-05, 1.1286424123623142e-05, 1.504856549816419e-05, 1.504856549816419e-05, 2.0064753997552253e-05, 2.0064753997552253e-05, 2.675300533006967e-05, 2.675300533006967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 14:58:13,708] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=16.746605211439935, CurrSamplesPerSec=22.357558188604017, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [93]  [120/160]  eta: 0:00:37  lr: 0.000027  min_lr: 0.000001  loss: 1.7808 (1.7252)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7094 (8.6217)  time: 0.7431 (0.5223 -- 2.6873)  data: 0.0023 (0.0002 -- 0.0161)  max mem: 16413
[2023-08-31 14:58:27,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15012
[2023-08-31 14:58:27,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15012
[2023-08-31 14:58:27,082] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:58:27,082] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 14:58:27,082] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000027  min_lr: 0.000001  loss: 1.9416 (1.7367)  loss_scale: 32768.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0581 (8.6643)  time: 0.8880 (0.5107 -- 4.5786)  data: 0.0018 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000001  loss: 1.5427 (1.7240)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1933 (8.6654)  time: 0.6905 (0.4956 -- 2.5320)  data: 0.0009 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [93] Total time: 0:02:23 (0.8945 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000001  loss: 1.5427 (1.7530)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1933 (8.6654)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2679 (0.2679)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4094 (2.4094 -- 2.4094)  data: 2.1685 (2.1685 -- 2.1685)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4661 (0.6994)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4407 (0.1948 -- 2.4094)  data: 0.2194 (0.0004 -- 2.1685)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4661 (0.6108)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2269 (0.1690 -- 0.4518)  data: 0.0200 (0.0001 -- 0.2336)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5004 (0.6661)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.2656)  time: 0.2110 (0.1335 -- 0.4518)  data: 0.0197 (0.0001 -- 0.2336)  max mem: 16413
Val: Total time: 0:00:07 (0.2939 s / it)
* Acc@1 83.610 Acc@5 97.303 loss 0.646
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 84.44%
Epoch: [94]  [  0/160]  eta: 0:20:29  lr: 0.000027  min_lr: 0.000001  loss: 1.9630 (1.9630)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4955 (9.4955)  time: 7.6850 (7.6850 -- 7.6850)  data: 7.1242 (7.1242 -- 7.1242)  max mem: 16413
Epoch: [94]  [ 20/160]  eta: 0:02:50  lr: 0.000027  min_lr: 0.000001  loss: 1.4952 (1.6627)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8324 (8.0927)  time: 0.8930 (0.5237 -- 3.9951)  data: 0.1615 (0.0003 -- 2.2943)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:01:57  lr: 0.000027  min_lr: 0.000001  loss: 1.7616 (1.6819)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3783 (8.1061)  time: 0.7359 (0.5425 -- 1.9003)  data: 0.0755 (0.0003 -- 1.1679)  max mem: 16413
Epoch: [94]  [ 60/160]  eta: 0:01:36  lr: 0.000027  min_lr: 0.000001  loss: 1.7555 (1.7088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9831 (8.2599)  time: 0.9429 (0.5275 -- 2.7653)  data: 0.0932 (0.0003 -- 1.0365)  max mem: 16413
[2023-08-31 15:00:00,354] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15109
[2023-08-31 15:00:00,354] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15109
[2023-08-31 15:00:00,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:00:00,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:00:00,355] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [94]  [ 80/160]  eta: 0:01:16  lr: 0.000026  min_lr: 0.000001  loss: 1.6193 (1.6964)  loss_scale: 8192.0000 (15170.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3475 (8.3722)  time: 0.9097 (0.5251 -- 2.2009)  data: 0.3057 (0.0004 -- 1.6603)  max mem: 16413
Epoch: [94]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000001  loss: 1.8029 (1.7147)  loss_scale: 8192.0000 (13788.5149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5097 (8.4509)  time: 0.8141 (0.5270 -- 2.2236)  data: 0.2023 (0.0003 -- 1.6706)  max mem: 16413
Epoch: [94]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 1.7629 (1.7358)  loss_scale: 8192.0000 (12863.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8131 (8.4936)  time: 0.8262 (0.5278 -- 2.6054)  data: 0.1606 (0.0004 -- 1.6041)  max mem: 16413
Epoch: [94]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.9109 (1.7650)  loss_scale: 8192.0000 (12200.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5566 (8.5349)  time: 0.8750 (0.5260 -- 3.0089)  data: 0.1508 (0.0004 -- 2.0549)  max mem: 16413
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.7554 (1.7615)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7638 (8.5956)  time: 0.7188 (0.4966 -- 2.7527)  data: 0.0013 (0.0002 -- 0.0080)  max mem: 16413
Epoch: [94] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.7554 (1.7439)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7638 (8.5956)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2775 (0.2775)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1808 (2.1808 -- 2.1808)  data: 1.9353 (1.9353 -- 1.9353)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4070 (0.7034)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4408 (0.2028 -- 2.1808)  data: 0.2223 (0.0004 -- 1.9353)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4556 (0.6184)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2341 (0.1709 -- 0.7373)  data: 0.0319 (0.0001 -- 0.4951)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5107 (0.6705)  acc1: 85.7143 (81.7427)  acc5: 100.0000 (96.2656)  time: 0.2192 (0.1326 -- 0.7373)  data: 0.0313 (0.0001 -- 0.4951)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.651
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 84.44%
Epoch: [95]  [  0/160]  eta: 0:20:49  lr: 0.000026  min_lr: 0.000001  loss: 1.6688 (1.6688)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1033 (7.1033)  time: 7.8108 (7.8108 -- 7.8108)  data: 6.8041 (6.8041 -- 6.8041)  max mem: 16413
Epoch: [95]  [ 20/160]  eta: 0:02:52  lr: 0.000026  min_lr: 0.000001  loss: 1.8720 (1.8411)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6871 (7.9307)  time: 0.9049 (0.5181 -- 4.5864)  data: 0.0878 (0.0003 -- 1.7317)  max mem: 16413
[2023-08-31 15:02:02,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:02:02,863] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 15:02:02,867] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:02:02,867] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [95]  [ 40/160]  eta: 0:02:06  lr: 0.000026  min_lr: 0.000001  loss: 1.7133 (1.7899)  loss_scale: 8192.0000 (8791.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0705 (8.0577)  time: 0.8688 (0.5206 -- 4.8301)  data: 0.0011 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [95]  [ 60/160]  eta: 0:01:38  lr: 0.000026  min_lr: 0.000001  loss: 1.7927 (1.7846)  loss_scale: 16384.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0598 (8.5857)  time: 0.8308 (0.5344 -- 3.4617)  data: 0.0020 (0.0002 -- 0.0074)  max mem: 16413
Epoch: [95]  [ 80/160]  eta: 0:01:17  lr: 0.000026  min_lr: 0.000001  loss: 1.7738 (1.7809)  loss_scale: 16384.0000 (12540.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8965 (8.4792)  time: 0.9336 (0.5250 -- 3.4145)  data: 0.0019 (0.0003 -- 0.0130)  max mem: 16413
Epoch: [95]  [100/160]  eta: 0:00:56  lr: 0.000026  min_lr: 0.000001  loss: 1.7164 (1.7752)  loss_scale: 16384.0000 (13301.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9896 (8.4376)  time: 0.8533 (0.5187 -- 3.5690)  data: 0.0014 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [95]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 1.7094 (1.7621)  loss_scale: 16384.0000 (13811.3058)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5181 (8.6692)  time: 0.7893 (0.5240 -- 3.3868)  data: 0.0017 (0.0004 -- 0.0061)  max mem: 16413
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.6507 (1.7591)  loss_scale: 16384.0000 (14176.2270)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2029 (8.5965)  time: 0.9601 (0.5338 -- 3.1807)  data: 0.0014 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.7379 (1.7567)  loss_scale: 16384.0000 (14438.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3422 (8.6467)  time: 0.6661 (0.4950 -- 2.0338)  data: 0.0009 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [95] Total time: 0:02:22 (0.8913 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.7379 (1.7580)  loss_scale: 16384.0000 (14438.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3422 (8.6467)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2395 (0.2395)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3929 (2.3929 -- 2.3929)  data: 2.1891 (2.1891 -- 2.1891)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4827 (0.7097)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4274 (0.1977 -- 2.3929)  data: 0.2192 (0.0006 -- 2.1891)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4232 (0.6074)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2298 (0.1689 -- 0.5273)  data: 0.0281 (0.0001 -- 0.3362)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4959 (0.6666)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.2156 (0.1324 -- 0.5273)  data: 0.0278 (0.0001 -- 0.3362)  max mem: 16413
Val: Total time: 0:00:07 (0.2953 s / it)
* Acc@1 83.402 Acc@5 97.510 loss 0.646
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.44%
Epoch: [96]  [  0/160]  eta: 0:19:31  lr: 0.000026  min_lr: 0.000001  loss: 1.5512 (1.5512)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1563 (7.1563)  time: 7.3203 (7.3203 -- 7.3203)  data: 5.9295 (5.9295 -- 5.9295)  max mem: 16413
[2023-08-31 15:04:01,004] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15361
[2023-08-31 15:04:01,004] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:04:01,004] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15361
[2023-08-31 15:04:01,006] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:04:01,006] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [96]  [ 20/160]  eta: 0:02:45  lr: 0.000026  min_lr: 0.000001  loss: 1.6478 (1.8367)  loss_scale: 8192.0000 (8582.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4101 (8.7804)  time: 0.8770 (0.5133 -- 4.5799)  data: 0.2111 (0.0003 -- 2.6935)  max mem: 16413
Epoch: [96]  [ 40/160]  eta: 0:02:07  lr: 0.000026  min_lr: 0.000001  loss: 1.6431 (1.7250)  loss_scale: 8192.0000 (8391.8049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7628 (8.7176)  time: 0.9409 (0.5283 -- 3.3107)  data: 0.3650 (0.0003 -- 2.7808)  max mem: 16413
Epoch: [96]  [ 60/160]  eta: 0:01:38  lr: 0.000026  min_lr: 0.000001  loss: 1.6283 (1.7114)  loss_scale: 8192.0000 (8326.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6154 (8.7617)  time: 0.8118 (0.5245 -- 2.5849)  data: 0.0838 (0.0003 -- 1.6540)  max mem: 16413
Epoch: [96]  [ 80/160]  eta: 0:01:17  lr: 0.000026  min_lr: 0.000001  loss: 1.6610 (1.7181)  loss_scale: 8192.0000 (8293.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9836 (8.7355)  time: 0.9246 (0.5182 -- 4.0495)  data: 0.0183 (0.0004 -- 0.3422)  max mem: 16413
Epoch: [96]  [100/160]  eta: 0:00:55  lr: 0.000026  min_lr: 0.000001  loss: 1.5677 (1.7166)  loss_scale: 8192.0000 (8273.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2146 (8.6990)  time: 0.7698 (0.5169 -- 2.4390)  data: 0.0013 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000026  min_lr: 0.000001  loss: 1.7162 (1.6992)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0706 (8.7009)  time: 0.8693 (0.5182 -- 2.9264)  data: 0.0112 (0.0002 -- 0.1866)  max mem: 16413
[2023-08-31 15:05:54,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:05:54,600] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 15:05:54,601] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:05:54,601] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000026  min_lr: 0.000001  loss: 1.6971 (1.7076)  loss_scale: 16384.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7365 (8.8019)  time: 0.9410 (0.5199 -- 3.6262)  data: 0.0687 (0.0005 -- 1.3383)  max mem: 16413
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000001  loss: 1.8335 (1.7292)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1621 (8.8981)  time: 0.6568 (0.4950 -- 2.6128)  data: 0.0224 (0.0001 -- 0.4302)  max mem: 16413
Epoch: [96] Total time: 0:02:22 (0.8912 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000001  loss: 1.8335 (1.7363)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1621 (8.8981)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2170 (0.2170)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3921 (2.3921 -- 2.3921)  data: 2.1696 (2.1696 -- 2.1696)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3581 (0.6918)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4343 (0.2008 -- 2.3921)  data: 0.2217 (0.0004 -- 2.1696)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4119 (0.6025)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2247 (0.1702 -- 0.4691)  data: 0.0220 (0.0001 -- 0.2606)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4831 (0.6584)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.6805)  time: 0.2097 (0.1320 -- 0.4691)  data: 0.0218 (0.0001 -- 0.2606)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 83.195 Acc@5 97.510 loss 0.635
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 84.44%
Epoch: [97]  [  0/160]  eta: 0:20:26  lr: 0.000026  min_lr: 0.000001  loss: 2.0025 (2.0025)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.1604 (16.1604)  time: 7.6633 (7.6633 -- 7.6633)  data: 7.1452 (7.1452 -- 7.1452)  max mem: 16413
Epoch: [97]  [ 20/160]  eta: 0:02:42  lr: 0.000025  min_lr: 0.000001  loss: 1.6553 (1.7359)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7168 (9.0074)  time: 0.8350 (0.5235 -- 4.4667)  data: 0.2816 (0.0002 -- 3.9523)  max mem: 16413
[2023-08-31 15:06:56,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15550
[2023-08-31 15:06:56,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15550
[2023-08-31 15:06:56,097] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:06:56,097] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:06:56,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [97]  [ 40/160]  eta: 0:01:59  lr: 0.000025  min_lr: 0.000001  loss: 1.8871 (1.7903)  loss_scale: 8192.0000 (14186.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8049 (8.6672)  time: 0.8291 (0.4974 -- 2.0830)  data: 0.1792 (0.0004 -- 1.2166)  max mem: 16413
Epoch: [97]  [ 60/160]  eta: 0:01:36  lr: 0.000025  min_lr: 0.000001  loss: 1.7637 (1.7774)  loss_scale: 8192.0000 (12220.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7547 (8.7261)  time: 0.8869 (0.5193 -- 3.2878)  data: 0.1933 (0.0004 -- 2.5265)  max mem: 16413
Epoch: [97]  [ 80/160]  eta: 0:01:14  lr: 0.000025  min_lr: 0.000001  loss: 1.6971 (1.7586)  loss_scale: 8192.0000 (11226.0741)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1048 (8.9176)  time: 0.8404 (0.5330 -- 3.6272)  data: 0.2627 (0.0004 -- 2.6347)  max mem: 16413
Epoch: [97]  [100/160]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000001  loss: 1.6942 (1.7358)  loss_scale: 8192.0000 (10625.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1138 (8.8846)  time: 0.8713 (0.5310 -- 3.2445)  data: 0.3218 (0.0005 -- 2.6978)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:36  lr: 0.000025  min_lr: 0.000001  loss: 1.8360 (1.7423)  loss_scale: 8192.0000 (10223.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6510 (8.9093)  time: 0.8599 (0.5203 -- 3.9330)  data: 0.3029 (0.0001 -- 3.4138)  max mem: 16413
Epoch: [97]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 1.7119 (1.7537)  loss_scale: 8192.0000 (9934.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8058 (8.9512)  time: 0.9119 (0.5229 -- 4.4200)  data: 0.1374 (0.0005 -- 1.2674)  max mem: 16413
[2023-08-31 15:08:45,121] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:08:45,121] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 15:08:45,121] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.7070 (1.7474)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5832 (8.9933)  time: 0.6850 (0.4948 -- 2.7018)  data: 0.0012 (0.0002 -- 0.0056)  max mem: 16413[2023-08-31 15:08:45,162] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0

Epoch: [97] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.7070 (1.7443)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5832 (8.9933)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2213 (0.2213)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4637 (2.4637 -- 2.4637)  data: 2.2544 (2.2544 -- 2.2544)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3647 (0.6999)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4265 (0.2012 -- 2.4637)  data: 0.2131 (0.0008 -- 2.2544)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3973 (0.6039)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2188 (0.1697 -- 0.4132)  data: 0.0158 (0.0001 -- 0.2237)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4964 (0.6674)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.2656)  time: 0.2046 (0.1332 -- 0.4132)  data: 0.0156 (0.0001 -- 0.2237)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 82.988 Acc@5 97.303 loss 0.641
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.44%
Epoch: [98]  [  0/160]  eta: 0:18:49  lr: 0.000025  min_lr: 0.000001  loss: 1.0119 (1.0119)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8828 (5.8828)  time: 7.0564 (7.0564 -- 7.0564)  data: 6.4154 (6.4154 -- 6.4154)  max mem: 16413
Epoch: [98]  [ 20/160]  eta: 0:02:44  lr: 0.000025  min_lr: 0.000001  loss: 1.7297 (1.7005)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2670 (8.6033)  time: 0.8775 (0.5201 -- 4.1513)  data: 0.3291 (0.0008 -- 3.6257)  max mem: 16413
Epoch: [98]  [ 40/160]  eta: 0:02:07  lr: 0.000025  min_lr: 0.000001  loss: 1.7704 (1.7474)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9288 (9.0125)  time: 0.9500 (0.5060 -- 5.1036)  data: 0.2392 (0.0003 -- 2.4140)  max mem: 16413
Epoch: [98]  [ 60/160]  eta: 0:01:39  lr: 0.000025  min_lr: 0.000001  loss: 1.7729 (1.7238)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0684 (8.8700)  time: 0.8587 (0.5261 -- 4.8169)  data: 0.3093 (0.0007 -- 4.2862)  max mem: 16413
Epoch: [98]  [ 80/160]  eta: 0:01:19  lr: 0.000025  min_lr: 0.000001  loss: 1.7100 (1.7380)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7467 (8.8830)  time: 0.9791 (0.5203 -- 4.1208)  data: 0.4345 (0.0007 -- 3.6046)  max mem: 16413
Epoch: [98]  [100/160]  eta: 0:00:57  lr: 0.000025  min_lr: 0.000001  loss: 1.6155 (1.7100)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7768 (8.8417)  time: 0.7866 (0.5184 -- 3.6340)  data: 0.2443 (0.0001 -- 3.1048)  max mem: 16413
Epoch: [98]  [120/160]  eta: 0:00:37  lr: 0.000025  min_lr: 0.000001  loss: 1.8474 (1.7175)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1728 (8.8303)  time: 0.9055 (0.5329 -- 3.9684)  data: 0.3065 (0.0006 -- 3.4268)  max mem: 16413
[2023-08-31 15:10:51,140] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:10:51,141] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:10:51,141] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:10:51,141] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:10:55,391] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15813
[2023-08-31 15:10:55,391] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15813
[2023-08-31 15:10:55,391] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:10:55,391] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:10:55,392] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000025  min_lr: 0.000001  loss: 1.7914 (1.7264)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2685 (8.7533)  time: 0.8054 (0.5235 -- 4.6214)  data: 0.2543 (0.0005 -- 4.1049)  max mem: 16413
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000001  loss: 1.7313 (1.7312)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0919 (8.8222)  time: 0.6948 (0.4941 -- 2.7871)  data: 0.1787 (0.0002 -- 2.2676)  max mem: 16413
Epoch: [98] Total time: 0:02:23 (0.8981 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000001  loss: 1.7313 (1.7324)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0919 (8.8222)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2095 (0.2095)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6073 (2.6073 -- 2.6073)  data: 2.3506 (2.3506 -- 2.3506)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3895 (0.6874)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4328 (0.1996 -- 2.6073)  data: 0.2150 (0.0007 -- 2.3506)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3895 (0.5908)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2176 (0.1690 -- 0.4492)  data: 0.0146 (0.0001 -- 0.2747)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4948 (0.6600)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.2656)  time: 0.2007 (0.1323 -- 0.4492)  data: 0.0142 (0.0001 -- 0.2747)  max mem: 16413
Val: Total time: 0:00:07 (0.2943 s / it)
* Acc@1 83.817 Acc@5 97.303 loss 0.633
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.44%
Epoch: [99]  [  0/160]  eta: 0:18:34  lr: 0.000025  min_lr: 0.000001  loss: 2.0296 (2.0296)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5139 (7.5139)  time: 6.9663 (6.9663 -- 6.9663)  data: 5.1865 (5.1865 -- 5.1865)  max mem: 16413
Epoch: [99]  [ 20/160]  eta: 0:02:38  lr: 0.000025  min_lr: 0.000001  loss: 1.7269 (1.7143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1402 (9.4371)  time: 0.8406 (0.5284 -- 3.2671)  data: 0.0583 (0.0007 -- 0.7778)  max mem: 16413
Epoch: [99]  [ 40/160]  eta: 0:02:10  lr: 0.000025  min_lr: 0.000001  loss: 1.7552 (1.7545)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2683 (9.2431)  time: 1.0353 (0.5321 -- 3.6291)  data: 0.1088 (0.0004 -- 2.1113)  max mem: 16413
Epoch: [99]  [ 60/160]  eta: 0:01:36  lr: 0.000025  min_lr: 0.000001  loss: 1.8555 (1.7742)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7843 (9.1234)  time: 0.7327 (0.5214 -- 2.7675)  data: 0.0013 (0.0001 -- 0.0040)  max mem: 16413
Epoch: [99]  [ 80/160]  eta: 0:01:16  lr: 0.000025  min_lr: 0.000001  loss: 1.6904 (1.7225)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0800 (9.1975)  time: 0.9325 (0.5232 -- 3.7189)  data: 0.0018 (0.0004 -- 0.0129)  max mem: 16413
Epoch: [99]  [100/160]  eta: 0:00:55  lr: 0.000025  min_lr: 0.000001  loss: 1.7549 (1.7172)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0351 (9.2163)  time: 0.7981 (0.5151 -- 1.9765)  data: 0.1900 (0.0006 -- 1.4388)  max mem: 16413
[2023-08-31 15:12:59,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:12:59,521] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:12:59,521] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:12:59,522] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:13:12,796] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15956
[2023-08-31 15:13:12,796] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15956
[2023-08-31 15:13:12,796] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:13:12,796] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:13:12,796] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [99]  [120/160]  eta: 0:00:36  lr: 0.000024  min_lr: 0.000001  loss: 1.6780 (1.7103)  loss_scale: 32768.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9545 (9.0835)  time: 0.8343 (0.5253 -- 4.7311)  data: 0.2676 (0.0002 -- 4.1821)  max mem: 16413
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 1.6879 (1.7032)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0178 (8.9518)  time: 1.0165 (0.5038 -- 5.2106)  data: 0.0874 (0.0002 -- 1.7180)  max mem: 16413
[2023-08-31 15:13:46,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=90, lr=[5.798217439836669e-07, 5.798217439836669e-07, 7.730956586448891e-07, 7.730956586448891e-07, 1.0307942115265189e-06, 1.0307942115265189e-06, 1.3743922820353586e-06, 1.3743922820353586e-06, 1.8325230427138114e-06, 1.8325230427138114e-06, 2.4433640569517485e-06, 2.4433640569517485e-06, 3.2578187426023315e-06, 3.2578187426023315e-06, 4.343758323469775e-06, 4.343758323469775e-06, 5.791677764626367e-06, 5.791677764626367e-06, 7.722237019501822e-06, 7.722237019501822e-06, 1.029631602600243e-05, 1.029631602600243e-05, 1.372842136800324e-05, 1.372842136800324e-05, 1.830456182400432e-05, 1.830456182400432e-05, 2.440608243200576e-05, 2.440608243200576e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 15:13:46,822] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=16.771370947637912, CurrSamplesPerSec=24.72711551650615, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.7299 (1.6985)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1822 (8.8743)  time: 0.5950 (0.4970 -- 1.9697)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [99] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.7299 (1.7239)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1822 (8.8743)
[2023-08-31 15:13:46,828] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-08-31 15:13:46,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-08-31 15:13:46,833] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-08-31 15:13:46,833] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-08-31 15:13:47,880] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-08-31 15:13:47,880] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1990 (0.1990)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5235 (2.5235 -- 2.5235)  data: 2.2985 (2.2985 -- 2.2985)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4540 (0.6860)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4407 (0.2076 -- 2.5235)  data: 0.2169 (0.0007 -- 2.2985)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4106 (0.5888)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2153 (0.1698 -- 0.3195)  data: 0.0066 (0.0001 -- 0.0708)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4975 (0.6553)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (96.2656)  time: 0.1977 (0.1329 -- 0.3195)  data: 0.0060 (0.0001 -- 0.0708)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.639
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 84.44%
Epoch: [100]  [  0/160]  eta: 0:20:26  lr: 0.000024  min_lr: 0.000001  loss: 1.9976 (1.9976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0110 (7.0110)  time: 7.6652 (7.6652 -- 7.6652)  data: 5.7644 (5.7644 -- 5.7644)  max mem: 16413
Epoch: [100]  [ 20/160]  eta: 0:02:44  lr: 0.000024  min_lr: 0.000001  loss: 1.5794 (1.7156)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5471 (8.2431)  time: 0.8492 (0.5259 -- 4.0083)  data: 0.0020 (0.0002 -- 0.0073)  max mem: 16413
Epoch: [100]  [ 40/160]  eta: 0:02:14  lr: 0.000024  min_lr: 0.000001  loss: 1.5163 (1.6662)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9947 (8.2777)  time: 1.0714 (0.5175 -- 4.7241)  data: 0.0013 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [100]  [ 60/160]  eta: 0:01:41  lr: 0.000024  min_lr: 0.000001  loss: 1.7069 (1.7080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4752 (8.3878)  time: 0.7815 (0.5267 -- 3.6753)  data: 0.0015 (0.0004 -- 0.0051)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:20  lr: 0.000024  min_lr: 0.000001  loss: 1.8431 (1.7135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7119 (8.6495)  time: 0.9829 (0.5190 -- 4.7145)  data: 0.0014 (0.0003 -- 0.0067)  max mem: 16413
[2023-08-31 15:15:19,769] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:15:19,769] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:15:19,771] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:15:19,771] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [100]  [100/160]  eta: 0:00:56  lr: 0.000024  min_lr: 0.000001  loss: 1.7464 (1.7300)  loss_scale: 32768.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3423 (8.7076)  time: 0.6740 (0.5265 -- 2.5406)  data: 0.0020 (0.0004 -- 0.0162)  max mem: 16413
[2023-08-31 15:15:33,983] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16104
[2023-08-31 15:15:33,983] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16104
[2023-08-31 15:15:33,984] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:15:33,984] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:15:33,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [120/160]  eta: 0:00:38  lr: 0.000024  min_lr: 0.000001  loss: 1.7450 (1.7353)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8270 (8.7337)  time: 1.0066 (0.5302 -- 5.6192)  data: 0.0022 (0.0005 -- 0.0123)  max mem: 16413
Epoch: [100]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 1.8349 (1.7405)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7481 (8.6189)  time: 0.8168 (0.5127 -- 4.0567)  data: 0.0013 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.8455 (1.7554)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3272 (8.5712)  time: 0.6492 (0.4947 -- 2.5590)  data: 0.0006 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [100] Total time: 0:02:23 (0.8990 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.8455 (1.7631)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3272 (8.5712)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2447 (0.2447)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4720 (2.4720 -- 2.4720)  data: 2.2500 (2.2500 -- 2.2500)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4385 (0.6760)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4218 (0.1946 -- 2.4720)  data: 0.2055 (0.0006 -- 2.2500)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4385 (0.5895)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (96.8254)  time: 0.2192 (0.1689 -- 0.4292)  data: 0.0127 (0.0001 -- 0.2400)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5211 (0.6445)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (95.8506)  time: 0.2030 (0.1324 -- 0.4292)  data: 0.0124 (0.0001 -- 0.2400)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 83.817 Acc@5 97.303 loss 0.636
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.44%
Epoch: [101]  [  0/160]  eta: 0:18:56  lr: 0.000024  min_lr: 0.000001  loss: 1.5607 (1.5607)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9343 (10.9343)  time: 7.1020 (7.1020 -- 7.1020)  data: 6.5638 (6.5638 -- 6.5638)  max mem: 16413
[2023-08-31 15:16:52,865] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16179
[2023-08-31 15:16:52,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:16:52,866] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16179
[2023-08-31 15:16:52,866] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:16:52,867] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [101]  [ 20/160]  eta: 0:02:53  lr: 0.000024  min_lr: 0.000001  loss: 1.8698 (1.7369)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8244 (8.4261)  time: 0.9461 (0.5164 -- 3.6167)  data: 0.1440 (0.0004 -- 1.6099)  max mem: 16413
Epoch: [101]  [ 40/160]  eta: 0:02:06  lr: 0.000024  min_lr: 0.000001  loss: 1.7523 (1.7157)  loss_scale: 8192.0000 (11988.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6079 (8.7324)  time: 0.8561 (0.5101 -- 4.7446)  data: 0.0020 (0.0001 -- 0.0142)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:42  lr: 0.000024  min_lr: 0.000001  loss: 1.5956 (1.6942)  loss_scale: 8192.0000 (10743.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7195 (8.4906)  time: 0.9697 (0.5103 -- 4.6268)  data: 0.0012 (0.0003 -- 0.0036)  max mem: 16413
[2023-08-31 15:17:36,931] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16226
[2023-08-31 15:17:36,931] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16226
[2023-08-31 15:17:36,932] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 15:17:36,932] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 15:17:36,932] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [101]  [ 80/160]  eta: 0:01:18  lr: 0.000024  min_lr: 0.000001  loss: 1.6513 (1.6861)  loss_scale: 4096.0000 (9355.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4786 (8.5569)  time: 0.8669 (0.5135 -- 4.2852)  data: 0.0011 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [101]  [100/160]  eta: 0:00:57  lr: 0.000024  min_lr: 0.000001  loss: 1.7426 (1.7042)  loss_scale: 4096.0000 (8313.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3352 (8.6841)  time: 0.8530 (0.5291 -- 3.3569)  data: 0.0019 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [101]  [120/160]  eta: 0:00:37  lr: 0.000024  min_lr: 0.000001  loss: 1.4558 (1.6916)  loss_scale: 4096.0000 (7616.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0259 (8.8641)  time: 0.8519 (0.5170 -- 4.3727)  data: 0.0016 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000024  min_lr: 0.000001  loss: 1.7392 (1.6954)  loss_scale: 4096.0000 (7117.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3882 (9.0178)  time: 0.8823 (0.5198 -- 3.7338)  data: 0.0015 (0.0004 -- 0.0080)  max mem: 16413
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000001  loss: 1.6530 (1.6910)  loss_scale: 4096.0000 (6758.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5444 (8.9834)  time: 0.6301 (0.4963 -- 2.5462)  data: 0.0006 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [101] Total time: 0:02:23 (0.8983 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000001  loss: 1.6530 (1.7374)  loss_scale: 4096.0000 (6758.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5444 (8.9834)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2053 (0.2053)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4753 (2.4753 -- 2.4753)  data: 2.2530 (2.2530 -- 2.2530)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3699 (0.6820)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4388 (0.2000 -- 2.4753)  data: 0.2248 (0.0004 -- 2.2530)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4095 (0.5954)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2227 (0.1688 -- 0.4474)  data: 0.0216 (0.0001 -- 0.2124)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4931 (0.6479)  acc1: 77.7778 (81.3278)  acc5: 100.0000 (96.6805)  time: 0.2087 (0.1328 -- 0.4474)  data: 0.0213 (0.0001 -- 0.2124)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.624
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.44%
Epoch: [102]  [  0/160]  eta: 0:21:36  lr: 0.000024  min_lr: 0.000001  loss: 1.8871 (1.8871)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5936 (9.5936)  time: 8.1056 (8.1056 -- 8.1056)  data: 7.5625 (7.5625 -- 7.5625)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:46  lr: 0.000024  min_lr: 0.000001  loss: 1.5866 (1.7230)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9698 (9.8982)  time: 0.8462 (0.5290 -- 4.0257)  data: 0.2978 (0.0007 -- 3.4838)  max mem: 16413
[2023-08-31 15:19:36,649] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:19:36,650] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-31 15:19:36,650] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:19:36,650] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [102]  [ 40/160]  eta: 0:02:01  lr: 0.000024  min_lr: 0.000001  loss: 1.7352 (1.7326)  loss_scale: 4096.0000 (4695.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7729 (9.0622)  time: 0.8247 (0.5230 -- 2.7438)  data: 0.2757 (0.0004 -- 2.2204)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:37  lr: 0.000024  min_lr: 0.000001  loss: 1.6748 (1.7187)  loss_scale: 8192.0000 (5841.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5659 (8.9302)  time: 0.9066 (0.5332 -- 2.6449)  data: 0.1653 (0.0006 -- 1.8558)  max mem: 16413
Epoch: [102]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 1.7313 (1.7305)  loss_scale: 8192.0000 (6422.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4083 (8.7777)  time: 0.8272 (0.5276 -- 3.2305)  data: 0.1678 (0.0004 -- 2.7121)  max mem: 16413
Epoch: [102]  [100/160]  eta: 0:00:57  lr: 0.000023  min_lr: 0.000001  loss: 1.6576 (1.7341)  loss_scale: 8192.0000 (6772.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5821 (8.7725)  time: 0.9984 (0.5430 -- 2.8143)  data: 0.2713 (0.0006 -- 2.2788)  max mem: 16413
Epoch: [102]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.7946 (1.7474)  loss_scale: 8192.0000 (7007.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9675 (8.8068)  time: 0.8126 (0.5210 -- 3.5329)  data: 0.0998 (0.0003 -- 1.3630)  max mem: 16413
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.7815 (1.7409)  loss_scale: 8192.0000 (7175.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4481 (8.6920)  time: 0.8798 (0.5206 -- 2.3759)  data: 0.1759 (0.0007 -- 1.8387)  max mem: 16413
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7324 (1.7390)  loss_scale: 8192.0000 (7296.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1977 (8.7037)  time: 0.6815 (0.4939 -- 2.4494)  data: 0.0645 (0.0002 -- 1.2785)  max mem: 16413
Epoch: [102] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7324 (1.7346)  loss_scale: 8192.0000 (7296.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1977 (8.7037)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2194 (0.2194)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4336 (2.4336 -- 2.4336)  data: 2.2102 (2.2102 -- 2.2102)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4695 (0.6879)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4400 (0.1957 -- 2.4336)  data: 0.2331 (0.0005 -- 2.2102)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4582 (0.6036)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2271 (0.1695 -- 0.5517)  data: 0.0278 (0.0001 -- 0.3470)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5263 (0.6550)  acc1: 88.8889 (80.9129)  acc5: 100.0000 (97.0954)  time: 0.2148 (0.1336 -- 0.5517)  data: 0.0276 (0.0001 -- 0.3470)  max mem: 16413
Val: Total time: 0:00:07 (0.2949 s / it)
* Acc@1 83.195 Acc@5 97.510 loss 0.637
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 84.44%
Epoch: [103]  [  0/160]  eta: 0:18:48  lr: 0.000023  min_lr: 0.000001  loss: 1.5573 (1.5573)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.2463 (14.2463)  time: 7.0549 (7.0549 -- 7.0549)  data: 5.8417 (5.8417 -- 5.8417)  max mem: 16413
[2023-08-31 15:21:39,234] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:21:39,235] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 15:21:39,236] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:21:39,237] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [103]  [ 20/160]  eta: 0:02:40  lr: 0.000023  min_lr: 0.000001  loss: 1.8533 (1.7910)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3416 (8.8457)  time: 0.8494 (0.5349 -- 2.9762)  data: 0.2072 (0.0002 -- 2.4355)  max mem: 16413
Epoch: [103]  [ 40/160]  eta: 0:02:01  lr: 0.000023  min_lr: 0.000001  loss: 1.9213 (1.7974)  loss_scale: 16384.0000 (15784.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2152 (9.0302)  time: 0.8659 (0.5275 -- 2.6273)  data: 0.1515 (0.0003 -- 2.1040)  max mem: 16413
Epoch: [103]  [ 60/160]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000001  loss: 1.7672 (1.7782)  loss_scale: 16384.0000 (15981.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0590 (8.8052)  time: 0.9679 (0.5224 -- 3.1014)  data: 0.0834 (0.0002 -- 1.1961)  max mem: 16413
Epoch: [103]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000001  loss: 1.5737 (1.7439)  loss_scale: 16384.0000 (16080.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4538 (8.7504)  time: 0.8671 (0.5139 -- 3.4180)  data: 0.0016 (0.0006 -- 0.0045)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 1.8175 (1.7558)  loss_scale: 16384.0000 (16140.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3500 (8.6452)  time: 0.8789 (0.5347 -- 3.4797)  data: 0.0015 (0.0004 -- 0.0040)  max mem: 16413
Epoch: [103]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.8318 (1.7530)  loss_scale: 16384.0000 (16180.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8238 (8.7396)  time: 0.7650 (0.5244 -- 2.4962)  data: 0.0023 (0.0003 -- 0.0146)  max mem: 16413
[2023-08-31 15:23:28,819] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:23:28,819] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:23:28,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:23:28,820] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [103]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.7875 (1.7508)  loss_scale: 16384.0000 (17371.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4515 (8.7275)  time: 0.9173 (0.5328 -- 3.7904)  data: 0.0017 (0.0004 -- 0.0043)  max mem: 16413
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.7477 (1.7482)  loss_scale: 32768.0000 (19200.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4380 (8.7101)  time: 0.8097 (0.4947 -- 3.5283)  data: 0.0162 (0.0002 -- 0.3120)  max mem: 16413
Epoch: [103] Total time: 0:02:22 (0.8917 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.7477 (1.7211)  loss_scale: 32768.0000 (19200.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4380 (8.7101)
Val:  [ 0/27]  eta: 0:01:13  loss: 0.2043 (0.2043)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7374 (2.7374 -- 2.7374)  data: 2.4833 (2.4833 -- 2.4833)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3633 (0.6863)  acc1: 77.7778 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4452 (0.2002 -- 2.7374)  data: 0.2297 (0.0004 -- 2.4833)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3719 (0.5921)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2081 (0.1693 -- 0.2850)  data: 0.0072 (0.0001 -- 0.0986)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4876 (0.6529)  acc1: 88.8889 (81.3278)  acc5: 100.0000 (97.0954)  time: 0.1934 (0.1327 -- 0.2850)  data: 0.0069 (0.0001 -- 0.0986)  max mem: 16413
Val: Total time: 0:00:07 (0.2919 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.627
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 84.44%
Epoch: [104]  [  0/160]  eta: 0:21:33  lr: 0.000023  min_lr: 0.000001  loss: 2.0281 (2.0281)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9717 (9.9717)  time: 8.0816 (8.0816 -- 8.0816)  data: 6.2307 (6.2307 -- 6.2307)  max mem: 16413
[2023-08-31 15:24:18,711] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16650
[2023-08-31 15:24:18,711] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16650
[2023-08-31 15:24:18,711] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:24:18,711] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:24:18,712] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [104]  [ 20/160]  eta: 0:02:58  lr: 0.000023  min_lr: 0.000001  loss: 1.6720 (1.7009)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6577 (8.4582)  time: 0.9371 (0.5246 -- 4.9788)  data: 0.0245 (0.0003 -- 0.4623)  max mem: 16413
Epoch: [104]  [ 40/160]  eta: 0:02:15  lr: 0.000023  min_lr: 0.000001  loss: 1.6824 (1.6983)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2726 (8.9953)  time: 0.9729 (0.5290 -- 3.9858)  data: 0.0016 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [104]  [ 60/160]  eta: 0:01:42  lr: 0.000023  min_lr: 0.000001  loss: 1.7553 (1.7246)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5515 (9.1519)  time: 0.8252 (0.5234 -- 3.4068)  data: 0.0011 (0.0003 -- 0.0022)  max mem: 16413
Epoch: [104]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000001  loss: 1.8073 (1.7037)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7994 (8.7023)  time: 0.7912 (0.5222 -- 3.5752)  data: 0.0016 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 1.6264 (1.6977)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9940 (8.6231)  time: 0.8542 (0.5121 -- 3.7794)  data: 0.0024 (0.0003 -- 0.0142)  max mem: 16413
Epoch: [104]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9892 (1.7298)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5552 (8.6825)  time: 0.8111 (0.5280 -- 2.8991)  data: 0.0020 (0.0007 -- 0.0069)  max mem: 16413
[2023-08-31 15:26:10,001] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:26:10,001] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:26:10,002] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:26:10,003] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [104]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.6839 (1.7322)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6984 (8.6150)  time: 0.8947 (0.5426 -- 3.7965)  data: 0.0017 (0.0004 -- 0.0062)  max mem: 16413
[2023-08-31 15:26:23,939] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16798
[2023-08-31 15:26:23,939] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16798
[2023-08-31 15:26:23,939] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:26:23,939] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:26:23,939] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.6926 (1.7259)  loss_scale: 32768.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5726 (8.7657)  time: 0.7212 (0.4791 -- 3.9227)  data: 0.0007 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [104] Total time: 0:02:23 (0.8982 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.6926 (1.7249)  loss_scale: 32768.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5726 (8.7657)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2124 (0.2124)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5644 (2.5644 -- 2.5644)  data: 2.3415 (2.3415 -- 2.3415)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3652 (0.6830)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4284 (0.1967 -- 2.5644)  data: 0.2169 (0.0006 -- 2.3415)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4129 (0.5799)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2181 (0.1690 -- 0.4729)  data: 0.0166 (0.0001 -- 0.2857)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4854 (0.6406)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (96.6805)  time: 0.2023 (0.1337 -- 0.4729)  data: 0.0163 (0.0001 -- 0.2857)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 84.232 Acc@5 97.303 loss 0.620
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 84.44%
Epoch: [105]  [  0/160]  eta: 0:24:06  lr: 0.000023  min_lr: 0.000001  loss: 1.5248 (1.5248)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7343 (9.7343)  time: 9.0423 (9.0423 -- 9.0423)  data: 8.5167 (8.5167 -- 8.5167)  max mem: 16413
Epoch: [105]  [ 20/160]  eta: 0:02:53  lr: 0.000022  min_lr: 0.000001  loss: 1.6251 (1.5289)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3025 (8.9312)  time: 0.8502 (0.5282 -- 3.8962)  data: 0.3008 (0.0003 -- 3.3088)  max mem: 16413
[2023-08-31 15:27:02,660] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16824
[2023-08-31 15:27:02,660] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16824
[2023-08-31 15:27:02,660] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:27:02,660] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:27:02,661] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [105]  [ 40/160]  eta: 0:02:07  lr: 0.000022  min_lr: 0.000001  loss: 1.7932 (1.6779)  loss_scale: 8192.0000 (12987.3171)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7456 (8.9058)  time: 0.8822 (0.5255 -- 3.7315)  data: 0.0747 (0.0003 -- 1.4681)  max mem: 16413
Epoch: [105]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 1.7185 (1.6918)  loss_scale: 8192.0000 (11415.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9286 (8.9660)  time: 0.8042 (0.5345 -- 3.5646)  data: 0.0147 (0.0009 -- 0.2471)  max mem: 16413
Epoch: [105]  [ 80/160]  eta: 0:01:16  lr: 0.000022  min_lr: 0.000001  loss: 1.9696 (1.7300)  loss_scale: 8192.0000 (10619.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7523 (9.1209)  time: 0.8868 (0.5288 -- 3.6607)  data: 0.0015 (0.0002 -- 0.0085)  max mem: 16413
Epoch: [105]  [100/160]  eta: 0:00:56  lr: 0.000022  min_lr: 0.000001  loss: 1.6429 (1.7066)  loss_scale: 8192.0000 (10138.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1892 (9.0433)  time: 0.8735 (0.5212 -- 3.1627)  data: 0.0014 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [105]  [120/160]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 1.5937 (1.7033)  loss_scale: 8192.0000 (9816.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3058 (8.9109)  time: 0.9251 (0.5113 -- 3.5689)  data: 0.0015 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [105]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.6157 (1.7026)  loss_scale: 8192.0000 (9586.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1934 (8.8817)  time: 0.7983 (0.5337 -- 3.0810)  data: 0.0013 (0.0003 -- 0.0018)  max mem: 16413
[2023-08-31 15:28:51,503] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:28:51,503] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 15:28:51,503] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:28:51,503] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8585 (1.7047)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2809 (8.8952)  time: 0.6619 (0.4967 -- 2.9177)  data: 0.0007 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [105] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.8585 (1.7113)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2809 (8.8952)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1726 (0.1726)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4899 (2.4899 -- 2.4899)  data: 2.2849 (2.2849 -- 2.2849)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3881 (0.7111)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4436 (0.1853 -- 2.4899)  data: 0.2287 (0.0004 -- 2.2849)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4224 (0.6012)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2188 (0.1685 -- 0.4712)  data: 0.0124 (0.0001 -- 0.2192)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5068 (0.6574)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.2017 (0.1329 -- 0.4712)  data: 0.0119 (0.0001 -- 0.2192)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 84.025 Acc@5 97.718 loss 0.629
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 84.44%
Epoch: [106]  [  0/160]  eta: 0:15:47  lr: 0.000022  min_lr: 0.000001  loss: 1.1699 (1.1699)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.6363 (13.6363)  time: 5.9190 (5.9190 -- 5.9190)  data: 4.6394 (4.6394 -- 4.6394)  max mem: 16413
Epoch: [106]  [ 20/160]  eta: 0:02:43  lr: 0.000022  min_lr: 0.000001  loss: 1.8066 (1.7167)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9865 (8.6313)  time: 0.9305 (0.5280 -- 3.3884)  data: 0.0025 (0.0005 -- 0.0122)  max mem: 16413
[2023-08-31 15:29:42,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=96, lr=[5.238378100529603e-07, 5.238378100529603e-07, 6.984504134039471e-07, 6.984504134039471e-07, 9.312672178719295e-07, 9.312672178719295e-07, 1.2416896238292394e-06, 1.2416896238292394e-06, 1.6555861651056525e-06, 1.6555861651056525e-06, 2.20744822014087e-06, 2.20744822014087e-06, 2.94326429352116e-06, 2.94326429352116e-06, 3.9243523913615465e-06, 3.9243523913615465e-06, 5.232469855148729e-06, 5.232469855148729e-06, 6.9766264735316386e-06, 6.9766264735316386e-06, 9.302168631375518e-06, 9.302168631375518e-06, 1.240289150850069e-05, 1.240289150850069e-05, 1.653718867800092e-05, 1.653718867800092e-05, 2.2049584904001228e-05, 2.2049584904001228e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 15:29:42,004] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=16.670472024777382, CurrSamplesPerSec=21.936726840687395, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:01:58  lr: 0.000022  min_lr: 0.000001  loss: 1.8699 (1.7880)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3624 (8.8318)  time: 0.7974 (0.5146 -- 4.3915)  data: 0.0015 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [106]  [ 60/160]  eta: 0:01:39  lr: 0.000022  min_lr: 0.000001  loss: 1.7489 (1.7639)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5202 (8.8125)  time: 1.0028 (0.5251 -- 3.8621)  data: 0.0016 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 1.5639 (1.7301)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3226 (8.8017)  time: 0.7361 (0.5245 -- 2.8808)  data: 0.0139 (0.0005 -- 0.2455)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.7449 (1.7099)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6143 (8.6751)  time: 0.9044 (0.5298 -- 4.0555)  data: 0.0014 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [106]  [120/160]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 1.5811 (1.6940)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7623 (8.5866)  time: 0.9563 (0.5317 -- 4.3701)  data: 0.0070 (0.0006 -- 0.0621)  max mem: 16413
[2023-08-31 15:30:55,434] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:30:55,435] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:30:55,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:30:55,438] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:30:57,121] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17084
[2023-08-31 15:30:57,122] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:30:57,122] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17084
[2023-08-31 15:30:57,122] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:30:57,122] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.8172 (1.7110)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0842 (8.5195)  time: 0.8946 (0.5391 -- 3.9637)  data: 0.1737 (0.0005 -- 3.4450)  max mem: 16413
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.7875 (1.7217)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1437 (8.4980)  time: 0.6106 (0.4970 -- 1.5399)  data: 0.0824 (0.0002 -- 1.0163)  max mem: 16413
Epoch: [106] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.7875 (1.7372)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1437 (8.4980)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2042 (0.2042)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3912 (2.3912 -- 2.3912)  data: 2.1388 (2.1388 -- 2.1388)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4382 (0.7132)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4268 (0.2066 -- 2.3912)  data: 0.2021 (0.0005 -- 2.1388)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4382 (0.6155)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2225 (0.1694 -- 0.3846)  data: 0.0140 (0.0001 -- 0.1921)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5083 (0.6748)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.2022 (0.1325 -- 0.3846)  data: 0.0136 (0.0001 -- 0.1921)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 83.817 Acc@5 97.510 loss 0.640
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.44%
Epoch: [107]  [  0/160]  eta: 0:20:24  lr: 0.000022  min_lr: 0.000001  loss: 1.4224 (1.4224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2911 (6.2911)  time: 7.6524 (7.6524 -- 7.6524)  data: 7.1329 (7.1329 -- 7.1329)  max mem: 16413
Epoch: [107]  [ 20/160]  eta: 0:02:43  lr: 0.000022  min_lr: 0.000001  loss: 1.6808 (1.7174)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8996 (10.0653)  time: 0.8471 (0.5296 -- 4.3587)  data: 0.2707 (0.0007 -- 3.8392)  max mem: 16413
Epoch: [107]  [ 40/160]  eta: 0:02:10  lr: 0.000022  min_lr: 0.000001  loss: 1.6970 (1.7696)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3730 (9.8754)  time: 1.0077 (0.5267 -- 4.9008)  data: 0.4647 (0.0003 -- 4.3824)  max mem: 16413
Epoch: [107]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 1.7177 (1.7748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1294 (9.6373)  time: 0.7483 (0.5373 -- 3.1509)  data: 0.1939 (0.0008 -- 2.6105)  max mem: 16413
Epoch: [107]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 1.6759 (1.7429)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0996 (9.3564)  time: 0.8385 (0.5236 -- 2.8910)  data: 0.2416 (0.0003 -- 2.3502)  max mem: 16413
[2023-08-31 15:33:01,301] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:33:01,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:33:01,305] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:33:01,306] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [107]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.6753 (1.7628)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2460 (9.0549)  time: 0.8498 (0.5221 -- 3.0055)  data: 0.2512 (0.0002 -- 2.4745)  max mem: 16413
Epoch: [107]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.5821 (1.7425)  loss_scale: 32768.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6055 (8.9256)  time: 0.8814 (0.5283 -- 2.6478)  data: 0.0768 (0.0004 -- 0.9679)  max mem: 16413
[2023-08-31 15:33:25,694] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17244
[2023-08-31 15:33:25,694] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17244
[2023-08-31 15:33:25,695] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:33:25,695] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:33:25,695] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [107]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.8182 (1.7500)  loss_scale: 16384.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0615 (8.9174)  time: 0.8796 (0.5270 -- 2.9695)  data: 0.1598 (0.0004 -- 2.4085)  max mem: 16413
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.6888 (1.7334)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6006 (9.0126)  time: 0.6793 (0.4946 -- 1.8429)  data: 0.1303 (0.0002 -- 1.3194)  max mem: 16413
Epoch: [107] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.6888 (1.7148)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6006 (9.0126)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2008 (0.2008)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3293 (2.3293 -- 2.3293)  data: 2.1074 (2.1074 -- 2.1074)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3325 (0.6808)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4420 (0.1925 -- 2.3293)  data: 0.2242 (0.0007 -- 2.1074)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4534 (0.6013)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2277 (0.1691 -- 0.5945)  data: 0.0203 (0.0001 -- 0.3495)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5140 (0.6577)  acc1: 88.8889 (81.7427)  acc5: 100.0000 (96.6805)  time: 0.2130 (0.1321 -- 0.5945)  data: 0.0200 (0.0001 -- 0.3495)  max mem: 16413
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 83.402 Acc@5 97.303 loss 0.630
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 84.44%
Epoch: [108]  [  0/160]  eta: 0:19:18  lr: 0.000021  min_lr: 0.000001  loss: 1.8004 (1.8004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9080 (7.9080)  time: 7.2431 (7.2431 -- 7.2431)  data: 5.4923 (5.4923 -- 5.4923)  max mem: 16413
Epoch: [108]  [ 20/160]  eta: 0:02:45  lr: 0.000021  min_lr: 0.000001  loss: 1.8247 (1.7630)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1607 (10.7900)  time: 0.8757 (0.5274 -- 3.3790)  data: 0.1687 (0.0004 -- 2.8309)  max mem: 16413
Epoch: [108]  [ 40/160]  eta: 0:02:05  lr: 0.000021  min_lr: 0.000001  loss: 1.8417 (1.7893)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5164 (9.8761)  time: 0.8982 (0.5241 -- 2.8111)  data: 0.1318 (0.0002 -- 1.6535)  max mem: 16413
Epoch: [108]  [ 60/160]  eta: 0:01:38  lr: 0.000021  min_lr: 0.000001  loss: 1.7889 (1.7834)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5680 (9.4404)  time: 0.8572 (0.5403 -- 3.0192)  data: 0.0020 (0.0002 -- 0.0153)  max mem: 16413
Epoch: [108]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000001  loss: 1.6460 (1.7681)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8512 (9.2484)  time: 0.8444 (0.5187 -- 2.9220)  data: 0.0014 (0.0003 -- 0.0042)  max mem: 16413
[2023-08-31 15:35:31,159] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:35:31,159] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:35:31,160] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:35:31,160] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:35:31,707] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17374
[2023-08-31 15:35:31,707] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:35:31,708] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17374
[2023-08-31 15:35:31,708] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:35:31,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [108]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000001  loss: 1.5215 (1.7433)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0671 (8.9520)  time: 0.9315 (0.5169 -- 3.8906)  data: 0.0019 (0.0005 -- 0.0061)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000001  loss: 1.8466 (1.7538)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9902 (8.9097)  time: 0.8863 (0.5287 -- 2.2966)  data: 0.2510 (0.0004 -- 1.7623)  max mem: 16413
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.8542 (1.7778)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7582 (9.0351)  time: 0.7907 (0.5180 -- 3.2586)  data: 0.2413 (0.0009 -- 2.7200)  max mem: 16413
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.6380 (1.7689)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9629 (9.0339)  time: 0.6490 (0.4962 -- 1.9861)  data: 0.1237 (0.0001 -- 1.4771)  max mem: 16413
Epoch: [108] Total time: 0:02:21 (0.8838 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.6380 (1.7413)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9629 (9.0339)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2002 (0.2002)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4767 (2.4767 -- 2.4767)  data: 2.2572 (2.2572 -- 2.2572)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4003 (0.6707)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4201 (0.2028 -- 2.4767)  data: 0.2067 (0.0007 -- 2.2572)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4635 (0.6005)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2225 (0.1687 -- 0.5034)  data: 0.0170 (0.0001 -- 0.3193)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5274 (0.6514)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2081 (0.1333 -- 0.5034)  data: 0.0164 (0.0001 -- 0.3193)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 83.817 Acc@5 97.510 loss 0.623
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.44%
Epoch: [109]  [  0/160]  eta: 0:18:34  lr: 0.000021  min_lr: 0.000000  loss: 2.1455 (2.1455)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0763 (8.0763)  time: 6.9667 (6.9667 -- 6.9667)  data: 6.3850 (6.3850 -- 6.3850)  max mem: 16413
Epoch: [109]  [ 20/160]  eta: 0:02:41  lr: 0.000021  min_lr: 0.000000  loss: 1.7067 (1.7560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0090 (9.8791)  time: 0.8652 (0.5286 -- 2.4861)  data: 0.0021 (0.0004 -- 0.0163)  max mem: 16413
Epoch: [109]  [ 40/160]  eta: 0:02:03  lr: 0.000021  min_lr: 0.000000  loss: 1.9562 (1.7936)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5448 (9.0049)  time: 0.8946 (0.5391 -- 2.8364)  data: 0.2723 (0.0003 -- 2.3035)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000000  loss: 1.8133 (1.7850)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0590 (8.7124)  time: 0.8511 (0.5278 -- 3.5459)  data: 0.2512 (0.0002 -- 2.2639)  max mem: 16413
[2023-08-31 15:37:32,129] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:37:32,129] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:37:32,130] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:37:32,130] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [ 80/160]  eta: 0:01:17  lr: 0.000021  min_lr: 0.000000  loss: 1.8402 (1.7914)  loss_scale: 32768.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0278 (8.7513)  time: 0.9433 (0.5264 -- 3.8445)  data: 0.3907 (0.0004 -- 3.2894)  max mem: 16413
[2023-08-31 15:37:52,040] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17525
[2023-08-31 15:37:52,040] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17525
[2023-08-31 15:37:52,040] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:37:52,040] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:37:52,040] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [109]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 1.7991 (1.7795)  loss_scale: 16384.0000 (19952.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0636 (8.7390)  time: 0.8729 (0.5087 -- 3.2107)  data: 0.3321 (0.0005 -- 2.6673)  max mem: 16413
Epoch: [109]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 1.7989 (1.7747)  loss_scale: 16384.0000 (19362.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5415 (8.6851)  time: 0.8750 (0.5307 -- 3.3669)  data: 0.3208 (0.0004 -- 2.8355)  max mem: 16413
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.8508 (1.7809)  loss_scale: 16384.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0850 (8.6346)  time: 0.7960 (0.5317 -- 2.6531)  data: 0.2541 (0.0003 -- 2.1209)  max mem: 16413
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.6703 (1.7758)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5473 (8.6163)  time: 0.7634 (0.4954 -- 2.6430)  data: 0.2450 (0.0002 -- 2.1321)  max mem: 16413
Epoch: [109] Total time: 0:02:23 (0.8980 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.6703 (1.7290)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5473 (8.6163)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1856 (0.1856)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3415 (2.3415 -- 2.3415)  data: 2.1114 (2.1114 -- 2.1114)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3637 (0.6856)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4150 (0.2094 -- 2.3415)  data: 0.1956 (0.0008 -- 2.1114)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3917 (0.5949)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2214 (0.1696 -- 0.4298)  data: 0.0133 (0.0001 -- 0.2061)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5177 (0.6478)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.2027 (0.1331 -- 0.4298)  data: 0.0128 (0.0001 -- 0.2061)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 83.817 Acc@5 97.925 loss 0.619
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 84.44%
Epoch: [110]  [  0/160]  eta: 0:23:25  lr: 0.000021  min_lr: 0.000000  loss: 2.0131 (2.0131)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8006 (7.8006)  time: 8.7870 (8.7870 -- 8.7870)  data: 3.7586 (3.7586 -- 3.7586)  max mem: 16413
Epoch: [110]  [ 20/160]  eta: 0:02:58  lr: 0.000021  min_lr: 0.000000  loss: 1.7768 (1.8079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4336 (9.2029)  time: 0.8962 (0.5261 -- 3.9487)  data: 0.0021 (0.0003 -- 0.0133)  max mem: 16413
Epoch: [110]  [ 40/160]  eta: 0:02:11  lr: 0.000021  min_lr: 0.000000  loss: 1.6816 (1.8078)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3748 (8.9068)  time: 0.9132 (0.5295 -- 3.3674)  data: 0.0014 (0.0003 -- 0.0031)  max mem: 16413
[2023-08-31 15:39:58,532] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:39:58,532] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:39:58,573] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:39:58,573] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [110]  [ 60/160]  eta: 0:01:40  lr: 0.000020  min_lr: 0.000000  loss: 1.7026 (1.7923)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3134 (8.5136)  time: 0.8256 (0.5234 -- 3.2998)  data: 0.0012 (0.0003 -- 0.0037)  max mem: 16413
[2023-08-31 15:40:07,753] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17665
[2023-08-31 15:40:07,753] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17665
[2023-08-31 15:40:07,753] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:40:07,753] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:40:07,753] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [110]  [ 80/160]  eta: 0:01:19  lr: 0.000020  min_lr: 0.000000  loss: 1.8126 (1.7897)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8627 (8.6690)  time: 0.9348 (0.5181 -- 4.3814)  data: 0.0014 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [110]  [100/160]  eta: 0:00:57  lr: 0.000020  min_lr: 0.000000  loss: 1.3564 (1.7532)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6776 (8.6704)  time: 0.8265 (0.5259 -- 3.4008)  data: 0.0015 (0.0004 -- 0.0062)  max mem: 16413
Epoch: [110]  [120/160]  eta: 0:00:38  lr: 0.000020  min_lr: 0.000000  loss: 1.6000 (1.7396)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1744 (8.5646)  time: 0.9987 (0.5094 -- 4.6098)  data: 0.0010 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.6658 (1.7319)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9116 (8.6176)  time: 0.8048 (0.5100 -- 3.3182)  data: 0.0011 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9443 (1.7451)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3660 (8.5302)  time: 0.6128 (0.4954 -- 2.4305)  data: 0.0006 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [110] Total time: 0:02:24 (0.9033 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9443 (1.7276)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3660 (8.5302)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1826 (0.1826)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5356 (2.5356 -- 2.5356)  data: 2.3097 (2.3097 -- 2.3097)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3454 (0.6609)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4502 (0.1986 -- 2.5356)  data: 0.2352 (0.0008 -- 2.3097)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4525 (0.5952)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (96.8254)  time: 0.2187 (0.1699 -- 0.5147)  data: 0.0154 (0.0001 -- 0.2680)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4948 (0.6463)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.2043 (0.1331 -- 0.5147)  data: 0.0152 (0.0001 -- 0.2680)  max mem: 16413
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 85.062 Acc@5 97.718 loss 0.613
Accuracy of the network on the 482 val images: 85.06%
[2023-08-31 15:41:35,270] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 15:41:35,271] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 15:41:35,271] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 15:41:35,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 15:41:36,828] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 15:41:36,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.06%
Epoch: [111]  [  0/160]  eta: 0:20:11  lr: 0.000020  min_lr: 0.000000  loss: 1.5980 (1.5980)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0525 (8.0525)  time: 7.5708 (7.5708 -- 7.5708)  data: 7.0280 (7.0280 -- 7.0280)  max mem: 16413
Epoch: [111]  [ 20/160]  eta: 0:02:56  lr: 0.000020  min_lr: 0.000000  loss: 1.7430 (1.7095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4152 (8.8092)  time: 0.9447 (0.5070 -- 3.7781)  data: 0.3265 (0.0004 -- 3.2510)  max mem: 16413
[2023-08-31 15:42:16,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:42:16,110] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:42:16,110] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:42:16,110] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [111]  [ 40/160]  eta: 0:02:04  lr: 0.000020  min_lr: 0.000000  loss: 1.6716 (1.6608)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9910 (8.4174)  time: 0.8077 (0.5284 -- 4.0585)  data: 0.2533 (0.0002 -- 3.5255)  max mem: 16413
[2023-08-31 15:42:29,115] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17813
[2023-08-31 15:42:29,115] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17813
[2023-08-31 15:42:29,115] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:42:29,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 15:42:29,115] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [111]  [ 60/160]  eta: 0:01:38  lr: 0.000020  min_lr: 0.000000  loss: 1.5531 (1.6495)  loss_scale: 32768.0000 (21487.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6340 (8.5751)  time: 0.8604 (0.5261 -- 2.4708)  data: 0.1594 (0.0005 -- 1.7628)  max mem: 16413
Epoch: [111]  [ 80/160]  eta: 0:01:13  lr: 0.000020  min_lr: 0.000000  loss: 1.8343 (1.6760)  loss_scale: 16384.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4247 (8.3768)  time: 0.7471 (0.5266 -- 2.6182)  data: 0.0559 (0.0004 -- 1.0940)  max mem: 16413
Epoch: [111]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 1.6881 (1.6874)  loss_scale: 16384.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8271 (8.5556)  time: 1.0134 (0.5099 -- 5.0675)  data: 0.0020 (0.0003 -- 0.0085)  max mem: 16413
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.7685 (1.6862)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3311 (8.6494)  time: 0.8127 (0.5331 -- 3.2226)  data: 0.0022 (0.0004 -- 0.0098)  max mem: 16413
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.7785 (1.6933)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6109 (8.7261)  time: 0.8790 (0.5346 -- 3.1296)  data: 0.0246 (0.0003 -- 0.4498)  max mem: 16413
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.6747 (1.6917)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3187 (8.7239)  time: 0.7809 (0.4948 -- 4.2278)  data: 0.0007 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [111] Total time: 0:02:23 (0.8997 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.6747 (1.7208)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3187 (8.7239)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1950 (0.1950)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3907 (2.3907 -- 2.3907)  data: 2.1796 (2.1796 -- 2.1796)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4000 (0.6511)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4251 (0.2099 -- 2.3907)  data: 0.2068 (0.0006 -- 2.1796)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4561 (0.5728)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2188 (0.1723 -- 0.3473)  data: 0.0129 (0.0001 -- 0.1334)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4926 (0.6306)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.5104)  time: 0.2024 (0.1327 -- 0.3473)  data: 0.0124 (0.0001 -- 0.1334)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 84.232 Acc@5 97.718 loss 0.612
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.06%
Epoch: [112]  [  0/160]  eta: 0:23:02  lr: 0.000020  min_lr: 0.000000  loss: 1.7679 (1.7679)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4115 (6.4115)  time: 8.6418 (8.6418 -- 8.6418)  data: 8.1051 (8.1051 -- 8.1051)  max mem: 16413
Epoch: [112]  [ 20/160]  eta: 0:02:47  lr: 0.000020  min_lr: 0.000000  loss: 1.7800 (1.7109)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1714 (8.7774)  time: 0.8245 (0.5319 -- 2.2097)  data: 0.1028 (0.0006 -- 1.0830)  max mem: 16413
[2023-08-31 15:44:34,850] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:44:34,851] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:44:34,851] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:44:34,852] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:44:43,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17953
[2023-08-31 15:44:43,514] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17953
[2023-08-31 15:44:43,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:44:43,514] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:44:43,514] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [112]  [ 40/160]  eta: 0:01:58  lr: 0.000020  min_lr: 0.000000  loss: 1.6899 (1.7080)  loss_scale: 32768.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5599 (8.3221)  time: 0.7713 (0.5131 -- 2.7840)  data: 0.0068 (0.0003 -- 0.0970)  max mem: 16413
Epoch: [112]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 1.7864 (1.7252)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1948 (8.3036)  time: 0.8988 (0.5346 -- 3.4737)  data: 0.0017 (0.0006 -- 0.0042)  max mem: 16413
[2023-08-31 15:45:24,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=103, lr=[4.6819354482928105e-07, 4.6819354482928105e-07, 6.242580597723747e-07, 6.242580597723747e-07, 8.323440796964997e-07, 8.323440796964997e-07, 1.1097921062619995e-06, 1.1097921062619995e-06, 1.4797228083493328e-06, 1.4797228083493328e-06, 1.972963744465777e-06, 1.972963744465777e-06, 2.630618325954369e-06, 2.630618325954369e-06, 3.5074911012724923e-06, 3.5074911012724923e-06, 4.676654801696657e-06, 4.676654801696657e-06, 6.235539735595542e-06, 6.235539735595542e-06, 8.314052980794057e-06, 8.314052980794057e-06, 1.1085403974392075e-05, 1.1085403974392075e-05, 1.4780538632522767e-05, 1.4780538632522767e-05, 1.970738484336369e-05, 1.970738484336369e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 15:45:24,656] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=16.66652729406213, CurrSamplesPerSec=23.4017312901543, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [112]  [ 80/160]  eta: 0:01:15  lr: 0.000020  min_lr: 0.000000  loss: 1.7352 (1.7232)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4676 (8.3976)  time: 0.9092 (0.5226 -- 3.5903)  data: 0.0016 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [112]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 1.8550 (1.7416)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1659 (8.4396)  time: 0.9071 (0.5140 -- 3.3531)  data: 0.0015 (0.0004 -- 0.0061)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000000  loss: 1.5527 (1.7128)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0952 (8.6015)  time: 0.9052 (0.5310 -- 2.9842)  data: 0.0015 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [112]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.7338 (1.7087)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2755 (8.8029)  time: 0.8307 (0.5241 -- 3.2031)  data: 0.0184 (0.0004 -- 0.3310)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.8493 (1.7176)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0044 (8.7529)  time: 0.6222 (0.4971 -- 2.5510)  data: 0.0007 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [112] Total time: 0:02:21 (0.8845 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.8493 (1.7058)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0044 (8.7529)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2031 (0.2031)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5739 (2.5739 -- 2.5739)  data: 2.3413 (2.3413 -- 2.3413)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3144 (0.6472)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4496 (0.1901 -- 2.5739)  data: 0.2349 (0.0006 -- 2.3413)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4405 (0.5810)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (96.8254)  time: 0.2146 (0.1698 -- 0.4489)  data: 0.0123 (0.0001 -- 0.2317)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4824 (0.6373)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.2010 (0.1323 -- 0.4489)  data: 0.0120 (0.0001 -- 0.2317)  max mem: 16413
Val: Total time: 0:00:07 (0.2908 s / it)
* Acc@1 83.817 Acc@5 97.510 loss 0.614
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 85.06%
Epoch: [113]  [  0/160]  eta: 0:20:03  lr: 0.000020  min_lr: 0.000000  loss: 0.9592 (0.9592)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6047 (7.6047)  time: 7.5220 (7.5220 -- 7.5220)  data: 6.2100 (6.2100 -- 6.2100)  max mem: 16413
[2023-08-31 15:46:46,902] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:46:46,902] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:46:46,904] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:46:46,905] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:46:54,776] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18093
[2023-08-31 15:46:54,776] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18093
[2023-08-31 15:46:54,776] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:46:54,776] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:46:54,776] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [113]  [ 20/160]  eta: 0:02:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8903 (1.8332)  loss_scale: 32768.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6001 (8.9626)  time: 0.8005 (0.5275 -- 3.3699)  data: 0.2328 (0.0004 -- 2.8209)  max mem: 16413
Epoch: [113]  [ 40/160]  eta: 0:02:01  lr: 0.000019  min_lr: 0.000000  loss: 1.7218 (1.7607)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7999 (8.7620)  time: 0.8949 (0.5160 -- 3.7450)  data: 0.3496 (0.0004 -- 3.2268)  max mem: 16413
[2023-08-31 15:47:24,855] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18127
[2023-08-31 15:47:24,855] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18127
[2023-08-31 15:47:24,855] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:47:24,855] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:47:24,856] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [113]  [ 60/160]  eta: 0:01:38  lr: 0.000019  min_lr: 0.000000  loss: 1.6504 (1.7625)  loss_scale: 8192.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4918 (9.1854)  time: 0.9197 (0.5278 -- 3.7615)  data: 0.1130 (0.0004 -- 1.2361)  max mem: 16413
Epoch: [113]  [ 80/160]  eta: 0:01:16  lr: 0.000019  min_lr: 0.000000  loss: 1.8069 (1.7884)  loss_scale: 8192.0000 (15170.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8340 (9.2419)  time: 0.8570 (0.5316 -- 2.2953)  data: 0.0016 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [113]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 1.7771 (1.7819)  loss_scale: 8192.0000 (13788.5149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2647 (9.2712)  time: 0.8748 (0.5201 -- 2.2659)  data: 0.0021 (0.0007 -- 0.0101)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.5280 (1.7542)  loss_scale: 8192.0000 (12863.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9016 (9.2552)  time: 0.8667 (0.5165 -- 3.4096)  data: 0.0014 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.7529 (1.7590)  loss_scale: 8192.0000 (12200.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7381 (9.3469)  time: 0.9514 (0.5201 -- 4.5936)  data: 0.0013 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.8681 (1.7796)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2059 (9.2900)  time: 0.6289 (0.4954 -- 1.8841)  data: 0.0007 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [113] Total time: 0:02:22 (0.8930 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.8681 (1.7425)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2059 (9.2900)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2172 (0.2172)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4450 (2.4450 -- 2.4450)  data: 2.2000 (2.2000 -- 2.2000)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3698 (0.6454)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4320 (0.2014 -- 2.4450)  data: 0.2119 (0.0007 -- 2.2000)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4374 (0.5786)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.8836)  time: 0.2246 (0.1690 -- 0.4214)  data: 0.0183 (0.0001 -- 0.2327)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5206 (0.6318)  acc1: 85.7143 (82.1577)  acc5: 100.0000 (97.9253)  time: 0.2100 (0.1330 -- 0.4214)  data: 0.0181 (0.0001 -- 0.2327)  max mem: 16413
Val: Total time: 0:00:07 (0.2936 s / it)
* Acc@1 83.610 Acc@5 97.925 loss 0.613
Accuracy of the network on the 482 val images: 83.61%
Max accuracy: 85.06%
Epoch: [114]  [  0/160]  eta: 0:17:33  lr: 0.000019  min_lr: 0.000000  loss: 1.7561 (1.7561)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.3179 (13.3179)  time: 6.5827 (6.5827 -- 6.5827)  data: 4.6602 (4.6602 -- 4.6602)  max mem: 16413
[2023-08-31 15:49:29,714] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:49:29,714] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 15:49:29,717] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:49:29,717] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [114]  [ 20/160]  eta: 0:02:43  lr: 0.000019  min_lr: 0.000000  loss: 1.6733 (1.7056)  loss_scale: 8192.0000 (10142.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2838 (9.2536)  time: 0.8996 (0.5331 -- 2.4544)  data: 0.0979 (0.0005 -- 1.9284)  max mem: 16413
[2023-08-31 15:49:42,585] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18270
[2023-08-31 15:49:42,585] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18270
[2023-08-31 15:49:42,585] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:49:42,585] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 15:49:42,585] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [114]  [ 40/160]  eta: 0:02:03  lr: 0.000019  min_lr: 0.000000  loss: 1.7333 (1.7501)  loss_scale: 8192.0000 (10989.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0125 (9.6678)  time: 0.8802 (0.5284 -- 3.6040)  data: 0.0013 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [114]  [ 60/160]  eta: 0:01:38  lr: 0.000019  min_lr: 0.000000  loss: 1.5926 (1.7042)  loss_scale: 8192.0000 (10072.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2568 (9.5116)  time: 0.8834 (0.5267 -- 3.7135)  data: 0.0017 (0.0004 -- 0.0049)  max mem: 16413
Epoch: [114]  [ 80/160]  eta: 0:01:15  lr: 0.000019  min_lr: 0.000000  loss: 1.7312 (1.7139)  loss_scale: 8192.0000 (9607.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3309 (9.5454)  time: 0.8295 (0.5321 -- 4.2448)  data: 0.0017 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.7142 (1.7137)  loss_scale: 8192.0000 (9327.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1197 (9.6245)  time: 0.8780 (0.5354 -- 2.3400)  data: 0.1728 (0.0003 -- 1.7326)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.5528 (1.7073)  loss_scale: 8192.0000 (9139.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7900 (9.5108)  time: 0.8687 (0.5160 -- 2.7708)  data: 0.0432 (0.0005 -- 0.7677)  max mem: 16413
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8142 (1.7253)  loss_scale: 8192.0000 (9005.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8563 (9.4196)  time: 0.8585 (0.5315 -- 4.1218)  data: 0.2156 (0.0006 -- 3.0744)  max mem: 16413
[2023-08-31 15:51:30,438] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:51:30,438] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 15:51:30,438] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:51:30,438] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9056 (1.7321)  loss_scale: 8192.0000 (8960.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3750 (9.4521)  time: 0.6808 (0.4924 -- 2.7947)  data: 0.1307 (0.0002 -- 1.7198)  max mem: 16413
Epoch: [114] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.9056 (1.7198)  loss_scale: 8192.0000 (8960.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3750 (9.4521)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2051 (0.2051)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3792 (2.3792 -- 2.3792)  data: 2.1433 (2.1433 -- 2.1433)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3306 (0.6477)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4334 (0.1990 -- 2.3792)  data: 0.2170 (0.0008 -- 2.1433)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4646 (0.5691)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2274 (0.1685 -- 0.4764)  data: 0.0252 (0.0001 -- 0.2566)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4966 (0.6245)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2133 (0.1331 -- 0.4764)  data: 0.0248 (0.0001 -- 0.2566)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 84.025 Acc@5 97.718 loss 0.604
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.06%
Epoch: [115]  [  0/160]  eta: 0:20:11  lr: 0.000019  min_lr: 0.000000  loss: 1.5710 (1.5710)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2907 (3.2907)  time: 7.5743 (7.5743 -- 7.5743)  data: 5.9133 (5.9133 -- 5.9133)  max mem: 16413
Epoch: [115]  [ 20/160]  eta: 0:02:46  lr: 0.000019  min_lr: 0.000000  loss: 1.6493 (1.6640)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7761 (8.8371)  time: 0.8681 (0.5235 -- 2.3917)  data: 0.1105 (0.0005 -- 1.6530)  max mem: 16413
Epoch: [115]  [ 40/160]  eta: 0:02:04  lr: 0.000019  min_lr: 0.000000  loss: 1.5576 (1.6458)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4228 (9.2757)  time: 0.8739 (0.5408 -- 3.3477)  data: 0.0914 (0.0002 -- 1.7992)  max mem: 16413
Epoch: [115]  [ 60/160]  eta: 0:01:37  lr: 0.000019  min_lr: 0.000000  loss: 1.5678 (1.6452)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1535 (9.7220)  time: 0.8556 (0.5333 -- 3.0771)  data: 0.0016 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:15  lr: 0.000019  min_lr: 0.000000  loss: 1.7289 (1.6781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6381 (9.5806)  time: 0.8660 (0.5317 -- 3.7297)  data: 0.0014 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [115]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 1.7117 (1.6858)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7475 (9.4306)  time: 0.9133 (0.5239 -- 4.5273)  data: 0.0017 (0.0004 -- 0.0086)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8463 (1.6983)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0558 (9.4557)  time: 0.7705 (0.5319 -- 3.7833)  data: 0.0017 (0.0006 -- 0.0048)  max mem: 16413
[2023-08-31 15:53:35,430] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:53:35,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:53:35,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:53:35,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [115]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.6859 (1.6978)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9408 (9.4374)  time: 0.8978 (0.5108 -- 3.1913)  data: 0.0088 (0.0004 -- 0.1486)  max mem: 16413
[2023-08-31 15:53:54,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18549
[2023-08-31 15:53:54,157] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18549
[2023-08-31 15:53:54,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:53:54,157] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:53:54,157] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7688 (1.7011)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9393 (9.2935)  time: 0.7072 (0.4956 -- 2.9361)  data: 0.1873 (0.0002 -- 2.4147)  max mem: 16413
Epoch: [115] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7688 (1.7016)  loss_scale: 16384.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9393 (9.2935)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1933 (0.1933)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5700 (2.5700 -- 2.5700)  data: 2.3347 (2.3347 -- 2.3347)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3256 (0.6346)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4440 (0.2029 -- 2.5700)  data: 0.2272 (0.0004 -- 2.3347)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4624 (0.5610)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2120 (0.1705 -- 0.3788)  data: 0.0084 (0.0001 -- 0.1555)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4776 (0.6211)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.1963 (0.1325 -- 0.3788)  data: 0.0081 (0.0001 -- 0.1555)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 84.025 Acc@5 97.925 loss 0.602
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.06%
Epoch: [116]  [  0/160]  eta: 0:22:14  lr: 0.000018  min_lr: 0.000000  loss: 1.9139 (1.9139)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6366 (8.6366)  time: 8.3381 (8.3381 -- 8.3381)  data: 7.8173 (7.8173 -- 7.8173)  max mem: 16413
Epoch: [116]  [ 20/160]  eta: 0:02:47  lr: 0.000018  min_lr: 0.000000  loss: 1.6292 (1.6871)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6175 (8.9375)  time: 0.8361 (0.5124 -- 2.5663)  data: 0.1991 (0.0003 -- 2.0421)  max mem: 16413
Epoch: [116]  [ 40/160]  eta: 0:02:05  lr: 0.000018  min_lr: 0.000000  loss: 1.7041 (1.7212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2900 (9.0553)  time: 0.8869 (0.5228 -- 3.1924)  data: 0.2984 (0.0002 -- 2.6749)  max mem: 16413
Epoch: [116]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 1.7316 (1.7143)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0428 (8.9130)  time: 0.8674 (0.5239 -- 2.5256)  data: 0.0928 (0.0005 -- 0.9124)  max mem: 16413
Epoch: [116]  [ 80/160]  eta: 0:01:20  lr: 0.000018  min_lr: 0.000000  loss: 1.6835 (1.7031)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0874 (8.7328)  time: 1.0490 (0.5306 -- 5.1052)  data: 0.0015 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [116]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000000  loss: 1.7724 (1.7085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1557 (8.6463)  time: 0.6750 (0.5127 -- 2.2805)  data: 0.0018 (0.0003 -- 0.0050)  max mem: 16413
[2023-08-31 15:55:57,009] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:55:57,009] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:55:57,010] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:55:57,010] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [116]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.4560 (1.6860)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9002 (8.6044)  time: 1.0029 (0.5254 -- 5.5037)  data: 0.0016 (0.0004 -- 0.0047)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.7606 (1.6981)  loss_scale: 32768.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6280 (8.6032)  time: 0.7594 (0.5310 -- 2.7914)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
[2023-08-31 15:56:25,889] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18709
[2023-08-31 15:56:25,890] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18709
[2023-08-31 15:56:25,890] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:56:25,890] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:56:25,890] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.6015 (1.6902)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3421 (8.5873)  time: 0.7356 (0.4952 -- 3.2024)  data: 0.0009 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [116] Total time: 0:02:24 (0.9004 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.6015 (1.6956)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3421 (8.5873)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2037 (0.2037)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4646 (2.4646 -- 2.4646)  data: 2.2396 (2.2396 -- 2.2396)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3880 (0.6465)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (97.9798)  time: 0.4341 (0.2048 -- 2.4646)  data: 0.2197 (0.0010 -- 2.2396)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4535 (0.5723)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2157 (0.1691 -- 0.3567)  data: 0.0128 (0.0001 -- 0.1598)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4804 (0.6355)  acc1: 88.8889 (80.9129)  acc5: 100.0000 (97.0954)  time: 0.1999 (0.1330 -- 0.3567)  data: 0.0121 (0.0001 -- 0.1598)  max mem: 16413
Val: Total time: 0:00:07 (0.2879 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.612
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 85.06%
Epoch: [117]  [  0/160]  eta: 0:16:47  lr: 0.000018  min_lr: 0.000000  loss: 1.4459 (1.4459)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6003 (7.6003)  time: 6.2958 (6.2958 -- 6.2958)  data: 5.7504 (5.7504 -- 5.7504)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:46  lr: 0.000018  min_lr: 0.000000  loss: 1.7135 (1.6324)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9549 (8.5663)  time: 0.9340 (0.5219 -- 4.1367)  data: 0.0197 (0.0010 -- 0.3168)  max mem: 16413
Epoch: [117]  [ 40/160]  eta: 0:02:07  lr: 0.000018  min_lr: 0.000000  loss: 1.7395 (1.6672)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5111 (8.6770)  time: 0.9317 (0.5244 -- 4.0001)  data: 0.0015 (0.0005 -- 0.0049)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 1.8591 (1.7419)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2919 (9.0457)  time: 0.8187 (0.5169 -- 3.8140)  data: 0.0489 (0.0005 -- 0.9368)  max mem: 16413
Epoch: [117]  [ 80/160]  eta: 0:01:17  lr: 0.000018  min_lr: 0.000000  loss: 1.8437 (1.7476)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6194 (8.7601)  time: 0.9451 (0.5260 -- 3.0614)  data: 0.0014 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [117]  [100/160]  eta: 0:00:57  lr: 0.000018  min_lr: 0.000000  loss: 1.6396 (1.7344)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6155 (8.6050)  time: 0.9136 (0.5212 -- 3.4566)  data: 0.0009 (0.0002 -- 0.0020)  max mem: 16413
[2023-08-31 15:58:31,608] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:58:31,608] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 15:58:31,608] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 15:58:31,608] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [117]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.6769 (1.7269)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1639 (8.5177)  time: 0.7582 (0.5231 -- 3.7521)  data: 0.0013 (0.0001 -- 0.0036)  max mem: 16413
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.7311 (1.7322)  loss_scale: 32768.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2802 (8.5633)  time: 0.8711 (0.5282 -- 2.7727)  data: 0.0818 (0.0003 -- 1.3416)  max mem: 16413
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.7562 (1.7286)  loss_scale: 32768.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8595 (8.6294)  time: 0.7253 (0.4955 -- 2.4193)  data: 0.1293 (0.0002 -- 1.8730)  max mem: 16413
Epoch: [117] Total time: 0:02:23 (0.8981 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.7562 (1.7225)  loss_scale: 32768.0000 (20684.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8595 (8.6294)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2061 (0.2061)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3815 (2.3815 -- 2.3815)  data: 2.1719 (2.1719 -- 2.1719)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3389 (0.6393)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4226 (0.2070 -- 2.3815)  data: 0.2087 (0.0007 -- 2.1719)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4299 (0.5750)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2259 (0.1686 -- 0.4914)  data: 0.0215 (0.0001 -- 0.3044)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4836 (0.6304)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2105 (0.1333 -- 0.4914)  data: 0.0210 (0.0001 -- 0.3044)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.607
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.06%
Epoch: [118]  [  0/160]  eta: 0:25:52  lr: 0.000018  min_lr: 0.000000  loss: 1.3396 (1.3396)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6412 (6.6412)  time: 9.7045 (9.7045 -- 9.7045)  data: 9.1623 (9.1623 -- 9.1623)  max mem: 16413
[2023-08-31 15:59:23,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18883
[2023-08-31 15:59:23,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18883
[2023-08-31 15:59:23,294] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 15:59:23,294] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 15:59:23,294] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [118]  [ 20/160]  eta: 0:02:46  lr: 0.000018  min_lr: 0.000000  loss: 1.6509 (1.7209)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6204 (8.2311)  time: 0.7637 (0.5255 -- 2.7499)  data: 0.2138 (0.0002 -- 2.2176)  max mem: 16413
Epoch: [118]  [ 40/160]  eta: 0:02:03  lr: 0.000018  min_lr: 0.000000  loss: 1.6348 (1.7185)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0384 (8.6232)  time: 0.8688 (0.5196 -- 2.5497)  data: 0.2078 (0.0004 -- 2.0168)  max mem: 16413
Epoch: [118]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.6360 (1.7127)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3944 (9.1847)  time: 0.8378 (0.5253 -- 2.3748)  data: 0.1652 (0.0002 -- 1.8365)  max mem: 16413
Epoch: [118]  [ 80/160]  eta: 0:01:17  lr: 0.000017  min_lr: 0.000000  loss: 1.8510 (1.7390)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9804 (9.1610)  time: 0.9569 (0.5335 -- 3.9228)  data: 0.4103 (0.0004 -- 3.3997)  max mem: 16413
Epoch: [118]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.6251 (1.7232)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2244 (9.1245)  time: 0.7793 (0.5289 -- 1.7801)  data: 0.1739 (0.0003 -- 1.2455)  max mem: 16413
[2023-08-31 16:01:02,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=109, lr=[4.134526430749807e-07, 4.134526430749807e-07, 5.51270190766641e-07, 5.51270190766641e-07, 7.35026921022188e-07, 7.35026921022188e-07, 9.800358946962506e-07, 9.800358946962506e-07, 1.3067145262616674e-06, 1.3067145262616674e-06, 1.7422860350155567e-06, 1.7422860350155567e-06, 2.3230480466874088e-06, 2.3230480466874088e-06, 3.097397395583212e-06, 3.097397395583212e-06, 4.12986319411095e-06, 4.12986319411095e-06, 5.506484258814599e-06, 5.506484258814599e-06, 7.341979011752798e-06, 7.341979011752798e-06, 9.789305349003732e-06, 9.789305349003732e-06, 1.3052407132004975e-05, 1.3052407132004975e-05, 1.7403209509339967e-05, 1.7403209509339967e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 16:01:02,067] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=16.62135043677268, CurrSamplesPerSec=21.89904492311655, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.7866 (1.7070)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3322 (9.1953)  time: 0.9052 (0.5389 -- 2.2085)  data: 0.1699 (0.0004 -- 1.4192)  max mem: 16413
[2023-08-31 16:01:13,922] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:01:13,922] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:01:13,923] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:01:13,923] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7129 (1.7075)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9005 (9.1752)  time: 0.8970 (0.5220 -- 2.3723)  data: 0.2010 (0.0002 -- 1.8651)  max mem: 16413
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.6963 (1.6913)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4540 (9.0869)  time: 0.7268 (0.4959 -- 3.3400)  data: 0.0532 (0.0002 -- 1.0483)  max mem: 16413
Epoch: [118] Total time: 0:02:23 (0.8992 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.6963 (1.7091)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4540 (9.0869)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1931 (0.1931)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2948 (2.2948 -- 2.2948)  data: 2.0869 (2.0869 -- 2.0869)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3676 (0.6448)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4371 (0.1993 -- 2.2948)  data: 0.2241 (0.0005 -- 2.0869)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4397 (0.5817)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2356 (0.1700 -- 0.6121)  data: 0.0303 (0.0001 -- 0.3677)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5341 (0.6349)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.5104)  time: 0.2220 (0.1324 -- 0.6121)  data: 0.0300 (0.0001 -- 0.3677)  max mem: 16413
Val: Total time: 0:00:07 (0.2960 s / it)
* Acc@1 84.440 Acc@5 97.718 loss 0.604
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.06%
Epoch: [119]  [  0/160]  eta: 0:20:40  lr: 0.000017  min_lr: 0.000000  loss: 1.6508 (1.6508)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5548 (11.5548)  time: 7.7527 (7.7527 -- 7.7527)  data: 6.8354 (6.8354 -- 6.8354)  max mem: 16413
[2023-08-31 16:02:00,486] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19051
[2023-08-31 16:02:00,486] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19051
[2023-08-31 16:02:00,486] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:02:00,486] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:02:00,486] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [119]  [ 20/160]  eta: 0:02:38  lr: 0.000017  min_lr: 0.000000  loss: 1.7378 (1.7312)  loss_scale: 16384.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8753 (8.8269)  time: 0.7989 (0.5265 -- 3.2930)  data: 0.2009 (0.0005 -- 2.0842)  max mem: 16413
Epoch: [119]  [ 40/160]  eta: 0:02:02  lr: 0.000017  min_lr: 0.000000  loss: 1.8608 (1.7948)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0661 (9.2472)  time: 0.9086 (0.5246 -- 2.8263)  data: 0.2012 (0.0004 -- 2.1962)  max mem: 16413
Epoch: [119]  [ 60/160]  eta: 0:01:42  lr: 0.000017  min_lr: 0.000000  loss: 1.8445 (1.7766)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1721 (9.2341)  time: 1.0180 (0.5147 -- 4.8245)  data: 0.0014 (0.0002 -- 0.0041)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:18  lr: 0.000017  min_lr: 0.000000  loss: 1.8539 (1.7889)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4204 (9.1045)  time: 0.8374 (0.5118 -- 3.9031)  data: 0.0015 (0.0002 -- 0.0055)  max mem: 16413
[2023-08-31 16:03:17,252] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19136
[2023-08-31 16:03:17,252] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19136
[2023-08-31 16:03:17,252] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:03:17,252] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:03:17,252] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [119]  [100/160]  eta: 0:00:58  lr: 0.000017  min_lr: 0.000000  loss: 1.5767 (1.7576)  loss_scale: 16384.0000 (17762.8515)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2111 (9.0387)  time: 0.9336 (0.5162 -- 3.4438)  data: 0.0012 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [119]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.6180 (1.7377)  loss_scale: 8192.0000 (16180.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3793 (9.0810)  time: 0.7720 (0.5167 -- 3.2979)  data: 0.0014 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.8322 (1.7509)  loss_scale: 8192.0000 (15047.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0932 (9.0543)  time: 0.8582 (0.5300 -- 2.7189)  data: 0.0539 (0.0002 -- 1.0471)  max mem: 16413
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.7016 (1.7415)  loss_scale: 8192.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0966 (8.9364)  time: 0.6732 (0.4930 -- 2.3605)  data: 0.1106 (0.0002 -- 1.8280)  max mem: 16413
Epoch: [119] Total time: 0:02:23 (0.8952 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.7016 (1.7312)  loss_scale: 8192.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0966 (8.9364)
[2023-08-31 16:04:07,064] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-08-31 16:04:07,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-08-31 16:04:07,069] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-08-31 16:04:07,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-08-31 16:04:08,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-08-31 16:04:08,115] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2135 (0.2135)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3196 (2.3196 -- 2.3196)  data: 2.0923 (2.0923 -- 2.0923)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3763 (0.6232)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4470 (0.2041 -- 2.3196)  data: 0.2243 (0.0008 -- 2.0923)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4288 (0.5667)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2352 (0.1698 -- 0.6217)  data: 0.0264 (0.0001 -- 0.3650)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4727 (0.6300)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.2170 (0.1323 -- 0.6217)  data: 0.0261 (0.0001 -- 0.3650)  max mem: 16413
Val: Total time: 0:00:08 (0.2965 s / it)
* Acc@1 84.232 Acc@5 97.095 loss 0.616
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.06%
Epoch: [120]  [  0/160]  eta: 0:19:12  lr: 0.000017  min_lr: 0.000000  loss: 1.8188 (1.8188)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5262 (10.5262)  time: 7.2054 (7.2054 -- 7.2054)  data: 6.2950 (6.2950 -- 6.2950)  max mem: 16413
Epoch: [120]  [ 20/160]  eta: 0:02:44  lr: 0.000017  min_lr: 0.000000  loss: 1.8497 (1.8229)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7416 (9.5989)  time: 0.8705 (0.5218 -- 3.2808)  data: 0.0549 (0.0003 -- 1.0695)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:09  lr: 0.000017  min_lr: 0.000000  loss: 1.7698 (1.7911)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7554 (8.8955)  time: 0.9748 (0.5177 -- 4.1971)  data: 0.4301 (0.0007 -- 3.6573)  max mem: 16413
Epoch: [120]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000000  loss: 1.7482 (1.7711)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9355 (9.1019)  time: 0.8032 (0.5259 -- 4.1255)  data: 0.2547 (0.0004 -- 3.6000)  max mem: 16413
[2023-08-31 16:05:20,236] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:05:20,236] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:05:20,239] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:05:20,240] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [120]  [ 80/160]  eta: 0:01:14  lr: 0.000017  min_lr: 0.000000  loss: 1.4515 (1.7227)  loss_scale: 16384.0000 (9810.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1974 (8.9925)  time: 0.7874 (0.5282 -- 3.1834)  data: 0.0554 (0.0008 -- 0.8469)  max mem: 16413
Epoch: [120]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.8397 (1.7517)  loss_scale: 16384.0000 (11111.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0945 (8.8829)  time: 0.8540 (0.5338 -- 2.9385)  data: 0.2124 (0.0002 -- 2.4063)  max mem: 16413
Epoch: [120]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.6062 (1.7472)  loss_scale: 16384.0000 (11983.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5648 (8.8167)  time: 0.8703 (0.5266 -- 3.0226)  data: 0.0955 (0.0003 -- 1.7360)  max mem: 16413
Epoch: [120]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.6239 (1.7322)  loss_scale: 16384.0000 (12607.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6290 (8.8211)  time: 0.8397 (0.5289 -- 1.8159)  data: 0.1544 (0.0004 -- 1.2947)  max mem: 16413
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.9028 (1.7383)  loss_scale: 16384.0000 (13056.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5193 (8.8079)  time: 0.7156 (0.4968 -- 1.5171)  data: 0.1130 (0.0003 -- 0.8695)  max mem: 16413
Epoch: [120] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.9028 (1.7272)  loss_scale: 16384.0000 (13056.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5193 (8.8079)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2128 (0.2128)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3462 (2.3462 -- 2.3462)  data: 2.1147 (2.1147 -- 2.1147)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4013 (0.6249)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4327 (0.1980 -- 2.3462)  data: 0.2207 (0.0008 -- 2.1147)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4670 (0.5621)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2245 (0.1693 -- 0.5136)  data: 0.0251 (0.0001 -- 0.3043)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4831 (0.6197)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (96.2656)  time: 0.2106 (0.1327 -- 0.5136)  data: 0.0248 (0.0001 -- 0.3043)  max mem: 16413
Val: Total time: 0:00:07 (0.2896 s / it)
* Acc@1 84.647 Acc@5 97.303 loss 0.612
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.06%
Epoch: [121]  [  0/160]  eta: 0:22:32  lr: 0.000017  min_lr: 0.000000  loss: 2.3321 (2.3321)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8727 (8.8727)  time: 8.4558 (8.4558 -- 8.4558)  data: 6.2983 (6.2983 -- 6.2983)  max mem: 16413
Epoch: [121]  [ 20/160]  eta: 0:02:47  lr: 0.000017  min_lr: 0.000000  loss: 1.6993 (1.6792)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2071 (8.5511)  time: 0.8300 (0.5266 -- 2.9200)  data: 0.0299 (0.0004 -- 0.5636)  max mem: 16413
[2023-08-31 16:07:17,120] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19387
[2023-08-31 16:07:17,120] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19387
[2023-08-31 16:07:17,121] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:07:17,121] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 16:07:17,121] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [121]  [ 40/160]  eta: 0:02:16  lr: 0.000016  min_lr: 0.000000  loss: 1.4404 (1.6150)  loss_scale: 8192.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0503 (8.4060)  time: 1.0713 (0.5214 -- 4.5712)  data: 0.0021 (0.0002 -- 0.0164)  max mem: 16413
Epoch: [121]  [ 60/160]  eta: 0:01:35  lr: 0.000016  min_lr: 0.000000  loss: 1.7098 (1.6624)  loss_scale: 8192.0000 (11817.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4298 (8.4438)  time: 0.5882 (0.5126 -- 0.8518)  data: 0.0016 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [121]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000000  loss: 1.5793 (1.6773)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2050 (8.6102)  time: 0.9524 (0.5265 -- 3.8080)  data: 0.0018 (0.0004 -- 0.0055)  max mem: 16413
Epoch: [121]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.6052 (1.6836)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1577 (8.6839)  time: 0.9093 (0.5230 -- 3.7079)  data: 0.2564 (0.0006 -- 3.1915)  max mem: 16413
Epoch: [121]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.8103 (1.7035)  loss_scale: 8192.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5188 (8.7122)  time: 0.7322 (0.5119 -- 2.0110)  data: 0.1447 (0.0002 -- 1.4678)  max mem: 16413
Epoch: [121]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.8194 (1.7099)  loss_scale: 8192.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9640 (8.9102)  time: 0.9027 (0.5245 -- 3.0533)  data: 0.1294 (0.0003 -- 1.7684)  max mem: 16413
[2023-08-31 16:09:05,263] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:09:05,263] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:09:05,263] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:09:05,263] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.6261 (1.7021)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6654 (9.0057)  time: 0.7387 (0.4953 -- 2.3666)  data: 0.1548 (0.0002 -- 1.8481)  max mem: 16413
Epoch: [121] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.6261 (1.7070)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6654 (9.0057)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.2086 (0.2086)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6332 (2.6332 -- 2.6332)  data: 2.3839 (2.3839 -- 2.3839)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3193 (0.6292)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4346 (0.1937 -- 2.6332)  data: 0.2177 (0.0002 -- 2.3839)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3872 (0.5609)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2076 (0.1685 -- 0.2431)  data: 0.0007 (0.0001 -- 0.0018)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4816 (0.6209)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.1926 (0.1332 -- 0.2402)  data: 0.0005 (0.0001 -- 0.0018)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 83.195 Acc@5 97.510 loss 0.619
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 85.06%
Epoch: [122]  [  0/160]  eta: 0:22:09  lr: 0.000016  min_lr: 0.000000  loss: 1.9508 (1.9508)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7767 (6.7767)  time: 8.3086 (8.3086 -- 8.3086)  data: 7.7393 (7.7393 -- 7.7393)  max mem: 16413
Epoch: [122]  [ 20/160]  eta: 0:02:48  lr: 0.000016  min_lr: 0.000000  loss: 1.7636 (1.7291)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6417 (8.7231)  time: 0.8507 (0.5240 -- 4.0598)  data: 0.3026 (0.0004 -- 3.5064)  max mem: 16413
Epoch: [122]  [ 40/160]  eta: 0:02:07  lr: 0.000016  min_lr: 0.000000  loss: 1.7951 (1.7647)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7913 (8.9314)  time: 0.9159 (0.5233 -- 4.2162)  data: 0.3710 (0.0004 -- 3.6886)  max mem: 16413
Epoch: [122]  [ 60/160]  eta: 0:01:37  lr: 0.000016  min_lr: 0.000000  loss: 1.7350 (1.7775)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0221 (8.6414)  time: 0.7776 (0.5180 -- 2.5441)  data: 0.2323 (0.0004 -- 2.0013)  max mem: 16413
Epoch: [122]  [ 80/160]  eta: 0:01:17  lr: 0.000016  min_lr: 0.000000  loss: 1.6735 (1.7765)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7628 (8.9205)  time: 0.9748 (0.5181 -- 3.0963)  data: 0.4282 (0.0005 -- 2.5529)  max mem: 16413
Epoch: [122]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.7428 (1.7586)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6605 (8.7027)  time: 0.8613 (0.5296 -- 4.0899)  data: 0.3171 (0.0005 -- 3.5725)  max mem: 16413
Epoch: [122]  [120/160]  eta: 0:00:37  lr: 0.000016  min_lr: 0.000000  loss: 1.7441 (1.7595)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6477 (8.7019)  time: 0.8762 (0.5218 -- 3.1046)  data: 0.3286 (0.0002 -- 2.5546)  max mem: 16413
[2023-08-31 16:11:10,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:11:10,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:11:10,269] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:11:10,270] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:11:11,879] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19647
[2023-08-31 16:11:11,880] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:11:11,879] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19647
[2023-08-31 16:11:11,880] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:11:11,880] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [122]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.5281 (1.7378)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1511 (8.5167)  time: 0.7830 (0.5213 -- 2.6775)  data: 0.1866 (0.0004 -- 2.1276)  max mem: 16413
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.6223 (1.7195)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9039 (8.5708)  time: 0.6460 (0.4971 -- 1.5732)  data: 0.0640 (0.0002 -- 0.7476)  max mem: 16413
Epoch: [122] Total time: 0:02:21 (0.8845 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.6223 (1.6871)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9039 (8.5708)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1899 (0.1899)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4399 (2.4399 -- 2.4399)  data: 2.2139 (2.2139 -- 2.2139)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3510 (0.6410)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4324 (0.2032 -- 2.4399)  data: 0.2148 (0.0006 -- 2.2139)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4467 (0.5647)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2252 (0.1688 -- 0.4251)  data: 0.0199 (0.0001 -- 0.2452)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4752 (0.6154)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (96.2656)  time: 0.2100 (0.1322 -- 0.4251)  data: 0.0196 (0.0001 -- 0.2452)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 84.855 Acc@5 97.303 loss 0.603
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.06%
Epoch: [123]  [  0/160]  eta: 0:20:43  lr: 0.000016  min_lr: 0.000000  loss: 0.8479 (0.8479)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3626 (9.3626)  time: 7.7742 (7.7742 -- 7.7742)  data: 7.2058 (7.2058 -- 7.2058)  max mem: 16413
Epoch: [123]  [ 20/160]  eta: 0:02:44  lr: 0.000016  min_lr: 0.000000  loss: 1.8438 (1.7385)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8957 (9.1577)  time: 0.8436 (0.5246 -- 3.3769)  data: 0.1279 (0.0005 -- 1.6911)  max mem: 16413
Epoch: [123]  [ 40/160]  eta: 0:02:03  lr: 0.000016  min_lr: 0.000000  loss: 1.7286 (1.7329)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5948 (8.7405)  time: 0.8738 (0.5299 -- 3.6268)  data: 0.2057 (0.0003 -- 3.1096)  max mem: 16413
Epoch: [123]  [ 60/160]  eta: 0:01:39  lr: 0.000016  min_lr: 0.000000  loss: 1.8996 (1.7718)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7108 (8.5494)  time: 0.9301 (0.5368 -- 2.4874)  data: 0.1329 (0.0005 -- 1.3542)  max mem: 16413
Epoch: [123]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000000  loss: 1.8068 (1.7440)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6908 (8.3966)  time: 0.8188 (0.5150 -- 2.8310)  data: 0.2756 (0.0006 -- 2.3051)  max mem: 16413
[2023-08-31 16:13:13,993] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:13:13,993] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:13:13,993] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:13:13,994] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [123]  [100/160]  eta: 0:00:56  lr: 0.000016  min_lr: 0.000000  loss: 1.8483 (1.7616)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3413 (8.4582)  time: 0.8645 (0.5316 -- 2.3874)  data: 0.0982 (0.0008 -- 1.8152)  max mem: 16413
[2023-08-31 16:13:26,810] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19792
[2023-08-31 16:13:26,810] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:13:26,810] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19792
[2023-08-31 16:13:26,811] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:13:26,811] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [123]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.8395 (1.7662)  loss_scale: 32768.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5259 (8.5566)  time: 0.8063 (0.5192 -- 3.3292)  data: 0.0094 (0.0004 -- 0.1607)  max mem: 16413
Epoch: [123]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.6804 (1.7515)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4712 (8.5985)  time: 0.9323 (0.5240 -- 3.5561)  data: 0.1334 (0.0007 -- 2.2512)  max mem: 16413
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7157 (1.7355)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5810 (8.6650)  time: 0.7024 (0.4999 -- 2.0821)  data: 0.0009 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [123] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7157 (1.7279)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5810 (8.6650)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1773 (0.1773)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2719 (2.2719 -- 2.2719)  data: 2.0613 (2.0613 -- 2.0613)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3227 (0.6619)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4108 (0.2089 -- 2.2719)  data: 0.1912 (0.0005 -- 2.0613)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3881 (0.5716)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2298 (0.1722 -- 0.5765)  data: 0.0220 (0.0001 -- 0.3954)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4545 (0.6299)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2122 (0.1334 -- 0.5765)  data: 0.0214 (0.0001 -- 0.3954)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 84.232 Acc@5 97.303 loss 0.613
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.06%
Epoch: [124]  [  0/160]  eta: 0:20:32  lr: 0.000016  min_lr: 0.000000  loss: 2.2580 (2.2580)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9984 (7.9984)  time: 7.7005 (7.7005 -- 7.7005)  data: 6.9929 (6.9929 -- 6.9929)  max mem: 16413
Epoch: [124]  [ 20/160]  eta: 0:02:58  lr: 0.000015  min_lr: 0.000000  loss: 1.6981 (1.7258)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1657 (8.8275)  time: 0.9505 (0.5276 -- 4.8350)  data: 0.0673 (0.0003 -- 0.8892)  max mem: 16413
Epoch: [124]  [ 40/160]  eta: 0:02:11  lr: 0.000015  min_lr: 0.000000  loss: 1.7183 (1.7145)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7363 (8.8036)  time: 0.9168 (0.5213 -- 3.1773)  data: 0.1028 (0.0002 -- 1.5801)  max mem: 16413
Epoch: [124]  [ 60/160]  eta: 0:01:41  lr: 0.000015  min_lr: 0.000000  loss: 1.8849 (1.7288)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3583 (8.6729)  time: 0.8474 (0.5072 -- 4.5892)  data: 0.2956 (0.0003 -- 3.6230)  max mem: 16413
Epoch: [124]  [ 80/160]  eta: 0:01:18  lr: 0.000015  min_lr: 0.000000  loss: 1.7310 (1.7247)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9803 (8.6983)  time: 0.8588 (0.5127 -- 3.1750)  data: 0.1709 (0.0002 -- 1.9372)  max mem: 16413
[2023-08-31 16:15:34,395] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:15:34,395] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:15:34,396] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:15:34,396] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:15:43,950] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19932
[2023-08-31 16:15:43,950] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:15:43,950] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19932
[2023-08-31 16:15:43,950] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 16:15:43,950] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [124]  [100/160]  eta: 0:00:57  lr: 0.000015  min_lr: 0.000000  loss: 1.7663 (1.7342)  loss_scale: 32768.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3729 (8.6497)  time: 0.8693 (0.5216 -- 3.9918)  data: 0.2494 (0.0004 -- 2.9909)  max mem: 16413
Epoch: [124]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8431 (1.7337)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8397 (8.6358)  time: 0.8383 (0.5161 -- 4.0742)  data: 0.2889 (0.0003 -- 3.5229)  max mem: 16413
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.7023 (1.7302)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4724 (8.6840)  time: 0.8157 (0.5225 -- 4.3338)  data: 0.2676 (0.0004 -- 3.8014)  max mem: 16413
[2023-08-31 16:16:38,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=115, lr=[3.601696481824693e-07, 3.601696481824693e-07, 4.802261975766258e-07, 4.802261975766258e-07, 6.403015967688343e-07, 6.403015967688343e-07, 8.537354623584458e-07, 8.537354623584458e-07, 1.1383139498112611e-06, 1.1383139498112611e-06, 1.5177519330816814e-06, 1.5177519330816814e-06, 2.0236692441089086e-06, 2.0236692441089086e-06, 2.698225658811878e-06, 2.698225658811878e-06, 3.5976342117491708e-06, 3.5976342117491708e-06, 4.796845615665561e-06, 4.796845615665561e-06, 6.395794154220747e-06, 6.395794154220747e-06, 8.527725538960998e-06, 8.527725538960998e-06, 1.1370300718614664e-05, 1.1370300718614664e-05, 1.5160400958152884e-05, 1.5160400958152884e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 16:16:38,658] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=16.67175843244535, CurrSamplesPerSec=24.55786938549274, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.5718 (1.7147)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1582 (8.6749)  time: 0.7434 (0.4928 -- 2.9879)  data: 0.2203 (0.0002 -- 2.4396)  max mem: 16413
Epoch: [124] Total time: 0:02:23 (0.8999 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.5718 (1.6804)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1582 (8.6749)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1707 (0.1707)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3986 (2.3986 -- 2.3986)  data: 2.1637 (2.1637 -- 2.1637)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.2963 (0.6364)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4311 (0.1994 -- 2.3986)  data: 0.2157 (0.0009 -- 2.1637)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4161 (0.5669)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2262 (0.1683 -- 0.4225)  data: 0.0229 (0.0001 -- 0.2466)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4656 (0.6253)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (96.6805)  time: 0.2101 (0.1325 -- 0.4225)  data: 0.0223 (0.0001 -- 0.2466)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.607
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.06%
Epoch: [125]  [  0/160]  eta: 0:17:44  lr: 0.000015  min_lr: 0.000000  loss: 2.1088 (2.1088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9323 (7.9323)  time: 6.6552 (6.6552 -- 6.6552)  data: 5.8195 (5.8195 -- 5.8195)  max mem: 16413
Epoch: [125]  [ 20/160]  eta: 0:02:52  lr: 0.000015  min_lr: 0.000000  loss: 1.8490 (1.7939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0825 (9.7382)  time: 0.9628 (0.5278 -- 3.6419)  data: 0.0014 (0.0002 -- 0.0023)  max mem: 16413
[2023-08-31 16:17:13,552] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20022
[2023-08-31 16:17:13,552] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20022
[2023-08-31 16:17:13,553] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:17:13,553] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:17:13,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [125]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000000  loss: 1.7975 (1.7432)  loss_scale: 8192.0000 (12587.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8316 (9.4057)  time: 0.8443 (0.5175 -- 3.7800)  data: 0.0012 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [125]  [ 60/160]  eta: 0:01:41  lr: 0.000015  min_lr: 0.000000  loss: 1.7314 (1.7435)  loss_scale: 8192.0000 (11146.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9965 (9.0239)  time: 0.9624 (0.5169 -- 3.6652)  data: 0.0013 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [125]  [ 80/160]  eta: 0:01:15  lr: 0.000015  min_lr: 0.000000  loss: 1.8867 (1.7769)  loss_scale: 8192.0000 (10416.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0963 (9.1674)  time: 0.7416 (0.5198 -- 3.6081)  data: 0.0020 (0.0004 -- 0.0128)  max mem: 16413
Epoch: [125]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.7742 (1.7761)  loss_scale: 8192.0000 (9976.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1966 (9.0430)  time: 0.8602 (0.5327 -- 3.4050)  data: 0.0016 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [125]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.5047 (1.7318)  loss_scale: 8192.0000 (9681.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7568 (9.1473)  time: 1.0083 (0.5230 -- 4.3692)  data: 0.0012 (0.0004 -- 0.0051)  max mem: 16413
Epoch: [125]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.7597 (1.7314)  loss_scale: 8192.0000 (9470.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6858 (9.1569)  time: 0.8198 (0.5234 -- 3.8544)  data: 0.0013 (0.0003 -- 0.0024)  max mem: 16413
[2023-08-31 16:19:05,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:19:05,142] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:19:05,142] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:19:05,142] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.6998 (1.7314)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1953 (9.1467)  time: 0.6716 (0.4946 -- 2.6985)  data: 0.0006 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [125] Total time: 0:02:23 (0.8973 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.6998 (1.7287)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1953 (9.1467)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.1993 (0.1993)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6387 (2.6387 -- 2.6387)  data: 2.3724 (2.3724 -- 2.3724)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3372 (0.6304)  acc1: 88.8889 (84.8485)  acc5: 100.0000 (97.9798)  time: 0.4563 (0.1957 -- 2.6387)  data: 0.2391 (0.0007 -- 2.3724)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4149 (0.5577)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (96.8254)  time: 0.2165 (0.1695 -- 0.4454)  data: 0.0159 (0.0001 -- 0.2462)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4598 (0.6169)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (96.6805)  time: 0.1993 (0.1329 -- 0.4454)  data: 0.0156 (0.0001 -- 0.2462)  max mem: 16413
Val: Total time: 0:00:07 (0.2944 s / it)
* Acc@1 85.477 Acc@5 97.510 loss 0.612
Accuracy of the network on the 482 val images: 85.48%
[2023-08-31 16:19:18,083] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 16:19:18,084] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 16:19:18,085] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 16:19:18,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 16:19:19,225] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 16:19:19,225] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.48%
Epoch: [126]  [  0/160]  eta: 0:24:17  lr: 0.000015  min_lr: 0.000000  loss: 1.6967 (1.6967)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3858 (7.3858)  time: 9.1080 (9.1080 -- 9.1080)  data: 8.5447 (8.5447 -- 8.5447)  max mem: 16413
Epoch: [126]  [ 20/160]  eta: 0:03:05  lr: 0.000015  min_lr: 0.000000  loss: 1.5737 (1.6201)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7510 (8.4245)  time: 0.9330 (0.5093 -- 4.6460)  data: 0.3944 (0.0003 -- 4.0858)  max mem: 16413
Epoch: [126]  [ 40/160]  eta: 0:02:14  lr: 0.000015  min_lr: 0.000000  loss: 1.7203 (1.6347)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5399 (8.4914)  time: 0.9104 (0.5255 -- 3.2903)  data: 0.3644 (0.0003 -- 2.7610)  max mem: 16413
Epoch: [126]  [ 60/160]  eta: 0:01:41  lr: 0.000015  min_lr: 0.000000  loss: 1.5960 (1.6392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7164 (8.2929)  time: 0.7944 (0.5199 -- 3.1130)  data: 0.2513 (0.0002 -- 2.5869)  max mem: 16413
Epoch: [126]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 1.8258 (1.6863)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5263 (8.3700)  time: 0.8042 (0.5157 -- 1.9182)  data: 0.1974 (0.0007 -- 1.3906)  max mem: 16413
Epoch: [126]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000000  loss: 1.6678 (1.6764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9696 (8.3893)  time: 0.8274 (0.5283 -- 3.1112)  data: 0.0014 (0.0004 -- 0.0033)  max mem: 16413
[2023-08-31 16:21:10,663] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:21:10,663] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:21:10,664] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:21:10,664] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [126]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.7631 (1.6990)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8871 (8.4505)  time: 0.8741 (0.5262 -- 3.7963)  data: 0.0020 (0.0005 -- 0.0060)  max mem: 16413
Epoch: [126]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.8254 (1.7134)  loss_scale: 32768.0000 (18940.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4945 (8.4427)  time: 0.9072 (0.5226 -- 2.9631)  data: 0.2512 (0.0004 -- 2.4363)  max mem: 16413
[2023-08-31 16:21:35,103] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20307
[2023-08-31 16:21:35,103] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20307
[2023-08-31 16:21:35,103] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:21:35,103] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:21:35,103] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.6685 (1.7194)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7451 (8.5718)  time: 0.7256 (0.4935 -- 2.5298)  data: 0.0005 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [126] Total time: 0:02:24 (0.9007 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.6685 (1.7049)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7451 (8.5718)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1961 (0.1961)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4148 (2.4148 -- 2.4148)  data: 2.2096 (2.2096 -- 2.2096)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3343 (0.6383)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4261 (0.2009 -- 2.4148)  data: 0.2067 (0.0005 -- 2.2096)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4248 (0.5670)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2229 (0.1714 -- 0.4111)  data: 0.0146 (0.0001 -- 0.2242)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4865 (0.6257)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.2656)  time: 0.2063 (0.1333 -- 0.4111)  data: 0.0143 (0.0001 -- 0.2242)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 84.647 Acc@5 97.510 loss 0.614
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.48%
Epoch: [127]  [  0/160]  eta: 0:21:57  lr: 0.000014  min_lr: 0.000000  loss: 2.1527 (2.1527)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5652 (6.5652)  time: 8.2373 (8.2373 -- 8.2373)  data: 7.7084 (7.7084 -- 7.7084)  max mem: 16413
Epoch: [127]  [ 20/160]  eta: 0:02:42  lr: 0.000014  min_lr: 0.000000  loss: 1.8861 (1.8189)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5346 (8.7831)  time: 0.8052 (0.5295 -- 3.3343)  data: 0.1908 (0.0003 -- 2.3353)  max mem: 16413
Epoch: [127]  [ 40/160]  eta: 0:02:10  lr: 0.000014  min_lr: 0.000000  loss: 1.4112 (1.6596)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3720 (8.7604)  time: 1.0201 (0.5246 -- 5.1788)  data: 0.4707 (0.0004 -- 4.6420)  max mem: 16413
Epoch: [127]  [ 60/160]  eta: 0:01:38  lr: 0.000014  min_lr: 0.000000  loss: 1.6939 (1.6912)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1215 (8.7765)  time: 0.7705 (0.5159 -- 4.0792)  data: 0.2316 (0.0002 -- 3.5571)  max mem: 16413
Epoch: [127]  [ 80/160]  eta: 0:01:17  lr: 0.000014  min_lr: 0.000000  loss: 1.7223 (1.7111)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2837 (8.6913)  time: 0.9118 (0.5214 -- 3.8899)  data: 0.3648 (0.0003 -- 3.3688)  max mem: 16413
Epoch: [127]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 1.6931 (1.7072)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3023 (8.5985)  time: 0.7779 (0.5350 -- 3.1486)  data: 0.2138 (0.0002 -- 2.6014)  max mem: 16413
[2023-08-31 16:23:41,706] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:23:41,707] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:23:41,708] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:23:41,708] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [127]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 1.6790 (1.7040)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8337 (8.6177)  time: 0.9886 (0.5052 -- 5.4406)  data: 0.4490 (0.0003 -- 4.9066)  max mem: 16413
[2023-08-31 16:23:52,214] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20448
[2023-08-31 16:23:52,214] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:23:52,214] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20448
[2023-08-31 16:23:52,215] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 16:23:52,215] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [127]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.6922 (1.7053)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7207 (8.6458)  time: 0.7872 (0.5251 -- 3.3280)  data: 0.2352 (0.0004 -- 2.8031)  max mem: 16413
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7155 (1.7157)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9876 (8.6746)  time: 0.6531 (0.4955 -- 1.7581)  data: 0.0466 (0.0002 -- 0.9123)  max mem: 16413
Epoch: [127] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7155 (1.7260)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9876 (8.6746)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1927 (0.1927)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5177 (2.5177 -- 2.5177)  data: 2.3167 (2.3167 -- 2.3167)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3267 (0.6288)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4603 (0.1930 -- 2.5177)  data: 0.2453 (0.0008 -- 2.3167)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4633 (0.5640)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2219 (0.1682 -- 0.5994)  data: 0.0192 (0.0001 -- 0.3706)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4733 (0.6213)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.2656)  time: 0.2066 (0.1328 -- 0.5994)  data: 0.0188 (0.0001 -- 0.3706)  max mem: 16413
Val: Total time: 0:00:07 (0.2942 s / it)
* Acc@1 83.817 Acc@5 97.510 loss 0.610
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 85.48%
Epoch: [128]  [  0/160]  eta: 0:20:54  lr: 0.000014  min_lr: 0.000000  loss: 1.9199 (1.9199)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8862 (8.8862)  time: 7.8403 (7.8403 -- 7.8403)  data: 7.2898 (7.2898 -- 7.2898)  max mem: 16413
Epoch: [128]  [ 20/160]  eta: 0:02:44  lr: 0.000014  min_lr: 0.000000  loss: 1.6116 (1.6479)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6563 (8.2280)  time: 0.8405 (0.5247 -- 2.7909)  data: 0.2885 (0.0004 -- 2.2609)  max mem: 16413
Epoch: [128]  [ 40/160]  eta: 0:02:10  lr: 0.000014  min_lr: 0.000000  loss: 1.7342 (1.7122)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4167 (8.4847)  time: 1.0047 (0.5300 -- 4.1762)  data: 0.4624 (0.0004 -- 3.6271)  max mem: 16413
[2023-08-31 16:25:16,434] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20533
[2023-08-31 16:25:16,434] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20533
[2023-08-31 16:25:16,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:25:16,435] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:25:16,435] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [128]  [ 60/160]  eta: 0:01:40  lr: 0.000014  min_lr: 0.000000  loss: 1.6816 (1.7109)  loss_scale: 16384.0000 (15309.6393)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1633 (8.3156)  time: 0.8324 (0.5002 -- 3.9990)  data: 0.2872 (0.0003 -- 3.4680)  max mem: 16413
Epoch: [128]  [ 80/160]  eta: 0:01:20  lr: 0.000014  min_lr: 0.000000  loss: 1.6375 (1.6923)  loss_scale: 8192.0000 (13552.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5231 (8.7669)  time: 0.9872 (0.5122 -- 4.2393)  data: 0.4508 (0.0002 -- 3.6950)  max mem: 16413
Epoch: [128]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 1.8836 (1.7179)  loss_scale: 8192.0000 (12490.7723)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5143 (8.7617)  time: 0.7276 (0.5272 -- 2.3604)  data: 0.1783 (0.0004 -- 1.8399)  max mem: 16413
Epoch: [128]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 1.7768 (1.7402)  loss_scale: 8192.0000 (11780.2314)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5670 (8.7992)  time: 0.9227 (0.5109 -- 3.9002)  data: 0.3711 (0.0003 -- 3.3628)  max mem: 16413
Epoch: [128]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.7651 (1.7350)  loss_scale: 8192.0000 (11271.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7219 (8.7251)  time: 0.7975 (0.5248 -- 2.7896)  data: 0.2241 (0.0003 -- 2.2425)  max mem: 16413
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.5520 (1.7292)  loss_scale: 8192.0000 (10905.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1738 (8.6628)  time: 0.6750 (0.4940 -- 2.7627)  data: 0.1516 (0.0001 -- 2.1919)  max mem: 16413
Epoch: [128] Total time: 0:02:23 (0.8943 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.5520 (1.7192)  loss_scale: 8192.0000 (10905.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1738 (8.6628)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1989 (0.1989)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5189 (2.5189 -- 2.5189)  data: 2.3078 (2.3078 -- 2.3078)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4704 (0.6452)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4322 (0.2036 -- 2.5189)  data: 0.2175 (0.0008 -- 2.3078)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4358 (0.5720)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2165 (0.1690 -- 0.3104)  data: 0.0108 (0.0001 -- 0.1269)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4811 (0.6317)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.1988 (0.1332 -- 0.3104)  data: 0.0105 (0.0001 -- 0.1269)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 83.817 Acc@5 97.510 loss 0.626
Accuracy of the network on the 482 val images: 83.82%
Max accuracy: 85.48%
Epoch: [129]  [  0/160]  eta: 0:19:47  lr: 0.000014  min_lr: 0.000000  loss: 0.9417 (0.9417)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3551 (12.3551)  time: 7.4210 (7.4210 -- 7.4210)  data: 6.8248 (6.8248 -- 6.8248)  max mem: 16413
Epoch: [129]  [ 20/160]  eta: 0:02:44  lr: 0.000014  min_lr: 0.000000  loss: 1.8399 (1.7952)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3782 (8.8928)  time: 0.8648 (0.5278 -- 3.5426)  data: 0.3193 (0.0002 -- 3.0004)  max mem: 16413
[2023-08-31 16:27:17,936] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:27:17,936] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:27:17,936] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:27:17,937] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:27:35,938] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20680
[2023-08-31 16:27:35,938] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20680
[2023-08-31 16:27:35,938] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:27:35,938] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:27:35,938] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [129]  [ 40/160]  eta: 0:02:08  lr: 0.000014  min_lr: 0.000000  loss: 1.8072 (1.7997)  loss_scale: 16384.0000 (11788.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7250 (8.8236)  time: 0.9543 (0.5215 -- 2.9521)  data: 0.2593 (0.0005 -- 2.4187)  max mem: 16413
Epoch: [129]  [ 60/160]  eta: 0:01:42  lr: 0.000014  min_lr: 0.000000  loss: 1.6987 (1.7827)  loss_scale: 8192.0000 (10609.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8700 (9.0851)  time: 0.9271 (0.5185 -- 2.2011)  data: 0.3433 (0.0003 -- 1.6662)  max mem: 16413
Epoch: [129]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000000  loss: 1.7133 (1.7699)  loss_scale: 8192.0000 (10012.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2621 (8.9080)  time: 0.7551 (0.5199 -- 2.2744)  data: 0.2116 (0.0002 -- 1.7535)  max mem: 16413
Epoch: [129]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 1.6496 (1.7576)  loss_scale: 8192.0000 (9651.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4938 (8.9787)  time: 0.9190 (0.5135 -- 4.3810)  data: 0.3731 (0.0003 -- 3.8725)  max mem: 16413
Epoch: [129]  [120/160]  eta: 0:00:37  lr: 0.000014  min_lr: 0.000000  loss: 1.8264 (1.7736)  loss_scale: 8192.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3647 (9.0538)  time: 0.8282 (0.5213 -- 2.1548)  data: 0.2827 (0.0001 -- 1.6448)  max mem: 16413
Epoch: [129]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.7041 (1.7720)  loss_scale: 8192.0000 (9237.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4476 (9.0085)  time: 0.9076 (0.5295 -- 2.4296)  data: 0.3618 (0.0003 -- 1.9087)  max mem: 16413
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7669 (1.7581)  loss_scale: 8192.0000 (9113.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2053 (8.9422)  time: 0.6484 (0.4945 -- 1.9980)  data: 0.1349 (0.0002 -- 1.4829)  max mem: 16413
Epoch: [129] Total time: 0:02:22 (0.8937 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.7669 (1.7507)  loss_scale: 8192.0000 (9113.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2053 (8.9422)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.2072 (0.2072)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5846 (2.5846 -- 2.5846)  data: 2.3264 (2.3264 -- 2.3264)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4282 (0.6410)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4426 (0.1985 -- 2.5846)  data: 0.2201 (0.0009 -- 2.3264)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4282 (0.5724)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2177 (0.1704 -- 0.3481)  data: 0.0137 (0.0001 -- 0.1753)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5161 (0.6226)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.1992 (0.1323 -- 0.3481)  data: 0.0133 (0.0001 -- 0.1753)  max mem: 16413
Val: Total time: 0:00:07 (0.2935 s / it)
* Acc@1 84.232 Acc@5 97.718 loss 0.625
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.48%
Epoch: [130]  [  0/160]  eta: 0:17:05  lr: 0.000013  min_lr: 0.000000  loss: 1.7664 (1.7664)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8271 (9.8271)  time: 6.4093 (6.4093 -- 6.4093)  data: 5.8555 (5.8555 -- 5.8555)  max mem: 16413
[2023-08-31 16:29:39,005] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:29:39,005] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:29:39,005] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:29:39,005] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [130]  [ 20/160]  eta: 0:02:47  lr: 0.000013  min_lr: 0.000000  loss: 1.8783 (1.7388)  loss_scale: 16384.0000 (12873.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1897 (7.7005)  time: 0.9337 (0.5281 -- 3.6515)  data: 0.2345 (0.0005 -- 3.1100)  max mem: 16413
Epoch: [130]  [ 40/160]  eta: 0:02:11  lr: 0.000013  min_lr: 0.000000  loss: 1.8389 (1.7720)  loss_scale: 16384.0000 (14585.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4723 (8.1621)  time: 0.9893 (0.5245 -- 4.5824)  data: 0.4396 (0.0002 -- 4.0486)  max mem: 16413
Epoch: [130]  [ 60/160]  eta: 0:01:41  lr: 0.000013  min_lr: 0.000000  loss: 1.7198 (1.7149)  loss_scale: 16384.0000 (15175.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0344 (8.1113)  time: 0.8638 (0.5244 -- 3.5597)  data: 0.3029 (0.0002 -- 3.0372)  max mem: 16413
Epoch: [130]  [ 80/160]  eta: 0:01:18  lr: 0.000013  min_lr: 0.000000  loss: 1.6104 (1.7035)  loss_scale: 16384.0000 (15473.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2560 (8.2259)  time: 0.8818 (0.5255 -- 4.0472)  data: 0.3405 (0.0004 -- 3.5353)  max mem: 16413
[2023-08-31 16:30:52,679] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20891
[2023-08-31 16:30:52,679] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20891
[2023-08-31 16:30:52,680] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:30:52,680] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:30:52,680] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [130]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000000  loss: 1.7396 (1.7189)  loss_scale: 8192.0000 (14842.9307)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0076 (8.3055)  time: 0.8082 (0.5129 -- 4.3054)  data: 0.2678 (0.0005 -- 3.7869)  max mem: 16413
Epoch: [130]  [120/160]  eta: 0:00:38  lr: 0.000013  min_lr: 0.000000  loss: 1.7155 (1.7259)  loss_scale: 8192.0000 (13743.6033)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4907 (8.5564)  time: 1.0326 (0.5359 -- 4.6411)  data: 0.4814 (0.0003 -- 4.0946)  max mem: 16413
Epoch: [130]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.5612 (1.7131)  loss_scale: 8192.0000 (12956.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8232 (8.7100)  time: 0.7849 (0.5088 -- 3.4695)  data: 0.2410 (0.0003 -- 2.9017)  max mem: 16413
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.5675 (1.6995)  loss_scale: 8192.0000 (12390.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8930 (8.7761)  time: 0.6242 (0.4956 -- 2.6802)  data: 0.1081 (0.0002 -- 2.1515)  max mem: 16413
Epoch: [130] Total time: 0:02:24 (0.9018 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.5675 (1.7361)  loss_scale: 8192.0000 (12390.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8930 (8.7761)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2037 (0.2037)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3460 (2.3460 -- 2.3460)  data: 2.1128 (2.1128 -- 2.1128)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4125 (0.6461)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4246 (0.2056 -- 2.3460)  data: 0.2076 (0.0007 -- 2.1128)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4178 (0.5707)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2243 (0.1687 -- 0.3706)  data: 0.0182 (0.0001 -- 0.1911)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4895 (0.6285)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (96.6805)  time: 0.2081 (0.1332 -- 0.3706)  data: 0.0179 (0.0001 -- 0.1911)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 83.402 Acc@5 97.510 loss 0.625
Accuracy of the network on the 482 val images: 83.40%
Max accuracy: 85.48%
Epoch: [131]  [  0/160]  eta: 0:21:41  lr: 0.000013  min_lr: 0.000000  loss: 1.2004 (1.2004)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7992 (6.7992)  time: 8.1350 (8.1350 -- 8.1350)  data: 7.5941 (7.5941 -- 7.5941)  max mem: 16413
Epoch: [131]  [ 20/160]  eta: 0:02:52  lr: 0.000013  min_lr: 0.000000  loss: 1.6578 (1.6519)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4537 (8.5875)  time: 0.8892 (0.5256 -- 3.7310)  data: 0.1931 (0.0005 -- 2.5276)  max mem: 16413
[2023-08-31 16:32:36,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=121, lr=[3.0888433446672044e-07, 3.0888433446672044e-07, 4.118457792889606e-07, 4.118457792889606e-07, 5.491277057186142e-07, 5.491277057186142e-07, 7.321702742914855e-07, 7.321702742914855e-07, 9.762270323886474e-07, 9.762270323886474e-07, 1.3016360431848632e-06, 1.3016360431848632e-06, 1.7355147242464842e-06, 1.7355147242464842e-06, 2.3140196323286457e-06, 2.3140196323286457e-06, 3.0853595097715273e-06, 3.0853595097715273e-06, 4.11381267969537e-06, 4.11381267969537e-06, 5.48508357292716e-06, 5.48508357292716e-06, 7.3134447639028795e-06, 7.3134447639028795e-06, 9.75125968520384e-06, 9.75125968520384e-06, 1.3001679580271786e-05, 1.3001679580271786e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 16:32:36,920] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=16.788430840767965, CurrSamplesPerSec=22.653199113885197, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [131]  [ 40/160]  eta: 0:02:15  lr: 0.000013  min_lr: 0.000000  loss: 1.7897 (1.7238)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6303 (9.0523)  time: 1.0194 (0.5199 -- 4.5498)  data: 0.3366 (0.0004 -- 2.5735)  max mem: 16413
[2023-08-31 16:32:54,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:32:54,876] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:32:54,878] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:32:54,878] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [131]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000000  loss: 1.7286 (1.7197)  loss_scale: 8192.0000 (8326.2951)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1073 (9.1345)  time: 0.6706 (0.5113 -- 2.8649)  data: 0.1182 (0.0005 -- 2.3398)  max mem: 16413
[2023-08-31 16:32:59,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21026
[2023-08-31 16:32:59,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21026
[2023-08-31 16:32:59,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:32:59,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:32:59,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [131]  [ 80/160]  eta: 0:01:18  lr: 0.000013  min_lr: 0.000000  loss: 1.6412 (1.7154)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6099 (9.0136)  time: 0.9647 (0.5226 -- 4.3465)  data: 0.3937 (0.0007 -- 3.6269)  max mem: 16413
Epoch: [131]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.7858 (1.7218)  loss_scale: 8192.0000 (8678.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8199 (9.0124)  time: 0.7135 (0.5256 -- 3.8822)  data: 0.1684 (0.0003 -- 3.3447)  max mem: 16413
Epoch: [131]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.6493 (1.7058)  loss_scale: 8192.0000 (8598.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1623 (8.9907)  time: 0.9219 (0.5201 -- 3.7705)  data: 0.3704 (0.0004 -- 3.2267)  max mem: 16413
Epoch: [131]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.5570 (1.6997)  loss_scale: 8192.0000 (8540.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2235 (8.9552)  time: 0.8694 (0.5296 -- 3.7022)  data: 0.2857 (0.0003 -- 3.1871)  max mem: 16413
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.5408 (1.6790)  loss_scale: 8192.0000 (8499.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4303 (9.0780)  time: 0.7068 (0.4955 -- 3.0436)  data: 0.1835 (0.0002 -- 2.5028)  max mem: 16413
Epoch: [131] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.5408 (1.7018)  loss_scale: 8192.0000 (8499.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4303 (9.0780)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2022 (0.2022)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3434 (2.3434 -- 2.3434)  data: 2.0839 (2.0839 -- 2.0839)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3902 (0.6392)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4263 (0.2033 -- 2.3434)  data: 0.2056 (0.0007 -- 2.0839)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4203 (0.5738)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2282 (0.1693 -- 0.4487)  data: 0.0221 (0.0001 -- 0.2612)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5249 (0.6295)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (96.6805)  time: 0.2118 (0.1322 -- 0.4487)  data: 0.0217 (0.0001 -- 0.2612)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 83.195 Acc@5 97.510 loss 0.616
Accuracy of the network on the 482 val images: 83.20%
Max accuracy: 85.48%
Epoch: [132]  [  0/160]  eta: 0:16:47  lr: 0.000013  min_lr: 0.000000  loss: 1.5026 (1.5026)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0656 (13.0656)  time: 6.2970 (6.2970 -- 6.2970)  data: 4.5929 (4.5929 -- 4.5929)  max mem: 16413
Epoch: [132]  [ 20/160]  eta: 0:02:42  lr: 0.000013  min_lr: 0.000000  loss: 1.7237 (1.6927)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6899 (8.9411)  time: 0.9023 (0.5390 -- 2.4883)  data: 0.2465 (0.0007 -- 1.9700)  max mem: 16413
[2023-08-31 16:35:05,074] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:35:05,074] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:35:05,074] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:35:05,075] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [132]  [ 40/160]  eta: 0:02:03  lr: 0.000013  min_lr: 0.000000  loss: 1.7870 (1.7440)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0879 (8.6661)  time: 0.8888 (0.5255 -- 2.2333)  data: 0.2937 (0.0003 -- 1.6668)  max mem: 16413
Epoch: [132]  [ 60/160]  eta: 0:01:42  lr: 0.000013  min_lr: 0.000000  loss: 1.7176 (1.7323)  loss_scale: 16384.0000 (11683.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8398 (8.6039)  time: 1.0164 (0.5104 -- 4.7014)  data: 0.4742 (0.0004 -- 4.1793)  max mem: 16413
Epoch: [132]  [ 80/160]  eta: 0:01:15  lr: 0.000013  min_lr: 0.000000  loss: 1.8947 (1.7525)  loss_scale: 16384.0000 (12844.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9305 (8.6076)  time: 0.7196 (0.5242 -- 2.3719)  data: 0.1755 (0.0002 -- 1.8551)  max mem: 16413
[2023-08-31 16:35:49,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21209
[2023-08-31 16:35:49,824] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:35:49,824] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21209
[2023-08-31 16:35:49,825] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:35:49,825] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [132]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.6899 (1.7346)  loss_scale: 8192.0000 (12571.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2982 (8.5176)  time: 0.8260 (0.5230 -- 2.7622)  data: 0.2136 (0.0005 -- 2.2306)  max mem: 16413
Epoch: [132]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.7006 (1.7260)  loss_scale: 8192.0000 (11847.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7237 (8.5163)  time: 0.9623 (0.5189 -- 2.4519)  data: 0.1752 (0.0004 -- 1.9398)  max mem: 16413
Epoch: [132]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8640 (1.7430)  loss_scale: 8192.0000 (11329.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0847 (8.5423)  time: 0.8067 (0.5189 -- 3.0122)  data: 0.0013 (0.0003 -- 0.0040)  max mem: 16413
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.6149 (1.7475)  loss_scale: 8192.0000 (10956.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9416 (8.5892)  time: 0.7347 (0.4959 -- 1.7101)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [132] Total time: 0:02:22 (0.8935 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.6149 (1.7111)  loss_scale: 8192.0000 (10956.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9416 (8.5892)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.1833 (0.1833)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6966 (2.6966 -- 2.6966)  data: 2.4856 (2.4856 -- 2.4856)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4078 (0.6494)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4484 (0.1967 -- 2.6966)  data: 0.2335 (0.0005 -- 2.4856)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4078 (0.5682)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (96.8254)  time: 0.2093 (0.1694 -- 0.3210)  data: 0.0043 (0.0001 -- 0.0683)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5184 (0.6255)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (96.2656)  time: 0.1945 (0.1329 -- 0.3210)  data: 0.0038 (0.0001 -- 0.0683)  max mem: 16413
Val: Total time: 0:00:07 (0.2914 s / it)
* Acc@1 85.062 Acc@5 97.303 loss 0.610
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.48%
Epoch: [133]  [  0/160]  eta: 0:17:01  lr: 0.000012  min_lr: 0.000000  loss: 1.9070 (1.9070)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6843 (9.6843)  time: 6.3852 (6.3852 -- 6.3852)  data: 5.6702 (5.6702 -- 5.6702)  max mem: 16413
Epoch: [133]  [ 20/160]  eta: 0:02:49  lr: 0.000012  min_lr: 0.000000  loss: 1.5303 (1.5538)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8134 (8.3070)  time: 0.9489 (0.5212 -- 3.4052)  data: 0.1890 (0.0004 -- 1.6715)  max mem: 16413
Epoch: [133]  [ 40/160]  eta: 0:02:08  lr: 0.000012  min_lr: 0.000000  loss: 1.5258 (1.5216)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8415 (8.1394)  time: 0.9304 (0.5208 -- 3.3918)  data: 0.0525 (0.0004 -- 0.5818)  max mem: 16413
[2023-08-31 16:37:55,687] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:37:55,688] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:37:55,688] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:37:55,688] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [133]  [ 60/160]  eta: 0:01:42  lr: 0.000012  min_lr: 0.000000  loss: 1.8970 (1.6309)  loss_scale: 8192.0000 (8594.8852)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2977 (8.1981)  time: 0.9357 (0.5198 -- 3.4288)  data: 0.0014 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [133]  [ 80/160]  eta: 0:01:18  lr: 0.000012  min_lr: 0.000000  loss: 1.9091 (1.6867)  loss_scale: 16384.0000 (10518.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5615 (8.4468)  time: 0.8258 (0.5236 -- 3.4223)  data: 0.0016 (0.0005 -- 0.0052)  max mem: 16413
Epoch: [133]  [100/160]  eta: 0:00:57  lr: 0.000012  min_lr: 0.000000  loss: 1.6099 (1.6868)  loss_scale: 16384.0000 (11679.6832)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7551 (8.4096)  time: 0.8608 (0.5162 -- 4.2145)  data: 0.0013 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [133]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.7154 (1.6958)  loss_scale: 16384.0000 (12457.2562)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3577 (8.4984)  time: 0.8122 (0.5235 -- 4.5652)  data: 0.0177 (0.0005 -- 0.3133)  max mem: 16413
Epoch: [133]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.5892 (1.6839)  loss_scale: 16384.0000 (13014.2411)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6408 (8.4267)  time: 0.9077 (0.5237 -- 4.4353)  data: 0.0012 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7175 (1.6796)  loss_scale: 16384.0000 (13414.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1814 (8.5046)  time: 0.7335 (0.4954 -- 2.8412)  data: 0.0017 (0.0002 -- 0.0157)  max mem: 16413
Epoch: [133] Total time: 0:02:23 (0.8962 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7175 (1.7088)  loss_scale: 16384.0000 (13414.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1814 (8.5046)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1822 (0.1822)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3916 (2.3916 -- 2.3916)  data: 2.1459 (2.1459 -- 2.1459)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3496 (0.6359)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4282 (0.2126 -- 2.3916)  data: 0.2008 (0.0004 -- 2.1459)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4527 (0.5628)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2222 (0.1688 -- 0.3930)  data: 0.0139 (0.0001 -- 0.2133)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4611 (0.6172)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2022 (0.1334 -- 0.3930)  data: 0.0136 (0.0001 -- 0.2133)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 84.647 Acc@5 97.718 loss 0.605
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.48%
Epoch: [134]  [  0/160]  eta: 0:21:47  lr: 0.000012  min_lr: 0.000000  loss: 2.3061 (2.3061)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7026 (11.7026)  time: 8.1696 (8.1696 -- 8.1696)  data: 5.9025 (5.9025 -- 5.9025)  max mem: 16413
Epoch: [134]  [ 20/160]  eta: 0:02:40  lr: 0.000012  min_lr: 0.000000  loss: 1.6888 (1.7473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1022 (9.7442)  time: 0.7944 (0.5151 -- 3.3022)  data: 0.1721 (0.0006 -- 2.0973)  max mem: 16413
[2023-08-31 16:39:58,104] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:39:58,104] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:39:58,105] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:39:58,105] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [134]  [ 40/160]  eta: 0:02:06  lr: 0.000012  min_lr: 0.000000  loss: 1.7524 (1.7136)  loss_scale: 32768.0000 (22378.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5669 (9.2128)  time: 0.9553 (0.5195 -- 2.9516)  data: 0.0498 (0.0004 -- 0.5957)  max mem: 16413
[2023-08-31 16:40:29,639] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21500
[2023-08-31 16:40:29,639] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21500
[2023-08-31 16:40:29,639] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:40:29,639] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:40:29,639] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [ 60/160]  eta: 0:01:41  lr: 0.000012  min_lr: 0.000000  loss: 1.6373 (1.6896)  loss_scale: 32768.0000 (25516.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6717 (8.8374)  time: 0.9333 (0.5130 -- 4.1808)  data: 0.0009 (0.0003 -- 0.0015)  max mem: 16413
Epoch: [134]  [ 80/160]  eta: 0:01:17  lr: 0.000012  min_lr: 0.000000  loss: 1.6522 (1.6782)  loss_scale: 16384.0000 (23261.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5857 (9.1136)  time: 0.8260 (0.5233 -- 3.0986)  data: 0.0015 (0.0003 -- 0.0033)  max mem: 16413
[2023-08-31 16:40:50,466] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21527
[2023-08-31 16:40:50,466] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21527
[2023-08-31 16:40:50,466] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:40:50,467] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:40:50,467] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [134]  [100/160]  eta: 0:00:57  lr: 0.000012  min_lr: 0.000000  loss: 1.6823 (1.6711)  loss_scale: 8192.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2566 (9.1718)  time: 0.9224 (0.5166 -- 3.5733)  data: 0.0013 (0.0001 -- 0.0054)  max mem: 16413
Epoch: [134]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.6538 (1.6545)  loss_scale: 8192.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9592 (9.3669)  time: 0.7647 (0.5205 -- 2.8809)  data: 0.0012 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [134]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8505 (1.6804)  loss_scale: 8192.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4612 (9.2619)  time: 0.9848 (0.5221 -- 4.4396)  data: 0.0013 (0.0004 -- 0.0051)  max mem: 16413
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.5534 (1.6706)  loss_scale: 8192.0000 (16128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2429 (9.1680)  time: 0.6628 (0.4949 -- 2.1721)  data: 0.0004 (0.0002 -- 0.0009)  max mem: 16413
Epoch: [134] Total time: 0:02:24 (0.9033 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.5534 (1.6688)  loss_scale: 8192.0000 (16128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2429 (9.1680)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1700 (0.1700)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5729 (2.5729 -- 2.5729)  data: 2.3488 (2.3488 -- 2.3488)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3667 (0.6448)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4341 (0.1943 -- 2.5729)  data: 0.2171 (0.0006 -- 2.3488)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4137 (0.5712)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2148 (0.1701 -- 0.3438)  data: 0.0099 (0.0001 -- 0.1555)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4722 (0.6258)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.1982 (0.1329 -- 0.3438)  data: 0.0090 (0.0001 -- 0.1555)  max mem: 16413
Val: Total time: 0:00:07 (0.2909 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.607
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.48%
Epoch: [135]  [  0/160]  eta: 0:18:57  lr: 0.000012  min_lr: 0.000000  loss: 1.3503 (1.3503)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0352 (7.0352)  time: 7.1114 (7.1114 -- 7.1114)  data: 6.5517 (6.5517 -- 6.5517)  max mem: 16413
Epoch: [135]  [ 20/160]  eta: 0:02:42  lr: 0.000012  min_lr: 0.000000  loss: 1.6387 (1.6656)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9035 (11.9772)  time: 0.8604 (0.5223 -- 3.1511)  data: 0.2750 (0.0003 -- 2.6040)  max mem: 16413
Epoch: [135]  [ 40/160]  eta: 0:02:02  lr: 0.000012  min_lr: 0.000000  loss: 1.6923 (1.6915)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0615 (10.5269)  time: 0.8804 (0.5131 -- 2.9847)  data: 0.2465 (0.0003 -- 2.4722)  max mem: 16413
[2023-08-31 16:42:55,546] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:42:55,546] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:42:55,546] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:42:55,546] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [135]  [ 60/160]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 1.6443 (1.6900)  loss_scale: 8192.0000 (8863.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0367 (10.0504)  time: 0.8836 (0.5298 -- 2.5727)  data: 0.1135 (0.0003 -- 1.3422)  max mem: 16413
Epoch: [135]  [ 80/160]  eta: 0:01:17  lr: 0.000012  min_lr: 0.000000  loss: 1.8139 (1.7088)  loss_scale: 16384.0000 (10720.3951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6174 (9.7560)  time: 0.9463 (0.5343 -- 5.0486)  data: 0.0014 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [135]  [100/160]  eta: 0:00:56  lr: 0.000012  min_lr: 0.000000  loss: 1.6925 (1.7031)  loss_scale: 16384.0000 (11841.9010)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3468 (9.4395)  time: 0.8571 (0.5200 -- 4.1137)  data: 0.0015 (0.0003 -- 0.0059)  max mem: 16413
Epoch: [135]  [120/160]  eta: 0:00:37  lr: 0.000012  min_lr: 0.000000  loss: 1.7346 (1.7064)  loss_scale: 16384.0000 (12592.6612)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3144 (9.2141)  time: 0.8277 (0.5248 -- 5.1363)  data: 0.0055 (0.0002 -- 0.0804)  max mem: 16413
Epoch: [135]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.4798 (1.6892)  loss_scale: 16384.0000 (13130.4397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3946 (9.1702)  time: 0.8424 (0.5286 -- 3.7291)  data: 0.0234 (0.0003 -- 0.4413)  max mem: 16413
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7427 (1.6962)  loss_scale: 16384.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5211 (9.0479)  time: 0.6956 (0.4949 -- 3.1728)  data: 0.0014 (0.0002 -- 0.0105)  max mem: 16413
Epoch: [135] Total time: 0:02:22 (0.8904 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7427 (1.7113)  loss_scale: 16384.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5211 (9.0479)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1787 (0.1787)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5888 (2.5888 -- 2.5888)  data: 2.3432 (2.3432 -- 2.3432)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3300 (0.6341)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4481 (0.2014 -- 2.5888)  data: 0.2322 (0.0009 -- 2.3432)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4493 (0.5731)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2132 (0.1696 -- 0.4103)  data: 0.0114 (0.0001 -- 0.2009)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4708 (0.6259)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.0954)  time: 0.1986 (0.1333 -- 0.4103)  data: 0.0111 (0.0001 -- 0.2009)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 84.025 Acc@5 97.510 loss 0.604
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.48%
Epoch: [136]  [  0/160]  eta: 0:17:27  lr: 0.000011  min_lr: 0.000000  loss: 2.2310 (2.2310)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1675 (11.1675)  time: 6.5495 (6.5495 -- 6.5495)  data: 6.0093 (6.0093 -- 6.0093)  max mem: 16413
Epoch: [136]  [ 20/160]  eta: 0:02:42  lr: 0.000011  min_lr: 0.000000  loss: 1.6894 (1.7109)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9707 (9.0778)  time: 0.8876 (0.5296 -- 2.5571)  data: 0.1957 (0.0007 -- 1.8533)  max mem: 16413
[2023-08-31 16:44:57,441] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:44:57,441] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:44:57,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:44:57,442] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:45:09,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21797
[2023-08-31 16:45:09,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:45:09,752] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 16:45:09,752] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21797
[2023-08-31 16:45:09,752] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [136]  [ 40/160]  eta: 0:02:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7130 (1.7206)  loss_scale: 32768.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7947 (8.4121)  time: 0.8466 (0.5242 -- 2.8728)  data: 0.1023 (0.0004 -- 1.2511)  max mem: 16413
Epoch: [136]  [ 60/160]  eta: 0:01:36  lr: 0.000011  min_lr: 0.000000  loss: 1.8631 (1.7630)  loss_scale: 16384.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5844 (8.2895)  time: 0.8920 (0.5184 -- 3.0552)  data: 0.2723 (0.0004 -- 2.5180)  max mem: 16413
Epoch: [136]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000000  loss: 1.5769 (1.7294)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9003 (8.2252)  time: 0.9182 (0.5229 -- 3.4467)  data: 0.1270 (0.0006 -- 2.2835)  max mem: 16413
Epoch: [136]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 1.6740 (1.7023)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5065 (8.5102)  time: 0.8105 (0.5354 -- 2.5074)  data: 0.0604 (0.0003 -- 1.1227)  max mem: 16413
[2023-08-31 16:46:20,549] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21877
[2023-08-31 16:46:20,549] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21877
[2023-08-31 16:46:20,550] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:46:20,550] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:46:20,550] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [136]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.8875 (1.7314)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1741 (8.7302)  time: 0.9177 (0.5294 -- 3.2940)  data: 0.3705 (0.0005 -- 2.7732)  max mem: 16413
Epoch: [136]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.6643 (1.7184)  loss_scale: 8192.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2919 (8.7568)  time: 0.7465 (0.5343 -- 2.4318)  data: 0.1777 (0.0002 -- 1.8932)  max mem: 16413
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.6933 (1.7220)  loss_scale: 8192.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3256 (8.8932)  time: 0.7612 (0.4958 -- 3.0401)  data: 0.1514 (0.0002 -- 2.4928)  max mem: 16413
Epoch: [136] Total time: 0:02:21 (0.8850 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.6933 (1.7169)  loss_scale: 8192.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3256 (8.8932)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1815 (0.1815)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4601 (2.4601 -- 2.4601)  data: 2.2543 (2.2543 -- 2.2543)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3747 (0.6392)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4297 (0.1990 -- 2.4601)  data: 0.2107 (0.0008 -- 2.2543)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4626 (0.5711)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2193 (0.1682 -- 0.3387)  data: 0.0105 (0.0001 -- 0.1433)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4762 (0.6242)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2046 (0.1329 -- 0.3387)  data: 0.0102 (0.0001 -- 0.1433)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 85.062 Acc@5 97.718 loss 0.605
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.48%
Epoch: [137]  [  0/160]  eta: 0:21:20  lr: 0.000011  min_lr: 0.000000  loss: 2.0854 (2.0854)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1883 (7.1883)  time: 8.0047 (8.0047 -- 8.0047)  data: 7.4702 (7.4702 -- 7.4702)  max mem: 16413
Epoch: [137]  [ 20/160]  eta: 0:02:49  lr: 0.000011  min_lr: 0.000000  loss: 1.6766 (1.7884)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0813 (9.0842)  time: 0.8713 (0.5233 -- 3.2606)  data: 0.2394 (0.0005 -- 2.6927)  max mem: 16413
Epoch: [137]  [ 40/160]  eta: 0:02:09  lr: 0.000011  min_lr: 0.000000  loss: 1.5657 (1.7657)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8453 (8.5912)  time: 0.9335 (0.5219 -- 2.5740)  data: 0.2464 (0.0003 -- 2.0501)  max mem: 16413
Epoch: [137]  [ 60/160]  eta: 0:01:40  lr: 0.000011  min_lr: 0.000000  loss: 1.6348 (1.7456)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4633 (8.9039)  time: 0.8618 (0.5087 -- 2.7323)  data: 0.0479 (0.0003 -- 0.9400)  max mem: 16413
[2023-08-31 16:48:16,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=127, lr=[2.6011623907340286e-07, 2.6011623907340286e-07, 3.468216520978705e-07, 3.468216520978705e-07, 4.6242886946382737e-07, 4.6242886946382737e-07, 6.165718259517698e-07, 6.165718259517698e-07, 8.220957679356931e-07, 8.220957679356931e-07, 1.096127690580924e-06, 1.096127690580924e-06, 1.461503587441232e-06, 1.461503587441232e-06, 1.9486714499216427e-06, 1.9486714499216427e-06, 2.598228599895524e-06, 2.598228599895524e-06, 3.4643047998606983e-06, 3.4643047998606983e-06, 4.619073066480931e-06, 4.619073066480931e-06, 6.158764088641242e-06, 6.158764088641242e-06, 8.211685451521655e-06, 8.211685451521655e-06, 1.0948913935362207e-05, 1.0948913935362207e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 16:48:16,075] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=16.78779914417245, CurrSamplesPerSec=22.180143758584993, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [137]  [ 80/160]  eta: 0:01:18  lr: 0.000011  min_lr: 0.000000  loss: 1.6147 (1.7326)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4930 (8.4933)  time: 0.8890 (0.5346 -- 3.4085)  data: 0.1503 (0.0004 -- 2.4477)  max mem: 16413
[2023-08-31 16:48:23,203] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:48:23,203] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:48:23,204] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:48:23,205] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [137]  [100/160]  eta: 0:00:57  lr: 0.000011  min_lr: 0.000000  loss: 1.8233 (1.7442)  loss_scale: 16384.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6769 (8.5965)  time: 0.8454 (0.5391 -- 2.1780)  data: 0.1150 (0.0003 -- 1.4484)  max mem: 16413
Epoch: [137]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.8595 (1.7442)  loss_scale: 16384.0000 (10561.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9548 (8.6101)  time: 0.8173 (0.5213 -- 2.5517)  data: 0.0902 (0.0005 -- 1.6251)  max mem: 16413
Epoch: [137]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.6506 (1.7311)  loss_scale: 16384.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8671 (8.5449)  time: 0.9001 (0.5222 -- 3.7188)  data: 0.0014 (0.0001 -- 0.0035)  max mem: 16413
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.9200 (1.7449)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8729 (8.5725)  time: 0.6628 (0.4945 -- 3.3136)  data: 0.0008 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [137] Total time: 0:02:23 (0.8946 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.9200 (1.7095)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8729 (8.5725)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1866 (0.1866)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3586 (2.3586 -- 2.3586)  data: 2.0948 (2.0948 -- 2.0948)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3143 (0.6360)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4250 (0.1973 -- 2.3586)  data: 0.2090 (0.0007 -- 2.0948)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4680 (0.5754)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2223 (0.1686 -- 0.3963)  data: 0.0207 (0.0001 -- 0.2073)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4965 (0.6305)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.2070 (0.1330 -- 0.3963)  data: 0.0205 (0.0001 -- 0.2073)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 84.440 Acc@5 97.718 loss 0.612
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.48%
Epoch: [138]  [  0/160]  eta: 0:23:47  lr: 0.000011  min_lr: 0.000000  loss: 1.9142 (1.9142)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4961 (6.4961)  time: 8.9200 (8.9200 -- 8.9200)  data: 8.3665 (8.3665 -- 8.3665)  max mem: 16413
Epoch: [138]  [ 20/160]  eta: 0:02:45  lr: 0.000011  min_lr: 0.000000  loss: 1.6431 (1.6697)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2188 (8.5155)  time: 0.7924 (0.5365 -- 3.3033)  data: 0.2352 (0.0005 -- 2.7699)  max mem: 16413
Epoch: [138]  [ 40/160]  eta: 0:02:03  lr: 0.000011  min_lr: 0.000000  loss: 1.6398 (1.6324)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4710 (8.4171)  time: 0.8695 (0.5262 -- 2.8871)  data: 0.3125 (0.0002 -- 2.3137)  max mem: 16413
[2023-08-31 16:50:24,948] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:50:24,948] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:50:24,948] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:50:24,948] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [138]  [ 60/160]  eta: 0:01:40  lr: 0.000011  min_lr: 0.000000  loss: 1.4307 (1.6147)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4995 (8.7661)  time: 0.9448 (0.5196 -- 2.4865)  data: 0.2273 (0.0002 -- 1.7826)  max mem: 16413
[2023-08-31 16:50:40,329] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22151
[2023-08-31 16:50:40,330] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:50:40,330] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22151
[2023-08-31 16:50:40,330] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:50:40,331] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [138]  [ 80/160]  eta: 0:01:16  lr: 0.000011  min_lr: 0.000000  loss: 1.5567 (1.6180)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7354 (8.8287)  time: 0.8064 (0.5219 -- 3.2351)  data: 0.2580 (0.0005 -- 2.7019)  max mem: 16413
Epoch: [138]  [100/160]  eta: 0:00:54  lr: 0.000011  min_lr: 0.000000  loss: 1.7687 (1.6513)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7894 (8.9862)  time: 0.7692 (0.5314 -- 2.3433)  data: 0.1823 (0.0005 -- 1.8194)  max mem: 16413
Epoch: [138]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.5694 (1.6559)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7230 (9.0539)  time: 0.8762 (0.5258 -- 2.6064)  data: 0.2830 (0.0004 -- 2.0500)  max mem: 16413
Epoch: [138]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.4644 (1.6402)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1561 (9.0024)  time: 0.9370 (0.5273 -- 3.0580)  data: 0.3201 (0.0004 -- 2.5150)  max mem: 16413
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.6778 (1.6378)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8177 (8.9703)  time: 0.6393 (0.4976 -- 2.0181)  data: 0.0853 (0.0002 -- 1.4726)  max mem: 16413
Epoch: [138] Total time: 0:02:21 (0.8819 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.6778 (1.6795)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8177 (8.9703)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1754 (0.1754)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5951 (2.5951 -- 2.5951)  data: 2.3702 (2.3702 -- 2.3702)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3352 (0.6495)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4393 (0.1876 -- 2.5951)  data: 0.2232 (0.0009 -- 2.3702)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4332 (0.5781)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2133 (0.1699 -- 0.3025)  data: 0.0076 (0.0001 -- 0.0737)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4646 (0.6336)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.1974 (0.1321 -- 0.3025)  data: 0.0073 (0.0001 -- 0.0737)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 84.025 Acc@5 97.510 loss 0.614
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.48%
Epoch: [139]  [  0/160]  eta: 0:22:29  lr: 0.000010  min_lr: 0.000000  loss: 1.3485 (1.3485)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9743 (9.9743)  time: 8.4371 (8.4371 -- 8.4371)  data: 5.2671 (5.2671 -- 5.2671)  max mem: 16413
[2023-08-31 16:52:19,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22252
[2023-08-31 16:52:19,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22252
[2023-08-31 16:52:19,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:52:19,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 16:52:19,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [139]  [ 20/160]  eta: 0:02:49  lr: 0.000010  min_lr: 0.000000  loss: 1.6837 (1.6306)  loss_scale: 16384.0000 (12873.1429)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3420 (8.5321)  time: 0.8496 (0.5347 -- 4.6798)  data: 0.0019 (0.0006 -- 0.0075)  max mem: 16413
Epoch: [139]  [ 40/160]  eta: 0:02:09  lr: 0.000010  min_lr: 0.000000  loss: 1.8656 (1.7386)  loss_scale: 8192.0000 (10589.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5130 (8.7883)  time: 0.9342 (0.5273 -- 3.9355)  data: 0.0933 (0.0004 -- 1.8158)  max mem: 16413
Epoch: [139]  [ 60/160]  eta: 0:01:35  lr: 0.000010  min_lr: 0.000000  loss: 1.5561 (1.7129)  loss_scale: 8192.0000 (9803.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1514 (9.0880)  time: 0.7169 (0.5347 -- 2.8162)  data: 0.1438 (0.0004 -- 2.2833)  max mem: 16413
Epoch: [139]  [ 80/160]  eta: 0:01:14  lr: 0.000010  min_lr: 0.000000  loss: 1.6484 (1.6951)  loss_scale: 8192.0000 (9405.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0602 (8.9560)  time: 0.8740 (0.5305 -- 2.3497)  data: 0.2639 (0.0007 -- 1.8214)  max mem: 16413
Epoch: [139]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.8005 (1.7221)  loss_scale: 8192.0000 (9165.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3014 (8.8993)  time: 0.9082 (0.5377 -- 3.0128)  data: 0.1263 (0.0003 -- 1.8556)  max mem: 16413
Epoch: [139]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.6475 (1.7176)  loss_scale: 8192.0000 (9004.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8286 (9.1359)  time: 0.9377 (0.5329 -- 3.7759)  data: 0.0025 (0.0004 -- 0.0171)  max mem: 16413
Epoch: [139]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.7597 (1.7177)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0738 (9.1888)  time: 0.7751 (0.5267 -- 2.3102)  data: 0.0022 (0.0003 -- 0.0149)  max mem: 16413
[2023-08-31 16:54:08,837] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:54:08,837] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 16:54:08,838] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:54:08,838] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7599 (1.7261)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1738 (9.1705)  time: 0.6706 (0.4953 -- 1.7456)  data: 0.0125 (0.0001 -- 0.2353)  max mem: 16413
Epoch: [139] Total time: 0:02:21 (0.8828 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7599 (1.7369)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1738 (9.1705)
[2023-08-31 16:54:21,172] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-08-31 16:54:21,174] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-08-31 16:54:21,174] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-08-31 16:54:21,174] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-08-31 16:54:22,173] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-08-31 16:54:22,173] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1888 (0.1888)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4290 (2.4290 -- 2.4290)  data: 2.2080 (2.2080 -- 2.2080)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3561 (0.6403)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4172 (0.2038 -- 2.4290)  data: 0.2063 (0.0010 -- 2.2080)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4113 (0.5742)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2210 (0.1706 -- 0.5567)  data: 0.0223 (0.0001 -- 0.3820)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5189 (0.6266)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2064 (0.1325 -- 0.5567)  data: 0.0216 (0.0001 -- 0.3820)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 84.440 Acc@5 97.718 loss 0.610
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.48%
Epoch: [140]  [  0/160]  eta: 0:19:07  lr: 0.000010  min_lr: 0.000000  loss: 1.7868 (1.7868)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.7039 (13.7039)  time: 7.1731 (7.1731 -- 7.1731)  data: 6.0571 (6.0571 -- 6.0571)  max mem: 16413
Epoch: [140]  [ 20/160]  eta: 0:02:49  lr: 0.000010  min_lr: 0.000000  loss: 1.8611 (1.8095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8510 (9.0201)  time: 0.9157 (0.5247 -- 3.4683)  data: 0.0435 (0.0004 -- 0.8409)  max mem: 16413
Epoch: [140]  [ 40/160]  eta: 0:02:07  lr: 0.000010  min_lr: 0.000000  loss: 1.7344 (1.7849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2344 (8.9154)  time: 0.9057 (0.5167 -- 4.6661)  data: 0.0016 (0.0003 -- 0.0073)  max mem: 16413
Epoch: [140]  [ 60/160]  eta: 0:01:42  lr: 0.000010  min_lr: 0.000000  loss: 1.9448 (1.8132)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1467 (8.7841)  time: 0.9318 (0.5276 -- 3.8270)  data: 0.0021 (0.0003 -- 0.0124)  max mem: 16413
Epoch: [140]  [ 80/160]  eta: 0:01:16  lr: 0.000010  min_lr: 0.000000  loss: 1.5892 (1.7647)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7379 (9.0185)  time: 0.7623 (0.5296 -- 3.1038)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [140]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 1.6030 (1.7482)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6214 (9.1300)  time: 0.9057 (0.5139 -- 4.8392)  data: 0.0018 (0.0003 -- 0.0120)  max mem: 16413
[2023-08-31 16:56:13,194] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:56:13,195] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:56:13,197] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:56:13,197] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:56:19,048] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22514
[2023-08-31 16:56:19,048] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22514
[2023-08-31 16:56:19,048] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:56:19,048] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:56:19,048] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [140]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.9468 (1.7822)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7914 (9.2368)  time: 0.8219 (0.5235 -- 3.6184)  data: 0.0015 (0.0006 -- 0.0032)  max mem: 16413
Epoch: [140]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.7927 (1.7765)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3096 (9.1957)  time: 0.8349 (0.5356 -- 3.1762)  data: 0.0015 (0.0001 -- 0.0042)  max mem: 16413
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.7166 (1.7750)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2368 (9.1013)  time: 0.7297 (0.4956 -- 4.5666)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [140] Total time: 0:02:22 (0.8929 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.7166 (1.7523)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2368 (9.1013)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1951 (0.1951)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3644 (2.3644 -- 2.3644)  data: 2.1348 (2.1348 -- 2.1348)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3931 (0.6347)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4467 (0.2062 -- 2.3644)  data: 0.2292 (0.0006 -- 2.1348)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4171 (0.5685)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2275 (0.1691 -- 0.6024)  data: 0.0236 (0.0001 -- 0.3760)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5265 (0.6230)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2113 (0.1329 -- 0.6024)  data: 0.0233 (0.0001 -- 0.3760)  max mem: 16413
Val: Total time: 0:00:07 (0.2925 s / it)
* Acc@1 85.685 Acc@5 97.718 loss 0.610
Accuracy of the network on the 482 val images: 85.68%
[2023-08-31 16:57:00,923] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-31 16:57:00,924] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-31 16:57:00,924] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-31 16:57:00,924] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-31 16:57:02,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-31 16:57:02,347] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 85.68%
Epoch: [141]  [  0/160]  eta: 0:26:28  lr: 0.000010  min_lr: 0.000000  loss: 1.2591 (1.2591)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9543 (5.9543)  time: 9.9279 (9.9279 -- 9.9279)  data: 6.3350 (6.3350 -- 6.3350)  max mem: 16413
Epoch: [141]  [ 20/160]  eta: 0:02:53  lr: 0.000010  min_lr: 0.000000  loss: 1.8254 (1.7576)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4598 (8.5118)  time: 0.8045 (0.5338 -- 4.1598)  data: 0.0016 (0.0007 -- 0.0025)  max mem: 16413
Epoch: [141]  [ 40/160]  eta: 0:02:08  lr: 0.000010  min_lr: 0.000000  loss: 1.5538 (1.7135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8987 (8.7023)  time: 0.8876 (0.5122 -- 3.4586)  data: 0.0016 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [141]  [ 60/160]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000000  loss: 1.6651 (1.7054)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1123 (8.9835)  time: 0.8807 (0.5209 -- 3.8747)  data: 0.0413 (0.0003 -- 0.7814)  max mem: 16413
Epoch: [141]  [ 80/160]  eta: 0:01:15  lr: 0.000010  min_lr: 0.000000  loss: 1.6611 (1.7197)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5802 (9.1067)  time: 0.7777 (0.5278 -- 2.9197)  data: 0.0013 (0.0002 -- 0.0020)  max mem: 16413
[2023-08-31 16:58:20,932] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:58:20,932] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:58:20,935] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 16:58:20,936] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 16:58:26,378] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22646
[2023-08-31 16:58:26,378] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22646
[2023-08-31 16:58:26,378] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:58:26,378] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 16:58:26,378] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [141]  [100/160]  eta: 0:00:57  lr: 0.000010  min_lr: 0.000000  loss: 1.7928 (1.7327)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8798 (9.0014)  time: 1.0175 (0.5276 -- 4.3636)  data: 0.0293 (0.0002 -- 0.5661)  max mem: 16413
Epoch: [141]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.6198 (1.7138)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4286 (8.9427)  time: 0.8423 (0.5291 -- 3.8030)  data: 0.0015 (0.0003 -- 0.0052)  max mem: 16413
Epoch: [141]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.5857 (1.6814)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3704 (9.1808)  time: 0.9356 (0.5348 -- 3.7136)  data: 0.0014 (0.0001 -- 0.0044)  max mem: 16413
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.6526 (1.6736)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1731 (9.1028)  time: 0.6905 (0.4950 -- 2.6538)  data: 0.0005 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [141] Total time: 0:02:24 (0.9002 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.6526 (1.6904)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1731 (9.1028)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.1885 (0.1885)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2756 (2.2756 -- 2.2756)  data: 2.0622 (2.0622 -- 2.0622)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4043 (0.6500)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4145 (0.2043 -- 2.2756)  data: 0.1964 (0.0008 -- 2.0622)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4043 (0.5759)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2290 (0.1686 -- 0.5537)  data: 0.0239 (0.0001 -- 0.3782)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5254 (0.6343)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2122 (0.1326 -- 0.5537)  data: 0.0236 (0.0001 -- 0.3782)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 84.440 Acc@5 97.303 loss 0.615
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [142]  [  0/160]  eta: 0:18:31  lr: 0.000010  min_lr: 0.000000  loss: 2.0518 (2.0518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4970 (9.4970)  time: 6.9472 (6.9472 -- 6.9472)  data: 5.7854 (5.7854 -- 5.7854)  max mem: 16413
Epoch: [142]  [ 20/160]  eta: 0:02:43  lr: 0.000010  min_lr: 0.000000  loss: 1.7322 (1.7676)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2106 (8.6715)  time: 0.8777 (0.5256 -- 2.6258)  data: 0.0356 (0.0008 -- 0.5151)  max mem: 16413
Epoch: [142]  [ 40/160]  eta: 0:01:58  lr: 0.000009  min_lr: 0.000000  loss: 1.8444 (1.8000)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4241 (9.1747)  time: 0.8076 (0.5325 -- 3.3325)  data: 0.0186 (0.0002 -- 0.3397)  max mem: 16413
[2023-08-31 17:00:30,114] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:00:30,114] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:00:30,115] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:00:30,115] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:00:31,210] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22777
[2023-08-31 17:00:31,210] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22777
[2023-08-31 17:00:31,210] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:00:31,210] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:00:31,210] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [142]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000000  loss: 1.6947 (1.7729)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0786 (9.0219)  time: 0.9701 (0.5255 -- 2.3594)  data: 0.2264 (0.0001 -- 1.5526)  max mem: 16413
Epoch: [142]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 1.8873 (1.7897)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1595 (9.0491)  time: 0.8758 (0.5278 -- 2.6549)  data: 0.1284 (0.0002 -- 2.1110)  max mem: 16413
Epoch: [142]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.7758 (1.7825)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3889 (9.0828)  time: 0.7734 (0.5278 -- 1.8993)  data: 0.0887 (0.0007 -- 0.8122)  max mem: 16413
Epoch: [142]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.5269 (1.7452)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4405 (9.0943)  time: 0.9337 (0.5220 -- 2.6689)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [142]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.7078 (1.7443)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2365 (9.1685)  time: 0.8444 (0.5201 -- 1.9857)  data: 0.0015 (0.0005 -- 0.0028)  max mem: 16413
[2023-08-31 17:01:56,934] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22878
[2023-08-31 17:01:56,934] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22878
[2023-08-31 17:01:56,934] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:01:56,934] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:01:56,934] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.5868 (1.7238)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7084 (9.2517)  time: 0.7522 (0.4881 -- 3.1510)  data: 0.0007 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [142] Total time: 0:02:23 (0.8947 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.5868 (1.7225)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7084 (9.2517)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1868 (0.1868)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4314 (2.4314 -- 2.4314)  data: 2.1830 (2.1830 -- 2.1830)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3682 (0.6505)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4483 (0.2055 -- 2.4314)  data: 0.2283 (0.0006 -- 2.1830)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4094 (0.5787)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (96.8254)  time: 0.2236 (0.1691 -- 0.5312)  data: 0.0179 (0.0001 -- 0.3146)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5041 (0.6314)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.2077 (0.1330 -- 0.5312)  data: 0.0175 (0.0001 -- 0.3146)  max mem: 16413
Val: Total time: 0:00:07 (0.2923 s / it)
* Acc@1 84.440 Acc@5 97.510 loss 0.614
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [143]  [  0/160]  eta: 0:20:56  lr: 0.000009  min_lr: 0.000000  loss: 1.4473 (1.4473)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3798 (8.3798)  time: 7.8500 (7.8500 -- 7.8500)  data: 6.5164 (6.5164 -- 6.5164)  max mem: 16413
Epoch: [143]  [ 20/160]  eta: 0:02:45  lr: 0.000009  min_lr: 0.000000  loss: 1.6323 (1.6738)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1512 (8.2128)  time: 0.8474 (0.5091 -- 3.5167)  data: 0.1389 (0.0007 -- 2.0005)  max mem: 16413
Epoch: [143]  [ 40/160]  eta: 0:02:07  lr: 0.000009  min_lr: 0.000000  loss: 1.8327 (1.7259)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1930 (8.6260)  time: 0.9306 (0.5216 -- 3.4883)  data: 0.0019 (0.0005 -- 0.0106)  max mem: 16413
Epoch: [143]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000000  loss: 1.5305 (1.6597)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0614 (8.5427)  time: 0.8462 (0.5345 -- 3.6837)  data: 0.0023 (0.0002 -- 0.0102)  max mem: 16413
Epoch: [143]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 1.7590 (1.6917)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3309 (8.4532)  time: 0.8649 (0.5253 -- 3.2258)  data: 0.0020 (0.0006 -- 0.0041)  max mem: 16413
Epoch: [143]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.6458 (1.7028)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5201 (8.5038)  time: 0.8092 (0.5277 -- 2.8246)  data: 0.0569 (0.0005 -- 1.1085)  max mem: 16413
[2023-08-31 17:03:54,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=133, lr=[2.1435939889621148e-07, 2.1435939889621148e-07, 2.858125318616153e-07, 2.858125318616153e-07, 3.810833758154871e-07, 3.810833758154871e-07, 5.081111677539828e-07, 5.081111677539828e-07, 6.774815570053104e-07, 6.774815570053104e-07, 9.033087426737471e-07, 9.033087426737471e-07, 1.2044116568983295e-06, 1.2044116568983295e-06, 1.6058822091977728e-06, 1.6058822091977728e-06, 2.1411762789303637e-06, 2.1411762789303637e-06, 2.8549017052404846e-06, 2.8549017052404846e-06, 3.806535606987313e-06, 3.806535606987313e-06, 5.0753808093164175e-06, 5.0753808093164175e-06, 6.7671744124218894e-06, 6.7671744124218894e-06, 9.02289921656252e-06, 9.02289921656252e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 17:03:54,989] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=16.743207887717357, CurrSamplesPerSec=22.219113992727486, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [143]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.7677 (1.7048)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4592 (8.5824)  time: 0.8189 (0.5315 -- 3.5697)  data: 0.2371 (0.0002 -- 3.0090)  max mem: 16413
[2023-08-31 17:04:01,618] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:04:01,619] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:04:01,618] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:04:01,619] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:04:13,519] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23020
[2023-08-31 17:04:13,519] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23020
[2023-08-31 17:04:13,520] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:04:13,520] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:04:13,520] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [143]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.8432 (1.7241)  loss_scale: 16384.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5316 (8.7465)  time: 0.8984 (0.5124 -- 2.7855)  data: 0.2992 (0.0005 -- 2.2507)  max mem: 16413
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7358 (1.7234)  loss_scale: 8192.0000 (8857.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9508 (8.7868)  time: 0.7419 (0.4940 -- 2.2591)  data: 0.1488 (0.0002 -- 1.7360)  max mem: 16413
Epoch: [143] Total time: 0:02:22 (0.8908 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7358 (1.7199)  loss_scale: 8192.0000 (8857.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9508 (8.7868)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1904 (0.1904)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5443 (2.5443 -- 2.5443)  data: 2.3355 (2.3355 -- 2.3355)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3664 (0.6399)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4329 (0.1983 -- 2.5443)  data: 0.2175 (0.0007 -- 2.3355)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4610 (0.5705)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2172 (0.1699 -- 0.3643)  data: 0.0123 (0.0001 -- 0.1860)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5173 (0.6203)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (96.6805)  time: 0.2024 (0.1325 -- 0.3643)  data: 0.0114 (0.0001 -- 0.1860)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 84.647 Acc@5 97.718 loss 0.606
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [144]  [  0/160]  eta: 0:19:39  lr: 0.000009  min_lr: 0.000000  loss: 0.8345 (0.8345)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.9605 (12.9605)  time: 7.3725 (7.3725 -- 7.3725)  data: 6.5238 (6.5238 -- 6.5238)  max mem: 16413
Epoch: [144]  [ 20/160]  eta: 0:02:38  lr: 0.000009  min_lr: 0.000000  loss: 1.5719 (1.6746)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3489 (8.9800)  time: 0.8203 (0.5356 -- 3.3166)  data: 0.1184 (0.0005 -- 1.1797)  max mem: 16413
Epoch: [144]  [ 40/160]  eta: 0:02:03  lr: 0.000009  min_lr: 0.000000  loss: 1.7451 (1.6756)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8462 (8.7806)  time: 0.9124 (0.5147 -- 2.7489)  data: 0.0847 (0.0004 -- 1.6595)  max mem: 16413
Epoch: [144]  [ 60/160]  eta: 0:01:38  lr: 0.000009  min_lr: 0.000000  loss: 1.8422 (1.7245)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3363 (9.0427)  time: 0.8919 (0.5200 -- 3.1414)  data: 0.0021 (0.0002 -- 0.0072)  max mem: 16413
Epoch: [144]  [ 80/160]  eta: 0:01:14  lr: 0.000009  min_lr: 0.000000  loss: 1.7600 (1.6898)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1954 (8.9917)  time: 0.7929 (0.5303 -- 1.9958)  data: 0.0402 (0.0005 -- 0.4215)  max mem: 16413
Epoch: [144]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.7967 (1.7122)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3927 (8.9925)  time: 0.9664 (0.5223 -- 2.6350)  data: 0.2801 (0.0001 -- 2.0843)  max mem: 16413
[2023-08-31 17:06:18,402] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:06:18,402] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:06:18,402] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:06:18,402] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [144]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.6933 (1.7203)  loss_scale: 16384.0000 (9004.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6299 (8.9005)  time: 0.8244 (0.5283 -- 2.6000)  data: 0.1756 (0.0003 -- 2.0813)  max mem: 16413
Epoch: [144]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.5551 (1.6981)  loss_scale: 16384.0000 (10051.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1865 (8.8993)  time: 0.9852 (0.5191 -- 4.0526)  data: 0.4390 (0.0002 -- 3.5446)  max mem: 16413
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.6208 (1.6856)  loss_scale: 16384.0000 (10803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3956 (8.9277)  time: 0.5601 (0.4951 -- 0.9648)  data: 0.0007 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [144] Total time: 0:02:21 (0.8871 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.6208 (1.6829)  loss_scale: 16384.0000 (10803.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3956 (8.9277)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1821 (0.1821)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3309 (2.3309 -- 2.3309)  data: 2.1141 (2.1141 -- 2.1141)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3578 (0.6381)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4216 (0.1937 -- 2.3309)  data: 0.2049 (0.0007 -- 2.1141)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4132 (0.5750)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (96.8254)  time: 0.2210 (0.1684 -- 0.3359)  data: 0.0139 (0.0001 -- 0.1366)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5276 (0.6241)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.2062 (0.1329 -- 0.3359)  data: 0.0136 (0.0001 -- 0.1366)  max mem: 16413
Val: Total time: 0:00:07 (0.2868 s / it)
* Acc@1 84.232 Acc@5 97.510 loss 0.609
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.68%
Epoch: [145]  [  0/160]  eta: 0:17:38  lr: 0.000009  min_lr: 0.000000  loss: 1.4405 (1.4405)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3679 (7.3679)  time: 6.6173 (6.6173 -- 6.6173)  data: 6.0549 (6.0549 -- 6.0549)  max mem: 16413
Epoch: [145]  [ 20/160]  eta: 0:02:59  lr: 0.000009  min_lr: 0.000000  loss: 1.6467 (1.7222)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5475 (9.1926)  time: 1.0179 (0.5286 -- 3.8896)  data: 0.1709 (0.0004 -- 2.0092)  max mem: 16413
Epoch: [145]  [ 40/160]  eta: 0:02:11  lr: 0.000009  min_lr: 0.000000  loss: 1.8098 (1.7256)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3178 (9.1038)  time: 0.8923 (0.5139 -- 3.5406)  data: 0.0015 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [145]  [ 60/160]  eta: 0:01:42  lr: 0.000009  min_lr: 0.000000  loss: 1.5653 (1.6970)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7662 (9.0201)  time: 0.8753 (0.5150 -- 2.1023)  data: 0.0018 (0.0002 -- 0.0053)  max mem: 16413
[2023-08-31 17:08:20,701] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:08:20,702] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:08:20,704] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:08:20,705] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [145]  [ 80/160]  eta: 0:01:17  lr: 0.000009  min_lr: 0.000000  loss: 1.8703 (1.7310)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2745 (8.8379)  time: 0.8109 (0.5197 -- 2.1960)  data: 0.0015 (0.0004 -- 0.0053)  max mem: 16413
[2023-08-31 17:08:27,670] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23285
[2023-08-31 17:08:27,670] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23285
[2023-08-31 17:08:27,670] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:08:27,670] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:08:27,671] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [145]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.4895 (1.6972)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7749 (8.8738)  time: 0.7965 (0.5213 -- 2.5144)  data: 0.0020 (0.0004 -- 0.0076)  max mem: 16413
[2023-08-31 17:08:47,363] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23308
[2023-08-31 17:08:47,363] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23308
[2023-08-31 17:08:47,364] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:08:47,364] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:08:47,364] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [145]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9003 (1.7167)  loss_scale: 8192.0000 (16587.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3931 (9.0052)  time: 0.9678 (0.5336 -- 3.4576)  data: 0.0353 (0.0005 -- 0.3724)  max mem: 16413
Epoch: [145]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.6267 (1.7172)  loss_scale: 8192.0000 (15396.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8520 (9.0136)  time: 0.8187 (0.5143 -- 2.9519)  data: 0.0373 (0.0004 -- 0.6040)  max mem: 16413
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6090 (1.7072)  loss_scale: 8192.0000 (14540.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9122 (8.9847)  time: 0.6735 (0.4955 -- 2.5430)  data: 0.0008 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [145] Total time: 0:02:23 (0.8948 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6090 (1.7148)  loss_scale: 8192.0000 (14540.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9122 (8.9847)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1967 (0.1967)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4122 (2.4122 -- 2.4122)  data: 2.1807 (2.1807 -- 2.1807)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3803 (0.6250)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4311 (0.2052 -- 2.4122)  data: 0.2134 (0.0004 -- 2.1807)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4413 (0.5635)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (96.8254)  time: 0.2278 (0.1686 -- 0.4798)  data: 0.0235 (0.0001 -- 0.2998)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5221 (0.6142)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (96.6805)  time: 0.2098 (0.1318 -- 0.4798)  data: 0.0232 (0.0001 -- 0.2998)  max mem: 16413
Val: Total time: 0:00:07 (0.2944 s / it)
* Acc@1 84.440 Acc@5 97.718 loss 0.604
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [146]  [  0/160]  eta: 0:19:47  lr: 0.000008  min_lr: 0.000000  loss: 1.8439 (1.8439)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1871 (6.1871)  time: 7.4231 (7.4231 -- 7.4231)  data: 6.8892 (6.8892 -- 6.8892)  max mem: 16413
Epoch: [146]  [ 20/160]  eta: 0:02:59  lr: 0.000008  min_lr: 0.000000  loss: 1.6360 (1.6389)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9947 (8.6206)  time: 0.9762 (0.5192 -- 5.2351)  data: 0.4280 (0.0003 -- 4.6853)  max mem: 16413
Epoch: [146]  [ 40/160]  eta: 0:02:11  lr: 0.000008  min_lr: 0.000000  loss: 1.6233 (1.6193)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8253 (8.8722)  time: 0.8910 (0.5189 -- 4.2068)  data: 0.3470 (0.0001 -- 3.6898)  max mem: 16413
Epoch: [146]  [ 60/160]  eta: 0:01:40  lr: 0.000008  min_lr: 0.000000  loss: 1.5865 (1.6139)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2853 (8.7767)  time: 0.8193 (0.5247 -- 3.8783)  data: 0.2748 (0.0002 -- 3.3671)  max mem: 16413
[2023-08-31 17:10:52,144] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:10:52,145] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:10:52,145] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:10:52,146] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [146]  [ 80/160]  eta: 0:01:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7129 (1.6308)  loss_scale: 8192.0000 (8596.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1771 (8.7644)  time: 0.9306 (0.5325 -- 3.1271)  data: 0.3814 (0.0004 -- 2.6049)  max mem: 16413
Epoch: [146]  [100/160]  eta: 0:00:57  lr: 0.000008  min_lr: 0.000000  loss: 1.8866 (1.6821)  loss_scale: 16384.0000 (10138.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0244 (8.7213)  time: 0.8289 (0.5157 -- 2.8216)  data: 0.2918 (0.0002 -- 2.2869)  max mem: 16413
Epoch: [146]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.6457 (1.6914)  loss_scale: 16384.0000 (11170.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7151 (8.6200)  time: 0.8041 (0.5419 -- 3.4842)  data: 0.2443 (0.0002 -- 2.9364)  max mem: 16413
Epoch: [146]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.6191 (1.6722)  loss_scale: 16384.0000 (11910.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8923 (8.7133)  time: 0.7867 (0.5361 -- 2.4902)  data: 0.2365 (0.0004 -- 1.9444)  max mem: 16413
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6549 (1.6857)  loss_scale: 16384.0000 (12441.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7607 (8.7554)  time: 0.7589 (0.4946 -- 4.2027)  data: 0.2113 (0.0002 -- 3.6945)  max mem: 16413
Epoch: [146] Total time: 0:02:22 (0.8925 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6549 (1.6824)  loss_scale: 16384.0000 (12441.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7607 (8.7554)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1871 (0.1871)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4895 (2.4895 -- 2.4895)  data: 2.2698 (2.2698 -- 2.2698)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3677 (0.6356)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4255 (0.1989 -- 2.4895)  data: 0.2110 (0.0006 -- 2.2698)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4081 (0.5640)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2207 (0.1689 -- 0.4885)  data: 0.0185 (0.0001 -- 0.3155)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5039 (0.6191)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2041 (0.1329 -- 0.4885)  data: 0.0177 (0.0001 -- 0.3155)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.602
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [147]  [  0/160]  eta: 0:21:32  lr: 0.000008  min_lr: 0.000000  loss: 1.4405 (1.4405)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5118 (9.5118)  time: 8.0797 (8.0797 -- 8.0797)  data: 7.5582 (7.5582 -- 7.5582)  max mem: 16413
Epoch: [147]  [ 20/160]  eta: 0:02:46  lr: 0.000008  min_lr: 0.000000  loss: 1.6301 (1.6377)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5978 (8.0539)  time: 0.8454 (0.5270 -- 5.1766)  data: 0.3023 (0.0003 -- 4.6476)  max mem: 16413
Epoch: [147]  [ 40/160]  eta: 0:02:08  lr: 0.000008  min_lr: 0.000000  loss: 1.7208 (1.7020)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2167 (8.5089)  time: 0.9511 (0.5240 -- 3.5480)  data: 0.4039 (0.0004 -- 3.0262)  max mem: 16413
[2023-08-31 17:12:54,012] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:12:54,012] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:12:54,014] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:12:54,014] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:12:54,547] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23566
[2023-08-31 17:12:54,547] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:12:54,547] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23566
[2023-08-31 17:12:54,547] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 17:12:54,547] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [147]  [ 60/160]  eta: 0:01:37  lr: 0.000008  min_lr: 0.000000  loss: 1.6937 (1.6749)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2312 (8.5251)  time: 0.7760 (0.5185 -- 3.1173)  data: 0.2310 (0.0002 -- 2.6004)  max mem: 16413
Epoch: [147]  [ 80/160]  eta: 0:01:18  lr: 0.000008  min_lr: 0.000000  loss: 1.6605 (1.6946)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9542 (8.5127)  time: 0.9918 (0.5299 -- 3.9787)  data: 0.4352 (0.0004 -- 3.4503)  max mem: 16413
Epoch: [147]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.9299 (1.7313)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9805 (8.6283)  time: 0.7998 (0.5183 -- 3.2784)  data: 0.2573 (0.0002 -- 2.7556)  max mem: 16413
[2023-08-31 17:13:50,788] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23630
[2023-08-31 17:13:50,789] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23630
[2023-08-31 17:13:50,789] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:13:50,789] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 17:13:50,789] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [147]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 1.7754 (1.7276)  loss_scale: 8192.0000 (15774.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3659 (8.6140)  time: 0.9412 (0.5089 -- 3.6218)  data: 0.3990 (0.0003 -- 3.0899)  max mem: 16413
Epoch: [147]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.5879 (1.7152)  loss_scale: 8192.0000 (14699.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6606 (8.6565)  time: 0.8362 (0.5255 -- 4.0622)  data: 0.2885 (0.0004 -- 3.5338)  max mem: 16413
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.8144 (1.7221)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1511 (8.7511)  time: 0.6536 (0.4943 -- 2.1408)  data: 0.1360 (0.0002 -- 1.6180)  max mem: 16413
Epoch: [147] Total time: 0:02:23 (0.8969 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.8144 (1.7011)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1511 (8.7511)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.1925 (0.1925)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2439 (2.2439 -- 2.2439)  data: 2.0254 (2.0254 -- 2.0254)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3650 (0.6259)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4164 (0.1950 -- 2.2439)  data: 0.2021 (0.0007 -- 2.0254)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4228 (0.5611)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.3545)  time: 0.2298 (0.1683 -- 0.4446)  data: 0.0229 (0.0001 -- 0.2574)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5002 (0.6197)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.0954)  time: 0.2151 (0.1333 -- 0.4446)  data: 0.0227 (0.0001 -- 0.2574)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 84.440 Acc@5 97.718 loss 0.605
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [148]  [  0/160]  eta: 0:22:20  lr: 0.000008  min_lr: 0.000000  loss: 1.5045 (1.5045)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3517 (10.3517)  time: 8.3754 (8.3754 -- 8.3754)  data: 6.8393 (6.8393 -- 6.8393)  max mem: 16413
Epoch: [148]  [ 20/160]  eta: 0:02:54  lr: 0.000008  min_lr: 0.000000  loss: 1.7758 (1.7044)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6631 (9.0156)  time: 0.8901 (0.5163 -- 4.0490)  data: 0.3491 (0.0004 -- 3.5280)  max mem: 16413
Epoch: [148]  [ 40/160]  eta: 0:02:04  lr: 0.000008  min_lr: 0.000000  loss: 1.6141 (1.6804)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7638 (8.5251)  time: 0.8244 (0.5180 -- 2.5946)  data: 0.2812 (0.0004 -- 2.0476)  max mem: 16413
Epoch: [148]  [ 60/160]  eta: 0:01:41  lr: 0.000008  min_lr: 0.000000  loss: 1.6269 (1.6759)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9068 (8.4922)  time: 0.9697 (0.5253 -- 4.2662)  data: 0.4308 (0.0004 -- 3.7387)  max mem: 16413
[2023-08-31 17:15:56,254] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:15:56,254] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:15:56,254] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:15:56,254] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [148]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000000  loss: 1.6061 (1.6739)  loss_scale: 8192.0000 (8394.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5130 (8.5336)  time: 0.7942 (0.5210 -- 3.0147)  data: 0.2455 (0.0005 -- 2.4744)  max mem: 16413
[2023-08-31 17:16:07,412] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23772
[2023-08-31 17:16:07,412] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23772
[2023-08-31 17:16:07,412] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:16:07,412] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:16:07,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [148]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.7143 (1.6820)  loss_scale: 16384.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4628 (8.6992)  time: 0.8741 (0.5099 -- 2.8999)  data: 0.3200 (0.0005 -- 2.3746)  max mem: 16413
Epoch: [148]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.6142 (1.6749)  loss_scale: 8192.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5779 (8.7791)  time: 0.8242 (0.5241 -- 3.0331)  data: 0.0053 (0.0003 -- 0.0667)  max mem: 16413
Epoch: [148]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7198 (1.6838)  loss_scale: 8192.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8257 (8.7437)  time: 0.9176 (0.5102 -- 3.9232)  data: 0.2255 (0.0001 -- 3.4005)  max mem: 16413
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.7347 (1.6939)  loss_scale: 8192.0000 (8857.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5276 (8.7859)  time: 0.6770 (0.4967 -- 1.9309)  data: 0.1506 (0.0001 -- 1.4380)  max mem: 16413
Epoch: [148] Total time: 0:02:23 (0.8956 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.7347 (1.7100)  loss_scale: 8192.0000 (8857.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5276 (8.7859)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1886 (0.1886)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6206 (2.6206 -- 2.6206)  data: 2.3916 (2.3916 -- 2.3916)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3838 (0.6289)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4409 (0.2005 -- 2.6206)  data: 0.2247 (0.0007 -- 2.3916)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4033 (0.5652)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2117 (0.1704 -- 0.2843)  data: 0.0080 (0.0001 -- 0.0770)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4718 (0.6227)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.1944 (0.1331 -- 0.2843)  data: 0.0073 (0.0001 -- 0.0770)  max mem: 16413
Val: Total time: 0:00:07 (0.2905 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.607
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.68%
Epoch: [149]  [  0/160]  eta: 0:16:34  lr: 0.000008  min_lr: 0.000000  loss: 1.4240 (1.4240)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1843 (7.1843)  time: 6.2153 (6.2153 -- 6.2153)  data: 5.6819 (5.6819 -- 5.6819)  max mem: 16413
Epoch: [149]  [ 20/160]  eta: 0:02:44  lr: 0.000007  min_lr: 0.000000  loss: 1.5348 (1.6507)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3183 (8.9397)  time: 0.9198 (0.5319 -- 3.0353)  data: 0.1585 (0.0005 -- 1.0667)  max mem: 16413
Epoch: [149]  [ 40/160]  eta: 0:02:02  lr: 0.000007  min_lr: 0.000000  loss: 1.7729 (1.6917)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5698 (8.6750)  time: 0.8669 (0.5229 -- 2.7216)  data: 0.1343 (0.0004 -- 1.8045)  max mem: 16413
Epoch: [149]  [ 60/160]  eta: 0:01:38  lr: 0.000007  min_lr: 0.000000  loss: 1.8422 (1.7144)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3522 (8.6452)  time: 0.9122 (0.5282 -- 3.5199)  data: 0.0399 (0.0002 -- 0.7737)  max mem: 16413
[2023-08-31 17:18:10,758] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:18:10,758] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:18:10,759] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:18:10,759] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [149]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.6500 (1.7068)  loss_scale: 16384.0000 (10214.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9338 (8.7696)  time: 0.8840 (0.5288 -- 3.6230)  data: 0.0019 (0.0005 -- 0.0065)  max mem: 16413
Epoch: [149]  [100/160]  eta: 0:00:57  lr: 0.000007  min_lr: 0.000000  loss: 1.5247 (1.7008)  loss_scale: 16384.0000 (11436.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0572 (8.7808)  time: 0.9468 (0.5184 -- 4.1783)  data: 0.0015 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [149]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.7795 (1.7062)  loss_scale: 16384.0000 (12254.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7469 (8.7183)  time: 0.7048 (0.5223 -- 2.6118)  data: 0.0018 (0.0002 -- 0.0055)  max mem: 16413
Epoch: [149]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.5310 (1.6919)  loss_scale: 16384.0000 (12839.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7320 (8.9658)  time: 0.9160 (0.5190 -- 4.6642)  data: 0.0018 (0.0003 -- 0.0054)  max mem: 16413
[2023-08-31 17:19:32,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=139, lr=[1.720773458201662e-07, 1.720773458201662e-07, 2.2943646109355492e-07, 2.2943646109355492e-07, 3.0591528145807326e-07, 3.0591528145807326e-07, 4.078870419440976e-07, 4.078870419440976e-07, 5.438493892587968e-07, 5.438493892587968e-07, 7.251325190117291e-07, 7.251325190117291e-07, 9.668433586823055e-07, 9.668433586823055e-07, 1.289124478243074e-06, 1.289124478243074e-06, 1.7188326376574321e-06, 1.7188326376574321e-06, 2.2917768502099095e-06, 2.2917768502099095e-06, 3.055702466946546e-06, 3.055702466946546e-06, 4.074269955928728e-06, 4.074269955928728e-06, 5.4323599412383035e-06, 5.4323599412383035e-06, 7.243146588317738e-06, 7.243146588317738e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 17:19:32,937] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=16.801567154644175, CurrSamplesPerSec=24.788017432305516, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7779 (1.7029)  loss_scale: 16384.0000 (13260.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4454 (8.9227)  time: 0.7119 (0.4947 -- 3.7506)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [149] Total time: 0:02:22 (0.8933 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7779 (1.7247)  loss_scale: 16384.0000 (13260.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4454 (8.9227)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1948 (0.1948)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5569 (2.5569 -- 2.5569)  data: 2.3115 (2.3115 -- 2.3115)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3734 (0.6259)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4294 (0.1984 -- 2.5569)  data: 0.2111 (0.0005 -- 2.3115)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4243 (0.5683)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2184 (0.1690 -- 0.4427)  data: 0.0134 (0.0001 -- 0.2549)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4874 (0.6234)  acc1: 77.7778 (82.1577)  acc5: 100.0000 (97.9253)  time: 0.2021 (0.1332 -- 0.4427)  data: 0.0131 (0.0001 -- 0.2549)  max mem: 16413
Val: Total time: 0:00:07 (0.2929 s / it)
* Acc@1 84.025 Acc@5 98.133 loss 0.608
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.68%
Epoch: [150]  [  0/160]  eta: 0:20:06  lr: 0.000007  min_lr: 0.000000  loss: 2.0932 (2.0932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4143 (5.4143)  time: 7.5416 (7.5416 -- 7.5416)  data: 7.0013 (7.0013 -- 7.0013)  max mem: 16413
Epoch: [150]  [ 20/160]  eta: 0:02:47  lr: 0.000007  min_lr: 0.000000  loss: 1.7293 (1.8227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3874 (7.8098)  time: 0.8793 (0.5249 -- 2.6725)  data: 0.1881 (0.0005 -- 1.8255)  max mem: 16413
[2023-08-31 17:20:14,070] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:20:14,070] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:20:14,071] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:20:14,071] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:20:21,672] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24039
[2023-08-31 17:20:21,672] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24039
[2023-08-31 17:20:21,672] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:20:21,672] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:20:21,672] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [150]  [ 40/160]  eta: 0:02:04  lr: 0.000007  min_lr: 0.000000  loss: 1.6290 (1.7074)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2930 (8.3937)  time: 0.8767 (0.5185 -- 3.5886)  data: 0.1764 (0.0003 -- 1.3805)  max mem: 16413
Epoch: [150]  [ 60/160]  eta: 0:01:39  lr: 0.000007  min_lr: 0.000000  loss: 1.7798 (1.7069)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2364 (8.4778)  time: 0.8968 (0.5200 -- 2.7169)  data: 0.3479 (0.0004 -- 2.1660)  max mem: 16413
Epoch: [150]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.5280 (1.6956)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2886 (8.6329)  time: 0.8314 (0.5276 -- 2.7134)  data: 0.2303 (0.0002 -- 2.1880)  max mem: 16413
Epoch: [150]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.8593 (1.7168)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8216 (8.9292)  time: 0.9152 (0.5257 -- 3.0883)  data: 0.1215 (0.0003 -- 1.3826)  max mem: 16413
Epoch: [150]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.6579 (1.6978)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9143 (8.8236)  time: 0.7797 (0.5380 -- 3.2332)  data: 0.2197 (0.0003 -- 2.4916)  max mem: 16413
Epoch: [150]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.7226 (1.7095)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5473 (8.6836)  time: 0.9248 (0.5231 -- 3.4875)  data: 0.3660 (0.0006 -- 2.9383)  max mem: 16413
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7165 (1.7108)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4009 (8.7140)  time: 0.6469 (0.4936 -- 2.2442)  data: 0.1285 (0.0002 -- 1.7301)  max mem: 16413
Epoch: [150] Total time: 0:02:22 (0.8878 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7165 (1.6951)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4009 (8.7140)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1899 (0.1899)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5750 (2.5750 -- 2.5750)  data: 2.3513 (2.3513 -- 2.3513)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3386 (0.6279)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4502 (0.1931 -- 2.5750)  data: 0.2342 (0.0005 -- 2.3513)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4606 (0.5693)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.3545)  time: 0.2193 (0.1677 -- 0.4386)  data: 0.0135 (0.0001 -- 0.2159)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5048 (0.6267)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (97.0954)  time: 0.2033 (0.1322 -- 0.4386)  data: 0.0132 (0.0001 -- 0.2159)  max mem: 16413
Val: Total time: 0:00:07 (0.2941 s / it)
* Acc@1 84.647 Acc@5 97.718 loss 0.605
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [151]  [  0/160]  eta: 0:15:45  lr: 0.000007  min_lr: 0.000000  loss: 1.1422 (1.1422)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4956 (7.4956)  time: 5.9113 (5.9113 -- 5.9113)  data: 5.3185 (5.3185 -- 5.3185)  max mem: 16413
[2023-08-31 17:22:23,827] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:22:23,827] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:22:23,827] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:22:23,827] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [151]  [ 20/160]  eta: 0:02:47  lr: 0.000007  min_lr: 0.000000  loss: 1.7367 (1.6340)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0535 (8.6431)  time: 0.9611 (0.5271 -- 2.1233)  data: 0.2663 (0.0005 -- 1.5885)  max mem: 16413
Epoch: [151]  [ 40/160]  eta: 0:02:01  lr: 0.000007  min_lr: 0.000000  loss: 1.7707 (1.6516)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8804 (8.9152)  time: 0.8216 (0.5291 -- 1.9901)  data: 0.1784 (0.0001 -- 1.4687)  max mem: 16413
[2023-08-31 17:22:54,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24201
[2023-08-31 17:22:54,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24201
[2023-08-31 17:22:54,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:22:54,080] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:22:54,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [151]  [ 60/160]  eta: 0:01:35  lr: 0.000007  min_lr: 0.000000  loss: 1.6220 (1.6387)  loss_scale: 16384.0000 (25247.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5616 (8.8652)  time: 0.8297 (0.5274 -- 2.6360)  data: 0.1899 (0.0004 -- 2.1113)  max mem: 16413
Epoch: [151]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.6587 (1.6482)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1991 (8.9453)  time: 0.9424 (0.5224 -- 3.1201)  data: 0.3450 (0.0009 -- 2.6025)  max mem: 16413
Epoch: [151]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.7232 (1.6485)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8436 (8.9939)  time: 0.8211 (0.5284 -- 2.3284)  data: 0.2529 (0.0003 -- 1.7833)  max mem: 16413
Epoch: [151]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.7379 (1.6741)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0756 (8.9591)  time: 0.8980 (0.5322 -- 2.6268)  data: 0.3464 (0.0003 -- 2.0895)  max mem: 16413
Epoch: [151]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.4367 (1.6538)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8584 (8.9858)  time: 0.7968 (0.5229 -- 2.0349)  data: 0.2211 (0.0004 -- 1.4839)  max mem: 16413
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.9217 (1.6835)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1517 (9.0697)  time: 0.7299 (0.4949 -- 2.6542)  data: 0.1724 (0.0002 -- 2.1144)  max mem: 16413
Epoch: [151] Total time: 0:02:20 (0.8795 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.9217 (1.6977)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1517 (9.0697)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1831 (0.1831)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5192 (2.5192 -- 2.5192)  data: 2.2977 (2.2977 -- 2.2977)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3524 (0.6350)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4475 (0.2096 -- 2.5192)  data: 0.2300 (0.0010 -- 2.2977)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4356 (0.5695)  acc1: 88.8889 (83.5979)  acc5: 100.0000 (97.8836)  time: 0.2184 (0.1686 -- 0.4299)  data: 0.0145 (0.0001 -- 0.2157)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5114 (0.6291)  acc1: 88.8889 (82.1577)  acc5: 100.0000 (97.9253)  time: 0.2017 (0.1328 -- 0.4299)  data: 0.0141 (0.0001 -- 0.2157)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.606
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [152]  [  0/160]  eta: 0:20:56  lr: 0.000007  min_lr: 0.000000  loss: 1.7722 (1.7722)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6035 (11.6035)  time: 7.8544 (7.8544 -- 7.8544)  data: 6.5048 (6.5048 -- 6.5048)  max mem: 16413
[2023-08-31 17:24:55,589] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:24:55,589] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:24:55,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:24:55,590] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:24:57,756] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24334
[2023-08-31 17:24:57,756] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24334
[2023-08-31 17:24:57,757] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:24:57,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 17:24:57,757] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [152]  [ 20/160]  eta: 0:02:42  lr: 0.000007  min_lr: 0.000000  loss: 1.6872 (1.6828)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7562 (9.0794)  time: 0.8282 (0.5235 -- 3.3209)  data: 0.2202 (0.0005 -- 2.7768)  max mem: 16413
Epoch: [152]  [ 40/160]  eta: 0:02:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8198 (1.7360)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9706 (8.9372)  time: 0.8363 (0.5358 -- 2.7815)  data: 0.1068 (0.0004 -- 1.4253)  max mem: 16413
Epoch: [152]  [ 60/160]  eta: 0:01:40  lr: 0.000007  min_lr: 0.000000  loss: 1.7755 (1.7149)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5591 (8.9220)  time: 1.0071 (0.5168 -- 2.9048)  data: 0.3023 (0.0004 -- 2.3818)  max mem: 16413
Epoch: [152]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.6788 (1.7026)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9748 (8.8884)  time: 0.8024 (0.5255 -- 4.0982)  data: 0.2542 (0.0003 -- 3.5973)  max mem: 16413
Epoch: [152]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.8327 (1.7128)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9268 (8.9169)  time: 0.9283 (0.5160 -- 4.2045)  data: 0.3547 (0.0005 -- 3.6981)  max mem: 16413
Epoch: [152]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.5854 (1.7040)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2398 (8.8774)  time: 0.7842 (0.5240 -- 2.7069)  data: 0.1460 (0.0003 -- 2.1718)  max mem: 16413
Epoch: [152]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.6539 (1.6973)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2208 (9.0200)  time: 0.9200 (0.5266 -- 3.3341)  data: 0.2380 (0.0006 -- 2.8160)  max mem: 16413
[2023-08-31 17:26:51,288] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:26:51,288] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:26:51,289] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:26:51,289] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7082 (1.7003)  loss_scale: 32768.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0458 (8.9024)  time: 0.6565 (0.4953 -- 2.2085)  data: 0.1025 (0.0002 -- 1.6999)  max mem: 16413
Epoch: [152] Total time: 0:02:22 (0.8914 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7082 (1.6919)  loss_scale: 32768.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0458 (8.9024)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1773 (0.1773)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4400 (2.4400 -- 2.4400)  data: 2.1863 (2.1863 -- 2.1863)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3458 (0.6310)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (97.9798)  time: 0.4242 (0.1990 -- 2.4400)  data: 0.2002 (0.0007 -- 2.1863)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4056 (0.5631)  acc1: 88.8889 (84.1270)  acc5: 100.0000 (97.8836)  time: 0.2125 (0.1691 -- 0.3066)  data: 0.0065 (0.0001 -- 0.1112)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4755 (0.6234)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.1931 (0.1325 -- 0.3066)  data: 0.0061 (0.0001 -- 0.1112)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.603
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.68%
Epoch: [153]  [  0/160]  eta: 0:22:56  lr: 0.000006  min_lr: 0.000000  loss: 1.4069 (1.4069)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8489 (8.8489)  time: 8.6040 (8.6040 -- 8.6040)  data: 6.8220 (6.8220 -- 6.8220)  max mem: 16413
[2023-08-31 17:27:27,239] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24491
[2023-08-31 17:27:27,239] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24491
[2023-08-31 17:27:27,239] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:27:27,239] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:27:27,239] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [153]  [ 20/160]  eta: 0:02:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7244 (1.6809)  loss_scale: 16384.0000 (24966.0952)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5056 (8.6977)  time: 0.8851 (0.5200 -- 4.5440)  data: 0.3402 (0.0002 -- 4.0121)  max mem: 16413
Epoch: [153]  [ 40/160]  eta: 0:02:11  lr: 0.000006  min_lr: 0.000000  loss: 1.7997 (1.7125)  loss_scale: 16384.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4711 (9.1556)  time: 0.9313 (0.5149 -- 5.0284)  data: 0.3999 (0.0003 -- 4.4978)  max mem: 16413
Epoch: [153]  [ 60/160]  eta: 0:01:41  lr: 0.000006  min_lr: 0.000000  loss: 1.7432 (1.7373)  loss_scale: 16384.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1496 (9.0178)  time: 0.8384 (0.5305 -- 4.1935)  data: 0.2944 (0.0001 -- 3.6778)  max mem: 16413
Epoch: [153]  [ 80/160]  eta: 0:01:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7228 (1.7286)  loss_scale: 16384.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3402 (9.1186)  time: 0.9037 (0.5239 -- 4.1091)  data: 0.3612 (0.0004 -- 3.5803)  max mem: 16413
Epoch: [153]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.8657 (1.7517)  loss_scale: 16384.0000 (18168.3960)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1155 (9.0974)  time: 0.7081 (0.5287 -- 1.8656)  data: 0.1039 (0.0004 -- 1.3032)  max mem: 16413
Epoch: [153]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.7092 (1.7396)  loss_scale: 16384.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8571 (9.0825)  time: 0.9298 (0.5380 -- 3.2490)  data: 0.3537 (0.0005 -- 2.7308)  max mem: 16413
[2023-08-31 17:29:18,607] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:29:18,608] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:29:18,610] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:29:18,610] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [153]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7928 (1.7347)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2943 (9.0735)  time: 0.8095 (0.5307 -- 2.7345)  data: 0.0938 (0.0005 -- 1.7882)  max mem: 16413
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7006 (1.7270)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1743 (9.0367)  time: 0.7456 (0.4946 -- 4.4182)  data: 0.0033 (0.0003 -- 0.0481)  max mem: 16413
Epoch: [153] Total time: 0:02:23 (0.8943 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7006 (1.7371)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1743 (9.0367)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1823 (0.1823)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4334 (2.4334 -- 2.4334)  data: 2.1994 (2.1994 -- 2.1994)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3100 (0.6084)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4343 (0.1992 -- 2.4334)  data: 0.2147 (0.0009 -- 2.1994)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4240 (0.5499)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.8836)  time: 0.2237 (0.1694 -- 0.3878)  data: 0.0175 (0.0001 -- 0.1842)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4772 (0.6070)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.9253)  time: 0.2059 (0.1324 -- 0.3878)  data: 0.0169 (0.0001 -- 0.1842)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 85.477 Acc@5 98.133 loss 0.600
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 85.68%
Epoch: [154]  [  0/160]  eta: 0:22:04  lr: 0.000006  min_lr: 0.000000  loss: 1.3621 (1.3621)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5920 (8.5920)  time: 8.2767 (8.2767 -- 8.2767)  data: 4.6999 (4.6999 -- 4.6999)  max mem: 16413
[2023-08-31 17:29:52,379] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24645
[2023-08-31 17:29:52,379] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:29:52,379] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24645
[2023-08-31 17:29:52,380] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:29:52,380] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [154]  [ 20/160]  eta: 0:02:51  lr: 0.000006  min_lr: 0.000000  loss: 1.7928 (1.7274)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3394 (8.6497)  time: 0.8721 (0.5131 -- 3.7162)  data: 0.0015 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [154]  [ 40/160]  eta: 0:02:05  lr: 0.000006  min_lr: 0.000000  loss: 1.7176 (1.7516)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1933 (9.1640)  time: 0.8609 (0.5141 -- 4.4251)  data: 0.0015 (0.0004 -- 0.0039)  max mem: 16413
Epoch: [154]  [ 60/160]  eta: 0:01:36  lr: 0.000006  min_lr: 0.000000  loss: 1.7102 (1.7431)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0594 (9.1917)  time: 0.7825 (0.5358 -- 2.6595)  data: 0.0021 (0.0003 -- 0.0065)  max mem: 16413
Epoch: [154]  [ 80/160]  eta: 0:01:15  lr: 0.000006  min_lr: 0.000000  loss: 1.6573 (1.7341)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7640 (9.2070)  time: 0.8830 (0.5333 -- 2.7588)  data: 0.0026 (0.0005 -- 0.0226)  max mem: 16413
Epoch: [154]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7358 (1.7337)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8644 (9.1978)  time: 0.8169 (0.5394 -- 2.3342)  data: 0.0166 (0.0003 -- 0.3036)  max mem: 16413
Epoch: [154]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.8699 (1.7552)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6998 (9.0006)  time: 0.8829 (0.5317 -- 1.9801)  data: 0.0805 (0.0003 -- 0.7716)  max mem: 16413
[2023-08-31 17:31:44,296] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:31:44,296] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:31:44,337] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:31:44,337] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [154]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7742 (1.7565)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7785 (8.8873)  time: 0.9446 (0.5359 -- 3.9554)  data: 0.0116 (0.0005 -- 0.2049)  max mem: 16413
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6006 (1.7358)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8823 (8.8604)  time: 0.6985 (0.4937 -- 4.0522)  data: 0.0008 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [154] Total time: 0:02:22 (0.8911 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6006 (1.7138)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8823 (8.8604)
Val:  [ 0/27]  eta: 0:01:11  loss: 0.1809 (0.1809)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6386 (2.6386 -- 2.6386)  data: 2.3384 (2.3384 -- 2.3384)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3285 (0.6033)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4494 (0.1995 -- 2.6386)  data: 0.2282 (0.0008 -- 2.3384)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4164 (0.5478)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2107 (0.1687 -- 0.3374)  data: 0.0087 (0.0001 -- 0.1371)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4737 (0.6033)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.9253)  time: 0.1931 (0.1327 -- 0.3374)  data: 0.0072 (0.0001 -- 0.1371)  max mem: 16413
Val: Total time: 0:00:07 (0.2904 s / it)
* Acc@1 84.855 Acc@5 98.133 loss 0.598
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [155]  [  0/160]  eta: 0:21:40  lr: 0.000006  min_lr: 0.000000  loss: 1.4189 (1.4189)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2930 (8.2930)  time: 8.1255 (8.1255 -- 8.1255)  data: 7.5827 (7.5827 -- 7.5827)  max mem: 16413
Epoch: [155]  [ 20/160]  eta: 0:02:50  lr: 0.000006  min_lr: 0.000000  loss: 1.7794 (1.7208)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3653 (8.8665)  time: 0.8709 (0.5200 -- 4.0942)  data: 0.1666 (0.0004 -- 1.8706)  max mem: 16413
[2023-08-31 17:32:38,455] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24823
[2023-08-31 17:32:38,455] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:32:38,455] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 17:32:38,455] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24823
[2023-08-31 17:32:38,456] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [155]  [ 40/160]  eta: 0:02:08  lr: 0.000006  min_lr: 0.000000  loss: 1.6383 (1.6826)  loss_scale: 16384.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0573 (8.7599)  time: 0.9240 (0.5262 -- 3.2907)  data: 0.0097 (0.0003 -- 0.1535)  max mem: 16413
Epoch: [155]  [ 60/160]  eta: 0:01:38  lr: 0.000006  min_lr: 0.000000  loss: 1.7811 (1.7197)  loss_scale: 16384.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0157 (9.0191)  time: 0.7922 (0.5155 -- 4.1688)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [155]  [ 80/160]  eta: 0:01:17  lr: 0.000006  min_lr: 0.000000  loss: 1.6765 (1.7049)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2837 (8.8845)  time: 0.9165 (0.5232 -- 3.7523)  data: 0.0015 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [155]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.8046 (1.7030)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5810 (8.8978)  time: 0.7957 (0.5317 -- 3.8203)  data: 0.0015 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [155]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.6384 (1.6818)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1768 (8.9058)  time: 0.8603 (0.5168 -- 3.4183)  data: 0.0014 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [155]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.6119 (1.6812)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5437 (8.7938)  time: 0.8920 (0.5257 -- 3.1314)  data: 0.0015 (0.0004 -- 0.0041)  max mem: 16413
[2023-08-31 17:34:29,840] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:34:29,840] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:34:29,841] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:34:29,841] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8706 (1.7001)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9022 (8.7668)  time: 0.6716 (0.4972 -- 2.3767)  data: 0.0007 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [155] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8706 (1.7179)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9022 (8.7668)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1789 (0.1789)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3304 (2.3304 -- 2.3304)  data: 2.1177 (2.1177 -- 2.1177)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3329 (0.6064)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (98.9899)  time: 0.4358 (0.1997 -- 2.3304)  data: 0.2193 (0.0007 -- 2.1177)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4356 (0.5484)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (98.4127)  time: 0.2220 (0.1696 -- 0.5313)  data: 0.0149 (0.0001 -- 0.2850)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4619 (0.6055)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.9253)  time: 0.2057 (0.1323 -- 0.5313)  data: 0.0146 (0.0001 -- 0.2850)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.596
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [156]  [  0/160]  eta: 0:19:42  lr: 0.000006  min_lr: 0.000000  loss: 2.0493 (2.0493)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0090 (8.0090)  time: 7.3906 (7.3906 -- 7.3906)  data: 4.4152 (4.4152 -- 4.4152)  max mem: 16413
[2023-08-31 17:35:00,267] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24973
[2023-08-31 17:35:00,267] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24973
[2023-08-31 17:35:00,267] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:35:00,267] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:35:00,268] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [156]  [ 20/160]  eta: 0:02:47  lr: 0.000006  min_lr: 0.000000  loss: 1.7330 (1.8273)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3168 (8.9206)  time: 0.8844 (0.5080 -- 3.5443)  data: 0.2376 (0.0001 -- 3.0114)  max mem: 16413
[2023-08-31 17:35:21,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=146, lr=[1.336984109907071e-07, 1.336984109907071e-07, 1.7826454798760943e-07, 1.7826454798760943e-07, 2.3768606398347925e-07, 2.3768606398347925e-07, 3.1691475197797236e-07, 3.1691475197797236e-07, 4.2255300263729644e-07, 4.2255300263729644e-07, 5.634040035163953e-07, 5.634040035163953e-07, 7.512053380218604e-07, 7.512053380218604e-07, 1.0016071173624805e-06, 1.0016071173624805e-06, 1.3354761564833073e-06, 1.3354761564833073e-06, 1.7806348753110764e-06, 1.7806348753110764e-06, 2.374179833748102e-06, 2.374179833748102e-06, 3.165573111664136e-06, 3.165573111664136e-06, 4.220764148885515e-06, 4.220764148885515e-06, 5.627685531847353e-06, 5.627685531847353e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 17:35:21,713] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=16.83656010948271, CurrSamplesPerSec=22.069873219679913, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [156]  [ 40/160]  eta: 0:02:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6932 (1.7350)  loss_scale: 16384.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1392 (8.7776)  time: 0.7970 (0.5309 -- 1.7832)  data: 0.1040 (0.0004 -- 1.2641)  max mem: 16413
Epoch: [156]  [ 60/160]  eta: 0:01:36  lr: 0.000006  min_lr: 0.000000  loss: 1.7765 (1.7372)  loss_scale: 16384.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5683 (9.0467)  time: 0.8827 (0.5395 -- 2.9358)  data: 0.2619 (0.0002 -- 2.4182)  max mem: 16413
Epoch: [156]  [ 80/160]  eta: 0:01:15  lr: 0.000006  min_lr: 0.000000  loss: 1.6283 (1.7179)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3905 (9.0414)  time: 0.9122 (0.5358 -- 3.0551)  data: 0.1737 (0.0010 -- 1.8385)  max mem: 16413
Epoch: [156]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 1.6584 (1.7032)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1909 (9.2064)  time: 0.8852 (0.5208 -- 2.3307)  data: 0.1576 (0.0001 -- 1.5869)  max mem: 16413
Epoch: [156]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.6717 (1.6934)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0321 (9.0864)  time: 0.8893 (0.5280 -- 4.0225)  data: 0.0474 (0.0004 -- 0.9178)  max mem: 16413
Epoch: [156]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8602 (1.7099)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6122 (8.9706)  time: 0.9128 (0.5340 -- 3.7280)  data: 0.0014 (0.0006 -- 0.0036)  max mem: 16413
[2023-08-31 17:36:53,005] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:36:53,005] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:36:53,007] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:36:53,008] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8165 (1.7025)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3466 (8.9196)  time: 0.6682 (0.4933 -- 3.0308)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [156] Total time: 0:02:23 (0.8969 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8165 (1.6906)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3466 (8.9196)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1772 (0.1772)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6276 (2.6276 -- 2.6276)  data: 2.3849 (2.3849 -- 2.3849)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3467 (0.6161)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (98.9899)  time: 0.4457 (0.2022 -- 2.6276)  data: 0.2255 (0.0004 -- 2.3849)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4104 (0.5532)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (98.4127)  time: 0.2106 (0.1692 -- 0.3005)  data: 0.0049 (0.0001 -- 0.0743)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4621 (0.6107)  acc1: 88.8889 (82.5726)  acc5: 100.0000 (97.9253)  time: 0.1943 (0.1331 -- 0.3005)  data: 0.0047 (0.0001 -- 0.0743)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 84.025 Acc@5 98.340 loss 0.597
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.68%
Epoch: [157]  [  0/160]  eta: 0:19:52  lr: 0.000005  min_lr: 0.000000  loss: 1.1400 (1.1400)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4734 (10.4734)  time: 7.4517 (7.4517 -- 7.4517)  data: 6.9377 (6.9377 -- 6.9377)  max mem: 16413
[2023-08-31 17:37:29,105] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25133
[2023-08-31 17:37:29,105] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25133
[2023-08-31 17:37:29,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:37:29,106] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:37:29,106] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [157]  [ 20/160]  eta: 0:02:43  lr: 0.000005  min_lr: 0.000000  loss: 1.8008 (1.6493)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3438 (8.8315)  time: 0.8522 (0.5207 -- 4.6600)  data: 0.2475 (0.0003 -- 3.0660)  max mem: 16413
Epoch: [157]  [ 40/160]  eta: 0:02:12  lr: 0.000005  min_lr: 0.000000  loss: 1.6362 (1.6537)  loss_scale: 16384.0000 (21578.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3865 (8.6270)  time: 1.0448 (0.5211 -- 4.1563)  data: 0.1720 (0.0004 -- 1.7862)  max mem: 16413
Epoch: [157]  [ 60/160]  eta: 0:01:40  lr: 0.000005  min_lr: 0.000000  loss: 1.6113 (1.6577)  loss_scale: 16384.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5449 (8.7830)  time: 0.7972 (0.5268 -- 3.0890)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [157]  [ 80/160]  eta: 0:01:17  lr: 0.000005  min_lr: 0.000000  loss: 1.7221 (1.6712)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4977 (8.5927)  time: 0.8633 (0.5214 -- 3.8151)  data: 0.0014 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [157]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.9072 (1.7160)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7923 (8.7632)  time: 0.8258 (0.5201 -- 3.2619)  data: 0.0572 (0.0007 -- 0.9337)  max mem: 16413
Epoch: [157]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.7291 (1.7244)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5443 (8.7801)  time: 0.8354 (0.5387 -- 2.5898)  data: 0.0965 (0.0007 -- 1.1499)  max mem: 16413
Epoch: [157]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6876 (1.7120)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9567 (8.8966)  time: 0.8904 (0.5378 -- 2.3692)  data: 0.2703 (0.0004 -- 1.8365)  max mem: 16413
[2023-08-31 17:39:24,147] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:39:24,147] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:39:24,148] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:39:24,148] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:39:29,520] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25268
[2023-08-31 17:39:29,520] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25268
[2023-08-31 17:39:29,520] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:39:29,520] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:39:29,520] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.4648 (1.6883)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6719 (8.8092)  time: 0.7524 (0.4945 -- 1.7133)  data: 0.1158 (0.0002 -- 1.1590)  max mem: 16413
Epoch: [157] Total time: 0:02:23 (0.8978 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.4648 (1.7221)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6719 (8.8092)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1726 (0.1726)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3702 (2.3702 -- 2.3702)  data: 2.1240 (2.1240 -- 2.1240)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3748 (0.6221)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4103 (0.1947 -- 2.3702)  data: 0.1941 (0.0003 -- 2.1240)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3823 (0.5537)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2313 (0.1689 -- 0.7506)  data: 0.0292 (0.0001 -- 0.5694)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4560 (0.6140)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2180 (0.1327 -- 0.7506)  data: 0.0290 (0.0001 -- 0.5694)  max mem: 16413
Val: Total time: 0:00:07 (0.2954 s / it)
* Acc@1 84.025 Acc@5 98.133 loss 0.600
Accuracy of the network on the 482 val images: 84.02%
Max accuracy: 85.68%
Epoch: [158]  [  0/160]  eta: 0:20:05  lr: 0.000005  min_lr: 0.000000  loss: 1.9650 (1.9650)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9961 (8.9961)  time: 7.5364 (7.5364 -- 7.5364)  data: 6.8421 (6.8421 -- 6.8421)  max mem: 16413
Epoch: [158]  [ 20/160]  eta: 0:02:47  lr: 0.000005  min_lr: 0.000000  loss: 1.7420 (1.7871)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1909 (8.6560)  time: 0.8830 (0.5119 -- 3.0506)  data: 0.2786 (0.0004 -- 2.4918)  max mem: 16413
Epoch: [158]  [ 40/160]  eta: 0:02:07  lr: 0.000005  min_lr: 0.000000  loss: 1.6078 (1.7081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2850 (8.3860)  time: 0.9202 (0.5173 -- 3.9820)  data: 0.0732 (0.0002 -- 0.9969)  max mem: 16413
Epoch: [158]  [ 60/160]  eta: 0:01:37  lr: 0.000005  min_lr: 0.000000  loss: 1.7464 (1.7229)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4685 (8.2763)  time: 0.7990 (0.5391 -- 3.7959)  data: 0.0017 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [158]  [ 80/160]  eta: 0:01:19  lr: 0.000005  min_lr: 0.000000  loss: 1.6967 (1.7250)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1193 (8.4061)  time: 1.0262 (0.5133 -- 4.5407)  data: 0.0336 (0.0004 -- 0.6492)  max mem: 16413
Epoch: [158]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.4630 (1.7004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7437 (8.3427)  time: 0.6870 (0.5202 -- 2.1404)  data: 0.0018 (0.0005 -- 0.0060)  max mem: 16413
[2023-08-31 17:41:33,002] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:41:33,002] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:41:33,003] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:41:33,003] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:41:33,589] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25398
[2023-08-31 17:41:33,589] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:41:33,589] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25398
[2023-08-31 17:41:33,590] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:41:33,590] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [158]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.6361 (1.6920)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2075 (8.2159)  time: 0.9121 (0.5186 -- 2.8664)  data: 0.0154 (0.0005 -- 0.2793)  max mem: 16413
Epoch: [158]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6444 (1.6872)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2986 (8.2786)  time: 0.9966 (0.5347 -- 4.7417)  data: 0.4470 (0.0004 -- 4.2310)  max mem: 16413
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.6703 (1.6891)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2780 (8.2869)  time: 0.6454 (0.4968 -- 2.2431)  data: 0.1311 (0.0002 -- 1.7443)  max mem: 16413
Epoch: [158] Total time: 0:02:24 (0.9025 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.6703 (1.6781)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2780 (8.2869)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1672 (0.1672)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4850 (2.4850 -- 2.4850)  data: 2.2569 (2.2569 -- 2.2569)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3177 (0.6137)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4577 (0.1950 -- 2.4850)  data: 0.2438 (0.0007 -- 2.2569)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4383 (0.5528)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.8836)  time: 0.2223 (0.1698 -- 0.6068)  data: 0.0214 (0.0001 -- 0.3996)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4535 (0.6077)  acc1: 88.8889 (82.9876)  acc5: 100.0000 (97.5104)  time: 0.2061 (0.1325 -- 0.6068)  data: 0.0208 (0.0001 -- 0.3996)  max mem: 16413
Val: Total time: 0:00:07 (0.2945 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.594
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.68%
Epoch: [159]  [  0/160]  eta: 0:19:56  lr: 0.000005  min_lr: 0.000000  loss: 2.1222 (2.1222)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0837 (13.0837)  time: 7.4800 (7.4800 -- 7.4800)  data: 6.9124 (6.9124 -- 6.9124)  max mem: 16413
Epoch: [159]  [ 20/160]  eta: 0:02:44  lr: 0.000005  min_lr: 0.000000  loss: 1.6504 (1.7010)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2702 (12.5325)  time: 0.8567 (0.5176 -- 3.7428)  data: 0.2976 (0.0004 -- 3.0056)  max mem: 16413
Epoch: [159]  [ 40/160]  eta: 0:02:09  lr: 0.000005  min_lr: 0.000000  loss: 1.6460 (1.7233)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1658 (10.5432)  time: 0.9778 (0.5298 -- 4.0746)  data: 0.4301 (0.0004 -- 3.5509)  max mem: 16413
Epoch: [159]  [ 60/160]  eta: 0:01:36  lr: 0.000005  min_lr: 0.000000  loss: 1.5918 (1.6727)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7014 (9.6920)  time: 0.7262 (0.5312 -- 2.5059)  data: 0.1443 (0.0004 -- 1.9604)  max mem: 16413
Epoch: [159]  [ 80/160]  eta: 0:01:16  lr: 0.000005  min_lr: 0.000000  loss: 1.5540 (1.6638)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2869 (9.5040)  time: 0.9338 (0.5252 -- 5.2971)  data: 0.3934 (0.0003 -- 4.7840)  max mem: 16413
[2023-08-31 17:43:37,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:43:37,753] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:43:37,753] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:43:37,753] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [159]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.6005 (1.6626)  loss_scale: 32768.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4130 (9.2183)  time: 0.8268 (0.5249 -- 3.0155)  data: 0.2856 (0.0001 -- 2.4805)  max mem: 16413
Epoch: [159]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.6904 (1.6686)  loss_scale: 32768.0000 (20987.7686)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6463 (9.1399)  time: 0.9035 (0.5276 -- 3.0796)  data: 0.2895 (0.0003 -- 2.5717)  max mem: 16413
[2023-08-31 17:44:12,512] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25567
[2023-08-31 17:44:12,512] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:44:12,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 17:44:12,512] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25567
[2023-08-31 17:44:12,512] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [159]  [140/160]  eta: 0:00:17  lr: 0.000005  min_lr: 0.000000  loss: 1.7793 (1.6821)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5425 (9.1200)  time: 0.7448 (0.5329 -- 2.0048)  data: 0.1473 (0.0002 -- 1.4774)  max mem: 16413
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.8172 (1.6871)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7106 (9.1817)  time: 0.7175 (0.4966 -- 2.8829)  data: 0.0104 (0.0002 -- 0.1922)  max mem: 16413
Epoch: [159] Total time: 0:02:20 (0.8791 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.8172 (1.6878)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7106 (9.1817)
[2023-08-31 17:44:37,228] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-08-31 17:44:37,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-08-31 17:44:37,233] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-08-31 17:44:37,233] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-08-31 17:44:38,292] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-08-31 17:44:38,293] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:01:14  loss: 0.1688 (0.1688)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7567 (2.7567 -- 2.7567)  data: 2.5190 (2.5190 -- 2.5190)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3313 (0.6177)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4687 (0.2040 -- 2.7567)  data: 0.2477 (0.0007 -- 2.5190)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4210 (0.5533)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2160 (0.1693 -- 0.4075)  data: 0.0108 (0.0001 -- 0.1948)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4533 (0.6083)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.1995 (0.1328 -- 0.4075)  data: 0.0106 (0.0001 -- 0.1948)  max mem: 16413
Val: Total time: 0:00:08 (0.2985 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.596
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [160]  [  0/160]  eta: 0:23:48  lr: 0.000005  min_lr: 0.000000  loss: 1.8921 (1.8921)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1064 (6.1064)  time: 8.9294 (8.9294 -- 8.9294)  data: 5.6299 (5.6299 -- 5.6299)  max mem: 16413
Epoch: [160]  [ 20/160]  eta: 0:02:47  lr: 0.000005  min_lr: 0.000000  loss: 1.7045 (1.7036)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1690 (10.3967)  time: 0.8120 (0.5264 -- 3.0351)  data: 0.1319 (0.0006 -- 2.5048)  max mem: 16413
Epoch: [160]  [ 40/160]  eta: 0:02:13  lr: 0.000005  min_lr: 0.000000  loss: 1.6116 (1.6626)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9821 (9.4698)  time: 1.0146 (0.5187 -- 5.0054)  data: 0.3789 (0.0004 -- 4.4816)  max mem: 16413
Epoch: [160]  [ 60/160]  eta: 0:01:39  lr: 0.000005  min_lr: 0.000000  loss: 1.8443 (1.6836)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1523 (9.1103)  time: 0.7636 (0.5292 -- 3.1862)  data: 0.2063 (0.0001 -- 2.6657)  max mem: 16413
Epoch: [160]  [ 80/160]  eta: 0:01:18  lr: 0.000005  min_lr: 0.000000  loss: 1.5872 (1.6501)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3118 (9.1710)  time: 0.9198 (0.5323 -- 3.2929)  data: 0.3573 (0.0001 -- 2.7850)  max mem: 16413
[2023-08-31 17:46:17,576] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:46:17,576] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:46:17,577] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:46:17,578] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [160]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.5543 (1.6541)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4940 (9.0238)  time: 0.7730 (0.5343 -- 1.8619)  data: 0.0015 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [160]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.6933 (1.6661)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5193 (8.9596)  time: 0.8650 (0.5320 -- 2.2778)  data: 0.1782 (0.0004 -- 1.6010)  max mem: 16413
[2023-08-31 17:46:44,090] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25727
[2023-08-31 17:46:44,090] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25727
[2023-08-31 17:46:44,090] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:46:44,090] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:46:44,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [160]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8380 (1.6791)  loss_scale: 16384.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4527 (9.0274)  time: 0.7942 (0.5155 -- 2.4907)  data: 0.1479 (0.0003 -- 1.9562)  max mem: 16413
Epoch: [160]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.6507 (1.6714)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0139 (9.0746)  time: 0.7371 (0.4976 -- 1.7697)  data: 0.2073 (0.0002 -- 1.2556)  max mem: 16413
Epoch: [160] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.6507 (1.6906)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0139 (9.0746)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1686 (0.1686)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5895 (2.5895 -- 2.5895)  data: 2.3751 (2.3751 -- 2.3751)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3353 (0.6207)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4497 (0.1971 -- 2.5895)  data: 0.2368 (0.0006 -- 2.3751)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4089 (0.5527)  acc1: 88.8889 (84.6561)  acc5: 100.0000 (97.3545)  time: 0.2160 (0.1687 -- 0.4418)  data: 0.0116 (0.0001 -- 0.2077)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4532 (0.6086)  acc1: 88.8889 (83.4025)  acc5: 100.0000 (97.0954)  time: 0.2004 (0.1331 -- 0.4418)  data: 0.0107 (0.0001 -- 0.2077)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 84.232 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.23%
Max accuracy: 85.68%
Epoch: [161]  [  0/160]  eta: 0:18:34  lr: 0.000005  min_lr: 0.000000  loss: 2.5634 (2.5634)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6948 (6.6948)  time: 6.9682 (6.9682 -- 6.9682)  data: 6.4221 (6.4221 -- 6.4221)  max mem: 16413
[2023-08-31 17:47:27,928] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25766
[2023-08-31 17:47:27,929] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25766
[2023-08-31 17:47:27,929] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:47:27,929] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:47:27,929] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [161]  [ 20/160]  eta: 0:02:40  lr: 0.000004  min_lr: 0.000000  loss: 1.4515 (1.5623)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0002 (9.0076)  time: 0.8538 (0.5270 -- 2.1560)  data: 0.3100 (0.0006 -- 1.6161)  max mem: 16413
Epoch: [161]  [ 40/160]  eta: 0:02:01  lr: 0.000004  min_lr: 0.000000  loss: 1.5298 (1.5797)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0067 (8.6715)  time: 0.8682 (0.5227 -- 2.5867)  data: 0.3232 (0.0003 -- 2.0553)  max mem: 16413
Epoch: [161]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000000  loss: 1.6653 (1.6330)  loss_scale: 8192.0000 (8997.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3129 (8.6740)  time: 0.8824 (0.5284 -- 2.7080)  data: 0.3277 (0.0006 -- 2.1825)  max mem: 16413
Epoch: [161]  [ 80/160]  eta: 0:01:17  lr: 0.000004  min_lr: 0.000000  loss: 1.6678 (1.6595)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1716 (9.1461)  time: 0.9474 (0.5220 -- 3.5404)  data: 0.3534 (0.0003 -- 2.9939)  max mem: 16413
Epoch: [161]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.7746 (1.6777)  loss_scale: 8192.0000 (8678.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6754 (8.9915)  time: 0.8064 (0.5415 -- 2.1949)  data: 0.2506 (0.0006 -- 1.6662)  max mem: 16413
Epoch: [161]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.6138 (1.6707)  loss_scale: 8192.0000 (8598.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7862 (8.9062)  time: 0.8424 (0.5276 -- 3.9803)  data: 0.0222 (0.0003 -- 0.3478)  max mem: 16413
[2023-08-31 17:49:23,865] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:49:23,865] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:49:23,866] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:49:23,866] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [161]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.6181 (1.6528)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3623 (8.8456)  time: 0.9670 (0.5225 -- 4.1308)  data: 0.0692 (0.0004 -- 1.3579)  max mem: 16413
Epoch: [161]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6426 (1.6519)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0736 (9.0374)  time: 0.7348 (0.4937 -- 4.0515)  data: 0.0006 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [161] Total time: 0:02:24 (0.9031 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6426 (1.6930)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0736 (9.0374)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1774 (0.1774)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4627 (2.4627 -- 2.4627)  data: 2.2506 (2.2506 -- 2.2506)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3557 (0.6214)  acc1: 88.8889 (81.8182)  acc5: 100.0000 (97.9798)  time: 0.4417 (0.2054 -- 2.4627)  data: 0.2249 (0.0007 -- 2.2506)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3989 (0.5494)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.3545)  time: 0.2208 (0.1691 -- 0.4151)  data: 0.0173 (0.0001 -- 0.2125)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4635 (0.6087)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.0954)  time: 0.2032 (0.1328 -- 0.4151)  data: 0.0169 (0.0001 -- 0.2125)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.599
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [162]  [  0/160]  eta: 0:18:12  lr: 0.000004  min_lr: 0.000000  loss: 1.6727 (1.6727)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7306 (6.7306)  time: 6.8273 (6.8273 -- 6.8273)  data: 5.2813 (5.2813 -- 5.2813)  max mem: 16413
Epoch: [162]  [ 20/160]  eta: 0:02:43  lr: 0.000004  min_lr: 0.000000  loss: 1.6796 (1.7249)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1082 (8.5726)  time: 0.8841 (0.5216 -- 2.8696)  data: 0.0252 (0.0003 -- 0.2407)  max mem: 16413
Epoch: [162]  [ 40/160]  eta: 0:01:59  lr: 0.000004  min_lr: 0.000000  loss: 1.5206 (1.6570)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0428 (8.4545)  time: 0.8089 (0.5209 -- 3.2935)  data: 0.0772 (0.0005 -- 1.4950)  max mem: 16413
Epoch: [162]  [ 60/160]  eta: 0:01:39  lr: 0.000004  min_lr: 0.000000  loss: 1.4482 (1.6333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3742 (8.5557)  time: 0.9981 (0.5305 -- 4.1527)  data: 0.0331 (0.0007 -- 0.3912)  max mem: 16413
[2023-08-31 17:51:04,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=152, lr=[9.961138567789659e-08, 9.961138567789659e-08, 1.3281518090386211e-07, 1.3281518090386211e-07, 1.7708690787181616e-07, 1.7708690787181616e-07, 2.3611587716242153e-07, 2.3611587716242153e-07, 3.148211695498954e-07, 3.148211695498954e-07, 4.197615593998605e-07, 4.197615593998605e-07, 5.59682079199814e-07, 5.59682079199814e-07, 7.462427722664186e-07, 7.462427722664186e-07, 9.949903630218917e-07, 9.949903630218917e-07, 1.326653817362522e-06, 1.326653817362522e-06, 1.768871756483363e-06, 1.768871756483363e-06, 2.3584956753111504e-06, 2.3584956753111504e-06, 3.144660900414867e-06, 3.144660900414867e-06, 4.192881200553156e-06, 4.192881200553156e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 17:51:04,354] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=16.86392136607185, CurrSamplesPerSec=23.290302518768602, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [162]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.7007 (1.6576)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8846 (8.4886)  time: 0.7795 (0.5080 -- 3.3074)  data: 0.0367 (0.0004 -- 0.6997)  max mem: 16413
Epoch: [162]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.7163 (1.6587)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8426 (8.5404)  time: 0.9328 (0.5281 -- 3.1484)  data: 0.0218 (0.0004 -- 0.4077)  max mem: 16413
[2023-08-31 17:51:25,228] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:51:25,228] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:51:25,228] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:51:25,228] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [162]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.6802 (1.6574)  loss_scale: 32768.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9911 (8.4777)  time: 0.7228 (0.5344 -- 2.2366)  data: 0.0017 (0.0006 -- 0.0038)  max mem: 16413
[2023-08-31 17:51:53,388] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26057
[2023-08-31 17:51:53,388] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26057
[2023-08-31 17:51:53,389] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:51:53,389] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:51:53,389] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [162]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.5950 (1.6525)  loss_scale: 32768.0000 (20334.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7373 (8.5203)  time: 0.9732 (0.5273 -- 2.3857)  data: 0.3025 (0.0003 -- 1.6161)  max mem: 16413
[2023-08-31 17:52:04,700] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26071
[2023-08-31 17:52:04,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:52:04,700] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26071
[2023-08-31 17:52:04,700] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:52:04,700] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [162]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.5834 (1.6510)  loss_scale: 16384.0000 (19404.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3751 (8.5285)  time: 0.6711 (0.4956 -- 2.0371)  data: 0.0433 (0.0002 -- 0.5493)  max mem: 16413
Epoch: [162] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.5834 (1.6858)  loss_scale: 16384.0000 (19404.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3751 (8.5285)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1766 (0.1766)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4864 (2.4864 -- 2.4864)  data: 2.2666 (2.2666 -- 2.2666)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3095 (0.6141)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4406 (0.1947 -- 2.4864)  data: 0.2264 (0.0002 -- 2.2666)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4407 (0.5491)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2198 (0.1691 -- 0.4331)  data: 0.0165 (0.0001 -- 0.2132)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4684 (0.6051)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.0954)  time: 0.2047 (0.1320 -- 0.4331)  data: 0.0162 (0.0001 -- 0.2132)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.596
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [163]  [  0/160]  eta: 0:15:33  lr: 0.000004  min_lr: 0.000000  loss: 0.9601 (0.9601)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9018 (13.9018)  time: 5.8326 (5.8326 -- 5.8326)  data: 4.3139 (4.3139 -- 4.3139)  max mem: 16413
Epoch: [163]  [ 20/160]  eta: 0:02:54  lr: 0.000004  min_lr: 0.000000  loss: 1.7918 (1.7088)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0771 (9.2477)  time: 1.0197 (0.5317 -- 4.1930)  data: 0.4629 (0.0006 -- 3.6831)  max mem: 16413
Epoch: [163]  [ 40/160]  eta: 0:02:07  lr: 0.000004  min_lr: 0.000000  loss: 1.7745 (1.7053)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9787 (9.2608)  time: 0.8711 (0.5214 -- 3.9439)  data: 0.3261 (0.0004 -- 3.4170)  max mem: 16413
Epoch: [163]  [ 60/160]  eta: 0:01:41  lr: 0.000004  min_lr: 0.000000  loss: 1.7220 (1.6877)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4942 (9.1826)  time: 0.9227 (0.5182 -- 3.8018)  data: 0.3826 (0.0003 -- 3.2717)  max mem: 16413
Epoch: [163]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.5590 (1.6834)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8201 (8.9138)  time: 0.7320 (0.5304 -- 2.9438)  data: 0.1806 (0.0004 -- 2.4009)  max mem: 16413
Epoch: [163]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.6538 (1.6863)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8278 (8.9793)  time: 0.8429 (0.5238 -- 3.0359)  data: 0.2930 (0.0003 -- 2.4923)  max mem: 16413
[2023-08-31 17:54:09,268] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:54:09,268] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:54:09,269] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:54:09,269] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [163]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.6405 (1.6795)  loss_scale: 8192.0000 (8259.7025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0386 (8.7917)  time: 0.8696 (0.5288 -- 2.5676)  data: 0.0402 (0.0005 -- 0.6137)  max mem: 16413
[2023-08-31 17:54:14,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26205
[2023-08-31 17:54:14,930] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 17:54:14,930] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 17:54:14,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26205
[2023-08-31 17:54:14,930] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [163]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.6177 (1.6827)  loss_scale: 8192.0000 (8482.4965)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5337 (8.8051)  time: 0.8914 (0.5224 -- 3.2946)  data: 0.0613 (0.0001 -- 1.1964)  max mem: 16413
Epoch: [163]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6036 (1.6809)  loss_scale: 8192.0000 (8448.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9227 (8.7240)  time: 0.7044 (0.4972 -- 1.8120)  data: 0.0012 (0.0002 -- 0.0073)  max mem: 16413
Epoch: [163] Total time: 0:02:22 (0.8903 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6036 (1.6927)  loss_scale: 8192.0000 (8448.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9227 (8.7240)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1758 (0.1758)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5900 (2.5900 -- 2.5900)  data: 2.3349 (2.3349 -- 2.3349)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3085 (0.6131)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4367 (0.2110 -- 2.5900)  data: 0.2133 (0.0004 -- 2.3349)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4415 (0.5497)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2145 (0.1702 -- 0.2961)  data: 0.0066 (0.0001 -- 0.1178)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4668 (0.6056)  acc1: 88.8889 (85.0622)  acc5: 100.0000 (97.0954)  time: 0.1971 (0.1331 -- 0.2961)  data: 0.0063 (0.0001 -- 0.1178)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 85.477 Acc@5 97.925 loss 0.597
Accuracy of the network on the 482 val images: 85.48%
Max accuracy: 85.68%
Epoch: [164]  [  0/160]  eta: 0:20:38  lr: 0.000004  min_lr: 0.000000  loss: 1.9539 (1.9539)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0657 (7.0657)  time: 7.7396 (7.7396 -- 7.7396)  data: 7.2013 (7.2013 -- 7.2013)  max mem: 16413
Epoch: [164]  [ 20/160]  eta: 0:02:40  lr: 0.000004  min_lr: 0.000000  loss: 1.7328 (1.7199)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9554 (8.7762)  time: 0.8137 (0.5308 -- 2.9821)  data: 0.0289 (0.0006 -- 0.5438)  max mem: 16413
Epoch: [164]  [ 40/160]  eta: 0:01:56  lr: 0.000004  min_lr: 0.000000  loss: 1.5977 (1.6924)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6547 (8.9324)  time: 0.7895 (0.5288 -- 2.4207)  data: 0.1844 (0.0005 -- 1.8729)  max mem: 16413
Epoch: [164]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7512 (1.6881)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9182 (9.0068)  time: 0.9451 (0.5205 -- 2.4093)  data: 0.2012 (0.0004 -- 1.3808)  max mem: 16413
Epoch: [164]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.8060 (1.7124)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6073 (9.1985)  time: 0.8921 (0.5230 -- 3.2180)  data: 0.1384 (0.0002 -- 1.2193)  max mem: 16413
[2023-08-31 17:56:16,934] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:56:16,934] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:56:16,935] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 17:56:16,935] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [164]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.7041 (1.6942)  loss_scale: 8192.0000 (8759.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1104 (9.2453)  time: 0.9231 (0.5343 -- 3.4187)  data: 0.0257 (0.0002 -- 0.4416)  max mem: 16413
Epoch: [164]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.6683 (1.6922)  loss_scale: 16384.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7576 (9.1411)  time: 0.8292 (0.5237 -- 2.7510)  data: 0.1101 (0.0004 -- 1.1980)  max mem: 16413
Epoch: [164]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7829 (1.7017)  loss_scale: 16384.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7977 (9.0431)  time: 0.8919 (0.5204 -- 2.9637)  data: 0.1965 (0.0005 -- 2.4247)  max mem: 16413
Epoch: [164]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.5923 (1.6893)  loss_scale: 16384.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4227 (9.0252)  time: 0.7318 (0.4955 -- 2.5246)  data: 0.1474 (0.0003 -- 2.0310)  max mem: 16413
Epoch: [164] Total time: 0:02:23 (0.8971 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.5923 (1.6987)  loss_scale: 16384.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4227 (9.0252)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.1724 (0.1724)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5967 (2.5967 -- 2.5967)  data: 2.3486 (2.3486 -- 2.3486)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3251 (0.6177)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4321 (0.1949 -- 2.5967)  data: 0.2145 (0.0007 -- 2.3486)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4117 (0.5514)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2141 (0.1687 -- 0.3069)  data: 0.0074 (0.0001 -- 0.1345)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4490 (0.6081)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.1968 (0.1324 -- 0.3069)  data: 0.0071 (0.0001 -- 0.1345)  max mem: 16413
Val: Total time: 0:00:07 (0.2912 s / it)
* Acc@1 84.855 Acc@5 98.133 loss 0.597
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [165]  [  0/160]  eta: 0:19:18  lr: 0.000004  min_lr: 0.000000  loss: 2.4052 (2.4052)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8990 (6.8990)  time: 7.2420 (7.2420 -- 7.2420)  data: 6.6155 (6.6155 -- 6.6155)  max mem: 16413
Epoch: [165]  [ 20/160]  eta: 0:02:38  lr: 0.000004  min_lr: 0.000000  loss: 1.6460 (1.7442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5092 (10.1480)  time: 0.8233 (0.5327 -- 3.0934)  data: 0.0400 (0.0001 -- 0.7220)  max mem: 16413
Epoch: [165]  [ 40/160]  eta: 0:01:57  lr: 0.000004  min_lr: 0.000000  loss: 1.6201 (1.7033)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0685 (9.1241)  time: 0.8303 (0.5240 -- 2.2533)  data: 0.0440 (0.0007 -- 0.5162)  max mem: 16413
Epoch: [165]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.7230 (1.6943)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2097 (8.9737)  time: 0.9998 (0.5206 -- 4.1675)  data: 0.0014 (0.0002 -- 0.0027)  max mem: 16413
[2023-08-31 17:58:21,400] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:58:21,400] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 17:58:21,400] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 17:58:21,401] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [165]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.5712 (1.6715)  loss_scale: 32768.0000 (20227.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7688 (9.0061)  time: 0.8822 (0.5257 -- 3.5452)  data: 0.0014 (0.0004 -- 0.0047)  max mem: 16413
[2023-08-31 17:58:50,287] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26494
[2023-08-31 17:58:50,287] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26494
[2023-08-31 17:58:50,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:58:50,288] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 17:58:50,288] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [165]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.8606 (1.6875)  loss_scale: 32768.0000 (21574.9703)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9485 (9.0286)  time: 0.7787 (0.5145 -- 2.8305)  data: 0.0014 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [165]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.6499 (1.6920)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7446 (8.9276)  time: 0.9548 (0.5244 -- 4.1764)  data: 0.0020 (0.0005 -- 0.0052)  max mem: 16413
Epoch: [165]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7106 (1.6985)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9337 (8.9122)  time: 0.8287 (0.5160 -- 3.4391)  data: 0.0020 (0.0003 -- 0.0158)  max mem: 16413
Epoch: [165]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7845 (1.6897)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8029 (8.8501)  time: 0.7289 (0.4963 -- 1.9849)  data: 0.0007 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [165] Total time: 0:02:23 (0.8951 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7845 (1.7264)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8029 (8.8501)
Val:  [ 0/27]  eta: 0:01:14  loss: 0.1733 (0.1733)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7573 (2.7573 -- 2.7573)  data: 2.5563 (2.5563 -- 2.5563)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3163 (0.6170)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4478 (0.1945 -- 2.7573)  data: 0.2332 (0.0006 -- 2.5563)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4290 (0.5521)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2083 (0.1697 -- 0.2503)  data: 0.0039 (0.0001 -- 0.0646)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4550 (0.6075)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.1932 (0.1324 -- 0.2503)  data: 0.0036 (0.0001 -- 0.0646)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.597
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [166]  [  0/160]  eta: 0:17:42  lr: 0.000003  min_lr: 0.000000  loss: 1.7725 (1.7725)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7019 (9.7019)  time: 6.6404 (6.6404 -- 6.6404)  data: 6.1130 (6.1130 -- 6.1130)  max mem: 16413
Epoch: [166]  [ 20/160]  eta: 0:02:47  lr: 0.000003  min_lr: 0.000000  loss: 1.8232 (1.8057)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7895 (8.5181)  time: 0.9216 (0.5313 -- 4.1477)  data: 0.2853 (0.0003 -- 3.6046)  max mem: 16413
Epoch: [166]  [ 40/160]  eta: 0:01:59  lr: 0.000003  min_lr: 0.000000  loss: 1.7501 (1.7962)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2178 (8.5663)  time: 0.7883 (0.5157 -- 2.8604)  data: 0.1998 (0.0006 -- 2.3376)  max mem: 16413
Epoch: [166]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000000  loss: 1.5634 (1.7135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0162 (8.4463)  time: 1.0193 (0.5338 -- 3.5660)  data: 0.0015 (0.0003 -- 0.0035)  max mem: 16413
[2023-08-31 18:00:54,021] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:00:54,022] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:00:54,022] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:00:54,022] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:00:56,294] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26626
[2023-08-31 18:00:56,295] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:00:56,295] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 18:00:56,295] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26626
[2023-08-31 18:00:56,295] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [166]  [ 80/160]  eta: 0:01:18  lr: 0.000003  min_lr: 0.000000  loss: 1.4487 (1.6852)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8600 (8.4951)  time: 0.8951 (0.5223 -- 3.7132)  data: 0.0013 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [166]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.7836 (1.7107)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4927 (8.4886)  time: 0.7151 (0.5282 -- 2.0484)  data: 0.0017 (0.0004 -- 0.0085)  max mem: 16413
Epoch: [166]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7411 (1.7101)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7123 (8.5227)  time: 0.8731 (0.5159 -- 4.5506)  data: 0.0018 (0.0003 -- 0.0051)  max mem: 16413
Epoch: [166]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.6413 (1.7052)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1876 (8.6420)  time: 0.9250 (0.5258 -- 3.2347)  data: 0.1492 (0.0006 -- 2.7042)  max mem: 16413
Epoch: [166]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7433 (1.7083)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0372 (8.5946)  time: 0.6281 (0.4949 -- 1.6464)  data: 0.0392 (0.0003 -- 0.6044)  max mem: 16413
Epoch: [166] Total time: 0:02:21 (0.8839 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7433 (1.7136)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0372 (8.5946)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1742 (0.1742)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2985 (2.2985 -- 2.2985)  data: 2.0776 (2.0776 -- 2.0776)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3247 (0.6181)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4341 (0.2010 -- 2.2985)  data: 0.2132 (0.0005 -- 2.0776)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4145 (0.5523)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2272 (0.1690 -- 0.5099)  data: 0.0202 (0.0001 -- 0.2586)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4520 (0.6085)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2094 (0.1334 -- 0.5099)  data: 0.0199 (0.0001 -- 0.2586)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 84.647 Acc@5 98.133 loss 0.597
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [167]  [  0/160]  eta: 0:21:30  lr: 0.000003  min_lr: 0.000000  loss: 2.0254 (2.0254)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7564 (5.7564)  time: 8.0629 (8.0629 -- 8.0629)  data: 7.4909 (7.4909 -- 7.4909)  max mem: 16413
Epoch: [167]  [ 20/160]  eta: 0:02:47  lr: 0.000003  min_lr: 0.000000  loss: 1.6394 (1.7053)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3179 (8.4975)  time: 0.8561 (0.5227 -- 3.6293)  data: 0.2305 (0.0003 -- 3.0874)  max mem: 16413
[2023-08-31 18:02:57,702] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:02:57,703] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:02:57,704] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:02:57,704] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [167]  [ 40/160]  eta: 0:01:57  lr: 0.000003  min_lr: 0.000000  loss: 1.6026 (1.6482)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5907 (8.1742)  time: 0.7485 (0.5244 -- 2.0438)  data: 0.0530 (0.0002 -- 1.0348)  max mem: 16413
Epoch: [167]  [ 60/160]  eta: 0:01:35  lr: 0.000003  min_lr: 0.000000  loss: 1.6976 (1.6676)  loss_scale: 32768.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8834 (8.2842)  time: 0.9122 (0.5216 -- 2.4705)  data: 0.0615 (0.0004 -- 0.8243)  max mem: 16413
[2023-08-31 18:03:19,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26781
[2023-08-31 18:03:19,334] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26781
[2023-08-31 18:03:19,334] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:03:19,334] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:03:19,334] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [167]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.7336 (1.6693)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6411 (8.3933)  time: 0.8783 (0.5129 -- 2.3980)  data: 0.0352 (0.0003 -- 0.4160)  max mem: 16413
Epoch: [167]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.8519 (1.6940)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6027 (8.6735)  time: 0.9457 (0.5223 -- 3.3236)  data: 0.0016 (0.0004 -- 0.0077)  max mem: 16413
Epoch: [167]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.8412 (1.6977)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6690 (8.7897)  time: 0.8517 (0.5198 -- 2.4437)  data: 0.0712 (0.0002 -- 1.3982)  max mem: 16413
Epoch: [167]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7569 (1.7091)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6415 (8.8289)  time: 0.8467 (0.5226 -- 2.4896)  data: 0.3044 (0.0001 -- 1.9768)  max mem: 16413
Epoch: [167]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7323 (1.7123)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6127 (8.8096)  time: 0.6652 (0.4962 -- 3.3160)  data: 0.1405 (0.0002 -- 2.7932)  max mem: 16413
Epoch: [167] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7323 (1.7063)  loss_scale: 16384.0000 (19046.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6127 (8.8096)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1788 (0.1788)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5224 (2.5224 -- 2.5224)  data: 2.2798 (2.2798 -- 2.2798)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3307 (0.6165)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4386 (0.2051 -- 2.5224)  data: 0.2129 (0.0005 -- 2.2798)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4110 (0.5540)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.8836)  time: 0.2172 (0.1681 -- 0.2975)  data: 0.0091 (0.0001 -- 0.1166)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4667 (0.6099)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.1993 (0.1328 -- 0.2975)  data: 0.0087 (0.0001 -- 0.1166)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.598
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [168]  [  0/160]  eta: 0:21:19  lr: 0.000003  min_lr: 0.000000  loss: 2.0991 (2.0991)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3330 (8.3330)  time: 7.9993 (7.9993 -- 7.9993)  data: 5.7481 (5.7481 -- 5.7481)  max mem: 16413
Epoch: [168]  [ 20/160]  eta: 0:02:52  lr: 0.000003  min_lr: 0.000000  loss: 1.6727 (1.6851)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1052 (8.6974)  time: 0.8969 (0.5164 -- 4.1595)  data: 0.1210 (0.0005 -- 2.3916)  max mem: 16413
[2023-08-31 18:05:23,772] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:05:23,773] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:05:23,773] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:05:23,774] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [168]  [ 40/160]  eta: 0:02:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6125 (1.7092)  loss_scale: 32768.0000 (20779.7073)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1774 (8.6777)  time: 0.7676 (0.5329 -- 3.0365)  data: 0.0013 (0.0002 -- 0.0026)  max mem: 16413
[2023-08-31 18:05:32,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26922
[2023-08-31 18:05:32,603] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26922
[2023-08-31 18:05:32,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:05:32,604] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:05:32,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [168]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7789 (1.7202)  loss_scale: 16384.0000 (19607.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4020 (8.8159)  time: 0.8911 (0.5198 -- 3.8129)  data: 0.0291 (0.0003 -- 0.5437)  max mem: 16413
Epoch: [168]  [ 80/160]  eta: 0:01:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7006 (1.7198)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4211 (8.8350)  time: 1.0096 (0.5306 -- 3.7759)  data: 0.0586 (0.0004 -- 1.1221)  max mem: 16413
Epoch: [168]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.5427 (1.7078)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9013 (8.8207)  time: 0.8092 (0.5258 -- 3.7209)  data: 0.0019 (0.0003 -- 0.0132)  max mem: 16413
[2023-08-31 18:06:32,439] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26989
[2023-08-31 18:06:32,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:06:32,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26989
[2023-08-31 18:06:32,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:06:32,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 18:06:39,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=160, lr=[7.016158269260065e-08, 7.016158269260065e-08, 9.354877692346754e-08, 9.354877692346754e-08, 1.247317025646234e-07, 1.247317025646234e-07, 1.6630893675283117e-07, 1.6630893675283117e-07, 2.217452490037749e-07, 2.217452490037749e-07, 2.956603320050332e-07, 2.956603320050332e-07, 3.9421377600671096e-07, 3.9421377600671096e-07, 5.256183680089479e-07, 5.256183680089479e-07, 7.008244906785972e-07, 7.008244906785972e-07, 9.344326542381296e-07, 9.344326542381296e-07, 1.2459102056508394e-06, 1.2459102056508394e-06, 1.6612136075344526e-06, 1.6612136075344526e-06, 2.2149514767126034e-06, 2.2149514767126034e-06, 2.9532686356168047e-06, 2.9532686356168047e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 18:06:39,240] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=16.844348179279, CurrSamplesPerSec=21.453344233702087, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [168]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.5963 (1.7049)  loss_scale: 8192.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3997 (8.8169)  time: 0.7194 (0.5180 -- 2.4488)  data: 0.1318 (0.0003 -- 1.9040)  max mem: 16413
Epoch: [168]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.4391 (1.6829)  loss_scale: 8192.0000 (15919.2057)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5905 (8.7323)  time: 0.9739 (0.5125 -- 4.9158)  data: 0.4248 (0.0004 -- 4.4008)  max mem: 16413
Epoch: [168]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8270 (1.6890)  loss_scale: 8192.0000 (15001.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1215 (8.6831)  time: 0.6262 (0.4950 -- 2.3154)  data: 0.0971 (0.0002 -- 1.7913)  max mem: 16413
Epoch: [168] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8270 (1.6732)  loss_scale: 8192.0000 (15001.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1215 (8.6831)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1746 (0.1746)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4572 (2.4572 -- 2.4572)  data: 2.2418 (2.2418 -- 2.2418)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3224 (0.6169)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4448 (0.1984 -- 2.4572)  data: 0.2344 (0.0004 -- 2.2418)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4146 (0.5539)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2180 (0.1697 -- 0.5547)  data: 0.0170 (0.0001 -- 0.3295)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4812 (0.6109)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.2043 (0.1332 -- 0.5547)  data: 0.0168 (0.0001 -- 0.3295)  max mem: 16413
Val: Total time: 0:00:07 (0.2890 s / it)
* Acc@1 84.855 Acc@5 98.133 loss 0.598
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [169]  [  0/160]  eta: 0:22:14  lr: 0.000003  min_lr: 0.000000  loss: 1.5875 (1.5875)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8666 (9.8666)  time: 8.3417 (8.3417 -- 8.3417)  data: 7.8123 (7.8123 -- 7.8123)  max mem: 16413
Epoch: [169]  [ 20/160]  eta: 0:02:42  lr: 0.000003  min_lr: 0.000000  loss: 1.7943 (1.7643)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3508 (9.2739)  time: 0.8051 (0.5243 -- 3.7726)  data: 0.2578 (0.0004 -- 3.2360)  max mem: 16413
Epoch: [169]  [ 40/160]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000000  loss: 1.5977 (1.6510)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0722 (8.5264)  time: 0.8468 (0.5203 -- 2.4713)  data: 0.2579 (0.0003 -- 1.9473)  max mem: 16413
Epoch: [169]  [ 60/160]  eta: 0:01:36  lr: 0.000003  min_lr: 0.000000  loss: 1.9234 (1.7130)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0956 (8.4882)  time: 0.8857 (0.5340 -- 2.3946)  data: 0.2282 (0.0005 -- 1.7750)  max mem: 16413
[2023-08-31 18:08:36,215] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:08:36,215] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:08:36,215] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:08:36,215] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [169]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000000  loss: 1.6284 (1.7195)  loss_scale: 8192.0000 (8495.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1367 (8.4700)  time: 0.9572 (0.5144 -- 5.1045)  data: 0.4130 (0.0002 -- 4.5860)  max mem: 16413
Epoch: [169]  [100/160]  eta: 0:00:57  lr: 0.000003  min_lr: 0.000000  loss: 1.6586 (1.7195)  loss_scale: 16384.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2170 (8.5446)  time: 0.9077 (0.5280 -- 4.0078)  data: 0.3659 (0.0004 -- 3.4756)  max mem: 16413
Epoch: [169]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7623 (1.7298)  loss_scale: 16384.0000 (11103.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0680 (8.6650)  time: 0.6603 (0.5232 -- 1.4742)  data: 0.1073 (0.0005 -- 0.9409)  max mem: 16413
Epoch: [169]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.6421 (1.7255)  loss_scale: 16384.0000 (11852.2553)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3160 (8.5671)  time: 1.0034 (0.5292 -- 4.3466)  data: 0.4518 (0.0004 -- 3.8214)  max mem: 16413
Epoch: [169]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6643 (1.7256)  loss_scale: 16384.0000 (12390.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4182 (8.5386)  time: 0.6315 (0.4953 -- 2.7719)  data: 0.1140 (0.0001 -- 2.2441)  max mem: 16413
Epoch: [169] Total time: 0:02:21 (0.8861 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6643 (1.6976)  loss_scale: 16384.0000 (12390.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4182 (8.5386)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1709 (0.1709)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3766 (2.3766 -- 2.3766)  data: 2.1487 (2.1487 -- 2.1487)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3209 (0.6165)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4170 (0.2008 -- 2.3766)  data: 0.2031 (0.0005 -- 2.1487)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4048 (0.5531)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2185 (0.1691 -- 0.3743)  data: 0.0114 (0.0001 -- 0.1388)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4789 (0.6102)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2029 (0.1325 -- 0.3743)  data: 0.0111 (0.0001 -- 0.1388)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.597
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [170]  [  0/160]  eta: 0:18:38  lr: 0.000003  min_lr: 0.000000  loss: 1.8357 (1.8357)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4296 (11.4296)  time: 6.9903 (6.9903 -- 6.9903)  data: 5.5647 (5.5647 -- 5.5647)  max mem: 16413
Epoch: [170]  [ 20/160]  eta: 0:02:34  lr: 0.000003  min_lr: 0.000000  loss: 1.5667 (1.5977)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7250 (8.4391)  time: 0.8056 (0.5312 -- 3.0131)  data: 0.2108 (0.0003 -- 2.1692)  max mem: 16413
Epoch: [170]  [ 40/160]  eta: 0:02:05  lr: 0.000003  min_lr: 0.000000  loss: 1.4457 (1.5340)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7200 (8.4211)  time: 0.9869 (0.5305 -- 3.7681)  data: 0.0241 (0.0003 -- 0.4498)  max mem: 16413
[2023-08-31 18:10:34,811] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:10:34,811] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:10:34,814] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:10:34,814] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [170]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000000  loss: 1.8541 (1.6458)  loss_scale: 32768.0000 (20412.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8226 (8.4540)  time: 0.9103 (0.5338 -- 3.2848)  data: 0.0145 (0.0003 -- 0.2473)  max mem: 16413
Epoch: [170]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.8313 (1.6810)  loss_scale: 32768.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5158 (8.7206)  time: 0.8059 (0.5226 -- 3.5069)  data: 0.0012 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [170]  [100/160]  eta: 0:00:57  lr: 0.000003  min_lr: 0.000000  loss: 1.7006 (1.6852)  loss_scale: 32768.0000 (25305.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9281 (8.8319)  time: 0.9644 (0.5201 -- 4.1357)  data: 0.0011 (0.0005 -- 0.0025)  max mem: 16413
[2023-08-31 18:11:30,510] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27308
[2023-08-31 18:11:30,510] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27308
[2023-08-31 18:11:30,552] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:11:30,552] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:11:30,553] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [170]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7081 (1.6829)  loss_scale: 16384.0000 (24779.1074)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1110 (8.9486)  time: 0.7317 (0.5333 -- 3.0020)  data: 0.0016 (0.0005 -- 0.0030)  max mem: 16413
Epoch: [170]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.6257 (1.6837)  loss_scale: 16384.0000 (23588.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4731 (8.9575)  time: 0.9128 (0.5043 -- 3.7879)  data: 0.0015 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [170]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6044 (1.6807)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2569 (8.8562)  time: 0.6458 (0.4954 -- 1.7346)  data: 0.0017 (0.0002 -- 0.0155)  max mem: 16413
Epoch: [170] Total time: 0:02:21 (0.8857 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6044 (1.6832)  loss_scale: 16384.0000 (22732.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2569 (8.8562)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1706 (0.1706)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3836 (2.3836 -- 2.3836)  data: 2.1111 (2.1111 -- 2.1111)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3250 (0.6152)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4183 (0.1923 -- 2.3836)  data: 0.2005 (0.0007 -- 2.1111)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4079 (0.5516)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.8836)  time: 0.2209 (0.1703 -- 0.4783)  data: 0.0189 (0.0001 -- 0.2809)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4660 (0.6090)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.2061 (0.1333 -- 0.4783)  data: 0.0187 (0.0001 -- 0.2809)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.596
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [171]  [  0/160]  eta: 0:26:36  lr: 0.000003  min_lr: 0.000000  loss: 1.6442 (1.6442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0970 (9.0970)  time: 9.9794 (9.9794 -- 9.9794)  data: 9.4461 (9.4461 -- 9.4461)  max mem: 16413
Epoch: [171]  [ 20/160]  eta: 0:02:49  lr: 0.000003  min_lr: 0.000000  loss: 1.6212 (1.6233)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3012 (8.9190)  time: 0.7692 (0.5219 -- 4.3323)  data: 0.2253 (0.0005 -- 3.8121)  max mem: 16413
Epoch: [171]  [ 40/160]  eta: 0:02:04  lr: 0.000003  min_lr: 0.000000  loss: 1.8379 (1.6962)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1783 (8.5648)  time: 0.8649 (0.5260 -- 3.6500)  data: 0.0383 (0.0003 -- 0.7270)  max mem: 16413
Epoch: [171]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.5250 (1.6568)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7852 (8.7951)  time: 0.8395 (0.5319 -- 3.5424)  data: 0.0017 (0.0005 -- 0.0032)  max mem: 16413
[2023-08-31 18:13:32,643] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:13:32,643] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:13:32,644] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:13:32,644] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:13:33,736] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27439
[2023-08-31 18:13:33,736] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:13:33,736] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27439
[2023-08-31 18:13:33,736] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:13:33,737] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [171]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.5683 (1.6562)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6109 (8.5861)  time: 0.8264 (0.5304 -- 3.6287)  data: 0.0120 (0.0004 -- 0.2127)  max mem: 16413
Epoch: [171]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.5553 (1.6644)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7719 (8.6462)  time: 0.8746 (0.5275 -- 3.7672)  data: 0.2382 (0.0003 -- 3.2209)  max mem: 16413
Epoch: [171]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7049 (1.6730)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8232 (8.5850)  time: 0.8474 (0.5347 -- 2.7552)  data: 0.2067 (0.0004 -- 1.8888)  max mem: 16413
Epoch: [171]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5564 (1.6629)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0280 (8.5844)  time: 0.9492 (0.5257 -- 3.4017)  data: 0.3313 (0.0004 -- 2.8736)  max mem: 16413
Epoch: [171]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7615 (1.6632)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3183 (8.7773)  time: 0.6629 (0.4960 -- 3.4646)  data: 0.1481 (0.0002 -- 2.9496)  max mem: 16413
Epoch: [171] Total time: 0:02:22 (0.8885 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7615 (1.6812)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3183 (8.7773)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1693 (0.1693)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5069 (2.5069 -- 2.5069)  data: 2.2608 (2.2608 -- 2.2608)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3237 (0.6134)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4407 (0.2052 -- 2.5069)  data: 0.2201 (0.0006 -- 2.2608)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4068 (0.5510)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2184 (0.1695 -- 0.3932)  data: 0.0143 (0.0001 -- 0.1511)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4552 (0.6085)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.2019 (0.1326 -- 0.3932)  data: 0.0141 (0.0001 -- 0.1511)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [172]  [  0/160]  eta: 0:22:25  lr: 0.000002  min_lr: 0.000000  loss: 2.0196 (2.0196)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6338 (10.6338)  time: 8.4119 (8.4119 -- 8.4119)  data: 7.9042 (7.9042 -- 7.9042)  max mem: 16413
Epoch: [172]  [ 20/160]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000000  loss: 1.7502 (1.7662)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5627 (9.9952)  time: 0.8926 (0.5269 -- 4.6993)  data: 0.1409 (0.0002 -- 1.7258)  max mem: 16413
Epoch: [172]  [ 40/160]  eta: 0:02:16  lr: 0.000002  min_lr: 0.000000  loss: 1.7024 (1.7121)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1495 (9.1874)  time: 1.0160 (0.5060 -- 5.6707)  data: 0.0011 (0.0003 -- 0.0037)  max mem: 16413
[2023-08-31 18:15:41,195] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:15:41,195] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:15:41,198] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:15:41,198] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:15:45,009] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27575
[2023-08-31 18:15:45,009] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27575
[2023-08-31 18:15:45,010] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:15:45,010] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:15:45,010] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [172]  [ 60/160]  eta: 0:01:40  lr: 0.000002  min_lr: 0.000000  loss: 1.7397 (1.7171)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4768 (9.0915)  time: 0.7447 (0.5255 -- 2.5375)  data: 0.0027 (0.0004 -- 0.0162)  max mem: 16413
Epoch: [172]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.9169 (1.7371)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6494 (9.0893)  time: 0.7628 (0.5278 -- 2.3991)  data: 0.0943 (0.0002 -- 1.8451)  max mem: 16413
Epoch: [172]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.6212 (1.7356)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3892 (8.9307)  time: 0.9185 (0.5272 -- 3.1233)  data: 0.3512 (0.0003 -- 2.5944)  max mem: 16413
Epoch: [172]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9300 (1.7653)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7847 (9.1082)  time: 0.9266 (0.5174 -- 4.5097)  data: 0.3555 (0.0004 -- 3.9745)  max mem: 16413
Epoch: [172]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.3831 (1.7390)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0662 (9.0968)  time: 0.9011 (0.5354 -- 3.3337)  data: 0.3497 (0.0003 -- 2.8110)  max mem: 16413
Epoch: [172]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6967 (1.7338)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9910 (9.1105)  time: 0.6383 (0.4943 -- 1.6018)  data: 0.1144 (0.0002 -- 1.0693)  max mem: 16413
Epoch: [172] Total time: 0:02:23 (0.8994 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6967 (1.6952)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9910 (9.1105)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1724 (0.1724)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5686 (2.5686 -- 2.5686)  data: 2.2996 (2.2996 -- 2.2996)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3252 (0.6109)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4269 (0.1981 -- 2.5686)  data: 0.2110 (0.0006 -- 2.2996)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4101 (0.5486)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2126 (0.1690 -- 0.3864)  data: 0.0118 (0.0001 -- 0.2127)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4553 (0.6068)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.5104)  time: 0.1986 (0.1327 -- 0.3864)  data: 0.0116 (0.0001 -- 0.2127)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.595
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [173]  [  0/160]  eta: 0:21:50  lr: 0.000002  min_lr: 0.000000  loss: 1.9351 (1.9351)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1923 (7.1923)  time: 8.1897 (8.1897 -- 8.1897)  data: 5.8815 (5.8815 -- 5.8815)  max mem: 16413
Epoch: [173]  [ 20/160]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8412 (1.8261)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8094 (8.0321)  time: 0.9104 (0.5153 -- 2.8023)  data: 0.0029 (0.0005 -- 0.0151)  max mem: 16413
[2023-08-31 18:17:50,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:17:50,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:17:50,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:17:50,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:17:51,235] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27706
[2023-08-31 18:17:51,235] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27706
[2023-08-31 18:17:51,236] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:17:51,236] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:17:51,236] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [173]  [ 40/160]  eta: 0:02:13  lr: 0.000002  min_lr: 0.000000  loss: 1.7930 (1.7761)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7462 (8.2196)  time: 0.9619 (0.5244 -- 4.1070)  data: 0.0027 (0.0003 -- 0.0155)  max mem: 16413
Epoch: [173]  [ 60/160]  eta: 0:01:41  lr: 0.000002  min_lr: 0.000000  loss: 1.4053 (1.7031)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1614 (8.3224)  time: 0.8036 (0.5195 -- 3.8240)  data: 0.0013 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [173]  [ 80/160]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5659 (1.6880)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4799 (8.1649)  time: 0.8690 (0.5238 -- 4.0599)  data: 0.0019 (0.0002 -- 0.0064)  max mem: 16413
Epoch: [173]  [100/160]  eta: 0:00:58  lr: 0.000002  min_lr: 0.000000  loss: 1.6395 (1.6754)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1824 (8.2758)  time: 0.9776 (0.5077 -- 5.4479)  data: 0.0015 (0.0003 -- 0.0096)  max mem: 16413
Epoch: [173]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7548 (1.6769)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1309 (8.4223)  time: 0.7308 (0.5237 -- 3.2514)  data: 0.0016 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [173]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.9270 (1.7017)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2230 (8.4740)  time: 0.8540 (0.5214 -- 4.0070)  data: 0.0018 (0.0004 -- 0.0078)  max mem: 16413
[2023-08-31 18:19:41,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:19:41,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:19:41,478] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:19:41,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [173]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6384 (1.6997)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4953 (8.5436)  time: 0.6756 (0.4951 -- 3.2965)  data: 0.0008 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [173] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6384 (1.6963)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4953 (8.5436)
Val:  [ 0/27]  eta: 0:01:12  loss: 0.1740 (0.1740)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6674 (2.6674 -- 2.6674)  data: 2.3890 (2.3890 -- 2.3890)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3126 (0.6100)  acc1: 88.8889 (83.8384)  acc5: 100.0000 (97.9798)  time: 0.4430 (0.2072 -- 2.6674)  data: 0.2183 (0.0007 -- 2.3890)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4226 (0.5499)  acc1: 88.8889 (86.2434)  acc5: 100.0000 (97.3545)  time: 0.2093 (0.1691 -- 0.2438)  data: 0.0008 (0.0001 -- 0.0017)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4786 (0.6069)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.0954)  time: 0.1919 (0.1330 -- 0.2438)  data: 0.0004 (0.0001 -- 0.0017)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.594
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 85.68%
Epoch: [174]  [  0/160]  eta: 0:16:41  lr: 0.000002  min_lr: 0.000000  loss: 1.5620 (1.5620)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1912 (7.1912)  time: 6.2611 (6.2611 -- 6.2611)  data: 5.2384 (5.2384 -- 5.2384)  max mem: 16413
[2023-08-31 18:20:06,516] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27850
[2023-08-31 18:20:06,516] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27850
[2023-08-31 18:20:06,516] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:20:06,516] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:20:06,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [174]  [ 20/160]  eta: 0:02:38  lr: 0.000002  min_lr: 0.000000  loss: 1.6208 (1.6102)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8448 (8.2013)  time: 0.8732 (0.5128 -- 2.6058)  data: 0.0227 (0.0002 -- 0.4215)  max mem: 16413
[2023-08-31 18:20:31,563] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27876
[2023-08-31 18:20:31,563] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:20:31,563] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27876
[2023-08-31 18:20:31,563] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 18:20:31,563] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [174]  [ 40/160]  eta: 0:02:03  lr: 0.000002  min_lr: 0.000000  loss: 1.8387 (1.6850)  loss_scale: 16384.0000 (19381.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6281 (8.6528)  time: 0.9259 (0.4985 -- 4.1043)  data: 0.0015 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [174]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.6969 (1.6798)  loss_scale: 8192.0000 (15712.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4110 (8.7250)  time: 0.9144 (0.5309 -- 3.5560)  data: 0.0615 (0.0007 -- 1.1961)  max mem: 16413
Epoch: [174]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000000  loss: 1.5491 (1.6603)  loss_scale: 8192.0000 (13855.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4089 (8.8521)  time: 0.9155 (0.5217 -- 5.1005)  data: 0.0288 (0.0004 -- 0.5538)  max mem: 16413
Epoch: [174]  [100/160]  eta: 0:00:57  lr: 0.000002  min_lr: 0.000000  loss: 1.4981 (1.6443)  loss_scale: 8192.0000 (12734.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4106 (8.9937)  time: 0.9064 (0.5149 -- 3.3494)  data: 0.0012 (0.0003 -- 0.0024)  max mem: 16413
Epoch: [174]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7128 (1.6534)  loss_scale: 8192.0000 (11983.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7393 (9.0073)  time: 0.8264 (0.5283 -- 3.5888)  data: 0.0019 (0.0007 -- 0.0096)  max mem: 16413
Epoch: [174]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6151 (1.6703)  loss_scale: 8192.0000 (11445.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4174 (8.9416)  time: 0.8839 (0.5245 -- 4.4698)  data: 0.0017 (0.0004 -- 0.0071)  max mem: 16413
[2023-08-31 18:22:14,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=166, lr=[4.564733825380591e-08, 4.564733825380591e-08, 6.086311767174122e-08, 6.086311767174122e-08, 8.115082356232162e-08, 8.115082356232162e-08, 1.082010980830955e-07, 1.082010980830955e-07, 1.4426813077746065e-07, 1.4426813077746065e-07, 1.923575077032809e-07, 1.923575077032809e-07, 2.5647667693770787e-07, 2.5647667693770787e-07, 3.4196890258361044e-07, 3.4196890258361044e-07, 4.5595853677814725e-07, 4.5595853677814725e-07, 6.079447157041963e-07, 6.079447157041963e-07, 8.105929542722618e-07, 8.105929542722618e-07, 1.0807906056963492e-06, 1.0807906056963492e-06, 1.4410541409284656e-06, 1.4410541409284656e-06, 1.921405521237954e-06, 1.921405521237954e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 18:22:14,568] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=16.83885560666922, CurrSamplesPerSec=24.752908359205907, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [174]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6273 (1.6698)  loss_scale: 8192.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4973 (9.0430)  time: 0.6245 (0.4942 -- 2.3568)  data: 0.0009 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [174] Total time: 0:02:23 (0.8947 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6273 (1.6757)  loss_scale: 8192.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4973 (9.0430)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1736 (0.1736)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5709 (2.5709 -- 2.5709)  data: 2.3538 (2.3538 -- 2.3538)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3069 (0.6156)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4563 (0.2058 -- 2.5709)  data: 0.2377 (0.0007 -- 2.3538)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4285 (0.5530)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2174 (0.1699 -- 0.4966)  data: 0.0132 (0.0001 -- 0.2477)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4726 (0.6094)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.0954)  time: 0.2011 (0.1324 -- 0.4966)  data: 0.0127 (0.0001 -- 0.2477)  max mem: 16413
Val: Total time: 0:00:07 (0.2926 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.594
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 85.68%
Epoch: [175]  [  0/160]  eta: 0:17:32  lr: 0.000002  min_lr: 0.000000  loss: 1.5785 (1.5785)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6187 (7.6187)  time: 6.5767 (6.5767 -- 6.5767)  data: 5.6573 (5.6573 -- 5.6573)  max mem: 16413
[2023-08-31 18:22:31,998] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:22:31,998] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:22:31,999] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:22:31,999] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [175]  [ 20/160]  eta: 0:02:35  lr: 0.000002  min_lr: 0.000000  loss: 1.5505 (1.6113)  loss_scale: 16384.0000 (14433.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3696 (8.4703)  time: 0.8350 (0.5264 -- 3.0239)  data: 0.1924 (0.0003 -- 2.5010)  max mem: 16413
Epoch: [175]  [ 40/160]  eta: 0:02:01  lr: 0.000002  min_lr: 0.000000  loss: 1.5888 (1.5951)  loss_scale: 16384.0000 (15384.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1535 (8.6649)  time: 0.9146 (0.5274 -- 3.4649)  data: 0.2289 (0.0005 -- 2.9476)  max mem: 16413
Epoch: [175]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.6230 (1.6409)  loss_scale: 16384.0000 (15712.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8742 (8.8733)  time: 0.8659 (0.5271 -- 2.7845)  data: 0.1092 (0.0005 -- 1.8098)  max mem: 16413
Epoch: [175]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000000  loss: 1.7892 (1.6585)  loss_scale: 16384.0000 (15878.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4783 (8.8452)  time: 0.9636 (0.5140 -- 4.6767)  data: 0.0012 (0.0003 -- 0.0024)  max mem: 16413
[2023-08-31 18:23:57,152] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28098
[2023-08-31 18:23:57,153] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:23:57,153] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28098
[2023-08-31 18:23:57,153] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:23:57,153] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [175]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.5752 (1.6506)  loss_scale: 16384.0000 (15735.1287)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1969 (8.7427)  time: 0.8820 (0.5277 -- 3.9421)  data: 0.0014 (0.0006 -- 0.0029)  max mem: 16413
Epoch: [175]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.6659 (1.6463)  loss_scale: 8192.0000 (14488.3306)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0051 (8.8066)  time: 0.9446 (0.5185 -- 3.9932)  data: 0.0012 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [175]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.4873 (1.6172)  loss_scale: 8192.0000 (13595.2340)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1124 (8.6939)  time: 0.7311 (0.5252 -- 3.3377)  data: 0.0014 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [175]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6532 (1.6296)  loss_scale: 8192.0000 (12953.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6793 (8.6677)  time: 0.7185 (0.4948 -- 4.2204)  data: 0.0007 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [175] Total time: 0:02:23 (0.8948 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6532 (1.6651)  loss_scale: 8192.0000 (12953.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6793 (8.6677)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1728 (0.1728)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5209 (2.5209 -- 2.5209)  data: 2.3117 (2.3117 -- 2.3117)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3089 (0.6116)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4369 (0.1948 -- 2.5209)  data: 0.2275 (0.0007 -- 2.3117)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4329 (0.5505)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2165 (0.1713 -- 0.4024)  data: 0.0171 (0.0001 -- 0.1808)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4680 (0.6067)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.0954)  time: 0.2032 (0.1335 -- 0.4024)  data: 0.0168 (0.0001 -- 0.1808)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.592
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [176]  [  0/160]  eta: 0:15:26  lr: 0.000002  min_lr: 0.000000  loss: 2.1211 (2.1211)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2500 (7.2500)  time: 5.7926 (5.7926 -- 5.7926)  data: 5.2322 (5.2322 -- 5.2322)  max mem: 16413
Epoch: [176]  [ 20/160]  eta: 0:02:43  lr: 0.000002  min_lr: 0.000000  loss: 1.8195 (1.8245)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6546 (8.5265)  time: 0.9376 (0.5214 -- 3.1423)  data: 0.3841 (0.0007 -- 2.6197)  max mem: 16413
Epoch: [176]  [ 40/160]  eta: 0:02:01  lr: 0.000002  min_lr: 0.000000  loss: 1.6188 (1.7217)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0897 (8.9009)  time: 0.8507 (0.5160 -- 3.9048)  data: 0.3109 (0.0002 -- 3.3730)  max mem: 16413
Epoch: [176]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.5635 (1.7024)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4987 (8.7238)  time: 0.9285 (0.5310 -- 2.9675)  data: 0.3684 (0.0008 -- 2.4540)  max mem: 16413
[2023-08-31 18:25:58,607] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:25:58,607] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:25:58,611] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:25:58,611] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [176]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 1.7909 (1.7265)  loss_scale: 16384.0000 (9607.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2707 (8.7432)  time: 0.7696 (0.5325 -- 2.4909)  data: 0.0563 (0.0006 -- 1.0952)  max mem: 16413
Epoch: [176]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.7278 (1.7379)  loss_scale: 16384.0000 (10949.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0445 (8.7394)  time: 0.8800 (0.5223 -- 4.4912)  data: 0.0150 (0.0004 -- 0.2611)  max mem: 16413
Epoch: [176]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.6376 (1.7362)  loss_scale: 16384.0000 (11847.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7200 (8.6867)  time: 0.9858 (0.5308 -- 4.1604)  data: 0.0015 (0.0005 -- 0.0047)  max mem: 16413
Epoch: [176]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.5677 (1.7162)  loss_scale: 16384.0000 (12491.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0070 (8.7775)  time: 0.8253 (0.5199 -- 4.2648)  data: 0.0012 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [176]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8482 (1.7174)  loss_scale: 16384.0000 (12953.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5092 (8.8132)  time: 0.6769 (0.4948 -- 2.9369)  data: 0.0009 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [176] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8482 (1.6841)  loss_scale: 16384.0000 (12953.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5092 (8.8132)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1714 (0.1714)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5020 (2.5020 -- 2.5020)  data: 2.2712 (2.2712 -- 2.2712)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3075 (0.6103)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4612 (0.1997 -- 2.5020)  data: 0.2430 (0.0007 -- 2.2712)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4301 (0.5492)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2214 (0.1688 -- 0.6096)  data: 0.0215 (0.0001 -- 0.3798)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4651 (0.6058)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.0954)  time: 0.2035 (0.1324 -- 0.6096)  data: 0.0206 (0.0001 -- 0.3798)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.592
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [177]  [  0/160]  eta: 0:22:17  lr: 0.000002  min_lr: 0.000000  loss: 1.7860 (1.7860)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7051 (7.7051)  time: 8.3589 (8.3589 -- 8.3589)  data: 6.1784 (6.1784 -- 6.1784)  max mem: 16413
Epoch: [177]  [ 20/160]  eta: 0:02:47  lr: 0.000002  min_lr: 0.000000  loss: 1.7667 (1.6480)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7388 (9.0243)  time: 0.8412 (0.5210 -- 3.4689)  data: 0.2934 (0.0006 -- 2.9135)  max mem: 16413
[2023-08-31 18:28:01,245] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:28:01,245] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:28:01,246] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:28:01,246] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [177]  [ 40/160]  eta: 0:02:08  lr: 0.000002  min_lr: 0.000000  loss: 1.7655 (1.6902)  loss_scale: 16384.0000 (18781.6585)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3288 (9.0653)  time: 0.9301 (0.5199 -- 4.0940)  data: 0.3886 (0.0001 -- 3.5802)  max mem: 16413
Epoch: [177]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.7837 (1.7260)  loss_scale: 32768.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4195 (9.0533)  time: 0.8249 (0.5204 -- 3.8085)  data: 0.0558 (0.0003 -- 1.0865)  max mem: 16413
[2023-08-31 18:28:25,148] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28382
[2023-08-31 18:28:25,148] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28382
[2023-08-31 18:28:25,149] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:28:25,149] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:28:25,149] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 18:28:31,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28387
[2023-08-31 18:28:31,102] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28387
[2023-08-31 18:28:31,102] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:28:31,102] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:28:31,103] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [177]  [ 80/160]  eta: 0:01:18  lr: 0.000002  min_lr: 0.000000  loss: 1.4127 (1.6638)  loss_scale: 8192.0000 (20429.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2720 (9.2121)  time: 0.9717 (0.5305 -- 3.8970)  data: 0.0021 (0.0001 -- 0.0067)  max mem: 16413
Epoch: [177]  [100/160]  eta: 0:00:57  lr: 0.000002  min_lr: 0.000000  loss: 1.7682 (1.6911)  loss_scale: 8192.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3412 (9.2972)  time: 0.8345 (0.5171 -- 4.0434)  data: 0.0017 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [177]  [120/160]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000000  loss: 1.6044 (1.6824)  loss_scale: 8192.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3601 (9.2693)  time: 0.9353 (0.5414 -- 3.3095)  data: 0.0021 (0.0005 -- 0.0057)  max mem: 16413
Epoch: [177]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6033 (1.6802)  loss_scale: 8192.0000 (15222.0142)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7511 (9.0723)  time: 0.7344 (0.5207 -- 2.7858)  data: 0.0013 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [177]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7343 (1.6811)  loss_scale: 8192.0000 (14387.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4076 (9.0250)  time: 0.6887 (0.4969 -- 3.8343)  data: 0.0015 (0.0002 -- 0.0146)  max mem: 16413
Epoch: [177] Total time: 0:02:23 (0.8940 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7343 (1.6870)  loss_scale: 8192.0000 (14387.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4076 (9.0250)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1708 (0.1708)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4603 (2.4603 -- 2.4603)  data: 2.2251 (2.2251 -- 2.2251)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3065 (0.6102)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4382 (0.1999 -- 2.4603)  data: 0.2206 (0.0007 -- 2.2251)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4340 (0.5485)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2142 (0.1698 -- 0.4048)  data: 0.0122 (0.0001 -- 0.1813)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4611 (0.6047)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.0954)  time: 0.1968 (0.1331 -- 0.4048)  data: 0.0113 (0.0001 -- 0.1813)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 85.270 Acc@5 97.925 loss 0.591
Accuracy of the network on the 482 val images: 85.27%
Max accuracy: 85.68%
Epoch: [178]  [  0/160]  eta: 0:17:18  lr: 0.000002  min_lr: 0.000000  loss: 2.2583 (2.2583)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5446 (6.5446)  time: 6.4897 (6.4897 -- 6.4897)  data: 5.9342 (5.9342 -- 5.9342)  max mem: 16413
Epoch: [178]  [ 20/160]  eta: 0:02:42  lr: 0.000001  min_lr: 0.000000  loss: 1.7384 (1.7231)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4521 (8.7847)  time: 0.8969 (0.5169 -- 3.2710)  data: 0.1476 (0.0003 -- 1.3732)  max mem: 16413
[2023-08-31 18:30:31,677] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:30:31,677] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:30:31,678] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:30:31,678] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [178]  [ 40/160]  eta: 0:02:03  lr: 0.000001  min_lr: 0.000000  loss: 1.7523 (1.7129)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2646 (8.5632)  time: 0.8953 (0.5260 -- 3.6181)  data: 0.2309 (0.0003 -- 3.0842)  max mem: 16413
Epoch: [178]  [ 60/160]  eta: 0:01:33  lr: 0.000001  min_lr: 0.000000  loss: 1.7139 (1.6863)  loss_scale: 16384.0000 (11549.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8452 (8.5735)  time: 0.7203 (0.5209 -- 2.1557)  data: 0.1000 (0.0004 -- 1.6224)  max mem: 16413
Epoch: [178]  [ 80/160]  eta: 0:01:13  lr: 0.000001  min_lr: 0.000000  loss: 1.6935 (1.6901)  loss_scale: 16384.0000 (12743.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3687 (8.7790)  time: 0.8900 (0.5289 -- 2.2918)  data: 0.1966 (0.0004 -- 1.0384)  max mem: 16413
[2023-08-31 18:31:11,910] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28562
[2023-08-31 18:31:11,910] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:31:11,910] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 18:31:11,910] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28562
[2023-08-31 18:31:11,910] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [178]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6170 (1.6831)  loss_scale: 8192.0000 (11923.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5407 (8.7414)  time: 0.9158 (0.5249 -- 2.8429)  data: 0.1173 (0.0002 -- 2.3150)  max mem: 16413
Epoch: [178]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8244 (1.7019)  loss_scale: 8192.0000 (11306.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5639 (9.1669)  time: 0.9505 (0.5280 -- 2.7672)  data: 0.1754 (0.0006 -- 1.6845)  max mem: 16413
Epoch: [178]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.8321 (1.7040)  loss_scale: 8192.0000 (10864.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3312 (9.0836)  time: 0.9781 (0.5338 -- 2.9223)  data: 0.1959 (0.0002 -- 2.3878)  max mem: 16413
Epoch: [178]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7759 (1.7147)  loss_scale: 8192.0000 (10547.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5319 (9.0591)  time: 0.5796 (0.4950 -- 1.2394)  data: 0.0137 (0.0002 -- 0.1737)  max mem: 16413
Epoch: [178] Total time: 0:02:22 (0.8909 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7759 (1.6996)  loss_scale: 8192.0000 (10547.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5319 (9.0591)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.1733 (0.1733)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3099 (2.3099 -- 2.3099)  data: 2.0811 (2.0811 -- 2.0811)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3176 (0.6110)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4028 (0.1937 -- 2.3099)  data: 0.1902 (0.0007 -- 2.0811)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4234 (0.5482)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2181 (0.1688 -- 0.4970)  data: 0.0143 (0.0001 -- 0.2732)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4606 (0.6052)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2044 (0.1332 -- 0.4970)  data: 0.0141 (0.0001 -- 0.2732)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.593
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [179]  [  0/160]  eta: 0:16:39  lr: 0.000001  min_lr: 0.000000  loss: 1.1424 (1.1424)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4525 (7.4525)  time: 6.2472 (6.2472 -- 6.2472)  data: 5.4384 (5.4384 -- 5.4384)  max mem: 16413
[2023-08-31 18:32:40,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28650
[2023-08-31 18:32:40,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28650
[2023-08-31 18:32:40,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 18:32:40,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-31 18:32:40,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [179]  [ 20/160]  eta: 0:02:46  lr: 0.000001  min_lr: 0.000000  loss: 1.7713 (1.7189)  loss_scale: 4096.0000 (6046.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7434 (8.9573)  time: 0.9353 (0.5273 -- 2.6608)  data: 0.2220 (0.0004 -- 2.1333)  max mem: 16413
Epoch: [179]  [ 40/160]  eta: 0:02:05  lr: 0.000001  min_lr: 0.000000  loss: 1.5083 (1.6840)  loss_scale: 4096.0000 (5095.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8413 (8.6185)  time: 0.9010 (0.5226 -- 3.8544)  data: 0.3192 (0.0003 -- 3.3284)  max mem: 16413
Epoch: [179]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6979 (1.7012)  loss_scale: 4096.0000 (4767.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3630 (8.7365)  time: 0.7819 (0.5259 -- 1.6866)  data: 0.1472 (0.0006 -- 1.1102)  max mem: 16413
Epoch: [179]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.7397 (1.7027)  loss_scale: 4096.0000 (4601.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0426 (8.5988)  time: 0.9006 (0.5169 -- 3.3793)  data: 0.3017 (0.0003 -- 2.8712)  max mem: 16413
Epoch: [179]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.7618 (1.7099)  loss_scale: 4096.0000 (4501.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3866 (8.7819)  time: 1.0087 (0.5266 -- 4.1972)  data: 0.4657 (0.0004 -- 3.6837)  max mem: 16413
Epoch: [179]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8371 (1.7277)  loss_scale: 4096.0000 (4434.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2508 (8.9114)  time: 0.8506 (0.5288 -- 3.7534)  data: 0.3006 (0.0003 -- 3.2262)  max mem: 16413
[2023-08-31 18:34:37,534] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:34:37,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-31 18:34:37,535] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:34:37,535] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [179]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.3994 (1.6974)  loss_scale: 4096.0000 (4444.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2401 (8.8759)  time: 0.9604 (0.5232 -- 4.5092)  data: 0.4123 (0.0004 -- 3.9586)  max mem: 16413
Epoch: [179]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6066 (1.6853)  loss_scale: 8192.0000 (4889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1729 (8.9237)  time: 0.6090 (0.4926 -- 1.6982)  data: 0.0963 (0.0001 -- 1.1989)  max mem: 16413
Epoch: [179] Total time: 0:02:24 (0.9043 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6066 (1.6971)  loss_scale: 8192.0000 (4889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1729 (8.9237)
[2023-08-31 18:34:49,733] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-08-31 18:34:49,734] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-08-31 18:34:49,734] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt
[2023-08-31 18:34:49,734] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt...
[2023-08-31 18:34:50,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt.
[2023-08-31 18:34:50,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1742 (0.1742)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5414 (2.5414 -- 2.5414)  data: 2.3210 (2.3210 -- 2.3210)  max mem: 16413
Val:  [10/27]  eta: 0:00:08  loss: 0.3207 (0.6112)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4827 (0.2042 -- 2.5414)  data: 0.2649 (0.0004 -- 2.3210)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4214 (0.5482)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2313 (0.1687 -- 0.7930)  data: 0.0298 (0.0001 -- 0.5791)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4541 (0.6051)  acc1: 88.8889 (84.6473)  acc5: 100.0000 (97.9253)  time: 0.2276 (0.1325 -- 0.7930)  data: 0.0444 (0.0001 -- 0.5791)  max mem: 16413
Val: Total time: 0:00:08 (0.3129 s / it)
* Acc@1 85.062 Acc@5 98.133 loss 0.593
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [180]  [  0/160]  eta: 0:25:57  lr: 0.000001  min_lr: 0.000000  loss: 1.8070 (1.8070)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8764 (7.8764)  time: 9.7366 (9.7366 -- 9.7366)  data: 5.6809 (5.6809 -- 5.6809)  max mem: 16413
Epoch: [180]  [ 20/160]  eta: 0:02:42  lr: 0.000001  min_lr: 0.000000  loss: 1.8050 (1.7930)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4091 (8.8378)  time: 0.7296 (0.5236 -- 2.5069)  data: 0.0906 (0.0005 -- 1.0944)  max mem: 16413
Epoch: [180]  [ 40/160]  eta: 0:02:11  lr: 0.000001  min_lr: 0.000000  loss: 1.8140 (1.7392)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9005 (8.9444)  time: 1.0330 (0.5155 -- 4.5386)  data: 0.4886 (0.0004 -- 3.9760)  max mem: 16413
Epoch: [180]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.7832 (1.7251)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2789 (9.0177)  time: 0.7485 (0.5097 -- 3.7842)  data: 0.2051 (0.0004 -- 3.2672)  max mem: 16413
Epoch: [180]  [ 80/160]  eta: 0:01:19  lr: 0.000001  min_lr: 0.000000  loss: 1.6565 (1.7198)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6459 (9.0403)  time: 1.0160 (0.5142 -- 4.2312)  data: 0.4751 (0.0004 -- 3.7063)  max mem: 16413
Epoch: [180]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.5944 (1.6889)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8867 (9.1031)  time: 0.7339 (0.5145 -- 2.8733)  data: 0.1917 (0.0003 -- 2.3328)  max mem: 16413
[2023-08-31 18:36:41,489] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:36:41,489] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:36:41,490] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:36:41,490] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [180]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.4818 (1.6694)  loss_scale: 16384.0000 (9139.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0277 (9.0383)  time: 0.8511 (0.5292 -- 2.9190)  data: 0.0787 (0.0003 -- 1.5435)  max mem: 16413
Epoch: [180]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5397 (1.6589)  loss_scale: 16384.0000 (10167.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0750 (9.1006)  time: 0.7842 (0.5222 -- 2.5742)  data: 0.0129 (0.0005 -- 0.1792)  max mem: 16413
Epoch: [180]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6073 (1.6589)  loss_scale: 16384.0000 (10905.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1815 (8.9939)  time: 0.8011 (0.4943 -- 2.4281)  data: 0.2024 (0.0002 -- 1.8986)  max mem: 16413
Epoch: [180] Total time: 0:02:23 (0.8948 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6073 (1.6771)  loss_scale: 16384.0000 (10905.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1815 (8.9939)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1746 (0.1746)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5724 (2.5724 -- 2.5724)  data: 2.3515 (2.3515 -- 2.3515)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3381 (0.6148)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4530 (0.1945 -- 2.5724)  data: 0.2415 (0.0007 -- 2.3515)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4072 (0.5497)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2178 (0.1680 -- 0.5090)  data: 0.0166 (0.0001 -- 0.2938)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4569 (0.6073)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2037 (0.1324 -- 0.5090)  data: 0.0163 (0.0001 -- 0.2938)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [181]  [  0/160]  eta: 0:19:50  lr: 0.000001  min_lr: 0.000000  loss: 2.0296 (2.0296)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7985 (7.7985)  time: 7.4424 (7.4424 -- 7.4424)  data: 6.4529 (6.4529 -- 6.4529)  max mem: 16413
Epoch: [181]  [ 20/160]  eta: 0:02:53  lr: 0.000001  min_lr: 0.000000  loss: 1.6344 (1.7271)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0076 (8.5301)  time: 0.9254 (0.5260 -- 5.8438)  data: 0.3790 (0.0005 -- 5.3046)  max mem: 16413
[2023-08-31 18:38:11,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=171, lr=[2.6316989744306575e-08, 2.6316989744306575e-08, 3.508931965907543e-08, 3.508931965907543e-08, 4.678575954543391e-08, 4.678575954543391e-08, 6.238101272724522e-08, 6.238101272724522e-08, 8.317468363632695e-08, 8.317468363632695e-08, 1.1089957818176927e-07, 1.1089957818176927e-07, 1.47866104242359e-07, 1.47866104242359e-07, 1.971548056564787e-07, 1.971548056564787e-07, 2.6287307420863825e-07, 2.6287307420863825e-07, 3.5049743227818433e-07, 3.5049743227818433e-07, 4.6732990970424583e-07, 4.6732990970424583e-07, 6.231065462723277e-07, 6.231065462723277e-07, 8.308087283631036e-07, 8.308087283631036e-07, 1.1077449711508049e-06, 1.1077449711508049e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 18:38:11,092] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=16.863769605138007, CurrSamplesPerSec=21.92512594664175, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [181]  [ 40/160]  eta: 0:02:03  lr: 0.000001  min_lr: 0.000000  loss: 1.6160 (1.6952)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4626 (8.4885)  time: 0.8054 (0.5211 -- 2.7671)  data: 0.2555 (0.0002 -- 2.2213)  max mem: 16413
Epoch: [181]  [ 60/160]  eta: 0:01:40  lr: 0.000001  min_lr: 0.000000  loss: 1.8372 (1.7431)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1067 (8.4499)  time: 0.9519 (0.5203 -- 3.3384)  data: 0.4137 (0.0003 -- 2.8169)  max mem: 16413
[2023-08-31 18:38:43,445] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:38:43,445] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:38:43,449] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:38:43,449] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [181]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.7520 (1.7564)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3830 (8.4505)  time: 0.7460 (0.5311 -- 3.4858)  data: 0.1946 (0.0002 -- 2.9337)  max mem: 16413
[2023-08-31 18:38:50,939] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29044
[2023-08-31 18:38:50,939] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:38:50,939] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29044
[2023-08-31 18:38:50,940] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:38:50,940] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [181]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6862 (1.7446)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5054 (8.4990)  time: 0.9761 (0.5226 -- 3.7694)  data: 0.3983 (0.0004 -- 3.2061)  max mem: 16413
Epoch: [181]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7525 (1.7389)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7386 (8.5086)  time: 0.8070 (0.5295 -- 3.7914)  data: 0.2529 (0.0002 -- 3.2609)  max mem: 16413
Epoch: [181]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5666 (1.7178)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6713 (8.3954)  time: 0.9170 (0.5386 -- 4.2425)  data: 0.2788 (0.0006 -- 3.7152)  max mem: 16413
Epoch: [181]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.5776 (1.7066)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5109 (8.4579)  time: 0.6303 (0.4958 -- 2.7503)  data: 0.1124 (0.0002 -- 2.2343)  max mem: 16413
Epoch: [181] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.5776 (1.7037)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5109 (8.4579)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1741 (0.1741)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1978 (2.1978 -- 2.1978)  data: 2.0033 (2.0033 -- 2.0033)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3380 (0.6140)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4412 (0.1952 -- 2.1978)  data: 0.2301 (0.0008 -- 2.0033)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4078 (0.5499)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2383 (0.1692 -- 0.7550)  data: 0.0350 (0.0001 -- 0.5123)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4563 (0.6071)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2245 (0.1325 -- 0.7550)  data: 0.0347 (0.0001 -- 0.5123)  max mem: 16413
Val: Total time: 0:00:07 (0.2945 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [182]  [  0/160]  eta: 0:20:53  lr: 0.000001  min_lr: 0.000000  loss: 1.6059 (1.6059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.9983 (12.9983)  time: 7.8320 (7.8320 -- 7.8320)  data: 7.3116 (7.3116 -- 7.3116)  max mem: 16413
Epoch: [182]  [ 20/160]  eta: 0:02:38  lr: 0.000001  min_lr: 0.000000  loss: 1.8057 (1.7628)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5971 (9.9969)  time: 0.7938 (0.5239 -- 3.2253)  data: 0.2471 (0.0004 -- 2.7011)  max mem: 16413
Epoch: [182]  [ 40/160]  eta: 0:01:59  lr: 0.000001  min_lr: 0.000000  loss: 1.7418 (1.7475)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0553 (9.4651)  time: 0.8595 (0.5329 -- 2.0192)  data: 0.2598 (0.0007 -- 1.4818)  max mem: 16413
[2023-08-31 18:40:52,738] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:40:52,740] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:40:52,780] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:40:52,780] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:40:55,667] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29176
[2023-08-31 18:40:55,667] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29176
[2023-08-31 18:40:55,668] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:40:55,668] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:40:55,668] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [182]  [ 60/160]  eta: 0:01:35  lr: 0.000001  min_lr: 0.000000  loss: 1.5476 (1.6826)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1714 (9.5461)  time: 0.8769 (0.5155 -- 2.1413)  data: 0.0701 (0.0008 -- 0.8095)  max mem: 16413
Epoch: [182]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.7981 (1.6917)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4711 (9.4399)  time: 0.9855 (0.5218 -- 3.7602)  data: 0.0018 (0.0007 -- 0.0031)  max mem: 16413
Epoch: [182]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6804 (1.6745)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5426 (9.4492)  time: 0.8521 (0.5149 -- 3.6102)  data: 0.0219 (0.0004 -- 0.4122)  max mem: 16413
Epoch: [182]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7198 (1.6811)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8036 (9.2857)  time: 0.9313 (0.5170 -- 3.5974)  data: 0.0015 (0.0003 -- 0.0117)  max mem: 16413
[2023-08-31 18:42:00,390] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29248
[2023-08-31 18:42:00,390] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29248
[2023-08-31 18:42:00,390] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:42:00,390] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:42:00,390] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [182]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5932 (1.6666)  loss_scale: 8192.0000 (15977.3050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5708 (9.0389)  time: 0.8729 (0.5281 -- 4.3565)  data: 0.0013 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [182]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7436 (1.6907)  loss_scale: 8192.0000 (15052.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4252 (9.1171)  time: 0.6236 (0.4959 -- 1.7118)  data: 0.0010 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [182] Total time: 0:02:23 (0.8953 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7436 (1.6891)  loss_scale: 8192.0000 (15052.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4252 (9.1171)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1737 (0.1737)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3575 (2.3575 -- 2.3575)  data: 2.1446 (2.1446 -- 2.1446)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3381 (0.6153)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4146 (0.2093 -- 2.3575)  data: 0.1976 (0.0007 -- 2.1446)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4100 (0.5507)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.3545)  time: 0.2240 (0.1690 -- 0.5459)  data: 0.0195 (0.0001 -- 0.3574)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4612 (0.6076)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2055 (0.1324 -- 0.5459)  data: 0.0190 (0.0001 -- 0.3574)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [183]  [  0/160]  eta: 0:19:37  lr: 0.000001  min_lr: 0.000000  loss: 1.6680 (1.6680)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6449 (7.6449)  time: 7.3568 (7.3568 -- 7.3568)  data: 6.7943 (6.7943 -- 6.7943)  max mem: 16413
Epoch: [183]  [ 20/160]  eta: 0:02:50  lr: 0.000001  min_lr: 0.000000  loss: 1.7357 (1.7067)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2729 (8.8942)  time: 0.9096 (0.5215 -- 3.7018)  data: 0.3625 (0.0007 -- 3.1707)  max mem: 16413
Epoch: [183]  [ 40/160]  eta: 0:02:03  lr: 0.000001  min_lr: 0.000000  loss: 1.5734 (1.6790)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6158 (8.7464)  time: 0.8329 (0.5150 -- 3.3539)  data: 0.1866 (0.0003 -- 2.8334)  max mem: 16413
Epoch: [183]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.7627 (1.7087)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0150 (8.6712)  time: 0.9099 (0.5227 -- 3.7158)  data: 0.3338 (0.0002 -- 3.1872)  max mem: 16413
Epoch: [183]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.6616 (1.6950)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6562 (8.8315)  time: 0.8446 (0.5172 -- 4.1084)  data: 0.2792 (0.0002 -- 3.5725)  max mem: 16413
[2023-08-31 18:44:02,862] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:44:02,862] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:44:02,863] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:44:02,863] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [183]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7393 (1.7096)  loss_scale: 8192.0000 (8516.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6461 (8.8327)  time: 0.9152 (0.5241 -- 3.0209)  data: 0.3002 (0.0003 -- 2.5009)  max mem: 16413
Epoch: [183]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5882 (1.6865)  loss_scale: 16384.0000 (9816.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4475 (8.9981)  time: 0.7725 (0.5308 -- 2.5940)  data: 0.2009 (0.0004 -- 2.0360)  max mem: 16413
Epoch: [183]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5928 (1.6781)  loss_scale: 16384.0000 (10748.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4070 (8.9918)  time: 0.8924 (0.5328 -- 4.3210)  data: 0.3442 (0.0003 -- 3.7867)  max mem: 16413
Epoch: [183]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6548 (1.6814)  loss_scale: 16384.0000 (11417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9987 (9.0631)  time: 0.6812 (0.4950 -- 2.3713)  data: 0.0277 (0.0002 -- 0.5419)  max mem: 16413
Epoch: [183] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6548 (1.6817)  loss_scale: 16384.0000 (11417.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9987 (9.0631)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1740 (0.1740)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5210 (2.5210 -- 2.5210)  data: 2.2989 (2.2989 -- 2.2989)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3378 (0.6150)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4515 (0.2040 -- 2.5210)  data: 0.2302 (0.0007 -- 2.2989)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4100 (0.5514)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2194 (0.1706 -- 0.4554)  data: 0.0121 (0.0001 -- 0.2239)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4657 (0.6076)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2002 (0.1322 -- 0.4554)  data: 0.0118 (0.0001 -- 0.2239)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 85.062 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 85.06%
Max accuracy: 85.68%
Epoch: [184]  [  0/160]  eta: 0:19:54  lr: 0.000001  min_lr: 0.000000  loss: 1.5065 (1.5065)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4007 (10.4007)  time: 7.4674 (7.4674 -- 7.4674)  data: 6.9256 (6.9256 -- 6.9256)  max mem: 16413
Epoch: [184]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.5498 (1.5844)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4456 (9.3956)  time: 0.8648 (0.5197 -- 3.7359)  data: 0.0702 (0.0003 -- 0.7425)  max mem: 16413
Epoch: [184]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.7110 (1.6461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8614 (8.7137)  time: 0.8615 (0.5157 -- 4.1227)  data: 0.0023 (0.0004 -- 0.0086)  max mem: 16413
Epoch: [184]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.7676 (1.6788)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1866 (8.4068)  time: 0.9319 (0.5197 -- 4.1229)  data: 0.0017 (0.0003 -- 0.0047)  max mem: 16413
[2023-08-31 18:46:04,595] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:46:04,595] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:46:04,598] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:46:04,598] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [184]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6514 (1.6600)  loss_scale: 32768.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1699 (8.4221)  time: 0.7931 (0.5334 -- 2.0378)  data: 0.0017 (0.0002 -- 0.0046)  max mem: 16413
[2023-08-31 18:46:23,759] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29525
[2023-08-31 18:46:23,759] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29525
[2023-08-31 18:46:23,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:46:23,759] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:46:23,759] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [184]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.5918 (1.6490)  loss_scale: 16384.0000 (19628.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2015 (8.4375)  time: 1.0112 (0.5152 -- 4.3251)  data: 0.0016 (0.0004 -- 0.0030)  max mem: 16413
[2023-08-31 18:46:40,100] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29544
[2023-08-31 18:46:40,100] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29544
[2023-08-31 18:46:40,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:46:40,100] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:46:40,100] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [184]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.6494 (1.6365)  loss_scale: 8192.0000 (17941.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2830 (8.4194)  time: 0.8228 (0.5091 -- 4.4533)  data: 0.0021 (0.0003 -- 0.0169)  max mem: 16413
Epoch: [184]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.8368 (1.6641)  loss_scale: 8192.0000 (16558.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8183 (8.5124)  time: 0.9186 (0.5312 -- 3.9236)  data: 0.0016 (0.0008 -- 0.0048)  max mem: 16413
Epoch: [184]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4960 (1.6512)  loss_scale: 8192.0000 (15564.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4912 (8.4434)  time: 0.7108 (0.4957 -- 3.9236)  data: 0.0007 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [184] Total time: 0:02:21 (0.8867 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4960 (1.6807)  loss_scale: 8192.0000 (15564.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4912 (8.4434)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1736 (0.1736)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5499 (2.5499 -- 2.5499)  data: 2.3128 (2.3128 -- 2.3128)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3421 (0.6155)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4259 (0.2002 -- 2.5499)  data: 0.2117 (0.0007 -- 2.3128)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4058 (0.5517)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2126 (0.1692 -- 0.3855)  data: 0.0108 (0.0001 -- 0.1991)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4624 (0.6085)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.9253)  time: 0.1975 (0.1326 -- 0.3855)  data: 0.0106 (0.0001 -- 0.1991)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 84.647 Acc@5 98.133 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [185]  [  0/160]  eta: 0:21:36  lr: 0.000001  min_lr: 0.000000  loss: 2.1989 (2.1989)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6912 (12.6912)  time: 8.1003 (8.1003 -- 8.1003)  data: 7.5826 (7.5826 -- 7.5826)  max mem: 16413
Epoch: [185]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.4870 (1.5905)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9170 (8.5019)  time: 0.8381 (0.5218 -- 3.0743)  data: 0.2232 (0.0004 -- 2.5261)  max mem: 16413
Epoch: [185]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.7070 (1.6056)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4326 (8.2104)  time: 0.8525 (0.5302 -- 1.9942)  data: 0.2114 (0.0004 -- 1.4453)  max mem: 16413
Epoch: [185]  [ 60/160]  eta: 0:01:35  lr: 0.000001  min_lr: 0.000000  loss: 1.6314 (1.5928)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7690 (8.4275)  time: 0.8051 (0.5265 -- 3.2313)  data: 0.2312 (0.0003 -- 2.7083)  max mem: 16413
[2023-08-31 18:48:39,843] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:48:39,844] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:48:39,849] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:48:39,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [185]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.7687 (1.6444)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3080 (8.5162)  time: 0.9011 (0.5314 -- 3.0756)  data: 0.2301 (0.0003 -- 2.5352)  max mem: 16413
Epoch: [185]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6908 (1.6464)  loss_scale: 16384.0000 (10463.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6841 (8.4473)  time: 0.9275 (0.5357 -- 4.5710)  data: 0.3682 (0.0004 -- 4.0653)  max mem: 16413
Epoch: [185]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7705 (1.6543)  loss_scale: 16384.0000 (11441.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6848 (8.6160)  time: 0.8630 (0.5213 -- 2.4625)  data: 0.3197 (0.0002 -- 1.9046)  max mem: 16413
Epoch: [185]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7845 (1.6594)  loss_scale: 16384.0000 (12142.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5875 (8.7348)  time: 0.8668 (0.5235 -- 2.4763)  data: 0.3135 (0.0007 -- 1.9114)  max mem: 16413
Epoch: [185]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.6481 (1.6543)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4639 (8.8425)  time: 0.6581 (0.4945 -- 1.8256)  data: 0.1273 (0.0003 -- 1.3126)  max mem: 16413
Epoch: [185] Total time: 0:02:21 (0.8863 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.6481 (1.6796)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4639 (8.8425)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1727 (0.1727)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4182 (2.4182 -- 2.4182)  data: 2.1883 (2.1883 -- 2.1883)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3482 (0.6145)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4199 (0.1994 -- 2.4182)  data: 0.2043 (0.0006 -- 2.1883)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3994 (0.5505)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2237 (0.1691 -- 0.5627)  data: 0.0218 (0.0001 -- 0.3747)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4599 (0.6075)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.2091 (0.1324 -- 0.5627)  data: 0.0216 (0.0001 -- 0.3747)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [186]  [  0/160]  eta: 0:19:18  lr: 0.000001  min_lr: 0.000000  loss: 1.3239 (1.3239)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3647 (8.3647)  time: 7.2375 (7.2375 -- 7.2375)  data: 6.5285 (6.5285 -- 6.5285)  max mem: 16413
Epoch: [186]  [ 20/160]  eta: 0:03:04  lr: 0.000001  min_lr: 0.000000  loss: 1.6478 (1.6097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1947 (9.0149)  time: 1.0222 (0.5265 -- 4.0822)  data: 0.0343 (0.0003 -- 0.6613)  max mem: 16413
[2023-08-31 18:50:32,695] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29785
[2023-08-31 18:50:32,695] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:50:32,695] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-31 18:50:32,695] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29785
[2023-08-31 18:50:32,695] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [186]  [ 40/160]  eta: 0:02:03  lr: 0.000001  min_lr: 0.000000  loss: 1.8201 (1.7723)  loss_scale: 8192.0000 (13187.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9291 (8.6918)  time: 0.7183 (0.5168 -- 2.2573)  data: 0.0014 (0.0002 -- 0.0061)  max mem: 16413
Epoch: [186]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.8149 (1.7683)  loss_scale: 8192.0000 (11549.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6862 (8.7639)  time: 0.9348 (0.5260 -- 3.3681)  data: 0.0324 (0.0003 -- 0.6048)  max mem: 16413
Epoch: [186]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.7902 (1.7574)  loss_scale: 8192.0000 (10720.3951)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1608 (8.6276)  time: 0.8976 (0.5335 -- 3.1875)  data: 0.2410 (0.0006 -- 2.6471)  max mem: 16413
Epoch: [186]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.7229 (1.7391)  loss_scale: 8192.0000 (10219.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6841 (8.7509)  time: 0.8498 (0.5161 -- 2.1649)  data: 0.1511 (0.0003 -- 1.5763)  max mem: 16413
Epoch: [186]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6127 (1.7205)  loss_scale: 8192.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1335 (8.7093)  time: 0.7988 (0.5325 -- 2.2369)  data: 0.0455 (0.0003 -- 0.6026)  max mem: 16413
Epoch: [186]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6324 (1.7054)  loss_scale: 8192.0000 (9644.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3227 (8.8804)  time: 0.9209 (0.5268 -- 4.4913)  data: 0.0319 (0.0002 -- 0.6129)  max mem: 16413
[2023-08-31 18:52:21,383] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:52:21,383] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:52:21,383] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:52:21,383] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [186]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7416 (1.7071)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9412 (8.8462)  time: 0.6864 (0.4948 -- 3.5654)  data: 0.0007 (0.0001 -- 0.0024)  max mem: 16413
Epoch: [186] Total time: 0:02:23 (0.8958 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7416 (1.7163)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9412 (8.8462)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.1726 (0.1726)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3375 (2.3375 -- 2.3375)  data: 2.1089 (2.1089 -- 2.1089)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3486 (0.6151)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4309 (0.2019 -- 2.3375)  data: 0.2126 (0.0008 -- 2.1089)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3998 (0.5508)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2290 (0.1691 -- 0.4962)  data: 0.0273 (0.0001 -- 0.3145)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4592 (0.6081)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.9253)  time: 0.2131 (0.1337 -- 0.4962)  data: 0.0270 (0.0001 -- 0.3145)  max mem: 16413
Val: Total time: 0:00:07 (0.2927 s / it)
* Acc@1 84.440 Acc@5 98.133 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [187]  [  0/160]  eta: 0:22:01  lr: 0.000001  min_lr: 0.000000  loss: 2.2928 (2.2928)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1578 (9.1578)  time: 8.2577 (8.2577 -- 8.2577)  data: 7.2432 (7.2432 -- 7.2432)  max mem: 16413
Epoch: [187]  [ 20/160]  eta: 0:02:49  lr: 0.000001  min_lr: 0.000000  loss: 1.5252 (1.6843)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2036 (8.6715)  time: 0.8576 (0.5158 -- 3.9665)  data: 0.3091 (0.0004 -- 3.4286)  max mem: 16413
Epoch: [187]  [ 40/160]  eta: 0:02:09  lr: 0.000001  min_lr: 0.000000  loss: 1.9488 (1.7535)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6747 (8.5054)  time: 0.9488 (0.5258 -- 3.7889)  data: 0.4002 (0.0004 -- 3.2731)  max mem: 16413
Epoch: [187]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5574 (1.7232)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5164 (8.5450)  time: 0.7205 (0.5230 -- 1.7341)  data: 0.1493 (0.0002 -- 1.1948)  max mem: 16413
[2023-08-31 18:53:47,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=177, lr=[1.2366359971096707e-08, 1.2366359971096707e-08, 1.6488479961462277e-08, 1.6488479961462277e-08, 2.1984639948616367e-08, 2.1984639948616367e-08, 2.9312853264821822e-08, 2.9312853264821822e-08, 3.908380435309577e-08, 3.908380435309577e-08, 5.2111739137461016e-08, 5.2111739137461016e-08, 6.948231884994803e-08, 6.948231884994803e-08, 9.26430917999307e-08, 9.26430917999307e-08, 1.235241223999076e-07, 1.235241223999076e-07, 1.6469882986654348e-07, 1.6469882986654348e-07, 2.1959843982205795e-07, 2.1959843982205795e-07, 2.9279791976274395e-07, 2.9279791976274395e-07, 3.9039722635032525e-07, 3.9039722635032525e-07, 5.20529635133767e-07, 5.20529635133767e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 18:53:47,255] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=16.891216171581704, CurrSamplesPerSec=20.7543259786336, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [187]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6975 (1.7435)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4585 (8.9117)  time: 0.8620 (0.5400 -- 2.4777)  data: 0.0124 (0.0004 -- 0.2239)  max mem: 16413
Epoch: [187]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.5507 (1.7309)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0830 (8.9506)  time: 0.8733 (0.5227 -- 2.1557)  data: 0.0013 (0.0003 -- 0.0026)  max mem: 16413
[2023-08-31 18:54:21,385] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30038
[2023-08-31 18:54:21,385] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30038
[2023-08-31 18:54:21,385] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:54:21,385] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:54:21,385] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [187]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.6474 (1.7173)  loss_scale: 16384.0000 (16180.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8627 (8.8379)  time: 0.9477 (0.5356 -- 2.4081)  data: 0.0422 (0.0002 -- 0.8154)  max mem: 16413
Epoch: [187]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8506 (1.7223)  loss_scale: 8192.0000 (15047.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6723 (8.9338)  time: 0.9044 (0.5230 -- 5.3825)  data: 0.0012 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [187]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5634 (1.7081)  loss_scale: 8192.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0493 (8.8965)  time: 0.7068 (0.4930 -- 3.3365)  data: 0.0006 (0.0001 -- 0.0021)  max mem: 16413
Epoch: [187] Total time: 0:02:24 (0.9010 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5634 (1.6950)  loss_scale: 8192.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0493 (8.8965)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.1721 (0.1721)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5811 (2.5811 -- 2.5811)  data: 2.3467 (2.3467 -- 2.3467)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3475 (0.6148)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4493 (0.2108 -- 2.5811)  data: 0.2304 (0.0006 -- 2.3467)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3986 (0.5509)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2160 (0.1692 -- 0.3813)  data: 0.0106 (0.0001 -- 0.1713)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4563 (0.6082)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.1991 (0.1322 -- 0.3813)  data: 0.0103 (0.0001 -- 0.1713)  max mem: 16413
Val: Total time: 0:00:07 (0.2920 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [188]  [  0/160]  eta: 0:20:48  lr: 0.000000  min_lr: 0.000000  loss: 1.2940 (1.2940)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1971 (6.1971)  time: 7.8028 (7.8028 -- 7.8028)  data: 7.2420 (7.2420 -- 7.2420)  max mem: 16413
Epoch: [188]  [ 20/160]  eta: 0:02:35  lr: 0.000000  min_lr: 0.000000  loss: 1.8448 (1.7208)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4152 (9.2899)  time: 0.7749 (0.5346 -- 3.5242)  data: 0.2192 (0.0005 -- 3.0074)  max mem: 16413
Epoch: [188]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.8953 (1.7518)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2680 (8.8529)  time: 0.9698 (0.5333 -- 3.9096)  data: 0.3359 (0.0004 -- 3.3574)  max mem: 16413
Epoch: [188]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.8842 (1.7824)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2182 (8.8493)  time: 0.9071 (0.5244 -- 3.4996)  data: 0.2145 (0.0002 -- 2.9499)  max mem: 16413
Epoch: [188]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.5834 (1.7477)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7896 (8.7722)  time: 0.8722 (0.5158 -- 3.7734)  data: 0.0842 (0.0003 -- 1.6660)  max mem: 16413
[2023-08-31 18:56:28,830] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:56:28,830] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 18:56:28,831] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:56:28,831] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [188]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.8064 (1.7438)  loss_scale: 16384.0000 (9327.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7605 (8.7570)  time: 0.9031 (0.5148 -- 3.4659)  data: 0.0019 (0.0002 -- 0.0137)  max mem: 16413
Epoch: [188]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6531 (1.7266)  loss_scale: 16384.0000 (10493.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8889 (8.7391)  time: 0.7612 (0.5299 -- 3.4322)  data: 0.0018 (0.0004 -- 0.0065)  max mem: 16413
Epoch: [188]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7226 (1.7351)  loss_scale: 16384.0000 (11329.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1563 (8.8703)  time: 0.8155 (0.5349 -- 2.6402)  data: 0.0670 (0.0007 -- 1.1561)  max mem: 16413
Epoch: [188]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.4449 (1.7084)  loss_scale: 16384.0000 (11929.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1546 (8.8421)  time: 0.6761 (0.4974 -- 2.3161)  data: 0.1007 (0.0002 -- 1.7920)  max mem: 16413
Epoch: [188] Total time: 0:02:20 (0.8804 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.4449 (1.7086)  loss_scale: 16384.0000 (11929.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1546 (8.8421)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.1722 (0.1722)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4229 (2.4229 -- 2.4229)  data: 2.2073 (2.2073 -- 2.2073)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3480 (0.6162)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4256 (0.2014 -- 2.4229)  data: 0.2124 (0.0007 -- 2.2073)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3989 (0.5518)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2178 (0.1717 -- 0.3456)  data: 0.0104 (0.0001 -- 0.1101)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4542 (0.6093)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2024 (0.1330 -- 0.3456)  data: 0.0101 (0.0001 -- 0.1101)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [189]  [  0/160]  eta: 0:21:32  lr: 0.000000  min_lr: 0.000000  loss: 0.9904 (0.9904)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1726 (12.1726)  time: 8.0763 (8.0763 -- 8.0763)  data: 7.5582 (7.5582 -- 7.5582)  max mem: 16413
Epoch: [189]  [ 20/160]  eta: 0:02:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7572 (1.7882)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5707 (10.1071)  time: 0.7724 (0.5117 -- 2.3672)  data: 0.1623 (0.0001 -- 1.8459)  max mem: 16413
Epoch: [189]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.5993 (1.7085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1799 (8.9609)  time: 1.0027 (0.5201 -- 5.5420)  data: 0.4523 (0.0002 -- 5.0374)  max mem: 16413
[2023-08-31 18:58:26,848] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:58:26,848] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 18:58:26,849] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 18:58:26,850] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [189]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.6885 (1.7099)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0022 (8.9921)  time: 0.8604 (0.5248 -- 3.8926)  data: 0.0990 (0.0002 -- 1.6138)  max mem: 16413
[2023-08-31 18:58:33,846] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30301
[2023-08-31 18:58:33,846] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30301
[2023-08-31 18:58:33,847] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:58:33,847] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 18:58:33,847] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [189]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.6432 (1.7149)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0136 (8.8897)  time: 0.7217 (0.5301 -- 2.6947)  data: 0.0017 (0.0005 -- 0.0058)  max mem: 16413
Epoch: [189]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8463 (1.7277)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9188 (8.7537)  time: 0.9577 (0.5249 -- 3.4328)  data: 0.0557 (0.0002 -- 1.0716)  max mem: 16413
Epoch: [189]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6492 (1.7179)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7185 (8.7898)  time: 0.8615 (0.5185 -- 3.0616)  data: 0.0964 (0.0005 -- 1.7219)  max mem: 16413
[2023-08-31 18:59:36,859] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30372
[2023-08-31 18:59:36,859] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30372
[2023-08-31 18:59:36,859] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:59:36,859] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 18:59:36,860] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [189]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6851 (1.7066)  loss_scale: 16384.0000 (16558.2979)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7548 (8.8671)  time: 0.9028 (0.5211 -- 4.5776)  data: 0.0012 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [189]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6005 (1.6899)  loss_scale: 8192.0000 (15564.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7445 (8.9005)  time: 0.6124 (0.4965 -- 2.0543)  data: 0.0023 (0.0002 -- 0.0305)  max mem: 16413
Epoch: [189] Total time: 0:02:21 (0.8837 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6005 (1.6971)  loss_scale: 8192.0000 (15564.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7445 (8.9005)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1721 (0.1721)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4677 (2.4677 -- 2.4677)  data: 2.2252 (2.2252 -- 2.2252)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3466 (0.6163)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4482 (0.2029 -- 2.4677)  data: 0.2259 (0.0006 -- 2.2252)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.3999 (0.5520)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2251 (0.1700 -- 0.5129)  data: 0.0202 (0.0001 -- 0.2505)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4549 (0.6095)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2102 (0.1335 -- 0.5129)  data: 0.0199 (0.0001 -- 0.2505)  max mem: 16413
Val: Total time: 0:00:07 (0.2947 s / it)
* Acc@1 84.855 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.85%
Max accuracy: 85.68%
Epoch: [190]  [  0/160]  eta: 0:20:07  lr: 0.000000  min_lr: 0.000000  loss: 0.9278 (0.9278)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5181 (7.5181)  time: 7.5469 (7.5469 -- 7.5469)  data: 6.9957 (6.9957 -- 6.9957)  max mem: 16413
Epoch: [190]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.4348 (1.5061)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2496 (9.8655)  time: 0.8353 (0.5357 -- 2.3782)  data: 0.2856 (0.0005 -- 1.8192)  max mem: 16413
Epoch: [190]  [ 40/160]  eta: 0:02:09  lr: 0.000000  min_lr: 0.000000  loss: 1.8153 (1.6422)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0026 (9.7190)  time: 1.0028 (0.5302 -- 3.7088)  data: 0.1760 (0.0002 -- 2.3939)  max mem: 16413
Epoch: [190]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6821 (1.6400)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3819 (9.3863)  time: 0.7168 (0.5275 -- 2.8225)  data: 0.0087 (0.0005 -- 0.1462)  max mem: 16413
Epoch: [190]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.7540 (1.6680)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9045 (9.4203)  time: 0.9433 (0.5323 -- 2.9000)  data: 0.2540 (0.0003 -- 1.8211)  max mem: 16413
Epoch: [190]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7783 (1.6818)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2529 (9.2352)  time: 0.8404 (0.5244 -- 3.0736)  data: 0.2066 (0.0004 -- 2.5559)  max mem: 16413
[2023-08-31 19:01:36,776] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:01:36,777] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 19:01:36,778] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:01:36,779] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [190]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6663 (1.6810)  loss_scale: 16384.0000 (9546.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9733 (9.1233)  time: 0.9473 (0.5250 -- 3.6784)  data: 0.4000 (0.0003 -- 3.1569)  max mem: 16413
Epoch: [190]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6514 (1.6727)  loss_scale: 16384.0000 (10515.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1773 (9.0671)  time: 0.7737 (0.5163 -- 3.3104)  data: 0.1883 (0.0004 -- 2.7687)  max mem: 16413
Epoch: [190]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7709 (1.6863)  loss_scale: 16384.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2899 (9.0445)  time: 0.7611 (0.4953 -- 3.3689)  data: 0.2380 (0.0002 -- 2.8402)  max mem: 16413
Epoch: [190] Total time: 0:02:23 (0.8966 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7709 (1.6825)  loss_scale: 16384.0000 (11212.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2899 (9.0445)
Val:  [ 0/27]  eta: 0:01:13  loss: 0.1717 (0.1717)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7114 (2.7114 -- 2.7114)  data: 2.4825 (2.4825 -- 2.4825)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3426 (0.6155)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4666 (0.2063 -- 2.7114)  data: 0.2451 (0.0007 -- 2.4825)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4044 (0.5520)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2175 (0.1690 -- 0.4247)  data: 0.0108 (0.0001 -- 0.2049)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4541 (0.6094)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.2005 (0.1321 -- 0.4247)  data: 0.0106 (0.0001 -- 0.2049)  max mem: 16413
Val: Total time: 0:00:08 (0.2982 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [191]  [  0/160]  eta: 0:21:02  lr: 0.000000  min_lr: 0.000000  loss: 1.2888 (1.2888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4674 (10.4674)  time: 7.8891 (7.8891 -- 7.8891)  data: 7.3451 (7.3451 -- 7.3451)  max mem: 16413
[2023-08-31 19:02:56,867] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30579
[2023-08-31 19:02:56,867] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30579
[2023-08-31 19:02:56,868] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 19:02:56,868] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 19:02:56,868] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [191]  [ 20/160]  eta: 0:02:40  lr: 0.000000  min_lr: 0.000000  loss: 1.6611 (1.6689)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6515 (9.3373)  time: 0.8075 (0.5200 -- 2.8927)  data: 0.2478 (0.0003 -- 2.3551)  max mem: 16413
Epoch: [191]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 1.9265 (1.7320)  loss_scale: 8192.0000 (11988.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2850 (8.9942)  time: 1.0014 (0.5258 -- 3.7140)  data: 0.4593 (0.0004 -- 3.1982)  max mem: 16413
Epoch: [191]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.6684 (1.7059)  loss_scale: 8192.0000 (10743.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9165 (8.7626)  time: 0.8257 (0.5175 -- 2.8594)  data: 0.2474 (0.0004 -- 2.3332)  max mem: 16413
Epoch: [191]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.6227 (1.6838)  loss_scale: 8192.0000 (10113.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9813 (8.6794)  time: 0.8092 (0.5333 -- 2.6048)  data: 0.0791 (0.0002 -- 1.5609)  max mem: 16413
Epoch: [191]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7572 (1.7078)  loss_scale: 8192.0000 (9733.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9317 (8.5606)  time: 0.9087 (0.5356 -- 3.2489)  data: 0.3470 (0.0009 -- 2.7004)  max mem: 16413
Epoch: [191]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6028 (1.6992)  loss_scale: 8192.0000 (9478.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0507 (8.7695)  time: 0.8859 (0.5176 -- 4.0136)  data: 0.3396 (0.0002 -- 3.4902)  max mem: 16413
Epoch: [191]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5943 (1.6773)  loss_scale: 8192.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8423 (8.7133)  time: 0.9080 (0.5230 -- 3.5869)  data: 0.3555 (0.0004 -- 3.0669)  max mem: 16413
[2023-08-31 19:04:50,599] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:04:50,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 19:04:50,599] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:04:50,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [191]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7104 (1.6770)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8695 (8.6476)  time: 0.6549 (0.4946 -- 2.5713)  data: 0.1318 (0.0002 -- 2.0643)  max mem: 16413
Epoch: [191] Total time: 0:02:23 (0.8957 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7104 (1.6714)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8695 (8.6476)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1718 (0.1718)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5354 (2.5354 -- 2.5354)  data: 2.3334 (2.3334 -- 2.3334)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3429 (0.6152)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4280 (0.1948 -- 2.5354)  data: 0.2145 (0.0004 -- 2.3334)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4037 (0.5518)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2155 (0.1693 -- 0.3924)  data: 0.0116 (0.0001 -- 0.2019)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4542 (0.6093)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.1996 (0.1332 -- 0.3924)  data: 0.0113 (0.0001 -- 0.2019)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [192]  [  0/160]  eta: 0:20:50  lr: 0.000000  min_lr: 0.000000  loss: 1.0234 (1.0234)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9227 (5.9227)  time: 7.8127 (7.8127 -- 7.8127)  data: 7.2975 (7.2975 -- 7.2975)  max mem: 16413
Epoch: [192]  [ 20/160]  eta: 0:02:50  lr: 0.000000  min_lr: 0.000000  loss: 1.6350 (1.6448)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4074 (8.2060)  time: 0.8887 (0.5333 -- 3.9318)  data: 0.3060 (0.0006 -- 3.3998)  max mem: 16413
Epoch: [192]  [ 40/160]  eta: 0:02:12  lr: 0.000000  min_lr: 0.000000  loss: 1.7737 (1.6610)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4061 (8.7085)  time: 0.9796 (0.5229 -- 3.6798)  data: 0.4419 (0.0003 -- 3.1538)  max mem: 16413
Epoch: [192]  [ 60/160]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 1.8377 (1.6854)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1476 (8.6132)  time: 0.6588 (0.5309 -- 1.4527)  data: 0.0568 (0.0002 -- 0.9265)  max mem: 16413
Epoch: [192]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.8643 (1.7174)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7174 (8.5252)  time: 1.0044 (0.5185 -- 3.7449)  data: 0.0025 (0.0005 -- 0.0183)  max mem: 16413
Epoch: [192]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.7701 (1.7376)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6514 (8.6435)  time: 0.7642 (0.5244 -- 2.1320)  data: 0.0644 (0.0003 -- 1.2575)  max mem: 16413
[2023-08-31 19:06:51,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:06:51,297] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:06:51,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 19:06:51,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [192]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6490 (1.7308)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0353 (8.6559)  time: 0.8367 (0.5230 -- 2.3663)  data: 0.1290 (0.0003 -- 1.2964)  max mem: 16413
[2023-08-31 19:07:01,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30847
[2023-08-31 19:07:01,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30847
[2023-08-31 19:07:01,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:07:01,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:07:01,524] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [192]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8292 (1.7407)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9059 (8.5928)  time: 0.8470 (0.5362 -- 2.3064)  data: 0.1511 (0.0003 -- 1.0449)  max mem: 16413
Epoch: [192]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6617 (1.7373)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0196 (8.5954)  time: 0.7685 (0.4960 -- 2.5539)  data: 0.0696 (0.0001 -- 1.3748)  max mem: 16413
Epoch: [192] Total time: 0:02:22 (0.8890 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6617 (1.7282)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0196 (8.5954)
Val:  [ 0/27]  eta: 0:01:13  loss: 0.1720 (0.1720)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.7087 (2.7087 -- 2.7087)  data: 2.4270 (2.4270 -- 2.4270)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3441 (0.6156)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4470 (0.2021 -- 2.7087)  data: 0.2217 (0.0005 -- 2.4270)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4034 (0.5521)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2125 (0.1683 -- 0.3184)  data: 0.0078 (0.0001 -- 0.1410)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4550 (0.6095)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.1960 (0.1320 -- 0.3184)  data: 0.0075 (0.0001 -- 0.1410)  max mem: 16413
Val: Total time: 0:00:07 (0.2940 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [193]  [  0/160]  eta: 0:21:30  lr: 0.000000  min_lr: 0.000000  loss: 1.7785 (1.7785)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4018 (6.4018)  time: 8.0644 (8.0644 -- 8.0644)  data: 5.1642 (5.1642 -- 5.1642)  max mem: 16413
Epoch: [193]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.7399 (1.6958)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5217 (8.7850)  time: 0.8078 (0.5312 -- 2.3247)  data: 0.0821 (0.0003 -- 1.3138)  max mem: 16413
Epoch: [193]  [ 40/160]  eta: 0:02:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8348 (1.7161)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3251 (8.9013)  time: 0.8486 (0.5290 -- 2.7938)  data: 0.2474 (0.0004 -- 2.2418)  max mem: 16413
Epoch: [193]  [ 60/160]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.7130 (1.7074)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4136 (8.7344)  time: 0.9906 (0.5090 -- 2.9693)  data: 0.3383 (0.0004 -- 2.4541)  max mem: 16413
Epoch: [193]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.6277 (1.6848)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2606 (8.5244)  time: 0.8392 (0.5315 -- 4.7907)  data: 0.2930 (0.0002 -- 4.2681)  max mem: 16413
[2023-08-31 19:09:08,274] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:09:08,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 19:09:08,274] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:09:08,274] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [193]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.6856 (1.6878)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3307 (8.6551)  time: 0.9527 (0.5154 -- 4.9951)  data: 0.4134 (0.0004 -- 4.4867)  max mem: 16413
[2023-08-31 19:09:25,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30997
[2023-08-31 19:09:25,831] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30997
[2023-08-31 19:09:25,832] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:09:25,832] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:09:25,832] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 19:09:26,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=183, lr=[3.936773415739102e-09, 3.936773415739102e-09, 5.249031220985469e-09, 5.249031220985469e-09, 6.998708294647292e-09, 6.998708294647292e-09, 9.331611059529723e-09, 9.331611059529723e-09, 1.2442148079372963e-08, 1.2442148079372963e-08, 1.6589530772497285e-08, 1.6589530772497285e-08, 2.211937436332971e-08, 2.211937436332971e-08, 2.949249915110628e-08, 2.949249915110628e-08, 3.9323332201475046e-08, 3.9323332201475046e-08, 5.243110960196673e-08, 5.243110960196673e-08, 6.990814613595564e-08, 6.990814613595564e-08, 9.321086151460751e-08, 9.321086151460751e-08, 1.2428114868614336e-07, 1.2428114868614336e-07, 1.6570819824819113e-07, 1.6570819824819113e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 19:09:26,915] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=16.92786883066909, CurrSamplesPerSec=22.756543252745345, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [193]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6881 (1.6883)  loss_scale: 32768.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1066 (8.5196)  time: 0.7897 (0.5228 -- 4.3637)  data: 0.2413 (0.0003 -- 3.8167)  max mem: 16413
Epoch: [193]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7133 (1.6890)  loss_scale: 16384.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9160 (8.4956)  time: 0.8726 (0.5297 -- 3.2569)  data: 0.3283 (0.0004 -- 2.7305)  max mem: 16413
Epoch: [193]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7215 (1.7018)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0295 (8.5867)  time: 0.6240 (0.4969 -- 2.5581)  data: 0.1031 (0.0002 -- 2.0472)  max mem: 16413
Epoch: [193] Total time: 0:02:22 (0.8880 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7215 (1.6963)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0295 (8.5867)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.1721 (0.1721)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2135 (2.2135 -- 2.2135)  data: 1.9951 (1.9951 -- 1.9951)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3446 (0.6159)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4138 (0.1939 -- 2.2135)  data: 0.2046 (0.0003 -- 1.9951)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4034 (0.5520)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2258 (0.1694 -- 0.4453)  data: 0.0224 (0.0001 -- 0.2462)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4547 (0.6094)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2131 (0.1333 -- 0.4453)  data: 0.0221 (0.0001 -- 0.2462)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [194]  [  0/160]  eta: 0:23:39  lr: 0.000000  min_lr: 0.000000  loss: 2.0025 (2.0025)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0852 (11.0852)  time: 8.8689 (8.8689 -- 8.8689)  data: 5.8530 (5.8530 -- 5.8530)  max mem: 16413
Epoch: [194]  [ 20/160]  eta: 0:02:41  lr: 0.000000  min_lr: 0.000000  loss: 1.7351 (1.7477)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4837 (8.9582)  time: 0.7659 (0.5217 -- 3.3819)  data: 0.0015 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [194]  [ 40/160]  eta: 0:02:10  lr: 0.000000  min_lr: 0.000000  loss: 1.7374 (1.7179)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4374 (8.8248)  time: 1.0159 (0.5150 -- 3.3638)  data: 0.0015 (0.0003 -- 0.0069)  max mem: 16413
Epoch: [194]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7826 (1.7206)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4433 (8.9683)  time: 0.7627 (0.5205 -- 3.1785)  data: 0.0015 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [194]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7570 (1.7296)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8903 (8.8829)  time: 0.7806 (0.5281 -- 2.4462)  data: 0.0311 (0.0007 -- 0.5891)  max mem: 16413
[2023-08-31 19:11:25,538] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:11:25,538] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:11:25,538] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 19:11:25,538] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [194]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.7635 (1.7463)  loss_scale: 32768.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5227 (8.8858)  time: 0.8829 (0.5283 -- 2.5535)  data: 0.0246 (0.0004 -- 0.4581)  max mem: 16413
[2023-08-31 19:11:53,306] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31158
[2023-08-31 19:11:53,307] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31158
[2023-08-31 19:11:53,307] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:11:53,307] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:11:53,307] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [194]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7138 (1.7512)  loss_scale: 32768.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7395 (8.7981)  time: 0.8780 (0.5272 -- 2.1409)  data: 0.1230 (0.0004 -- 1.3314)  max mem: 16413
Epoch: [194]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6031 (1.7340)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5114 (8.8909)  time: 0.9059 (0.5328 -- 2.7788)  data: 0.3112 (0.0004 -- 2.2163)  max mem: 16413
Epoch: [194]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6079 (1.7268)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7882 (8.9232)  time: 0.7184 (0.4957 -- 2.1481)  data: 0.1165 (0.0001 -- 1.6392)  max mem: 16413
Epoch: [194] Total time: 0:02:22 (0.8910 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6079 (1.7293)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7882 (8.9232)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1720 (0.1720)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3853 (2.3853 -- 2.3853)  data: 2.1731 (2.1731 -- 2.1731)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3436 (0.6157)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4314 (0.1941 -- 2.3853)  data: 0.2160 (0.0008 -- 2.1731)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4033 (0.5518)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2275 (0.1695 -- 0.4235)  data: 0.0217 (0.0001 -- 0.2277)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4546 (0.6093)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2119 (0.1330 -- 0.4235)  data: 0.0214 (0.0001 -- 0.2277)  max mem: 16413
Val: Total time: 0:00:07 (0.2933 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [195]  [  0/160]  eta: 0:25:20  lr: 0.000000  min_lr: 0.000000  loss: 1.2518 (1.2518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8566 (8.8566)  time: 9.5037 (9.5037 -- 9.5037)  data: 6.3772 (6.3772 -- 6.3772)  max mem: 16413
[2023-08-31 19:12:53,604] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31210
[2023-08-31 19:12:53,605] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31210
[2023-08-31 19:12:53,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 19:12:53,605] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 19:12:53,605] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [195]  [ 20/160]  eta: 0:02:45  lr: 0.000000  min_lr: 0.000000  loss: 1.5409 (1.5606)  loss_scale: 8192.0000 (12092.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9622 (8.7539)  time: 0.7639 (0.5163 -- 3.8000)  data: 0.0487 (0.0003 -- 0.8046)  max mem: 16413
Epoch: [195]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.7770 (1.6843)  loss_scale: 8192.0000 (10190.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8966 (8.8510)  time: 0.9439 (0.5245 -- 4.5091)  data: 0.4007 (0.0001 -- 3.9623)  max mem: 16413
Epoch: [195]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.7607 (1.7017)  loss_scale: 8192.0000 (9534.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2954 (8.7693)  time: 0.8483 (0.5193 -- 3.4894)  data: 0.1065 (0.0003 -- 1.5251)  max mem: 16413
Epoch: [195]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7618 (1.7189)  loss_scale: 8192.0000 (9203.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0060 (8.6850)  time: 0.8072 (0.5231 -- 2.5832)  data: 0.0013 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [195]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.6041 (1.7090)  loss_scale: 8192.0000 (9003.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8326 (9.0079)  time: 0.8124 (0.5268 -- 2.5354)  data: 0.0046 (0.0005 -- 0.0621)  max mem: 16413
Epoch: [195]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.6723 (1.7097)  loss_scale: 8192.0000 (8869.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8975 (8.9620)  time: 0.9968 (0.5208 -- 3.0854)  data: 0.0013 (0.0004 -- 0.0040)  max mem: 16413
[2023-08-31 19:14:44,854] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:14:44,854] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 19:14:44,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:14:44,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [195]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 2.0115 (1.7342)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6401 (8.8994)  time: 0.8545 (0.5321 -- 5.9566)  data: 0.0148 (0.0004 -- 0.2589)  max mem: 16413
Epoch: [195]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6232 (1.7178)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4272 (8.8442)  time: 0.6534 (0.4955 -- 3.0889)  data: 0.0008 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [195] Total time: 0:02:22 (0.8915 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6232 (1.7096)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4272 (8.8442)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.1723 (0.1723)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5368 (2.5368 -- 2.5368)  data: 2.3009 (2.3009 -- 2.3009)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3448 (0.6158)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4630 (0.2010 -- 2.5368)  data: 0.2434 (0.0003 -- 2.3009)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4035 (0.5519)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2240 (0.1712 -- 0.6452)  data: 0.0190 (0.0001 -- 0.3562)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4546 (0.6094)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2100 (0.1328 -- 0.6452)  data: 0.0187 (0.0001 -- 0.3562)  max mem: 16413
Val: Total time: 0:00:08 (0.2964 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [196]  [  0/160]  eta: 0:16:49  lr: 0.000000  min_lr: 0.000000  loss: 1.4514 (1.4514)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6415 (10.6415)  time: 6.3112 (6.3112 -- 6.3112)  data: 5.7735 (5.7735 -- 5.7735)  max mem: 16413
Epoch: [196]  [ 20/160]  eta: 0:02:45  lr: 0.000000  min_lr: 0.000000  loss: 1.6618 (1.6378)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6010 (8.8245)  time: 0.9275 (0.5192 -- 3.5990)  data: 0.3331 (0.0008 -- 3.0525)  max mem: 16413
Epoch: [196]  [ 40/160]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 1.6800 (1.6598)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5767 (8.7133)  time: 0.9005 (0.5297 -- 3.5934)  data: 0.3187 (0.0002 -- 3.0809)  max mem: 16413
Epoch: [196]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8226 (1.7076)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1806 (8.4196)  time: 0.8432 (0.5242 -- 3.4942)  data: 0.2361 (0.0004 -- 2.9337)  max mem: 16413
Epoch: [196]  [ 80/160]  eta: 0:01:17  lr: 0.000000  min_lr: 0.000000  loss: 1.7661 (1.7293)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8900 (8.7517)  time: 0.9115 (0.5138 -- 4.9636)  data: 0.3684 (0.0003 -- 4.4139)  max mem: 16413
Epoch: [196]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.6584 (1.7224)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2629 (8.9953)  time: 0.8944 (0.5244 -- 2.8535)  data: 0.0776 (0.0005 -- 1.5223)  max mem: 16413
[2023-08-31 19:16:48,291] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:16:48,291] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 19:16:48,291] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:16:48,291] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 19:16:56,819] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31474
[2023-08-31 19:16:56,819] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31474
[2023-08-31 19:16:56,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:16:56,819] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:16:56,819] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [196]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7159 (1.7177)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0069 (8.9499)  time: 0.9083 (0.5108 -- 4.8464)  data: 0.0020 (0.0001 -- 0.0167)  max mem: 16413
Epoch: [196]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7541 (1.7217)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8347 (9.0216)  time: 0.8993 (0.5231 -- 3.9094)  data: 0.0012 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [196]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6849 (1.7162)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3242 (9.0963)  time: 0.6273 (0.4934 -- 2.7878)  data: 0.0007 (0.0002 -- 0.0040)  max mem: 16413
Epoch: [196] Total time: 0:02:24 (0.9001 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6849 (1.7100)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3242 (9.0963)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.1723 (0.1723)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3944 (2.3944 -- 2.3944)  data: 2.1905 (2.1905 -- 2.1905)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3443 (0.6161)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4587 (0.1951 -- 2.3944)  data: 0.2515 (0.0006 -- 2.1905)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4038 (0.5520)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2298 (0.1690 -- 0.7907)  data: 0.0301 (0.0001 -- 0.5673)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4545 (0.6095)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2161 (0.1327 -- 0.7907)  data: 0.0298 (0.0001 -- 0.5673)  max mem: 16413
Val: Total time: 0:00:07 (0.2955 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [197]  [  0/160]  eta: 0:22:07  lr: 0.000000  min_lr: 0.000000  loss: 1.8279 (1.8279)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6013 (7.6013)  time: 8.2964 (8.2964 -- 8.2964)  data: 7.7647 (7.7647 -- 7.7647)  max mem: 16413
Epoch: [197]  [ 20/160]  eta: 0:02:49  lr: 0.000000  min_lr: 0.000000  loss: 1.7671 (1.7406)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5529 (9.3701)  time: 0.8565 (0.5221 -- 4.2284)  data: 0.0249 (0.0007 -- 0.4623)  max mem: 16413
Epoch: [197]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 1.6588 (1.7191)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3332 (9.5350)  time: 0.9218 (0.5246 -- 3.9933)  data: 0.0020 (0.0005 -- 0.0160)  max mem: 16413
Epoch: [197]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.8379 (1.7596)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9313 (9.4060)  time: 0.8105 (0.5189 -- 3.4469)  data: 0.0015 (0.0006 -- 0.0030)  max mem: 16413
Epoch: [197]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.6190 (1.7173)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1898 (9.2582)  time: 0.8888 (0.5306 -- 2.7329)  data: 0.0103 (0.0003 -- 0.1782)  max mem: 16413
[2023-08-31 19:18:59,236] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:18:59,236] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 19:18:59,237] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:18:59,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-31 19:19:00,921] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31606
[2023-08-31 19:19:00,922] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:19:00,922] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31606
[2023-08-31 19:19:00,922] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-31 19:19:00,922] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-31 19:19:06,775] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31612
[2023-08-31 19:19:06,775] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31612
[2023-08-31 19:19:06,775] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 19:19:06,775] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 19:19:06,775] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [197]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.4898 (1.6878)  loss_scale: 16384.0000 (16140.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4911 (9.0725)  time: 0.8635 (0.5240 -- 2.3157)  data: 0.0021 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [197]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7756 (1.7090)  loss_scale: 8192.0000 (14826.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9569 (8.8776)  time: 0.8617 (0.5249 -- 2.3577)  data: 0.0020 (0.0004 -- 0.0128)  max mem: 16413
Epoch: [197]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6491 (1.7105)  loss_scale: 8192.0000 (13885.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9254 (8.9584)  time: 0.9033 (0.5217 -- 4.2280)  data: 0.0012 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [197]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7307 (1.7152)  loss_scale: 8192.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1809 (8.9501)  time: 0.6757 (0.4961 -- 2.8103)  data: 0.0012 (0.0002 -- 0.0073)  max mem: 16413
Epoch: [197] Total time: 0:02:23 (0.8965 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7307 (1.6806)  loss_scale: 8192.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1809 (8.9501)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.1721 (0.1721)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4675 (2.4675 -- 2.4675)  data: 2.2253 (2.2253 -- 2.2253)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3450 (0.6163)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4406 (0.1984 -- 2.4675)  data: 0.2239 (0.0006 -- 2.2253)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4029 (0.5522)  acc1: 88.8889 (85.1852)  acc5: 100.0000 (97.8836)  time: 0.2184 (0.1694 -- 0.4323)  data: 0.0155 (0.0001 -- 0.2187)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4547 (0.6096)  acc1: 88.8889 (83.8174)  acc5: 100.0000 (97.5104)  time: 0.2032 (0.1329 -- 0.4323)  data: 0.0152 (0.0001 -- 0.2187)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 84.440 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.44%
Max accuracy: 85.68%
Epoch: [198]  [  0/160]  eta: 0:19:44  lr: 0.000000  min_lr: 0.000000  loss: 1.5664 (1.5664)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7512 (7.7512)  time: 7.4013 (7.4013 -- 7.4013)  data: 6.8566 (6.8566 -- 6.8566)  max mem: 16413
Epoch: [198]  [ 20/160]  eta: 0:02:52  lr: 0.000000  min_lr: 0.000000  loss: 1.8047 (1.7197)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2508 (8.8533)  time: 0.9253 (0.5262 -- 4.1066)  data: 0.0747 (0.0003 -- 1.2406)  max mem: 16413
Epoch: [198]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.7742 (1.7858)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9926 (9.0436)  time: 0.8365 (0.5269 -- 3.2860)  data: 0.0355 (0.0005 -- 0.6885)  max mem: 16413
Epoch: [198]  [ 60/160]  eta: 0:01:41  lr: 0.000000  min_lr: 0.000000  loss: 1.6316 (1.7063)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8485 (9.1741)  time: 0.9506 (0.5271 -- 4.1693)  data: 0.0121 (0.0004 -- 0.2171)  max mem: 16413
[2023-08-31 19:21:11,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:21:11,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:21:11,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 19:21:11,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [198]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.9071 (1.7156)  loss_scale: 16384.0000 (10214.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5520 (9.0925)  time: 0.7940 (0.5239 -- 3.2640)  data: 0.0015 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [198]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.7483 (1.7232)  loss_scale: 16384.0000 (11436.3564)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9159 (9.0091)  time: 0.7896 (0.5324 -- 2.5628)  data: 0.0018 (0.0006 -- 0.0034)  max mem: 16413
[2023-08-31 19:21:44,354] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31782
[2023-08-31 19:21:44,354] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31782
[2023-08-31 19:21:44,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 19:21:44,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-31 19:21:44,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [198]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8290 (1.7408)  loss_scale: 8192.0000 (10967.8017)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5480 (8.9894)  time: 0.9329 (0.5194 -- 3.2637)  data: 0.1341 (0.0005 -- 1.5582)  max mem: 16413
Epoch: [198]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5971 (1.7274)  loss_scale: 8192.0000 (10574.0709)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5799 (8.8941)  time: 0.9176 (0.5335 -- 2.1540)  data: 0.1528 (0.0005 -- 1.1851)  max mem: 16413
Epoch: [198]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7126 (1.7235)  loss_scale: 8192.0000 (10291.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1416 (8.9580)  time: 0.7607 (0.4945 -- 3.0367)  data: 0.0009 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [198] Total time: 0:02:25 (0.9063 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7126 (1.7131)  loss_scale: 8192.0000 (10291.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1416 (8.9580)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.1724 (0.1724)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5062 (2.5062 -- 2.5062)  data: 2.3007 (2.3007 -- 2.3007)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3447 (0.6164)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4331 (0.1981 -- 2.5062)  data: 0.2248 (0.0005 -- 2.3007)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4029 (0.5523)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2194 (0.1688 -- 0.4141)  data: 0.0185 (0.0001 -- 0.1942)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4546 (0.6097)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2070 (0.1329 -- 0.4141)  data: 0.0182 (0.0001 -- 0.1942)  max mem: 16413
Val: Total time: 0:00:07 (0.2918 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Epoch: [199]  [  0/160]  eta: 0:18:44  lr: 0.000000  min_lr: 0.000000  loss: 1.3355 (1.3355)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2132 (7.2132)  time: 7.0311 (7.0311 -- 7.0311)  data: 5.7102 (5.7102 -- 5.7102)  max mem: 16413
Epoch: [199]  [ 20/160]  eta: 0:02:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6065 (1.6462)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9232 (8.1720)  time: 0.8234 (0.5241 -- 3.3842)  data: 0.2185 (0.0008 -- 2.8158)  max mem: 16413
Epoch: [199]  [ 40/160]  eta: 0:01:59  lr: 0.000000  min_lr: 0.000000  loss: 1.7199 (1.6849)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7128 (8.7277)  time: 0.8629 (0.5229 -- 2.4279)  data: 0.1771 (0.0004 -- 1.8850)  max mem: 16413
Epoch: [199]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.7407 (1.6770)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7334 (8.7382)  time: 0.9522 (0.5187 -- 4.3042)  data: 0.0017 (0.0004 -- 0.0047)  max mem: 16413
[2023-08-31 19:23:51,272] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:23:51,273] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-31 19:23:51,273] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-31 19:23:51,273] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [199]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.9414 (1.7365)  loss_scale: 8192.0000 (9203.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1125 (9.0412)  time: 0.8050 (0.5249 -- 2.8788)  data: 0.0018 (0.0003 -- 0.0058)  max mem: 16413
Epoch: [199]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.6052 (1.7170)  loss_scale: 16384.0000 (10625.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9045 (8.9371)  time: 0.9024 (0.5415 -- 2.5129)  data: 0.0425 (0.0005 -- 0.4272)  max mem: 16413
Epoch: [199]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8078 (1.7246)  loss_scale: 16384.0000 (11577.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6836 (8.9197)  time: 0.9007 (0.5232 -- 3.7324)  data: 0.2178 (0.0003 -- 3.2016)  max mem: 16413
Epoch: [199]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5110 (1.7019)  loss_scale: 16384.0000 (12258.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4671 (8.9387)  time: 0.9163 (0.5257 -- 3.6838)  data: 0.0762 (0.0004 -- 0.8871)  max mem: 16413
[2023-08-31 19:25:04,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=189, lr=[1.1136245707468657e-09, 1.1136245707468657e-09, 1.4848327609958208e-09, 1.4848327609958208e-09, 1.9797770146610946e-09, 1.9797770146610946e-09, 2.6397026862147927e-09, 2.6397026862147927e-09, 3.5196035816197234e-09, 3.5196035816197234e-09, 4.6928047754929645e-09, 4.6928047754929645e-09, 6.257073033990619e-09, 6.257073033990619e-09, 8.342764045320827e-09, 8.342764045320827e-09, 1.1123685393761101e-08, 1.1123685393761101e-08, 1.4831580525014802e-08, 1.4831580525014802e-08, 1.9775440700019736e-08, 1.9775440700019736e-08, 2.636725426669298e-08, 2.636725426669298e-08, 3.515633902225731e-08, 3.515633902225731e-08, 4.687511869634308e-08, 4.687511869634308e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-31 19:25:04,092] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=16.893521318415072, CurrSamplesPerSec=24.64875474728875, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [199]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6151 (1.6929)  loss_scale: 16384.0000 (12748.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9342 (9.0000)  time: 0.6103 (0.4970 -- 1.6807)  data: 0.0142 (0.0002 -- 0.2695)  max mem: 16413
Epoch: [199] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6151 (1.6929)  loss_scale: 16384.0000 (12748.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9342 (9.0000)
[2023-08-31 19:25:04,096] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-08-31 19:25:04,098] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt
[2023-08-31 19:25:04,098] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt...
[2023-08-31 19:25:04,098] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-08-31 19:25:05,090] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt.
[2023-08-31 19:25:05,091] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/27]  eta: 0:01:11  loss: 0.1722 (0.1722)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6405 (2.6405 -- 2.6405)  data: 2.4202 (2.4202 -- 2.4202)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3446 (0.6162)  acc1: 88.8889 (82.8283)  acc5: 100.0000 (97.9798)  time: 0.4429 (0.2015 -- 2.6405)  data: 0.2297 (0.0004 -- 2.4202)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4034 (0.5522)  acc1: 88.8889 (85.7143)  acc5: 100.0000 (97.8836)  time: 0.2216 (0.1688 -- 0.3674)  data: 0.0148 (0.0001 -- 0.1849)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.4546 (0.6096)  acc1: 88.8889 (84.2324)  acc5: 100.0000 (97.5104)  time: 0.2066 (0.1330 -- 0.3674)  data: 0.0145 (0.0001 -- 0.1849)  max mem: 16413
Val: Total time: 0:00:08 (0.2986 s / it)
* Acc@1 84.647 Acc@5 97.925 loss 0.595
Accuracy of the network on the 482 val images: 84.65%
Max accuracy: 85.68%
Test:  [  0/603]  eta: 1:10:23  loss: 0.1649 (0.1649)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 7.0046 (7.0046 -- 7.0046)  data: 6.7600 (6.7600 -- 6.7600)  max mem: 16413
Test:  [ 10/603]  eta: 0:10:33  loss: 0.4404 (0.5487)  acc1: 83.3333 (86.3636)  acc5: 100.0000 (98.4848)  time: 1.0685 (0.1277 -- 7.0046)  data: 0.9161 (0.0002 -- 6.7600)  max mem: 16413
Test:  [ 20/603]  eta: 0:14:37  loss: 0.7177 (0.8658)  acc1: 66.6667 (71.4286)  acc5: 100.0000 (94.4444)  time: 1.2301 (0.1228 -- 14.1079)  data: 1.0827 (0.0002 -- 13.9812)  max mem: 16413
Test:  [ 30/603]  eta: 0:10:52  loss: 0.6222 (0.7486)  acc1: 66.6667 (76.8817)  acc5: 100.0000 (96.2366)  time: 1.1778 (0.1202 -- 14.1079)  data: 1.0375 (0.0003 -- 13.9812)  max mem: 16413
Test:  [ 40/603]  eta: 0:12:26  loss: 0.4623 (0.7363)  acc1: 83.3333 (77.2358)  acc5: 100.0000 (96.7480)  time: 1.1371 (0.1202 -- 14.5318)  data: 0.9912 (0.0003 -- 14.3881)  max mem: 16413
Test:  [ 50/603]  eta: 0:11:38  loss: 0.4623 (0.7073)  acc1: 83.3333 (78.7582)  acc5: 100.0000 (97.0588)  time: 1.4558 (0.1214 -- 14.5318)  data: 1.2920 (0.0002 -- 14.3881)  max mem: 16413
Test:  [ 60/603]  eta: 0:19:49  loss: 0.6308 (0.7888)  acc1: 83.3333 (74.8634)  acc5: 100.0000 (95.9016)  time: 3.9623 (0.1214 -- 41.3152)  data: 3.8091 (0.0002 -- 41.1911)  max mem: 16413
Test:  [ 70/603]  eta: 0:17:08  loss: 0.6266 (0.7494)  acc1: 83.3333 (76.7606)  acc5: 100.0000 (96.4789)  time: 3.6321 (0.1158 -- 41.3152)  data: 3.4881 (0.0001 -- 41.1911)  max mem: 16413
Test:  [ 80/603]  eta: 0:17:30  loss: 0.5081 (0.7435)  acc1: 83.3333 (76.9547)  acc5: 100.0000 (96.7078)  time: 1.4547 (0.1158 -- 12.3381)  data: 1.2934 (0.0001 -- 12.1147)  max mem: 16413
Test:  [ 90/603]  eta: 0:16:28  loss: 0.4208 (0.7265)  acc1: 83.3333 (77.8388)  acc5: 100.0000 (96.8864)  time: 1.9117 (0.1225 -- 12.3381)  data: 1.7373 (0.0004 -- 12.1147)  max mem: 16413
Test:  [100/603]  eta: 0:18:42  loss: 0.6652 (0.7719)  acc1: 83.3333 (75.7426)  acc5: 100.0000 (96.2046)  time: 3.1334 (0.1225 -- 24.0880)  data: 2.9161 (0.0005 -- 23.7659)  max mem: 16413
Test:  [110/603]  eta: 0:16:59  loss: 0.6300 (0.7500)  acc1: 83.3333 (76.7267)  acc5: 100.0000 (96.5465)  time: 2.7108 (0.1177 -- 24.0880)  data: 2.4677 (0.0002 -- 23.7659)  max mem: 16413
Test:  [120/603]  eta: 0:16:46  loss: 0.5127 (0.7476)  acc1: 83.3333 (76.8595)  acc5: 100.0000 (96.6942)  time: 1.3380 (0.1177 -- 13.4683)  data: 1.0846 (0.0002 -- 12.8048)  max mem: 16413
Test:  [130/603]  eta: 0:16:07  loss: 0.4539 (0.7342)  acc1: 83.3333 (77.2265)  acc5: 100.0000 (96.9466)  time: 1.9265 (0.1260 -- 13.4683)  data: 1.5960 (0.0007 -- 12.8048)  max mem: 16413
Test:  [140/603]  eta: 0:17:21  loss: 0.4680 (0.7232)  acc1: 83.3333 (77.8960)  acc5: 100.0000 (97.0449)  time: 3.2478 (0.1245 -- 27.0748)  data: 2.8937 (0.0004 -- 26.7451)  max mem: 16413
Test:  [150/603]  eta: 0:16:18  loss: 0.4134 (0.7022)  acc1: 83.3333 (78.8079)  acc5: 100.0000 (97.1302)  time: 2.8991 (0.1127 -- 27.0748)  data: 2.6606 (0.0001 -- 26.7451)  max mem: 16413
Test:  [160/603]  eta: 0:16:26  loss: 0.4075 (0.6952)  acc1: 83.3333 (78.9855)  acc5: 100.0000 (97.2050)  time: 2.0754 (0.1127 -- 25.9619)  data: 1.8538 (0.0001 -- 25.8212)  max mem: 16413
Test:  [170/603]  eta: 0:16:08  loss: 0.4368 (0.6878)  acc1: 83.3333 (79.1423)  acc5: 100.0000 (97.3684)  time: 2.8209 (0.1288 -- 25.9619)  data: 2.5346 (0.0003 -- 25.8212)  max mem: 16413
Test:  [180/603]  eta: 0:15:54  loss: 0.4368 (0.6820)  acc1: 83.3333 (79.5580)  acc5: 100.0000 (97.4217)  time: 2.4976 (0.1228 -- 18.6170)  data: 2.2534 (0.0002 -- 18.3473)  max mem: 16413
Test:  [190/603]  eta: 0:15:59  loss: 0.3432 (0.6676)  acc1: 83.3333 (80.1920)  acc5: 100.0000 (97.4695)  time: 3.0747 (0.1176 -- 33.0058)  data: 2.8781 (0.0002 -- 32.8438)  max mem: 16413
Test:  [200/603]  eta: 0:15:28  loss: 0.3622 (0.6627)  acc1: 83.3333 (80.2653)  acc5: 100.0000 (97.5124)  time: 2.7306 (0.1176 -- 33.0058)  data: 2.4708 (0.0002 -- 32.8438)  max mem: 16413
Test:  [210/603]  eta: 0:15:11  loss: 0.5481 (0.6591)  acc1: 83.3333 (80.3318)  acc5: 100.0000 (97.6303)  time: 2.2653 (0.1217 -- 20.8262)  data: 1.9454 (0.0005 -- 20.6744)  max mem: 16413
Test:  [220/603]  eta: 0:15:04  loss: 0.4526 (0.6531)  acc1: 83.3333 (80.6938)  acc5: 100.0000 (97.7376)  time: 2.9279 (0.1217 -- 20.8262)  data: 2.6763 (0.0002 -- 20.6744)  max mem: 16413
Test:  [230/603]  eta: 0:15:04  loss: 0.4041 (0.6450)  acc1: 83.3333 (81.0967)  acc5: 100.0000 (97.6912)  time: 3.5402 (0.1233 -- 32.4814)  data: 3.3478 (0.0002 -- 32.3305)  max mem: 16413
Test:  [240/603]  eta: 0:14:46  loss: 0.3572 (0.6418)  acc1: 83.3333 (81.1203)  acc5: 100.0000 (97.7178)  time: 3.3268 (0.1327 -- 32.4814)  data: 3.0788 (0.0005 -- 32.3305)  max mem: 16413
Test:  [250/603]  eta: 0:14:19  loss: 0.4282 (0.6449)  acc1: 83.3333 (81.1421)  acc5: 100.0000 (97.7424)  time: 2.5484 (0.1407 -- 18.6319)  data: 2.2275 (0.0005 -- 18.0629)  max mem: 16413
Test:  [260/603]  eta: 0:13:53  loss: 0.5238 (0.6433)  acc1: 83.3333 (81.2261)  acc5: 100.0000 (97.7011)  time: 2.3128 (0.1159 -- 11.5803)  data: 2.0615 (0.0002 -- 11.3040)  max mem: 16413
Test:  [270/603]  eta: 0:13:39  loss: 0.3228 (0.6356)  acc1: 83.3333 (81.6113)  acc5: 100.0000 (97.6630)  time: 2.7937 (0.1159 -- 28.8798)  data: 2.6174 (0.0002 -- 28.7028)  max mem: 16413
Test:  [280/603]  eta: 0:13:24  loss: 0.3592 (0.6427)  acc1: 83.3333 (81.4354)  acc5: 100.0000 (97.5682)  time: 3.2486 (0.1212 -- 28.8798)  data: 3.0798 (0.0002 -- 28.7028)  max mem: 16413
Test:  [290/603]  eta: 0:13:02  loss: 0.6411 (0.6454)  acc1: 83.3333 (81.4433)  acc5: 100.0000 (97.5945)  time: 3.0487 (0.1241 -- 22.2680)  data: 2.7476 (0.0003 -- 22.0717)  max mem: 16413
Test:  [300/603]  eta: 0:12:27  loss: 0.4101 (0.6421)  acc1: 83.3333 (81.5615)  acc5: 100.0000 (97.6190)  time: 2.1555 (0.1270 -- 22.2680)  data: 1.7497 (0.0006 -- 22.0717)  max mem: 16413
Test:  [310/603]  eta: 0:12:07  loss: 0.3000 (0.6372)  acc1: 100.0000 (81.8328)  acc5: 100.0000 (97.5348)  time: 2.2415 (0.1231 -- 27.2400)  data: 1.9096 (0.0003 -- 27.0789)  max mem: 16413
Test:  [320/603]  eta: 0:11:39  loss: 0.3652 (0.6413)  acc1: 83.3333 (81.7238)  acc5: 100.0000 (97.5078)  time: 2.5472 (0.1231 -- 27.2400)  data: 2.2404 (0.0003 -- 27.0789)  max mem: 16413
Test:  [330/603]  eta: 0:11:18  loss: 0.8173 (0.6451)  acc1: 83.3333 (81.7221)  acc5: 100.0000 (97.4824)  time: 2.4904 (0.1200 -- 16.9789)  data: 2.1970 (0.0004 -- 16.7986)  max mem: 16413
Test:  [340/603]  eta: 0:10:47  loss: 0.4555 (0.6427)  acc1: 83.3333 (81.7693)  acc5: 100.0000 (97.5073)  time: 2.2825 (0.1200 -- 16.9789)  data: 2.0138 (0.0004 -- 16.7986)  max mem: 16413
Test:  [350/603]  eta: 0:10:28  loss: 0.3249 (0.6384)  acc1: 83.3333 (82.0038)  acc5: 100.0000 (97.4359)  time: 2.4612 (0.1220 -- 20.5542)  data: 2.2378 (0.0002 -- 20.4002)  max mem: 16413
Test:  [360/603]  eta: 0:10:06  loss: 0.4363 (0.6420)  acc1: 83.3333 (81.9021)  acc5: 100.0000 (97.4146)  time: 3.0798 (0.1131 -- 20.5542)  data: 2.8567 (0.0001 -- 20.4002)  max mem: 16413
Test:  [370/603]  eta: 0:09:37  loss: 0.8389 (0.6459)  acc1: 83.3333 (81.8509)  acc5: 100.0000 (97.3944)  time: 2.4109 (0.1131 -- 18.9778)  data: 2.2112 (0.0001 -- 18.3570)  max mem: 16413
Test:  [380/603]  eta: 0:09:18  loss: 0.4947 (0.6461)  acc1: 83.3333 (81.8023)  acc5: 100.0000 (97.4628)  time: 2.6684 (0.1151 -- 31.8995)  data: 2.4810 (0.0002 -- 31.7270)  max mem: 16413
Test:  [390/603]  eta: 0:08:56  loss: 0.4342 (0.6428)  acc1: 83.3333 (81.9267)  acc5: 100.0000 (97.4425)  time: 3.2136 (0.1243 -- 31.8995)  data: 3.0012 (0.0005 -- 31.7270)  max mem: 16413
Test:  [400/603]  eta: 0:08:27  loss: 0.4768 (0.6441)  acc1: 83.3333 (81.8371)  acc5: 100.0000 (97.3815)  time: 2.4474 (0.1212 -- 23.2749)  data: 2.2693 (0.0002 -- 23.1442)  max mem: 16413
Test:  [410/603]  eta: 0:08:01  loss: 0.5017 (0.6453)  acc1: 83.3333 (81.8735)  acc5: 100.0000 (97.4047)  time: 2.0488 (0.1212 -- 14.1416)  data: 1.8722 (0.0002 -- 13.7057)  max mem: 16413
Test:  [420/603]  eta: 0:07:38  loss: 0.4829 (0.6457)  acc1: 83.3333 (81.7894)  acc5: 100.0000 (97.4664)  time: 2.6127 (0.1274 -- 17.3910)  data: 2.3292 (0.0004 -- 16.9987)  max mem: 16413
Test:  [430/603]  eta: 0:07:16  loss: 0.4666 (0.6432)  acc1: 83.3333 (81.9026)  acc5: 100.0000 (97.4478)  time: 3.1484 (0.1200 -- 30.0459)  data: 2.8571 (0.0003 -- 29.9116)  max mem: 16413
Test:  [440/603]  eta: 0:06:53  loss: 0.4394 (0.6442)  acc1: 83.3333 (81.8216)  acc5: 100.0000 (97.3923)  time: 3.1248 (0.1178 -- 30.0459)  data: 2.9144 (0.0003 -- 29.9116)  max mem: 16413
Test:  [450/603]  eta: 0:06:29  loss: 0.4994 (0.6453)  acc1: 83.3333 (81.8551)  acc5: 100.0000 (97.4132)  time: 3.0219 (0.1178 -- 23.2533)  data: 2.7715 (0.0004 -- 23.0366)  max mem: 16413
Test:  [460/603]  eta: 0:06:02  loss: 0.4994 (0.6458)  acc1: 83.3333 (81.7787)  acc5: 100.0000 (97.4693)  time: 2.5961 (0.1344 -- 15.4459)  data: 2.3186 (0.0008 -- 15.1783)  max mem: 16413
Test:  [470/603]  eta: 0:05:39  loss: 0.4414 (0.6434)  acc1: 83.3333 (81.8825)  acc5: 100.0000 (97.4522)  time: 2.7411 (0.1242 -- 28.8620)  data: 2.4264 (0.0002 -- 28.5959)  max mem: 16413
Test:  [480/603]  eta: 0:05:13  loss: 0.2780 (0.6438)  acc1: 83.3333 (81.8434)  acc5: 100.0000 (97.4012)  time: 2.8492 (0.1242 -- 28.8620)  data: 2.5260 (0.0002 -- 28.5959)  max mem: 16413
Test:  [490/603]  eta: 0:04:50  loss: 0.6130 (0.6453)  acc1: 83.3333 (81.7380)  acc5: 100.0000 (97.3863)  time: 2.8497 (0.1299 -- 20.6060)  data: 2.5933 (0.0007 -- 20.3048)  max mem: 16413
Test:  [500/603]  eta: 0:04:26  loss: 0.6310 (0.6495)  acc1: 83.3333 (81.6035)  acc5: 100.0000 (97.3054)  time: 3.5528 (0.1250 -- 18.7737)  data: 3.3336 (0.0007 -- 18.0374)  max mem: 16413
Test:  [510/603]  eta: 0:03:59  loss: 0.5824 (0.6495)  acc1: 83.3333 (81.6373)  acc5: 100.0000 (97.2603)  time: 2.8933 (0.1214 -- 18.7737)  data: 2.7103 (0.0004 -- 18.0374)  max mem: 16413
Test:  [520/603]  eta: 0:03:35  loss: 0.6433 (0.6506)  acc1: 83.3333 (81.4459)  acc5: 100.0000 (97.3129)  time: 2.7988 (0.1183 -- 19.0732)  data: 2.6168 (0.0003 -- 18.8685)  max mem: 16413
Test:  [530/603]  eta: 0:03:07  loss: 0.6433 (0.6524)  acc1: 83.3333 (81.3559)  acc5: 100.0000 (97.2693)  time: 2.3557 (0.1183 -- 19.0732)  data: 2.0471 (0.0003 -- 18.8685)  max mem: 16413
Test:  [540/603]  eta: 0:02:43  loss: 0.6311 (0.6561)  acc1: 83.3333 (81.2384)  acc5: 100.0000 (97.1965)  time: 2.3273 (0.1213 -- 33.5484)  data: 2.0547 (0.0002 -- 33.4063)  max mem: 16413
Test:  [550/603]  eta: 0:02:19  loss: 0.5188 (0.6560)  acc1: 83.3333 (81.2765)  acc5: 100.0000 (97.1567)  time: 4.1902 (0.1213 -- 42.8969)  data: 3.9948 (0.0002 -- 42.6864)  max mem: 16413
Test:  [560/603]  eta: 0:01:53  loss: 0.6207 (0.6569)  acc1: 83.3333 (81.1052)  acc5: 100.0000 (97.2074)  time: 3.7243 (0.1173 -- 42.8969)  data: 3.4937 (0.0003 -- 42.6864)  max mem: 16413
Test:  [570/603]  eta: 0:01:26  loss: 0.7207 (0.6585)  acc1: 66.6667 (81.0274)  acc5: 100.0000 (97.1687)  time: 2.5666 (0.1173 -- 23.5282)  data: 2.3510 (0.0003 -- 23.3761)  max mem: 16413
Test:  [580/603]  eta: 0:01:00  loss: 0.4899 (0.6618)  acc1: 83.3333 (80.9237)  acc5: 100.0000 (97.1027)  time: 2.3834 (0.1265 -- 14.1893)  data: 2.1297 (0.0005 -- 13.8676)  max mem: 16413
Test:  [590/603]  eta: 0:00:33  loss: 0.4899 (0.6617)  acc1: 83.3333 (80.9645)  acc5: 100.0000 (97.0671)  time: 1.8866 (0.1199 -- 10.5176)  data: 1.6512 (0.0003 -- 10.3232)  max mem: 16413
Test:  [600/603]  eta: 0:00:07  loss: 0.5924 (0.6623)  acc1: 83.3333 (80.8098)  acc5: 100.0000 (97.1159)  time: 1.5037 (0.1104 -- 13.1469)  data: 1.3547 (0.0001 -- 13.0364)  max mem: 16413
Test:  [602/603]  eta: 0:00:02  loss: 0.5924 (0.6626)  acc1: 83.3333 (80.8299)  acc5: 100.0000 (97.0954)  time: 1.0833 (0.1104 -- 13.1469)  data: 0.9431 (0.0001 -- 13.0364)  max mem: 16413
Test: Total time: 0:25:54 (2.5786 s / it)
* Acc@1 81.286 Acc@5 96.805 loss 0.665
Start merging results...
Reading individual output files
Computing final results
Accuracy of the network on the 7230 test videos: Top-1: 87.34%, Top-5: 99.79%
Training time 8:48:27
/home/vislab-001/.local/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
