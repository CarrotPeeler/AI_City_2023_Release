[2023-08-30 13:13:22,237] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-08-30 13:13:22,399] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
Namespace(aa='rand-m7-n4-mstd0.5-inc1', attn_drop_rate=0.0, auto_resume=True, batch_size=6, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/vislab-001/Jared/Envy_AI_City/slowfast', data_root='', data_set='AI-City-Track-3', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/deepspeed_config.json', deepspeed_mpi=False, device='cuda', disable_eval_during_finetuning=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop=0.0, drop_path=0.1, enable_deepspeed=True, epochs=200, eval=False, eval_data_path=None, finetune='/home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth', fname_tmpl='img_{:05}.jpg', gpu=0, head_drop_rate=0.0, imagenet_default_mean_and_std=True, init_scale=0.001, input_size=224, layer_decay=0.75, local_rank=0, log_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', lr=0.0005, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=16, num_frames=16, num_sample=2, num_segments=1, num_workers=8, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='/home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output', pin_mem=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', sampling_rate=4, save_ckpt=True, save_ckpt_freq=20, seed=0, short_side_size=224, smoothing=0.1, sparse_sample=False, start_epoch=0, start_idx=1, test_num_crop=3, test_num_segment=5, train_interpolation='bicubic', tubelet_size=2, update_freq=1, use_mean_pooling=True, validation=False, warmup_epochs=5, warmup_lr=1e-08, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, with_checkpoint=False, world_size=2)
Number of the class = 16
Number of the class = 16
Number of the class = 16
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f7d92a8aa90>
Mixup is activated!
Patch size = (16, 16)
Load ckpt from /home/vislab-001/Jared/Envy_AI_City/slowfast/checkpoints/vit_b_k710_dl_from_giant.pth
Load state_dict by model_key = module
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Weights of VisionTransformer not initialized from pretrained model: ['head.weight', 'head.bias']
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head_dropout): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=768, out_features=16, bias=True)
)
number of params: 86239504
LR = 0.00002344
Batch size = 12
Update frequent = 1
Number of training examples = 1927
Number of training training per epoch = 160
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'pos_embed', 'cls_token'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
[2023-08-30 13:13:27,636] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-30 13:13:27,637] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-30 13:13:27,662] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.1, git-hash=unknown, git-branch=unknown
[2023-08-30 13:13:27,662] [INFO] [comm.py:631:init_distributed] cdb=None
[2023-08-30 13:13:27,774] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/vislab-001/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/vislab-001/.cache/torch_extensions/py38_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.3986654281616211 seconds
[2023-08-30 13:13:28,736] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-08-30 13:13:28,745] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-08-30 13:13:28,745] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-08-30 13:13:28,764] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-08-30 13:13:28,764] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-08-30 13:13:28,764] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-08-30 13:13:28,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 13:13:28,764] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   amp_params ................... False
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7d456935e0>
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-08-30 13:13:28,765] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   dump_state ................... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 128, 'scale_window': 128, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   gradient_clipping ............ 0
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 128
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   loss_scale ................... 0
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-08-30 13:13:28,766] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   optimizer_params ............. {'lr': 0.0005, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   pld_params ................... False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   steps_per_print .............. 1000
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   train_batch_size ............. 12
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  6
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   world_size ................... 2
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   zero_enabled ................. False
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-08-30 13:13:28,767] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[2023-08-30 13:13:28,767] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 12, 
    "train_micro_batch_size_per_gpu": 6, 
    "steps_per_print": 1000, 
    "gradient_clipping": 0, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 0.0005, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 7, 
        "loss_scale_window": 128
    }
}
model.gradient_accumulation_steps() = 1
Use step level LR scheduler!
Set warmup steps = 800
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Start training for 200 epochs
Epoch: [0]  [  0/160]  eta: 0:31:30  lr: 0.000000  min_lr: 0.000000  loss: 2.7733 (2.7733)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  time: 11.8146 (11.8146 -- 11.8146)  data: 7.3374 (7.3374 -- 7.3374)  max mem: 16413
Epoch: [0]  [ 20/160]  eta: 0:02:37  lr: 0.000001  min_lr: 0.000000  loss: 2.7731 (2.7731)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5500 (1.5783)  time: 0.5876 (0.5039 -- 1.4767)  data: 0.0419 (0.0004 -- 0.8104)  max mem: 16413
Epoch: [0]  [ 40/160]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000000  loss: 2.7726 (2.7728)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4745 (1.5372)  time: 0.8834 (0.5042 -- 2.1225)  data: 0.2684 (0.0004 -- 1.3346)  max mem: 16413
Epoch: [0]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 2.7725 (2.7727)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4275 (1.5261)  time: 0.9355 (0.4866 -- 3.7585)  data: 0.4129 (0.0003 -- 3.2442)  max mem: 16413
Epoch: [0]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 2.7724 (2.7727)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4292 (1.5208)  time: 0.8230 (0.5163 -- 3.0894)  data: 0.2876 (0.0003 -- 2.5799)  max mem: 16413
Epoch: [0]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 2.7723 (2.7726)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4845 (1.5130)  time: 0.8307 (0.5050 -- 2.7151)  data: 0.2936 (0.0003 -- 2.1926)  max mem: 16413
Epoch: [0]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 2.7719 (2.7725)  loss_scale: 128.0000 (128.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5031 (1.5125)  time: 0.8181 (0.5207 -- 4.0829)  data: 0.2735 (0.0002 -- 3.5668)  max mem: 16413
[2023-08-30 13:15:25,425] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:15:25,425] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
[2023-08-30 13:15:25,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:15:25,431] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 128 to 256
Epoch: [0]  [140/160]  eta: 0:00:17  lr: 0.000004  min_lr: 0.000000  loss: 2.7716 (2.7724)  loss_scale: 256.0000 (139.8014)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4326 (1.5000)  time: 0.8710 (0.5165 -- 3.4157)  data: 0.3349 (0.0002 -- 2.8893)  max mem: 16413
Epoch: [0]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 2.7718 (2.7723)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3411 (1.4885)  time: 0.6727 (0.4882 -- 2.3085)  data: 0.1485 (0.0002 -- 1.7837)  max mem: 16413
Epoch: [0] Total time: 0:02:19 (0.8736 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 2.7718 (2.7723)  loss_scale: 256.0000 (153.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3411 (1.4885)
Val:  [ 0/27]  eta: 0:01:18  loss: 2.7695 (2.7695)  acc1: 22.2222 (22.2222)  acc5: 88.8889 (88.8889)  time: 2.9066 (2.9066 -- 2.9066)  data: 2.4990 (2.4990 -- 2.4990)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7695 (2.7695)  acc1: 33.3333 (32.3232)  acc5: 66.6667 (71.7172)  time: 0.4593 (0.2029 -- 2.9066)  data: 0.2305 (0.0005 -- 2.4990)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7691 (2.7693)  acc1: 33.3333 (33.8624)  acc5: 66.6667 (70.3704)  time: 0.2085 (0.1695 -- 0.2446)  data: 0.0043 (0.0001 -- 0.0452)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7691 (2.7695)  acc1: 33.3333 (33.6100)  acc5: 66.6667 (68.0498)  time: 0.1961 (0.1685 -- 0.2446)  data: 0.0034 (0.0001 -- 0.0452)  max mem: 16413
Val: Total time: 0:00:08 (0.3006 s / it)
* Acc@1 32.365 Acc@5 70.332 loss 2.769
Accuracy of the network on the 482 val images: 32.37%
[2023-08-30 13:15:56,764] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-30 13:15:56,767] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 13:15:56,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
/home/vislab-001/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-08-30 13:15:56,770] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 13:15:57,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 13:15:57,784] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 32.37%
Epoch: [1]  [  0/160]  eta: 0:20:12  lr: 0.000005  min_lr: 0.000000  loss: 2.7710 (2.7710)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7301 (1.7301)  time: 7.5809 (7.5809 -- 7.5809)  data: 4.6686 (4.6686 -- 4.6686)  max mem: 16413
Epoch: [1]  [ 20/160]  eta: 0:02:40  lr: 0.000005  min_lr: 0.000000  loss: 2.7713 (2.7708)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5160 (1.5230)  time: 0.8256 (0.5095 -- 3.3815)  data: 0.0599 (0.0004 -- 0.7109)  max mem: 16413
Epoch: [1]  [ 40/160]  eta: 0:02:04  lr: 0.000006  min_lr: 0.000000  loss: 2.7701 (2.7706)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5802 (1.5390)  time: 0.9298 (0.5182 -- 3.3109)  data: 0.0026 (0.0003 -- 0.0128)  max mem: 16413
Epoch: [1]  [ 60/160]  eta: 0:01:38  lr: 0.000006  min_lr: 0.000000  loss: 2.7694 (2.7701)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5243 (1.5471)  time: 0.8785 (0.5111 -- 3.3732)  data: 0.0016 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [1]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000000  loss: 2.7676 (2.7695)  loss_scale: 256.0000 (256.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5141 (1.5314)  time: 0.8267 (0.5244 -- 3.8930)  data: 0.0013 (0.0003 -- 0.0028)  max mem: 16413
[2023-08-30 13:17:28,468] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:17:28,469] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
[2023-08-30 13:17:28,469] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:17:28,470] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 256 to 512
Epoch: [1]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 2.7666 (2.7686)  loss_scale: 256.0000 (268.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5525 (1.5320)  time: 0.9085 (0.4913 -- 4.0448)  data: 0.0009 (0.0003 -- 0.0017)  max mem: 16413
Epoch: [1]  [120/160]  eta: 0:00:37  lr: 0.000008  min_lr: 0.000000  loss: 2.7632 (2.7679)  loss_scale: 512.0000 (308.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5019 (1.5222)  time: 0.8753 (0.5183 -- 3.2562)  data: 0.0020 (0.0003 -- 0.0108)  max mem: 16413
Epoch: [1]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 2.7596 (2.7668)  loss_scale: 512.0000 (337.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5083 (1.5227)  time: 0.8185 (0.5093 -- 3.9266)  data: 0.0012 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [1]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 2.7550 (2.7657)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4879 (1.5260)  time: 0.6074 (0.4934 -- 1.4394)  data: 0.0009 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [1] Total time: 0:02:20 (0.8780 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 2.7550 (2.7657)  loss_scale: 512.0000 (358.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.4879 (1.5260)
Val:  [ 0/27]  eta: 0:01:08  loss: 2.7372 (2.7372)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.5187 (2.5187 -- 2.5187)  data: 2.2909 (2.2909 -- 2.2909)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.7418 (2.7422)  acc1: 33.3333 (35.3535)  acc5: 88.8889 (84.8485)  time: 0.4203 (0.2023 -- 2.5187)  data: 0.2092 (0.0006 -- 2.2909)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.7409 (2.7405)  acc1: 44.4444 (40.7407)  acc5: 88.8889 (88.3598)  time: 0.2024 (0.1713 -- 0.2235)  data: 0.0007 (0.0001 -- 0.0017)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.7420 (2.7419)  acc1: 44.4444 (42.3237)  acc5: 88.8889 (87.5519)  time: 0.1894 (0.1329 -- 0.2235)  data: 0.0005 (0.0001 -- 0.0017)  max mem: 16413
Val: Total time: 0:00:07 (0.2798 s / it)
* Acc@1 43.154 Acc@5 85.685 loss 2.741
Accuracy of the network on the 482 val images: 43.15%
[2023-08-30 13:18:25,828] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 13:18:25,830] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 13:18:25,830] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 13:18:25,830] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 13:18:27,162] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 13:18:27,162] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 43.15%
Epoch: [2]  [  0/160]  eta: 0:18:47  lr: 0.000009  min_lr: 0.000000  loss: 2.7575 (2.7575)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.3791 (1.3791)  time: 7.0487 (7.0487 -- 7.0487)  data: 6.5103 (6.5103 -- 6.5103)  max mem: 16413
Epoch: [2]  [ 20/160]  eta: 0:02:45  lr: 0.000010  min_lr: 0.000000  loss: 2.7575 (2.7542)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5636 (1.5543)  time: 0.8869 (0.5332 -- 3.6194)  data: 0.3382 (0.0003 -- 3.0695)  max mem: 16413
Epoch: [2]  [ 40/160]  eta: 0:02:01  lr: 0.000011  min_lr: 0.000000  loss: 2.7514 (2.7521)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5054 (1.5474)  time: 0.8396 (0.5165 -- 3.0425)  data: 0.2873 (0.0008 -- 2.4841)  max mem: 16413
Epoch: [2]  [ 60/160]  eta: 0:01:37  lr: 0.000011  min_lr: 0.000000  loss: 2.7440 (2.7495)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5128 (1.5371)  time: 0.9041 (0.5196 -- 4.0225)  data: 0.0375 (0.0003 -- 0.3875)  max mem: 16413
[2023-08-30 13:19:30,136] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:19:30,136] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
[2023-08-30 13:19:30,136] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:19:30,136] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 512 to 1024
Epoch: [2]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 2.7417 (2.7472)  loss_scale: 1024.0000 (619.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5304 (1.5302)  time: 0.8398 (0.5225 -- 3.0437)  data: 0.0019 (0.0005 -- 0.0035)  max mem: 16413
Epoch: [2]  [100/160]  eta: 0:00:54  lr: 0.000012  min_lr: 0.000000  loss: 2.7356 (2.7448)  loss_scale: 1024.0000 (699.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5351 (1.5397)  time: 0.7791 (0.5229 -- 2.4721)  data: 0.0570 (0.0002 -- 1.1070)  max mem: 16413
Epoch: [2]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 2.7302 (2.7423)  loss_scale: 1024.0000 (753.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5345 (1.5465)  time: 0.9267 (0.5130 -- 4.4207)  data: 0.0941 (0.0003 -- 1.8542)  max mem: 16413
Epoch: [2]  [140/160]  eta: 0:00:17  lr: 0.000013  min_lr: 0.000000  loss: 2.7154 (2.7386)  loss_scale: 1024.0000 (791.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5078 (1.5494)  time: 0.7636 (0.5346 -- 2.7985)  data: 0.1430 (0.0004 -- 2.0646)  max mem: 16413
Epoch: [2]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 2.7096 (2.7350)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5448 (1.5532)  time: 0.6888 (0.4930 -- 3.2752)  data: 0.1668 (0.0002 -- 2.7518)  max mem: 16413
Epoch: [2] Total time: 0:02:19 (0.8695 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 2.7096 (2.7355)  loss_scale: 1024.0000 (819.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5448 (1.5532)
Val:  [ 0/27]  eta: 0:01:04  loss: 2.6361 (2.6361)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.4069 (2.4069 -- 2.4069)  data: 2.1938 (2.1938 -- 2.1938)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.6589 (2.6560)  acc1: 22.2222 (32.3232)  acc5: 100.0000 (88.8889)  time: 0.4122 (0.1993 -- 2.4069)  data: 0.2005 (0.0005 -- 2.1938)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.6434 (2.6488)  acc1: 33.3333 (35.9788)  acc5: 100.0000 (89.9471)  time: 0.2101 (0.1701 -- 0.2607)  data: 0.0055 (0.0001 -- 0.0749)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.6497 (2.6529)  acc1: 33.3333 (36.9295)  acc5: 85.7143 (87.5519)  time: 0.1949 (0.1328 -- 0.2607)  data: 0.0053 (0.0001 -- 0.0749)  max mem: 16413
Val: Total time: 0:00:07 (0.2812 s / it)
* Acc@1 39.212 Acc@5 86.722 loss 2.652
Accuracy of the network on the 482 val images: 39.21%
Max accuracy: 43.15%
Epoch: [3]  [  0/160]  eta: 0:23:39  lr: 0.000014  min_lr: 0.000000  loss: 2.7099 (2.7099)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9418 (1.9418)  time: 8.8711 (8.8711 -- 8.8711)  data: 4.6535 (4.6535 -- 4.6535)  max mem: 16413
Epoch: [3]  [ 20/160]  eta: 0:02:48  lr: 0.000015  min_lr: 0.000000  loss: 2.7162 (2.7093)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6227 (1.6525)  time: 0.8192 (0.5150 -- 3.7232)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
[2023-08-30 13:21:29,040] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:21:29,040] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:21:29,041] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
[2023-08-30 13:21:29,041] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 1024 to 2048
Epoch: [3]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000000  loss: 2.6960 (2.7022)  loss_scale: 1024.0000 (1248.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.5839 (1.6180)  time: 0.8851 (0.5315 -- 3.9036)  data: 0.1371 (0.0004 -- 2.1886)  max mem: 16413
Epoch: [3]  [ 60/160]  eta: 0:01:35  lr: 0.000016  min_lr: 0.000000  loss: 2.6937 (2.6945)  loss_scale: 2048.0000 (1510.8197)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6071 (1.6149)  time: 0.7600 (0.5281 -- 2.8231)  data: 0.2070 (0.0003 -- 2.2965)  max mem: 16413
Epoch: [3]  [ 80/160]  eta: 0:01:15  lr: 0.000016  min_lr: 0.000000  loss: 2.6693 (2.6889)  loss_scale: 2048.0000 (1643.4568)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7420 (1.6454)  time: 0.9315 (0.5228 -- 3.6193)  data: 0.1306 (0.0003 -- 1.5976)  max mem: 16413
Epoch: [3]  [100/160]  eta: 0:00:54  lr: 0.000017  min_lr: 0.000000  loss: 2.6833 (2.6863)  loss_scale: 2048.0000 (1723.5644)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.6663 (1.6466)  time: 0.7354 (0.5285 -- 3.1960)  data: 0.0018 (0.0002 -- 0.0098)  max mem: 16413
Epoch: [3]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 2.6492 (2.6808)  loss_scale: 2048.0000 (1777.1901)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7757 (1.6723)  time: 1.0245 (0.5235 -- 5.6945)  data: 0.3978 (0.0006 -- 5.1918)  max mem: 16413
Epoch: [3]  [140/160]  eta: 0:00:17  lr: 0.000018  min_lr: 0.000000  loss: 2.6597 (2.6759)  loss_scale: 2048.0000 (1815.6028)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7432 (1.6841)  time: 0.6692 (0.5268 -- 1.8418)  data: 0.1238 (0.0002 -- 1.2863)  max mem: 16413
Epoch: [3]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.6423 (2.6715)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7562 (1.6941)  time: 0.7100 (0.4955 -- 4.1689)  data: 0.1866 (0.0002 -- 3.6399)  max mem: 16413
Epoch: [3] Total time: 0:02:19 (0.8694 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.6423 (2.6707)  loss_scale: 2048.0000 (1843.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.7562 (1.6941)
Val:  [ 0/27]  eta: 0:01:03  loss: 2.4603 (2.4603)  acc1: 11.1111 (11.1111)  acc5: 100.0000 (100.0000)  time: 2.3518 (2.3518 -- 2.3518)  data: 2.1244 (2.1244 -- 2.1244)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.5336 (2.5137)  acc1: 33.3333 (37.3737)  acc5: 100.0000 (89.8990)  time: 0.4313 (0.2006 -- 2.3518)  data: 0.2106 (0.0004 -- 2.1244)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.4874 (2.4997)  acc1: 33.3333 (38.6243)  acc5: 88.8889 (91.0053)  time: 0.2161 (0.1702 -- 0.4426)  data: 0.0098 (0.0001 -- 0.1700)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.4941 (2.5094)  acc1: 33.3333 (36.9295)  acc5: 88.8889 (87.1369)  time: 0.1986 (0.1332 -- 0.4426)  data: 0.0089 (0.0001 -- 0.1700)  max mem: 16413
Val: Total time: 0:00:07 (0.2838 s / it)
* Acc@1 39.627 Acc@5 87.552 loss 2.508
Accuracy of the network on the 482 val images: 39.63%
Max accuracy: 43.15%
[2023-08-30 13:23:27,937] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:23:27,937] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[2023-08-30 13:23:27,938] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:23:27,938] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 2048 to 4096
Epoch: [4]  [  0/160]  eta: 0:19:25  lr: 0.000019  min_lr: 0.000000  loss: 2.5660 (2.5660)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9344 (1.9344)  time: 7.2863 (7.2863 -- 7.2863)  data: 6.7368 (6.7368 -- 6.7368)  max mem: 16413
Epoch: [4]  [ 20/160]  eta: 0:02:39  lr: 0.000019  min_lr: 0.000000  loss: 2.6097 (2.6011)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8394 (1.8384)  time: 0.8305 (0.5264 -- 3.8277)  data: 0.2802 (0.0003 -- 3.2853)  max mem: 16413
Epoch: [4]  [ 40/160]  eta: 0:02:02  lr: 0.000020  min_lr: 0.000000  loss: 2.6058 (2.6009)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8788 (1.8621)  time: 0.8990 (0.5242 -- 3.4128)  data: 0.1574 (0.0003 -- 2.1857)  max mem: 16413
Epoch: [4]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000000  loss: 2.5851 (2.5911)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.8731 (1.9031)  time: 0.8682 (0.5233 -- 2.7077)  data: 0.0675 (0.0003 -- 1.1587)  max mem: 16413
Epoch: [4]  [ 80/160]  eta: 0:01:14  lr: 0.000021  min_lr: 0.000001  loss: 2.5898 (2.5892)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9235 (1.9672)  time: 0.8238 (0.5321 -- 2.6121)  data: 0.0981 (0.0003 -- 1.9305)  max mem: 16413
Epoch: [4]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.5931 (2.5866)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9870 (1.9883)  time: 0.8891 (0.5180 -- 2.8514)  data: 0.1548 (0.0003 -- 1.5911)  max mem: 16413
Epoch: [4]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.5529 (2.5807)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.1224 (2.0041)  time: 0.8875 (0.5111 -- 3.1876)  data: 0.1435 (0.0002 -- 1.9464)  max mem: 16413
[2023-08-30 13:25:18,750] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:25:18,751] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[2023-08-30 13:25:18,751] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:25:18,751] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096 to 8192
Epoch: [4]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.6066 (2.5839)  loss_scale: 8192.0000 (4473.6454)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2431 (2.0263)  time: 0.7932 (0.5175 -- 2.9655)  data: 0.2098 (0.0003 -- 2.4485)  max mem: 16413
Epoch: [4]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.5624 (2.5829)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9180 (2.0265)  time: 0.6637 (0.4957 -- 2.3243)  data: 0.1212 (0.0002 -- 1.7997)  max mem: 16413
Epoch: [4] Total time: 0:02:19 (0.8743 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.5624 (2.5884)  loss_scale: 8192.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9180 (2.0265)
Val:  [ 0/27]  eta: 0:00:56  loss: 2.2899 (2.2899)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.1027 (2.1027 -- 2.1027)  data: 1.8978 (1.8978 -- 1.8978)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 2.3604 (2.3536)  acc1: 44.4444 (40.4040)  acc5: 100.0000 (93.9394)  time: 0.4014 (0.1989 -- 2.1027)  data: 0.1843 (0.0007 -- 1.8978)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.3105 (2.3346)  acc1: 44.4444 (37.5661)  acc5: 100.0000 (95.2381)  time: 0.2260 (0.1694 -- 0.4554)  data: 0.0188 (0.0001 -- 0.2438)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.3105 (2.3498)  acc1: 33.3333 (36.5145)  acc5: 100.0000 (91.2863)  time: 0.2094 (0.1331 -- 0.4554)  data: 0.0185 (0.0001 -- 0.2438)  max mem: 16413
Val: Total time: 0:00:07 (0.2821 s / it)
* Acc@1 40.664 Acc@5 91.079 loss 2.348
Accuracy of the network on the 482 val images: 40.66%
Max accuracy: 43.15%
Epoch: [5]  [  0/160]  eta: 0:20:05  lr: 0.000023  min_lr: 0.000001  loss: 2.4381 (2.4381)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 1.9857 (1.9857)  time: 7.5358 (7.5358 -- 7.5358)  data: 4.9916 (4.9916 -- 4.9916)  max mem: 16413
Epoch: [5]  [ 20/160]  eta: 0:02:34  lr: 0.000023  min_lr: 0.000001  loss: 2.5408 (2.5320)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2476 (2.2385)  time: 0.7812 (0.5268 -- 3.2009)  data: 0.2010 (0.0007 -- 2.6846)  max mem: 16413
Epoch: [5]  [ 40/160]  eta: 0:01:57  lr: 0.000023  min_lr: 0.000001  loss: 2.5075 (2.5200)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2302 (2.2981)  time: 0.8462 (0.5243 -- 2.2604)  data: 0.1132 (0.0005 -- 1.3188)  max mem: 16413
Epoch: [5]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 2.5367 (2.5206)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2604 (2.3025)  time: 0.9038 (0.5233 -- 3.1865)  data: 0.0019 (0.0004 -- 0.0099)  max mem: 16413
Epoch: [5]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.5259 (2.5139)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.2150 (2.3128)  time: 0.9106 (0.5190 -- 2.6678)  data: 0.0013 (0.0003 -- 0.0038)  max mem: 16413
[2023-08-30 13:27:18,187] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:27:18,187] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:27:18,188] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[2023-08-30 13:27:18,188] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192 to 16384
Epoch: [5]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.4974 (2.5112)  loss_scale: 8192.0000 (8597.5446)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4007 (2.3404)  time: 0.9032 (0.5273 -- 2.9291)  data: 0.0012 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [5]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.4674 (2.5080)  loss_scale: 16384.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4325 (2.4040)  time: 0.7829 (0.5264 -- 3.1036)  data: 0.0020 (0.0003 -- 0.0163)  max mem: 16413
Epoch: [5]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.5174 (2.5072)  loss_scale: 16384.0000 (10806.4681)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.4565 (2.4454)  time: 0.7844 (0.5244 -- 1.9059)  data: 0.0018 (0.0003 -- 0.0074)  max mem: 16413
Epoch: [5]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.4380 (2.4983)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7055 (2.5054)  time: 0.7588 (0.4969 -- 2.9642)  data: 0.0008 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [5] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.4380 (2.5005)  loss_scale: 16384.0000 (11468.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7055 (2.5054)
Val:  [ 0/27]  eta: 0:01:07  loss: 2.1362 (2.1362)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4880 (2.4880 -- 2.4880)  data: 2.2561 (2.2561 -- 2.2561)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.2068 (2.1928)  acc1: 44.4444 (42.4242)  acc5: 100.0000 (93.9394)  time: 0.4464 (0.1938 -- 2.4880)  data: 0.2339 (0.0006 -- 2.2561)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.1630 (2.1711)  acc1: 44.4444 (41.2698)  acc5: 100.0000 (94.7090)  time: 0.2176 (0.1691 -- 0.5143)  data: 0.0160 (0.0001 -- 0.2925)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.1630 (2.1888)  acc1: 33.3333 (39.4191)  acc5: 88.8889 (92.9461)  time: 0.2022 (0.1329 -- 0.5143)  data: 0.0155 (0.0001 -- 0.2925)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 42.946 Acc@5 91.909 loss 2.184
Accuracy of the network on the 482 val images: 42.95%
Max accuracy: 43.15%
Epoch: [6]  [  0/160]  eta: 0:17:31  lr: 0.000023  min_lr: 0.000001  loss: 2.5791 (2.5791)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.3411 (2.3411)  time: 6.5728 (6.5728 -- 6.5728)  data: 5.4163 (5.4163 -- 5.4163)  max mem: 16413
Epoch: [6]  [ 20/160]  eta: 0:02:41  lr: 0.000023  min_lr: 0.000001  loss: 2.5137 (2.5218)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.6837 (2.8178)  time: 0.8843 (0.5285 -- 3.0100)  data: 0.0619 (0.0008 -- 0.6479)  max mem: 16413
[2023-08-30 13:28:54,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[5.567550977443171e-07, 5.567550977443171e-07, 7.423401303257562e-07, 7.423401303257562e-07, 9.897868404343417e-07, 9.897868404343417e-07, 1.3197157872457889e-06, 1.3197157872457889e-06, 1.759621049661052e-06, 1.759621049661052e-06, 2.346161399548069e-06, 2.346161399548069e-06, 3.1282151993974255e-06, 3.1282151993974255e-06, 4.170953599196567e-06, 4.170953599196567e-06, 5.561271465595423e-06, 5.561271465595423e-06, 7.415028620793898e-06, 7.415028620793898e-06, 9.886704827725196e-06, 9.886704827725196e-06, 1.3182273103633595e-05, 1.3182273103633595e-05, 1.757636413817813e-05, 1.757636413817813e-05, 2.3435152184237504e-05, 2.3435152184237504e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 13:28:54,867] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=17.200482473834402, CurrSamplesPerSec=21.95517591701189, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [6]  [ 40/160]  eta: 0:01:56  lr: 0.000023  min_lr: 0.000001  loss: 2.4976 (2.4976)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2475 (3.1777)  time: 0.7759 (0.5214 -- 1.7872)  data: 0.1118 (0.0001 -- 0.9261)  max mem: 16413
Epoch: [6]  [ 60/160]  eta: 0:01:32  lr: 0.000023  min_lr: 0.000001  loss: 2.4348 (2.4717)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7800 (3.1540)  time: 0.8392 (0.5236 -- 3.1431)  data: 0.2780 (0.0008 -- 2.6099)  max mem: 16413
[2023-08-30 13:29:17,299] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:29:17,299] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:29:17,299] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
[2023-08-30 13:29:17,299] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384 to 32768
Epoch: [6]  [ 80/160]  eta: 0:01:13  lr: 0.000023  min_lr: 0.000001  loss: 2.4717 (2.4636)  loss_scale: 32768.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.7446 (3.1652)  time: 0.8899 (0.5378 -- 3.2736)  data: 0.1513 (0.0005 -- 1.9760)  max mem: 16413
Epoch: [6]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.4269 (2.4574)  loss_scale: 32768.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2082 (3.2384)  time: 0.8778 (0.5200 -- 3.7307)  data: 0.0017 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [6]  [120/160]  eta: 0:00:35  lr: 0.000023  min_lr: 0.000001  loss: 2.3928 (2.4490)  loss_scale: 32768.0000 (24102.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1362 (3.2483)  time: 0.8091 (0.5218 -- 2.4311)  data: 0.0015 (0.0004 -- 0.0041)  max mem: 16413
Epoch: [6]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.4213 (2.4403)  loss_scale: 32768.0000 (25331.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2127 (3.3019)  time: 0.8724 (0.5172 -- 3.4623)  data: 0.0013 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [6]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.3815 (2.4379)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0638 (3.2845)  time: 0.7187 (0.4958 -- 4.2148)  data: 0.0123 (0.0001 -- 0.2300)  max mem: 16413
Epoch: [6] Total time: 0:02:19 (0.8711 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.3815 (2.4364)  loss_scale: 32768.0000 (26214.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0638 (3.2845)
Val:  [ 0/27]  eta: 0:00:58  loss: 2.0090 (2.0090)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.1745 (2.1745 -- 2.1745)  data: 1.9660 (1.9660 -- 1.9660)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 2.0545 (2.0660)  acc1: 44.4444 (42.4242)  acc5: 100.0000 (93.9394)  time: 0.4163 (0.1971 -- 2.1745)  data: 0.1933 (0.0008 -- 1.9660)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 2.0110 (2.0344)  acc1: 44.4444 (40.7407)  acc5: 100.0000 (94.7090)  time: 0.2204 (0.1702 -- 0.3773)  data: 0.0120 (0.0001 -- 0.1027)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 2.0545 (2.0602)  acc1: 44.4444 (39.0042)  acc5: 88.8889 (92.9461)  time: 0.2038 (0.1342 -- 0.3773)  data: 0.0116 (0.0001 -- 0.1027)  max mem: 16413
Val: Total time: 0:00:07 (0.2807 s / it)
* Acc@1 44.606 Acc@5 91.494 loss 2.049
Accuracy of the network on the 482 val images: 44.61%
[2023-08-30 13:30:43,496] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 13:30:43,498] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 13:30:43,498] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 13:30:43,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 13:30:44,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 13:30:44,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 44.61%
Epoch: [7]  [  0/160]  eta: 0:19:07  lr: 0.000023  min_lr: 0.000001  loss: 2.2857 (2.2857)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9530 (3.9530)  time: 7.1703 (7.1703 -- 7.1703)  data: 6.6192 (6.6192 -- 6.6192)  max mem: 16413
Epoch: [7]  [ 20/160]  eta: 0:02:37  lr: 0.000023  min_lr: 0.000001  loss: 2.3618 (2.3644)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0425 (3.2411)  time: 0.8240 (0.5163 -- 2.6467)  data: 0.2342 (0.0004 -- 2.1145)  max mem: 16413
[2023-08-30 13:31:17,932] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:31:17,932] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
[2023-08-30 13:31:17,933] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:31:17,933] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768 to 65536
Epoch: [7]  [ 40/160]  eta: 0:01:57  lr: 0.000023  min_lr: 0.000001  loss: 2.3583 (2.3764)  loss_scale: 32768.0000 (39960.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1049 (3.2098)  time: 0.8297 (0.5373 -- 2.1117)  data: 0.2762 (0.0002 -- 1.5651)  max mem: 16413
[2023-08-30 13:31:38,058] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1173
[2023-08-30 13:31:38,059] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1173
[2023-08-30 13:31:38,059] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-08-30 13:31:38,059] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
[2023-08-30 13:31:38,059] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
Epoch: [7]  [ 60/160]  eta: 0:01:33  lr: 0.000023  min_lr: 0.000001  loss: 2.4380 (2.3924)  loss_scale: 65536.0000 (44048.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2222 (3.2771)  time: 0.8456 (0.5282 -- 3.0141)  data: 0.2579 (0.0006 -- 2.5100)  max mem: 16413
Epoch: [7]  [ 80/160]  eta: 0:01:13  lr: 0.000023  min_lr: 0.000001  loss: 2.3981 (2.3963)  loss_scale: 32768.0000 (41263.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3097 (3.3123)  time: 0.8631 (0.5330 -- 3.6958)  data: 0.3091 (0.0004 -- 3.1768)  max mem: 16413
[2023-08-30 13:32:05,896] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1206
[2023-08-30 13:32:05,896] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1206
[2023-08-30 13:32:05,896] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:32:05,896] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:32:05,896] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [7]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.3451 (2.3861)  loss_scale: 16384.0000 (37147.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0828 (3.3267)  time: 0.8367 (0.5302 -- 3.4878)  data: 0.2693 (0.0005 -- 2.9535)  max mem: 16413
Epoch: [7]  [120/160]  eta: 0:00:35  lr: 0.000023  min_lr: 0.000001  loss: 2.3625 (2.3859)  loss_scale: 16384.0000 (33715.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2430 (3.3247)  time: 0.8629 (0.5238 -- 2.5290)  data: 0.1425 (0.0003 -- 1.9820)  max mem: 16413
Epoch: [7]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.3393 (2.3816)  loss_scale: 16384.0000 (31257.4184)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8926 (3.3814)  time: 1.0424 (0.5185 -- 4.2514)  data: 0.0012 (0.0003 -- 0.0029)  max mem: 16413
Epoch: [7]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.3682 (2.3809)  loss_scale: 16384.0000 (29491.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5338 (3.4014)  time: 0.5175 (0.4951 -- 0.5828)  data: 0.0005 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [7] Total time: 0:02:19 (0.8700 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.3682 (2.3858)  loss_scale: 16384.0000 (29491.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5338 (3.4014)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.9097 (1.9097)  acc1: 22.2222 (22.2222)  acc5: 100.0000 (100.0000)  time: 2.4444 (2.4444 -- 2.4444)  data: 2.1583 (2.1583 -- 2.1583)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.9160 (1.9573)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (93.9394)  time: 0.4363 (0.1979 -- 2.4444)  data: 0.2047 (0.0008 -- 2.1583)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.8747 (1.9279)  acc1: 33.3333 (41.2698)  acc5: 100.0000 (94.7090)  time: 0.2162 (0.1698 -- 0.3217)  data: 0.0051 (0.0001 -- 0.0757)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.9349 (1.9568)  acc1: 33.3333 (38.5892)  acc5: 100.0000 (94.1909)  time: 0.1989 (0.1324 -- 0.3217)  data: 0.0044 (0.0001 -- 0.0757)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 44.398 Acc@5 92.739 loss 1.945
Accuracy of the network on the 482 val images: 44.40%
Max accuracy: 44.61%
Epoch: [8]  [  0/160]  eta: 0:24:34  lr: 0.000023  min_lr: 0.000001  loss: 2.4122 (2.4122)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5017 (5.5017)  time: 9.2157 (9.2157 -- 9.2157)  data: 5.6629 (5.6629 -- 5.6629)  max mem: 16413
Epoch: [8]  [ 20/160]  eta: 0:02:39  lr: 0.000023  min_lr: 0.000001  loss: 2.3408 (2.3718)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2985 (3.6267)  time: 0.7327 (0.5230 -- 2.5710)  data: 0.0048 (0.0002 -- 0.0723)  max mem: 16413
Epoch: [8]  [ 40/160]  eta: 0:02:08  lr: 0.000023  min_lr: 0.000001  loss: 2.2837 (2.3438)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9686 (3.9085)  time: 1.0064 (0.5198 -- 3.6984)  data: 0.1270 (0.0002 -- 1.2702)  max mem: 16413
[2023-08-30 13:34:05,023] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:34:05,023] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:34:05,024] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:34:05,024] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [8]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 2.3170 (2.3324)  loss_scale: 16384.0000 (17995.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8802 (3.9300)  time: 0.7500 (0.5229 -- 2.3156)  data: 0.0038 (0.0004 -- 0.0514)  max mem: 16413
Epoch: [8]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.3139 (2.3377)  loss_scale: 32768.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8281 (3.9557)  time: 0.8324 (0.5229 -- 3.1700)  data: 0.0096 (0.0003 -- 0.1186)  max mem: 16413
Epoch: [8]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.4321 (2.3474)  loss_scale: 32768.0000 (23846.0198)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7811 (3.9808)  time: 0.8123 (0.5358 -- 3.1215)  data: 0.0396 (0.0002 -- 0.4636)  max mem: 16413
Epoch: [8]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.3527 (2.3488)  loss_scale: 32768.0000 (25320.7273)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8604 (4.0132)  time: 0.9406 (0.5253 -- 3.2660)  data: 0.3863 (0.0007 -- 2.7305)  max mem: 16413
Epoch: [8]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.4616 (2.3595)  loss_scale: 32768.0000 (26377.0780)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9924 (4.2271)  time: 0.7798 (0.5446 -- 2.4378)  data: 0.1709 (0.0002 -- 1.9122)  max mem: 16413
Epoch: [8]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.3708 (2.3606)  loss_scale: 32768.0000 (27136.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8710 (4.2454)  time: 0.7288 (0.4970 -- 2.6826)  data: 0.2104 (0.0002 -- 2.1801)  max mem: 16413
Epoch: [8] Total time: 0:02:20 (0.8770 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.3708 (2.3526)  loss_scale: 32768.0000 (27136.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8710 (4.2454)
Val:  [ 0/27]  eta: 0:00:57  loss: 1.8379 (1.8379)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.1369 (2.1369 -- 2.1369)  data: 1.9201 (1.9201 -- 1.9201)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.8729 (1.8666)  acc1: 55.5556 (48.4848)  acc5: 100.0000 (93.9394)  time: 0.3903 (0.1947 -- 2.1369)  data: 0.1770 (0.0003 -- 1.9201)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.7759 (1.8243)  acc1: 55.5556 (47.6190)  acc5: 100.0000 (95.2381)  time: 0.2242 (0.1698 -- 0.5133)  data: 0.0181 (0.0001 -- 0.2941)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.8826 (1.8578)  acc1: 55.5556 (48.1328)  acc5: 100.0000 (94.1909)  time: 0.2110 (0.1334 -- 0.5133)  data: 0.0179 (0.0001 -- 0.2941)  max mem: 16413
Val: Total time: 0:00:07 (0.2820 s / it)
* Acc@1 52.905 Acc@5 92.739 loss 1.844
Accuracy of the network on the 482 val images: 52.90%
[2023-08-30 13:35:39,784] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 13:35:39,785] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 13:35:39,786] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 13:35:39,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 13:35:41,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 13:35:41,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 52.90%
Epoch: [9]  [  0/160]  eta: 0:27:11  lr: 0.000023  min_lr: 0.000001  loss: 2.2831 (2.2831)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2592 (3.2592)  time: 10.1997 (10.1997 -- 10.1997)  data: 9.6577 (9.6577 -- 9.6577)  max mem: 16413
Epoch: [9]  [ 20/160]  eta: 0:02:51  lr: 0.000023  min_lr: 0.000001  loss: 2.3014 (2.3040)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9312 (4.3858)  time: 0.7743 (0.5317 -- 2.7513)  data: 0.2184 (0.0004 -- 2.2218)  max mem: 16413
[2023-08-30 13:36:08,476] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:36:08,476] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
[2023-08-30 13:36:08,479] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:36:08,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
Epoch: [9]  [ 40/160]  eta: 0:01:59  lr: 0.000023  min_lr: 0.000001  loss: 2.3142 (2.3013)  loss_scale: 65536.0000 (47153.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3930 (4.6201)  time: 0.7613 (0.5285 -- 2.7226)  data: 0.2151 (0.0003 -- 2.1616)  max mem: 16413
Epoch: [9]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 2.2704 (2.3128)  loss_scale: 65536.0000 (53180.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0699 (4.4453)  time: 0.8822 (0.5272 -- 2.6656)  data: 0.0958 (0.0004 -- 1.8902)  max mem: 16413
Epoch: [9]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.3654 (2.3118)  loss_scale: 65536.0000 (56231.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4192 (4.4524)  time: 0.9068 (0.5235 -- 2.4768)  data: 0.2409 (0.0003 -- 1.9338)  max mem: 16413
Epoch: [9]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.3521 (2.3166)  loss_scale: 65536.0000 (58073.9802)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4123 (4.5237)  time: 0.8207 (0.5253 -- 3.2865)  data: 0.2541 (0.0003 -- 2.7271)  max mem: 16413
Epoch: [9]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.2948 (2.3122)  loss_scale: 65536.0000 (59307.3719)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5797 (4.5606)  time: 0.9358 (0.5187 -- 2.9231)  data: 0.2706 (0.0005 -- 2.4124)  max mem: 16413
[2023-08-30 13:37:38,990] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1568
[2023-08-30 13:37:38,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1568
[2023-08-30 13:37:39,021] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 13:37:39,021] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
[2023-08-30 13:37:39,021] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
Epoch: [9]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.1806 (2.2978)  loss_scale: 32768.0000 (57169.7021)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8435 (4.5525)  time: 0.7194 (0.5244 -- 2.0935)  data: 0.1101 (0.0004 -- 1.5544)  max mem: 16413
Epoch: [9]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.2063 (2.2929)  loss_scale: 32768.0000 (54272.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3851 (4.5197)  time: 0.7197 (0.4971 -- 2.6859)  data: 0.1605 (0.0002 -- 2.1484)  max mem: 16413
Epoch: [9] Total time: 0:02:20 (0.8755 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.2063 (2.2936)  loss_scale: 32768.0000 (54272.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3851 (4.5197)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.7386 (1.7386)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.3912 (2.3912 -- 2.3912)  data: 2.1284 (2.1284 -- 2.1284)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.7386 (1.7775)  acc1: 44.4444 (45.4545)  acc5: 100.0000 (94.9495)  time: 0.4154 (0.1911 -- 2.3912)  data: 0.1978 (0.0006 -- 2.1284)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6924 (1.7343)  acc1: 44.4444 (44.9735)  acc5: 100.0000 (95.7672)  time: 0.2155 (0.1685 -- 0.3214)  data: 0.0119 (0.0001 -- 0.1129)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.7784 (1.7713)  acc1: 55.5556 (46.0581)  acc5: 100.0000 (94.6058)  time: 0.2000 (0.1328 -- 0.3214)  data: 0.0109 (0.0001 -- 0.1129)  max mem: 16413
Val: Total time: 0:00:07 (0.2847 s / it)
* Acc@1 51.245 Acc@5 92.739 loss 1.756
Accuracy of the network on the 482 val images: 51.24%
Max accuracy: 52.90%
Epoch: [10]  [  0/160]  eta: 0:21:38  lr: 0.000023  min_lr: 0.000001  loss: 2.4528 (2.4528)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8145 (4.8145)  time: 8.1167 (8.1167 -- 8.1167)  data: 7.1555 (7.1555 -- 7.1555)  max mem: 16413
Epoch: [10]  [ 20/160]  eta: 0:02:58  lr: 0.000023  min_lr: 0.000001  loss: 2.1810 (2.2370)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3114 (4.4170)  time: 0.9343 (0.5047 -- 4.9197)  data: 0.1205 (0.0002 -- 1.7410)  max mem: 16413
[2023-08-30 13:38:40,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1627
[2023-08-30 13:38:40,944] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1627
[2023-08-30 13:38:40,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:38:40,944] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:38:40,945] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [10]  [ 40/160]  eta: 0:02:06  lr: 0.000023  min_lr: 0.000001  loss: 2.3301 (2.2779)  loss_scale: 16384.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6709 (4.5621)  time: 0.8248 (0.5144 -- 3.5265)  data: 0.0015 (0.0001 -- 0.0069)  max mem: 16413
Epoch: [10]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 2.1092 (2.2264)  loss_scale: 16384.0000 (23635.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5889 (4.7427)  time: 0.7609 (0.5261 -- 2.7228)  data: 0.0018 (0.0007 -- 0.0073)  max mem: 16413
Epoch: [10]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000001  loss: 2.1241 (2.2060)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2615 (4.7296)  time: 0.9751 (0.5265 -- 4.6191)  data: 0.0297 (0.0002 -- 0.5490)  max mem: 16413
Epoch: [10]  [100/160]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000001  loss: 2.3352 (2.2297)  loss_scale: 16384.0000 (20763.8812)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8470 (4.8160)  time: 0.8581 (0.5246 -- 3.7291)  data: 0.0017 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [10]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1874 (2.2210)  loss_scale: 16384.0000 (20039.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4538 (4.8618)  time: 0.8013 (0.5211 -- 2.4101)  data: 0.0013 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [10]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.2457 (2.2218)  loss_scale: 16384.0000 (19521.3617)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0977 (4.8222)  time: 0.7989 (0.5259 -- 4.3504)  data: 0.0009 (0.0003 -- 0.0017)  max mem: 16413
[2023-08-30 13:40:27,656] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:40:27,656] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:40:27,656] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:40:27,656] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [10]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.2326 (2.2310)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2106 (4.8907)  time: 0.6778 (0.4949 -- 3.6800)  data: 0.0006 (0.0001 -- 0.0025)  max mem: 16413
Epoch: [10] Total time: 0:02:20 (0.8765 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.2326 (2.2548)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2106 (4.8907)
Val:  [ 0/27]  eta: 0:00:58  loss: 1.6813 (1.6813)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.1808 (2.1808 -- 2.1808)  data: 1.9569 (1.9569 -- 1.9569)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.6990 (1.7072)  acc1: 55.5556 (48.4848)  acc5: 100.0000 (95.9596)  time: 0.3996 (0.2111 -- 2.1808)  data: 0.1791 (0.0005 -- 1.9569)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.6477 (1.6637)  acc1: 55.5556 (51.3228)  acc5: 100.0000 (96.2963)  time: 0.2170 (0.1709 -- 0.2973)  data: 0.0051 (0.0001 -- 0.0864)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6942 (1.6987)  acc1: 55.5556 (52.6971)  acc5: 100.0000 (95.4357)  time: 0.1999 (0.1349 -- 0.2973)  data: 0.0048 (0.0001 -- 0.0864)  max mem: 16413
Val: Total time: 0:00:07 (0.2788 s / it)
* Acc@1 55.394 Acc@5 93.154 loss 1.687
Accuracy of the network on the 482 val images: 55.39%
[2023-08-30 13:40:36,931] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 13:40:36,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 13:40:36,933] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 13:40:36,933] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 13:40:38,354] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 13:40:38,355] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 55.39%
Epoch: [11]  [  0/160]  eta: 0:22:24  lr: 0.000023  min_lr: 0.000001  loss: 2.3899 (2.3899)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2657 (6.2657)  time: 8.4052 (8.4052 -- 8.4052)  data: 7.8546 (7.8546 -- 7.8546)  max mem: 16413
[2023-08-30 13:40:54,830] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1768
[2023-08-30 13:40:54,830] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1768
[2023-08-30 13:40:54,830] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:40:54,830] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:40:54,830] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [ 20/160]  eta: 0:02:44  lr: 0.000023  min_lr: 0.000001  loss: 2.3412 (2.3431)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1466 (5.6667)  time: 0.8162 (0.5206 -- 4.1708)  data: 0.1727 (0.0004 -- 1.7105)  max mem: 16413
Epoch: [11]  [ 40/160]  eta: 0:02:02  lr: 0.000023  min_lr: 0.000001  loss: 2.2589 (2.3098)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0442 (5.4510)  time: 0.8552 (0.5402 -- 2.8541)  data: 0.2228 (0.0005 -- 1.0672)  max mem: 16413
Epoch: [11]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1915 (2.2858)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0753 (5.4111)  time: 0.8517 (0.5236 -- 2.9511)  data: 0.0710 (0.0010 -- 1.2939)  max mem: 16413
Epoch: [11]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.2323 (2.2731)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5415 (5.7421)  time: 0.8316 (0.5156 -- 2.5810)  data: 0.0776 (0.0003 -- 0.9176)  max mem: 16413
Epoch: [11]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.2007 (2.2556)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9006 (5.7256)  time: 0.9279 (0.5259 -- 2.7973)  data: 0.0015 (0.0003 -- 0.0054)  max mem: 16413
Epoch: [11]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1367 (2.2420)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6392 (5.7260)  time: 0.8706 (0.5133 -- 3.1629)  data: 0.0767 (0.0002 -- 1.0008)  max mem: 16413
[2023-08-30 13:42:42,867] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:42:42,867] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:42:42,867] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:42:42,867] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [11]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1414 (2.2256)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6009 (5.7031)  time: 0.8824 (0.5418 -- 2.9709)  data: 0.1216 (0.0004 -- 1.3684)  max mem: 16413
[2023-08-30 13:42:48,595] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1902
[2023-08-30 13:42:48,595] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 1902
[2023-08-30 13:42:48,595] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:42:48,595] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:42:48,595] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [11]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1768 (2.2176)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4398 (5.6151)  time: 0.6667 (0.4977 -- 2.1841)  data: 0.0685 (0.0002 -- 0.6465)  max mem: 16413
Epoch: [11] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1768 (2.2011)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4398 (5.6151)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.6088 (1.6088)  acc1: 33.3333 (33.3333)  acc5: 100.0000 (100.0000)  time: 2.2766 (2.2766 -- 2.2766)  data: 2.0354 (2.0354 -- 2.0354)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5666 (1.6236)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (96.9697)  time: 0.4251 (0.2045 -- 2.2766)  data: 0.1995 (0.0006 -- 2.0354)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.5618 (1.5839)  acc1: 55.5556 (51.8519)  acc5: 100.0000 (96.8254)  time: 0.2283 (0.1690 -- 0.4224)  data: 0.0203 (0.0001 -- 0.2442)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.6062 (1.6215)  acc1: 55.5556 (52.6971)  acc5: 100.0000 (96.2656)  time: 0.2106 (0.1327 -- 0.4224)  data: 0.0200 (0.0001 -- 0.2442)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 56.224 Acc@5 93.983 loss 1.611
Accuracy of the network on the 482 val images: 56.22%
[2023-08-30 13:43:07,602] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 13:43:07,604] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 13:43:07,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 13:43:07,604] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 13:43:08,985] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 13:43:08,986] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 56.22%
Epoch: [12]  [  0/160]  eta: 0:22:43  lr: 0.000023  min_lr: 0.000001  loss: 1.6885 (1.6885)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7573 (6.7573)  time: 8.5230 (8.5230 -- 8.5230)  data: 4.5388 (4.5388 -- 4.5388)  max mem: 16413
Epoch: [12]  [ 20/160]  eta: 0:02:44  lr: 0.000023  min_lr: 0.000001  loss: 2.0997 (2.0885)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9317 (5.3357)  time: 0.8039 (0.5230 -- 2.4724)  data: 0.1097 (0.0006 -- 1.9326)  max mem: 16413
Epoch: [12]  [ 40/160]  eta: 0:02:02  lr: 0.000023  min_lr: 0.000001  loss: 2.1258 (2.1251)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0057 (5.4667)  time: 0.8703 (0.5263 -- 3.5765)  data: 0.2439 (0.0002 -- 3.0532)  max mem: 16413
Epoch: [12]  [ 60/160]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000001  loss: 2.1374 (2.1342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0714 (5.5999)  time: 0.9321 (0.5240 -- 3.6935)  data: 0.3797 (0.0005 -- 3.1768)  max mem: 16413
[2023-08-30 13:44:24,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=6, lr=[5.547884169782988e-07, 5.547884169782988e-07, 7.397178893043984e-07, 7.397178893043984e-07, 9.862905190725312e-07, 9.862905190725312e-07, 1.3150540254300417e-06, 1.3150540254300417e-06, 1.7534053672400554e-06, 1.7534053672400554e-06, 2.3378738229867407e-06, 2.3378738229867407e-06, 3.1171650973156544e-06, 3.1171650973156544e-06, 4.1562201297542055e-06, 4.1562201297542055e-06, 5.541626839672274e-06, 5.541626839672274e-06, 7.388835786229699e-06, 7.388835786229699e-06, 9.851781048306264e-06, 9.851781048306264e-06, 1.3135708064408354e-05, 1.3135708064408354e-05, 1.7514277419211137e-05, 1.7514277419211137e-05, 2.3352369892281517e-05, 2.3352369892281517e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 13:44:24,625] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=17.18645287122697, CurrSamplesPerSec=22.70213189177486, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [12]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.1959 (2.1450)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9333 (5.7330)  time: 0.7770 (0.5259 -- 3.4255)  data: 0.0602 (0.0006 -- 1.1669)  max mem: 16413
Epoch: [12]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.3290 (2.1786)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7408 (5.6681)  time: 0.8956 (0.5143 -- 3.1662)  data: 0.0785 (0.0004 -- 1.0771)  max mem: 16413
[2023-08-30 13:44:50,288] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:44:50,288] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:44:50,288] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:44:50,289] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [12]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.2785 (2.2051)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1625 (5.7316)  time: 0.8238 (0.5407 -- 2.9623)  data: 0.2376 (0.0005 -- 2.4367)  max mem: 16413
Epoch: [12]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.0401 (2.1844)  loss_scale: 32768.0000 (19869.9574)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9206 (5.6388)  time: 0.7438 (0.5295 -- 2.1403)  data: 0.0914 (0.0004 -- 1.6169)  max mem: 16413
Epoch: [12]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.2738 (2.1855)  loss_scale: 32768.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4093 (5.6398)  time: 0.8180 (0.4992 -- 3.0434)  data: 0.1256 (0.0002 -- 2.4995)  max mem: 16413
Epoch: [12] Total time: 0:02:19 (0.8743 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.2738 (2.1960)  loss_scale: 32768.0000 (21401.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4093 (5.6398)
Val:  [ 0/27]  eta: 0:01:03  loss: 1.5469 (1.5469)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3391 (2.3391 -- 2.3391)  data: 2.1315 (2.1315 -- 2.1315)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.5239 (1.5651)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (95.9596)  time: 0.4322 (0.2126 -- 2.3391)  data: 0.2077 (0.0007 -- 2.1315)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4905 (1.5347)  acc1: 55.5556 (52.9101)  acc5: 100.0000 (96.2963)  time: 0.2215 (0.1694 -- 0.4173)  data: 0.0081 (0.0001 -- 0.1432)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5639 (1.5713)  acc1: 55.5556 (53.5270)  acc5: 100.0000 (95.8506)  time: 0.2039 (0.1332 -- 0.4173)  data: 0.0078 (0.0001 -- 0.1432)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 57.261 Acc@5 93.568 loss 1.558
Accuracy of the network on the 482 val images: 57.26%
[2023-08-30 13:45:36,646] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 13:45:36,647] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 13:45:36,648] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 13:45:36,648] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 13:45:38,152] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 13:45:38,153] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 57.26%
Epoch: [13]  [  0/160]  eta: 0:22:45  lr: 0.000023  min_lr: 0.000001  loss: 2.3107 (2.3107)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4359 (4.4359)  time: 8.5346 (8.5346 -- 8.5346)  data: 7.2855 (7.2855 -- 7.2855)  max mem: 16413
Epoch: [13]  [ 20/160]  eta: 0:02:57  lr: 0.000023  min_lr: 0.000001  loss: 2.1575 (2.2182)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3902 (5.5695)  time: 0.9076 (0.5176 -- 4.2278)  data: 0.0570 (0.0002 -- 1.1216)  max mem: 16413
[2023-08-30 13:46:16,261] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2113
[2023-08-30 13:46:16,261] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2113
[2023-08-30 13:46:16,261] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:46:16,261] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:46:16,262] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [13]  [ 40/160]  eta: 0:02:10  lr: 0.000023  min_lr: 0.000001  loss: 2.2525 (2.2278)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8525 (5.4099)  time: 0.8988 (0.5292 -- 3.4759)  data: 0.0015 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [13]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 2.2825 (2.2313)  loss_scale: 16384.0000 (25247.4754)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5319 (5.5228)  time: 0.6765 (0.5195 -- 2.5204)  data: 0.0017 (0.0003 -- 0.0056)  max mem: 16413
Epoch: [13]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.1846 (2.2213)  loss_scale: 16384.0000 (23058.9630)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1936 (5.5247)  time: 0.9756 (0.5228 -- 4.5480)  data: 0.0013 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [13]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.2422 (2.2126)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5801 (5.6091)  time: 0.7757 (0.5366 -- 3.6307)  data: 0.0014 (0.0001 -- 0.0040)  max mem: 16413
Epoch: [13]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1025 (2.1987)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3360 (5.6198)  time: 0.9172 (0.5292 -- 2.4968)  data: 0.0024 (0.0006 -- 0.0126)  max mem: 16413
Epoch: [13]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.0639 (2.1836)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5646 (5.6963)  time: 0.7647 (0.5395 -- 2.3895)  data: 0.0021 (0.0005 -- 0.0131)  max mem: 16413
Epoch: [13]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0951 (2.1736)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7000 (5.8072)  time: 0.6879 (0.4963 -- 2.1626)  data: 0.0007 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [13] Total time: 0:02:20 (0.8754 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0951 (2.1814)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7000 (5.8072)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.4972 (1.4972)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.2372 (2.2372 -- 2.2372)  data: 2.0188 (2.0188 -- 2.0188)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.4629 (1.5170)  acc1: 44.4444 (53.5354)  acc5: 100.0000 (96.9697)  time: 0.4002 (0.1826 -- 2.2372)  data: 0.1875 (0.0003 -- 2.0188)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.4232 (1.4794)  acc1: 55.5556 (56.0847)  acc5: 100.0000 (96.8254)  time: 0.2280 (0.1712 -- 0.5376)  data: 0.0196 (0.0001 -- 0.3445)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.5056 (1.5188)  acc1: 55.5556 (56.4315)  acc5: 100.0000 (96.2656)  time: 0.2128 (0.1336 -- 0.5376)  data: 0.0194 (0.0001 -- 0.3445)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 59.336 Acc@5 94.191 loss 1.501
Accuracy of the network on the 482 val images: 59.34%
[2023-08-30 13:48:06,004] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 13:48:06,006] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 13:48:06,006] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 13:48:06,006] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 13:48:07,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 13:48:07,475] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 59.34%
Epoch: [14]  [  0/160]  eta: 0:23:24  lr: 0.000023  min_lr: 0.000001  loss: 1.9903 (1.9903)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2088 (7.2088)  time: 8.7785 (8.7785 -- 8.7785)  data: 8.2234 (8.2234 -- 8.2234)  max mem: 16413
[2023-08-30 13:48:17,357] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:48:17,357] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:48:17,357] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:48:17,357] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [14]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 2.1778 (2.1375)  loss_scale: 32768.0000 (31207.6190)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4611 (6.1391)  time: 0.8012 (0.5345 -- 3.1985)  data: 0.2496 (0.0004 -- 2.6580)  max mem: 16413
[2023-08-30 13:48:48,462] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2280
[2023-08-30 13:48:48,462] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2280
[2023-08-30 13:48:48,462] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:48:48,462] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:48:48,463] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [14]  [ 40/160]  eta: 0:01:59  lr: 0.000023  min_lr: 0.000001  loss: 2.1401 (2.1406)  loss_scale: 32768.0000 (31569.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6483 (6.5612)  time: 0.8088 (0.5284 -- 3.1631)  data: 0.2627 (0.0002 -- 2.6325)  max mem: 16413
Epoch: [14]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 2.1327 (2.1479)  loss_scale: 16384.0000 (26590.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7566 (6.2780)  time: 0.8547 (0.5322 -- 2.8017)  data: 0.3051 (0.0005 -- 2.2613)  max mem: 16413
Epoch: [14]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.1274 (2.1403)  loss_scale: 16384.0000 (24070.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4064 (6.1725)  time: 0.8625 (0.5180 -- 2.2953)  data: 0.2296 (0.0005 -- 1.7418)  max mem: 16413
Epoch: [14]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.1892 (2.1471)  loss_scale: 16384.0000 (22548.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8855 (6.0536)  time: 0.8091 (0.5404 -- 2.8487)  data: 0.0505 (0.0002 -- 0.6653)  max mem: 16413
Epoch: [14]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9157 (2.1278)  loss_scale: 16384.0000 (21529.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6851 (6.0609)  time: 0.9972 (0.5241 -- 2.2724)  data: 0.2166 (0.0003 -- 1.5000)  max mem: 16413
Epoch: [14]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.2124 (2.1415)  loss_scale: 16384.0000 (20799.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0507 (6.3266)  time: 0.7817 (0.5279 -- 1.8180)  data: 0.1313 (0.0002 -- 1.2736)  max mem: 16413
Epoch: [14]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1476 (2.1406)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8932 (6.3941)  time: 0.7545 (0.4975 -- 2.3550)  data: 0.1290 (0.0002 -- 1.3676)  max mem: 16413
Epoch: [14] Total time: 0:02:20 (0.8777 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1476 (2.1598)  loss_scale: 16384.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8932 (6.3941)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.4329 (1.4329)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.3299 (2.3299 -- 2.3299)  data: 2.0963 (2.0963 -- 2.0963)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3517 (1.4696)  acc1: 44.4444 (51.5152)  acc5: 100.0000 (95.9596)  time: 0.4331 (0.2010 -- 2.3299)  data: 0.2172 (0.0003 -- 2.0963)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3495 (1.4357)  acc1: 55.5556 (53.4392)  acc5: 100.0000 (96.2963)  time: 0.2240 (0.1694 -- 0.5070)  data: 0.0192 (0.0001 -- 0.2851)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4619 (1.4826)  acc1: 55.5556 (53.5270)  acc5: 100.0000 (95.4357)  time: 0.2097 (0.1328 -- 0.5070)  data: 0.0190 (0.0001 -- 0.2851)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 57.884 Acc@5 93.776 loss 1.462
Accuracy of the network on the 482 val images: 57.88%
Max accuracy: 59.34%
Epoch: [15]  [  0/160]  eta: 0:16:33  lr: 0.000023  min_lr: 0.000001  loss: 2.5059 (2.5059)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6652 (4.6652)  time: 6.2076 (6.2076 -- 6.2076)  data: 5.5661 (5.5661 -- 5.5661)  max mem: 16413
[2023-08-30 13:50:52,022] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:50:52,023] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:50:52,023] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:50:52,023] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:50:54,209] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2413
[2023-08-30 13:50:54,209] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2413
[2023-08-30 13:50:54,209] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:50:54,209] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:50:54,209] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [15]  [ 20/160]  eta: 0:02:39  lr: 0.000023  min_lr: 0.000001  loss: 2.1314 (2.1346)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7050 (6.5942)  time: 0.8861 (0.5225 -- 2.6164)  data: 0.1067 (0.0002 -- 2.0975)  max mem: 16413
Epoch: [15]  [ 40/160]  eta: 0:01:56  lr: 0.000023  min_lr: 0.000001  loss: 2.1458 (2.1559)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7887 (6.2983)  time: 0.8001 (0.5328 -- 3.0352)  data: 0.0305 (0.0003 -- 0.5691)  max mem: 16413
Epoch: [15]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.2037 (2.1533)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9495 (6.3316)  time: 0.9821 (0.5163 -- 3.9342)  data: 0.0102 (0.0003 -- 0.1778)  max mem: 16413
Epoch: [15]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.0422 (2.1377)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1244 (6.4789)  time: 0.8039 (0.5179 -- 3.2326)  data: 0.0013 (0.0003 -- 0.0051)  max mem: 16413
Epoch: [15]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1688 (2.1405)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2315 (6.5190)  time: 0.9112 (0.5228 -- 3.6527)  data: 0.0014 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [15]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0401 (2.1467)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2011 (6.4424)  time: 0.7538 (0.5216 -- 2.6585)  data: 0.0019 (0.0002 -- 0.0112)  max mem: 16413
Epoch: [15]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.3036 (2.1548)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2345 (6.5128)  time: 0.9290 (0.5221 -- 3.8863)  data: 0.3152 (0.0004 -- 3.3294)  max mem: 16413
[2023-08-30 13:52:44,382] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:52:44,382] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:52:44,382] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:52:44,382] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [15]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.3638 (2.1719)  loss_scale: 32768.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5321 (6.3966)  time: 0.7500 (0.4963 -- 2.5307)  data: 0.2283 (0.0002 -- 1.9926)  max mem: 16413
Epoch: [15] Total time: 0:02:22 (0.8876 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.3638 (2.1453)  loss_scale: 32768.0000 (18636.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5321 (6.3966)
Val:  [ 0/27]  eta: 0:01:06  loss: 1.4325 (1.4325)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4450 (2.4450 -- 2.4450)  data: 2.2222 (2.2222 -- 2.2222)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3577 (1.4298)  acc1: 55.5556 (52.5253)  acc5: 100.0000 (95.9596)  time: 0.4182 (0.1834 -- 2.4450)  data: 0.2046 (0.0002 -- 2.2222)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3277 (1.4003)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (95.7672)  time: 0.2204 (0.1704 -- 0.4969)  data: 0.0166 (0.0001 -- 0.2994)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3577 (1.4418)  acc1: 55.5556 (55.1867)  acc5: 100.0000 (95.8506)  time: 0.2064 (0.1327 -- 0.4969)  data: 0.0163 (0.0001 -- 0.2994)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 59.129 Acc@5 94.606 loss 1.418
Accuracy of the network on the 482 val images: 59.13%
Max accuracy: 59.34%
Epoch: [16]  [  0/160]  eta: 0:18:32  lr: 0.000023  min_lr: 0.000001  loss: 2.2525 (2.2525)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0655 (6.0655)  time: 6.9550 (6.9550 -- 6.9550)  data: 5.5507 (5.5507 -- 5.5507)  max mem: 16413
Epoch: [16]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 2.1364 (2.1586)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7164 (6.4893)  time: 0.8951 (0.5334 -- 5.1269)  data: 0.3033 (0.0002 -- 4.6024)  max mem: 16413
Epoch: [16]  [ 40/160]  eta: 0:02:03  lr: 0.000023  min_lr: 0.000001  loss: 2.2187 (2.1576)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1457 (6.6684)  time: 0.8718 (0.5301 -- 3.0276)  data: 0.2881 (0.0002 -- 2.4934)  max mem: 16413
Epoch: [16]  [ 60/160]  eta: 0:01:33  lr: 0.000023  min_lr: 0.000001  loss: 2.0537 (2.1271)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5303 (6.6159)  time: 0.7232 (0.5132 -- 2.1469)  data: 0.0824 (0.0003 -- 1.1449)  max mem: 16413
[2023-08-30 13:54:07,988] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2627
[2023-08-30 13:54:07,989] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2627
[2023-08-30 13:54:07,989] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:54:07,989] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:54:07,989] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [16]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.1373 (2.1242)  loss_scale: 16384.0000 (29936.1975)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3113 (6.7569)  time: 0.9327 (0.5227 -- 2.5288)  data: 0.0729 (0.0004 -- 0.7937)  max mem: 16413
Epoch: [16]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0464 (2.1178)  loss_scale: 16384.0000 (27252.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2632 (6.7099)  time: 0.8599 (0.5177 -- 3.5430)  data: 0.0013 (0.0002 -- 0.0028)  max mem: 16413
Epoch: [16]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.2528 (2.1432)  loss_scale: 16384.0000 (25456.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6983 (6.7014)  time: 0.8527 (0.5207 -- 2.7055)  data: 0.0519 (0.0004 -- 0.6481)  max mem: 16413
Epoch: [16]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.1185 (2.1395)  loss_scale: 16384.0000 (24169.3050)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9726 (6.6945)  time: 0.8037 (0.5307 -- 3.7127)  data: 0.2014 (0.0004 -- 3.1615)  max mem: 16413
Epoch: [16]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.2850 (2.1503)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5186 (6.5832)  time: 0.6914 (0.4979 -- 2.5119)  data: 0.1004 (0.0002 -- 1.9973)  max mem: 16413
Epoch: [16] Total time: 0:02:19 (0.8689 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.2850 (2.1490)  loss_scale: 16384.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5186 (6.5832)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.3967 (1.3967)  acc1: 44.4444 (44.4444)  acc5: 100.0000 (100.0000)  time: 2.4091 (2.4091 -- 2.4091)  data: 2.1790 (2.1790 -- 2.1790)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.3420 (1.4201)  acc1: 44.4444 (51.5152)  acc5: 100.0000 (96.9697)  time: 0.4344 (0.1917 -- 2.4091)  data: 0.2141 (0.0003 -- 2.1790)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.3122 (1.3867)  acc1: 55.5556 (54.4974)  acc5: 100.0000 (95.7672)  time: 0.2203 (0.1697 -- 0.4141)  data: 0.0154 (0.0001 -- 0.1684)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.4016 (1.4248)  acc1: 55.5556 (55.1867)  acc5: 100.0000 (95.0207)  time: 0.2048 (0.1332 -- 0.4141)  data: 0.0152 (0.0001 -- 0.1684)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 58.299 Acc@5 94.398 loss 1.401
Accuracy of the network on the 482 val images: 58.30%
Max accuracy: 59.34%
Epoch: [17]  [  0/160]  eta: 0:21:03  lr: 0.000023  min_lr: 0.000001  loss: 2.5865 (2.5865)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4650 (7.4650)  time: 7.8970 (7.8970 -- 7.8970)  data: 7.3497 (7.3497 -- 7.3497)  max mem: 16413
Epoch: [17]  [ 20/160]  eta: 0:02:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0516 (2.0778)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1038 (6.3699)  time: 0.7804 (0.5242 -- 4.0009)  data: 0.2298 (0.0004 -- 3.4648)  max mem: 16413
[2023-08-30 13:56:11,355] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:56:11,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:56:11,356] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:56:11,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [17]  [ 40/160]  eta: 0:02:05  lr: 0.000023  min_lr: 0.000001  loss: 2.0740 (2.0807)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5188 (6.4503)  time: 0.9624 (0.5227 -- 4.0211)  data: 0.2661 (0.0004 -- 3.4947)  max mem: 16413
Epoch: [17]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.0829 (2.0878)  loss_scale: 32768.0000 (23098.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7416 (6.5840)  time: 0.8329 (0.5262 -- 3.6685)  data: 0.1571 (0.0005 -- 1.7923)  max mem: 16413
Epoch: [17]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.1870 (2.1124)  loss_scale: 32768.0000 (25486.2222)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9324 (6.7141)  time: 0.8515 (0.5298 -- 3.1766)  data: 0.3036 (0.0005 -- 2.6676)  max mem: 16413
Epoch: [17]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0752 (2.1178)  loss_scale: 32768.0000 (26928.1584)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2326 (6.6994)  time: 0.8660 (0.5215 -- 3.7774)  data: 0.3015 (0.0002 -- 3.2506)  max mem: 16413
Epoch: [17]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1392 (2.1121)  loss_scale: 32768.0000 (27893.4215)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7727 (6.7028)  time: 0.8842 (0.5213 -- 3.2152)  data: 0.3401 (0.0002 -- 2.6900)  max mem: 16413
Epoch: [17]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9414 (2.0911)  loss_scale: 32768.0000 (28584.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3121 (6.8218)  time: 0.8444 (0.5197 -- 4.2802)  data: 0.2991 (0.0003 -- 3.7579)  max mem: 16413
[2023-08-30 13:57:51,583] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2875
[2023-08-30 13:57:51,583] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 13:57:51,583] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 2875
[2023-08-30 13:57:51,583] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 13:57:51,583] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [17]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0987 (2.0882)  loss_scale: 32768.0000 (28569.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3121 (6.8212)  time: 0.6659 (0.4881 -- 3.3212)  data: 0.1533 (0.0001 -- 2.8012)  max mem: 16413
Epoch: [17] Total time: 0:02:21 (0.8821 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0987 (2.0972)  loss_scale: 32768.0000 (28569.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3121 (6.8212)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.3505 (1.3505)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2929 (2.2929 -- 2.2929)  data: 2.0528 (2.0528 -- 2.0528)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.3417 (1.3950)  acc1: 55.5556 (53.5354)  acc5: 100.0000 (95.9596)  time: 0.4052 (0.2019 -- 2.2929)  data: 0.1891 (0.0008 -- 2.0528)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2875 (1.3501)  acc1: 55.5556 (56.6138)  acc5: 100.0000 (96.2963)  time: 0.2232 (0.1690 -- 0.4569)  data: 0.0186 (0.0001 -- 0.2431)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3917 (1.4007)  acc1: 55.5556 (56.0166)  acc5: 100.0000 (95.4357)  time: 0.2092 (0.1327 -- 0.4569)  data: 0.0183 (0.0001 -- 0.2431)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 59.129 Acc@5 93.983 loss 1.374
Accuracy of the network on the 482 val images: 59.13%
Max accuracy: 59.34%
Epoch: [18]  [  0/160]  eta: 0:23:03  lr: 0.000023  min_lr: 0.000001  loss: 2.3920 (2.3920)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9705 (5.9705)  time: 8.6478 (8.6478 -- 8.6478)  data: 8.1258 (8.1258 -- 8.1258)  max mem: 16413
Epoch: [18]  [ 20/160]  eta: 0:02:35  lr: 0.000023  min_lr: 0.000001  loss: 2.0537 (2.0702)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1821 (6.8663)  time: 0.7336 (0.5340 -- 1.6276)  data: 0.1291 (0.0005 -- 1.0502)  max mem: 16413
Epoch: [18]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 2.0100 (2.0677)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3144 (6.9011)  time: 1.0138 (0.5085 -- 4.2637)  data: 0.4471 (0.0003 -- 3.7499)  max mem: 16413
Epoch: [18]  [ 60/160]  eta: 0:01:40  lr: 0.000023  min_lr: 0.000001  loss: 2.2437 (2.1186)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2065 (6.9625)  time: 0.8795 (0.5175 -- 3.3240)  data: 0.1160 (0.0004 -- 1.6579)  max mem: 16413
Epoch: [18]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.0788 (2.1035)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3682 (7.0844)  time: 0.8041 (0.5211 -- 2.6090)  data: 0.1421 (0.0002 -- 2.0822)  max mem: 16413
Epoch: [18]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0401 (2.1085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8384 (6.9987)  time: 0.7825 (0.5230 -- 2.3256)  data: 0.2344 (0.0002 -- 1.7944)  max mem: 16413
[2023-08-30 13:59:51,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=11, lr=[5.500275261384e-07, 5.500275261384e-07, 7.333700348512001e-07, 7.333700348512001e-07, 9.778267131349334e-07, 9.778267131349334e-07, 1.303768950846578e-06, 1.303768950846578e-06, 1.7383586011287706e-06, 1.7383586011287706e-06, 2.317811468171694e-06, 2.317811468171694e-06, 3.0904152908955924e-06, 3.0904152908955924e-06, 4.120553721194123e-06, 4.120553721194123e-06, 5.494071628258831e-06, 5.494071628258831e-06, 7.325428837678441e-06, 7.325428837678441e-06, 9.767238450237921e-06, 9.767238450237921e-06, 1.3022984600317229e-05, 1.3022984600317229e-05, 1.736397946708964e-05, 1.736397946708964e-05, 2.3151972622786183e-05, 2.3151972622786183e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 13:59:51,354] [INFO] [timer.py:260:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=17.38223370533825, CurrSamplesPerSec=21.589520010294684, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [18]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 2.1272 (2.1180)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5500 (7.1041)  time: 1.0642 (0.5185 -- 4.4748)  data: 0.5226 (0.0003 -- 3.9546)  max mem: 16413
[2023-08-30 13:59:57,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:59:57,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 13:59:57,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 13:59:57,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [18]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1117 (2.1204)  loss_scale: 32768.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2916 (7.1412)  time: 0.6560 (0.5239 -- 2.5562)  data: 0.1064 (0.0003 -- 2.0287)  max mem: 16413
[2023-08-30 14:00:20,283] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3037
[2023-08-30 14:00:20,283] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3037
[2023-08-30 14:00:20,283] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:00:20,283] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:00:20,283] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [18]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0770 (2.1117)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4151 (7.0432)  time: 0.6573 (0.4840 -- 2.1372)  data: 0.1193 (0.0002 -- 1.5885)  max mem: 16413
Epoch: [18] Total time: 0:02:19 (0.8747 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0770 (2.1027)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4151 (7.0432)
Val:  [ 0/27]  eta: 0:01:05  loss: 1.3258 (1.3258)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4094 (2.4094 -- 2.4094)  data: 2.1813 (2.1813 -- 2.1813)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2471 (1.3384)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (95.9596)  time: 0.4197 (0.2034 -- 2.4094)  data: 0.1993 (0.0007 -- 2.1813)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2407 (1.3061)  acc1: 55.5556 (57.6720)  acc5: 100.0000 (96.2963)  time: 0.2170 (0.1708 -- 0.3080)  data: 0.0065 (0.0001 -- 0.1160)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.3349 (1.3498)  acc1: 55.5556 (57.2614)  acc5: 100.0000 (96.2656)  time: 0.1988 (0.1329 -- 0.3080)  data: 0.0062 (0.0001 -- 0.1160)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 61.826 Acc@5 94.398 loss 1.325
Accuracy of the network on the 482 val images: 61.83%
[2023-08-30 14:00:29,018] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:00:29,020] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:00:29,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:00:29,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:00:30,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:00:30,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 61.83%
Epoch: [19]  [  0/160]  eta: 0:16:50  lr: 0.000023  min_lr: 0.000001  loss: 1.8302 (1.8302)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4047 (5.4047)  time: 6.3172 (6.3172 -- 6.3172)  data: 5.7666 (5.7666 -- 5.7666)  max mem: 16413
Epoch: [19]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 2.1399 (2.0695)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5584 (7.1335)  time: 0.9219 (0.5235 -- 3.0318)  data: 0.2927 (0.0004 -- 2.4721)  max mem: 16413
Epoch: [19]  [ 40/160]  eta: 0:02:04  lr: 0.000023  min_lr: 0.000001  loss: 2.1007 (2.0969)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1843 (7.4322)  time: 0.8822 (0.5284 -- 3.1232)  data: 0.3330 (0.0002 -- 2.5933)  max mem: 16413
Epoch: [19]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.1052 (2.1233)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9253 (7.1021)  time: 0.8741 (0.5227 -- 4.4234)  data: 0.2765 (0.0003 -- 3.8989)  max mem: 16413
Epoch: [19]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.0053 (2.0973)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5481 (7.0646)  time: 0.8062 (0.5176 -- 2.6720)  data: 0.0562 (0.0003 -- 1.1053)  max mem: 16413
Epoch: [19]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.8938 (2.0775)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1806 (7.1074)  time: 0.8712 (0.5239 -- 3.3524)  data: 0.2347 (0.0002 -- 2.8020)  max mem: 16413
Epoch: [19]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1364 (2.0903)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9136 (7.0836)  time: 0.8621 (0.5253 -- 4.0052)  data: 0.3190 (0.0005 -- 3.4895)  max mem: 16413
[2023-08-30 14:02:24,356] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:02:24,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:02:24,356] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:02:24,357] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [19]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1518 (2.0978)  loss_scale: 32768.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2517 (7.0107)  time: 0.8887 (0.5287 -- 2.6653)  data: 0.2799 (0.0002 -- 2.1291)  max mem: 16413
[2023-08-30 14:02:43,891] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3187
[2023-08-30 14:02:43,891] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3187
[2023-08-30 14:02:43,892] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:02:43,892] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:02:43,892] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [19]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1309 (2.0983)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5802 (7.0064)  time: 0.6853 (0.4966 -- 2.2848)  data: 0.0257 (0.0002 -- 0.5004)  max mem: 16413
Epoch: [19] Total time: 0:02:21 (0.8853 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1309 (2.1064)  loss_scale: 16384.0000 (18534.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5802 (7.0064)
[2023-08-30 14:02:51,947] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-19 is about to be saved!
[2023-08-30 14:02:51,949] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt
[2023-08-30 14:02:51,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt...
[2023-08-30 14:02:51,950] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
[2023-08-30 14:02:52,969] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-19/mp_rank_00_model_states.pt.
[2023-08-30 14:02:52,969] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-19 is ready now!
Val:  [ 0/27]  eta: 0:01:05  loss: 1.3084 (1.3084)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4430 (2.4430 -- 2.4430)  data: 2.1811 (2.1811 -- 2.1811)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2118 (1.3166)  acc1: 55.5556 (54.5455)  acc5: 100.0000 (96.9697)  time: 0.4235 (0.1876 -- 2.4430)  data: 0.2075 (0.0008 -- 2.1811)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2041 (1.2810)  acc1: 55.5556 (56.6138)  acc5: 100.0000 (96.8254)  time: 0.2101 (0.1729 -- 0.3091)  data: 0.0073 (0.0002 -- 0.0910)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2840 (1.3203)  acc1: 55.5556 (58.0913)  acc5: 100.0000 (96.2656)  time: 0.1959 (0.1326 -- 0.3091)  data: 0.0070 (0.0001 -- 0.0910)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 61.618 Acc@5 94.813 loss 1.295
Accuracy of the network on the 482 val images: 61.62%
Max accuracy: 61.83%
Epoch: [20]  [  0/160]  eta: 0:15:10  lr: 0.000023  min_lr: 0.000001  loss: 2.5835 (2.5835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1328 (6.1328)  time: 5.6914 (5.6914 -- 5.6914)  data: 5.1435 (5.1435 -- 5.1435)  max mem: 16413
Epoch: [20]  [ 20/160]  eta: 0:02:39  lr: 0.000023  min_lr: 0.000001  loss: 1.9874 (2.0488)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8954 (6.7026)  time: 0.9135 (0.5333 -- 3.8549)  data: 0.1417 (0.0010 -- 2.2118)  max mem: 16413
Epoch: [20]  [ 40/160]  eta: 0:01:58  lr: 0.000023  min_lr: 0.000001  loss: 2.1154 (2.1032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8906 (6.8821)  time: 0.8336 (0.5321 -- 2.8471)  data: 0.2068 (0.0009 -- 2.2957)  max mem: 16413
Epoch: [20]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 2.1467 (2.0872)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5659 (6.9092)  time: 0.8807 (0.5205 -- 2.5188)  data: 0.1747 (0.0005 -- 1.4394)  max mem: 16413
Epoch: [20]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.0414 (2.0708)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7824 (6.8602)  time: 0.8362 (0.5419 -- 3.4317)  data: 0.2312 (0.0005 -- 2.8931)  max mem: 16413
Epoch: [20]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.2139 (2.0846)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4209 (6.8901)  time: 0.8707 (0.5263 -- 2.8631)  data: 0.1638 (0.0002 -- 2.3232)  max mem: 16413
[2023-08-30 14:04:46,371] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:04:46,371] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:04:46,375] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:04:46,375] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [20]  [120/160]  eta: 0:00:35  lr: 0.000023  min_lr: 0.000001  loss: 2.1184 (2.0858)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1535 (6.9442)  time: 0.7952 (0.5258 -- 2.9032)  data: 0.0395 (0.0007 -- 0.5216)  max mem: 16413
Epoch: [20]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9824 (2.0743)  loss_scale: 32768.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5407 (6.9542)  time: 0.9461 (0.5129 -- 4.7509)  data: 0.3287 (0.0004 -- 4.2441)  max mem: 16413
Epoch: [20]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1912 (2.0773)  loss_scale: 32768.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2969 (6.9362)  time: 0.6494 (0.4961 -- 1.9378)  data: 0.1258 (0.0001 -- 1.4327)  max mem: 16413
Epoch: [20] Total time: 0:02:19 (0.8732 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1912 (2.0822)  loss_scale: 32768.0000 (20889.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2969 (6.9362)
Val:  [ 0/27]  eta: 0:01:04  loss: 1.2788 (1.2788)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3722 (2.3722 -- 2.3722)  data: 2.1343 (2.1343 -- 2.1343)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.2233 (1.2844)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (95.9596)  time: 0.4185 (0.2013 -- 2.3722)  data: 0.2063 (0.0009 -- 2.1343)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.2078 (1.2575)  acc1: 55.5556 (58.2011)  acc5: 100.0000 (95.7672)  time: 0.2228 (0.1701 -- 0.4737)  data: 0.0216 (0.0001 -- 0.2939)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2887 (1.3023)  acc1: 66.6667 (59.3361)  acc5: 100.0000 (95.0207)  time: 0.2089 (0.1329 -- 0.4737)  data: 0.0210 (0.0001 -- 0.2939)  max mem: 16413
Val: Total time: 0:00:07 (0.2895 s / it)
* Acc@1 63.693 Acc@5 94.191 loss 1.280
Accuracy of the network on the 482 val images: 63.69%
[2023-08-30 14:05:28,321] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:05:28,323] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:05:28,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:05:28,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:05:29,491] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:05:29,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 63.69%
Epoch: [21]  [  0/160]  eta: 0:22:41  lr: 0.000023  min_lr: 0.000001  loss: 2.4687 (2.4687)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3596 (5.3596)  time: 8.5066 (8.5066 -- 8.5066)  data: 5.7558 (5.7558 -- 5.7558)  max mem: 16413
[2023-08-30 14:05:45,652] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3369
[2023-08-30 14:05:45,652] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:05:45,652] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3369
[2023-08-30 14:05:45,652] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 14:05:45,652] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [21]  [ 20/160]  eta: 0:02:42  lr: 0.000023  min_lr: 0.000001  loss: 2.1037 (2.1183)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6210 (7.1565)  time: 0.7918 (0.5107 -- 3.2197)  data: 0.0016 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [21]  [ 40/160]  eta: 0:02:09  lr: 0.000023  min_lr: 0.000001  loss: 2.0287 (2.0929)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3917 (7.0927)  time: 0.9884 (0.5359 -- 3.6066)  data: 0.0027 (0.0002 -- 0.0279)  max mem: 16413
Epoch: [21]  [ 60/160]  eta: 0:01:39  lr: 0.000023  min_lr: 0.000001  loss: 2.1963 (2.1206)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9161 (7.1578)  time: 0.8427 (0.5244 -- 3.9051)  data: 0.0012 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [21]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 1.9795 (2.0908)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5609 (7.1824)  time: 0.8412 (0.5243 -- 2.9320)  data: 0.0011 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [21]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0758 (2.0836)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4794 (7.2099)  time: 0.7628 (0.5257 -- 3.0938)  data: 0.0349 (0.0006 -- 0.5443)  max mem: 16413
Epoch: [21]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0092 (2.0742)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4646 (7.2678)  time: 0.9201 (0.5288 -- 2.5448)  data: 0.0017 (0.0004 -- 0.0058)  max mem: 16413
[2023-08-30 14:07:37,997] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:07:37,998] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:07:38,002] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:07:38,002] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:07:38,574] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3499
[2023-08-30 14:07:38,574] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3499
[2023-08-30 14:07:38,574] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:07:38,574] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:07:38,574] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [21]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9687 (2.0598)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5226 (7.2309)  time: 0.9091 (0.5289 -- 4.6691)  data: 0.0023 (0.0004 -- 0.0206)  max mem: 16413
Epoch: [21]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0222 (2.0563)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1523 (7.2076)  time: 0.5955 (0.4996 -- 1.8024)  data: 0.0015 (0.0002 -- 0.0099)  max mem: 16413
Epoch: [21] Total time: 0:02:21 (0.8813 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0222 (2.0592)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1523 (7.2076)
Val:  [ 0/27]  eta: 0:00:57  loss: 1.2573 (1.2573)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1257 (2.1257 -- 2.1257)  data: 1.8899 (1.8899 -- 1.8899)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1839 (1.2523)  acc1: 66.6667 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4120 (0.1963 -- 2.1257)  data: 0.2021 (0.0005 -- 1.8899)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1839 (1.2293)  acc1: 66.6667 (60.8466)  acc5: 100.0000 (96.2963)  time: 0.2263 (0.1706 -- 0.4318)  data: 0.0249 (0.0001 -- 0.2076)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2197 (1.2767)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (95.8506)  time: 0.2141 (0.1335 -- 0.4318)  data: 0.0246 (0.0001 -- 0.2076)  max mem: 16413
Val: Total time: 0:00:07 (0.2830 s / it)
* Acc@1 64.315 Acc@5 94.606 loss 1.246
Accuracy of the network on the 482 val images: 64.32%
[2023-08-30 14:07:58,223] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:07:58,225] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:07:58,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:07:58,225] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:07:59,585] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:07:59,586] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 64.32%
Epoch: [22]  [  0/160]  eta: 0:20:07  lr: 0.000023  min_lr: 0.000001  loss: 1.8119 (1.8119)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5568 (8.5568)  time: 7.5439 (7.5439 -- 7.5439)  data: 6.8213 (6.8213 -- 6.8213)  max mem: 16413
Epoch: [22]  [ 20/160]  eta: 0:02:41  lr: 0.000023  min_lr: 0.000001  loss: 2.1024 (2.0064)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4988 (7.3936)  time: 0.8328 (0.5364 -- 4.0854)  data: 0.1514 (0.0004 -- 2.0589)  max mem: 16413
Epoch: [22]  [ 40/160]  eta: 0:02:10  lr: 0.000023  min_lr: 0.000001  loss: 2.1753 (2.0859)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0509 (7.5667)  time: 1.0221 (0.5262 -- 4.4193)  data: 0.0228 (0.0003 -- 0.4153)  max mem: 16413
Epoch: [22]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1813 (2.1135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7142 (7.6037)  time: 0.7059 (0.5234 -- 2.8979)  data: 0.0019 (0.0003 -- 0.0053)  max mem: 16413
Epoch: [22]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 1.9459 (2.0809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2138 (7.3794)  time: 0.8905 (0.5211 -- 3.5145)  data: 0.0019 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [22]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.2194 (2.0919)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3351 (7.3776)  time: 0.7601 (0.5314 -- 2.5295)  data: 0.0013 (0.0006 -- 0.0021)  max mem: 16413
[2023-08-30 14:09:37,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:09:37,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:09:37,479] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:09:37,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [22]  [120/160]  eta: 0:00:35  lr: 0.000023  min_lr: 0.000001  loss: 2.1711 (2.0961)  loss_scale: 32768.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8699 (7.4137)  time: 0.8141 (0.5269 -- 2.6571)  data: 0.0020 (0.0003 -- 0.0089)  max mem: 16413
Epoch: [22]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.0943 (2.0967)  loss_scale: 32768.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9750 (7.4015)  time: 0.9131 (0.5326 -- 3.1930)  data: 0.0028 (0.0003 -- 0.0154)  max mem: 16413
[2023-08-30 14:10:17,786] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3675
[2023-08-30 14:10:17,786] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3675
[2023-08-30 14:10:17,786] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:10:17,786] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:10:17,786] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [22]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1399 (2.1046)  loss_scale: 32768.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1887 (7.4690)  time: 0.7218 (0.4816 -- 3.3761)  data: 0.0012 (0.0002 -- 0.0058)  max mem: 16413
Epoch: [22] Total time: 0:02:20 (0.8761 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1399 (2.0818)  loss_scale: 32768.0000 (21196.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1887 (7.4690)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.2390 (1.2390)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2529 (2.2529 -- 2.2529)  data: 2.0288 (2.0288 -- 2.0288)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.1827 (1.2416)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (94.9495)  time: 0.4104 (0.1993 -- 2.2529)  data: 0.1949 (0.0006 -- 2.0288)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1543 (1.2079)  acc1: 55.5556 (59.7884)  acc5: 100.0000 (95.7672)  time: 0.2236 (0.1715 -- 0.3320)  data: 0.0185 (0.0001 -- 0.1265)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1979 (1.2534)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (95.0207)  time: 0.2093 (0.1330 -- 0.3320)  data: 0.0182 (0.0001 -- 0.1265)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 63.278 Acc@5 94.606 loss 1.226
Accuracy of the network on the 482 val images: 63.28%
Max accuracy: 64.32%
Epoch: [23]  [  0/160]  eta: 0:20:27  lr: 0.000023  min_lr: 0.000001  loss: 2.1708 (2.1708)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2753 (11.2753)  time: 7.6697 (7.6697 -- 7.6697)  data: 7.0993 (7.0993 -- 7.0993)  max mem: 16413
Epoch: [23]  [ 20/160]  eta: 0:02:44  lr: 0.000023  min_lr: 0.000001  loss: 2.0469 (2.0775)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8561 (7.2980)  time: 0.8525 (0.5284 -- 3.5683)  data: 0.3006 (0.0004 -- 3.0151)  max mem: 16413
Epoch: [23]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 2.0815 (2.0497)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9309 (6.8356)  time: 0.9388 (0.5275 -- 3.9189)  data: 0.3913 (0.0006 -- 3.3736)  max mem: 16413
[2023-08-30 14:11:21,701] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3735
[2023-08-30 14:11:21,702] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 3735
[2023-08-30 14:11:21,702] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 14:11:21,702] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 14:11:21,702] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [23]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.1258 (2.0674)  loss_scale: 16384.0000 (15578.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3662 (6.9232)  time: 0.8134 (0.5287 -- 3.3821)  data: 0.2628 (0.0004 -- 2.8446)  max mem: 16413
Epoch: [23]  [ 80/160]  eta: 0:01:17  lr: 0.000023  min_lr: 0.000001  loss: 2.2095 (2.0940)  loss_scale: 8192.0000 (13754.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4278 (6.8650)  time: 0.9487 (0.5200 -- 3.6217)  data: 0.4020 (0.0005 -- 3.1001)  max mem: 16413
Epoch: [23]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1162 (2.0810)  loss_scale: 8192.0000 (12652.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9434 (6.9610)  time: 0.7255 (0.5290 -- 3.2950)  data: 0.1739 (0.0003 -- 2.7736)  max mem: 16413
Epoch: [23]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0400 (2.0785)  loss_scale: 8192.0000 (11915.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9569 (7.3052)  time: 0.8620 (0.5278 -- 4.0964)  data: 0.2979 (0.0006 -- 3.5636)  max mem: 16413
Epoch: [23]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 2.1092 (2.0868)  loss_scale: 8192.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4080 (7.2933)  time: 0.8186 (0.5170 -- 4.1015)  data: 0.2746 (0.0003 -- 3.5569)  max mem: 16413
Epoch: [23]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 1.9708 (2.0784)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4811 (7.3785)  time: 0.7276 (0.4994 -- 1.9517)  data: 0.0928 (0.0001 -- 1.2531)  max mem: 16413
Epoch: [23] Total time: 0:02:20 (0.8806 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 1.9708 (2.0771)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4811 (7.3785)
Val:  [ 0/27]  eta: 0:00:53  loss: 1.2081 (1.2081)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 1.9972 (1.9972 -- 1.9972)  data: 1.7774 (1.7774 -- 1.7774)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.1615 (1.2182)  acc1: 66.6667 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.3994 (0.1964 -- 1.9972)  data: 0.1879 (0.0006 -- 1.7774)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1289 (1.1922)  acc1: 66.6667 (59.7884)  acc5: 100.0000 (96.2963)  time: 0.2343 (0.1687 -- 0.5057)  data: 0.0292 (0.0001 -- 0.2746)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2397 (1.2327)  acc1: 55.5556 (59.7510)  acc5: 100.0000 (95.0207)  time: 0.2189 (0.1330 -- 0.5057)  data: 0.0287 (0.0001 -- 0.2746)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 64.315 Acc@5 94.813 loss 1.204
Accuracy of the network on the 482 val images: 64.32%
Max accuracy: 64.32%
Epoch: [24]  [  0/160]  eta: 0:15:45  lr: 0.000023  min_lr: 0.000001  loss: 1.7202 (1.7202)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2965 (6.2965)  time: 5.9065 (5.9065 -- 5.9065)  data: 4.5695 (4.5695 -- 4.5695)  max mem: 16413
Epoch: [24]  [ 20/160]  eta: 0:02:33  lr: 0.000023  min_lr: 0.000001  loss: 2.0016 (2.0460)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1467 (7.2953)  time: 0.8551 (0.5308 -- 2.8369)  data: 0.1771 (0.0003 -- 1.8340)  max mem: 16413
[2023-08-30 14:13:24,476] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:13:24,476] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 14:13:24,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:13:24,477] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [24]  [ 40/160]  eta: 0:02:04  lr: 0.000023  min_lr: 0.000001  loss: 1.9569 (2.0299)  loss_scale: 16384.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3587 (7.3492)  time: 0.9683 (0.5312 -- 3.6314)  data: 0.4137 (0.0003 -- 3.1088)  max mem: 16413
Epoch: [24]  [ 60/160]  eta: 0:01:38  lr: 0.000023  min_lr: 0.000001  loss: 2.0019 (2.0164)  loss_scale: 16384.0000 (13160.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7354 (7.3339)  time: 0.8951 (0.5226 -- 4.7650)  data: 0.3487 (0.0005 -- 4.2282)  max mem: 16413
Epoch: [24]  [ 80/160]  eta: 0:01:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0630 (2.0336)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1501 (7.3960)  time: 0.9708 (0.5265 -- 4.0630)  data: 0.4243 (0.0002 -- 3.5265)  max mem: 16413
Epoch: [24]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 1.9564 (2.0060)  loss_scale: 16384.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9961 (7.5299)  time: 0.7038 (0.5231 -- 2.7076)  data: 0.1584 (0.0003 -- 2.1868)  max mem: 16413
Epoch: [24]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 1.9751 (2.0016)  loss_scale: 16384.0000 (14759.1405)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3323 (7.6788)  time: 0.9250 (0.5284 -- 3.8376)  data: 0.3757 (0.0005 -- 3.3252)  max mem: 16413
Epoch: [24]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0725 (2.0188)  loss_scale: 16384.0000 (14989.6170)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7879 (7.5843)  time: 0.8073 (0.5317 -- 3.1670)  data: 0.2502 (0.0004 -- 2.6280)  max mem: 16413
[2023-08-30 14:15:14,171] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:15:14,171] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:15:14,172] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:15:14,172] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:15:17,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=17, lr=[5.425206546193266e-07, 5.425206546193266e-07, 7.233608728257689e-07, 7.233608728257689e-07, 9.644811637676918e-07, 9.644811637676918e-07, 1.2859748850235891e-06, 1.2859748850235891e-06, 1.7146331800314522e-06, 1.7146331800314522e-06, 2.2861775733752696e-06, 2.2861775733752696e-06, 3.0482367645003596e-06, 3.0482367645003596e-06, 4.064315686000479e-06, 4.064315686000479e-06, 5.4190875813339725e-06, 5.4190875813339725e-06, 7.225450108445297e-06, 7.225450108445297e-06, 9.633933477927063e-06, 9.633933477927063e-06, 1.2845244637236082e-05, 1.2845244637236082e-05, 1.712699284964811e-05, 1.712699284964811e-05, 2.283599046619748e-05, 2.283599046619748e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 14:15:17,689] [INFO] [timer.py:260:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=17.580098503567026, CurrSamplesPerSec=24.00334214015242, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [24]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.1223 (2.0240)  loss_scale: 16384.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6918 (7.4744)  time: 0.6781 (0.4961 -- 2.7822)  data: 0.1592 (0.0002 -- 2.2608)  max mem: 16413
Epoch: [24] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.1223 (2.0297)  loss_scale: 16384.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6918 (7.4744)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.2107 (1.2107)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3042 (2.3042 -- 2.3042)  data: 2.0296 (2.0296 -- 2.0296)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1909 (1.2098)  acc1: 66.6667 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4267 (0.1988 -- 2.3042)  data: 0.2084 (0.0004 -- 2.0296)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1529 (1.1796)  acc1: 66.6667 (59.2593)  acc5: 100.0000 (96.2963)  time: 0.2168 (0.1707 -- 0.4904)  data: 0.0133 (0.0001 -- 0.2546)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.2111 (1.2224)  acc1: 55.5556 (59.3361)  acc5: 100.0000 (95.4357)  time: 0.2027 (0.1328 -- 0.4904)  data: 0.0131 (0.0001 -- 0.2546)  max mem: 16413
Val: Total time: 0:00:07 (0.2824 s / it)
* Acc@1 63.485 Acc@5 94.606 loss 1.192
Accuracy of the network on the 482 val images: 63.49%
Max accuracy: 64.32%
Epoch: [25]  [  0/160]  eta: 0:23:15  lr: 0.000023  min_lr: 0.000001  loss: 2.3115 (2.3115)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3341 (7.3341)  time: 8.7225 (8.7225 -- 8.7225)  data: 8.1685 (8.1685 -- 8.1685)  max mem: 16413
Epoch: [25]  [ 20/160]  eta: 0:02:47  lr: 0.000023  min_lr: 0.000001  loss: 1.9290 (2.1065)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8038 (8.2912)  time: 0.8234 (0.5298 -- 3.2710)  data: 0.2740 (0.0005 -- 2.7303)  max mem: 16413
Epoch: [25]  [ 40/160]  eta: 0:02:06  lr: 0.000023  min_lr: 0.000001  loss: 1.9512 (2.0627)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4916 (8.0783)  time: 0.8973 (0.5182 -- 3.9987)  data: 0.3444 (0.0005 -- 3.4309)  max mem: 16413
Epoch: [25]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 2.1059 (2.0860)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8318 (7.8926)  time: 0.7829 (0.5183 -- 3.5774)  data: 0.2428 (0.0004 -- 3.0319)  max mem: 16413
Epoch: [25]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.0304 (2.0797)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1780 (7.8046)  time: 0.8498 (0.5175 -- 3.3894)  data: 0.2945 (0.0003 -- 2.8536)  max mem: 16413
[2023-08-30 14:16:43,779] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4084
[2023-08-30 14:16:43,779] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:16:43,779] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4084
[2023-08-30 14:16:43,779] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:16:43,780] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [25]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 1.9365 (2.0572)  loss_scale: 16384.0000 (30010.2970)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7683 (7.6789)  time: 0.7773 (0.5397 -- 2.6145)  data: 0.2126 (0.0003 -- 2.0660)  max mem: 16413
Epoch: [25]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9401 (2.0423)  loss_scale: 16384.0000 (27758.0165)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7189 (7.6824)  time: 0.8897 (0.5104 -- 3.6597)  data: 0.3468 (0.0003 -- 3.1426)  max mem: 16413
Epoch: [25]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.1083 (2.0468)  loss_scale: 16384.0000 (26144.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5836 (7.7103)  time: 0.8956 (0.5223 -- 3.4975)  data: 0.3513 (0.0008 -- 2.9682)  max mem: 16413
Epoch: [25]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0370 (2.0405)  loss_scale: 16384.0000 (24985.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3390 (7.7859)  time: 0.6620 (0.4971 -- 2.2733)  data: 0.1439 (0.0002 -- 1.7628)  max mem: 16413
Epoch: [25] Total time: 0:02:19 (0.8735 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0370 (2.0347)  loss_scale: 16384.0000 (24985.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3390 (7.7859)
Val:  [ 0/27]  eta: 0:00:56  loss: 1.1698 (1.1698)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1028 (2.1028 -- 2.1028)  data: 1.8998 (1.8998 -- 1.8998)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.1224 (1.1752)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (96.9697)  time: 0.4088 (0.1978 -- 2.1028)  data: 0.1916 (0.0005 -- 1.8998)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0969 (1.1537)  acc1: 55.5556 (60.8466)  acc5: 100.0000 (96.8254)  time: 0.2306 (0.1721 -- 0.4849)  data: 0.0241 (0.0001 -- 0.2715)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1726 (1.1916)  acc1: 55.5556 (61.4108)  acc5: 100.0000 (95.4357)  time: 0.2159 (0.1333 -- 0.4849)  data: 0.0236 (0.0001 -- 0.2715)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 65.145 Acc@5 94.813 loss 1.162
Accuracy of the network on the 482 val images: 65.15%
[2023-08-30 14:17:52,991] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:17:52,993] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:17:52,993] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:17:52,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:17:54,457] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:17:54,458] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.15%
Epoch: [26]  [  0/160]  eta: 0:18:11  lr: 0.000023  min_lr: 0.000001  loss: 1.7169 (1.7169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6081 (4.6081)  time: 6.8250 (6.8250 -- 6.8250)  data: 6.2924 (6.2924 -- 6.2924)  max mem: 16413
[2023-08-30 14:18:04,556] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4166
[2023-08-30 14:18:04,556] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4166
[2023-08-30 14:18:04,557] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 14:18:04,557] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 14:18:04,557] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [26]  [ 20/160]  eta: 0:02:45  lr: 0.000023  min_lr: 0.000001  loss: 2.1698 (2.1130)  loss_scale: 8192.0000 (10532.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7443 (6.7051)  time: 0.8973 (0.5205 -- 4.3026)  data: 0.1284 (0.0003 -- 2.5357)  max mem: 16413
Epoch: [26]  [ 40/160]  eta: 0:02:02  lr: 0.000023  min_lr: 0.000001  loss: 2.0023 (2.0763)  loss_scale: 8192.0000 (9390.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5830 (7.2015)  time: 0.8614 (0.5308 -- 3.4692)  data: 0.0158 (0.0003 -- 0.2929)  max mem: 16413
Epoch: [26]  [ 60/160]  eta: 0:01:35  lr: 0.000023  min_lr: 0.000001  loss: 2.0293 (2.0872)  loss_scale: 8192.0000 (8997.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1815 (6.8612)  time: 0.8136 (0.5325 -- 2.7276)  data: 0.1212 (0.0005 -- 1.1868)  max mem: 16413
Epoch: [26]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.0255 (2.0574)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8009 (6.9828)  time: 0.8651 (0.5337 -- 3.0385)  data: 0.0727 (0.0007 -- 1.0215)  max mem: 16413
Epoch: [26]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.1897 (2.0680)  loss_scale: 8192.0000 (8678.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4686 (7.0926)  time: 0.9020 (0.5172 -- 3.1353)  data: 0.2884 (0.0008 -- 2.5944)  max mem: 16413
Epoch: [26]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9734 (2.0558)  loss_scale: 8192.0000 (8598.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6577 (7.2113)  time: 0.8228 (0.5307 -- 3.1083)  data: 0.1944 (0.0004 -- 2.5886)  max mem: 16413
[2023-08-30 14:19:57,888] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:19:57,888] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:19:57,888] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 14:19:57,888] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [26]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0587 (2.0478)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1770 (7.3818)  time: 0.8825 (0.5146 -- 2.7595)  data: 0.3354 (0.0004 -- 2.2269)  max mem: 16413
Epoch: [26]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0528 (2.0566)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0640 (7.4097)  time: 0.6720 (0.4978 -- 2.8595)  data: 0.1528 (0.0002 -- 2.3393)  max mem: 16413
Epoch: [26] Total time: 0:02:20 (0.8791 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0528 (2.0362)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0640 (7.4097)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.1819 (1.1819)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2367 (2.2367 -- 2.2367)  data: 2.0207 (2.0207 -- 2.0207)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.1230 (1.1527)  acc1: 55.5556 (57.5758)  acc5: 100.0000 (94.9495)  time: 0.3995 (0.1937 -- 2.2367)  data: 0.1848 (0.0006 -- 2.0207)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0958 (1.1179)  acc1: 55.5556 (60.8466)  acc5: 100.0000 (95.7672)  time: 0.2152 (0.1698 -- 0.4118)  data: 0.0106 (0.0001 -- 0.1954)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1571 (1.1633)  acc1: 66.6667 (61.8257)  acc5: 100.0000 (95.0207)  time: 0.2010 (0.1330 -- 0.4118)  data: 0.0102 (0.0001 -- 0.1954)  max mem: 16413
Val: Total time: 0:00:07 (0.2788 s / it)
* Acc@1 64.523 Acc@5 94.813 loss 1.138
Accuracy of the network on the 482 val images: 64.52%
Max accuracy: 65.15%
Epoch: [27]  [  0/160]  eta: 0:19:44  lr: 0.000023  min_lr: 0.000001  loss: 2.0035 (2.0035)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6231 (6.6231)  time: 7.4013 (7.4013 -- 7.4013)  data: 5.0148 (5.0148 -- 5.0148)  max mem: 16413
Epoch: [27]  [ 20/160]  eta: 0:02:42  lr: 0.000023  min_lr: 0.000001  loss: 1.9815 (1.9622)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9430 (7.3775)  time: 0.8504 (0.5373 -- 3.9251)  data: 0.0021 (0.0007 -- 0.0092)  max mem: 16413
Epoch: [27]  [ 40/160]  eta: 0:02:06  lr: 0.000023  min_lr: 0.000001  loss: 1.9608 (1.9789)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7520 (7.7378)  time: 0.9422 (0.5382 -- 3.5548)  data: 0.0032 (0.0004 -- 0.0273)  max mem: 16413
Epoch: [27]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0687 (1.9941)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1376 (7.6789)  time: 0.7866 (0.5197 -- 3.3404)  data: 0.0011 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [27]  [ 80/160]  eta: 0:01:16  lr: 0.000023  min_lr: 0.000001  loss: 2.0415 (2.0148)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0932 (7.4609)  time: 0.9288 (0.5149 -- 3.2437)  data: 0.0014 (0.0001 -- 0.0041)  max mem: 16413
Epoch: [27]  [100/160]  eta: 0:00:55  lr: 0.000023  min_lr: 0.000001  loss: 2.0737 (2.0393)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1447 (7.4294)  time: 0.8083 (0.5280 -- 3.5731)  data: 0.0026 (0.0002 -- 0.0152)  max mem: 16413
[2023-08-30 14:21:58,379] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:21:58,380] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:21:58,380] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:21:58,380] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:22:03,093] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4429
[2023-08-30 14:22:03,093] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4429
[2023-08-30 14:22:03,093] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:22:03,093] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:22:03,093] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [27]  [120/160]  eta: 0:00:37  lr: 0.000023  min_lr: 0.000001  loss: 2.0702 (2.0395)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6050 (7.5632)  time: 1.0480 (0.5148 -- 5.7266)  data: 0.0014 (0.0005 -- 0.0029)  max mem: 16413
Epoch: [27]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 2.0172 (2.0328)  loss_scale: 16384.0000 (17081.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6452 (7.5309)  time: 0.7417 (0.5018 -- 3.8114)  data: 0.0013 (0.0001 -- 0.0035)  max mem: 16413
Epoch: [27]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0217 (2.0310)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1501 (7.5123)  time: 0.6198 (0.4951 -- 2.0673)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [27] Total time: 0:02:21 (0.8838 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0217 (2.0352)  loss_scale: 16384.0000 (16998.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1501 (7.5123)
Val:  [ 0/27]  eta: 0:00:58  loss: 1.1466 (1.1466)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.1577 (2.1577 -- 2.1577)  data: 1.9387 (1.9387 -- 1.9387)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.1401 (1.1569)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (95.9596)  time: 0.4176 (0.2068 -- 2.1577)  data: 0.1922 (0.0004 -- 1.9387)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.1067 (1.1058)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (96.2963)  time: 0.2278 (0.1704 -- 0.3946)  data: 0.0127 (0.0001 -- 0.1659)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1197 (1.1598)  acc1: 55.5556 (60.5809)  acc5: 100.0000 (95.0207)  time: 0.2077 (0.1328 -- 0.3946)  data: 0.0124 (0.0001 -- 0.1659)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 64.730 Acc@5 94.813 loss 1.131
Accuracy of the network on the 482 val images: 64.73%
Max accuracy: 65.15%
Epoch: [28]  [  0/160]  eta: 0:18:34  lr: 0.000023  min_lr: 0.000001  loss: 2.0759 (2.0759)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9115 (6.9115)  time: 6.9657 (6.9657 -- 6.9657)  data: 5.6248 (5.6248 -- 5.6248)  max mem: 16413
Epoch: [28]  [ 20/160]  eta: 0:02:34  lr: 0.000023  min_lr: 0.000001  loss: 2.0224 (1.9163)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9172 (7.2202)  time: 0.8133 (0.5267 -- 2.7103)  data: 0.0840 (0.0005 -- 1.2956)  max mem: 16413
Epoch: [28]  [ 40/160]  eta: 0:01:59  lr: 0.000023  min_lr: 0.000001  loss: 2.1048 (1.9878)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8331 (7.8913)  time: 0.8816 (0.5313 -- 3.0652)  data: 0.2578 (0.0004 -- 2.4938)  max mem: 16413
Epoch: [28]  [ 60/160]  eta: 0:01:36  lr: 0.000023  min_lr: 0.000001  loss: 1.9579 (1.9709)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9272 (7.5843)  time: 0.8943 (0.5164 -- 2.5933)  data: 0.2026 (0.0007 -- 1.6329)  max mem: 16413
[2023-08-30 14:24:06,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:24:06,568] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:24:06,568] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:24:06,569] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [28]  [ 80/160]  eta: 0:01:15  lr: 0.000023  min_lr: 0.000001  loss: 2.0255 (2.0041)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8512 (7.8975)  time: 0.8776 (0.5212 -- 3.4390)  data: 0.0014 (0.0007 -- 0.0030)  max mem: 16413
Epoch: [28]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 2.1610 (2.0174)  loss_scale: 32768.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4815 (7.7062)  time: 0.8084 (0.5308 -- 2.4203)  data: 0.0686 (0.0003 -- 0.9159)  max mem: 16413
[2023-08-30 14:24:38,612] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4593
[2023-08-30 14:24:38,612] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:24:38,612] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4593
[2023-08-30 14:24:38,613] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:24:38,613] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [28]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0368 (2.0089)  loss_scale: 32768.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9851 (7.6935)  time: 0.8958 (0.5133 -- 3.0875)  data: 0.2433 (0.0003 -- 2.5630)  max mem: 16413
Epoch: [28]  [140/160]  eta: 0:00:18  lr: 0.000023  min_lr: 0.000001  loss: 1.9649 (2.0005)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1861 (7.7147)  time: 0.8995 (0.5260 -- 3.8571)  data: 0.3534 (0.0003 -- 3.3437)  max mem: 16413
Epoch: [28]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0360 (2.0037)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1431 (7.7157)  time: 0.6869 (0.4961 -- 2.7426)  data: 0.1636 (0.0002 -- 2.2332)  max mem: 16413
Epoch: [28] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0360 (2.0207)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1431 (7.7157)
Val:  [ 0/27]  eta: 0:00:59  loss: 1.1469 (1.1469)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2076 (2.2076 -- 2.2076)  data: 1.9805 (1.9805 -- 1.9805)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.1233 (1.1316)  acc1: 55.5556 (56.5657)  acc5: 100.0000 (95.9596)  time: 0.4031 (0.2034 -- 2.2076)  data: 0.1874 (0.0009 -- 1.9805)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0939 (1.0939)  acc1: 55.5556 (60.3175)  acc5: 100.0000 (96.2963)  time: 0.2280 (0.1687 -- 0.5414)  data: 0.0242 (0.0001 -- 0.3642)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.1134 (1.1409)  acc1: 55.5556 (60.5809)  acc5: 100.0000 (95.4357)  time: 0.2123 (0.1321 -- 0.5414)  data: 0.0239 (0.0001 -- 0.3642)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 65.145 Acc@5 95.021 loss 1.112
Accuracy of the network on the 482 val images: 65.15%
[2023-08-30 14:25:21,539] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:25:21,541] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:25:21,541] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:25:21,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:25:22,950] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:25:22,950] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 65.15%
Epoch: [29]  [  0/160]  eta: 0:20:43  lr: 0.000023  min_lr: 0.000001  loss: 2.1245 (2.1245)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1568 (8.1568)  time: 7.7741 (7.7741 -- 7.7741)  data: 5.0407 (5.0407 -- 5.0407)  max mem: 16413
Epoch: [29]  [ 20/160]  eta: 0:02:48  lr: 0.000023  min_lr: 0.000001  loss: 2.1440 (2.1139)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2511 (7.7098)  time: 0.8723 (0.5159 -- 2.9761)  data: 0.0024 (0.0002 -- 0.0145)  max mem: 16413
Epoch: [29]  [ 40/160]  eta: 0:02:07  lr: 0.000023  min_lr: 0.000001  loss: 1.9655 (2.0348)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4660 (7.7226)  time: 0.9193 (0.5145 -- 2.9782)  data: 0.0961 (0.0003 -- 1.4769)  max mem: 16413
Epoch: [29]  [ 60/160]  eta: 0:01:37  lr: 0.000023  min_lr: 0.000001  loss: 2.3129 (2.1033)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8078 (7.7859)  time: 0.7975 (0.5227 -- 2.8641)  data: 0.0014 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [29]  [ 80/160]  eta: 0:01:14  lr: 0.000023  min_lr: 0.000001  loss: 2.0273 (2.0939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6513 (7.9714)  time: 0.8182 (0.5290 -- 2.8802)  data: 0.1473 (0.0003 -- 1.4731)  max mem: 16413
[2023-08-30 14:26:41,656] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:26:41,656] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:26:41,657] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:26:41,657] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [29]  [100/160]  eta: 0:00:54  lr: 0.000023  min_lr: 0.000001  loss: 1.9836 (2.0572)  loss_scale: 32768.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5227 (7.8960)  time: 0.8007 (0.5328 -- 2.2070)  data: 0.2187 (0.0004 -- 1.5557)  max mem: 16413
[2023-08-30 14:27:01,160] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4746
[2023-08-30 14:27:01,160] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4746
[2023-08-30 14:27:01,160] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:27:01,160] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:27:01,160] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [29]  [120/160]  eta: 0:00:36  lr: 0.000023  min_lr: 0.000001  loss: 2.0323 (2.0515)  loss_scale: 16384.0000 (19633.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2799 (7.8368)  time: 0.9340 (0.5242 -- 3.3423)  data: 0.3775 (0.0004 -- 2.8048)  max mem: 16413
Epoch: [29]  [140/160]  eta: 0:00:17  lr: 0.000023  min_lr: 0.000001  loss: 1.9776 (2.0376)  loss_scale: 16384.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0416 (7.8737)  time: 0.7280 (0.5232 -- 3.1289)  data: 0.1820 (0.0004 -- 2.6045)  max mem: 16413
Epoch: [29]  [159/160]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000001  loss: 2.0253 (2.0316)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2398 (7.8086)  time: 0.7634 (0.4955 -- 3.6687)  data: 0.2387 (0.0003 -- 3.1043)  max mem: 16413
Epoch: [29] Total time: 0:02:19 (0.8745 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000001  loss: 2.0253 (1.9971)  loss_scale: 16384.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2398 (7.8086)
Val:  [ 0/27]  eta: 0:01:00  loss: 1.1300 (1.1300)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2560 (2.2560 -- 2.2560)  data: 2.0425 (2.0425 -- 2.0425)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.1053 (1.1271)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4053 (0.2009 -- 2.2560)  data: 0.1879 (0.0007 -- 2.0425)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0484 (1.0762)  acc1: 55.5556 (61.9048)  acc5: 100.0000 (96.2963)  time: 0.2201 (0.1715 -- 0.4517)  data: 0.0132 (0.0001 -- 0.2368)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0821 (1.1186)  acc1: 57.1429 (62.2407)  acc5: 100.0000 (95.0207)  time: 0.2018 (0.1326 -- 0.4517)  data: 0.0126 (0.0001 -- 0.2368)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 66.598 Acc@5 94.813 loss 1.093
Accuracy of the network on the 482 val images: 66.60%
[2023-08-30 14:27:50,541] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:27:50,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:27:50,542] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:27:50,542] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:27:51,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:27:51,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.60%
Epoch: [30]  [  0/160]  eta: 0:15:03  lr: 0.000023  min_lr: 0.000001  loss: 2.0440 (2.0440)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3461 (8.3461)  time: 5.6463 (5.6463 -- 5.6463)  data: 4.5303 (4.5303 -- 4.5303)  max mem: 16413
Epoch: [30]  [ 20/160]  eta: 0:02:39  lr: 0.000022  min_lr: 0.000001  loss: 1.9961 (2.0406)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0844 (7.3300)  time: 0.9138 (0.5317 -- 3.4708)  data: 0.1930 (0.0007 -- 2.9173)  max mem: 16413
Epoch: [30]  [ 40/160]  eta: 0:01:57  lr: 0.000022  min_lr: 0.000001  loss: 2.0676 (2.0169)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7138 (7.6014)  time: 0.8171 (0.5258 -- 2.5728)  data: 0.1596 (0.0006 -- 1.2387)  max mem: 16413
Epoch: [30]  [ 60/160]  eta: 0:01:33  lr: 0.000022  min_lr: 0.000001  loss: 1.9515 (1.9999)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3393 (7.8307)  time: 0.8508 (0.5233 -- 2.8944)  data: 0.1716 (0.0004 -- 2.0904)  max mem: 16413
[2023-08-30 14:29:02,761] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:29:02,762] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:29:02,763] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:29:02,763] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [30]  [ 80/160]  eta: 0:01:12  lr: 0.000022  min_lr: 0.000001  loss: 1.9672 (1.9767)  loss_scale: 16384.0000 (17597.6296)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3849 (7.9290)  time: 0.8170 (0.5293 -- 2.7150)  data: 0.0173 (0.0003 -- 0.3145)  max mem: 16413
Epoch: [30]  [100/160]  eta: 0:00:54  lr: 0.000022  min_lr: 0.000001  loss: 1.9151 (1.9763)  loss_scale: 32768.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7437 (7.8420)  time: 0.9010 (0.5199 -- 3.2608)  data: 0.1608 (0.0009 -- 1.4990)  max mem: 16413
[2023-08-30 14:29:25,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4903
[2023-08-30 14:29:25,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 4903
[2023-08-30 14:29:25,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:29:25,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:29:25,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [30]  [120/160]  eta: 0:00:37  lr: 0.000022  min_lr: 0.000001  loss: 2.0322 (1.9866)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9889 (7.8522)  time: 1.0147 (0.5201 -- 4.7001)  data: 0.0876 (0.0004 -- 1.1464)  max mem: 16413
Epoch: [30]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 2.0126 (1.9997)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6331 (7.8920)  time: 0.8778 (0.5162 -- 4.2906)  data: 0.0010 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [30]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.8552 (2.0023)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2240 (7.8480)  time: 0.6391 (0.4961 -- 1.8351)  data: 0.0007 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [30] Total time: 0:02:21 (0.8860 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.8552 (1.9970)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2240 (7.8480)
Val:  [ 0/27]  eta: 0:01:02  loss: 1.0532 (1.0532)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3237 (2.3237 -- 2.3237)  data: 2.1029 (2.1029 -- 2.1029)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.0532 (1.1005)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (95.9596)  time: 0.4084 (0.2018 -- 2.3237)  data: 0.1921 (0.0007 -- 2.1029)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0313 (1.0612)  acc1: 66.6667 (62.4339)  acc5: 100.0000 (96.2963)  time: 0.2150 (0.1695 -- 0.2882)  data: 0.0052 (0.0001 -- 0.0535)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0631 (1.1101)  acc1: 57.1429 (61.4108)  acc5: 100.0000 (95.0207)  time: 0.2013 (0.1332 -- 0.2882)  data: 0.0050 (0.0001 -- 0.0535)  max mem: 16413
Val: Total time: 0:00:07 (0.2819 s / it)
* Acc@1 65.145 Acc@5 95.021 loss 1.090
Accuracy of the network on the 482 val images: 65.15%
Max accuracy: 66.60%
Epoch: [31]  [  0/160]  eta: 0:18:22  lr: 0.000022  min_lr: 0.000001  loss: 1.7041 (1.7041)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4041 (8.4041)  time: 6.8907 (6.8907 -- 6.8907)  data: 6.3749 (6.3749 -- 6.3749)  max mem: 16413
Epoch: [31]  [ 20/160]  eta: 0:02:51  lr: 0.000022  min_lr: 0.000001  loss: 1.9190 (1.9395)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1035 (8.0683)  time: 0.9391 (0.5256 -- 2.7224)  data: 0.3915 (0.0004 -- 2.2123)  max mem: 16413
[2023-08-30 14:31:01,484] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=23, lr=[5.323438495058938e-07, 5.323438495058938e-07, 7.097917993411918e-07, 7.097917993411918e-07, 9.463890657882557e-07, 9.463890657882557e-07, 1.2618520877176744e-06, 1.2618520877176744e-06, 1.6824694502902324e-06, 1.6824694502902324e-06, 2.2432926003869768e-06, 2.2432926003869768e-06, 2.991056800515969e-06, 2.991056800515969e-06, 3.988075734021291e-06, 3.988075734021291e-06, 5.317434312028389e-06, 5.317434312028389e-06, 7.089912416037852e-06, 7.089912416037852e-06, 9.453216554717135e-06, 9.453216554717135e-06, 1.2604288739622848e-05, 1.2604288739622848e-05, 1.680571831949713e-05, 1.680571831949713e-05, 2.2407624425996174e-05, 2.2407624425996174e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 14:31:01,485] [INFO] [timer.py:260:stop] epoch=0/micro_step=5000/global_step=5000, RunningAvgSamplesPerSec=17.592361356906878, CurrSamplesPerSec=22.105164964219178, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [31]  [ 40/160]  eta: 0:01:59  lr: 0.000022  min_lr: 0.000001  loss: 2.0556 (1.9955)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3031 (7.7994)  time: 0.7493 (0.5333 -- 2.8139)  data: 0.1990 (0.0002 -- 2.2654)  max mem: 16413
Epoch: [31]  [ 60/160]  eta: 0:01:34  lr: 0.000022  min_lr: 0.000001  loss: 1.8235 (1.9663)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2276 (7.7321)  time: 0.8420 (0.5273 -- 3.0124)  data: 0.2909 (0.0004 -- 2.4498)  max mem: 16413
[2023-08-30 14:31:29,368] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:31:29,368] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:31:29,369] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:31:29,369] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [31]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 2.0338 (1.9723)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3100 (7.4948)  time: 0.8927 (0.5224 -- 2.3283)  data: 0.3466 (0.0002 -- 1.7970)  max mem: 16413
[2023-08-30 14:31:40,549] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5046
[2023-08-30 14:31:40,549] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:31:40,549] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5046
[2023-08-30 14:31:40,549] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:31:40,549] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [31]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.1455 (1.9978)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4103 (7.5260)  time: 0.8625 (0.5049 -- 5.7497)  data: 0.3223 (0.0004 -- 5.2384)  max mem: 16413
Epoch: [31]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9258 (1.9995)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6675 (7.4152)  time: 0.8985 (0.5263 -- 3.3813)  data: 0.3464 (0.0003 -- 2.8288)  max mem: 16413
Epoch: [31]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 1.8347 (1.9822)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9478 (7.5449)  time: 0.7802 (0.5352 -- 2.8957)  data: 0.2340 (0.0004 -- 2.3634)  max mem: 16413
Epoch: [31]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9092 (1.9764)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1533 (7.4601)  time: 0.6636 (0.4945 -- 2.3882)  data: 0.1449 (0.0002 -- 1.8802)  max mem: 16413
Epoch: [31] Total time: 0:02:18 (0.8685 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9092 (1.9844)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1533 (7.4601)
Val:  [ 0/27]  eta: 0:01:01  loss: 1.1017 (1.1017)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2784 (2.2784 -- 2.2784)  data: 2.0674 (2.0674 -- 2.0674)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 1.0615 (1.0953)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (94.9495)  time: 0.4108 (0.1965 -- 2.2784)  data: 0.1894 (0.0007 -- 2.0674)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 1.0211 (1.0631)  acc1: 66.6667 (60.8466)  acc5: 100.0000 (95.2381)  time: 0.2180 (0.1686 -- 0.4010)  data: 0.0106 (0.0001 -- 0.1905)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0824 (1.1095)  acc1: 57.1429 (60.1660)  acc5: 100.0000 (94.1909)  time: 0.2009 (0.1323 -- 0.4010)  data: 0.0102 (0.0001 -- 0.1905)  max mem: 16413
Val: Total time: 0:00:07 (0.2824 s / it)
* Acc@1 65.560 Acc@5 94.813 loss 1.081
Accuracy of the network on the 482 val images: 65.56%
Max accuracy: 66.60%
Epoch: [32]  [  0/160]  eta: 0:21:10  lr: 0.000022  min_lr: 0.000001  loss: 2.5421 (2.5421)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3890 (9.3890)  time: 7.9423 (7.9423 -- 7.9423)  data: 6.6824 (6.6824 -- 6.6824)  max mem: 16413
Epoch: [32]  [ 20/160]  eta: 0:02:47  lr: 0.000022  min_lr: 0.000001  loss: 1.9108 (1.9254)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3962 (8.0063)  time: 0.8626 (0.5254 -- 3.6374)  data: 0.1974 (0.0008 -- 2.7667)  max mem: 16413
Epoch: [32]  [ 40/160]  eta: 0:02:07  lr: 0.000022  min_lr: 0.000001  loss: 1.9990 (1.9426)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1884 (7.5641)  time: 0.9253 (0.5224 -- 4.0934)  data: 0.2571 (0.0002 -- 3.5515)  max mem: 16413
[2023-08-30 14:33:41,789] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:33:41,789] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:33:41,792] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:33:41,793] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:33:43,619] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5176
[2023-08-30 14:33:43,619] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5176
[2023-08-30 14:33:43,661] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:33:43,661] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:33:43,661] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [32]  [ 60/160]  eta: 0:01:34  lr: 0.000022  min_lr: 0.000001  loss: 1.8730 (1.9601)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8865 (7.4671)  time: 0.7093 (0.5231 -- 2.4382)  data: 0.1622 (0.0003 -- 1.9028)  max mem: 16413
Epoch: [32]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 1.9787 (1.9567)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3820 (7.4801)  time: 0.8681 (0.5148 -- 3.8540)  data: 0.1252 (0.0004 -- 0.8186)  max mem: 16413
Epoch: [32]  [100/160]  eta: 0:00:54  lr: 0.000022  min_lr: 0.000001  loss: 2.0937 (1.9764)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3612 (7.6828)  time: 0.8052 (0.5268 -- 1.9256)  data: 0.2575 (0.0002 -- 1.4089)  max mem: 16413
Epoch: [32]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.1099 (2.0015)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4435 (7.7421)  time: 0.9310 (0.5309 -- 3.2415)  data: 0.2912 (0.0008 -- 2.7288)  max mem: 16413
Epoch: [32]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 2.0225 (2.0061)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2380 (7.8331)  time: 0.7744 (0.5259 -- 2.8138)  data: 0.0832 (0.0001 -- 1.0647)  max mem: 16413
Epoch: [32]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9919 (2.0025)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6225 (7.7189)  time: 0.7086 (0.4956 -- 2.1740)  data: 0.0323 (0.0003 -- 0.3300)  max mem: 16413
Epoch: [32] Total time: 0:02:19 (0.8697 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9919 (1.9815)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6225 (7.7189)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.0174 (1.0174)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5752 (2.5752 -- 2.5752)  data: 2.3415 (2.3415 -- 2.3415)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0527 (1.0880)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (96.9697)  time: 0.4476 (0.2044 -- 2.5752)  data: 0.2229 (0.0006 -- 2.3415)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9768 (1.0559)  acc1: 66.6667 (64.5503)  acc5: 100.0000 (96.8254)  time: 0.2148 (0.1695 -- 0.3270)  data: 0.0074 (0.0001 -- 0.0925)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0824 (1.1058)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (95.4357)  time: 0.1959 (0.1327 -- 0.3270)  data: 0.0070 (0.0001 -- 0.0925)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 66.805 Acc@5 95.436 loss 1.073
Accuracy of the network on the 482 val images: 66.80%
[2023-08-30 14:35:14,999] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:35:15,000] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:35:15,000] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:35:15,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:35:16,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:35:16,411] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 66.80%
Epoch: [33]  [  0/160]  eta: 0:17:17  lr: 0.000022  min_lr: 0.000001  loss: 2.1802 (2.1802)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4843 (6.4843)  time: 6.4829 (6.4829 -- 6.4829)  data: 5.9407 (5.9407 -- 5.9407)  max mem: 16413
Epoch: [33]  [ 20/160]  eta: 0:02:55  lr: 0.000022  min_lr: 0.000001  loss: 2.0398 (1.9867)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9638 (6.9541)  time: 0.9954 (0.5248 -- 3.2910)  data: 0.1035 (0.0005 -- 0.9947)  max mem: 16413
[2023-08-30 14:35:46,202] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:35:46,202] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:35:46,203] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:35:46,203] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [33]  [ 40/160]  eta: 0:02:05  lr: 0.000022  min_lr: 0.000001  loss: 1.7987 (1.9434)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0048 (7.0224)  time: 0.8160 (0.5237 -- 2.3386)  data: 0.2001 (0.0003 -- 1.8266)  max mem: 16413
Epoch: [33]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 2.1128 (1.9917)  loss_scale: 32768.0000 (26053.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4234 (7.2873)  time: 0.8339 (0.5202 -- 3.1060)  data: 0.0271 (0.0007 -- 0.5080)  max mem: 16413
Epoch: [33]  [ 80/160]  eta: 0:01:13  lr: 0.000022  min_lr: 0.000001  loss: 2.0679 (2.0049)  loss_scale: 32768.0000 (27711.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2871 (7.6696)  time: 0.7446 (0.5333 -- 1.9324)  data: 0.0722 (0.0003 -- 1.4068)  max mem: 16413
[2023-08-30 14:36:32,982] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5361
[2023-08-30 14:36:32,982] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:36:32,982] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5361
[2023-08-30 14:36:32,982] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:36:32,982] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [33]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9405 (1.9968)  loss_scale: 16384.0000 (25468.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4017 (7.7080)  time: 0.9158 (0.5341 -- 2.2678)  data: 0.1427 (0.0005 -- 1.3396)  max mem: 16413
Epoch: [33]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.0658 (2.0011)  loss_scale: 16384.0000 (23966.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6184 (7.7779)  time: 0.8245 (0.5307 -- 2.3267)  data: 0.1347 (0.0005 -- 1.7634)  max mem: 16413
Epoch: [33]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9581 (1.9809)  loss_scale: 16384.0000 (22891.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1514 (7.8080)  time: 0.9985 (0.5232 -- 4.9891)  data: 0.0178 (0.0002 -- 0.3364)  max mem: 16413
Epoch: [33]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9738 (1.9808)  loss_scale: 16384.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5202 (7.7452)  time: 0.6562 (0.4944 -- 2.1769)  data: 0.0009 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [33] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9738 (1.9909)  loss_scale: 16384.0000 (22118.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5202 (7.7452)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.9960 (0.9960)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2856 (2.2856 -- 2.2856)  data: 2.0498 (2.0498 -- 2.0498)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9960 (1.0763)  acc1: 55.5556 (60.6061)  acc5: 100.0000 (96.9697)  time: 0.4076 (0.1943 -- 2.2856)  data: 0.1872 (0.0005 -- 2.0498)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9297 (1.0316)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (96.8254)  time: 0.2183 (0.1690 -- 0.3512)  data: 0.0090 (0.0001 -- 0.1671)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0594 (1.0809)  acc1: 57.1429 (63.0705)  acc5: 100.0000 (95.8506)  time: 0.2046 (0.1327 -- 0.3512)  data: 0.0088 (0.0001 -- 0.1671)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 66.390 Acc@5 95.643 loss 1.056
Accuracy of the network on the 482 val images: 66.39%
Max accuracy: 66.80%
Epoch: [34]  [  0/160]  eta: 0:19:51  lr: 0.000022  min_lr: 0.000001  loss: 2.0060 (2.0060)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6186 (11.6186)  time: 7.4482 (7.4482 -- 7.4482)  data: 6.9372 (6.9372 -- 6.9372)  max mem: 16413
Epoch: [34]  [ 20/160]  eta: 0:02:48  lr: 0.000022  min_lr: 0.000001  loss: 2.0215 (1.9546)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7624 (7.7993)  time: 0.8929 (0.5298 -- 4.8044)  data: 0.3213 (0.0002 -- 4.2548)  max mem: 16413
Epoch: [34]  [ 40/160]  eta: 0:02:02  lr: 0.000022  min_lr: 0.000001  loss: 2.0460 (1.9787)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9184 (8.0029)  time: 0.8244 (0.5260 -- 3.7169)  data: 0.2168 (0.0004 -- 3.1599)  max mem: 16413
[2023-08-30 14:38:37,011] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:38:37,011] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:38:37,011] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:38:37,011] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [34]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 2.0383 (1.9847)  loss_scale: 32768.0000 (19338.4918)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0701 (7.8040)  time: 0.8789 (0.5170 -- 4.0344)  data: 0.3129 (0.0001 -- 3.5072)  max mem: 16413
[2023-08-30 14:38:50,569] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5507
[2023-08-30 14:38:50,569] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:38:50,569] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5507
[2023-08-30 14:38:50,570] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:38:50,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [34]  [ 80/160]  eta: 0:01:13  lr: 0.000022  min_lr: 0.000001  loss: 2.0996 (2.0006)  loss_scale: 16384.0000 (19822.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9009 (7.9358)  time: 0.7486 (0.5128 -- 2.3150)  data: 0.2008 (0.0003 -- 1.7874)  max mem: 16413
Epoch: [34]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.8639 (1.9905)  loss_scale: 16384.0000 (19141.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2435 (7.9740)  time: 0.9779 (0.5152 -- 4.3762)  data: 0.2627 (0.0004 -- 3.8547)  max mem: 16413
Epoch: [34]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9242 (1.9740)  loss_scale: 16384.0000 (18685.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9287 (7.9487)  time: 0.8995 (0.5310 -- 3.4930)  data: 0.3081 (0.0005 -- 2.9368)  max mem: 16413
Epoch: [34]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9628 (1.9738)  loss_scale: 16384.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3575 (8.0338)  time: 0.7514 (0.5265 -- 3.3540)  data: 0.0335 (0.0005 -- 0.6352)  max mem: 16413
Epoch: [34]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0700 (1.9750)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5652 (8.0565)  time: 0.7261 (0.4963 -- 3.3209)  data: 0.0008 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [34] Total time: 0:02:20 (0.8809 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0700 (1.9854)  loss_scale: 16384.0000 (18124.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5652 (8.0565)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.9986 (0.9986)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2163 (2.2163 -- 2.2163)  data: 2.0269 (2.0269 -- 2.0269)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9986 (1.0688)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (96.9697)  time: 0.4110 (0.1944 -- 2.2163)  data: 0.2006 (0.0008 -- 2.0269)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9400 (1.0250)  acc1: 55.5556 (62.4339)  acc5: 100.0000 (96.8254)  time: 0.2239 (0.1690 -- 0.3911)  data: 0.0190 (0.0001 -- 0.1969)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 1.0160 (1.0657)  acc1: 66.6667 (63.0705)  acc5: 100.0000 (95.8506)  time: 0.2111 (0.1334 -- 0.3911)  data: 0.0187 (0.0001 -- 0.1969)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 66.598 Acc@5 95.436 loss 1.037
Accuracy of the network on the 482 val images: 66.60%
Max accuracy: 66.80%
Epoch: [35]  [  0/160]  eta: 0:17:10  lr: 0.000022  min_lr: 0.000001  loss: 2.0268 (2.0268)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5518 (6.5518)  time: 6.4414 (6.4414 -- 6.4414)  data: 5.6481 (5.6481 -- 5.6481)  max mem: 16413
Epoch: [35]  [ 20/160]  eta: 0:02:31  lr: 0.000022  min_lr: 0.000001  loss: 1.9280 (1.8786)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0742 (7.8095)  time: 0.8170 (0.5306 -- 2.9142)  data: 0.1820 (0.0002 -- 2.3745)  max mem: 16413
[2023-08-30 14:40:51,899] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:40:51,899] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:40:51,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:40:51,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:40:53,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5639
[2023-08-30 14:40:53,523] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5639
[2023-08-30 14:40:53,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:40:53,524] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:40:53,524] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [35]  [ 40/160]  eta: 0:02:03  lr: 0.000022  min_lr: 0.000001  loss: 2.0961 (1.9798)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1578 (8.0392)  time: 0.9664 (0.5247 -- 4.7373)  data: 0.4125 (0.0003 -- 4.1960)  max mem: 16413
Epoch: [35]  [ 60/160]  eta: 0:01:34  lr: 0.000022  min_lr: 0.000001  loss: 1.9720 (1.9809)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2009 (7.8974)  time: 0.7897 (0.5341 -- 2.7036)  data: 0.1490 (0.0005 -- 1.2222)  max mem: 16413
Epoch: [35]  [ 80/160]  eta: 0:01:14  lr: 0.000022  min_lr: 0.000001  loss: 1.8413 (1.9607)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8836 (7.8218)  time: 0.8604 (0.5266 -- 2.7268)  data: 0.1888 (0.0002 -- 1.4168)  max mem: 16413
Epoch: [35]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.0605 (1.9724)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6642 (7.8783)  time: 0.9038 (0.5350 -- 2.1423)  data: 0.2137 (0.0002 -- 1.4940)  max mem: 16413
Epoch: [35]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9371 (1.9711)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0141 (7.8981)  time: 0.8052 (0.5258 -- 2.3568)  data: 0.2544 (0.0002 -- 1.8084)  max mem: 16413
Epoch: [35]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 1.8780 (1.9714)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7787 (7.7847)  time: 0.8632 (0.5187 -- 3.5025)  data: 0.3191 (0.0005 -- 2.9658)  max mem: 16413
Epoch: [35]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 1.9744 (1.9732)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5499 (7.8248)  time: 0.6735 (0.4953 -- 3.3516)  data: 0.1490 (0.0002 -- 2.8423)  max mem: 16413
Epoch: [35] Total time: 0:02:19 (0.8720 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 1.9744 (1.9634)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5499 (7.8248)
Val:  [ 0/27]  eta: 0:01:09  loss: 1.0211 (1.0211)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.5821 (2.5821 -- 2.5821)  data: 2.3453 (2.3453 -- 2.3453)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 1.0146 (1.0654)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (96.9697)  time: 0.4281 (0.1946 -- 2.5821)  data: 0.2150 (0.0006 -- 2.3453)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9277 (1.0160)  acc1: 55.5556 (62.4339)  acc5: 100.0000 (96.8254)  time: 0.2152 (0.1693 -- 0.4061)  data: 0.0125 (0.0001 -- 0.2275)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9997 (1.0553)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (96.2656)  time: 0.2018 (0.1333 -- 0.4061)  data: 0.0123 (0.0001 -- 0.2275)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 67.012 Acc@5 95.851 loss 1.025
Accuracy of the network on the 482 val images: 67.01%
[2023-08-30 14:42:41,891] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:42:41,893] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:42:41,893] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:42:41,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:42:43,615] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:42:43,616] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.01%
Epoch: [36]  [  0/160]  eta: 0:19:23  lr: 0.000022  min_lr: 0.000001  loss: 2.1048 (2.1048)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5389 (9.5389)  time: 7.2742 (7.2742 -- 7.2742)  data: 5.0998 (5.0998 -- 5.0998)  max mem: 16413
[2023-08-30 14:42:56,767] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:42:56,767] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:42:56,773] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:42:56,774] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [36]  [ 20/160]  eta: 0:02:47  lr: 0.000022  min_lr: 0.000001  loss: 1.8860 (1.8498)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8704 (7.6017)  time: 0.8954 (0.5290 -- 3.5035)  data: 0.2697 (0.0005 -- 2.9703)  max mem: 16413
Epoch: [36]  [ 40/160]  eta: 0:02:02  lr: 0.000022  min_lr: 0.000001  loss: 2.0279 (1.9235)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6040 (8.3603)  time: 0.8362 (0.5295 -- 2.9612)  data: 0.2935 (0.0002 -- 2.4487)  max mem: 16413
[2023-08-30 14:43:30,136] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5808
[2023-08-30 14:43:30,136] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5808
[2023-08-30 14:43:30,136] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:43:30,136] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:43:30,136] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [36]  [ 60/160]  eta: 0:01:38  lr: 0.000022  min_lr: 0.000001  loss: 1.7320 (1.8819)  loss_scale: 16384.0000 (27127.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7914 (8.3095)  time: 0.8969 (0.5144 -- 4.5912)  data: 0.3524 (0.0006 -- 4.0717)  max mem: 16413
Epoch: [36]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 2.0147 (1.9154)  loss_scale: 16384.0000 (24474.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0721 (8.1210)  time: 0.8462 (0.5265 -- 3.6711)  data: 0.2933 (0.0003 -- 3.1496)  max mem: 16413
Epoch: [36]  [100/160]  eta: 0:00:54  lr: 0.000022  min_lr: 0.000001  loss: 1.9503 (1.9297)  loss_scale: 16384.0000 (22872.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0663 (8.2228)  time: 0.7819 (0.5276 -- 2.3249)  data: 0.1584 (0.0004 -- 1.7869)  max mem: 16413
Epoch: [36]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 2.1268 (1.9501)  loss_scale: 16384.0000 (21800.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9068 (8.1405)  time: 0.8892 (0.5269 -- 3.9530)  data: 0.1106 (0.0004 -- 1.8036)  max mem: 16413
Epoch: [36]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9066 (1.9441)  loss_scale: 16384.0000 (21031.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3423 (8.2957)  time: 0.9036 (0.5270 -- 3.8811)  data: 0.3471 (0.0002 -- 3.3094)  max mem: 16413
Epoch: [36]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0328 (1.9553)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8366 (8.3003)  time: 0.5922 (0.4974 -- 1.7109)  data: 0.0663 (0.0002 -- 1.1623)  max mem: 16413
Epoch: [36] Total time: 0:02:19 (0.8724 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0328 (1.9405)  loss_scale: 16384.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8366 (8.3003)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.9659 (0.9659)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3419 (2.3419 -- 2.3419)  data: 2.1175 (2.1175 -- 2.1175)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9659 (1.0492)  acc1: 55.5556 (59.5960)  acc5: 100.0000 (95.9596)  time: 0.4350 (0.2059 -- 2.3419)  data: 0.2199 (0.0006 -- 2.1175)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9247 (1.0008)  acc1: 55.5556 (62.4339)  acc5: 100.0000 (96.2963)  time: 0.2200 (0.1693 -- 0.4913)  data: 0.0164 (0.0001 -- 0.2753)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9735 (1.0467)  acc1: 55.5556 (61.4108)  acc5: 100.0000 (95.4357)  time: 0.2058 (0.1325 -- 0.4913)  data: 0.0158 (0.0001 -- 0.2753)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 66.390 Acc@5 95.436 loss 1.016
Accuracy of the network on the 482 val images: 66.39%
Max accuracy: 67.01%
Epoch: [37]  [  0/160]  eta: 0:21:17  lr: 0.000022  min_lr: 0.000001  loss: 2.1447 (2.1447)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8221 (9.8221)  time: 7.9835 (7.9835 -- 7.9835)  data: 6.4040 (6.4040 -- 6.4040)  max mem: 16413
[2023-08-30 14:45:35,761] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:45:35,761] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:45:35,761] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:45:35,761] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [37]  [ 20/160]  eta: 0:02:55  lr: 0.000022  min_lr: 0.000001  loss: 2.0062 (2.0295)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1738 (7.4889)  time: 0.9207 (0.5079 -- 4.9213)  data: 0.3805 (0.0003 -- 4.4024)  max mem: 16413
[2023-08-30 14:45:56,344] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5960
[2023-08-30 14:45:56,344] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:45:56,344] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 5960
[2023-08-30 14:45:56,345] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:45:56,345] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [37]  [ 40/160]  eta: 0:02:12  lr: 0.000022  min_lr: 0.000001  loss: 1.9764 (2.0005)  loss_scale: 32768.0000 (25575.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2529 (7.5005)  time: 0.9504 (0.5169 -- 4.1055)  data: 0.4133 (0.0002 -- 3.5785)  max mem: 16413
Epoch: [37]  [ 60/160]  eta: 0:01:39  lr: 0.000022  min_lr: 0.000001  loss: 2.0704 (2.0269)  loss_scale: 16384.0000 (22561.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5356 (8.0504)  time: 0.7790 (0.5160 -- 3.2066)  data: 0.2328 (0.0003 -- 2.6828)  max mem: 16413
[2023-08-30 14:46:24,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=30, lr=[5.196002051909714e-07, 5.196002051909714e-07, 6.928002735879619e-07, 6.928002735879619e-07, 9.237336981172825e-07, 9.237336981172825e-07, 1.2316449308230434e-06, 1.2316449308230434e-06, 1.642193241097391e-06, 1.642193241097391e-06, 2.189590988129855e-06, 2.189590988129855e-06, 2.9194546508398063e-06, 2.9194546508398063e-06, 3.892606201119742e-06, 3.892606201119742e-06, 5.190141601492989e-06, 5.190141601492989e-06, 6.920188801990652e-06, 6.920188801990652e-06, 9.226918402654204e-06, 9.226918402654204e-06, 1.2302557870205604e-05, 1.2302557870205604e-05, 1.640341049360747e-05, 1.640341049360747e-05, 2.187121399147663e-05, 2.187121399147663e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 14:46:24,577] [INFO] [timer.py:260:stop] epoch=0/micro_step=6000/global_step=6000, RunningAvgSamplesPerSec=17.877243979245907, CurrSamplesPerSec=21.577737545620224, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [37]  [ 80/160]  eta: 0:01:16  lr: 0.000022  min_lr: 0.000001  loss: 1.9997 (2.0056)  loss_scale: 16384.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8764 (7.9596)  time: 0.8404 (0.5288 -- 4.1685)  data: 0.2903 (0.0004 -- 3.6373)  max mem: 16413
Epoch: [37]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9900 (1.9980)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3136 (7.9684)  time: 0.8004 (0.5249 -- 3.1480)  data: 0.2504 (0.0007 -- 2.6085)  max mem: 16413
Epoch: [37]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.8337 (1.9752)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5682 (7.9097)  time: 0.8412 (0.5289 -- 2.4811)  data: 0.2820 (0.0005 -- 1.9344)  max mem: 16413
Epoch: [37]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 2.0401 (1.9755)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2679 (7.8273)  time: 0.8451 (0.5174 -- 3.1494)  data: 0.2177 (0.0007 -- 2.3080)  max mem: 16413
Epoch: [37]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0898 (1.9802)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7248 (7.7226)  time: 0.7301 (0.4950 -- 3.1465)  data: 0.1769 (0.0002 -- 2.6223)  max mem: 16413
Epoch: [37] Total time: 0:02:21 (0.8852 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0898 (1.9817)  loss_scale: 16384.0000 (18739.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7248 (7.7226)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.9450 (0.9450)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.3474 (2.3474 -- 2.3474)  data: 2.0841 (2.0841 -- 2.0841)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.9450 (1.0424)  acc1: 55.5556 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4099 (0.2019 -- 2.3474)  data: 0.1904 (0.0005 -- 2.0841)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8852 (0.9928)  acc1: 55.5556 (64.5503)  acc5: 100.0000 (96.2963)  time: 0.2172 (0.1697 -- 0.3781)  data: 0.0103 (0.0001 -- 0.1810)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9744 (1.0309)  acc1: 55.5556 (63.4855)  acc5: 100.0000 (95.8506)  time: 0.1993 (0.1329 -- 0.3781)  data: 0.0100 (0.0001 -- 0.1810)  max mem: 16413
Val: Total time: 0:00:07 (0.2843 s / it)
* Acc@1 66.805 Acc@5 95.643 loss 0.997
Accuracy of the network on the 482 val images: 66.80%
Max accuracy: 67.01%
Epoch: [38]  [  0/160]  eta: 0:16:24  lr: 0.000022  min_lr: 0.000001  loss: 2.2382 (2.2382)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5092 (10.5092)  time: 6.1516 (6.1516 -- 6.1516)  data: 5.5785 (5.5785 -- 5.5785)  max mem: 16413
[2023-08-30 14:47:54,730] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:47:54,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:47:54,731] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:47:54,731] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [38]  [ 20/160]  eta: 0:02:35  lr: 0.000022  min_lr: 0.000001  loss: 1.8305 (1.9170)  loss_scale: 32768.0000 (25746.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7539 (7.5977)  time: 0.8613 (0.5303 -- 1.9678)  data: 0.1438 (0.0008 -- 0.8596)  max mem: 16413
Epoch: [38]  [ 40/160]  eta: 0:01:58  lr: 0.000022  min_lr: 0.000001  loss: 1.9170 (1.8953)  loss_scale: 32768.0000 (29171.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7605 (7.4118)  time: 0.8615 (0.5362 -- 2.5759)  data: 0.2304 (0.0001 -- 2.0622)  max mem: 16413
[2023-08-30 14:48:37,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6138
[2023-08-30 14:48:37,991] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6138
[2023-08-30 14:48:37,991] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:48:37,991] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:48:37,991] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [38]  [ 60/160]  eta: 0:01:36  lr: 0.000022  min_lr: 0.000001  loss: 1.7482 (1.8646)  loss_scale: 32768.0000 (29544.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5439 (7.5962)  time: 0.9057 (0.5214 -- 3.4127)  data: 0.3384 (0.0002 -- 2.9006)  max mem: 16413
Epoch: [38]  [ 80/160]  eta: 0:01:13  lr: 0.000022  min_lr: 0.000001  loss: 2.1302 (1.8976)  loss_scale: 16384.0000 (26295.3086)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0452 (7.9423)  time: 0.8027 (0.5299 -- 3.3786)  data: 0.2602 (0.0003 -- 2.8654)  max mem: 16413
Epoch: [38]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9756 (1.9226)  loss_scale: 16384.0000 (24332.6733)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0134 (7.8650)  time: 0.9108 (0.5240 -- 4.0551)  data: 0.3529 (0.0004 -- 3.5176)  max mem: 16413
Epoch: [38]  [120/160]  eta: 0:00:35  lr: 0.000022  min_lr: 0.000001  loss: 2.1139 (1.9474)  loss_scale: 16384.0000 (23018.8430)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2603 (8.0146)  time: 0.7506 (0.5212 -- 3.0432)  data: 0.2008 (0.0003 -- 2.4888)  max mem: 16413
Epoch: [38]  [140/160]  eta: 0:00:17  lr: 0.000022  min_lr: 0.000001  loss: 1.8653 (1.9380)  loss_scale: 16384.0000 (22077.7305)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5148 (7.9729)  time: 0.9075 (0.5235 -- 3.1931)  data: 0.3593 (0.0004 -- 2.6631)  max mem: 16413
[2023-08-30 14:49:59,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6238
[2023-08-30 14:49:59,097] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6238
[2023-08-30 14:49:59,097] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 14:49:59,097] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 14:49:59,097] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [38]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0301 (1.9433)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5278 (7.9209)  time: 0.6808 (0.4827 -- 2.7233)  data: 0.1485 (0.0002 -- 2.1819)  max mem: 16413
Epoch: [38] Total time: 0:02:19 (0.8703 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0301 (1.9495)  loss_scale: 16384.0000 (21299.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5278 (7.9209)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.9895 (0.9895)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.4456 (2.4456 -- 2.4456)  data: 2.1468 (2.1468 -- 2.1468)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9895 (1.0328)  acc1: 55.5556 (58.5859)  acc5: 100.0000 (96.9697)  time: 0.4311 (0.1935 -- 2.4456)  data: 0.2084 (0.0003 -- 2.1468)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9271 (0.9832)  acc1: 55.5556 (62.4339)  acc5: 100.0000 (96.8254)  time: 0.2199 (0.1697 -- 0.3871)  data: 0.0171 (0.0001 -- 0.1930)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9551 (1.0188)  acc1: 57.1429 (62.6556)  acc5: 100.0000 (96.2656)  time: 0.2070 (0.1334 -- 0.3871)  data: 0.0168 (0.0001 -- 0.1930)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 66.805 Acc@5 96.266 loss 0.997
Accuracy of the network on the 482 val images: 66.80%
Max accuracy: 67.01%
Epoch: [39]  [  0/160]  eta: 0:22:53  lr: 0.000022  min_lr: 0.000001  loss: 2.0095 (2.0095)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2015 (8.2015)  time: 8.5842 (8.5842 -- 8.5842)  data: 6.3010 (6.3010 -- 6.3010)  max mem: 16413
Epoch: [39]  [ 20/160]  eta: 0:02:57  lr: 0.000022  min_lr: 0.000001  loss: 1.9281 (2.0129)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2830 (7.7368)  time: 0.9038 (0.5240 -- 5.6725)  data: 0.3480 (0.0002 -- 5.1312)  max mem: 16413
Epoch: [39]  [ 40/160]  eta: 0:02:09  lr: 0.000022  min_lr: 0.000001  loss: 1.9734 (1.9844)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9902 (7.7600)  time: 0.8857 (0.5199 -- 3.5224)  data: 0.3379 (0.0002 -- 2.9819)  max mem: 16413
Epoch: [39]  [ 60/160]  eta: 0:01:37  lr: 0.000022  min_lr: 0.000001  loss: 1.9351 (1.9831)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2980 (7.7464)  time: 0.7464 (0.5204 -- 2.7566)  data: 0.1874 (0.0003 -- 2.2467)  max mem: 16413
Epoch: [39]  [ 80/160]  eta: 0:01:15  lr: 0.000022  min_lr: 0.000001  loss: 2.1330 (2.0113)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3802 (7.7145)  time: 0.8740 (0.5317 -- 3.8106)  data: 0.0155 (0.0003 -- 0.2798)  max mem: 16413
Epoch: [39]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 2.0295 (2.0135)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9865 (7.7976)  time: 0.8251 (0.5184 -- 3.2662)  data: 0.0411 (0.0004 -- 0.6911)  max mem: 16413
Epoch: [39]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9625 (1.9997)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5501 (7.8621)  time: 0.8834 (0.5303 -- 2.8303)  data: 0.0346 (0.0005 -- 0.4926)  max mem: 16413
[2023-08-30 14:52:04,229] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:52:04,229] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 14:52:04,231] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:52:04,232] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [39]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 2.0740 (1.9975)  loss_scale: 16384.0000 (9005.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3825 (7.7794)  time: 0.8651 (0.5271 -- 4.8259)  data: 0.0060 (0.0004 -- 0.0954)  max mem: 16413
Epoch: [39]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0000 (1.9871)  loss_scale: 16384.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3192 (7.7594)  time: 0.6651 (0.4983 -- 2.2714)  data: 0.0008 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [39] Total time: 0:02:21 (0.8815 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0000 (1.9643)  loss_scale: 16384.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3192 (7.7594)
[2023-08-30 14:52:28,480] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-39 is about to be saved!
[2023-08-30 14:52:28,482] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt
[2023-08-30 14:52:28,482] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt...
[2023-08-30 14:52:28,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
[2023-08-30 14:52:29,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-39/mp_rank_00_model_states.pt.
[2023-08-30 14:52:29,361] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-39 is ready now!
Val:  [ 0/27]  eta: 0:01:00  loss: 0.8687 (0.8687)  acc1: 55.5556 (55.5556)  acc5: 100.0000 (100.0000)  time: 2.2492 (2.2492 -- 2.2492)  data: 2.0532 (2.0532 -- 2.0532)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8687 (1.0217)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4134 (0.2031 -- 2.2492)  data: 0.1981 (0.0008 -- 2.0532)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8549 (0.9799)  acc1: 55.5556 (64.0212)  acc5: 100.0000 (96.2963)  time: 0.2246 (0.1698 -- 0.3551)  data: 0.0164 (0.0001 -- 0.1161)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9933 (1.0216)  acc1: 55.5556 (62.6556)  acc5: 100.0000 (95.0207)  time: 0.2097 (0.1326 -- 0.3551)  data: 0.0161 (0.0001 -- 0.1161)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 67.427 Acc@5 95.228 loss 0.990
Accuracy of the network on the 482 val images: 67.43%
[2023-08-30 14:52:37,138] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:52:37,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:52:37,139] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:52:37,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:52:38,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:52:38,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 67.43%
Epoch: [40]  [  0/160]  eta: 0:22:09  lr: 0.000022  min_lr: 0.000001  loss: 1.3545 (1.3545)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7882 (6.7882)  time: 8.3082 (8.3082 -- 8.3082)  data: 7.7757 (7.7757 -- 7.7757)  max mem: 16413
Epoch: [40]  [ 20/160]  eta: 0:02:53  lr: 0.000022  min_lr: 0.000001  loss: 1.9356 (1.9653)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0033 (7.2258)  time: 0.8893 (0.5210 -- 5.5447)  data: 0.3415 (0.0004 -- 5.0290)  max mem: 16413
Epoch: [40]  [ 40/160]  eta: 0:02:10  lr: 0.000022  min_lr: 0.000001  loss: 1.9642 (1.9519)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2085 (7.8123)  time: 0.9296 (0.5276 -- 4.3972)  data: 0.3899 (0.0002 -- 3.8874)  max mem: 16413
Epoch: [40]  [ 60/160]  eta: 0:01:32  lr: 0.000022  min_lr: 0.000001  loss: 1.9586 (1.9473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6945 (8.0599)  time: 0.5846 (0.5136 -- 1.1993)  data: 0.0345 (0.0002 -- 0.6534)  max mem: 16413
Epoch: [40]  [ 80/160]  eta: 0:01:13  lr: 0.000022  min_lr: 0.000001  loss: 1.8454 (1.9398)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2003 (8.2012)  time: 0.9189 (0.5344 -- 3.0058)  data: 0.3279 (0.0004 -- 2.4766)  max mem: 16413
[2023-08-30 14:54:05,133] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:54:05,133] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:54:05,135] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:54:05,135] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [40]  [100/160]  eta: 0:00:55  lr: 0.000022  min_lr: 0.000001  loss: 1.9933 (1.9413)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5582 (8.1813)  time: 0.8920 (0.5239 -- 3.6474)  data: 0.3045 (0.0004 -- 3.1302)  max mem: 16413
[2023-08-30 14:54:12,731] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6503
[2023-08-30 14:54:12,731] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6503
[2023-08-30 14:54:12,732] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:54:12,732] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:54:12,732] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 14:54:28,540] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6519
[2023-08-30 14:54:28,540] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6519
[2023-08-30 14:54:28,540] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 14:54:28,540] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 14:54:28,540] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [40]  [120/160]  eta: 0:00:36  lr: 0.000022  min_lr: 0.000001  loss: 1.9367 (1.9497)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3227 (8.1318)  time: 0.9072 (0.5202 -- 3.6898)  data: 0.0808 (0.0003 -- 1.5917)  max mem: 16413
Epoch: [40]  [140/160]  eta: 0:00:18  lr: 0.000022  min_lr: 0.000001  loss: 1.9833 (1.9591)  loss_scale: 8192.0000 (16035.4043)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5363 (8.0536)  time: 0.9034 (0.5085 -- 4.6400)  data: 0.0016 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [40]  [159/160]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000001  loss: 2.0784 (1.9570)  loss_scale: 8192.0000 (15104.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0185 (8.1010)  time: 0.6688 (0.4957 -- 2.4733)  data: 0.0006 (0.0001 -- 0.0022)  max mem: 16413
Epoch: [40] Total time: 0:02:21 (0.8856 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000001  loss: 2.0784 (1.9623)  loss_scale: 8192.0000 (15104.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0185 (8.1010)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.8549 (0.8549)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.4065 (2.4065 -- 2.4065)  data: 2.1761 (2.1761 -- 2.1761)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8671 (1.0101)  acc1: 66.6667 (63.6364)  acc5: 100.0000 (95.9596)  time: 0.4275 (0.1919 -- 2.4065)  data: 0.2044 (0.0002 -- 2.1761)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8671 (0.9661)  acc1: 66.6667 (65.6085)  acc5: 100.0000 (96.2963)  time: 0.2156 (0.1692 -- 0.3087)  data: 0.0070 (0.0001 -- 0.0655)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9550 (1.0115)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (95.4357)  time: 0.1999 (0.1322 -- 0.3087)  data: 0.0068 (0.0001 -- 0.0655)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 68.672 Acc@5 95.228 loss 0.980
Accuracy of the network on the 482 val images: 68.67%
[2023-08-30 14:55:07,750] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 14:55:07,752] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 14:55:07,752] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 14:55:07,752] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 14:55:09,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 14:55:09,257] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 68.67%
Epoch: [41]  [  0/160]  eta: 0:20:14  lr: 0.000022  min_lr: 0.000001  loss: 2.2886 (2.2886)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6946 (6.6946)  time: 7.5924 (7.5924 -- 7.5924)  data: 6.9332 (6.9332 -- 6.9332)  max mem: 16413
Epoch: [41]  [ 20/160]  eta: 0:02:50  lr: 0.000022  min_lr: 0.000001  loss: 2.0482 (1.9400)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6173 (8.7963)  time: 0.9008 (0.5147 -- 4.5904)  data: 0.3546 (0.0003 -- 4.0368)  max mem: 16413
Epoch: [41]  [ 40/160]  eta: 0:02:02  lr: 0.000021  min_lr: 0.000001  loss: 2.0527 (1.9785)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1984 (8.2039)  time: 0.8197 (0.5190 -- 2.6633)  data: 0.2544 (0.0004 -- 2.1458)  max mem: 16413
Epoch: [41]  [ 60/160]  eta: 0:01:35  lr: 0.000021  min_lr: 0.000001  loss: 1.9992 (1.9777)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9271 (8.2402)  time: 0.8041 (0.5278 -- 2.7776)  data: 0.2404 (0.0005 -- 2.2571)  max mem: 16413
Epoch: [41]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000001  loss: 2.0389 (1.9749)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7355 (8.1850)  time: 0.9576 (0.5313 -- 4.3085)  data: 0.0490 (0.0001 -- 0.7255)  max mem: 16413
[2023-08-30 14:56:32,249] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:56:32,250] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 14:56:32,251] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:56:32,252] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [41]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000001  loss: 1.8988 (1.9666)  loss_scale: 16384.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9081 (8.0215)  time: 0.8428 (0.5156 -- 3.3315)  data: 0.2838 (0.0002 -- 2.8248)  max mem: 16413
Epoch: [41]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000001  loss: 1.8920 (1.9467)  loss_scale: 16384.0000 (10426.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5605 (7.9346)  time: 0.9180 (0.5059 -- 4.8688)  data: 0.2765 (0.0004 -- 4.3560)  max mem: 16413
Epoch: [41]  [140/160]  eta: 0:00:17  lr: 0.000021  min_lr: 0.000001  loss: 1.8996 (1.9486)  loss_scale: 16384.0000 (11271.2624)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3739 (8.0123)  time: 0.6969 (0.5350 -- 1.8918)  data: 0.1362 (0.0006 -- 1.3442)  max mem: 16413
Epoch: [41]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 2.0378 (1.9637)  loss_scale: 16384.0000 (11878.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1863 (8.0508)  time: 0.6800 (0.4983 -- 1.7687)  data: 0.1409 (0.0003 -- 1.2002)  max mem: 16413
Epoch: [41] Total time: 0:02:19 (0.8715 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 2.0378 (1.9517)  loss_scale: 16384.0000 (11878.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1863 (8.0508)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.8564 (0.8564)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2394 (2.2394 -- 2.2394)  data: 2.0075 (2.0075 -- 2.0075)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9138 (1.0122)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (96.9697)  time: 0.4292 (0.2052 -- 2.2394)  data: 0.2120 (0.0009 -- 2.0075)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9138 (0.9661)  acc1: 55.5556 (63.4921)  acc5: 100.0000 (96.8254)  time: 0.2234 (0.1707 -- 0.5388)  data: 0.0197 (0.0001 -- 0.3071)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9706 (1.0104)  acc1: 55.5556 (63.4855)  acc5: 100.0000 (96.2656)  time: 0.2081 (0.1334 -- 0.5388)  data: 0.0194 (0.0001 -- 0.3071)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 68.465 Acc@5 95.436 loss 0.971
Accuracy of the network on the 482 val images: 68.46%
Max accuracy: 68.67%
Epoch: [42]  [  0/160]  eta: 0:19:05  lr: 0.000021  min_lr: 0.000001  loss: 1.9893 (1.9893)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1534 (10.1534)  time: 7.1575 (7.1575 -- 7.1575)  data: 6.0938 (6.0938 -- 6.0938)  max mem: 16413
Epoch: [42]  [ 20/160]  eta: 0:02:44  lr: 0.000021  min_lr: 0.000001  loss: 1.9583 (1.9495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3509 (8.7733)  time: 0.8765 (0.5258 -- 3.1264)  data: 0.2409 (0.0003 -- 1.9294)  max mem: 16413
Epoch: [42]  [ 40/160]  eta: 0:02:06  lr: 0.000021  min_lr: 0.000001  loss: 1.9652 (1.9326)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6606 (8.4772)  time: 0.9269 (0.5349 -- 4.7006)  data: 0.3039 (0.0006 -- 4.1449)  max mem: 16413
[2023-08-30 14:58:31,078] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:58:31,078] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 14:58:31,078] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 14:58:31,079] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [42]  [ 60/160]  eta: 0:01:33  lr: 0.000021  min_lr: 0.000001  loss: 1.9605 (1.9297)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3010 (8.1256)  time: 0.6839 (0.5287 -- 2.0589)  data: 0.1369 (0.0008 -- 1.5227)  max mem: 16413
[2023-08-30 14:58:39,412] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6785
[2023-08-30 14:58:39,412] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6785
[2023-08-30 14:58:39,412] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:58:39,412] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 14:58:39,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [42]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000001  loss: 2.1309 (1.9670)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8944 (7.9970)  time: 1.0056 (0.5291 -- 4.5496)  data: 0.4510 (0.0005 -- 4.0170)  max mem: 16413
Epoch: [42]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000001  loss: 1.8853 (1.9570)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5347 (7.9815)  time: 0.8125 (0.5279 -- 3.8242)  data: 0.2641 (0.0007 -- 3.3030)  max mem: 16413
Epoch: [42]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000001  loss: 2.0081 (1.9634)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0588 (8.0242)  time: 0.9449 (0.5265 -- 4.3260)  data: 0.3952 (0.0002 -- 3.7841)  max mem: 16413
Epoch: [42]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.8807 (1.9563)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2261 (7.9773)  time: 0.7670 (0.5280 -- 3.0056)  data: 0.1645 (0.0002 -- 2.4693)  max mem: 16413
Epoch: [42]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.9126 (1.9479)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3316 (7.9996)  time: 0.7425 (0.4964 -- 4.3723)  data: 0.0199 (0.0002 -- 0.3881)  max mem: 16413
Epoch: [42] Total time: 0:02:21 (0.8865 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.9126 (1.9548)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3316 (7.9996)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.8549 (0.8549)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3738 (2.3738 -- 2.3738)  data: 2.1380 (2.1380 -- 2.1380)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.9045 (1.0150)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (96.9697)  time: 0.4244 (0.2115 -- 2.3738)  data: 0.2002 (0.0008 -- 2.1380)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.9045 (0.9702)  acc1: 55.5556 (63.4921)  acc5: 100.0000 (96.8254)  time: 0.2144 (0.1694 -- 0.2833)  data: 0.0034 (0.0001 -- 0.0517)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9528 (1.0129)  acc1: 57.1429 (63.4855)  acc5: 100.0000 (96.2656)  time: 0.1958 (0.1330 -- 0.2833)  data: 0.0031 (0.0001 -- 0.0517)  max mem: 16413
Val: Total time: 0:00:07 (0.2833 s / it)
* Acc@1 68.672 Acc@5 95.851 loss 0.974
Accuracy of the network on the 482 val images: 68.67%
Max accuracy: 68.67%
Epoch: [43]  [  0/160]  eta: 0:23:20  lr: 0.000021  min_lr: 0.000001  loss: 2.1302 (2.1302)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7456 (5.7456)  time: 8.7533 (8.7533 -- 8.7533)  data: 8.2344 (8.2344 -- 8.2344)  max mem: 16413
Epoch: [43]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000001  loss: 2.0684 (2.0156)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9154 (7.3518)  time: 0.8370 (0.5141 -- 2.5803)  data: 0.1803 (0.0003 -- 2.0456)  max mem: 16413
[2023-08-30 15:00:43,112] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:00:43,114] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:00:43,154] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:00:43,154] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:00:45,370] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6918
[2023-08-30 15:00:45,370] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:00:45,370] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 6918
[2023-08-30 15:00:45,370] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:00:45,371] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [43]  [ 40/160]  eta: 0:02:00  lr: 0.000021  min_lr: 0.000001  loss: 1.8369 (1.9493)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3857 (7.9639)  time: 0.7799 (0.5224 -- 2.6316)  data: 0.0602 (0.0003 -- 0.9656)  max mem: 16413
Epoch: [43]  [ 60/160]  eta: 0:01:40  lr: 0.000021  min_lr: 0.000001  loss: 2.0418 (1.9349)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1977 (8.0417)  time: 1.0084 (0.5198 -- 4.1012)  data: 0.0152 (0.0003 -- 0.2757)  max mem: 16413
Epoch: [43]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000001  loss: 1.9257 (1.9309)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4579 (8.1932)  time: 0.7596 (0.5253 -- 2.8280)  data: 0.0015 (0.0002 -- 0.0072)  max mem: 16413
Epoch: [43]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000001  loss: 1.8769 (1.9302)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6639 (8.1677)  time: 0.8463 (0.5182 -- 3.0923)  data: 0.0280 (0.0003 -- 0.5215)  max mem: 16413
[2023-08-30 15:01:55,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=36, lr=[5.044188189952753e-07, 5.044188189952753e-07, 6.725584253270338e-07, 6.725584253270338e-07, 8.967445671027117e-07, 8.967445671027117e-07, 1.1956594228036155e-06, 1.1956594228036155e-06, 1.5942125637381541e-06, 1.5942125637381541e-06, 2.1256167516508723e-06, 2.1256167516508723e-06, 2.8341556688678293e-06, 2.8341556688678293e-06, 3.778874225157106e-06, 3.778874225157106e-06, 5.038498966876141e-06, 5.038498966876141e-06, 6.717998622501522e-06, 6.717998622501522e-06, 8.957331496668695e-06, 8.957331496668695e-06, 1.1943108662224928e-05, 1.1943108662224928e-05, 1.592414488296657e-05, 1.592414488296657e-05, 2.123219317728876e-05, 2.123219317728876e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 15:01:55,075] [INFO] [timer.py:260:stop] epoch=0/micro_step=7000/global_step=7000, RunningAvgSamplesPerSec=18.032229889565702, CurrSamplesPerSec=21.78307571459176, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [43]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.7579 (1.9088)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6597 (8.1569)  time: 0.8293 (0.5224 -- 3.5655)  data: 0.2086 (0.0001 -- 3.0543)  max mem: 16413
Epoch: [43]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000001  loss: 1.9938 (1.9016)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2764 (8.0753)  time: 0.9909 (0.5259 -- 5.6412)  data: 0.0013 (0.0003 -- 0.0037)  max mem: 16413
Epoch: [43]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 1.9966 (1.9086)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2858 (8.1418)  time: 0.6160 (0.4935 -- 2.4341)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [43] Total time: 0:02:21 (0.8849 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 1.9966 (1.9318)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2858 (8.1418)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.8099 (0.8099)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2849 (2.2849 -- 2.2849)  data: 2.0660 (2.0660 -- 2.0660)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.8926 (1.0033)  acc1: 66.6667 (61.6162)  acc5: 100.0000 (94.9495)  time: 0.4084 (0.1985 -- 2.2849)  data: 0.1902 (0.0006 -- 2.0660)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8926 (0.9567)  acc1: 66.6667 (64.0212)  acc5: 100.0000 (95.7672)  time: 0.2293 (0.1726 -- 0.6309)  data: 0.0229 (0.0001 -- 0.4282)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9570 (1.0022)  acc1: 66.6667 (63.9004)  acc5: 100.0000 (95.4357)  time: 0.2120 (0.1331 -- 0.6309)  data: 0.0225 (0.0001 -- 0.4282)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 69.295 Acc@5 95.643 loss 0.962
Accuracy of the network on the 482 val images: 69.29%
[2023-08-30 15:02:35,479] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:02:35,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:02:35,481] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:02:35,481] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:02:36,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:02:36,913] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.29%
Epoch: [44]  [  0/160]  eta: 0:17:02  lr: 0.000021  min_lr: 0.000001  loss: 2.0402 (2.0402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9085 (8.9085)  time: 6.3936 (6.3936 -- 6.3936)  data: 5.8615 (5.8615 -- 5.8615)  max mem: 16413
[2023-08-30 15:02:48,112] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:02:48,112] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:02:48,112] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:02:48,112] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [44]  [ 20/160]  eta: 0:02:40  lr: 0.000021  min_lr: 0.000001  loss: 2.0583 (1.9548)  loss_scale: 32768.0000 (27306.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0038 (7.7165)  time: 0.8873 (0.5327 -- 4.2107)  data: 0.3054 (0.0004 -- 3.6900)  max mem: 16413
[2023-08-30 15:03:14,277] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7075
[2023-08-30 15:03:14,277] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:03:14,277] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 15:03:14,277] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7075
[2023-08-30 15:03:14,277] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [44]  [ 40/160]  eta: 0:02:02  lr: 0.000021  min_lr: 0.000001  loss: 1.9711 (1.9695)  loss_scale: 32768.0000 (27573.0732)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3567 (7.8159)  time: 0.8772 (0.5149 -- 3.5474)  data: 0.3343 (0.0004 -- 3.0322)  max mem: 16413
Epoch: [44]  [ 60/160]  eta: 0:01:35  lr: 0.000021  min_lr: 0.000001  loss: 2.1026 (1.9905)  loss_scale: 16384.0000 (23904.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3273 (7.8050)  time: 0.8203 (0.5335 -- 2.6835)  data: 0.1784 (0.0002 -- 2.1525)  max mem: 16413
Epoch: [44]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000001  loss: 1.9439 (1.9615)  loss_scale: 16384.0000 (22047.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0519 (7.7554)  time: 0.9333 (0.5204 -- 5.7674)  data: 0.3791 (0.0003 -- 5.2407)  max mem: 16413
Epoch: [44]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000001  loss: 1.8579 (1.9460)  loss_scale: 16384.0000 (20926.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4532 (7.7660)  time: 0.9134 (0.5107 -- 4.7059)  data: 0.3694 (0.0001 -- 4.1716)  max mem: 16413
Epoch: [44]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000001  loss: 1.9279 (1.9548)  loss_scale: 16384.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5590 (7.7545)  time: 0.8219 (0.5241 -- 2.8284)  data: 0.2756 (0.0002 -- 2.3217)  max mem: 16413
Epoch: [44]  [140/160]  eta: 0:00:17  lr: 0.000021  min_lr: 0.000001  loss: 1.8174 (1.9501)  loss_scale: 16384.0000 (19637.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2265 (7.7236)  time: 0.7395 (0.5276 -- 4.1078)  data: 0.1943 (0.0006 -- 3.5750)  max mem: 16413
Epoch: [44]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000001  loss: 2.0442 (1.9493)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5363 (7.7381)  time: 0.7233 (0.4948 -- 3.0939)  data: 0.1938 (0.0002 -- 2.5304)  max mem: 16413
Epoch: [44] Total time: 0:02:20 (0.8766 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000001  loss: 2.0442 (1.9511)  loss_scale: 16384.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5363 (7.7381)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7957 (0.7957)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2820 (2.2820 -- 2.2820)  data: 2.0562 (2.0562 -- 2.0562)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.8358 (0.9854)  acc1: 55.5556 (61.6162)  acc5: 100.0000 (95.9596)  time: 0.4065 (0.2090 -- 2.2820)  data: 0.1879 (0.0006 -- 2.0562)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8358 (0.9394)  acc1: 55.5556 (63.4921)  acc5: 100.0000 (96.2963)  time: 0.2215 (0.1713 -- 0.3380)  data: 0.0124 (0.0001 -- 0.1204)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9123 (0.9841)  acc1: 66.6667 (64.3154)  acc5: 100.0000 (95.8506)  time: 0.2041 (0.1326 -- 0.3380)  data: 0.0121 (0.0001 -- 0.1204)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 69.710 Acc@5 95.851 loss 0.947
Accuracy of the network on the 482 val images: 69.71%
[2023-08-30 15:05:04,900] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:05:04,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:05:04,902] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:05:04,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:05:06,271] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:05:06,271] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.71%
Epoch: [45]  [  0/160]  eta: 0:19:20  lr: 0.000021  min_lr: 0.000001  loss: 2.3004 (2.3004)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4720 (7.4720)  time: 7.2522 (7.2522 -- 7.2522)  data: 5.0016 (5.0016 -- 5.0016)  max mem: 16413
[2023-08-30 15:05:17,968] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:05:17,968] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:05:17,968] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:05:17,968] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:05:19,654] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7207
[2023-08-30 15:05:19,654] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7207
[2023-08-30 15:05:19,654] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:05:19,654] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:05:19,654] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [45]  [ 20/160]  eta: 0:03:03  lr: 0.000021  min_lr: 0.000001  loss: 2.0254 (1.9618)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1440 (8.7100)  time: 1.0110 (0.5268 -- 4.0931)  data: 0.0024 (0.0004 -- 0.0195)  max mem: 16413
Epoch: [45]  [ 40/160]  eta: 0:02:06  lr: 0.000021  min_lr: 0.000001  loss: 2.0145 (1.9455)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3211 (8.7775)  time: 0.7827 (0.5232 -- 3.5232)  data: 0.0011 (0.0002 -- 0.0027)  max mem: 16413
Epoch: [45]  [ 60/160]  eta: 0:01:36  lr: 0.000021  min_lr: 0.000001  loss: 2.0671 (1.9396)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3025 (8.7242)  time: 0.7932 (0.5154 -- 2.6169)  data: 0.0025 (0.0002 -- 0.0167)  max mem: 16413
Epoch: [45]  [ 80/160]  eta: 0:01:15  lr: 0.000021  min_lr: 0.000000  loss: 1.9003 (1.9119)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8798 (8.6029)  time: 0.8645 (0.5177 -- 2.3897)  data: 0.1643 (0.0003 -- 1.6390)  max mem: 16413
Epoch: [45]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 2.0456 (1.9195)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7083 (8.5189)  time: 0.9557 (0.5229 -- 3.4285)  data: 0.0984 (0.0004 -- 1.2695)  max mem: 16413
Epoch: [45]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 1.8565 (1.9054)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9975 (8.5193)  time: 0.8415 (0.5251 -- 3.8347)  data: 0.0015 (0.0004 -- 0.0028)  max mem: 16413
[2023-08-30 15:07:12,650] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:07:12,650] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:07:12,651] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:07:12,651] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:07:13,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7337
[2023-08-30 15:07:13,204] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:07:13,204] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7337
[2023-08-30 15:07:13,204] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 15:07:13,205] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [45]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.8819 (1.9083)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1843 (8.3857)  time: 0.8802 (0.5174 -- 3.5729)  data: 0.0011 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [45]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9358 (1.9146)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2433 (8.2829)  time: 0.6086 (0.4968 -- 1.6467)  data: 0.0011 (0.0002 -- 0.0064)  max mem: 16413
Epoch: [45] Total time: 0:02:21 (0.8841 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9358 (1.9074)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2433 (8.2829)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.8078 (0.8078)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3841 (2.3841 -- 2.3841)  data: 2.1419 (2.1419 -- 2.1419)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8078 (0.9902)  acc1: 55.5556 (63.6364)  acc5: 100.0000 (96.9697)  time: 0.4270 (0.2077 -- 2.3841)  data: 0.2046 (0.0006 -- 2.1419)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8167 (0.9442)  acc1: 55.5556 (65.6085)  acc5: 100.0000 (96.8254)  time: 0.2261 (0.1693 -- 0.4631)  data: 0.0189 (0.0001 -- 0.2667)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9281 (0.9824)  acc1: 57.1429 (65.1452)  acc5: 100.0000 (96.2656)  time: 0.2085 (0.1372 -- 0.4631)  data: 0.0185 (0.0001 -- 0.2667)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 69.710 Acc@5 96.266 loss 0.947
Accuracy of the network on the 482 val images: 69.71%
[2023-08-30 15:07:35,637] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:07:35,638] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:07:35,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:07:35,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:07:37,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:07:37,053] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 69.71%
Epoch: [46]  [  0/160]  eta: 0:17:14  lr: 0.000021  min_lr: 0.000000  loss: 2.3981 (2.3981)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8681 (6.8681)  time: 6.4657 (6.4657 -- 6.4657)  data: 5.8818 (5.8818 -- 5.8818)  max mem: 16413
Epoch: [46]  [ 20/160]  eta: 0:02:45  lr: 0.000021  min_lr: 0.000000  loss: 1.9746 (2.0028)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2921 (7.4973)  time: 0.9183 (0.5169 -- 2.5557)  data: 0.2323 (0.0007 -- 1.5233)  max mem: 16413
Epoch: [46]  [ 40/160]  eta: 0:02:11  lr: 0.000021  min_lr: 0.000000  loss: 1.8629 (1.9730)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1762 (7.9653)  time: 1.0058 (0.5227 -- 3.7740)  data: 0.0524 (0.0004 -- 1.0236)  max mem: 16413
Epoch: [46]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000000  loss: 2.0974 (2.0093)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9161 (7.8279)  time: 0.7189 (0.5181 -- 2.4680)  data: 0.0017 (0.0002 -- 0.0071)  max mem: 16413
Epoch: [46]  [ 80/160]  eta: 0:01:17  lr: 0.000021  min_lr: 0.000000  loss: 1.8871 (1.9694)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0479 (8.0120)  time: 0.9578 (0.5216 -- 4.0391)  data: 0.0016 (0.0004 -- 0.0063)  max mem: 16413
Epoch: [46]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 1.9063 (1.9420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2991 (8.1667)  time: 0.8258 (0.5101 -- 4.3056)  data: 0.0010 (0.0004 -- 0.0026)  max mem: 16413
[2023-08-30 15:09:17,757] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:09:17,757] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:09:17,759] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:09:17,759] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:09:18,857] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7468
[2023-08-30 15:09:18,857] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:09:18,857] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7468
[2023-08-30 15:09:18,857] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 15:09:18,857] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [46]  [120/160]  eta: 0:00:37  lr: 0.000021  min_lr: 0.000000  loss: 1.9423 (1.9391)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8090 (8.1227)  time: 0.8762 (0.5411 -- 2.9066)  data: 0.0019 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [46]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.8629 (1.9390)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6919 (8.0093)  time: 0.7603 (0.5280 -- 4.2570)  data: 0.0017 (0.0004 -- 0.0066)  max mem: 16413
Epoch: [46]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9438 (1.9530)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9023 (8.0637)  time: 0.6714 (0.4967 -- 2.1381)  data: 0.0015 (0.0001 -- 0.0127)  max mem: 16413
Epoch: [46] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9438 (1.9699)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9023 (8.0637)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.8188 (0.8188)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3081 (2.3081 -- 2.3081)  data: 2.0436 (2.0436 -- 2.0436)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.8188 (0.9945)  acc1: 55.5556 (63.6364)  acc5: 100.0000 (96.9697)  time: 0.4068 (0.2061 -- 2.3081)  data: 0.1872 (0.0006 -- 2.0436)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8109 (0.9419)  acc1: 55.5556 (65.6085)  acc5: 100.0000 (96.8254)  time: 0.2229 (0.1702 -- 0.4351)  data: 0.0170 (0.0001 -- 0.2371)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9240 (0.9813)  acc1: 57.1429 (65.1452)  acc5: 100.0000 (96.2656)  time: 0.2072 (0.1373 -- 0.4351)  data: 0.0165 (0.0001 -- 0.2371)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 70.332 Acc@5 96.266 loss 0.948
Accuracy of the network on the 482 val images: 70.33%
[2023-08-30 15:10:05,450] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:10:05,452] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:10:05,452] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:10:05,452] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:10:06,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:10:06,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.33%
Epoch: [47]  [  0/160]  eta: 0:21:20  lr: 0.000021  min_lr: 0.000000  loss: 1.8658 (1.8658)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2632 (5.2632)  time: 8.0030 (8.0030 -- 8.0030)  data: 7.4671 (7.4671 -- 7.4671)  max mem: 16413
Epoch: [47]  [ 20/160]  eta: 0:02:45  lr: 0.000021  min_lr: 0.000000  loss: 1.9254 (1.9091)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4810 (8.3164)  time: 0.8400 (0.5305 -- 3.4873)  data: 0.2794 (0.0005 -- 2.9489)  max mem: 16413
Epoch: [47]  [ 40/160]  eta: 0:02:11  lr: 0.000021  min_lr: 0.000000  loss: 1.8113 (1.8904)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3537 (8.0814)  time: 1.0032 (0.5299 -- 5.0810)  data: 0.4450 (0.0004 -- 4.5618)  max mem: 16413
Epoch: [47]  [ 60/160]  eta: 0:01:41  lr: 0.000021  min_lr: 0.000000  loss: 1.9775 (1.9006)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5104 (8.0853)  time: 0.8383 (0.5171 -- 4.5046)  data: 0.3103 (0.0002 -- 3.9983)  max mem: 16413
[2023-08-30 15:11:22,946] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:11:22,947] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:11:22,953] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:11:22,953] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [47]  [ 80/160]  eta: 0:01:17  lr: 0.000021  min_lr: 0.000000  loss: 1.8528 (1.9105)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1801 (7.8888)  time: 0.8627 (0.5268 -- 3.9819)  data: 0.3178 (0.0001 -- 3.4510)  max mem: 16413
[2023-08-30 15:11:32,154] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7609
[2023-08-30 15:11:32,154] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7609
[2023-08-30 15:11:32,154] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:11:32,154] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:11:32,154] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [47]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000000  loss: 1.9080 (1.8994)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9299 (7.8662)  time: 0.7050 (0.5196 -- 2.0001)  data: 0.1473 (0.0003 -- 1.4605)  max mem: 16413
Epoch: [47]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000000  loss: 1.9836 (1.9208)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5646 (7.9164)  time: 0.8884 (0.5170 -- 3.4880)  data: 0.2197 (0.0004 -- 2.9288)  max mem: 16413
Epoch: [47]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 2.0384 (1.9296)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8280 (7.9417)  time: 0.9278 (0.5150 -- 4.6361)  data: 0.0012 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [47]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.9160 (1.9296)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3683 (8.0353)  time: 0.5844 (0.4986 -- 1.3797)  data: 0.0009 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [47] Total time: 0:02:20 (0.8780 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.9160 (1.9367)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3683 (8.0353)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.7764 (0.7764)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.0835 (2.0835 -- 2.0835)  data: 1.8824 (1.8824 -- 1.8824)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.8044 (1.0004)  acc1: 66.6667 (62.6263)  acc5: 100.0000 (95.9596)  time: 0.4174 (0.2063 -- 2.0835)  data: 0.2029 (0.0010 -- 1.8824)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8044 (0.9354)  acc1: 66.6667 (65.0794)  acc5: 100.0000 (96.2963)  time: 0.2295 (0.1693 -- 0.4129)  data: 0.0224 (0.0001 -- 0.1740)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9102 (0.9790)  acc1: 66.6667 (65.5602)  acc5: 100.0000 (95.8506)  time: 0.2141 (0.1337 -- 0.4129)  data: 0.0220 (0.0001 -- 0.1740)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 70.747 Acc@5 95.643 loss 0.939
Accuracy of the network on the 482 val images: 70.75%
[2023-08-30 15:12:35,011] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:12:35,012] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:12:35,012] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:12:35,013] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:12:36,529] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:12:36,529] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 70.75%
Epoch: [48]  [  0/160]  eta: 0:20:51  lr: 0.000021  min_lr: 0.000000  loss: 1.9711 (1.9711)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2294 (6.2294)  time: 7.8238 (7.8238 -- 7.8238)  data: 7.2797 (7.2797 -- 7.2797)  max mem: 16413
Epoch: [48]  [ 20/160]  eta: 0:02:49  lr: 0.000021  min_lr: 0.000000  loss: 2.0531 (2.0079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3398 (7.4083)  time: 0.8802 (0.5337 -- 4.0938)  data: 0.3336 (0.0003 -- 3.5659)  max mem: 16413
Epoch: [48]  [ 40/160]  eta: 0:02:08  lr: 0.000021  min_lr: 0.000000  loss: 1.8608 (1.9236)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2414 (7.4048)  time: 0.9303 (0.5077 -- 5.2439)  data: 0.3943 (0.0004 -- 4.7314)  max mem: 16413
[2023-08-30 15:13:35,015] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:13:35,015] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:13:35,016] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:13:35,017] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:13:36,113] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7740
[2023-08-30 15:13:36,113] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7740
[2023-08-30 15:13:36,114] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:13:36,114] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:13:36,114] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [48]  [ 60/160]  eta: 0:01:37  lr: 0.000021  min_lr: 0.000000  loss: 2.0094 (1.9398)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9096 (7.8021)  time: 0.7768 (0.5314 -- 2.3824)  data: 0.2214 (0.0005 -- 1.8426)  max mem: 16413
Epoch: [48]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000000  loss: 1.8133 (1.9306)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7674 (7.8278)  time: 0.8707 (0.5243 -- 3.8067)  data: 0.3250 (0.0005 -- 3.2912)  max mem: 16413
Epoch: [48]  [100/160]  eta: 0:00:55  lr: 0.000021  min_lr: 0.000000  loss: 1.8554 (1.9281)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8246 (7.8863)  time: 0.8484 (0.5275 -- 3.1079)  data: 0.3006 (0.0003 -- 2.5550)  max mem: 16413
Epoch: [48]  [120/160]  eta: 0:00:35  lr: 0.000021  min_lr: 0.000000  loss: 1.8618 (1.9144)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8481 (7.8772)  time: 0.7097 (0.5338 -- 2.4114)  data: 0.1580 (0.0001 -- 1.8617)  max mem: 16413
Epoch: [48]  [140/160]  eta: 0:00:17  lr: 0.000021  min_lr: 0.000000  loss: 1.9307 (1.9270)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4892 (7.8960)  time: 0.8782 (0.5253 -- 4.4102)  data: 0.3266 (0.0004 -- 3.8919)  max mem: 16413
Epoch: [48]  [159/160]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000000  loss: 1.8456 (1.9132)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0227 (7.9060)  time: 0.7453 (0.4985 -- 4.0075)  data: 0.2239 (0.0002 -- 3.4651)  max mem: 16413
Epoch: [48] Total time: 0:02:20 (0.8756 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000000  loss: 1.8456 (1.9162)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0227 (7.9060)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.7880 (0.7880)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.1874 (2.1874 -- 2.1874)  data: 1.9752 (1.9752 -- 1.9752)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7920 (0.9848)  acc1: 66.6667 (64.6465)  acc5: 100.0000 (95.9596)  time: 0.4018 (0.1919 -- 2.1874)  data: 0.1864 (0.0005 -- 1.9752)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8366 (0.9343)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (96.2963)  time: 0.2247 (0.1696 -- 0.3700)  data: 0.0180 (0.0001 -- 0.1645)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9014 (0.9833)  acc1: 55.5556 (64.7303)  acc5: 100.0000 (95.8506)  time: 0.2113 (0.1327 -- 0.3700)  data: 0.0178 (0.0001 -- 0.1645)  max mem: 16413
Val: Total time: 0:00:07 (0.2840 s / it)
* Acc@1 69.710 Acc@5 95.643 loss 0.939
Accuracy of the network on the 482 val images: 69.71%
Max accuracy: 70.75%
Epoch: [49]  [  0/160]  eta: 0:24:25  lr: 0.000021  min_lr: 0.000000  loss: 2.3434 (2.3434)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0831 (9.0831)  time: 9.1587 (9.1587 -- 9.1587)  data: 8.6397 (8.6397 -- 8.6397)  max mem: 16413
Epoch: [49]  [ 20/160]  eta: 0:02:50  lr: 0.000021  min_lr: 0.000000  loss: 1.9322 (1.9345)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2346 (8.0805)  time: 0.8209 (0.5171 -- 2.8365)  data: 0.2771 (0.0005 -- 2.2895)  max mem: 16413
[2023-08-30 15:15:36,461] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:15:36,461] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:15:36,462] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:15:36,462] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:15:37,575] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7871
[2023-08-30 15:15:37,575] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7871
[2023-08-30 15:15:37,575] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:15:37,575] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:15:37,575] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [49]  [ 40/160]  eta: 0:01:59  lr: 0.000021  min_lr: 0.000000  loss: 1.9962 (1.9631)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6390 (7.5249)  time: 0.7674 (0.5256 -- 2.1103)  data: 0.2235 (0.0001 -- 1.5815)  max mem: 16413
Epoch: [49]  [ 60/160]  eta: 0:01:38  lr: 0.000021  min_lr: 0.000000  loss: 1.8859 (1.9454)  loss_scale: 16384.0000 (16921.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7933 (7.7462)  time: 0.9639 (0.5285 -- 2.9662)  data: 0.3494 (0.0008 -- 2.0232)  max mem: 16413
Epoch: [49]  [ 80/160]  eta: 0:01:16  lr: 0.000021  min_lr: 0.000000  loss: 2.0542 (1.9497)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6475 (7.7543)  time: 0.8748 (0.5389 -- 2.8710)  data: 0.3267 (0.0003 -- 2.3527)  max mem: 16413
Epoch: [49]  [100/160]  eta: 0:00:56  lr: 0.000021  min_lr: 0.000000  loss: 1.9118 (1.9456)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2065 (7.7248)  time: 0.8463 (0.5230 -- 3.7180)  data: 0.3052 (0.0004 -- 3.2072)  max mem: 16413
Epoch: [49]  [120/160]  eta: 0:00:36  lr: 0.000021  min_lr: 0.000000  loss: 1.9811 (1.9380)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5262 (7.7285)  time: 0.8366 (0.5218 -- 3.5680)  data: 0.2923 (0.0001 -- 3.0642)  max mem: 16413
[2023-08-30 15:17:10,176] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7977
[2023-08-30 15:17:10,176] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 7977
[2023-08-30 15:17:10,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 15:17:10,177] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 15:17:10,177] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [49]  [140/160]  eta: 0:00:18  lr: 0.000021  min_lr: 0.000000  loss: 1.8731 (1.9306)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9878 (7.6829)  time: 0.9331 (0.5175 -- 3.2864)  data: 0.3895 (0.0004 -- 2.7784)  max mem: 16413
[2023-08-30 15:17:27,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=44, lr=[4.869534833689233e-07, 4.869534833689233e-07, 6.492713111585644e-07, 6.492713111585644e-07, 8.656950815447526e-07, 8.656950815447526e-07, 1.1542601087263367e-06, 1.1542601087263367e-06, 1.5390134783017824e-06, 1.5390134783017824e-06, 2.052017971069043e-06, 2.052017971069043e-06, 2.7360239614253907e-06, 2.7360239614253907e-06, 3.6480319485671876e-06, 3.6480319485671876e-06, 4.864042598089584e-06, 4.864042598089584e-06, 6.485390130786111e-06, 6.485390130786111e-06, 8.647186841048149e-06, 8.647186841048149e-06, 1.1529582454730866e-05, 1.1529582454730866e-05, 1.537277660630782e-05, 1.537277660630782e-05, 2.0497035475077093e-05, 2.0497035475077093e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 15:17:27,067] [INFO] [timer.py:260:stop] epoch=0/micro_step=8000/global_step=8000, RunningAvgSamplesPerSec=18.11033701183116, CurrSamplesPerSec=24.538138419235946, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [49]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9067 (1.9227)  loss_scale: 8192.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6160 (7.6693)  time: 0.6587 (0.4968 -- 3.3466)  data: 0.1425 (0.0001 -- 2.8389)  max mem: 16413
Epoch: [49] Total time: 0:02:22 (0.8918 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9067 (1.9268)  loss_scale: 8192.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6160 (7.6693)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.7611 (0.7611)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3046 (2.3046 -- 2.3046)  data: 2.0628 (2.0628 -- 2.0628)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7611 (0.9761)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (96.9697)  time: 0.4162 (0.1875 -- 2.3046)  data: 0.2038 (0.0005 -- 2.0628)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.8098 (0.9226)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (96.8254)  time: 0.2182 (0.1737 -- 0.3716)  data: 0.0100 (0.0001 -- 0.1599)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8856 (0.9712)  acc1: 66.6667 (67.2199)  acc5: 100.0000 (96.2656)  time: 0.2039 (0.1332 -- 0.3716)  data: 0.0092 (0.0001 -- 0.1599)  max mem: 16413
Val: Total time: 0:00:07 (0.2837 s / it)
* Acc@1 71.162 Acc@5 96.058 loss 0.926
Accuracy of the network on the 482 val images: 71.16%
[2023-08-30 15:17:34,738] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:17:34,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:17:34,740] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:17:34,740] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:17:36,122] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:17:36,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 71.16%
Epoch: [50]  [  0/160]  eta: 0:18:13  lr: 0.000020  min_lr: 0.000000  loss: 1.6273 (1.6273)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1136 (12.1136)  time: 6.8365 (6.8365 -- 6.8365)  data: 5.7921 (5.7921 -- 5.7921)  max mem: 16413
Epoch: [50]  [ 20/160]  eta: 0:02:45  lr: 0.000020  min_lr: 0.000000  loss: 1.7557 (1.8035)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1508 (7.9390)  time: 0.8976 (0.5313 -- 3.9681)  data: 0.2476 (0.0006 -- 2.1838)  max mem: 16413
Epoch: [50]  [ 40/160]  eta: 0:02:02  lr: 0.000020  min_lr: 0.000000  loss: 1.8393 (1.8448)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1368 (7.6405)  time: 0.8567 (0.5281 -- 3.6034)  data: 0.0207 (0.0002 -- 0.3799)  max mem: 16413
Epoch: [50]  [ 60/160]  eta: 0:01:35  lr: 0.000020  min_lr: 0.000000  loss: 1.7967 (1.8395)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9497 (7.8967)  time: 0.8230 (0.5246 -- 3.9087)  data: 0.2724 (0.0003 -- 3.3776)  max mem: 16413
Epoch: [50]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000000  loss: 2.0803 (1.8734)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5147 (7.9887)  time: 0.8607 (0.5319 -- 3.3080)  data: 0.3001 (0.0004 -- 2.7723)  max mem: 16413
Epoch: [50]  [100/160]  eta: 0:00:54  lr: 0.000020  min_lr: 0.000000  loss: 1.9092 (1.8793)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5939 (7.8713)  time: 0.8135 (0.5325 -- 2.6354)  data: 0.1276 (0.0003 -- 1.4144)  max mem: 16413
[2023-08-30 15:19:13,496] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:19:13,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 15:19:13,496] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:19:13,496] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [50]  [120/160]  eta: 0:00:35  lr: 0.000020  min_lr: 0.000000  loss: 1.8487 (1.8718)  loss_scale: 16384.0000 (9207.5372)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3798 (7.6915)  time: 0.8354 (0.5222 -- 2.8636)  data: 0.1797 (0.0003 -- 2.3091)  max mem: 16413
Epoch: [50]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.8933 (1.8827)  loss_scale: 16384.0000 (10225.4752)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6741 (7.8621)  time: 0.9357 (0.5229 -- 3.3327)  data: 0.3914 (0.0003 -- 2.8279)  max mem: 16413
Epoch: [50]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.0024 (1.8910)  loss_scale: 16384.0000 (10956.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8852 (7.9043)  time: 0.6225 (0.4947 -- 1.6878)  data: 0.0946 (0.0001 -- 1.1494)  max mem: 16413
Epoch: [50] Total time: 0:02:19 (0.8702 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.0024 (1.8888)  loss_scale: 16384.0000 (10956.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8852 (7.9043)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.7596 (0.7596)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.5063 (2.5063 -- 2.5063)  data: 2.2277 (2.2277 -- 2.2277)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7596 (0.9686)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (95.9596)  time: 0.4372 (0.2038 -- 2.5063)  data: 0.2118 (0.0007 -- 2.2277)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7443 (0.9151)  acc1: 66.6667 (68.2540)  acc5: 100.0000 (96.2963)  time: 0.2144 (0.1699 -- 0.3356)  data: 0.0058 (0.0001 -- 0.0836)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8641 (0.9575)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (96.2656)  time: 0.1974 (0.1331 -- 0.3356)  data: 0.0055 (0.0001 -- 0.0836)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 73.029 Acc@5 96.266 loss 0.914
Accuracy of the network on the 482 val images: 73.03%
[2023-08-30 15:20:03,148] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:20:03,149] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:20:03,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:20:03,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:20:04,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:20:04,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.03%
Epoch: [51]  [  0/160]  eta: 0:21:52  lr: 0.000020  min_lr: 0.000000  loss: 1.9871 (1.9871)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4574 (11.4574)  time: 8.2001 (8.2001 -- 8.2001)  data: 5.6165 (5.6165 -- 5.6165)  max mem: 16413
Epoch: [51]  [ 20/160]  eta: 0:02:45  lr: 0.000020  min_lr: 0.000000  loss: 1.9214 (1.9402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7159 (8.2908)  time: 0.8307 (0.5239 -- 3.9242)  data: 0.0179 (0.0009 -- 0.3109)  max mem: 16413
Epoch: [51]  [ 40/160]  eta: 0:02:08  lr: 0.000020  min_lr: 0.000000  loss: 1.9491 (1.8899)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7139 (7.9584)  time: 0.9559 (0.5185 -- 4.3338)  data: 0.0023 (0.0003 -- 0.0164)  max mem: 16413
Epoch: [51]  [ 60/160]  eta: 0:01:39  lr: 0.000020  min_lr: 0.000000  loss: 1.8589 (1.8872)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0248 (8.2973)  time: 0.8257 (0.5136 -- 3.4583)  data: 0.0310 (0.0002 -- 0.3828)  max mem: 16413
[2023-08-30 15:21:18,955] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:21:18,955] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:21:18,956] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:21:18,957] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [51]  [ 80/160]  eta: 0:01:16  lr: 0.000020  min_lr: 0.000000  loss: 1.8738 (1.8759)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2048 (8.0427)  time: 0.8657 (0.5177 -- 3.7919)  data: 0.0731 (0.0002 -- 0.8382)  max mem: 16413
[2023-08-30 15:21:28,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8248
[2023-08-30 15:21:28,079] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8248
[2023-08-30 15:21:28,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:21:28,079] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:21:28,079] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [51]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.9709 (1.8874)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6307 (8.2013)  time: 0.8017 (0.5365 -- 2.3782)  data: 0.2460 (0.0005 -- 1.8605)  max mem: 16413
Epoch: [51]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 2.0289 (1.9025)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0321 (8.3012)  time: 0.8310 (0.5312 -- 2.0977)  data: 0.1348 (0.0004 -- 1.5662)  max mem: 16413
[2023-08-30 15:22:04,669] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8290
[2023-08-30 15:22:04,670] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 15:22:04,670] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8290
[2023-08-30 15:22:04,670] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 15:22:04,670] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [51]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.8688 (1.9043)  loss_scale: 8192.0000 (17371.6879)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6807 (8.2695)  time: 0.8908 (0.5200 -- 3.2795)  data: 0.2304 (0.0003 -- 2.7568)  max mem: 16413
Epoch: [51]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.1236 (1.9090)  loss_scale: 8192.0000 (16281.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8551 (8.2512)  time: 0.6949 (0.4959 -- 3.4850)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [51] Total time: 0:02:21 (0.8849 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.1236 (1.9326)  loss_scale: 8192.0000 (16281.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8551 (8.2512)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7364 (0.7364)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.2867 (2.2867 -- 2.2867)  data: 2.0823 (2.0823 -- 2.0823)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7386 (0.9717)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4079 (0.1984 -- 2.2867)  data: 0.1966 (0.0005 -- 2.0823)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7386 (0.9173)  acc1: 66.6667 (69.3122)  acc5: 100.0000 (96.2963)  time: 0.2209 (0.1711 -- 0.3975)  data: 0.0135 (0.0002 -- 0.1727)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8604 (0.9585)  acc1: 66.6667 (68.8797)  acc5: 100.0000 (95.8506)  time: 0.2065 (0.1332 -- 0.3975)  data: 0.0122 (0.0001 -- 0.1727)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 72.822 Acc@5 96.058 loss 0.916
Accuracy of the network on the 482 val images: 72.82%
Max accuracy: 73.03%
Epoch: [52]  [  0/160]  eta: 0:20:11  lr: 0.000020  min_lr: 0.000000  loss: 2.2121 (2.2121)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7482 (11.7482)  time: 7.5717 (7.5717 -- 7.5717)  data: 7.0569 (7.0569 -- 7.0569)  max mem: 16413
Epoch: [52]  [ 20/160]  eta: 0:02:37  lr: 0.000020  min_lr: 0.000000  loss: 1.9649 (1.9713)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7134 (8.4889)  time: 0.8060 (0.5159 -- 3.0883)  data: 0.1515 (0.0002 -- 1.6377)  max mem: 16413
Epoch: [52]  [ 40/160]  eta: 0:02:05  lr: 0.000020  min_lr: 0.000000  loss: 1.9438 (1.9350)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7840 (8.6235)  time: 0.9563 (0.5184 -- 2.6140)  data: 0.0766 (0.0003 -- 1.5007)  max mem: 16413
Epoch: [52]  [ 60/160]  eta: 0:01:40  lr: 0.000020  min_lr: 0.000000  loss: 1.8231 (1.9463)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7985 (8.6379)  time: 0.9172 (0.5152 -- 4.9683)  data: 0.0014 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [52]  [ 80/160]  eta: 0:01:17  lr: 0.000020  min_lr: 0.000000  loss: 1.8141 (1.9167)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1897 (8.4221)  time: 0.8858 (0.5188 -- 3.7739)  data: 0.0014 (0.0004 -- 0.0023)  max mem: 16413
[2023-08-30 15:24:07,052] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:24:07,053] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 15:24:07,053] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:24:07,054] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [52]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.8168 (1.9130)  loss_scale: 8192.0000 (8354.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1632 (8.2487)  time: 0.7427 (0.5204 -- 2.9560)  data: 0.0016 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [52]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.9427 (1.9194)  loss_scale: 16384.0000 (9681.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7607 (8.2782)  time: 0.8130 (0.5324 -- 3.2428)  data: 0.0025 (0.0003 -- 0.0143)  max mem: 16413
Epoch: [52]  [140/160]  eta: 0:00:17  lr: 0.000020  min_lr: 0.000000  loss: 1.8565 (1.9190)  loss_scale: 16384.0000 (10632.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3103 (8.2215)  time: 0.8300 (0.5339 -- 2.4519)  data: 0.0032 (0.0009 -- 0.0160)  max mem: 16413
Epoch: [52]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.7786 (1.9095)  loss_scale: 16384.0000 (11315.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1374 (8.1169)  time: 0.7128 (0.4985 -- 1.6220)  data: 0.0008 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [52] Total time: 0:02:20 (0.8771 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.7786 (1.9273)  loss_scale: 16384.0000 (11315.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1374 (8.1169)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.7303 (0.7303)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.5414 (2.5414 -- 2.5414)  data: 2.3287 (2.3287 -- 2.3287)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7361 (0.9705)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (94.9495)  time: 0.4227 (0.1931 -- 2.5414)  data: 0.2127 (0.0006 -- 2.3287)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7679 (0.9180)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (95.7672)  time: 0.2065 (0.1693 -- 0.2387)  data: 0.0024 (0.0001 -- 0.0344)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8590 (0.9571)  acc1: 66.6667 (67.6349)  acc5: 100.0000 (95.8506)  time: 0.1920 (0.1330 -- 0.2387)  data: 0.0021 (0.0001 -- 0.0344)  max mem: 16413
Val: Total time: 0:00:07 (0.2836 s / it)
* Acc@1 72.407 Acc@5 96.266 loss 0.910
Accuracy of the network on the 482 val images: 72.41%
Max accuracy: 73.03%
Epoch: [53]  [  0/160]  eta: 0:21:55  lr: 0.000020  min_lr: 0.000000  loss: 2.0983 (2.0983)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7449 (5.7449)  time: 8.2223 (8.2223 -- 8.2223)  data: 7.7029 (7.7029 -- 7.7029)  max mem: 16413
Epoch: [53]  [ 20/160]  eta: 0:02:39  lr: 0.000020  min_lr: 0.000000  loss: 1.9408 (1.9163)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1137 (8.4502)  time: 0.7829 (0.5279 -- 3.4674)  data: 0.2310 (0.0004 -- 2.9468)  max mem: 16413
Epoch: [53]  [ 40/160]  eta: 0:02:02  lr: 0.000020  min_lr: 0.000000  loss: 1.8136 (1.8921)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5949 (8.4795)  time: 0.8934 (0.5314 -- 3.6979)  data: 0.2498 (0.0003 -- 1.7667)  max mem: 16413
Epoch: [53]  [ 60/160]  eta: 0:01:34  lr: 0.000020  min_lr: 0.000000  loss: 2.0271 (1.9448)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8317 (8.1326)  time: 0.7999 (0.5126 -- 3.3188)  data: 0.1309 (0.0004 -- 1.5519)  max mem: 16413
[2023-08-30 15:26:06,293] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:26:06,293] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:26:06,293] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:26:06,293] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:26:07,941] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8550
[2023-08-30 15:26:07,941] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8550
[2023-08-30 15:26:07,941] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:26:07,941] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:26:07,941] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [53]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000000  loss: 1.7723 (1.9035)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0827 (8.0837)  time: 0.8672 (0.5243 -- 3.2196)  data: 0.1479 (0.0008 -- 1.0723)  max mem: 16413
Epoch: [53]  [100/160]  eta: 0:00:54  lr: 0.000020  min_lr: 0.000000  loss: 2.1075 (1.9345)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2563 (8.0546)  time: 0.8652 (0.5215 -- 2.4758)  data: 0.1270 (0.0003 -- 1.4766)  max mem: 16413
Epoch: [53]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8729 (1.9238)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0353 (7.8969)  time: 0.8708 (0.5324 -- 3.1838)  data: 0.0020 (0.0005 -- 0.0163)  max mem: 16413
Epoch: [53]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.8252 (1.9141)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0569 (7.8629)  time: 0.8580 (0.5189 -- 4.2625)  data: 0.0013 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [53]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9196 (1.9141)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9899 (7.9878)  time: 0.6837 (0.4963 -- 2.2946)  data: 0.0733 (0.0002 -- 0.7835)  max mem: 16413
Epoch: [53] Total time: 0:02:20 (0.8757 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9196 (1.8988)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9899 (7.9878)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.7288 (0.7288)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (100.0000)  time: 2.3085 (2.3085 -- 2.3085)  data: 2.0523 (2.0523 -- 2.0523)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7669 (0.9655)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (94.9495)  time: 0.4135 (0.2086 -- 2.3085)  data: 0.1877 (0.0008 -- 2.0523)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7669 (0.9077)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (95.7672)  time: 0.2192 (0.1749 -- 0.3574)  data: 0.0095 (0.0001 -- 0.1744)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8523 (0.9527)  acc1: 66.6667 (68.0498)  acc5: 100.0000 (95.4357)  time: 0.2017 (0.1331 -- 0.3574)  data: 0.0091 (0.0001 -- 0.1744)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 73.237 Acc@5 95.851 loss 0.904
Accuracy of the network on the 482 val images: 73.24%
[2023-08-30 15:27:29,829] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:27:29,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:27:29,831] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:27:29,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:27:31,218] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:27:31,218] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.24%
Epoch: [54]  [  0/160]  eta: 0:22:20  lr: 0.000020  min_lr: 0.000000  loss: 2.4848 (2.4848)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4965 (7.4965)  time: 8.3750 (8.3750 -- 8.3750)  data: 7.8414 (7.8414 -- 7.8414)  max mem: 16413
Epoch: [54]  [ 20/160]  eta: 0:02:46  lr: 0.000020  min_lr: 0.000000  loss: 1.8754 (1.9674)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7384 (8.2907)  time: 0.8321 (0.5371 -- 3.8117)  data: 0.2741 (0.0002 -- 3.2645)  max mem: 16413
[2023-08-30 15:28:10,330] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:28:10,330] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:28:10,331] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:28:10,331] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [54]  [ 40/160]  eta: 0:02:06  lr: 0.000020  min_lr: 0.000000  loss: 1.7011 (1.9175)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1178 (7.7998)  time: 0.9131 (0.5231 -- 4.1656)  data: 0.3721 (0.0004 -- 3.6229)  max mem: 16413
[2023-08-30 15:28:24,579] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8693
[2023-08-30 15:28:24,579] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8693
[2023-08-30 15:28:24,579] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:28:24,579] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:28:24,579] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [54]  [ 60/160]  eta: 0:01:39  lr: 0.000020  min_lr: 0.000000  loss: 1.8947 (1.9049)  loss_scale: 32768.0000 (20144.2623)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2700 (7.8328)  time: 0.8682 (0.5250 -- 2.4985)  data: 0.1691 (0.0008 -- 1.8271)  max mem: 16413
Epoch: [54]  [ 80/160]  eta: 0:01:14  lr: 0.000020  min_lr: 0.000000  loss: 1.9608 (1.9249)  loss_scale: 16384.0000 (19215.8025)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5868 (7.9698)  time: 0.7327 (0.5187 -- 3.7547)  data: 0.0011 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [54]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 2.0045 (1.9402)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8277 (8.0399)  time: 0.8872 (0.5157 -- 3.7763)  data: 0.0025 (0.0002 -- 0.0104)  max mem: 16413
Epoch: [54]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8965 (1.9318)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2137 (8.0043)  time: 0.8244 (0.5238 -- 2.3462)  data: 0.2189 (0.0004 -- 1.8340)  max mem: 16413
Epoch: [54]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.8479 (1.9296)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7212 (7.9905)  time: 0.9615 (0.5149 -- 4.6867)  data: 0.4163 (0.0004 -- 4.1733)  max mem: 16413
Epoch: [54]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9191 (1.9239)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6784 (7.9444)  time: 0.6690 (0.4963 -- 3.4840)  data: 0.1493 (0.0002 -- 2.9689)  max mem: 16413
Epoch: [54] Total time: 0:02:21 (0.8849 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.9191 (1.9052)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6784 (7.9444)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.7398 (0.7398)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.3126 (2.3126 -- 2.3126)  data: 2.0699 (2.0699 -- 2.0699)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7398 (0.9703)  acc1: 77.7778 (66.6667)  acc5: 100.0000 (95.9596)  time: 0.4061 (0.2030 -- 2.3126)  data: 0.1898 (0.0008 -- 2.0699)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7192 (0.9153)  acc1: 77.7778 (68.2540)  acc5: 100.0000 (96.2963)  time: 0.2206 (0.1695 -- 0.4159)  data: 0.0151 (0.0001 -- 0.2099)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.9006 (0.9542)  acc1: 66.6667 (66.3900)  acc5: 100.0000 (95.8506)  time: 0.2049 (0.1326 -- 0.4159)  data: 0.0147 (0.0001 -- 0.2099)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 71.784 Acc@5 95.851 loss 0.902
Accuracy of the network on the 482 val images: 71.78%
Max accuracy: 73.24%
Epoch: [55]  [  0/160]  eta: 0:20:54  lr: 0.000020  min_lr: 0.000000  loss: 1.9597 (1.9597)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3197 (11.3197)  time: 7.8416 (7.8416 -- 7.8416)  data: 7.2522 (7.2522 -- 7.2522)  max mem: 16413
[2023-08-30 15:30:18,467] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8813
[2023-08-30 15:30:18,467] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 8813
[2023-08-30 15:30:18,467] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 15:30:18,467] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 15:30:18,468] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [55]  [ 20/160]  eta: 0:02:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8383 (1.9223)  loss_scale: 16384.0000 (13263.2381)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4136 (8.3751)  time: 0.7850 (0.5315 -- 3.3537)  data: 0.2323 (0.0008 -- 2.8327)  max mem: 16413
Epoch: [55]  [ 40/160]  eta: 0:02:05  lr: 0.000020  min_lr: 0.000000  loss: 1.9955 (1.9754)  loss_scale: 8192.0000 (10789.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4421 (7.8955)  time: 0.9661 (0.5300 -- 3.7311)  data: 0.1271 (0.0004 -- 1.6696)  max mem: 16413
Epoch: [55]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8899 (1.9550)  loss_scale: 8192.0000 (9937.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1852 (8.0821)  time: 0.8082 (0.5200 -- 2.4046)  data: 0.1720 (0.0003 -- 1.7505)  max mem: 16413
Epoch: [55]  [ 80/160]  eta: 0:01:18  lr: 0.000020  min_lr: 0.000000  loss: 1.7743 (1.9063)  loss_scale: 8192.0000 (9506.7654)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0439 (8.1011)  time: 0.9992 (0.5177 -- 4.6531)  data: 0.0370 (0.0007 -- 0.6114)  max mem: 16413
Epoch: [55]  [100/160]  eta: 0:00:56  lr: 0.000020  min_lr: 0.000000  loss: 1.9591 (1.9268)  loss_scale: 8192.0000 (9246.4158)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4528 (8.0505)  time: 0.7814 (0.5250 -- 3.2067)  data: 0.0012 (0.0002 -- 0.0049)  max mem: 16413
Epoch: [55]  [120/160]  eta: 0:00:36  lr: 0.000020  min_lr: 0.000000  loss: 2.0000 (1.9299)  loss_scale: 8192.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9755 (8.0654)  time: 0.7917 (0.5374 -- 2.3794)  data: 0.0017 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [55]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.9333 (1.9371)  loss_scale: 8192.0000 (8947.2908)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9311 (8.0749)  time: 0.8834 (0.5247 -- 4.5125)  data: 0.0019 (0.0007 -- 0.0108)  max mem: 16413
[2023-08-30 15:32:09,801] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:32:09,802] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 15:32:09,803] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:32:09,804] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [55]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 1.8883 (1.9341)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6785 (8.1128)  time: 0.6864 (0.4970 -- 3.2836)  data: 0.0009 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [55] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 1.8883 (1.9223)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6785 (8.1128)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7198 (0.7198)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2611 (2.2611 -- 2.2611)  data: 1.9874 (1.9874 -- 1.9874)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.7198 (0.9653)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4055 (0.1966 -- 2.2611)  data: 0.1829 (0.0003 -- 1.9874)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7067 (0.9123)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (96.2963)  time: 0.2225 (0.1689 -- 0.4300)  data: 0.0156 (0.0001 -- 0.2272)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8753 (0.9582)  acc1: 66.6667 (66.8050)  acc5: 100.0000 (95.8506)  time: 0.2041 (0.1334 -- 0.4300)  data: 0.0147 (0.0001 -- 0.2272)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 72.199 Acc@5 96.266 loss 0.903
Accuracy of the network on the 482 val images: 72.20%
Max accuracy: 73.24%
Epoch: [56]  [  0/160]  eta: 0:24:43  lr: 0.000020  min_lr: 0.000000  loss: 2.0977 (2.0977)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1000 (8.1000)  time: 9.2723 (9.2723 -- 9.2723)  data: 8.7360 (8.7360 -- 8.7360)  max mem: 16413
Epoch: [56]  [ 20/160]  eta: 0:03:00  lr: 0.000020  min_lr: 0.000000  loss: 1.9685 (2.0246)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7469 (8.3003)  time: 0.8916 (0.5133 -- 5.1010)  data: 0.3538 (0.0003 -- 4.5824)  max mem: 16413
[2023-08-30 15:33:10,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=49, lr=[4.6738112792318307e-07, 4.6738112792318307e-07, 6.231748372309107e-07, 6.231748372309107e-07, 8.308997829745477e-07, 8.308997829745477e-07, 1.1078663772993968e-06, 1.1078663772993968e-06, 1.4771551697325293e-06, 1.4771551697325293e-06, 1.969540226310039e-06, 1.969540226310039e-06, 2.626053635080052e-06, 2.626053635080052e-06, 3.5014048467734025e-06, 3.5014048467734025e-06, 4.66853979569787e-06, 4.66853979569787e-06, 6.22471972759716e-06, 6.22471972759716e-06, 8.299626303462881e-06, 8.299626303462881e-06, 1.1066168404617173e-05, 1.1066168404617173e-05, 1.475489120615623e-05, 1.475489120615623e-05, 1.9673188274874975e-05, 1.9673188274874975e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 15:33:10,794] [INFO] [timer.py:260:stop] epoch=0/micro_step=9000/global_step=9000, RunningAvgSamplesPerSec=18.007366610363032, CurrSamplesPerSec=20.734840435940587, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [56]  [ 40/160]  eta: 0:02:08  lr: 0.000020  min_lr: 0.000000  loss: 1.7622 (1.9284)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7769 (8.5280)  time: 0.8456 (0.5047 -- 4.0433)  data: 0.3049 (0.0001 -- 3.5297)  max mem: 16413
Epoch: [56]  [ 60/160]  eta: 0:01:40  lr: 0.000020  min_lr: 0.000000  loss: 1.8842 (1.9146)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4257 (8.2841)  time: 0.8553 (0.5284 -- 3.6731)  data: 0.3005 (0.0003 -- 3.0952)  max mem: 16413
Epoch: [56]  [ 80/160]  eta: 0:01:17  lr: 0.000020  min_lr: 0.000000  loss: 1.8511 (1.9243)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5647 (8.3797)  time: 0.8886 (0.5222 -- 4.3079)  data: 0.3441 (0.0002 -- 3.7849)  max mem: 16413
Epoch: [56]  [100/160]  eta: 0:00:55  lr: 0.000020  min_lr: 0.000000  loss: 1.7324 (1.8851)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0788 (8.4432)  time: 0.7519 (0.5269 -- 3.4951)  data: 0.2039 (0.0002 -- 2.9622)  max mem: 16413
[2023-08-30 15:34:12,602] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:34:12,603] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:34:12,607] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:34:12,607] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [56]  [120/160]  eta: 0:00:37  lr: 0.000020  min_lr: 0.000000  loss: 1.8066 (1.8821)  loss_scale: 32768.0000 (17873.4545)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9102 (8.3343)  time: 1.0243 (0.5312 -- 4.2690)  data: 0.4771 (0.0002 -- 3.7630)  max mem: 16413
Epoch: [56]  [140/160]  eta: 0:00:18  lr: 0.000020  min_lr: 0.000000  loss: 1.8218 (1.8763)  loss_scale: 32768.0000 (19986.1560)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5762 (8.4218)  time: 0.7797 (0.5070 -- 3.5461)  data: 0.2404 (0.0001 -- 3.0072)  max mem: 16413
Epoch: [56]  [159/160]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000000  loss: 2.0666 (1.8869)  loss_scale: 32768.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6223 (8.4577)  time: 0.6979 (0.4964 -- 4.1882)  data: 0.1844 (0.0002 -- 3.6773)  max mem: 16413
Epoch: [56] Total time: 0:02:23 (0.8966 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000000  loss: 2.0666 (1.8754)  loss_scale: 32768.0000 (21504.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6223 (8.4577)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.7162 (0.7162)  acc1: 66.6667 (66.6667)  acc5: 100.0000 (100.0000)  time: 2.2830 (2.2830 -- 2.2830)  data: 2.0405 (2.0405 -- 2.0405)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7162 (0.9412)  acc1: 66.6667 (65.6566)  acc5: 100.0000 (95.9596)  time: 0.4132 (0.1987 -- 2.2830)  data: 0.1942 (0.0005 -- 2.0405)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7354 (0.8898)  acc1: 66.6667 (68.7831)  acc5: 100.0000 (96.2963)  time: 0.2239 (0.1697 -- 0.3562)  data: 0.0162 (0.0001 -- 0.1869)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8419 (0.9331)  acc1: 66.6667 (68.0498)  acc5: 100.0000 (96.2656)  time: 0.2093 (0.1330 -- 0.3562)  data: 0.0160 (0.0001 -- 0.1869)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 73.237 Acc@5 96.888 loss 0.883
Accuracy of the network on the 482 val images: 73.24%
[2023-08-30 15:35:00,841] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:35:00,843] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:35:00,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:35:00,843] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:35:02,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:35:02,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 73.24%
Epoch: [57]  [  0/160]  eta: 0:19:23  lr: 0.000020  min_lr: 0.000000  loss: 1.9484 (1.9484)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.4961 (14.4961)  time: 7.2693 (7.2693 -- 7.2693)  data: 6.7028 (6.7028 -- 6.7028)  max mem: 16413
Epoch: [57]  [ 20/160]  eta: 0:02:42  lr: 0.000020  min_lr: 0.000000  loss: 2.0209 (1.9560)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1640 (7.6773)  time: 0.8534 (0.5312 -- 4.4519)  data: 0.1372 (0.0007 -- 2.2118)  max mem: 16413
[2023-08-30 15:35:33,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9146
[2023-08-30 15:35:33,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9146
[2023-08-30 15:35:33,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:35:33,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:35:33,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [57]  [ 40/160]  eta: 0:02:00  lr: 0.000020  min_lr: 0.000000  loss: 1.8697 (1.9001)  loss_scale: 16384.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0666 (7.9206)  time: 0.8490 (0.5229 -- 3.4390)  data: 0.0351 (0.0003 -- 0.6392)  max mem: 16413
Epoch: [57]  [ 60/160]  eta: 0:01:36  lr: 0.000020  min_lr: 0.000000  loss: 1.8490 (1.8925)  loss_scale: 16384.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9686 (7.9425)  time: 0.8809 (0.5233 -- 2.9658)  data: 0.0019 (0.0003 -- 0.0155)  max mem: 16413
Epoch: [57]  [ 80/160]  eta: 0:01:16  lr: 0.000019  min_lr: 0.000000  loss: 1.9042 (1.8907)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8499 (8.1080)  time: 0.9101 (0.5273 -- 3.1886)  data: 0.1294 (0.0003 -- 2.0369)  max mem: 16413
Epoch: [57]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.9162 (1.8925)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7360 (8.0963)  time: 0.8273 (0.5181 -- 4.1485)  data: 0.0011 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [57]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.7808 (1.8742)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1803 (8.0177)  time: 0.7725 (0.5319 -- 3.8511)  data: 0.0127 (0.0004 -- 0.2197)  max mem: 16413
Epoch: [57]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.9356 (1.8796)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3155 (7.9986)  time: 0.9799 (0.5091 -- 5.0347)  data: 0.0080 (0.0005 -- 0.1341)  max mem: 16413
[2023-08-30 15:37:20,763] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:37:20,763] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:37:20,763] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:37:20,763] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [57]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.8996 (1.8808)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3549 (7.9912)  time: 0.6206 (0.4978 -- 1.7292)  data: 0.0009 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [57] Total time: 0:02:20 (0.8789 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.8996 (1.8974)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3549 (7.9912)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.6613 (0.6613)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4729 (2.4729 -- 2.4729)  data: 2.2395 (2.2395 -- 2.2395)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.7011 (0.9413)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4290 (0.1992 -- 2.4729)  data: 0.2059 (0.0006 -- 2.2395)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.7011 (0.8919)  acc1: 66.6667 (72.4868)  acc5: 100.0000 (96.8254)  time: 0.2164 (0.1700 -- 0.3100)  data: 0.0082 (0.0001 -- 0.1352)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8484 (0.9370)  acc1: 66.6667 (70.5394)  acc5: 100.0000 (96.2656)  time: 0.2009 (0.1329 -- 0.3100)  data: 0.0079 (0.0001 -- 0.1352)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 75.104 Acc@5 96.888 loss 0.882
Accuracy of the network on the 482 val images: 75.10%
[2023-08-30 15:37:30,588] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:37:30,590] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:37:30,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:37:30,590] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:37:32,123] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:37:32,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.10%
Epoch: [58]  [  0/160]  eta: 0:16:39  lr: 0.000019  min_lr: 0.000000  loss: 1.5915 (1.5915)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4871 (5.4871)  time: 6.2457 (6.2457 -- 6.2457)  data: 5.7011 (5.7011 -- 5.7011)  max mem: 16413
Epoch: [58]  [ 20/160]  eta: 0:02:39  lr: 0.000019  min_lr: 0.000000  loss: 1.7502 (1.7884)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2581 (8.4546)  time: 0.8831 (0.5349 -- 3.2087)  data: 0.2695 (0.0005 -- 2.6811)  max mem: 16413
[2023-08-30 15:38:02,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9306
[2023-08-30 15:38:02,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9306
[2023-08-30 15:38:02,256] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:38:02,256] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:38:02,257] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [ 40/160]  eta: 0:02:01  lr: 0.000019  min_lr: 0.000000  loss: 1.9072 (1.8647)  loss_scale: 16384.0000 (26773.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3151 (8.1611)  time: 0.8744 (0.5214 -- 2.5606)  data: 0.2471 (0.0004 -- 2.0353)  max mem: 16413
Epoch: [58]  [ 60/160]  eta: 0:01:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8126 (1.8670)  loss_scale: 16384.0000 (23367.3443)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4613 (8.0022)  time: 0.8611 (0.5361 -- 2.9155)  data: 0.3095 (0.0003 -- 2.3899)  max mem: 16413
Epoch: [58]  [ 80/160]  eta: 0:01:13  lr: 0.000019  min_lr: 0.000000  loss: 1.7629 (1.8527)  loss_scale: 16384.0000 (21643.0617)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9944 (7.9473)  time: 0.8150 (0.5246 -- 3.3162)  data: 0.2706 (0.0004 -- 2.8023)  max mem: 16413
Epoch: [58]  [100/160]  eta: 0:00:54  lr: 0.000019  min_lr: 0.000000  loss: 1.9230 (1.8661)  loss_scale: 16384.0000 (20601.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4406 (8.1650)  time: 0.8825 (0.5309 -- 3.0091)  data: 0.3275 (0.0007 -- 2.4577)  max mem: 16413
Epoch: [58]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.9901 (1.8827)  loss_scale: 16384.0000 (19904.5289)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3027 (8.2146)  time: 0.8376 (0.5348 -- 2.1986)  data: 0.2897 (0.0008 -- 1.6794)  max mem: 16413
Epoch: [58]  [140/160]  eta: 0:00:17  lr: 0.000019  min_lr: 0.000000  loss: 1.9943 (1.8991)  loss_scale: 16384.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8845 (8.3246)  time: 0.8372 (0.5281 -- 3.1248)  data: 0.2902 (0.0004 -- 2.6008)  max mem: 16413
[2023-08-30 15:39:51,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:39:51,193] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:39:51,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:39:51,193] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:39:52,692] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9438
[2023-08-30 15:39:52,692] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9438
[2023-08-30 15:39:52,692] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:39:52,692] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:39:52,692] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [58]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.8557 (1.8915)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8759 (8.2560)  time: 0.7767 (0.4858 -- 3.3463)  data: 0.2484 (0.0002 -- 2.8495)  max mem: 16413
Epoch: [58] Total time: 0:02:21 (0.8817 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.8557 (1.8922)  loss_scale: 16384.0000 (19353.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8759 (8.2560)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.6843 (0.6843)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5784 (2.5784 -- 2.5784)  data: 2.3621 (2.3621 -- 2.3621)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6843 (0.9489)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (94.9495)  time: 0.4373 (0.1883 -- 2.5784)  data: 0.2160 (0.0006 -- 2.3621)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6926 (0.8909)  acc1: 77.7778 (71.4286)  acc5: 100.0000 (95.7672)  time: 0.2129 (0.1712 -- 0.2529)  data: 0.0040 (0.0001 -- 0.0469)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8275 (0.9367)  acc1: 66.6667 (69.2946)  acc5: 100.0000 (95.4357)  time: 0.1968 (0.1329 -- 0.2529)  data: 0.0035 (0.0001 -- 0.0469)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 73.651 Acc@5 96.058 loss 0.882
Accuracy of the network on the 482 val images: 73.65%
Max accuracy: 75.10%
Epoch: [59]  [  0/160]  eta: 0:18:26  lr: 0.000019  min_lr: 0.000000  loss: 2.1447 (2.1447)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8295 (9.8295)  time: 6.9135 (6.9135 -- 6.9135)  data: 5.4622 (5.4622 -- 5.4622)  max mem: 16413
Epoch: [59]  [ 20/160]  eta: 0:02:38  lr: 0.000019  min_lr: 0.000000  loss: 1.9204 (1.8368)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4365 (8.8591)  time: 0.8458 (0.5324 -- 2.2379)  data: 0.1968 (0.0007 -- 1.6894)  max mem: 16413
Epoch: [59]  [ 40/160]  eta: 0:02:00  lr: 0.000019  min_lr: 0.000000  loss: 2.0589 (1.9032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5670 (8.6463)  time: 0.8728 (0.5246 -- 2.5071)  data: 0.3274 (0.0003 -- 1.9814)  max mem: 16413
Epoch: [59]  [ 60/160]  eta: 0:01:34  lr: 0.000019  min_lr: 0.000000  loss: 1.8629 (1.9019)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3340 (8.5977)  time: 0.8253 (0.5325 -- 2.8310)  data: 0.2332 (0.0006 -- 2.0295)  max mem: 16413
Epoch: [59]  [ 80/160]  eta: 0:01:14  lr: 0.000019  min_lr: 0.000000  loss: 1.7391 (1.8698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7619 (8.6216)  time: 0.8903 (0.5227 -- 3.7862)  data: 0.0519 (0.0007 -- 1.0084)  max mem: 16413
Epoch: [59]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 1.9609 (1.8931)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2650 (8.6321)  time: 0.9954 (0.5100 -- 3.8167)  data: 0.0010 (0.0003 -- 0.0021)  max mem: 16413
Epoch: [59]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8575 (1.8780)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0080 (8.6421)  time: 0.7448 (0.5121 -- 4.2825)  data: 0.0015 (0.0001 -- 0.0047)  max mem: 16413
[2023-08-30 15:41:58,670] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:41:58,671] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:41:58,672] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:41:58,672] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [59]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8270 (1.8746)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2038 (8.6628)  time: 0.9702 (0.5294 -- 3.9507)  data: 0.0015 (0.0005 -- 0.0043)  max mem: 16413
[2023-08-30 15:42:18,027] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9591
[2023-08-30 15:42:18,027] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9591
[2023-08-30 15:42:18,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:42:18,027] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:42:18,027] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [59]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.9242 (1.8732)  loss_scale: 32768.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1776 (8.5892)  time: 0.5877 (0.4838 -- 1.7699)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [59] Total time: 0:02:21 (0.8813 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.9242 (1.8655)  loss_scale: 32768.0000 (18841.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1776 (8.5892)
[2023-08-30 15:42:22,028] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-59 is about to be saved!
[2023-08-30 15:42:22,029] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt
[2023-08-30 15:42:22,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt...
[2023-08-30 15:42:22,029] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
[2023-08-30 15:42:22,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-59/mp_rank_00_model_states.pt.
[2023-08-30 15:42:22,924] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-59 is ready now!
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6755 (0.6755)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3833 (2.3833 -- 2.3833)  data: 2.1371 (2.1371 -- 2.1371)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6893 (0.9368)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4233 (0.2055 -- 2.3833)  data: 0.2009 (0.0006 -- 2.1371)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6893 (0.8778)  acc1: 77.7778 (71.9577)  acc5: 100.0000 (96.2963)  time: 0.2187 (0.1688 -- 0.2963)  data: 0.0073 (0.0001 -- 0.0698)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8163 (0.9218)  acc1: 66.6667 (70.5394)  acc5: 100.0000 (95.8506)  time: 0.2015 (0.1331 -- 0.2963)  data: 0.0070 (0.0001 -- 0.0698)  max mem: 16413
Val: Total time: 0:00:07 (0.2871 s / it)
* Acc@1 75.519 Acc@5 96.680 loss 0.866
Accuracy of the network on the 482 val images: 75.52%
[2023-08-30 15:42:30,719] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:42:30,721] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:42:30,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:42:30,721] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:42:32,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:42:32,132] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 75.52%
Epoch: [60]  [  0/160]  eta: 0:24:42  lr: 0.000019  min_lr: 0.000000  loss: 2.3340 (2.3340)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4963 (7.4963)  time: 9.2665 (9.2665 -- 9.2665)  data: 8.7246 (8.7246 -- 8.7246)  max mem: 16413
Epoch: [60]  [ 20/160]  eta: 0:02:58  lr: 0.000019  min_lr: 0.000000  loss: 1.8675 (1.8600)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4206 (8.3535)  time: 0.8753 (0.5224 -- 2.7994)  data: 0.3319 (0.0005 -- 2.2885)  max mem: 16413
Epoch: [60]  [ 40/160]  eta: 0:02:05  lr: 0.000019  min_lr: 0.000000  loss: 1.9307 (1.8933)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3618 (8.1718)  time: 0.8137 (0.5121 -- 3.5096)  data: 0.2673 (0.0002 -- 3.0034)  max mem: 16413
Epoch: [60]  [ 60/160]  eta: 0:01:37  lr: 0.000019  min_lr: 0.000000  loss: 1.8812 (1.8746)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9851 (8.0861)  time: 0.8063 (0.5245 -- 2.4825)  data: 0.2627 (0.0003 -- 1.9519)  max mem: 16413
Epoch: [60]  [ 80/160]  eta: 0:01:17  lr: 0.000019  min_lr: 0.000000  loss: 1.8980 (1.8915)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9401 (8.6357)  time: 0.9575 (0.5269 -- 3.6383)  data: 0.4074 (0.0001 -- 3.1029)  max mem: 16413
Epoch: [60]  [100/160]  eta: 0:00:56  lr: 0.000019  min_lr: 0.000000  loss: 1.9172 (1.8957)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8653 (8.7531)  time: 0.8073 (0.5155 -- 3.9482)  data: 0.2219 (0.0004 -- 3.3982)  max mem: 16413
[2023-08-30 15:44:25,562] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:44:25,562] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:44:25,563] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:44:25,564] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [60]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 2.0503 (1.9090)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3264 (8.6408)  time: 0.9476 (0.5132 -- 4.2718)  data: 0.4020 (0.0004 -- 3.7562)  max mem: 16413
Epoch: [60]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8672 (1.9125)  loss_scale: 32768.0000 (18824.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0651 (8.6011)  time: 0.8136 (0.5215 -- 3.3422)  data: 0.2691 (0.0004 -- 2.8389)  max mem: 16413
Epoch: [60]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.8590 (1.9075)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2997 (8.5666)  time: 0.6688 (0.4956 -- 3.5559)  data: 0.1520 (0.0002 -- 3.0262)  max mem: 16413
Epoch: [60] Total time: 0:02:22 (0.8910 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.8590 (1.9189)  loss_scale: 32768.0000 (20480.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2997 (8.5666)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.6829 (0.6829)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2944 (2.2944 -- 2.2944)  data: 2.0806 (2.0806 -- 2.0806)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6888 (0.9465)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4022 (0.1999 -- 2.2944)  data: 0.1902 (0.0008 -- 2.0806)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6888 (0.8844)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (96.2963)  time: 0.2212 (0.1680 -- 0.4938)  data: 0.0175 (0.0001 -- 0.3049)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8503 (0.9298)  acc1: 66.6667 (70.1245)  acc5: 100.0000 (95.8506)  time: 0.2054 (0.1325 -- 0.4938)  data: 0.0172 (0.0001 -- 0.3049)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 74.896 Acc@5 96.058 loss 0.875
Accuracy of the network on the 482 val images: 74.90%
Max accuracy: 75.52%
Epoch: [61]  [  0/160]  eta: 0:16:33  lr: 0.000019  min_lr: 0.000000  loss: 1.9898 (1.9898)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0748 (8.0748)  time: 6.2095 (6.2095 -- 6.2095)  data: 5.4979 (5.4979 -- 5.4979)  max mem: 16413
Epoch: [61]  [ 20/160]  eta: 0:02:41  lr: 0.000019  min_lr: 0.000000  loss: 1.8204 (1.7964)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3863 (8.2701)  time: 0.8985 (0.5225 -- 2.1018)  data: 0.2282 (0.0008 -- 1.5768)  max mem: 16413
[2023-08-30 15:45:39,082] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9796
[2023-08-30 15:45:39,083] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:45:39,083] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9796
[2023-08-30 15:45:39,083] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 15:45:39,083] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [61]  [ 40/160]  eta: 0:01:55  lr: 0.000019  min_lr: 0.000000  loss: 1.7903 (1.8271)  loss_scale: 32768.0000 (30769.9512)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5842 (7.7590)  time: 0.7640 (0.5250 -- 1.8473)  data: 0.1629 (0.0002 -- 1.3378)  max mem: 16413
Epoch: [61]  [ 60/160]  eta: 0:01:33  lr: 0.000019  min_lr: 0.000000  loss: 1.8709 (1.8678)  loss_scale: 16384.0000 (26053.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5347 (7.9160)  time: 0.8805 (0.5150 -- 3.6945)  data: 0.2853 (0.0009 -- 3.1683)  max mem: 16413
Epoch: [61]  [ 80/160]  eta: 0:01:13  lr: 0.000019  min_lr: 0.000000  loss: 1.9087 (1.8708)  loss_scale: 16384.0000 (23665.7778)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3781 (7.7453)  time: 0.8842 (0.5393 -- 4.0156)  data: 0.1045 (0.0004 -- 2.0625)  max mem: 16413
Epoch: [61]  [100/160]  eta: 0:00:54  lr: 0.000019  min_lr: 0.000000  loss: 1.8810 (1.8636)  loss_scale: 16384.0000 (22223.8416)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6128 (7.9039)  time: 0.8176 (0.5154 -- 3.0352)  data: 0.1268 (0.0006 -- 2.5064)  max mem: 16413
[2023-08-30 15:46:40,988] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9867
[2023-08-30 15:46:40,988] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 15:46:40,988] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 9867
[2023-08-30 15:46:40,988] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 15:46:40,988] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [61]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8692 (1.8701)  loss_scale: 8192.0000 (20310.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8705 (8.0606)  time: 1.0020 (0.5133 -- 3.5959)  data: 0.2788 (0.0004 -- 2.0850)  max mem: 16413
Epoch: [61]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8433 (1.8777)  loss_scale: 8192.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6829 (8.1277)  time: 0.7992 (0.5231 -- 3.6373)  data: 0.1558 (0.0003 -- 3.0821)  max mem: 16413
Epoch: [61]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.0496 (1.8879)  loss_scale: 8192.0000 (17356.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2668 (8.1871)  time: 0.6863 (0.4960 -- 3.8548)  data: 0.1751 (0.0002 -- 3.3455)  max mem: 16413
Epoch: [61] Total time: 0:02:20 (0.8771 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.0496 (1.8859)  loss_scale: 8192.0000 (17356.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2668 (8.1871)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.6543 (0.6543)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3971 (2.3971 -- 2.3971)  data: 2.1196 (2.1196 -- 2.1196)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6930 (0.9318)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (95.9596)  time: 0.4286 (0.1987 -- 2.3971)  data: 0.2025 (0.0010 -- 2.1196)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6930 (0.8864)  acc1: 66.6667 (70.3704)  acc5: 100.0000 (96.2963)  time: 0.2188 (0.1690 -- 0.3264)  data: 0.0119 (0.0001 -- 0.1271)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8291 (0.9280)  acc1: 66.6667 (69.7095)  acc5: 100.0000 (95.8506)  time: 0.2009 (0.1327 -- 0.3264)  data: 0.0114 (0.0001 -- 0.1271)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 74.274 Acc@5 96.473 loss 0.865
Accuracy of the network on the 482 val images: 74.27%
Max accuracy: 75.52%
Epoch: [62]  [  0/160]  eta: 0:21:04  lr: 0.000019  min_lr: 0.000000  loss: 1.8046 (1.8046)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3989 (11.3989)  time: 7.9005 (7.9005 -- 7.9005)  data: 6.1491 (6.1491 -- 6.1491)  max mem: 16413
Epoch: [62]  [ 20/160]  eta: 0:02:38  lr: 0.000019  min_lr: 0.000000  loss: 2.0742 (1.9832)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0320 (8.1311)  time: 0.7940 (0.5227 -- 3.8410)  data: 0.1203 (0.0002 -- 1.5844)  max mem: 16413
Epoch: [62]  [ 40/160]  eta: 0:02:09  lr: 0.000019  min_lr: 0.000000  loss: 1.7246 (1.9079)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2647 (8.8521)  time: 1.0227 (0.5246 -- 3.9489)  data: 0.2411 (0.0005 -- 2.2454)  max mem: 16413
Epoch: [62]  [ 60/160]  eta: 0:01:40  lr: 0.000019  min_lr: 0.000000  loss: 1.9484 (1.9108)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5953 (8.7139)  time: 0.8530 (0.5134 -- 4.8498)  data: 0.0014 (0.0001 -- 0.0045)  max mem: 16413
[2023-08-30 15:48:45,492] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:48:45,493] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 15:48:45,494] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:48:45,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 15:48:47,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=55, lr=[4.4590002707514483e-07, 4.4590002707514483e-07, 5.945333694335264e-07, 5.945333694335264e-07, 7.927111592447019e-07, 7.927111592447019e-07, 1.0569482123262692e-06, 1.0569482123262692e-06, 1.4092642831016922e-06, 1.4092642831016922e-06, 1.8790190441355896e-06, 1.8790190441355896e-06, 2.5053587255141196e-06, 2.5053587255141196e-06, 3.340478300685493e-06, 3.340478300685493e-06, 4.4539710675806566e-06, 4.4539710675806566e-06, 5.938628090107543e-06, 5.938628090107543e-06, 7.918170786810056e-06, 7.918170786810056e-06, 1.0557561049080076e-05, 1.0557561049080076e-05, 1.40767480654401e-05, 1.40767480654401e-05, 1.87689974205868e-05, 1.87689974205868e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 15:48:47,163] [INFO] [timer.py:260:stop] epoch=0/micro_step=10000/global_step=10000, RunningAvgSamplesPerSec=18.067469979298366, CurrSamplesPerSec=22.73682612792739, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [62]  [ 80/160]  eta: 0:01:19  lr: 0.000019  min_lr: 0.000000  loss: 1.9886 (1.9174)  loss_scale: 8192.0000 (8697.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4162 (8.7539)  time: 0.9492 (0.5183 -- 3.7089)  data: 0.0013 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [62]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.8217 (1.9111)  loss_scale: 16384.0000 (10219.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2597 (8.8324)  time: 0.6992 (0.5254 -- 2.1381)  data: 0.0016 (0.0003 -- 0.0036)  max mem: 16413
Epoch: [62]  [120/160]  eta: 0:00:37  lr: 0.000019  min_lr: 0.000000  loss: 1.9422 (1.9187)  loss_scale: 16384.0000 (11238.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8667 (8.8869)  time: 0.9538 (0.5202 -- 4.5571)  data: 0.0016 (0.0002 -- 0.0045)  max mem: 16413
Epoch: [62]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.7943 (1.9053)  loss_scale: 16384.0000 (11968.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3341 (8.6951)  time: 0.8764 (0.5212 -- 4.5646)  data: 0.0021 (0.0004 -- 0.0092)  max mem: 16413
Epoch: [62]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 2.0069 (1.9073)  loss_scale: 16384.0000 (12492.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6377 (8.6529)  time: 0.5823 (0.4957 -- 1.7270)  data: 0.0008 (0.0002 -- 0.0036)  max mem: 16413
Epoch: [62] Total time: 0:02:21 (0.8874 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 2.0069 (1.8788)  loss_scale: 16384.0000 (12492.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6377 (8.6529)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.6180 (0.6180)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5549 (2.5549 -- 2.5549)  data: 2.3091 (2.3091 -- 2.3091)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6605 (0.9170)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4292 (0.2041 -- 2.5549)  data: 0.2109 (0.0005 -- 2.3091)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6605 (0.8683)  acc1: 77.7778 (73.0159)  acc5: 100.0000 (96.2963)  time: 0.2119 (0.1703 -- 0.3236)  data: 0.0062 (0.0001 -- 0.1093)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8106 (0.9099)  acc1: 66.6667 (70.9544)  acc5: 100.0000 (95.4357)  time: 0.1950 (0.1366 -- 0.3236)  data: 0.0058 (0.0001 -- 0.1093)  max mem: 16413
Val: Total time: 0:00:07 (0.2881 s / it)
* Acc@1 76.141 Acc@5 96.266 loss 0.855
Accuracy of the network on the 482 val images: 76.14%
[2023-08-30 15:50:00,338] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:50:00,340] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:50:00,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:50:00,340] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:50:01,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:50:01,906] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.14%
Epoch: [63]  [  0/160]  eta: 0:19:11  lr: 0.000019  min_lr: 0.000000  loss: 1.6471 (1.6471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5903 (7.5903)  time: 7.1958 (7.1958 -- 7.1958)  data: 6.0921 (6.0921 -- 6.0921)  max mem: 16413
Epoch: [63]  [ 20/160]  eta: 0:02:36  lr: 0.000019  min_lr: 0.000000  loss: 1.8067 (1.7392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9372 (8.4619)  time: 0.8106 (0.5331 -- 2.5802)  data: 0.0018 (0.0005 -- 0.0054)  max mem: 16413
Epoch: [63]  [ 40/160]  eta: 0:01:58  lr: 0.000019  min_lr: 0.000000  loss: 1.8071 (1.8611)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4441 (8.4393)  time: 0.8582 (0.5174 -- 3.6003)  data: 0.0012 (0.0002 -- 0.0025)  max mem: 16413
[2023-08-30 15:50:46,494] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:50:46,495] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:50:46,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:50:46,495] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:50:49,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10128
[2023-08-30 15:50:49,303] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10128
[2023-08-30 15:50:49,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:50:49,304] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:50:49,304] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [63]  [ 60/160]  eta: 0:01:35  lr: 0.000019  min_lr: 0.000000  loss: 1.8864 (1.8628)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4860 (8.3414)  time: 0.8709 (0.5256 -- 2.4295)  data: 0.1131 (0.0004 -- 1.7996)  max mem: 16413
Epoch: [63]  [ 80/160]  eta: 0:01:13  lr: 0.000019  min_lr: 0.000000  loss: 1.8376 (1.8490)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6443 (8.3318)  time: 0.8021 (0.5400 -- 4.3548)  data: 0.2505 (0.0004 -- 3.8255)  max mem: 16413
Epoch: [63]  [100/160]  eta: 0:00:55  lr: 0.000019  min_lr: 0.000000  loss: 1.9703 (1.8660)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2393 (8.4323)  time: 0.9418 (0.5351 -- 4.0134)  data: 0.3870 (0.0003 -- 3.5018)  max mem: 16413
Epoch: [63]  [120/160]  eta: 0:00:36  lr: 0.000019  min_lr: 0.000000  loss: 1.9217 (1.8759)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0853 (8.3131)  time: 0.8071 (0.5307 -- 2.0628)  data: 0.1287 (0.0004 -- 1.4398)  max mem: 16413
Epoch: [63]  [140/160]  eta: 0:00:18  lr: 0.000019  min_lr: 0.000000  loss: 1.8896 (1.8772)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0745 (8.3135)  time: 0.9199 (0.5257 -- 4.0175)  data: 0.0610 (0.0002 -- 0.5964)  max mem: 16413
Epoch: [63]  [159/160]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000000  loss: 1.7525 (1.8655)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8511 (8.3256)  time: 0.6605 (0.4934 -- 2.4411)  data: 0.0009 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [63] Total time: 0:02:20 (0.8757 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000000  loss: 1.7525 (1.8638)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8511 (8.3256)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.6608 (0.6608)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3451 (2.3451 -- 2.3451)  data: 2.1069 (2.1069 -- 2.1069)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6608 (0.9029)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4101 (0.1981 -- 2.3451)  data: 0.1925 (0.0008 -- 2.1069)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6409 (0.8485)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.2963)  time: 0.2189 (0.1714 -- 0.3289)  data: 0.0105 (0.0001 -- 0.1031)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7788 (0.8963)  acc1: 66.6667 (72.1992)  acc5: 100.0000 (95.8506)  time: 0.2037 (0.1329 -- 0.3289)  data: 0.0102 (0.0001 -- 0.1031)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 76.763 Acc@5 96.680 loss 0.843
Accuracy of the network on the 482 val images: 76.76%
[2023-08-30 15:52:29,740] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:52:29,742] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:52:29,742] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:52:29,742] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:52:31,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:52:31,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 76.76%
Epoch: [64]  [  0/160]  eta: 0:18:13  lr: 0.000019  min_lr: 0.000000  loss: 1.6498 (1.6498)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3443 (7.3443)  time: 6.8371 (6.8371 -- 6.8371)  data: 5.7606 (5.7606 -- 5.7606)  max mem: 16413
[2023-08-30 15:52:53,036] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:52:53,036] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:52:53,037] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:52:53,037] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [64]  [ 20/160]  eta: 0:02:41  lr: 0.000019  min_lr: 0.000000  loss: 1.9361 (1.9368)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6324 (8.7663)  time: 0.8715 (0.5167 -- 2.7387)  data: 0.0629 (0.0003 -- 0.8625)  max mem: 16413
[2023-08-30 15:53:02,912] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10265
[2023-08-30 15:53:02,912] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10265
[2023-08-30 15:53:02,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:53:02,913] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:53:02,913] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [64]  [ 40/160]  eta: 0:01:59  lr: 0.000019  min_lr: 0.000000  loss: 1.7887 (1.9060)  loss_scale: 16384.0000 (19580.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0998 (8.4921)  time: 0.8269 (0.5226 -- 5.3455)  data: 0.0017 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [64]  [ 60/160]  eta: 0:01:39  lr: 0.000018  min_lr: 0.000000  loss: 1.8617 (1.8855)  loss_scale: 16384.0000 (18532.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6084 (8.4141)  time: 1.0010 (0.5187 -- 3.9747)  data: 0.0379 (0.0004 -- 0.7327)  max mem: 16413
Epoch: [64]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 1.8863 (1.8810)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3200 (8.6649)  time: 0.8133 (0.5191 -- 3.5131)  data: 0.0355 (0.0002 -- 0.6891)  max mem: 16413
Epoch: [64]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.8657 (1.8864)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6711 (8.6624)  time: 0.8580 (0.5264 -- 3.2754)  data: 0.2233 (0.0004 -- 2.7145)  max mem: 16413
Epoch: [64]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.9237 (1.8929)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3253 (8.5494)  time: 0.9371 (0.5280 -- 3.2100)  data: 0.3910 (0.0006 -- 2.6927)  max mem: 16413
Epoch: [64]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.8794 (1.8847)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1424 (8.3590)  time: 0.9452 (0.5112 -- 3.6748)  data: 0.4066 (0.0003 -- 3.1610)  max mem: 16413
[2023-08-30 15:54:50,670] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:54:50,671] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:54:50,671] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:54:50,671] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [64]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9834 (1.8895)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7789 (8.3988)  time: 0.5392 (0.4980 -- 0.8576)  data: 0.0183 (0.0001 -- 0.3543)  max mem: 16413
Epoch: [64] Total time: 0:02:22 (0.8886 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9834 (1.8857)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7789 (8.3988)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.6998 (0.6998)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.1942 (2.1942 -- 2.1942)  data: 1.9873 (1.9873 -- 1.9873)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6998 (0.9113)  acc1: 77.7778 (69.6970)  acc5: 100.0000 (94.9495)  time: 0.4210 (0.1931 -- 2.1942)  data: 0.2022 (0.0007 -- 1.9873)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6707 (0.8599)  acc1: 66.6667 (71.9577)  acc5: 100.0000 (95.7672)  time: 0.2266 (0.1708 -- 0.4777)  data: 0.0202 (0.0001 -- 0.2199)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8232 (0.9130)  acc1: 66.6667 (70.9544)  acc5: 100.0000 (95.4357)  time: 0.2091 (0.1329 -- 0.4777)  data: 0.0199 (0.0001 -- 0.2199)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 75.519 Acc@5 96.473 loss 0.860
Accuracy of the network on the 482 val images: 75.52%
Max accuracy: 76.76%
Epoch: [65]  [  0/160]  eta: 0:24:38  lr: 0.000018  min_lr: 0.000000  loss: 1.6363 (1.6363)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6463 (10.6463)  time: 9.2436 (9.2436 -- 9.2436)  data: 6.2185 (6.2185 -- 6.2185)  max mem: 16413
[2023-08-30 15:55:24,418] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10416
[2023-08-30 15:55:24,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:55:24,419] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 15:55:24,418] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10416
[2023-08-30 15:55:24,420] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [65]  [ 20/160]  eta: 0:02:51  lr: 0.000018  min_lr: 0.000000  loss: 1.7546 (1.7842)  loss_scale: 32768.0000 (28867.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5139 (8.5993)  time: 0.8209 (0.5255 -- 3.7673)  data: 0.2455 (0.0005 -- 2.6139)  max mem: 16413
Epoch: [65]  [ 40/160]  eta: 0:02:05  lr: 0.000018  min_lr: 0.000000  loss: 1.7754 (1.7937)  loss_scale: 16384.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7756 (8.4813)  time: 0.8576 (0.5238 -- 4.4647)  data: 0.0280 (0.0004 -- 0.4174)  max mem: 16413
Epoch: [65]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.8069 (1.7842)  loss_scale: 16384.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0020 (8.4772)  time: 0.8099 (0.5279 -- 4.1604)  data: 0.2513 (0.0003 -- 3.6386)  max mem: 16413
Epoch: [65]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 2.0151 (1.8485)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8085 (8.4167)  time: 0.9417 (0.5109 -- 3.7738)  data: 0.4087 (0.0003 -- 3.2568)  max mem: 16413
Epoch: [65]  [100/160]  eta: 0:00:54  lr: 0.000018  min_lr: 0.000000  loss: 1.8007 (1.8449)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3761 (8.3821)  time: 0.7241 (0.5285 -- 3.5342)  data: 0.1726 (0.0003 -- 3.0134)  max mem: 16413
Epoch: [65]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7936 (1.8545)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0176 (8.4137)  time: 0.9523 (0.5331 -- 3.5640)  data: 0.1028 (0.0004 -- 1.0326)  max mem: 16413
Epoch: [65]  [140/160]  eta: 0:00:17  lr: 0.000018  min_lr: 0.000000  loss: 1.7684 (1.8477)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3062 (8.5209)  time: 0.7681 (0.5420 -- 2.2535)  data: 0.1008 (0.0004 -- 1.7208)  max mem: 16413
[2023-08-30 15:57:11,525] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:57:11,525] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:57:11,526] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:57:11,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:57:12,632] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10547
[2023-08-30 15:57:12,632] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10547
[2023-08-30 15:57:12,632] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:57:12,632] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:57:12,632] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [65]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.9677 (1.8571)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8944 (8.4118)  time: 0.6957 (0.4952 -- 2.1609)  data: 0.0476 (0.0002 -- 0.4729)  max mem: 16413
Epoch: [65] Total time: 0:02:20 (0.8758 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.9677 (1.8650)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8944 (8.4118)
Val:  [ 0/27]  eta: 0:00:53  loss: 0.6241 (0.6241)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 1.9944 (1.9944 -- 1.9944)  data: 1.7812 (1.7812 -- 1.7812)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6387 (0.9078)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (96.9697)  time: 0.4089 (0.1986 -- 1.9944)  data: 0.1950 (0.0009 -- 1.7812)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6387 (0.8591)  acc1: 66.6667 (74.6032)  acc5: 100.0000 (96.8254)  time: 0.2329 (0.1694 -- 0.5093)  data: 0.0286 (0.0001 -- 0.2964)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.8280 (0.9036)  acc1: 66.6667 (72.6141)  acc5: 100.0000 (95.8506)  time: 0.2166 (0.1326 -- 0.5093)  data: 0.0282 (0.0001 -- 0.2964)  max mem: 16413
Val: Total time: 0:00:07 (0.2829 s / it)
* Acc@1 77.178 Acc@5 96.266 loss 0.846
Accuracy of the network on the 482 val images: 77.18%
[2023-08-30 15:57:28,710] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 15:57:28,711] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 15:57:28,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 15:57:28,712] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 15:57:29,824] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 15:57:29,825] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.18%
Epoch: [66]  [  0/160]  eta: 0:17:27  lr: 0.000018  min_lr: 0.000000  loss: 1.4628 (1.4628)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0355 (9.0355)  time: 6.5457 (6.5457 -- 6.5457)  data: 6.0092 (6.0092 -- 6.0092)  max mem: 16413
Epoch: [66]  [ 20/160]  eta: 0:02:48  lr: 0.000018  min_lr: 0.000000  loss: 1.7777 (1.7599)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5284 (8.0386)  time: 0.9346 (0.5178 -- 3.8054)  data: 0.1391 (0.0004 -- 1.2109)  max mem: 16413
Epoch: [66]  [ 40/160]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000000  loss: 1.7901 (1.7704)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2456 (7.7680)  time: 0.8640 (0.5179 -- 3.8779)  data: 0.0368 (0.0003 -- 0.4106)  max mem: 16413
Epoch: [66]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7996 (1.8160)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0560 (8.0367)  time: 0.8028 (0.5215 -- 2.9538)  data: 0.0016 (0.0003 -- 0.0043)  max mem: 16413
Epoch: [66]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 2.0292 (1.8536)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8207 (8.1037)  time: 0.8695 (0.5297 -- 2.5755)  data: 0.2180 (0.0002 -- 1.9836)  max mem: 16413
Epoch: [66]  [100/160]  eta: 0:00:54  lr: 0.000018  min_lr: 0.000000  loss: 1.6624 (1.8380)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8016 (8.0831)  time: 0.7921 (0.5355 -- 1.8663)  data: 0.1195 (0.0004 -- 1.0739)  max mem: 16413
[2023-08-30 15:59:15,434] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:59:15,434] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 15:59:15,437] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 15:59:15,437] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [66]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.9500 (1.8520)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6130 (8.0474)  time: 0.8575 (0.5328 -- 2.1851)  data: 0.3004 (0.0006 -- 1.6571)  max mem: 16413
Epoch: [66]  [140/160]  eta: 0:00:17  lr: 0.000018  min_lr: 0.000000  loss: 1.8686 (1.8540)  loss_scale: 32768.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6089 (8.0343)  time: 0.8235 (0.5156 -- 2.5237)  data: 0.2027 (0.0004 -- 1.9930)  max mem: 16413
[2023-08-30 15:59:41,746] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10706
[2023-08-30 15:59:41,747] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 15:59:41,747] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 15:59:41,747] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10706
[2023-08-30 15:59:41,747] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [66]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8832 (1.8474)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7561 (8.0862)  time: 0.8046 (0.4948 -- 2.8992)  data: 0.1512 (0.0002 -- 1.5894)  max mem: 16413
Epoch: [66] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8832 (1.8868)  loss_scale: 16384.0000 (19456.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7561 (8.0862)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.6188 (0.6188)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3185 (2.3185 -- 2.3185)  data: 2.1049 (2.1049 -- 2.1049)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6188 (0.9053)  acc1: 77.7778 (67.6768)  acc5: 100.0000 (96.9697)  time: 0.4085 (0.2032 -- 2.3185)  data: 0.1926 (0.0005 -- 2.1049)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6943 (0.8510)  acc1: 77.7778 (72.4868)  acc5: 100.0000 (96.8254)  time: 0.2185 (0.1700 -- 0.3062)  data: 0.0113 (0.0001 -- 0.1138)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7718 (0.8995)  acc1: 77.7778 (71.7842)  acc5: 100.0000 (95.8506)  time: 0.2036 (0.1332 -- 0.3062)  data: 0.0109 (0.0001 -- 0.1138)  max mem: 16413
Val: Total time: 0:00:07 (0.2843 s / it)
* Acc@1 76.349 Acc@5 96.266 loss 0.843
Accuracy of the network on the 482 val images: 76.35%
Max accuracy: 77.18%
Epoch: [67]  [  0/160]  eta: 0:19:44  lr: 0.000018  min_lr: 0.000000  loss: 1.9888 (1.9888)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6414 (5.6414)  time: 7.4017 (7.4017 -- 7.4017)  data: 6.8855 (6.8855 -- 6.8855)  max mem: 16413
Epoch: [67]  [ 20/160]  eta: 0:02:37  lr: 0.000018  min_lr: 0.000000  loss: 1.8475 (1.8858)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6550 (7.7492)  time: 0.8102 (0.5269 -- 2.0880)  data: 0.0663 (0.0002 -- 1.0261)  max mem: 16413
Epoch: [67]  [ 40/160]  eta: 0:01:53  lr: 0.000018  min_lr: 0.000000  loss: 1.8521 (1.8757)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7337 (8.0242)  time: 0.7534 (0.5321 -- 2.1587)  data: 0.0058 (0.0003 -- 0.0938)  max mem: 16413
Epoch: [67]  [ 60/160]  eta: 0:01:34  lr: 0.000018  min_lr: 0.000000  loss: 1.8120 (1.8566)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8175 (8.0372)  time: 0.9630 (0.5152 -- 2.7016)  data: 0.1577 (0.0005 -- 1.3479)  max mem: 16413
Epoch: [67]  [ 80/160]  eta: 0:01:14  lr: 0.000018  min_lr: 0.000000  loss: 1.8287 (1.8611)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6522 (8.1105)  time: 0.8603 (0.5315 -- 3.3020)  data: 0.0512 (0.0004 -- 0.8911)  max mem: 16413
Epoch: [67]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.9710 (1.8827)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2938 (8.0658)  time: 0.9149 (0.5187 -- 3.0345)  data: 0.2384 (0.0003 -- 2.4979)  max mem: 16413
[2023-08-30 16:01:44,944] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:01:44,945] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:01:44,945] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:01:44,946] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [67]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.7989 (1.8755)  loss_scale: 16384.0000 (17196.4298)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9234 (8.1078)  time: 0.8095 (0.5191 -- 2.6895)  data: 0.2151 (0.0005 -- 2.1532)  max mem: 16413
Epoch: [67]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.9281 (1.8809)  loss_scale: 32768.0000 (19405.1631)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5652 (8.0388)  time: 0.8712 (0.5391 -- 2.4313)  data: 0.0016 (0.0006 -- 0.0038)  max mem: 16413
Epoch: [67]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8364 (1.8781)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2430 (8.0858)  time: 0.7213 (0.4959 -- 3.2253)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [67] Total time: 0:02:20 (0.8808 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8364 (1.8779)  loss_scale: 32768.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2430 (8.0858)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.6063 (0.6063)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2112 (2.2112 -- 2.2112)  data: 1.9929 (1.9929 -- 1.9929)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6063 (0.9033)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4042 (0.1939 -- 2.2112)  data: 0.1868 (0.0008 -- 1.9929)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6283 (0.8499)  acc1: 77.7778 (73.5450)  acc5: 100.0000 (96.8254)  time: 0.2210 (0.1712 -- 0.4133)  data: 0.0128 (0.0001 -- 0.1911)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7692 (0.8956)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (96.2656)  time: 0.2053 (0.1328 -- 0.4133)  data: 0.0125 (0.0001 -- 0.1911)  max mem: 16413
Val: Total time: 0:00:07 (0.2823 s / it)
* Acc@1 77.386 Acc@5 96.473 loss 0.836
Accuracy of the network on the 482 val images: 77.39%
[2023-08-30 16:02:27,236] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 16:02:27,238] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 16:02:27,238] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 16:02:27,238] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 16:02:28,549] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 16:02:28,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.39%
[2023-08-30 16:02:35,087] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10880
[2023-08-30 16:02:35,087] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 10880
[2023-08-30 16:02:35,087] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:02:35,088] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:02:35,088] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [68]  [  0/160]  eta: 0:17:26  lr: 0.000018  min_lr: 0.000000  loss: 2.1274 (2.1274)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4164 (11.4164)  time: 6.5385 (6.5385 -- 6.5385)  data: 6.0264 (6.0264 -- 6.0264)  max mem: 16413
Epoch: [68]  [ 20/160]  eta: 0:02:37  lr: 0.000018  min_lr: 0.000000  loss: 1.6224 (1.6910)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4098 (8.5064)  time: 0.8566 (0.5231 -- 3.7643)  data: 0.0843 (0.0002 -- 0.9933)  max mem: 16413
Epoch: [68]  [ 40/160]  eta: 0:02:08  lr: 0.000018  min_lr: 0.000000  loss: 1.9073 (1.7835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6228 (9.0165)  time: 1.0182 (0.5359 -- 3.8880)  data: 0.0031 (0.0001 -- 0.0160)  max mem: 16413
Epoch: [68]  [ 60/160]  eta: 0:01:35  lr: 0.000018  min_lr: 0.000000  loss: 1.7297 (1.7914)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9996 (8.7666)  time: 0.7185 (0.5319 -- 2.8677)  data: 0.0014 (0.0003 -- 0.0035)  max mem: 16413
Epoch: [68]  [ 80/160]  eta: 0:01:15  lr: 0.000018  min_lr: 0.000000  loss: 2.0775 (1.8622)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9878 (8.3813)  time: 0.9020 (0.5130 -- 2.8852)  data: 0.0017 (0.0004 -- 0.0037)  max mem: 16413
Epoch: [68]  [100/160]  eta: 0:00:55  lr: 0.000018  min_lr: 0.000000  loss: 1.9364 (1.8752)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5234 (8.4581)  time: 0.8446 (0.5153 -- 3.3669)  data: 0.0020 (0.0002 -- 0.0141)  max mem: 16413
[2023-08-30 16:04:19,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=61, lr=[4.2272779146247105e-07, 4.2272779146247105e-07, 5.636370552832948e-07, 5.636370552832948e-07, 7.515160737110597e-07, 7.515160737110597e-07, 1.0020214316147463e-06, 1.0020214316147463e-06, 1.3360285754863282e-06, 1.3360285754863282e-06, 1.7813714339817711e-06, 1.7813714339817711e-06, 2.375161911975695e-06, 2.375161911975695e-06, 3.1668825493009265e-06, 3.1668825493009265e-06, 4.222510065734568e-06, 4.222510065734568e-06, 5.6300134209794245e-06, 5.6300134209794245e-06, 7.5066845613059e-06, 7.5066845613059e-06, 1.0008912748407866e-05, 1.0008912748407866e-05, 1.3345216997877155e-05, 1.3345216997877155e-05, 1.7793622663836206e-05, 1.7793622663836206e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 16:04:19,621] [INFO] [timer.py:260:stop] epoch=0/micro_step=11000/global_step=11000, RunningAvgSamplesPerSec=17.927209894171217, CurrSamplesPerSec=20.969318221586732, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [68]  [120/160]  eta: 0:00:37  lr: 0.000018  min_lr: 0.000000  loss: 1.9222 (1.8777)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2142 (8.3833)  time: 0.9481 (0.5146 -- 2.8568)  data: 0.0012 (0.0002 -- 0.0035)  max mem: 16413
[2023-08-30 16:04:27,663] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:04:27,663] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:04:27,664] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:04:27,665] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:04:34,494] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11017
[2023-08-30 16:04:34,494] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11017
[2023-08-30 16:04:34,496] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:04:34,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:04:34,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [68]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 2.1128 (1.9005)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6039 (8.4994)  time: 0.7650 (0.5216 -- 2.6081)  data: 0.0012 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [68]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.8058 (1.8895)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1854 (8.5400)  time: 0.7094 (0.4960 -- 2.2675)  data: 0.0007 (0.0002 -- 0.0020)  max mem: 16413
Epoch: [68] Total time: 0:02:21 (0.8829 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.8058 (1.8833)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1854 (8.5400)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5982 (0.5982)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4152 (2.4152 -- 2.4152)  data: 2.1741 (2.1741 -- 2.1741)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6251 (0.9011)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (96.9697)  time: 0.4156 (0.1930 -- 2.4152)  data: 0.1994 (0.0007 -- 2.1741)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6466 (0.8301)  acc1: 77.7778 (74.0741)  acc5: 100.0000 (96.8254)  time: 0.2173 (0.1701 -- 0.4469)  data: 0.0135 (0.0001 -- 0.2242)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7410 (0.8836)  acc1: 77.7778 (73.0290)  acc5: 100.0000 (95.8506)  time: 0.1984 (0.1338 -- 0.4469)  data: 0.0128 (0.0001 -- 0.2242)  max mem: 16413
Val: Total time: 0:00:07 (0.2870 s / it)
* Acc@1 77.178 Acc@5 96.266 loss 0.829
Accuracy of the network on the 482 val images: 77.18%
Max accuracy: 77.39%
Epoch: [69]  [  0/160]  eta: 0:23:42  lr: 0.000018  min_lr: 0.000000  loss: 2.5114 (2.5114)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3125 (7.3125)  time: 8.8929 (8.8929 -- 8.8929)  data: 8.3743 (8.3743 -- 8.3743)  max mem: 16413
Epoch: [69]  [ 20/160]  eta: 0:02:54  lr: 0.000018  min_lr: 0.000000  loss: 1.8960 (1.8809)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9256 (9.7400)  time: 0.8638 (0.5239 -- 3.7565)  data: 0.3154 (0.0004 -- 3.2323)  max mem: 16413
Epoch: [69]  [ 40/160]  eta: 0:02:04  lr: 0.000018  min_lr: 0.000000  loss: 1.7860 (1.8691)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4850 (9.2381)  time: 0.8180 (0.5244 -- 3.8291)  data: 0.2737 (0.0003 -- 3.2868)  max mem: 16413
[2023-08-30 16:05:50,689] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11095
[2023-08-30 16:05:50,689] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11095
[2023-08-30 16:05:50,689] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:05:50,689] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:05:50,689] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [69]  [ 60/160]  eta: 0:01:36  lr: 0.000018  min_lr: 0.000000  loss: 1.9810 (1.8956)  loss_scale: 16384.0000 (15578.2295)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8053 (8.8919)  time: 0.8028 (0.5291 -- 3.1837)  data: 0.1953 (0.0003 -- 2.6699)  max mem: 16413
Epoch: [69]  [ 80/160]  eta: 0:01:16  lr: 0.000018  min_lr: 0.000000  loss: 1.9644 (1.8906)  loss_scale: 8192.0000 (13754.4691)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5141 (8.9223)  time: 0.9582 (0.5235 -- 3.5529)  data: 0.4152 (0.0005 -- 3.0255)  max mem: 16413
Epoch: [69]  [100/160]  eta: 0:00:56  lr: 0.000018  min_lr: 0.000000  loss: 2.0564 (1.9113)  loss_scale: 8192.0000 (12652.9901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0143 (8.7899)  time: 0.8654 (0.5293 -- 3.6231)  data: 0.2998 (0.0005 -- 3.1090)  max mem: 16413
Epoch: [69]  [120/160]  eta: 0:00:36  lr: 0.000018  min_lr: 0.000000  loss: 1.8979 (1.9174)  loss_scale: 8192.0000 (11915.6364)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6878 (8.9185)  time: 0.7946 (0.5334 -- 3.3653)  data: 0.2110 (0.0002 -- 2.8269)  max mem: 16413
Epoch: [69]  [140/160]  eta: 0:00:18  lr: 0.000018  min_lr: 0.000000  loss: 1.8817 (1.9174)  loss_scale: 8192.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8221 (8.8556)  time: 0.9138 (0.5305 -- 4.2376)  data: 0.0016 (0.0003 -- 0.0064)  max mem: 16413
Epoch: [69]  [159/160]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000000  loss: 1.6871 (1.9018)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3643 (8.7077)  time: 0.6567 (0.4960 -- 2.6974)  data: 0.0015 (0.0001 -- 0.0138)  max mem: 16413
Epoch: [69] Total time: 0:02:21 (0.8864 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000000  loss: 1.6871 (1.8802)  loss_scale: 8192.0000 (11008.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3643 (8.7077)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5992 (0.5992)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4351 (2.4351 -- 2.4351)  data: 2.1944 (2.1944 -- 2.1944)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6179 (0.8969)  acc1: 77.7778 (68.6869)  acc5: 100.0000 (95.9596)  time: 0.4190 (0.1921 -- 2.4351)  data: 0.2004 (0.0005 -- 2.1944)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6179 (0.8439)  acc1: 77.7778 (74.6032)  acc5: 100.0000 (96.2963)  time: 0.2193 (0.1689 -- 0.4881)  data: 0.0150 (0.0001 -- 0.2875)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7432 (0.8897)  acc1: 77.7778 (74.2739)  acc5: 100.0000 (95.8506)  time: 0.2027 (0.1327 -- 0.4881)  data: 0.0147 (0.0001 -- 0.2875)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 77.801 Acc@5 96.680 loss 0.824
Accuracy of the network on the 482 val images: 77.80%
[2023-08-30 16:07:27,209] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 16:07:27,210] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 16:07:27,210] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 16:07:27,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 16:07:28,570] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 16:07:28,571] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 77.80%
Epoch: [70]  [  0/160]  eta: 0:24:45  lr: 0.000018  min_lr: 0.000000  loss: 1.7264 (1.7264)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8852 (11.8852)  time: 9.2865 (9.2865 -- 9.2865)  data: 6.1080 (6.1080 -- 6.1080)  max mem: 16413
Epoch: [70]  [ 20/160]  eta: 0:02:54  lr: 0.000018  min_lr: 0.000000  loss: 1.7357 (1.7369)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9762 (8.2768)  time: 0.8434 (0.5275 -- 3.9179)  data: 0.1216 (0.0003 -- 2.3947)  max mem: 16413
[2023-08-30 16:07:58,237] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:07:58,237] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:07:58,237] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 16:07:58,237] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [70]  [ 40/160]  eta: 0:02:11  lr: 0.000018  min_lr: 0.000000  loss: 1.6650 (1.7498)  loss_scale: 16384.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9643 (8.2882)  time: 0.9451 (0.5268 -- 4.4696)  data: 0.0087 (0.0002 -- 0.1512)  max mem: 16413
Epoch: [70]  [ 60/160]  eta: 0:01:38  lr: 0.000018  min_lr: 0.000000  loss: 1.9434 (1.7990)  loss_scale: 16384.0000 (13160.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4621 (8.4921)  time: 0.7578 (0.5218 -- 2.6882)  data: 0.0016 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [70]  [ 80/160]  eta: 0:01:19  lr: 0.000018  min_lr: 0.000000  loss: 1.7780 (1.8178)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1712 (8.7566)  time: 1.0030 (0.5005 -- 5.3603)  data: 0.0016 (0.0003 -- 0.0075)  max mem: 16413
Epoch: [70]  [100/160]  eta: 0:00:57  lr: 0.000017  min_lr: 0.000000  loss: 2.0327 (1.8454)  loss_scale: 16384.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7868 (8.5197)  time: 0.8077 (0.5247 -- 3.6289)  data: 0.0013 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [70]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.9177 (1.8664)  loss_scale: 16384.0000 (14759.1405)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6786 (8.5278)  time: 0.7688 (0.5182 -- 2.3294)  data: 0.0018 (0.0004 -- 0.0044)  max mem: 16413
Epoch: [70]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.8387 (1.8767)  loss_scale: 16384.0000 (14989.6170)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7407 (8.6234)  time: 0.8256 (0.5328 -- 3.8009)  data: 0.0115 (0.0005 -- 0.2011)  max mem: 16413
[2023-08-30 16:09:46,545] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:09:46,545] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:09:46,550] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:09:46,550] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [70]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.9178 (1.8849)  loss_scale: 16384.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3529 (8.4876)  time: 0.6840 (0.4972 -- 3.0496)  data: 0.1584 (0.0002 -- 2.5075)  max mem: 16413
Epoch: [70] Total time: 0:02:21 (0.8843 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.9178 (1.8640)  loss_scale: 16384.0000 (15974.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3529 (8.4876)
Val:  [ 0/27]  eta: 0:01:09  loss: 0.5825 (0.5825)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.5643 (2.5643 -- 2.5643)  data: 2.3347 (2.3347 -- 2.3347)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6306 (0.8911)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (96.9697)  time: 0.4343 (0.2046 -- 2.5643)  data: 0.2139 (0.0004 -- 2.3347)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6306 (0.8272)  acc1: 77.7778 (75.1323)  acc5: 100.0000 (96.8254)  time: 0.2135 (0.1703 -- 0.2919)  data: 0.0054 (0.0001 -- 0.0876)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7464 (0.8709)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (96.2656)  time: 0.1954 (0.1332 -- 0.2919)  data: 0.0048 (0.0001 -- 0.0876)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 79.046 Acc@5 96.266 loss 0.817
Accuracy of the network on the 482 val images: 79.05%
[2023-08-30 16:09:57,895] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 16:09:57,897] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 16:09:57,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 16:09:57,897] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 16:09:59,269] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 16:09:59,270] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.05%
Epoch: [71]  [  0/160]  eta: 0:18:51  lr: 0.000017  min_lr: 0.000000  loss: 1.5910 (1.5910)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8518 (7.8518)  time: 7.0738 (7.0738 -- 7.0738)  data: 5.0604 (5.0604 -- 5.0604)  max mem: 16413
[2023-08-30 16:10:08,095] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11363
[2023-08-30 16:10:08,095] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11363
[2023-08-30 16:10:08,095] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:10:08,095] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:10:08,095] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [71]  [ 20/160]  eta: 0:02:33  lr: 0.000017  min_lr: 0.000000  loss: 1.8773 (1.7622)  loss_scale: 16384.0000 (18724.5714)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5517 (8.5872)  time: 0.7992 (0.5294 -- 2.6253)  data: 0.1683 (0.0004 -- 1.6106)  max mem: 16413
Epoch: [71]  [ 40/160]  eta: 0:02:06  lr: 0.000017  min_lr: 0.000000  loss: 1.7466 (1.7849)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5102 (8.2433)  time: 1.0014 (0.5250 -- 3.8427)  data: 0.0612 (0.0008 -- 1.1940)  max mem: 16413
Epoch: [71]  [ 60/160]  eta: 0:01:40  lr: 0.000017  min_lr: 0.000000  loss: 1.9125 (1.8145)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6886 (8.7786)  time: 0.8999 (0.5280 -- 4.2272)  data: 0.0013 (0.0004 -- 0.0049)  max mem: 16413
Epoch: [71]  [ 80/160]  eta: 0:01:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7484 (1.8056)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6907 (8.8043)  time: 0.8966 (0.5205 -- 2.7817)  data: 0.0015 (0.0001 -- 0.0029)  max mem: 16413
Epoch: [71]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.9175 (1.8282)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6192 (8.8647)  time: 0.7397 (0.5117 -- 2.9298)  data: 0.0015 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [71]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.9957 (1.8544)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9458 (8.8319)  time: 0.9597 (0.5209 -- 3.6985)  data: 0.0019 (0.0003 -- 0.0139)  max mem: 16413
[2023-08-30 16:12:01,948] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:12:01,948] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:12:01,949] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:12:01,950] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [71]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 2.0625 (1.8731)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3206 (8.8236)  time: 0.8890 (0.5004 -- 4.4400)  data: 0.0015 (0.0003 -- 0.0095)  max mem: 16413
[2023-08-30 16:12:10,588] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11501
[2023-08-30 16:12:10,588] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11501
[2023-08-30 16:12:10,588] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:12:10,588] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:12:10,588] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [71]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8443 (1.8734)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4506 (8.7678)  time: 0.6263 (0.4962 -- 2.8864)  data: 0.0004 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [71] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8443 (1.8630)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4506 (8.7678)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.5596 (0.5596)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2205 (2.2205 -- 2.2205)  data: 2.0033 (2.0033 -- 2.0033)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6022 (0.9072)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (95.9596)  time: 0.4014 (0.2007 -- 2.2205)  data: 0.1923 (0.0007 -- 2.0033)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6022 (0.8366)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.2963)  time: 0.2220 (0.1700 -- 0.4736)  data: 0.0205 (0.0001 -- 0.2490)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7472 (0.8787)  acc1: 77.7778 (75.1037)  acc5: 100.0000 (95.4357)  time: 0.2095 (0.1332 -- 0.4736)  data: 0.0202 (0.0001 -- 0.2490)  max mem: 16413
Val: Total time: 0:00:07 (0.2832 s / it)
* Acc@1 78.838 Acc@5 96.266 loss 0.819
Accuracy of the network on the 482 val images: 78.84%
Max accuracy: 79.05%
Epoch: [72]  [  0/160]  eta: 0:19:05  lr: 0.000017  min_lr: 0.000000  loss: 1.6667 (1.6667)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3517 (8.3517)  time: 7.1576 (7.1576 -- 7.1576)  data: 6.6155 (6.6155 -- 6.6155)  max mem: 16413
[2023-08-30 16:12:39,240] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11524
[2023-08-30 16:12:39,240] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11524
[2023-08-30 16:12:39,241] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:12:39,241] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:12:39,241] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [72]  [ 20/160]  eta: 0:02:44  lr: 0.000017  min_lr: 0.000000  loss: 1.9097 (1.8682)  loss_scale: 8192.0000 (9752.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3156 (8.6807)  time: 0.8780 (0.5337 -- 4.2455)  data: 0.3168 (0.0004 -- 3.6777)  max mem: 16413
Epoch: [72]  [ 40/160]  eta: 0:02:12  lr: 0.000017  min_lr: 0.000000  loss: 1.8590 (1.8748)  loss_scale: 8192.0000 (8991.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2772 (8.2062)  time: 1.0291 (0.5174 -- 3.9913)  data: 0.4858 (0.0003 -- 3.4492)  max mem: 16413
Epoch: [72]  [ 60/160]  eta: 0:01:36  lr: 0.000017  min_lr: 0.000000  loss: 1.7586 (1.8430)  loss_scale: 8192.0000 (8729.1803)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7099 (8.5381)  time: 0.6881 (0.5216 -- 3.1297)  data: 0.1395 (0.0001 -- 2.5968)  max mem: 16413
Epoch: [72]  [ 80/160]  eta: 0:01:16  lr: 0.000017  min_lr: 0.000000  loss: 1.9146 (1.8569)  loss_scale: 8192.0000 (8596.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4352 (8.7092)  time: 0.9428 (0.5159 -- 4.0557)  data: 0.4006 (0.0003 -- 3.4827)  max mem: 16413
Epoch: [72]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.9801 (1.8553)  loss_scale: 8192.0000 (8516.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0171 (8.4535)  time: 0.7901 (0.5339 -- 3.2694)  data: 0.2377 (0.0003 -- 2.7350)  max mem: 16413
Epoch: [72]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.9359 (1.8549)  loss_scale: 8192.0000 (8462.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1414 (8.5255)  time: 0.9012 (0.5274 -- 3.5987)  data: 0.3556 (0.0004 -- 3.0860)  max mem: 16413
[2023-08-30 16:14:32,054] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:14:32,054] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:14:32,054] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 16:14:32,054] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [72]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.9598 (1.8723)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0382 (8.5354)  time: 0.8306 (0.5185 -- 3.9406)  data: 0.2919 (0.0004 -- 3.4021)  max mem: 16413
Epoch: [72]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8147 (1.8731)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3514 (8.5294)  time: 0.6729 (0.4950 -- 2.9553)  data: 0.1560 (0.0002 -- 2.4302)  max mem: 16413
Epoch: [72] Total time: 0:02:21 (0.8831 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8147 (1.8622)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3514 (8.5294)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.5780 (0.5780)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4119 (2.4119 -- 2.4119)  data: 2.1929 (2.1929 -- 2.1929)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6867 (0.8842)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (96.9697)  time: 0.4267 (0.1934 -- 2.4119)  data: 0.2129 (0.0007 -- 2.1929)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6770 (0.8257)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.8254)  time: 0.2186 (0.1704 -- 0.3455)  data: 0.0144 (0.0001 -- 0.1368)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7389 (0.8750)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (96.2656)  time: 0.2039 (0.1328 -- 0.3455)  data: 0.0141 (0.0001 -- 0.1368)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 78.008 Acc@5 96.473 loss 0.816
Accuracy of the network on the 482 val images: 78.01%
Max accuracy: 79.05%
Epoch: [73]  [  0/160]  eta: 0:16:47  lr: 0.000017  min_lr: 0.000000  loss: 2.3571 (2.3571)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7956 (10.7956)  time: 6.3000 (6.3000 -- 6.3000)  data: 5.7333 (5.7333 -- 5.7333)  max mem: 16413
[2023-08-30 16:15:13,211] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11688
[2023-08-30 16:15:13,211] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:15:13,211] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11688
[2023-08-30 16:15:13,212] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:15:13,212] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [73]  [ 20/160]  eta: 0:02:55  lr: 0.000017  min_lr: 0.000000  loss: 1.8291 (1.9002)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8947 (7.8128)  time: 0.9983 (0.5350 -- 3.2069)  data: 0.1740 (0.0003 -- 1.6183)  max mem: 16413
Epoch: [73]  [ 40/160]  eta: 0:02:04  lr: 0.000017  min_lr: 0.000000  loss: 1.7443 (1.8466)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5941 (7.8094)  time: 0.8107 (0.5237 -- 2.6926)  data: 0.2112 (0.0004 -- 2.1216)  max mem: 16413
Epoch: [73]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000000  loss: 1.7194 (1.8287)  loss_scale: 8192.0000 (9266.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7825 (8.0847)  time: 0.8915 (0.5366 -- 3.1112)  data: 0.2775 (0.0007 -- 2.5750)  max mem: 16413
Epoch: [73]  [ 80/160]  eta: 0:01:17  lr: 0.000017  min_lr: 0.000000  loss: 1.8436 (1.8259)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0435 (8.0695)  time: 0.9182 (0.5153 -- 2.8792)  data: 0.0681 (0.0004 -- 1.3353)  max mem: 16413
Epoch: [73]  [100/160]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000000  loss: 1.8005 (1.8318)  loss_scale: 8192.0000 (8840.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7368 (7.9407)  time: 0.7888 (0.5212 -- 4.0880)  data: 0.0011 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [73]  [120/160]  eta: 0:00:37  lr: 0.000017  min_lr: 0.000000  loss: 1.8376 (1.8324)  loss_scale: 8192.0000 (8733.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4997 (8.0349)  time: 0.8821 (0.5326 -- 3.0193)  data: 0.0025 (0.0002 -- 0.0145)  max mem: 16413
[2023-08-30 16:17:07,854] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:17:07,854] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 16:17:07,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:17:07,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [73]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.8038 (1.8317)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5361 (8.1023)  time: 0.9173 (0.5132 -- 5.0669)  data: 0.0012 (0.0004 -- 0.0023)  max mem: 16413
Epoch: [73]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8445 (1.8333)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2105 (8.1163)  time: 0.5697 (0.4964 -- 1.4360)  data: 0.0007 (0.0003 -- 0.0018)  max mem: 16413
Epoch: [73] Total time: 0:02:21 (0.8833 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8445 (1.8630)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2105 (8.1163)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.5289 (0.5289)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2291 (2.2291 -- 2.2291)  data: 1.9798 (1.9798 -- 1.9798)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.6629 (0.8768)  acc1: 77.7778 (70.7071)  acc5: 100.0000 (95.9596)  time: 0.4060 (0.1906 -- 2.2291)  data: 0.1982 (0.0007 -- 1.9798)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6629 (0.8108)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.2963)  time: 0.2246 (0.1694 -- 0.4588)  data: 0.0255 (0.0001 -- 0.2700)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6848 (0.8544)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.8506)  time: 0.2131 (0.1328 -- 0.4588)  data: 0.0253 (0.0001 -- 0.2700)  max mem: 16413
Val: Total time: 0:00:07 (0.2855 s / it)
* Acc@1 79.046 Acc@5 96.473 loss 0.799
Accuracy of the network on the 482 val images: 79.05%
[2023-08-30 16:17:28,005] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 16:17:28,007] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 16:17:28,007] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 16:17:28,007] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 16:17:29,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 16:17:29,399] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.05%
Epoch: [74]  [  0/160]  eta: 0:20:07  lr: 0.000017  min_lr: 0.000000  loss: 1.5235 (1.5235)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8305 (5.8305)  time: 7.5461 (7.5461 -- 7.5461)  data: 7.0243 (7.0243 -- 7.0243)  max mem: 16413
Epoch: [74]  [ 20/160]  eta: 0:02:35  lr: 0.000017  min_lr: 0.000000  loss: 1.8092 (1.7886)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1468 (8.1036)  time: 0.7895 (0.5202 -- 2.4365)  data: 0.1381 (0.0002 -- 1.9175)  max mem: 16413
Epoch: [74]  [ 40/160]  eta: 0:02:02  lr: 0.000017  min_lr: 0.000000  loss: 1.8130 (1.8061)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1268 (8.8013)  time: 0.9240 (0.5279 -- 2.8281)  data: 0.3619 (0.0004 -- 2.3037)  max mem: 16413
Epoch: [74]  [ 60/160]  eta: 0:01:37  lr: 0.000017  min_lr: 0.000000  loss: 1.9475 (1.8484)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3277 (8.8837)  time: 0.8836 (0.5213 -- 3.1207)  data: 0.3109 (0.0005 -- 2.5934)  max mem: 16413
Epoch: [74]  [ 80/160]  eta: 0:01:14  lr: 0.000017  min_lr: 0.000000  loss: 1.9043 (1.8603)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4230 (8.8652)  time: 0.8192 (0.5308 -- 2.3163)  data: 0.1963 (0.0004 -- 1.4647)  max mem: 16413
Epoch: [74]  [100/160]  eta: 0:00:54  lr: 0.000017  min_lr: 0.000000  loss: 1.7308 (1.8473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5004 (8.7562)  time: 0.8246 (0.5274 -- 2.5649)  data: 0.0635 (0.0002 -- 0.7640)  max mem: 16413
[2023-08-30 16:19:08,598] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:19:08,598] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:19:08,599] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:19:08,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:19:09,621] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11947
[2023-08-30 16:19:09,621] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 11947
[2023-08-30 16:19:09,621] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:19:09,621] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:19:09,621] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [74]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 2.0187 (1.8635)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6004 (8.6538)  time: 0.9172 (0.5022 -- 4.0062)  data: 0.0285 (0.0002 -- 0.5512)  max mem: 16413
Epoch: [74]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.9954 (1.8716)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3258 (8.6952)  time: 0.8899 (0.5260 -- 2.7824)  data: 0.1501 (0.0004 -- 1.5091)  max mem: 16413
[2023-08-30 16:19:50,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=68, lr=[3.980991634758528e-07, 3.980991634758528e-07, 5.307988846344704e-07, 5.307988846344704e-07, 7.077318461792938e-07, 7.077318461792938e-07, 9.436424615723918e-07, 9.436424615723918e-07, 1.258189948763189e-06, 1.258189948763189e-06, 1.6775865983509187e-06, 1.6775865983509187e-06, 2.2367821311345584e-06, 2.2367821311345584e-06, 2.9823761748460777e-06, 2.9823761748460777e-06, 3.976501566461437e-06, 3.976501566461437e-06, 5.302002088615249e-06, 5.302002088615249e-06, 7.069336118153666e-06, 7.069336118153666e-06, 9.425781490871555e-06, 9.425781490871555e-06, 1.2567708654495406e-05, 1.2567708654495406e-05, 1.675694487266054e-05, 1.675694487266054e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 16:19:50,935] [INFO] [timer.py:260:stop] epoch=0/micro_step=12000/global_step=12000, RunningAvgSamplesPerSec=17.868972307540012, CurrSamplesPerSec=23.91683884199342, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [74]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.8413 (1.8646)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4585 (8.7406)  time: 0.6763 (0.4948 -- 2.3363)  data: 0.0296 (0.0002 -- 0.5820)  max mem: 16413
Epoch: [74] Total time: 0:02:21 (0.8846 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.8413 (1.8692)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4585 (8.7406)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.5779 (0.5779)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.6125 (2.6125 -- 2.6125)  data: 2.3799 (2.3799 -- 2.3799)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6106 (0.8875)  acc1: 77.7778 (71.7172)  acc5: 100.0000 (96.9697)  time: 0.4328 (0.2000 -- 2.6125)  data: 0.2187 (0.0005 -- 2.3799)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6359 (0.8288)  acc1: 77.7778 (75.6614)  acc5: 100.0000 (96.8254)  time: 0.2112 (0.1693 -- 0.3364)  data: 0.0096 (0.0001 -- 0.1630)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7058 (0.8678)  acc1: 77.7778 (74.6888)  acc5: 100.0000 (95.8506)  time: 0.1964 (0.1328 -- 0.3364)  data: 0.0086 (0.0001 -- 0.1630)  max mem: 16413
Val: Total time: 0:00:07 (0.2911 s / it)
* Acc@1 79.461 Acc@5 96.473 loss 0.807
Accuracy of the network on the 482 val images: 79.46%
[2023-08-30 16:19:58,802] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 16:19:58,804] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 16:19:58,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 16:19:58,804] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 16:20:00,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 16:20:00,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 79.46%
Epoch: [75]  [  0/160]  eta: 0:18:12  lr: 0.000017  min_lr: 0.000000  loss: 2.1056 (2.1056)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3189 (8.3189)  time: 6.8257 (6.8257 -- 6.8257)  data: 6.2822 (6.2822 -- 6.2822)  max mem: 16413
Epoch: [75]  [ 20/160]  eta: 0:02:40  lr: 0.000017  min_lr: 0.000000  loss: 1.8660 (1.8952)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3276 (7.8852)  time: 0.8605 (0.5388 -- 3.1206)  data: 0.1132 (0.0003 -- 1.9895)  max mem: 16413
[2023-08-30 16:20:35,405] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12033
[2023-08-30 16:20:35,405] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:20:35,405] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12033
[2023-08-30 16:20:35,406] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:20:35,406] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [75]  [ 40/160]  eta: 0:02:02  lr: 0.000017  min_lr: 0.000000  loss: 1.9472 (1.9278)  loss_scale: 16384.0000 (14785.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0153 (7.9566)  time: 0.8835 (0.5231 -- 3.8318)  data: 0.0265 (0.0004 -- 0.5018)  max mem: 16413
Epoch: [75]  [ 60/160]  eta: 0:01:34  lr: 0.000017  min_lr: 0.000000  loss: 1.9072 (1.9116)  loss_scale: 8192.0000 (12623.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3610 (8.0694)  time: 0.7969 (0.5316 -- 2.7333)  data: 0.0879 (0.0006 -- 1.7026)  max mem: 16413
Epoch: [75]  [ 80/160]  eta: 0:01:15  lr: 0.000017  min_lr: 0.000000  loss: 1.7952 (1.8892)  loss_scale: 8192.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8523 (8.0449)  time: 0.9506 (0.5272 -- 2.0432)  data: 0.2076 (0.0005 -- 1.5073)  max mem: 16413
Epoch: [75]  [100/160]  eta: 0:00:55  lr: 0.000017  min_lr: 0.000000  loss: 1.6967 (1.8594)  loss_scale: 8192.0000 (10868.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5690 (8.0444)  time: 0.8485 (0.5341 -- 3.4288)  data: 0.0012 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [75]  [120/160]  eta: 0:00:36  lr: 0.000017  min_lr: 0.000000  loss: 1.7283 (1.8553)  loss_scale: 8192.0000 (10426.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6190 (7.9860)  time: 0.9143 (0.5244 -- 3.2699)  data: 0.0018 (0.0004 -- 0.0072)  max mem: 16413
Epoch: [75]  [140/160]  eta: 0:00:18  lr: 0.000017  min_lr: 0.000000  loss: 1.7791 (1.8438)  loss_scale: 8192.0000 (10109.2766)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9925 (8.0373)  time: 0.8031 (0.5100 -- 3.6706)  data: 0.0011 (0.0004 -- 0.0023)  max mem: 16413
Epoch: [75]  [159/160]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000000  loss: 1.7640 (1.8421)  loss_scale: 8192.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9661 (8.1418)  time: 0.7150 (0.4963 -- 2.5731)  data: 0.0006 (0.0002 -- 0.0023)  max mem: 16413
Epoch: [75] Total time: 0:02:21 (0.8857 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000000  loss: 1.7640 (1.8367)  loss_scale: 8192.0000 (9881.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9661 (8.1418)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.5364 (0.5364)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.2587 (2.2587 -- 2.2587)  data: 2.0161 (2.0161 -- 2.0161)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5534 (0.8698)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4029 (0.1978 -- 2.2587)  data: 0.1847 (0.0006 -- 2.0161)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5899 (0.8120)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2215 (0.1704 -- 0.4099)  data: 0.0152 (0.0001 -- 0.2066)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6976 (0.8527)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (95.8506)  time: 0.2068 (0.1329 -- 0.4099)  data: 0.0148 (0.0001 -- 0.2066)  max mem: 16413
Val: Total time: 0:00:07 (0.2846 s / it)
* Acc@1 80.083 Acc@5 96.680 loss 0.791
Accuracy of the network on the 482 val images: 80.08%
[2023-08-30 16:22:29,788] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 16:22:29,790] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 16:22:29,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 16:22:29,790] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 16:22:31,363] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 16:22:31,363] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.08%
Epoch: [76]  [  0/160]  eta: 0:20:41  lr: 0.000017  min_lr: 0.000000  loss: 1.8791 (1.8791)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2237 (12.2237)  time: 7.7576 (7.7576 -- 7.7576)  data: 6.2249 (6.2249 -- 6.2249)  max mem: 16413
[2023-08-30 16:22:40,220] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:22:40,220] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 16:22:40,221] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:22:40,222] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [76]  [ 20/160]  eta: 0:02:39  lr: 0.000017  min_lr: 0.000000  loss: 1.7697 (1.7715)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9286 (8.4058)  time: 0.8118 (0.5199 -- 3.4056)  data: 0.2683 (0.0004 -- 2.8614)  max mem: 16413
Epoch: [76]  [ 40/160]  eta: 0:02:03  lr: 0.000017  min_lr: 0.000000  loss: 1.6509 (1.6918)  loss_scale: 16384.0000 (15984.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8527 (8.7310)  time: 0.9106 (0.5159 -- 3.1060)  data: 0.2310 (0.0003 -- 2.5350)  max mem: 16413
Epoch: [76]  [ 60/160]  eta: 0:01:38  lr: 0.000017  min_lr: 0.000000  loss: 1.7834 (1.7418)  loss_scale: 16384.0000 (16115.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0805 (8.6265)  time: 0.9039 (0.5341 -- 2.9349)  data: 0.0016 (0.0004 -- 0.0073)  max mem: 16413
Epoch: [76]  [ 80/160]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000000  loss: 1.9600 (1.8082)  loss_scale: 16384.0000 (16181.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2580 (8.6247)  time: 0.7826 (0.5247 -- 2.3732)  data: 0.0100 (0.0003 -- 0.1716)  max mem: 16413
Epoch: [76]  [100/160]  eta: 0:00:54  lr: 0.000016  min_lr: 0.000000  loss: 1.8719 (1.8111)  loss_scale: 16384.0000 (16221.7822)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0877 (8.5948)  time: 0.7985 (0.5397 -- 3.1078)  data: 0.0086 (0.0002 -- 0.1222)  max mem: 16413
Epoch: [76]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.8630 (1.8183)  loss_scale: 16384.0000 (16248.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5638 (8.6388)  time: 0.9666 (0.5181 -- 4.8912)  data: 0.0010 (0.0003 -- 0.0022)  max mem: 16413
[2023-08-30 16:24:30,346] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:24:30,346] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:24:30,347] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:24:30,347] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:24:37,354] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12299
[2023-08-30 16:24:37,354] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12299
[2023-08-30 16:24:37,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:24:37,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:24:37,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [76]  [140/160]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000000  loss: 1.8359 (1.8185)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5806 (8.8144)  time: 0.7652 (0.5330 -- 2.7153)  data: 0.0015 (0.0003 -- 0.0044)  max mem: 16413
Epoch: [76]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.8378 (1.8149)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9036 (8.7652)  time: 0.7109 (0.4961 -- 3.2380)  data: 0.0013 (0.0003 -- 0.0084)  max mem: 16413
Epoch: [76] Total time: 0:02:20 (0.8763 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.8378 (1.8479)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9036 (8.7652)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.5423 (0.5423)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.4637 (2.4637 -- 2.4637)  data: 2.1891 (2.1891 -- 2.1891)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5657 (0.8591)  acc1: 77.7778 (72.7273)  acc5: 100.0000 (95.9596)  time: 0.4271 (0.2015 -- 2.4637)  data: 0.1999 (0.0007 -- 2.1891)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5657 (0.7929)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (96.2963)  time: 0.2155 (0.1714 -- 0.2443)  data: 0.0048 (0.0001 -- 0.0504)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6802 (0.8374)  acc1: 77.7778 (76.3485)  acc5: 100.0000 (95.4357)  time: 0.1982 (0.1328 -- 0.2443)  data: 0.0045 (0.0001 -- 0.0504)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 80.913 Acc@5 96.266 loss 0.777
Accuracy of the network on the 482 val images: 80.91%
[2023-08-30 16:24:59,348] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 16:24:59,349] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 16:24:59,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 16:24:59,350] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 16:25:00,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 16:25:00,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 80.91%
Epoch: [77]  [  0/160]  eta: 0:20:03  lr: 0.000016  min_lr: 0.000000  loss: 2.3232 (2.3232)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1980 (7.1980)  time: 7.5208 (7.5208 -- 7.5208)  data: 6.9993 (6.9993 -- 6.9993)  max mem: 16413
Epoch: [77]  [ 20/160]  eta: 0:02:46  lr: 0.000016  min_lr: 0.000000  loss: 1.9134 (1.9348)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8488 (8.2168)  time: 0.8732 (0.5197 -- 4.1308)  data: 0.3258 (0.0004 -- 3.6051)  max mem: 16413
Epoch: [77]  [ 40/160]  eta: 0:02:02  lr: 0.000016  min_lr: 0.000000  loss: 1.8835 (1.8687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8084 (8.3790)  time: 0.8514 (0.5232 -- 4.0523)  data: 0.3002 (0.0003 -- 3.5078)  max mem: 16413
Epoch: [77]  [ 60/160]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7822 (1.8251)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6971 (8.3613)  time: 0.8274 (0.5353 -- 3.1044)  data: 0.2072 (0.0004 -- 1.4316)  max mem: 16413
Epoch: [77]  [ 80/160]  eta: 0:01:16  lr: 0.000016  min_lr: 0.000000  loss: 1.7420 (1.8112)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3516 (8.3958)  time: 0.9254 (0.5187 -- 3.2742)  data: 0.3749 (0.0003 -- 2.7298)  max mem: 16413
Epoch: [77]  [100/160]  eta: 0:00:55  lr: 0.000016  min_lr: 0.000000  loss: 1.7224 (1.7997)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3859 (8.4211)  time: 0.8575 (0.5216 -- 3.4791)  data: 0.3159 (0.0003 -- 2.9164)  max mem: 16413
[2023-08-30 16:26:41,582] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:26:41,582] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:26:41,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:26:41,585] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:26:42,152] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12429
[2023-08-30 16:26:42,152] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12429
[2023-08-30 16:26:42,152] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:26:42,152] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:26:42,152] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [77]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7252 (1.7942)  loss_scale: 16384.0000 (16519.4050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4906 (8.4506)  time: 0.7795 (0.5259 -- 2.7804)  data: 0.2240 (0.0004 -- 2.2384)  max mem: 16413
Epoch: [77]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.7259 (1.7861)  loss_scale: 16384.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0819 (8.3868)  time: 0.9137 (0.5182 -- 3.8925)  data: 0.3657 (0.0004 -- 3.3527)  max mem: 16413
Epoch: [77]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9553 (1.8023)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4445 (8.3189)  time: 0.7186 (0.4962 -- 4.5537)  data: 0.2031 (0.0002 -- 4.0530)  max mem: 16413
Epoch: [77] Total time: 0:02:21 (0.8872 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9553 (1.8501)  loss_scale: 16384.0000 (16486.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4445 (8.3189)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.5476 (0.5476)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3297 (2.3297 -- 2.3297)  data: 2.0292 (2.0292 -- 2.0292)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5897 (0.8517)  acc1: 77.7778 (73.7374)  acc5: 100.0000 (95.9596)  time: 0.4141 (0.2032 -- 2.3297)  data: 0.1896 (0.0007 -- 2.0292)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5971 (0.7982)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (96.2963)  time: 0.2231 (0.1696 -- 0.3375)  data: 0.0135 (0.0001 -- 0.1639)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6884 (0.8404)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.4357)  time: 0.2080 (0.1332 -- 0.3375)  data: 0.0128 (0.0001 -- 0.1639)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 80.498 Acc@5 96.266 loss 0.780
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 80.91%
Epoch: [78]  [  0/160]  eta: 0:21:43  lr: 0.000016  min_lr: 0.000000  loss: 1.8070 (1.8070)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9164 (7.9164)  time: 8.1473 (8.1473 -- 8.1473)  data: 7.5700 (7.5700 -- 7.5700)  max mem: 16413
Epoch: [78]  [ 20/160]  eta: 0:02:42  lr: 0.000016  min_lr: 0.000000  loss: 1.6724 (1.7240)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5595 (8.6796)  time: 0.8117 (0.5198 -- 3.9842)  data: 0.2629 (0.0004 -- 3.4359)  max mem: 16413
Epoch: [78]  [ 40/160]  eta: 0:02:02  lr: 0.000016  min_lr: 0.000000  loss: 1.8034 (1.8312)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9506 (8.2890)  time: 0.8807 (0.5237 -- 2.9759)  data: 0.2711 (0.0004 -- 2.4567)  max mem: 16413
Epoch: [78]  [ 60/160]  eta: 0:01:38  lr: 0.000016  min_lr: 0.000000  loss: 1.9039 (1.8388)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5881 (8.5049)  time: 0.8915 (0.5319 -- 3.1937)  data: 0.0219 (0.0003 -- 0.4103)  max mem: 16413
[2023-08-30 16:28:44,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:28:44,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:28:44,130] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:28:44,130] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [78]  [ 80/160]  eta: 0:01:15  lr: 0.000016  min_lr: 0.000000  loss: 1.7282 (1.8146)  loss_scale: 16384.0000 (16990.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7977 (8.4580)  time: 0.8298 (0.5293 -- 2.2625)  data: 0.1445 (0.0002 -- 1.3790)  max mem: 16413
[2023-08-30 16:28:57,116] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12573
[2023-08-30 16:28:57,116] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12573
[2023-08-30 16:28:57,116] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:28:57,116] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:28:57,116] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [78]  [100/160]  eta: 0:00:54  lr: 0.000016  min_lr: 0.000000  loss: 1.8527 (1.8361)  loss_scale: 32768.0000 (18817.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5452 (8.5775)  time: 0.7553 (0.5147 -- 3.4529)  data: 0.1595 (0.0002 -- 2.9344)  max mem: 16413
Epoch: [78]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7911 (1.8432)  loss_scale: 16384.0000 (18415.0744)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5158 (8.6207)  time: 0.9285 (0.5298 -- 2.8177)  data: 0.1142 (0.0002 -- 1.1395)  max mem: 16413
Epoch: [78]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.7369 (1.8314)  loss_scale: 16384.0000 (18126.9787)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8378 (8.6245)  time: 0.8826 (0.5111 -- 3.6899)  data: 0.0113 (0.0003 -- 0.1874)  max mem: 16413
Epoch: [78]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.8494 (1.8346)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3951 (8.6640)  time: 0.6622 (0.4970 -- 2.3431)  data: 0.0008 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [78] Total time: 0:02:20 (0.8780 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.8494 (1.8556)  loss_scale: 16384.0000 (17920.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3951 (8.6640)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5097 (0.5097)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3762 (2.3762 -- 2.3762)  data: 2.1455 (2.1455 -- 2.1455)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5834 (0.8574)  acc1: 88.8889 (74.7475)  acc5: 100.0000 (94.9495)  time: 0.4208 (0.2054 -- 2.3762)  data: 0.2019 (0.0005 -- 2.1455)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5834 (0.8001)  acc1: 77.7778 (76.7196)  acc5: 100.0000 (95.7672)  time: 0.2192 (0.1705 -- 0.3965)  data: 0.0133 (0.0001 -- 0.1872)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6976 (0.8460)  acc1: 77.7778 (75.5187)  acc5: 100.0000 (95.4357)  time: 0.2031 (0.1330 -- 0.3965)  data: 0.0130 (0.0001 -- 0.1872)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 80.083 Acc@5 96.680 loss 0.783
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.91%
Epoch: [79]  [  0/160]  eta: 0:18:02  lr: 0.000016  min_lr: 0.000000  loss: 1.9484 (1.9484)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4227 (5.4227)  time: 6.7655 (6.7655 -- 6.7655)  data: 5.7846 (5.7846 -- 5.7846)  max mem: 16413
Epoch: [79]  [ 20/160]  eta: 0:02:45  lr: 0.000016  min_lr: 0.000000  loss: 1.8018 (1.8341)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3323 (8.4472)  time: 0.9035 (0.5262 -- 2.7224)  data: 0.3597 (0.0004 -- 2.1742)  max mem: 16413
Epoch: [79]  [ 40/160]  eta: 0:02:05  lr: 0.000016  min_lr: 0.000000  loss: 1.9819 (1.9109)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3609 (8.4378)  time: 0.9021 (0.5173 -- 2.9978)  data: 0.3526 (0.0004 -- 2.4865)  max mem: 16413
Epoch: [79]  [ 60/160]  eta: 0:01:37  lr: 0.000016  min_lr: 0.000000  loss: 2.0265 (1.9186)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6762 (8.5482)  time: 0.8170 (0.5361 -- 3.2957)  data: 0.2718 (0.0002 -- 2.7750)  max mem: 16413
[2023-08-30 16:30:59,093] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:30:59,093] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:30:59,094] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:30:59,094] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:31:06,336] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12710
[2023-08-30 16:31:06,336] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12710
[2023-08-30 16:31:06,336] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:31:06,337] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:31:06,337] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [79]  [ 80/160]  eta: 0:01:18  lr: 0.000016  min_lr: 0.000000  loss: 1.9031 (1.9044)  loss_scale: 16384.0000 (18002.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9108 (8.2020)  time: 1.0108 (0.5215 -- 4.8003)  data: 0.4719 (0.0004 -- 4.2787)  max mem: 16413
Epoch: [79]  [100/160]  eta: 0:00:54  lr: 0.000016  min_lr: 0.000000  loss: 2.0011 (1.8989)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0131 (8.2073)  time: 0.6457 (0.5194 -- 2.2427)  data: 0.1012 (0.0002 -- 1.7178)  max mem: 16413
Epoch: [79]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7072 (1.8580)  loss_scale: 16384.0000 (17467.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7162 (8.2875)  time: 0.9078 (0.5159 -- 3.0643)  data: 0.3624 (0.0008 -- 2.5273)  max mem: 16413
Epoch: [79]  [140/160]  eta: 0:00:18  lr: 0.000016  min_lr: 0.000000  loss: 1.8407 (1.8517)  loss_scale: 16384.0000 (17313.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1255 (8.2591)  time: 0.8512 (0.5238 -- 3.4554)  data: 0.2976 (0.0005 -- 2.9045)  max mem: 16413
Epoch: [79]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7846 (1.8557)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0152 (8.2810)  time: 0.6989 (0.4952 -- 2.5088)  data: 0.1739 (0.0002 -- 1.9733)  max mem: 16413
Epoch: [79] Total time: 0:02:20 (0.8811 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.7846 (1.8607)  loss_scale: 16384.0000 (17203.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0152 (8.2810)
[2023-08-30 16:32:19,744] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-79 is about to be saved!
[2023-08-30 16:32:19,746] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
[2023-08-30 16:32:19,748] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt
[2023-08-30 16:32:19,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt...
[2023-08-30 16:32:20,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-79/mp_rank_00_model_states.pt.
[2023-08-30 16:32:20,797] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-79 is ready now!
Val:  [ 0/27]  eta: 0:00:58  loss: 0.4672 (0.4672)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1680 (2.1680 -- 2.1680)  data: 1.9273 (1.9273 -- 1.9273)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5725 (0.8620)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (94.9495)  time: 0.4325 (0.2065 -- 2.1680)  data: 0.2063 (0.0007 -- 1.9273)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5725 (0.8022)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2319 (0.1690 -- 0.5560)  data: 0.0234 (0.0001 -- 0.3304)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7044 (0.8392)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.0207)  time: 0.2122 (0.1337 -- 0.5560)  data: 0.0231 (0.0001 -- 0.3304)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 80.705 Acc@5 96.058 loss 0.777
Accuracy of the network on the 482 val images: 80.71%
Max accuracy: 80.91%
Epoch: [80]  [  0/160]  eta: 0:20:44  lr: 0.000016  min_lr: 0.000000  loss: 1.8991 (1.8991)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0335 (10.0335)  time: 7.7794 (7.7794 -- 7.7794)  data: 7.2560 (7.2560 -- 7.2560)  max mem: 16413
Epoch: [80]  [ 20/160]  eta: 0:02:42  lr: 0.000016  min_lr: 0.000000  loss: 1.8086 (1.8732)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1520 (7.8046)  time: 0.8326 (0.5253 -- 2.9874)  data: 0.0232 (0.0002 -- 0.4261)  max mem: 16413
[2023-08-30 16:33:09,071] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:33:09,071] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:33:09,071] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:33:09,072] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [80]  [ 40/160]  eta: 0:02:00  lr: 0.000016  min_lr: 0.000000  loss: 1.7990 (1.8446)  loss_scale: 16384.0000 (17183.2195)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3181 (8.2238)  time: 0.8294 (0.5266 -- 3.5921)  data: 0.0012 (0.0003 -- 0.0033)  max mem: 16413
[2023-08-30 16:33:20,176] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12852
[2023-08-30 16:33:20,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:33:20,176] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 12852
[2023-08-30 16:33:20,176] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:33:20,177] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [80]  [ 60/160]  eta: 0:01:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7766 (1.8420)  loss_scale: 32768.0000 (19875.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3402 (8.5979)  time: 0.8848 (0.5213 -- 3.1043)  data: 0.0155 (0.0002 -- 0.1687)  max mem: 16413
Epoch: [80]  [ 80/160]  eta: 0:01:14  lr: 0.000016  min_lr: 0.000000  loss: 1.9485 (1.8698)  loss_scale: 16384.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7628 (8.5288)  time: 0.8282 (0.5274 -- 3.8815)  data: 0.0024 (0.0004 -- 0.0163)  max mem: 16413
Epoch: [80]  [100/160]  eta: 0:00:53  lr: 0.000016  min_lr: 0.000000  loss: 1.7929 (1.8591)  loss_scale: 16384.0000 (18492.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2450 (8.3920)  time: 0.7765 (0.5178 -- 4.1313)  data: 0.0016 (0.0002 -- 0.0098)  max mem: 16413
Epoch: [80]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.8438 (1.8582)  loss_scale: 16384.0000 (18144.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1869 (8.5001)  time: 0.9319 (0.5318 -- 3.1012)  data: 0.3152 (0.0003 -- 2.3807)  max mem: 16413
Epoch: [80]  [140/160]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000000  loss: 1.6284 (1.8335)  loss_scale: 16384.0000 (17894.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0226 (8.5948)  time: 0.8391 (0.5279 -- 3.7100)  data: 0.2170 (0.0004 -- 2.8378)  max mem: 16413
Epoch: [80]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.9167 (1.8402)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3601 (8.6746)  time: 0.7258 (0.4963 -- 3.0269)  data: 0.0254 (0.0002 -- 0.4964)  max mem: 16413
Epoch: [80] Total time: 0:02:20 (0.8765 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.9167 (1.8335)  loss_scale: 16384.0000 (17715.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3601 (8.6746)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4962 (0.4962)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3335 (2.3335 -- 2.3335)  data: 2.1037 (2.1037 -- 2.1037)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5666 (0.8415)  acc1: 77.7778 (74.7475)  acc5: 100.0000 (95.9596)  time: 0.4113 (0.1992 -- 2.3335)  data: 0.1926 (0.0009 -- 2.1037)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5666 (0.7909)  acc1: 77.7778 (77.2487)  acc5: 100.0000 (96.2963)  time: 0.2189 (0.1716 -- 0.3774)  data: 0.0129 (0.0001 -- 0.1829)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6858 (0.8305)  acc1: 77.7778 (75.9336)  acc5: 100.0000 (96.2656)  time: 0.2020 (0.1333 -- 0.3774)  data: 0.0124 (0.0001 -- 0.1829)  max mem: 16413
Val: Total time: 0:00:07 (0.2853 s / it)
* Acc@1 79.046 Acc@5 96.473 loss 0.774
Accuracy of the network on the 482 val images: 79.05%
Max accuracy: 80.91%
Epoch: [81]  [  0/160]  eta: 0:19:41  lr: 0.000016  min_lr: 0.000000  loss: 2.1844 (2.1844)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5993 (6.5993)  time: 7.3874 (7.3874 -- 7.3874)  data: 5.2051 (5.2051 -- 5.2051)  max mem: 16413
Epoch: [81]  [ 20/160]  eta: 0:02:43  lr: 0.000016  min_lr: 0.000000  loss: 1.8089 (1.8679)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1721 (9.1590)  time: 0.8538 (0.5300 -- 3.2723)  data: 0.1491 (0.0010 -- 1.1692)  max mem: 16413
[2023-08-30 16:35:21,571] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:35:21,571] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:35:21,572] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:35:21,572] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:35:37,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=74, lr=[3.7226363924115567e-07, 3.7226363924115567e-07, 4.963515189882075e-07, 4.963515189882075e-07, 6.618020253176101e-07, 6.618020253176101e-07, 8.824027004234801e-07, 8.824027004234801e-07, 1.1765369338979734e-06, 1.1765369338979734e-06, 1.5687159118639646e-06, 1.5687159118639646e-06, 2.0916212158186196e-06, 2.0916212158186196e-06, 2.7888282877581594e-06, 2.7888282877581594e-06, 3.718437717010879e-06, 3.718437717010879e-06, 4.957916956014506e-06, 4.957916956014506e-06, 6.610555941352674e-06, 6.610555941352674e-06, 8.814074588470233e-06, 8.814074588470233e-06, 1.1752099451293643e-05, 1.1752099451293643e-05, 1.566946593505819e-05, 1.566946593505819e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 16:35:37,054] [INFO] [timer.py:260:stop] epoch=0/micro_step=13000/global_step=13000, RunningAvgSamplesPerSec=17.854303030743186, CurrSamplesPerSec=21.940054567178702, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [81]  [ 40/160]  eta: 0:02:00  lr: 0.000016  min_lr: 0.000000  loss: 1.5478 (1.7642)  loss_scale: 32768.0000 (24376.1951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1438 (8.8419)  time: 0.8291 (0.5259 -- 3.1156)  data: 0.0517 (0.0002 -- 1.0005)  max mem: 16413
[2023-08-30 16:35:51,129] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13016
[2023-08-30 16:35:51,129] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13016
[2023-08-30 16:35:51,130] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:35:51,130] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:35:51,130] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [81]  [ 60/160]  eta: 0:01:35  lr: 0.000016  min_lr: 0.000000  loss: 1.9610 (1.8368)  loss_scale: 32768.0000 (25784.6557)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9154 (8.4489)  time: 0.8546 (0.5358 -- 2.4028)  data: 0.1170 (0.0003 -- 0.9374)  max mem: 16413
Epoch: [81]  [ 80/160]  eta: 0:01:12  lr: 0.000016  min_lr: 0.000000  loss: 1.8227 (1.8210)  loss_scale: 16384.0000 (23463.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4647 (8.4943)  time: 0.7690 (0.5308 -- 1.7527)  data: 0.1213 (0.0005 -- 1.2182)  max mem: 16413
Epoch: [81]  [100/160]  eta: 0:00:54  lr: 0.000016  min_lr: 0.000000  loss: 1.8527 (1.8242)  loss_scale: 16384.0000 (22061.6238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4707 (8.4566)  time: 0.9437 (0.5316 -- 3.5678)  data: 0.2995 (0.0005 -- 3.0385)  max mem: 16413
Epoch: [81]  [120/160]  eta: 0:00:36  lr: 0.000016  min_lr: 0.000000  loss: 1.7248 (1.8104)  loss_scale: 16384.0000 (21123.1736)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2902 (8.5009)  time: 0.9297 (0.5293 -- 3.6036)  data: 0.0957 (0.0007 -- 0.7905)  max mem: 16413
Epoch: [81]  [140/160]  eta: 0:00:17  lr: 0.000016  min_lr: 0.000000  loss: 1.7586 (1.8109)  loss_scale: 16384.0000 (20450.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9229 (8.5670)  time: 0.7651 (0.5243 -- 2.4801)  data: 0.0014 (0.0003 -- 0.0033)  max mem: 16413
Epoch: [81]  [159/160]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000000  loss: 1.8322 (1.8099)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8754 (8.6568)  time: 0.7391 (0.4963 -- 3.9781)  data: 0.0005 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [81] Total time: 0:02:20 (0.8785 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000000  loss: 1.8322 (1.8328)  loss_scale: 16384.0000 (19968.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8754 (8.6568)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5490 (0.5490)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3838 (2.3838 -- 2.3838)  data: 2.1716 (2.1716 -- 2.1716)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5490 (0.8376)  acc1: 88.8889 (76.7677)  acc5: 100.0000 (94.9495)  time: 0.4320 (0.2002 -- 2.3838)  data: 0.2168 (0.0008 -- 2.1716)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5791 (0.7851)  acc1: 88.8889 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2171 (0.1708 -- 0.4010)  data: 0.0109 (0.0001 -- 0.1971)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.7011 (0.8217)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.8506)  time: 0.2001 (0.1332 -- 0.4010)  data: 0.0103 (0.0001 -- 0.1971)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 80.083 Acc@5 96.888 loss 0.769
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.91%
Epoch: [82]  [  0/160]  eta: 0:20:11  lr: 0.000016  min_lr: 0.000000  loss: 2.0590 (2.0590)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5187 (8.5187)  time: 7.5714 (7.5714 -- 7.5714)  data: 7.0372 (7.0372 -- 7.0372)  max mem: 16413
Epoch: [82]  [ 20/160]  eta: 0:02:45  lr: 0.000016  min_lr: 0.000000  loss: 1.8299 (1.9203)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6761 (8.3651)  time: 0.8664 (0.5256 -- 4.5729)  data: 0.2352 (0.0005 -- 4.0490)  max mem: 16413
[2023-08-30 16:37:55,535] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:37:55,535] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:37:55,536] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:37:55,536] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [82]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000000  loss: 1.7300 (1.8222)  loss_scale: 32768.0000 (22777.7561)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2603 (8.6654)  time: 0.9007 (0.5264 -- 3.5507)  data: 0.0500 (0.0005 -- 0.9488)  max mem: 16413
Epoch: [82]  [ 60/160]  eta: 0:01:36  lr: 0.000015  min_lr: 0.000000  loss: 1.8046 (1.7964)  loss_scale: 32768.0000 (26053.2459)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3420 (8.8414)  time: 0.7925 (0.5277 -- 3.0463)  data: 0.0152 (0.0002 -- 0.2724)  max mem: 16413
Epoch: [82]  [ 80/160]  eta: 0:01:14  lr: 0.000015  min_lr: 0.000000  loss: 2.0369 (1.8430)  loss_scale: 32768.0000 (27711.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4473 (8.8342)  time: 0.8115 (0.5268 -- 2.6745)  data: 0.1082 (0.0003 -- 1.1172)  max mem: 16413
[2023-08-30 16:38:41,496] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13201
[2023-08-30 16:38:41,496] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13201
[2023-08-30 16:38:41,496] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:38:41,496] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:38:41,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [82]  [100/160]  eta: 0:00:54  lr: 0.000015  min_lr: 0.000000  loss: 1.9264 (1.8616)  loss_scale: 16384.0000 (25468.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8889 (8.7197)  time: 0.8409 (0.5270 -- 2.5889)  data: 0.1470 (0.0005 -- 1.8609)  max mem: 16413
Epoch: [82]  [120/160]  eta: 0:00:35  lr: 0.000015  min_lr: 0.000000  loss: 1.7786 (1.8524)  loss_scale: 16384.0000 (23966.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2406 (8.7446)  time: 0.8448 (0.5153 -- 3.1240)  data: 0.2493 (0.0004 -- 2.6007)  max mem: 16413
[2023-08-30 16:39:25,531] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13251
[2023-08-30 16:39:25,531] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:39:25,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 16:39:25,531] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13251
[2023-08-30 16:39:25,532] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [82]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.6747 (1.8406)  loss_scale: 8192.0000 (22310.1277)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0875 (8.6783)  time: 0.9728 (0.5137 -- 4.1593)  data: 0.4359 (0.0004 -- 3.6345)  max mem: 16413
Epoch: [82]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.6820 (1.8284)  loss_scale: 8192.0000 (20633.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3031 (8.5933)  time: 0.6406 (0.4956 -- 1.7644)  data: 0.1111 (0.0002 -- 1.2397)  max mem: 16413
Epoch: [82] Total time: 0:02:20 (0.8778 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.6820 (1.8511)  loss_scale: 8192.0000 (20633.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3031 (8.5933)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.5291 (0.5291)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3533 (2.3533 -- 2.3533)  data: 2.1270 (2.1270 -- 2.1270)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6160 (0.8351)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (94.9495)  time: 0.4198 (0.2011 -- 2.3533)  data: 0.2016 (0.0009 -- 2.1270)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.6151 (0.7851)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2227 (0.1706 -- 0.3951)  data: 0.0137 (0.0001 -- 0.1802)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6879 (0.8225)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.8506)  time: 0.2052 (0.1329 -- 0.3951)  data: 0.0126 (0.0001 -- 0.1802)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 80.083 Acc@5 96.473 loss 0.771
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.91%
Epoch: [83]  [  0/160]  eta: 0:19:35  lr: 0.000015  min_lr: 0.000000  loss: 2.2552 (2.2552)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7952 (8.7952)  time: 7.3454 (7.3454 -- 7.3454)  data: 6.8177 (6.8177 -- 6.8177)  max mem: 16413
Epoch: [83]  [ 20/160]  eta: 0:02:40  lr: 0.000015  min_lr: 0.000000  loss: 1.7752 (1.9080)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0677 (8.7256)  time: 0.8354 (0.5274 -- 2.2894)  data: 0.2056 (0.0003 -- 1.7662)  max mem: 16413
Epoch: [83]  [ 40/160]  eta: 0:02:04  lr: 0.000015  min_lr: 0.000000  loss: 1.5994 (1.8068)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8579 (8.9172)  time: 0.9228 (0.5219 -- 3.0428)  data: 0.0511 (0.0002 -- 0.5211)  max mem: 16413
Epoch: [83]  [ 60/160]  eta: 0:01:40  lr: 0.000015  min_lr: 0.000000  loss: 1.7640 (1.7956)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3454 (8.8189)  time: 0.9335 (0.5290 -- 3.1756)  data: 0.0134 (0.0002 -- 0.2449)  max mem: 16413
Epoch: [83]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 1.7498 (1.8052)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2953 (8.6912)  time: 0.8045 (0.5277 -- 3.2482)  data: 0.0015 (0.0002 -- 0.0050)  max mem: 16413
[2023-08-30 16:41:26,524] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:41:26,524] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 16:41:26,526] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:41:26,526] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [83]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 2.0768 (1.8503)  loss_scale: 8192.0000 (8273.1089)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6202 (8.7461)  time: 0.8064 (0.5371 -- 3.0251)  data: 0.0150 (0.0003 -- 0.2739)  max mem: 16413
Epoch: [83]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.6201 (1.8273)  loss_scale: 16384.0000 (9613.7521)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4038 (8.6682)  time: 0.9156 (0.5282 -- 2.8837)  data: 0.0013 (0.0003 -- 0.0025)  max mem: 16413
Epoch: [83]  [140/160]  eta: 0:00:17  lr: 0.000015  min_lr: 0.000000  loss: 1.6311 (1.8161)  loss_scale: 16384.0000 (10574.0709)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8689 (8.6074)  time: 0.7466 (0.5242 -- 2.4727)  data: 0.0017 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [83]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.9958 (1.8374)  loss_scale: 16384.0000 (11264.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5014 (8.5966)  time: 0.8117 (0.4940 -- 4.8204)  data: 0.0011 (0.0001 -- 0.0046)  max mem: 16413
Epoch: [83] Total time: 0:02:22 (0.8896 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.9958 (1.8569)  loss_scale: 16384.0000 (11264.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5014 (8.5966)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.5009 (0.5009)  acc1: 88.8889 (88.8889)  acc5: 100.0000 (100.0000)  time: 2.3963 (2.3963 -- 2.3963)  data: 2.1589 (2.1589 -- 2.1589)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6157 (0.8364)  acc1: 77.7778 (75.7576)  acc5: 100.0000 (94.9495)  time: 0.4319 (0.2034 -- 2.3963)  data: 0.2099 (0.0008 -- 2.1589)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5688 (0.7773)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (95.7672)  time: 0.2254 (0.1705 -- 0.3912)  data: 0.0169 (0.0001 -- 0.1863)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6925 (0.8195)  acc1: 77.7778 (77.1784)  acc5: 100.0000 (95.0207)  time: 0.2079 (0.1331 -- 0.3912)  data: 0.0160 (0.0001 -- 0.1863)  max mem: 16413
Val: Total time: 0:00:07 (0.2931 s / it)
* Acc@1 79.461 Acc@5 95.851 loss 0.768
Accuracy of the network on the 482 val images: 79.46%
Max accuracy: 80.91%
Epoch: [84]  [  0/160]  eta: 0:20:58  lr: 0.000015  min_lr: 0.000000  loss: 2.1751 (2.1751)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5913 (7.5913)  time: 7.8665 (7.8665 -- 7.8665)  data: 4.3794 (4.3794 -- 4.3794)  max mem: 16413
Epoch: [84]  [ 20/160]  eta: 0:02:44  lr: 0.000015  min_lr: 0.000000  loss: 1.7720 (1.7821)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7551 (8.6385)  time: 0.8368 (0.5293 -- 3.3274)  data: 0.0394 (0.0003 -- 0.7321)  max mem: 16413
Epoch: [84]  [ 40/160]  eta: 0:02:11  lr: 0.000015  min_lr: 0.000000  loss: 1.8881 (1.8163)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0550 (8.4153)  time: 1.0093 (0.5188 -- 4.6974)  data: 0.0012 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [84]  [ 60/160]  eta: 0:01:39  lr: 0.000015  min_lr: 0.000000  loss: 1.9840 (1.8623)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1567 (8.4640)  time: 0.8039 (0.5093 -- 3.6002)  data: 0.0013 (0.0004 -- 0.0045)  max mem: 16413
[2023-08-30 16:43:29,797] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:43:29,797] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:43:29,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:43:29,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [84]  [ 80/160]  eta: 0:01:18  lr: 0.000015  min_lr: 0.000000  loss: 1.7164 (1.8435)  loss_scale: 32768.0000 (19013.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7055 (8.3483)  time: 0.9228 (0.5317 -- 4.0034)  data: 0.0016 (0.0001 -- 0.0035)  max mem: 16413
Epoch: [84]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000000  loss: 1.8299 (1.8456)  loss_scale: 32768.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6053 (8.3302)  time: 0.8010 (0.5232 -- 3.4727)  data: 0.0013 (0.0002 -- 0.0028)  max mem: 16413
[2023-08-30 16:43:59,274] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13541
[2023-08-30 16:43:59,274] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13541
[2023-08-30 16:43:59,274] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:43:59,274] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:43:59,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [84]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8130 (1.8429)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4470 (8.2617)  time: 0.8838 (0.5284 -- 2.9749)  data: 0.0011 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [84]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.6997 (1.8319)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0675 (8.3430)  time: 0.8633 (0.5148 -- 3.8833)  data: 0.0030 (0.0002 -- 0.0164)  max mem: 16413
Epoch: [84]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.9173 (1.8316)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7566 (8.4181)  time: 0.6265 (0.4964 -- 2.5699)  data: 0.0004 (0.0002 -- 0.0009)  max mem: 16413
Epoch: [84] Total time: 0:02:22 (0.8894 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.9173 (1.8597)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7566 (8.4181)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4451 (0.4451)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3393 (2.3393 -- 2.3393)  data: 2.1280 (2.1280 -- 2.1280)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.6153 (0.8104)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4280 (0.1975 -- 2.3393)  data: 0.2175 (0.0004 -- 2.1280)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5723 (0.7638)  acc1: 88.8889 (79.8942)  acc5: 100.0000 (96.2963)  time: 0.2245 (0.1698 -- 0.4604)  data: 0.0228 (0.0001 -- 0.2527)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6734 (0.8040)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2115 (0.1336 -- 0.4604)  data: 0.0225 (0.0001 -- 0.2527)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 80.083 Acc@5 96.473 loss 0.753
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 80.91%
Epoch: [85]  [  0/160]  eta: 0:20:45  lr: 0.000015  min_lr: 0.000000  loss: 1.2995 (1.2995)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7227 (6.7227)  time: 7.7847 (7.7847 -- 7.7847)  data: 7.2172 (7.2172 -- 7.2172)  max mem: 16413
Epoch: [85]  [ 20/160]  eta: 0:02:39  lr: 0.000015  min_lr: 0.000000  loss: 1.8477 (1.7827)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5302 (8.2791)  time: 0.8072 (0.5266 -- 3.5953)  data: 0.1707 (0.0005 -- 1.7619)  max mem: 16413
Epoch: [85]  [ 40/160]  eta: 0:02:05  lr: 0.000015  min_lr: 0.000000  loss: 1.9423 (1.7954)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4959 (8.0428)  time: 0.9530 (0.5223 -- 4.0118)  data: 0.3582 (0.0006 -- 3.5110)  max mem: 16413
Epoch: [85]  [ 60/160]  eta: 0:01:38  lr: 0.000015  min_lr: 0.000000  loss: 1.8411 (1.7780)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8019 (8.0759)  time: 0.8479 (0.5173 -- 4.0632)  data: 0.3052 (0.0003 -- 3.5067)  max mem: 16413
[2023-08-30 16:46:01,431] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:46:01,432] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:46:01,432] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:46:01,432] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [85]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 1.9120 (1.8069)  loss_scale: 32768.0000 (18608.9877)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8898 (7.9308)  time: 0.8575 (0.5314 -- 2.4702)  data: 0.2778 (0.0002 -- 1.9606)  max mem: 16413
Epoch: [85]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.7243 (1.7876)  loss_scale: 32768.0000 (21412.7525)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4396 (7.9563)  time: 0.7956 (0.5274 -- 2.3226)  data: 0.2437 (0.0005 -- 1.7876)  max mem: 16413
[2023-08-30 16:46:27,634] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13702
[2023-08-30 16:46:27,635] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13702
[2023-08-30 16:46:27,635] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:46:27,635] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:46:27,635] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [85]  [120/160]  eta: 0:00:36  lr: 0.000015  min_lr: 0.000000  loss: 1.9079 (1.8022)  loss_scale: 16384.0000 (20716.9587)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6182 (8.2512)  time: 0.8664 (0.5255 -- 2.3206)  data: 0.3126 (0.0004 -- 1.7757)  max mem: 16413
Epoch: [85]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.7421 (1.8037)  loss_scale: 16384.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3473 (8.2655)  time: 0.8615 (0.5280 -- 3.0015)  data: 0.3112 (0.0007 -- 2.4705)  max mem: 16413
Epoch: [85]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.8795 (1.8186)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8658 (8.3193)  time: 0.6531 (0.4973 -- 2.6744)  data: 0.1320 (0.0001 -- 2.1369)  max mem: 16413
Epoch: [85] Total time: 0:02:20 (0.8758 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.8795 (1.8418)  loss_scale: 16384.0000 (19660.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8658 (8.3193)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4561 (0.4561)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3191 (2.3191 -- 2.3191)  data: 2.0747 (2.0747 -- 2.0747)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5339 (0.7988)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (94.9495)  time: 0.4116 (0.2024 -- 2.3191)  data: 0.1949 (0.0004 -- 2.0747)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5586 (0.7575)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (95.7672)  time: 0.2174 (0.1688 -- 0.3796)  data: 0.0130 (0.0001 -- 0.1869)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6572 (0.7978)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.0207)  time: 0.2024 (0.1331 -- 0.3796)  data: 0.0127 (0.0001 -- 0.1869)  max mem: 16413
Val: Total time: 0:00:07 (0.2833 s / it)
* Acc@1 82.158 Acc@5 96.266 loss 0.745
Accuracy of the network on the 482 val images: 82.16%
[2023-08-30 16:47:21,404] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 16:47:21,405] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 16:47:21,406] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 16:47:21,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 16:47:22,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 16:47:22,853] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.16%
Epoch: [86]  [  0/160]  eta: 0:20:40  lr: 0.000015  min_lr: 0.000000  loss: 1.4715 (1.4715)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8780 (10.8780)  time: 7.7518 (7.7518 -- 7.7518)  data: 6.1971 (6.1971 -- 6.1971)  max mem: 16413
Epoch: [86]  [ 20/160]  eta: 0:02:44  lr: 0.000015  min_lr: 0.000000  loss: 1.8963 (1.8921)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2898 (8.1515)  time: 0.8466 (0.5318 -- 3.7788)  data: 0.0630 (0.0003 -- 1.2214)  max mem: 16413
Epoch: [86]  [ 40/160]  eta: 0:02:02  lr: 0.000015  min_lr: 0.000000  loss: 1.6403 (1.8140)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2668 (8.1108)  time: 0.8592 (0.5342 -- 3.7359)  data: 0.2446 (0.0006 -- 3.1843)  max mem: 16413
Epoch: [86]  [ 60/160]  eta: 0:01:34  lr: 0.000015  min_lr: 0.000000  loss: 1.8234 (1.8042)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5090 (8.3549)  time: 0.7816 (0.5187 -- 2.9516)  data: 0.2353 (0.0004 -- 2.3861)  max mem: 16413
[2023-08-30 16:48:31,135] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:48:31,135] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:48:31,135] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:48:31,135] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [86]  [ 80/160]  eta: 0:01:15  lr: 0.000015  min_lr: 0.000000  loss: 1.9366 (1.8173)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9042 (8.3940)  time: 0.9567 (0.5156 -- 2.6406)  data: 0.3287 (0.0004 -- 1.9599)  max mem: 16413
[2023-08-30 16:48:40,057] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13841
[2023-08-30 16:48:40,057] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13841
[2023-08-30 16:48:40,057] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:48:40,057] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:48:40,058] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [86]  [100/160]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000000  loss: 1.7061 (1.8058)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3285 (8.4887)  time: 0.8658 (0.5257 -- 3.0977)  data: 0.3117 (0.0005 -- 2.5398)  max mem: 16413
[2023-08-30 16:49:03,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13867
[2023-08-30 16:49:03,255] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 13867
[2023-08-30 16:49:03,256] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:49:03,256] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:49:03,256] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [86]  [120/160]  eta: 0:00:37  lr: 0.000015  min_lr: 0.000000  loss: 1.8915 (1.8216)  loss_scale: 8192.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3925 (8.5200)  time: 0.9480 (0.5222 -- 4.6310)  data: 0.3993 (0.0004 -- 4.0970)  max mem: 16413
Epoch: [86]  [140/160]  eta: 0:00:18  lr: 0.000015  min_lr: 0.000000  loss: 1.6580 (1.8138)  loss_scale: 8192.0000 (15570.6099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5685 (8.5747)  time: 0.7885 (0.5276 -- 4.4993)  data: 0.1879 (0.0005 -- 3.1098)  max mem: 16413
Epoch: [86]  [159/160]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000000  loss: 1.8259 (1.8076)  loss_scale: 8192.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8372 (8.5556)  time: 0.7152 (0.4983 -- 2.4449)  data: 0.1326 (0.0002 -- 1.4367)  max mem: 16413
Epoch: [86] Total time: 0:02:22 (0.8905 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000000  loss: 1.8259 (1.7972)  loss_scale: 8192.0000 (14694.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8372 (8.5556)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.4462 (0.4462)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2050 (2.2050 -- 2.2050)  data: 1.9831 (1.9831 -- 1.9831)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5565 (0.8049)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (94.9495)  time: 0.4155 (0.1982 -- 2.2050)  data: 0.2019 (0.0007 -- 1.9831)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5565 (0.7516)  acc1: 88.8889 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2338 (0.1694 -- 0.5231)  data: 0.0293 (0.0001 -- 0.3443)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6535 (0.8000)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.4357)  time: 0.2207 (0.1333 -- 0.5231)  data: 0.0289 (0.0001 -- 0.3443)  max mem: 16413
Val: Total time: 0:00:07 (0.2916 s / it)
* Acc@1 80.913 Acc@5 96.266 loss 0.742
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 82.16%
Epoch: [87]  [  0/160]  eta: 0:19:26  lr: 0.000015  min_lr: 0.000000  loss: 1.6770 (1.6770)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2893 (11.2893)  time: 7.2876 (7.2876 -- 7.2876)  data: 6.7584 (6.7584 -- 6.7584)  max mem: 16413
Epoch: [87]  [ 20/160]  eta: 0:02:40  lr: 0.000015  min_lr: 0.000000  loss: 1.8289 (1.7918)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1481 (8.9544)  time: 0.8399 (0.5119 -- 2.6235)  data: 0.2610 (0.0005 -- 2.0715)  max mem: 16413
Epoch: [87]  [ 40/160]  eta: 0:02:04  lr: 0.000015  min_lr: 0.000000  loss: 1.6346 (1.7513)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3643 (8.9899)  time: 0.9169 (0.5249 -- 3.1339)  data: 0.3326 (0.0004 -- 2.6011)  max mem: 16413
Epoch: [87]  [ 60/160]  eta: 0:01:42  lr: 0.000015  min_lr: 0.000000  loss: 1.8601 (1.7729)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5869 (8.9277)  time: 1.0198 (0.5295 -- 5.0331)  data: 0.0015 (0.0003 -- 0.0076)  max mem: 16413
[2023-08-30 16:51:08,427] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:51:08,427] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:51:08,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 16:51:08,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 16:51:10,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=81, lr=[3.454829411413636e-07, 3.454829411413636e-07, 4.6064392152181815e-07, 4.6064392152181815e-07, 6.141918953624241e-07, 6.141918953624241e-07, 8.189225271498989e-07, 8.189225271498989e-07, 1.0918967028665318e-06, 1.0918967028665318e-06, 1.455862270488709e-06, 1.455862270488709e-06, 1.9411496939849457e-06, 1.9411496939849457e-06, 2.5881995919799273e-06, 2.5881995919799273e-06, 3.4509327893065697e-06, 3.4509327893065697e-06, 4.6012437190754266e-06, 4.6012437190754266e-06, 6.134991625433902e-06, 6.134991625433902e-06, 8.179988833911868e-06, 8.179988833911868e-06, 1.0906651778549159e-05, 1.0906651778549159e-05, 1.4542202371398879e-05, 1.4542202371398879e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 16:51:10,130] [INFO] [timer.py:260:stop] epoch=0/micro_step=14000/global_step=14000, RunningAvgSamplesPerSec=17.81157875272877, CurrSamplesPerSec=20.516473173541343, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [87]  [ 80/160]  eta: 0:01:16  lr: 0.000015  min_lr: 0.000000  loss: 2.0439 (1.8330)  loss_scale: 8192.0000 (8697.6790)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9962 (8.7439)  time: 0.7321 (0.5276 -- 3.6589)  data: 0.0024 (0.0004 -- 0.0151)  max mem: 16413
Epoch: [87]  [100/160]  eta: 0:00:56  lr: 0.000015  min_lr: 0.000000  loss: 1.8732 (1.8391)  loss_scale: 16384.0000 (10219.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2281 (8.8384)  time: 0.9084 (0.5342 -- 3.2380)  data: 0.0024 (0.0006 -- 0.0102)  max mem: 16413
Epoch: [87]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.9363 (1.8526)  loss_scale: 16384.0000 (11238.6116)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5065 (8.8568)  time: 0.8077 (0.5348 -- 3.6136)  data: 0.0021 (0.0004 -- 0.0084)  max mem: 16413
Epoch: [87]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.8833 (1.8612)  loss_scale: 16384.0000 (11968.4539)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6117 (8.7013)  time: 0.7788 (0.5243 -- 2.7942)  data: 0.0021 (0.0003 -- 0.0170)  max mem: 16413
Epoch: [87]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.9893 (1.8656)  loss_scale: 16384.0000 (12492.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0965 (8.7339)  time: 0.7484 (0.4957 -- 2.5878)  data: 0.0149 (0.0002 -- 0.2809)  max mem: 16413
Epoch: [87] Total time: 0:02:21 (0.8865 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.9893 (1.8262)  loss_scale: 16384.0000 (12492.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0965 (8.7339)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4226 (0.4226)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2802 (2.2802 -- 2.2802)  data: 2.0559 (2.0559 -- 2.0559)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5287 (0.7999)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4315 (0.2027 -- 2.2802)  data: 0.2162 (0.0006 -- 2.0559)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5388 (0.7538)  acc1: 77.7778 (78.8360)  acc5: 100.0000 (96.2963)  time: 0.2263 (0.1694 -- 0.3859)  data: 0.0233 (0.0001 -- 0.1701)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6491 (0.7989)  acc1: 77.7778 (76.7635)  acc5: 100.0000 (95.8506)  time: 0.2109 (0.1328 -- 0.3859)  data: 0.0230 (0.0001 -- 0.1701)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 80.083 Acc@5 96.473 loss 0.741
Accuracy of the network on the 482 val images: 80.08%
Max accuracy: 82.16%
Epoch: [88]  [  0/160]  eta: 0:23:16  lr: 0.000014  min_lr: 0.000000  loss: 1.7392 (1.7392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7360 (5.7360)  time: 8.7258 (8.7258 -- 8.7258)  data: 8.1512 (8.1512 -- 8.1512)  max mem: 16413
Epoch: [88]  [ 20/160]  eta: 0:02:45  lr: 0.000014  min_lr: 0.000000  loss: 1.9548 (1.8692)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2933 (8.5312)  time: 0.8071 (0.5307 -- 3.1965)  data: 0.0883 (0.0004 -- 0.8626)  max mem: 16413
Epoch: [88]  [ 40/160]  eta: 0:02:02  lr: 0.000014  min_lr: 0.000000  loss: 1.7814 (1.8033)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9893 (8.2972)  time: 0.8560 (0.5202 -- 4.5295)  data: 0.1017 (0.0004 -- 1.4371)  max mem: 16413
[2023-08-30 16:53:08,068] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:53:08,068] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:53:08,068] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:53:08,069] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:53:22,632] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14140
[2023-08-30 16:53:22,632] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14140
[2023-08-30 16:53:22,633] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:53:22,633] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 16:53:22,633] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [88]  [ 60/160]  eta: 0:01:37  lr: 0.000014  min_lr: 0.000000  loss: 1.7619 (1.8151)  loss_scale: 32768.0000 (20681.4426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3681 (8.3012)  time: 0.8892 (0.5247 -- 3.9254)  data: 0.1060 (0.0007 -- 1.1112)  max mem: 16413
Epoch: [88]  [ 80/160]  eta: 0:01:17  lr: 0.000014  min_lr: 0.000000  loss: 1.6890 (1.8041)  loss_scale: 16384.0000 (19620.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1718 (8.4746)  time: 0.9507 (0.5108 -- 3.5282)  data: 0.0032 (0.0003 -- 0.0163)  max mem: 16413
Epoch: [88]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 1.8584 (1.8164)  loss_scale: 16384.0000 (18979.4851)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3796 (8.6618)  time: 0.7403 (0.5204 -- 2.9814)  data: 0.0013 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [88]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.8561 (1.8294)  loss_scale: 16384.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9461 (8.6772)  time: 0.7998 (0.5372 -- 2.6168)  data: 0.0049 (0.0004 -- 0.0680)  max mem: 16413
Epoch: [88]  [140/160]  eta: 0:00:17  lr: 0.000014  min_lr: 0.000000  loss: 1.6529 (1.8132)  loss_scale: 16384.0000 (18243.1773)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1076 (8.6401)  time: 0.8195 (0.5265 -- 2.7308)  data: 0.1642 (0.0004 -- 2.1976)  max mem: 16413
Epoch: [88]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.9244 (1.8140)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7709 (8.6233)  time: 0.7416 (0.4977 -- 2.9202)  data: 0.1709 (0.0002 -- 2.3905)  max mem: 16413
Epoch: [88] Total time: 0:02:20 (0.8768 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.9244 (1.8174)  loss_scale: 16384.0000 (18022.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7709 (8.6233)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4075 (0.4075)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2658 (2.2658 -- 2.2658)  data: 2.0454 (2.0454 -- 2.0454)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4901 (0.8087)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4248 (0.2031 -- 2.2658)  data: 0.2021 (0.0006 -- 2.0454)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5506 (0.7591)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2281 (0.1730 -- 0.3946)  data: 0.0151 (0.0001 -- 0.1661)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6531 (0.8043)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2105 (0.1333 -- 0.3946)  data: 0.0148 (0.0001 -- 0.1661)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 80.913 Acc@5 96.473 loss 0.740
Accuracy of the network on the 482 val images: 80.91%
Max accuracy: 82.16%
Epoch: [89]  [  0/160]  eta: 0:22:41  lr: 0.000014  min_lr: 0.000000  loss: 1.8048 (1.8048)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2023 (9.2023)  time: 8.5110 (8.5110 -- 8.5110)  data: 7.9878 (7.9878 -- 7.9878)  max mem: 16413
[2023-08-30 16:55:04,598] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14248
[2023-08-30 16:55:04,598] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14248
[2023-08-30 16:55:04,600] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:55:04,600] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 16:55:04,600] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [89]  [ 20/160]  eta: 0:02:45  lr: 0.000014  min_lr: 0.000000  loss: 1.7280 (1.7625)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3637 (8.4773)  time: 0.8193 (0.5242 -- 2.8048)  data: 0.1642 (0.0008 -- 2.2914)  max mem: 16413
Epoch: [89]  [ 40/160]  eta: 0:01:58  lr: 0.000014  min_lr: 0.000000  loss: 1.7105 (1.7898)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4204 (8.8222)  time: 0.7750 (0.5203 -- 2.9243)  data: 0.2329 (0.0002 -- 2.3933)  max mem: 16413
Epoch: [89]  [ 60/160]  eta: 0:01:33  lr: 0.000014  min_lr: 0.000000  loss: 1.8998 (1.8187)  loss_scale: 8192.0000 (9266.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3146 (9.0090)  time: 0.8349 (0.5306 -- 2.4356)  data: 0.2127 (0.0004 -- 1.8629)  max mem: 16413
Epoch: [89]  [ 80/160]  eta: 0:01:13  lr: 0.000014  min_lr: 0.000000  loss: 1.9031 (1.8257)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4219 (8.7729)  time: 0.8891 (0.5318 -- 3.6928)  data: 0.2481 (0.0003 -- 3.1679)  max mem: 16413
Epoch: [89]  [100/160]  eta: 0:00:56  lr: 0.000014  min_lr: 0.000000  loss: 1.6574 (1.7999)  loss_scale: 8192.0000 (8840.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2416 (8.6702)  time: 1.0246 (0.5110 -- 3.4207)  data: 0.1442 (0.0003 -- 1.5044)  max mem: 16413
Epoch: [89]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.7433 (1.7905)  loss_scale: 8192.0000 (8733.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1429 (8.8536)  time: 0.7965 (0.5324 -- 3.4059)  data: 0.0014 (0.0005 -- 0.0029)  max mem: 16413
[2023-08-30 16:56:55,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:56:55,193] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 16:56:55,193] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:56:55,194] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [89]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.9336 (1.8109)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1063 (8.7875)  time: 0.8291 (0.5250 -- 3.7580)  data: 0.0024 (0.0004 -- 0.0114)  max mem: 16413
Epoch: [89]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7281 (1.8063)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0128 (8.7750)  time: 0.6394 (0.4971 -- 2.5686)  data: 0.0013 (0.0002 -- 0.0090)  max mem: 16413
Epoch: [89] Total time: 0:02:20 (0.8756 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7281 (1.8422)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0128 (8.7750)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.4387 (0.4387)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4561 (2.4561 -- 2.4561)  data: 2.1727 (2.1727 -- 2.1727)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5112 (0.7970)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4232 (0.1917 -- 2.4561)  data: 0.2018 (0.0006 -- 2.1727)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5447 (0.7447)  acc1: 77.7778 (79.3651)  acc5: 100.0000 (96.2963)  time: 0.2142 (0.1692 -- 0.3119)  data: 0.0084 (0.0001 -- 0.1175)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6409 (0.7921)  acc1: 77.7778 (78.0083)  acc5: 100.0000 (95.8506)  time: 0.1982 (0.1339 -- 0.3119)  data: 0.0081 (0.0001 -- 0.1175)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.737
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.16%
Epoch: [90]  [  0/160]  eta: 0:19:46  lr: 0.000014  min_lr: 0.000000  loss: 1.9046 (1.9046)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4608 (7.4608)  time: 7.4170 (7.4170 -- 7.4170)  data: 6.8975 (6.8975 -- 6.8975)  max mem: 16413
Epoch: [90]  [ 20/160]  eta: 0:02:44  lr: 0.000014  min_lr: 0.000000  loss: 1.8968 (1.8491)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7938 (9.1761)  time: 0.8595 (0.5225 -- 3.6811)  data: 0.2648 (0.0004 -- 3.1593)  max mem: 16413
Epoch: [90]  [ 40/160]  eta: 0:02:02  lr: 0.000014  min_lr: 0.000000  loss: 1.8728 (1.8756)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8505 (8.8471)  time: 0.8601 (0.5231 -- 2.4085)  data: 0.2384 (0.0002 -- 1.8635)  max mem: 16413
Epoch: [90]  [ 60/160]  eta: 0:01:38  lr: 0.000014  min_lr: 0.000000  loss: 1.9506 (1.8845)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4988 (8.7602)  time: 0.9237 (0.5226 -- 4.2883)  data: 0.3778 (0.0004 -- 3.7759)  max mem: 16413
Epoch: [90]  [ 80/160]  eta: 0:01:16  lr: 0.000014  min_lr: 0.000000  loss: 1.9662 (1.8893)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2733 (8.8583)  time: 0.8758 (0.5319 -- 4.4421)  data: 0.3230 (0.0003 -- 3.9188)  max mem: 16413
Epoch: [90]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 1.8252 (1.8893)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8568 (8.9122)  time: 0.7657 (0.5214 -- 2.9354)  data: 0.2192 (0.0004 -- 2.4055)  max mem: 16413
[2023-08-30 16:58:58,326] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:58:58,326] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 16:58:58,327] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 16:58:58,327] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [90]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.7818 (1.8733)  loss_scale: 32768.0000 (18550.4793)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4192 (8.7413)  time: 0.9003 (0.5264 -- 4.2376)  data: 0.3525 (0.0002 -- 3.7244)  max mem: 16413
Epoch: [90]  [140/160]  eta: 0:00:17  lr: 0.000014  min_lr: 0.000000  loss: 1.7577 (1.8685)  loss_scale: 32768.0000 (20567.1489)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6367 (8.7695)  time: 0.7703 (0.5327 -- 3.2006)  data: 0.2199 (0.0001 -- 2.6691)  max mem: 16413
Epoch: [90]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.8739 (1.8660)  loss_scale: 32768.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7755 (8.8315)  time: 0.7065 (0.4985 -- 1.6562)  data: 0.1754 (0.0002 -- 1.1260)  max mem: 16413
Epoch: [90] Total time: 0:02:20 (0.8759 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.8739 (1.8615)  loss_scale: 32768.0000 (22016.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7755 (8.8315)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4005 (0.4005)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2597 (2.2597 -- 2.2597)  data: 2.0189 (2.0189 -- 2.0189)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5112 (0.7966)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (94.9495)  time: 0.4254 (0.1829 -- 2.2597)  data: 0.2112 (0.0006 -- 2.0189)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5336 (0.7390)  acc1: 77.7778 (79.8942)  acc5: 100.0000 (95.7672)  time: 0.2226 (0.1697 -- 0.4549)  data: 0.0174 (0.0001 -- 0.2360)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6383 (0.7870)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.0207)  time: 0.2102 (0.1328 -- 0.4549)  data: 0.0172 (0.0001 -- 0.2360)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 81.743 Acc@5 96.266 loss 0.727
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 82.16%
Epoch: [91]  [  0/160]  eta: 0:22:05  lr: 0.000014  min_lr: 0.000000  loss: 1.8535 (1.8535)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9834 (8.9834)  time: 8.2830 (8.2830 -- 8.2830)  data: 7.7461 (7.7461 -- 7.7461)  max mem: 16413
Epoch: [91]  [ 20/160]  eta: 0:02:40  lr: 0.000014  min_lr: 0.000000  loss: 1.6140 (1.7391)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2168 (8.8660)  time: 0.7861 (0.5147 -- 3.4268)  data: 0.1874 (0.0004 -- 2.9017)  max mem: 16413
[2023-08-30 17:00:28,554] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14599
[2023-08-30 17:00:28,555] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:00:28,555] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14599
[2023-08-30 17:00:28,555] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:00:28,556] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [91]  [ 40/160]  eta: 0:02:06  lr: 0.000014  min_lr: 0.000000  loss: 1.8216 (1.7741)  loss_scale: 32768.0000 (31968.7805)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4170 (8.8748)  time: 0.9552 (0.5122 -- 4.0106)  data: 0.3352 (0.0003 -- 3.4895)  max mem: 16413
Epoch: [91]  [ 60/160]  eta: 0:01:36  lr: 0.000014  min_lr: 0.000000  loss: 1.9523 (1.8327)  loss_scale: 16384.0000 (26859.0164)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1393 (8.6465)  time: 0.7966 (0.5325 -- 2.6069)  data: 0.1591 (0.0006 -- 2.0784)  max mem: 16413
Epoch: [91]  [ 80/160]  eta: 0:01:14  lr: 0.000014  min_lr: 0.000000  loss: 1.7050 (1.8178)  loss_scale: 16384.0000 (24272.5926)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4653 (8.5544)  time: 0.8051 (0.5160 -- 2.8301)  data: 0.2615 (0.0005 -- 2.3077)  max mem: 16413
Epoch: [91]  [100/160]  eta: 0:00:55  lr: 0.000014  min_lr: 0.000000  loss: 1.8247 (1.8173)  loss_scale: 16384.0000 (22710.4950)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3066 (8.6125)  time: 0.9008 (0.5301 -- 3.2529)  data: 0.2748 (0.0005 -- 2.7290)  max mem: 16413
Epoch: [91]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.6412 (1.8047)  loss_scale: 16384.0000 (21664.7934)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8490 (8.6469)  time: 0.8403 (0.5216 -- 1.7934)  data: 0.0539 (0.0002 -- 1.0507)  max mem: 16413
Epoch: [91]  [140/160]  eta: 0:00:18  lr: 0.000014  min_lr: 0.000000  loss: 1.8803 (1.8103)  loss_scale: 16384.0000 (20915.7447)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6801 (8.7657)  time: 0.8698 (0.5301 -- 2.4918)  data: 0.0014 (0.0003 -- 0.0055)  max mem: 16413
Epoch: [91]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7456 (1.8029)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1371 (8.9208)  time: 0.7264 (0.4978 -- 2.5353)  data: 0.0008 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [91] Total time: 0:02:21 (0.8835 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7456 (1.8072)  loss_scale: 16384.0000 (20377.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1371 (8.9208)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.4532 (0.4532)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2681 (2.2681 -- 2.2681)  data: 2.0266 (2.0266 -- 2.0266)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5459 (0.7983)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (94.9495)  time: 0.4057 (0.2058 -- 2.2681)  data: 0.1855 (0.0009 -- 2.0266)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5506 (0.7386)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (95.7672)  time: 0.2230 (0.1692 -- 0.4461)  data: 0.0138 (0.0001 -- 0.2592)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6495 (0.7858)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.4357)  time: 0.2047 (0.1332 -- 0.4461)  data: 0.0135 (0.0001 -- 0.2592)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 81.535 Acc@5 96.473 loss 0.729
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.16%
Epoch: [92]  [  0/160]  eta: 0:19:46  lr: 0.000014  min_lr: 0.000000  loss: 0.9627 (0.9627)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1256 (12.1256)  time: 7.4155 (7.4155 -- 7.4155)  data: 5.9856 (5.9856 -- 5.9856)  max mem: 16413
[2023-08-30 17:02:31,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:02:31,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:02:31,902] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:02:31,902] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [92]  [ 20/160]  eta: 0:02:50  lr: 0.000014  min_lr: 0.000000  loss: 1.8555 (1.7511)  loss_scale: 32768.0000 (26526.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3920 (8.9655)  time: 0.9088 (0.5417 -- 4.8272)  data: 0.3541 (0.0006 -- 4.2627)  max mem: 16413
Epoch: [92]  [ 40/160]  eta: 0:02:07  lr: 0.000014  min_lr: 0.000000  loss: 1.7582 (1.7531)  loss_scale: 32768.0000 (29571.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2285 (8.9170)  time: 0.8980 (0.5197 -- 3.3740)  data: 0.3481 (0.0003 -- 2.8319)  max mem: 16413
[2023-08-30 17:03:02,045] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14765
[2023-08-30 17:03:02,045] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:03:02,045] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 17:03:02,045] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14765
[2023-08-30 17:03:02,046] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:03:10,885] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14775
[2023-08-30 17:03:10,885] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 14775
[2023-08-30 17:03:10,885] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:03:10,885] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:03:10,885] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [92]  [ 60/160]  eta: 0:01:38  lr: 0.000014  min_lr: 0.000000  loss: 1.7607 (1.7663)  loss_scale: 16384.0000 (25516.0656)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9095 (8.7280)  time: 0.8204 (0.5126 -- 3.8909)  data: 0.2757 (0.0004 -- 3.3476)  max mem: 16413
Epoch: [92]  [ 80/160]  eta: 0:01:15  lr: 0.000014  min_lr: 0.000000  loss: 1.9772 (1.8194)  loss_scale: 8192.0000 (21238.5185)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1390 (8.7085)  time: 0.8350 (0.5260 -- 3.4799)  data: 0.2840 (0.0002 -- 2.9357)  max mem: 16413
Epoch: [92]  [100/160]  eta: 0:00:54  lr: 0.000014  min_lr: 0.000000  loss: 1.6821 (1.8022)  loss_scale: 8192.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5863 (8.5009)  time: 0.7136 (0.5382 -- 2.1690)  data: 0.1549 (0.0005 -- 1.6333)  max mem: 16413
Epoch: [92]  [120/160]  eta: 0:00:36  lr: 0.000014  min_lr: 0.000000  loss: 1.7902 (1.8010)  loss_scale: 8192.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1227 (8.6178)  time: 0.9400 (0.5320 -- 3.4592)  data: 0.3517 (0.0001 -- 2.7349)  max mem: 16413
Epoch: [92]  [140/160]  eta: 0:00:17  lr: 0.000014  min_lr: 0.000000  loss: 1.7231 (1.8029)  loss_scale: 8192.0000 (15686.8085)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9833 (8.6402)  time: 0.7895 (0.5319 -- 2.0720)  data: 0.1444 (0.0002 -- 1.5251)  max mem: 16413
Epoch: [92]  [159/160]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000000  loss: 1.7545 (1.7983)  loss_scale: 8192.0000 (14796.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1500 (8.7009)  time: 0.6663 (0.4949 -- 2.2611)  data: 0.1408 (0.0002 -- 1.7045)  max mem: 16413
Epoch: [92] Total time: 0:02:18 (0.8645 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000000  loss: 1.7545 (1.8158)  loss_scale: 8192.0000 (14796.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1500 (8.7009)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.4481 (0.4481)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3566 (2.3566 -- 2.3566)  data: 2.0518 (2.0518 -- 2.0518)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5156 (0.7917)  acc1: 88.8889 (80.8081)  acc5: 100.0000 (94.9495)  time: 0.4233 (0.2034 -- 2.3566)  data: 0.1949 (0.0007 -- 2.0518)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5348 (0.7368)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (95.7672)  time: 0.2269 (0.1709 -- 0.4853)  data: 0.0199 (0.0001 -- 0.3019)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6496 (0.7815)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (95.4357)  time: 0.2117 (0.1324 -- 0.4853)  data: 0.0196 (0.0001 -- 0.3019)  max mem: 16413
Val: Total time: 0:00:07 (0.2922 s / it)
* Acc@1 81.950 Acc@5 96.473 loss 0.725
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.16%
Epoch: [93]  [  0/160]  eta: 0:20:29  lr: 0.000014  min_lr: 0.000000  loss: 1.8663 (1.8663)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.8698 (4.8698)  time: 7.6841 (7.6841 -- 7.6841)  data: 5.8797 (5.8797 -- 5.8797)  max mem: 16413
Epoch: [93]  [ 20/160]  eta: 0:02:36  lr: 0.000014  min_lr: 0.000000  loss: 1.8711 (1.8526)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4657 (10.1104)  time: 0.7931 (0.5201 -- 3.6712)  data: 0.0822 (0.0004 -- 1.0070)  max mem: 16413
[2023-08-30 17:05:09,561] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:05:09,561] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:05:09,561] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:05:09,561] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [93]  [ 40/160]  eta: 0:02:04  lr: 0.000013  min_lr: 0.000000  loss: 1.7863 (1.8300)  loss_scale: 16384.0000 (11588.6829)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3802 (9.8508)  time: 0.9455 (0.5300 -- 3.3504)  data: 0.0800 (0.0002 -- 0.8932)  max mem: 16413
Epoch: [93]  [ 60/160]  eta: 0:01:38  lr: 0.000013  min_lr: 0.000000  loss: 1.7520 (1.8312)  loss_scale: 16384.0000 (13160.9180)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4461 (9.1616)  time: 0.8847 (0.5305 -- 4.7954)  data: 0.0013 (0.0003 -- 0.0061)  max mem: 16413
Epoch: [93]  [ 80/160]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000000  loss: 1.8799 (1.8342)  loss_scale: 16384.0000 (13956.7407)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3074 (9.0847)  time: 0.9205 (0.5268 -- 3.8895)  data: 0.0028 (0.0001 -- 0.0113)  max mem: 16413
Epoch: [93]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000000  loss: 1.6301 (1.7984)  loss_scale: 16384.0000 (14437.3861)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7986 (9.0628)  time: 0.7939 (0.5234 -- 3.2357)  data: 0.0015 (0.0003 -- 0.0042)  max mem: 16413
[2023-08-30 17:06:31,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=86, lr=[3.1802836648250215e-07, 3.1802836648250215e-07, 4.2403782197666955e-07, 4.2403782197666955e-07, 5.653837626355594e-07, 5.653837626355594e-07, 7.538450168474125e-07, 7.538450168474125e-07, 1.0051266891298835e-06, 1.0051266891298835e-06, 1.3401689188398446e-06, 1.3401689188398446e-06, 1.7868918917864594e-06, 1.7868918917864594e-06, 2.382522522381946e-06, 2.382522522381946e-06, 3.176696696509261e-06, 3.176696696509261e-06, 4.235595595345682e-06, 4.235595595345682e-06, 5.647460793794242e-06, 5.647460793794242e-06, 7.529947725058989e-06, 7.529947725058989e-06, 1.0039930300078653e-05, 1.0039930300078653e-05, 1.3386573733438203e-05, 1.3386573733438203e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 17:06:31,316] [INFO] [timer.py:260:stop] epoch=0/micro_step=15000/global_step=15000, RunningAvgSamplesPerSec=17.814218773298233, CurrSamplesPerSec=22.693697418558084, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [93]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.7395 (1.7944)  loss_scale: 16384.0000 (14759.1405)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5403 (9.0615)  time: 0.8298 (0.5373 -- 3.4809)  data: 0.0018 (0.0004 -- 0.0048)  max mem: 16413
Epoch: [93]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.9073 (1.8107)  loss_scale: 16384.0000 (14989.6170)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2453 (9.0471)  time: 0.8006 (0.5168 -- 3.2016)  data: 0.0251 (0.0007 -- 0.4577)  max mem: 16413
[2023-08-30 17:06:58,878] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:06:58,878] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:06:58,878] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:06:58,878] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:07:00,406] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15035
[2023-08-30 17:07:00,406] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15035
[2023-08-30 17:07:00,406] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:07:00,406] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:07:00,406] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [93]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7413 (1.8006)  loss_scale: 16384.0000 (15462.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5641 (8.9260)  time: 0.6971 (0.4941 -- 2.8476)  data: 0.0114 (0.0002 -- 0.2137)  max mem: 16413
Epoch: [93] Total time: 0:02:20 (0.8778 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.7413 (1.8107)  loss_scale: 16384.0000 (15462.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5641 (8.9260)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.4289 (0.4289)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1176 (2.1176 -- 2.1176)  data: 1.8987 (1.8987 -- 1.8987)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5748 (0.7852)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (94.9495)  time: 0.4045 (0.2026 -- 2.1176)  data: 0.1915 (0.0008 -- 1.8987)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5188 (0.7245)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (95.7672)  time: 0.2302 (0.1699 -- 0.5391)  data: 0.0273 (0.0001 -- 0.3363)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6361 (0.7752)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (95.8506)  time: 0.2160 (0.1347 -- 0.5391)  data: 0.0270 (0.0001 -- 0.3363)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.725
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.16%
Epoch: [94]  [  0/160]  eta: 0:20:46  lr: 0.000013  min_lr: 0.000000  loss: 1.8929 (1.8929)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6891 (8.6891)  time: 7.7918 (7.7918 -- 7.7918)  data: 7.2123 (7.2123 -- 7.2123)  max mem: 16413
Epoch: [94]  [ 20/160]  eta: 0:02:42  lr: 0.000013  min_lr: 0.000000  loss: 1.7609 (1.8112)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9130 (9.3264)  time: 0.8318 (0.5218 -- 3.4694)  data: 0.2629 (0.0005 -- 2.9401)  max mem: 16413
Epoch: [94]  [ 40/160]  eta: 0:02:16  lr: 0.000013  min_lr: 0.000000  loss: 1.7584 (1.7849)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7031 (8.9105)  time: 1.1151 (0.5275 -- 6.2715)  data: 0.0688 (0.0004 -- 0.6287)  max mem: 16413
Epoch: [94]  [ 60/160]  eta: 0:01:43  lr: 0.000013  min_lr: 0.000000  loss: 1.8330 (1.7739)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1640 (8.5764)  time: 0.8111 (0.5020 -- 5.2589)  data: 0.0012 (0.0001 -- 0.0028)  max mem: 16413
Epoch: [94]  [ 80/160]  eta: 0:01:18  lr: 0.000013  min_lr: 0.000000  loss: 1.9448 (1.8008)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4645 (8.5993)  time: 0.8317 (0.5058 -- 2.6775)  data: 0.0015 (0.0001 -- 0.0054)  max mem: 16413
Epoch: [94]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000000  loss: 1.9131 (1.8225)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3317 (8.5127)  time: 0.8134 (0.5168 -- 4.1221)  data: 0.0012 (0.0004 -- 0.0030)  max mem: 16413
Epoch: [94]  [120/160]  eta: 0:00:38  lr: 0.000013  min_lr: 0.000000  loss: 1.8537 (1.8235)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8082 (8.4259)  time: 1.0367 (0.5043 -- 4.0045)  data: 0.0010 (0.0004 -- 0.0021)  max mem: 16413
[2023-08-30 17:09:08,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:09:08,799] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:09:08,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:09:08,800] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [94]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8806 (1.8390)  loss_scale: 32768.0000 (18359.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0849 (8.3894)  time: 0.6547 (0.5085 -- 2.3550)  data: 0.0019 (0.0001 -- 0.0113)  max mem: 16413
Epoch: [94]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.6928 (1.8218)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7807 (8.4843)  time: 0.6810 (0.4941 -- 2.2548)  data: 0.0009 (0.0003 -- 0.0026)  max mem: 16413
Epoch: [94] Total time: 0:02:22 (0.8923 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.6928 (1.8113)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7807 (8.4843)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.4178 (0.4178)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2988 (2.2988 -- 2.2988)  data: 2.0539 (2.0539 -- 2.0539)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5066 (0.7780)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (94.9495)  time: 0.4150 (0.1962 -- 2.2988)  data: 0.1973 (0.0008 -- 2.0539)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5242 (0.7223)  acc1: 77.7778 (80.4233)  acc5: 100.0000 (95.7672)  time: 0.2255 (0.1699 -- 0.5258)  data: 0.0232 (0.0001 -- 0.3443)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6370 (0.7692)  acc1: 77.7778 (77.5934)  acc5: 100.0000 (95.8506)  time: 0.2077 (0.1330 -- 0.5258)  data: 0.0228 (0.0001 -- 0.3443)  max mem: 16413
Val: Total time: 0:00:07 (0.2888 s / it)
* Acc@1 80.498 Acc@5 96.680 loss 0.719
Accuracy of the network on the 482 val images: 80.50%
Max accuracy: 82.16%
Epoch: [95]  [  0/160]  eta: 0:21:41  lr: 0.000013  min_lr: 0.000000  loss: 1.9026 (1.9026)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0037 (11.0037)  time: 8.1332 (8.1332 -- 8.1332)  data: 7.6174 (7.6174 -- 7.6174)  max mem: 16413
Epoch: [95]  [ 20/160]  eta: 0:02:46  lr: 0.000013  min_lr: 0.000000  loss: 2.0145 (1.9896)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3146 (8.5654)  time: 0.8405 (0.5301 -- 3.5257)  data: 0.1142 (0.0006 -- 1.4669)  max mem: 16413
[2023-08-30 17:10:06,188] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15221
[2023-08-30 17:10:06,188] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15221
[2023-08-30 17:10:06,189] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:10:06,189] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 17:10:06,189] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:10:18,164] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15234
[2023-08-30 17:10:18,164] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:10:18,164] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15234
[2023-08-30 17:10:18,164] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 17:10:18,164] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [95]  [ 40/160]  eta: 0:02:06  lr: 0.000013  min_lr: 0.000000  loss: 1.7413 (1.8392)  loss_scale: 16384.0000 (23377.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0583 (8.7627)  time: 0.9224 (0.5202 -- 3.2209)  data: 0.1517 (0.0007 -- 1.7756)  max mem: 16413
Epoch: [95]  [ 60/160]  eta: 0:01:36  lr: 0.000013  min_lr: 0.000000  loss: 1.8393 (1.8136)  loss_scale: 8192.0000 (18398.4262)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1846 (8.6929)  time: 0.7748 (0.5278 -- 2.9233)  data: 0.2056 (0.0004 -- 2.4010)  max mem: 16413
Epoch: [95]  [ 80/160]  eta: 0:01:16  lr: 0.000013  min_lr: 0.000000  loss: 1.9059 (1.8251)  loss_scale: 8192.0000 (15878.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3474 (8.4748)  time: 0.9354 (0.5208 -- 3.1667)  data: 0.3264 (0.0005 -- 2.6364)  max mem: 16413
Epoch: [95]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000000  loss: 1.7016 (1.8264)  loss_scale: 8192.0000 (14356.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6381 (8.6402)  time: 0.8385 (0.5178 -- 3.2492)  data: 0.1817 (0.0003 -- 2.7275)  max mem: 16413
Epoch: [95]  [120/160]  eta: 0:00:37  lr: 0.000013  min_lr: 0.000000  loss: 1.7589 (1.8239)  loss_scale: 8192.0000 (13337.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2945 (9.2065)  time: 0.9717 (0.5272 -- 3.5784)  data: 0.3705 (0.0001 -- 3.0492)  max mem: 16413
Epoch: [95]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8126 (1.8279)  loss_scale: 8192.0000 (12607.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3533 (9.0883)  time: 0.7757 (0.5230 -- 2.9512)  data: 0.0102 (0.0003 -- 0.1844)  max mem: 16413
Epoch: [95]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8855 (1.8343)  loss_scale: 8192.0000 (12083.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9969 (9.0122)  time: 0.7083 (0.4964 -- 2.8179)  data: 0.1155 (0.0002 -- 2.2987)  max mem: 16413
Epoch: [95] Total time: 0:02:22 (0.8936 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8855 (1.8140)  loss_scale: 8192.0000 (12083.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9969 (9.0122)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3971 (0.3971)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3624 (2.3624 -- 2.3624)  data: 2.1387 (2.1387 -- 2.1387)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5296 (0.7675)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (94.9495)  time: 0.4216 (0.2131 -- 2.3624)  data: 0.1965 (0.0008 -- 2.1387)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5197 (0.7109)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (95.7672)  time: 0.2195 (0.1705 -- 0.3259)  data: 0.0070 (0.0001 -- 0.1111)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6178 (0.7620)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.2008 (0.1364 -- 0.3259)  data: 0.0062 (0.0001 -- 0.1111)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 81.535 Acc@5 96.680 loss 0.714
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.16%
Epoch: [96]  [  0/160]  eta: 0:18:48  lr: 0.000013  min_lr: 0.000000  loss: 1.8294 (1.8294)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.1600 (13.1600)  time: 7.0523 (7.0523 -- 7.0523)  data: 5.7679 (5.7679 -- 5.7679)  max mem: 16413
[2023-08-30 17:12:22,158] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:12:22,158] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:12:22,159] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:12:22,160] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [96]  [ 20/160]  eta: 0:02:46  lr: 0.000013  min_lr: 0.000000  loss: 1.8665 (1.9090)  loss_scale: 16384.0000 (15213.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0723 (10.2295)  time: 0.8997 (0.5255 -- 3.2547)  data: 0.3431 (0.0003 -- 2.7064)  max mem: 16413
[2023-08-30 17:12:45,373] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15390
[2023-08-30 17:12:45,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:12:45,373] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15390
[2023-08-30 17:12:45,373] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 17:12:45,373] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [96]  [ 40/160]  eta: 0:01:58  lr: 0.000013  min_lr: 0.000000  loss: 1.7523 (1.8142)  loss_scale: 8192.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6388 (9.1965)  time: 0.7805 (0.5170 -- 3.9522)  data: 0.2328 (0.0003 -- 3.4334)  max mem: 16413
Epoch: [96]  [ 60/160]  eta: 0:01:37  lr: 0.000013  min_lr: 0.000000  loss: 1.7501 (1.7899)  loss_scale: 8192.0000 (11817.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6685 (9.1400)  time: 0.9347 (0.5264 -- 3.8402)  data: 0.2815 (0.0008 -- 2.3218)  max mem: 16413
Epoch: [96]  [ 80/160]  eta: 0:01:14  lr: 0.000013  min_lr: 0.000000  loss: 1.7091 (1.7957)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4037 (9.3308)  time: 0.8174 (0.5265 -- 3.2605)  data: 0.2254 (0.0004 -- 2.7296)  max mem: 16413
Epoch: [96]  [100/160]  eta: 0:00:56  lr: 0.000013  min_lr: 0.000000  loss: 1.8619 (1.7848)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7782 (9.1325)  time: 0.9791 (0.5123 -- 3.6241)  data: 0.0871 (0.0004 -- 1.7137)  max mem: 16413
Epoch: [96]  [120/160]  eta: 0:00:36  lr: 0.000013  min_lr: 0.000000  loss: 1.7109 (1.7758)  loss_scale: 8192.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4767 (9.1565)  time: 0.8051 (0.5145 -- 3.9109)  data: 0.0259 (0.0002 -- 0.4903)  max mem: 16413
Epoch: [96]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.8368 (1.7838)  loss_scale: 8192.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0241 (9.0990)  time: 0.9070 (0.5121 -- 4.5260)  data: 0.0013 (0.0003 -- 0.0036)  max mem: 16413
[2023-08-30 17:14:32,716] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:14:32,716] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:14:32,716] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:14:32,716] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [96]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.8199 (1.7925)  loss_scale: 8192.0000 (9625.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5675 (9.0559)  time: 0.6131 (0.4931 -- 2.4334)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [96] Total time: 0:02:21 (0.8830 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.8199 (1.7987)  loss_scale: 8192.0000 (9625.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5675 (9.0559)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.4167 (0.4167)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1899 (2.1899 -- 2.1899)  data: 1.9914 (1.9914 -- 1.9914)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5032 (0.7585)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (94.9495)  time: 0.4323 (0.2030 -- 2.1899)  data: 0.2155 (0.0005 -- 1.9914)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5269 (0.7028)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (95.7672)  time: 0.2309 (0.1694 -- 0.5946)  data: 0.0233 (0.0001 -- 0.3555)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6459 (0.7534)  acc1: 77.7778 (80.9129)  acc5: 100.0000 (95.4357)  time: 0.2151 (0.1334 -- 0.5946)  data: 0.0223 (0.0001 -- 0.3555)  max mem: 16413
Val: Total time: 0:00:07 (0.2887 s / it)
* Acc@1 81.743 Acc@5 96.473 loss 0.706
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 82.16%
Epoch: [97]  [  0/160]  eta: 0:25:20  lr: 0.000013  min_lr: 0.000000  loss: 2.2326 (2.2326)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8969 (5.8969)  time: 9.5055 (9.5055 -- 9.5055)  data: 6.0167 (6.0167 -- 6.0167)  max mem: 16413
Epoch: [97]  [ 20/160]  eta: 0:02:45  lr: 0.000013  min_lr: 0.000000  loss: 1.7609 (1.7951)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2206 (9.5820)  time: 0.7656 (0.5357 -- 3.1684)  data: 0.0016 (0.0003 -- 0.0034)  max mem: 16413
Epoch: [97]  [ 40/160]  eta: 0:02:10  lr: 0.000013  min_lr: 0.000000  loss: 1.8889 (1.8348)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3610 (9.8434)  time: 0.9818 (0.5244 -- 4.5285)  data: 0.0011 (0.0003 -- 0.0018)  max mem: 16413
Epoch: [97]  [ 60/160]  eta: 0:01:39  lr: 0.000013  min_lr: 0.000000  loss: 1.9419 (1.8471)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9765 (9.8721)  time: 0.8089 (0.5120 -- 3.3618)  data: 0.0012 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [97]  [ 80/160]  eta: 0:01:17  lr: 0.000013  min_lr: 0.000000  loss: 1.8316 (1.8164)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0664 (9.2093)  time: 0.8931 (0.5256 -- 3.9194)  data: 0.0015 (0.0003 -- 0.0082)  max mem: 16413
Epoch: [97]  [100/160]  eta: 0:00:55  lr: 0.000013  min_lr: 0.000000  loss: 1.8095 (1.8078)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2120 (9.3428)  time: 0.7812 (0.5305 -- 2.5976)  data: 0.0021 (0.0007 -- 0.0140)  max mem: 16413
Epoch: [97]  [120/160]  eta: 0:00:37  lr: 0.000013  min_lr: 0.000000  loss: 1.8578 (1.8098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9901 (9.4454)  time: 0.9092 (0.5281 -- 3.9933)  data: 0.0013 (0.0004 -- 0.0032)  max mem: 16413
[2023-08-30 17:16:36,722] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:16:36,722] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:16:36,722] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:16:36,722] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [97]  [140/160]  eta: 0:00:18  lr: 0.000013  min_lr: 0.000000  loss: 1.7707 (1.8010)  loss_scale: 32768.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2906 (9.2724)  time: 0.8038 (0.5258 -- 3.3925)  data: 0.0019 (0.0004 -- 0.0070)  max mem: 16413
Epoch: [97]  [159/160]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000000  loss: 1.7351 (1.7999)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7465 (9.2733)  time: 0.6665 (0.4947 -- 2.8034)  data: 0.0008 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [97] Total time: 0:02:21 (0.8822 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000000  loss: 1.7351 (1.8056)  loss_scale: 32768.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7465 (9.2733)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.3921 (0.3921)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1662 (2.1662 -- 2.1662)  data: 1.9255 (1.9255 -- 1.9255)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.5318 (0.7614)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.3999 (0.1950 -- 2.1662)  data: 0.1805 (0.0008 -- 1.9255)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5273 (0.7068)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2252 (0.1694 -- 0.5068)  data: 0.0210 (0.0001 -- 0.3057)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6171 (0.7561)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2074 (0.1332 -- 0.5068)  data: 0.0198 (0.0001 -- 0.3057)  max mem: 16413
Val: Total time: 0:00:07 (0.2835 s / it)
* Acc@1 81.950 Acc@5 97.095 loss 0.708
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.16%
[2023-08-30 17:17:15,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15680
[2023-08-30 17:17:15,592] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15680
[2023-08-30 17:17:15,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:17:15,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 17:17:15,592] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [98]  [  0/160]  eta: 0:16:40  lr: 0.000013  min_lr: 0.000000  loss: 1.3677 (1.3677)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2682 (10.2682)  time: 6.2501 (6.2501 -- 6.2501)  data: 5.7102 (5.7102 -- 5.7102)  max mem: 16413
Epoch: [98]  [ 20/160]  eta: 0:02:31  lr: 0.000013  min_lr: 0.000000  loss: 1.8454 (1.8361)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7385 (8.0233)  time: 0.8268 (0.5249 -- 3.7484)  data: 0.0459 (0.0004 -- 0.8916)  max mem: 16413
Epoch: [98]  [ 40/160]  eta: 0:01:58  lr: 0.000013  min_lr: 0.000000  loss: 1.8424 (1.8537)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9925 (7.9030)  time: 0.8866 (0.5138 -- 4.0940)  data: 0.1589 (0.0003 -- 2.4279)  max mem: 16413
[2023-08-30 17:17:52,916] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15722
[2023-08-30 17:17:52,916] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15722
[2023-08-30 17:17:52,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:17:52,916] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:17:52,916] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [98]  [ 60/160]  eta: 0:01:35  lr: 0.000013  min_lr: 0.000000  loss: 1.7547 (1.8064)  loss_scale: 8192.0000 (13832.3934)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5328 (8.2076)  time: 0.8758 (0.5242 -- 2.5080)  data: 0.1365 (0.0004 -- 1.9688)  max mem: 16413
Epoch: [98]  [ 80/160]  eta: 0:01:15  lr: 0.000012  min_lr: 0.000000  loss: 1.9614 (1.8359)  loss_scale: 8192.0000 (12439.7037)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8870 (8.5122)  time: 0.9018 (0.5201 -- 3.6870)  data: 0.1414 (0.0004 -- 2.1113)  max mem: 16413
Epoch: [98]  [100/160]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 1.6738 (1.7964)  loss_scale: 8192.0000 (11598.5743)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5903 (8.4761)  time: 0.8554 (0.5290 -- 3.0438)  data: 0.0015 (0.0003 -- 0.0042)  max mem: 16413
Epoch: [98]  [120/160]  eta: 0:00:35  lr: 0.000012  min_lr: 0.000000  loss: 1.8690 (1.8028)  loss_scale: 8192.0000 (11035.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9923 (8.6885)  time: 0.7587 (0.5325 -- 2.3478)  data: 0.0489 (0.0003 -- 0.9345)  max mem: 16413
Epoch: [98]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8462 (1.8189)  loss_scale: 8192.0000 (10632.1702)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0505 (8.6310)  time: 0.9740 (0.5171 -- 4.2168)  data: 0.0015 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [98]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7687 (1.8233)  loss_scale: 8192.0000 (10342.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3568 (8.7440)  time: 0.6293 (0.4958 -- 2.0775)  data: 0.0008 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [98] Total time: 0:02:19 (0.8744 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7687 (1.8114)  loss_scale: 8192.0000 (10342.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3568 (8.7440)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3753 (0.3753)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3406 (2.3406 -- 2.3406)  data: 2.0974 (2.0974 -- 2.0974)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4829 (0.7496)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4173 (0.1904 -- 2.3406)  data: 0.2006 (0.0009 -- 2.0974)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5069 (0.6912)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2203 (0.1699 -- 0.3167)  data: 0.0125 (0.0001 -- 0.0953)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6007 (0.7416)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2038 (0.1335 -- 0.3167)  data: 0.0120 (0.0001 -- 0.0953)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 82.158 Acc@5 96.888 loss 0.699
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.16%
Epoch: [99]  [  0/160]  eta: 0:20:55  lr: 0.000012  min_lr: 0.000000  loss: 2.3375 (2.3375)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8291 (6.8291)  time: 7.8498 (7.8498 -- 7.8498)  data: 7.3193 (7.3193 -- 7.3193)  max mem: 16413
[2023-08-30 17:19:53,366] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:19:53,366] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:19:53,370] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:19:53,370] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [99]  [ 20/160]  eta: 0:02:41  lr: 0.000012  min_lr: 0.000000  loss: 1.6744 (1.7159)  loss_scale: 8192.0000 (12092.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1187 (8.4967)  time: 0.8167 (0.5220 -- 3.0276)  data: 0.1982 (0.0005 -- 2.4716)  max mem: 16413
Epoch: [99]  [ 40/160]  eta: 0:01:57  lr: 0.000012  min_lr: 0.000000  loss: 1.9620 (1.8078)  loss_scale: 16384.0000 (14186.1463)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9471 (8.5325)  time: 0.7979 (0.5319 -- 2.9540)  data: 0.0375 (0.0007 -- 0.7213)  max mem: 16413
Epoch: [99]  [ 60/160]  eta: 0:01:32  lr: 0.000012  min_lr: 0.000000  loss: 1.8660 (1.8348)  loss_scale: 16384.0000 (14906.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6603 (8.6999)  time: 0.8043 (0.5328 -- 2.8642)  data: 0.0102 (0.0004 -- 0.1583)  max mem: 16413
Epoch: [99]  [ 80/160]  eta: 0:01:13  lr: 0.000012  min_lr: 0.000000  loss: 1.7487 (1.8180)  loss_scale: 16384.0000 (15271.5062)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9643 (8.8542)  time: 0.9087 (0.5306 -- 2.3047)  data: 0.1606 (0.0007 -- 1.7766)  max mem: 16413
Epoch: [99]  [100/160]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 1.9188 (1.8304)  loss_scale: 16384.0000 (15491.8020)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0647 (8.8164)  time: 0.9508 (0.5261 -- 3.2406)  data: 0.0185 (0.0004 -- 0.3393)  max mem: 16413
Epoch: [99]  [120/160]  eta: 0:00:35  lr: 0.000012  min_lr: 0.000000  loss: 1.7397 (1.8356)  loss_scale: 16384.0000 (15639.2727)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0632 (8.7253)  time: 0.7661 (0.5278 -- 2.4288)  data: 0.2082 (0.0001 -- 1.8591)  max mem: 16413
[2023-08-30 17:21:44,240] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:21:44,241] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:21:44,242] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:21:44,242] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [99]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8145 (1.8358)  loss_scale: 16384.0000 (15977.3050)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8422 (8.6126)  time: 0.9523 (0.5304 -- 3.7374)  data: 0.3524 (0.0004 -- 3.2149)  max mem: 16413
[2023-08-30 17:21:51,032] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15988
[2023-08-30 17:21:51,032] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 15988
[2023-08-30 17:21:51,032] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:21:51,032] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:21:51,032] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 17:21:56,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=93, lr=[2.9017803916243546e-07, 2.9017803916243546e-07, 3.8690405221658065e-07, 3.8690405221658065e-07, 5.158720696221075e-07, 5.158720696221075e-07, 6.8782942616281e-07, 6.8782942616281e-07, 9.171059015504133e-07, 9.171059015504133e-07, 1.2228078687338845e-06, 1.2228078687338845e-06, 1.6304104916451792e-06, 1.6304104916451792e-06, 2.1738806555269056e-06, 2.1738806555269056e-06, 2.8985075407025407e-06, 2.8985075407025407e-06, 3.864676720936721e-06, 3.864676720936721e-06, 5.152902294582295e-06, 5.152902294582295e-06, 6.870536392776393e-06, 6.870536392776393e-06, 9.160715190368523e-06, 9.160715190368523e-06, 1.2214286920491366e-05, 1.2214286920491366e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 17:21:56,624] [INFO] [timer.py:260:stop] epoch=0/micro_step=16000/global_step=16000, RunningAvgSamplesPerSec=17.716915556074554, CurrSamplesPerSec=24.481528554605934, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [99]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7931 (1.8220)  loss_scale: 16384.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3634 (8.6348)  time: 0.6183 (0.4979 -- 2.3113)  data: 0.0894 (0.0001 -- 1.7640)  max mem: 16413
Epoch: [99] Total time: 0:02:19 (0.8727 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7931 (1.8254)  loss_scale: 16384.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3634 (8.6348)
[2023-08-30 17:21:56,627] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-99 is about to be saved!
[2023-08-30 17:21:56,629] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt
[2023-08-30 17:21:56,629] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
[2023-08-30 17:21:56,629] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt...
[2023-08-30 17:21:57,630] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-99/mp_rank_00_model_states.pt.
[2023-08-30 17:21:57,630] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-99 is ready now!
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3627 (0.3627)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3078 (2.3078 -- 2.3078)  data: 2.0620 (2.0620 -- 2.0620)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4597 (0.7490)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4176 (0.1988 -- 2.3078)  data: 0.1933 (0.0005 -- 2.0620)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4786 (0.6918)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.8254)  time: 0.2220 (0.1708 -- 0.3373)  data: 0.0110 (0.0001 -- 0.1144)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6079 (0.7422)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2030 (0.1329 -- 0.3373)  data: 0.0108 (0.0001 -- 0.1144)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 82.158 Acc@5 97.303 loss 0.698
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.16%
Epoch: [100]  [  0/160]  eta: 0:22:08  lr: 0.000012  min_lr: 0.000000  loss: 2.2782 (2.2782)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3420 (7.3420)  time: 8.3015 (8.3015 -- 8.3015)  data: 7.7871 (7.7871 -- 7.7871)  max mem: 16413
Epoch: [100]  [ 20/160]  eta: 0:02:57  lr: 0.000012  min_lr: 0.000000  loss: 1.7321 (1.7531)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1021 (8.3345)  time: 0.9189 (0.5228 -- 4.3643)  data: 0.3739 (0.0004 -- 3.8340)  max mem: 16413
Epoch: [100]  [ 40/160]  eta: 0:02:12  lr: 0.000012  min_lr: 0.000000  loss: 1.8181 (1.8030)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5030 (8.6550)  time: 0.9259 (0.5215 -- 3.9765)  data: 0.3756 (0.0003 -- 3.4591)  max mem: 16413
Epoch: [100]  [ 60/160]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 1.8128 (1.8008)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0721 (8.6810)  time: 0.6996 (0.5230 -- 3.0384)  data: 0.1569 (0.0002 -- 2.5199)  max mem: 16413
Epoch: [100]  [ 80/160]  eta: 0:01:14  lr: 0.000012  min_lr: 0.000000  loss: 1.9244 (1.8119)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8853 (8.8938)  time: 0.8259 (0.5285 -- 3.3642)  data: 0.2714 (0.0002 -- 2.8378)  max mem: 16413
Epoch: [100]  [100/160]  eta: 0:00:55  lr: 0.000012  min_lr: 0.000000  loss: 1.8542 (1.8086)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2817 (8.7974)  time: 0.8853 (0.5332 -- 3.5910)  data: 0.2987 (0.0007 -- 3.0308)  max mem: 16413
[2023-08-30 17:23:53,109] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:23:53,109] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:23:53,110] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:23:53,110] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [100]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.8832 (1.8260)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1510 (8.6810)  time: 0.8428 (0.5242 -- 3.6094)  data: 0.2249 (0.0002 -- 3.0146)  max mem: 16413
[2023-08-30 17:23:59,445] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16124
[2023-08-30 17:23:59,445] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16124
[2023-08-30 17:23:59,446] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:23:59,446] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:23:59,446] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [100]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.7839 (1.8240)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8735 (8.6385)  time: 0.8608 (0.5039 -- 3.7649)  data: 0.0832 (0.0004 -- 0.9018)  max mem: 16413
Epoch: [100]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.7608 (1.8213)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6770 (8.5611)  time: 0.6952 (0.4958 -- 2.1418)  data: 0.0010 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [100] Total time: 0:02:20 (0.8805 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.7608 (1.8478)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6770 (8.5611)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3866 (0.3866)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2878 (2.2878 -- 2.2878)  data: 2.0309 (2.0309 -- 2.0309)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4766 (0.7487)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (94.9495)  time: 0.4248 (0.1981 -- 2.2878)  data: 0.2067 (0.0008 -- 2.0309)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4890 (0.6928)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (95.7672)  time: 0.2236 (0.1707 -- 0.4447)  data: 0.0188 (0.0001 -- 0.2330)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6411 (0.7440)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2100 (0.1326 -- 0.4447)  data: 0.0185 (0.0001 -- 0.2330)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.699
Accuracy of the network on the 482 val images: 82.37%
[2023-08-30 17:24:34,156] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 17:24:34,158] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 17:24:34,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 17:24:34,158] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 17:24:35,548] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 17:24:35,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [101]  [  0/160]  eta: 0:23:19  lr: 0.000012  min_lr: 0.000000  loss: 1.8298 (1.8298)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3913 (8.3913)  time: 8.7460 (8.7460 -- 8.7460)  data: 8.2258 (8.2258 -- 8.2258)  max mem: 16413
Epoch: [101]  [ 20/160]  eta: 0:02:47  lr: 0.000012  min_lr: 0.000000  loss: 1.6215 (1.7174)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0336 (8.4958)  time: 0.8159 (0.5210 -- 3.2311)  data: 0.2064 (0.0003 -- 2.6942)  max mem: 16413
Epoch: [101]  [ 40/160]  eta: 0:02:08  lr: 0.000012  min_lr: 0.000000  loss: 1.7213 (1.7342)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3331 (8.3285)  time: 0.9419 (0.5234 -- 2.7395)  data: 0.2480 (0.0005 -- 2.2035)  max mem: 16413
Epoch: [101]  [ 60/160]  eta: 0:01:37  lr: 0.000012  min_lr: 0.000000  loss: 1.8817 (1.7753)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6931 (8.2647)  time: 0.7756 (0.5199 -- 4.2163)  data: 0.2268 (0.0001 -- 3.6900)  max mem: 16413
Epoch: [101]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 1.7643 (1.7790)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9144 (8.3778)  time: 0.9121 (0.5207 -- 4.2858)  data: 0.3576 (0.0003 -- 3.7563)  max mem: 16413
[2023-08-30 17:26:02,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:26:02,233] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:26:02,234] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:26:02,234] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [101]  [100/160]  eta: 0:00:54  lr: 0.000012  min_lr: 0.000000  loss: 1.7346 (1.7807)  loss_scale: 16384.0000 (17681.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3436 (8.4073)  time: 0.7180 (0.5244 -- 2.5006)  data: 0.1704 (0.0003 -- 1.9739)  max mem: 16413
Epoch: [101]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.7082 (1.7698)  loss_scale: 32768.0000 (20175.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3272 (8.4528)  time: 0.8913 (0.5224 -- 2.7641)  data: 0.0923 (0.0002 -- 0.9833)  max mem: 16413
Epoch: [101]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.9295 (1.7858)  loss_scale: 32768.0000 (21961.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7668 (8.5126)  time: 0.9242 (0.5340 -- 3.1293)  data: 0.2200 (0.0003 -- 2.0069)  max mem: 16413
Epoch: [101]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.8638 (1.7988)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7770 (8.5334)  time: 0.6459 (0.4961 -- 2.4584)  data: 0.1243 (0.0002 -- 1.9133)  max mem: 16413
Epoch: [101] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.8638 (1.8149)  loss_scale: 32768.0000 (23244.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7770 (8.5334)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3543 (0.3543)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3309 (2.3309 -- 2.3309)  data: 2.0906 (2.0906 -- 2.0906)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4782 (0.7573)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (94.9495)  time: 0.4199 (0.2096 -- 2.3309)  data: 0.1915 (0.0007 -- 2.0906)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4902 (0.7002)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (95.7672)  time: 0.2230 (0.1693 -- 0.2755)  data: 0.0094 (0.0001 -- 0.1033)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6004 (0.7473)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2028 (0.1335 -- 0.2755)  data: 0.0090 (0.0001 -- 0.1033)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 82.365 Acc@5 96.888 loss 0.699
Accuracy of the network on the 482 val images: 82.37%
[2023-08-30 17:27:04,098] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 17:27:04,099] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 17:27:04,099] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 17:27:04,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 17:27:05,265] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 17:27:05,265] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
[2023-08-30 17:27:13,210] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16320
[2023-08-30 17:27:13,210] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16320
[2023-08-30 17:27:13,210] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:27:13,210] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:27:13,210] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [102]  [  0/160]  eta: 0:21:11  lr: 0.000012  min_lr: 0.000000  loss: 2.3227 (2.3227)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3258 (9.3258)  time: 7.9441 (7.9441 -- 7.9441)  data: 7.4217 (7.4217 -- 7.4217)  max mem: 16413
Epoch: [102]  [ 20/160]  eta: 0:02:57  lr: 0.000012  min_lr: 0.000000  loss: 1.8351 (1.8415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6230 (8.2468)  time: 0.9313 (0.5087 -- 5.5747)  data: 0.2492 (0.0003 -- 2.7167)  max mem: 16413
Epoch: [102]  [ 40/160]  eta: 0:02:13  lr: 0.000012  min_lr: 0.000000  loss: 1.7399 (1.8123)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7675 (8.3049)  time: 0.9599 (0.5149 -- 4.7480)  data: 0.0016 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [102]  [ 60/160]  eta: 0:01:38  lr: 0.000012  min_lr: 0.000000  loss: 1.8280 (1.7870)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9381 (8.5380)  time: 0.7228 (0.5195 -- 2.6113)  data: 0.0058 (0.0003 -- 0.0978)  max mem: 16413
Epoch: [102]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 1.5572 (1.7538)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1029 (8.5091)  time: 0.8795 (0.5040 -- 6.0566)  data: 0.1548 (0.0003 -- 2.2410)  max mem: 16413
Epoch: [102]  [100/160]  eta: 0:00:54  lr: 0.000012  min_lr: 0.000000  loss: 1.7402 (1.7564)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1970 (8.4794)  time: 0.7231 (0.5165 -- 2.6535)  data: 0.1478 (0.0003 -- 1.5825)  max mem: 16413
Epoch: [102]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.7517 (1.7597)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4690 (8.4408)  time: 0.9259 (0.5239 -- 3.6689)  data: 0.3730 (0.0005 -- 3.1275)  max mem: 16413
[2023-08-30 17:29:04,216] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:29:04,217] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:29:04,217] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:29:04,217] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:29:05,897] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16452
[2023-08-30 17:29:05,897] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:29:05,897] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16452
[2023-08-30 17:29:05,898] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 17:29:05,898] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:29:07,510] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16455
[2023-08-30 17:29:07,510] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16455
[2023-08-30 17:29:07,511] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:29:07,511] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:29:07,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [102]  [140/160]  eta: 0:00:18  lr: 0.000012  min_lr: 0.000000  loss: 1.8433 (1.7579)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1252 (8.4960)  time: 0.8542 (0.5298 -- 3.6806)  data: 0.3061 (0.0004 -- 3.1507)  max mem: 16413
Epoch: [102]  [159/160]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000000  loss: 1.9063 (1.7638)  loss_scale: 8192.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0027 (8.5254)  time: 0.6883 (0.4937 -- 2.2550)  data: 0.1633 (0.0002 -- 1.7000)  max mem: 16413
Epoch: [102] Total time: 0:02:21 (0.8820 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000000  loss: 1.9063 (1.7732)  loss_scale: 8192.0000 (15411.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0027 (8.5254)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3741 (0.3741)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2701 (2.2701 -- 2.2701)  data: 2.0519 (2.0519 -- 2.0519)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4974 (0.7548)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (94.9495)  time: 0.4175 (0.2001 -- 2.2701)  data: 0.2023 (0.0005 -- 2.0519)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4974 (0.6951)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (95.7672)  time: 0.2255 (0.1704 -- 0.4268)  data: 0.0202 (0.0001 -- 0.2278)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6096 (0.7439)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2073 (0.1327 -- 0.4268)  data: 0.0196 (0.0001 -- 0.2278)  max mem: 16413
Val: Total time: 0:00:07 (0.2876 s / it)
* Acc@1 82.365 Acc@5 96.473 loss 0.699
Accuracy of the network on the 482 val images: 82.37%
[2023-08-30 17:29:34,211] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 17:29:34,213] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 17:29:34,213] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 17:29:34,213] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 17:29:35,607] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 17:29:35,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.37%
Epoch: [103]  [  0/160]  eta: 0:18:44  lr: 0.000012  min_lr: 0.000000  loss: 1.6308 (1.6308)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0547 (9.0547)  time: 7.0297 (7.0297 -- 7.0297)  data: 5.9997 (5.9997 -- 5.9997)  max mem: 16413
Epoch: [103]  [ 20/160]  eta: 0:03:10  lr: 0.000012  min_lr: 0.000000  loss: 1.9264 (1.8697)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5789 (9.1177)  time: 1.0781 (0.5119 -- 6.4394)  data: 0.0796 (0.0002 -- 1.0201)  max mem: 16413
Epoch: [103]  [ 40/160]  eta: 0:02:10  lr: 0.000012  min_lr: 0.000000  loss: 1.8683 (1.8612)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9701 (8.2104)  time: 0.7947 (0.5027 -- 3.6376)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [103]  [ 60/160]  eta: 0:01:41  lr: 0.000012  min_lr: 0.000000  loss: 1.8043 (1.8627)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9894 (8.9552)  time: 0.8586 (0.5265 -- 2.9319)  data: 0.0018 (0.0002 -- 0.0074)  max mem: 16413
Epoch: [103]  [ 80/160]  eta: 0:01:16  lr: 0.000012  min_lr: 0.000000  loss: 1.8244 (1.8455)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3857 (8.9395)  time: 0.7881 (0.5054 -- 4.3088)  data: 0.0016 (0.0003 -- 0.0061)  max mem: 16413
Epoch: [103]  [100/160]  eta: 0:00:57  lr: 0.000012  min_lr: 0.000000  loss: 1.8514 (1.8495)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3455 (8.9830)  time: 0.9321 (0.5248 -- 4.1262)  data: 0.0015 (0.0002 -- 0.0055)  max mem: 16413
[2023-08-30 17:31:13,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:31:13,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:31:13,927] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:31:13,928] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [103]  [120/160]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000000  loss: 1.8227 (1.8360)  loss_scale: 16384.0000 (9342.9421)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9830 (8.9852)  time: 0.7005 (0.5249 -- 3.5159)  data: 0.0013 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [103]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.7978 (1.8441)  loss_scale: 16384.0000 (10341.6738)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3637 (9.0403)  time: 0.9187 (0.5269 -- 2.2643)  data: 0.1197 (0.0008 -- 1.5779)  max mem: 16413
Epoch: [103]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7922 (1.8353)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8978 (8.9393)  time: 0.6539 (0.4944 -- 1.5114)  data: 0.0682 (0.0002 -- 0.9850)  max mem: 16413
Epoch: [103] Total time: 0:02:21 (0.8814 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7922 (1.7968)  loss_scale: 16384.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8978 (8.9393)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3362 (0.3362)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5425 (2.5425 -- 2.5425)  data: 2.2776 (2.2776 -- 2.2776)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.5164 (0.7511)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4274 (0.1953 -- 2.5425)  data: 0.2092 (0.0006 -- 2.2776)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.5164 (0.6879)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2099 (0.1708 -- 0.2516)  data: 0.0028 (0.0001 -- 0.0287)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5952 (0.7428)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.1967 (0.1335 -- 0.2516)  data: 0.0023 (0.0001 -- 0.0287)  max mem: 16413
Val: Total time: 0:00:07 (0.2863 s / it)
* Acc@1 81.950 Acc@5 96.888 loss 0.699
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.37%
Epoch: [104]  [  0/160]  eta: 0:25:38  lr: 0.000011  min_lr: 0.000000  loss: 1.9735 (1.9735)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1856 (8.1856)  time: 9.6131 (9.6131 -- 9.6131)  data: 5.9158 (5.9158 -- 5.9158)  max mem: 16413
[2023-08-30 17:32:29,716] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16659
[2023-08-30 17:32:29,716] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16659
[2023-08-30 17:32:29,716] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:32:29,716] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:32:29,716] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [104]  [ 20/160]  eta: 0:02:52  lr: 0.000011  min_lr: 0.000000  loss: 1.7712 (1.7425)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1511 (9.0578)  time: 0.8128 (0.5175 -- 4.3018)  data: 0.0502 (0.0003 -- 0.9776)  max mem: 16413
Epoch: [104]  [ 40/160]  eta: 0:02:03  lr: 0.000011  min_lr: 0.000000  loss: 1.7813 (1.7402)  loss_scale: 8192.0000 (11988.2927)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7013 (9.1201)  time: 0.8179 (0.5351 -- 3.0046)  data: 0.0015 (0.0005 -- 0.0035)  max mem: 16413
Epoch: [104]  [ 60/160]  eta: 0:01:31  lr: 0.000011  min_lr: 0.000000  loss: 1.8784 (1.7687)  loss_scale: 8192.0000 (10743.6066)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8419 (9.0978)  time: 0.6870 (0.5307 -- 2.2937)  data: 0.0882 (0.0004 -- 1.7269)  max mem: 16413
Epoch: [104]  [ 80/160]  eta: 0:01:14  lr: 0.000011  min_lr: 0.000000  loss: 1.7381 (1.7564)  loss_scale: 8192.0000 (10113.5802)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4090 (8.8147)  time: 0.9601 (0.5199 -- 2.6132)  data: 0.2447 (0.0008 -- 2.0679)  max mem: 16413
Epoch: [104]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.6490 (1.7517)  loss_scale: 8192.0000 (9733.0693)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5720 (8.8756)  time: 0.9887 (0.5072 -- 5.6576)  data: 0.4424 (0.0003 -- 5.1096)  max mem: 16413
Epoch: [104]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.9922 (1.7860)  loss_scale: 8192.0000 (9478.3471)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7276 (8.8606)  time: 0.8369 (0.5221 -- 2.6523)  data: 0.2938 (0.0001 -- 2.1036)  max mem: 16413
Epoch: [104]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.9395 (1.8041)  loss_scale: 8192.0000 (9295.8865)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5617 (8.8181)  time: 0.7778 (0.5343 -- 3.1162)  data: 0.2226 (0.0003 -- 2.5801)  max mem: 16413
[2023-08-30 17:34:19,463] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:34:19,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:34:19,463] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:34:19,463] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [104]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.7830 (1.8042)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9016 (8.7120)  time: 0.7361 (0.4949 -- 4.0945)  data: 0.2196 (0.0002 -- 3.5494)  max mem: 16413
Epoch: [104] Total time: 0:02:21 (0.8838 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.7830 (1.7983)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9016 (8.7120)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.3342 (0.3342)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1361 (2.1361 -- 2.1361)  data: 1.9302 (1.9302 -- 1.9302)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4573 (0.7449)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (96.9697)  time: 0.4138 (0.1928 -- 2.1361)  data: 0.2010 (0.0007 -- 1.9302)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4573 (0.6843)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2293 (0.1694 -- 0.4942)  data: 0.0242 (0.0001 -- 0.2694)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6045 (0.7379)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2147 (0.1328 -- 0.4942)  data: 0.0238 (0.0001 -- 0.2694)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 82.573 Acc@5 96.888 loss 0.689
Accuracy of the network on the 482 val images: 82.57%
[2023-08-30 17:34:33,574] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 17:34:33,576] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 17:34:33,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 17:34:33,576] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 17:34:34,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 17:34:34,974] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.57%
Epoch: [105]  [  0/160]  eta: 0:16:09  lr: 0.000011  min_lr: 0.000000  loss: 1.5936 (1.5936)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8946 (9.8946)  time: 6.0624 (6.0624 -- 6.0624)  data: 5.4971 (5.4971 -- 5.4971)  max mem: 16413
Epoch: [105]  [ 20/160]  eta: 0:02:33  lr: 0.000011  min_lr: 0.000000  loss: 1.7682 (1.7063)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6953 (9.2817)  time: 0.8470 (0.5230 -- 2.6725)  data: 0.2169 (0.0003 -- 2.1489)  max mem: 16413
Epoch: [105]  [ 40/160]  eta: 0:02:06  lr: 0.000011  min_lr: 0.000000  loss: 1.9801 (1.8473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1136 (8.7587)  time: 1.0041 (0.5257 -- 4.2625)  data: 0.0347 (0.0003 -- 0.6726)  max mem: 16413
Epoch: [105]  [ 60/160]  eta: 0:01:34  lr: 0.000011  min_lr: 0.000000  loss: 1.8263 (1.8453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7556 (8.8604)  time: 0.7305 (0.5251 -- 2.4809)  data: 0.0018 (0.0005 -- 0.0061)  max mem: 16413
Epoch: [105]  [ 80/160]  eta: 0:01:14  lr: 0.000011  min_lr: 0.000000  loss: 1.9367 (1.8814)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4461 (8.9705)  time: 0.9053 (0.5225 -- 4.5017)  data: 0.0015 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [105]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.7228 (1.8408)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2618 (8.8429)  time: 0.9370 (0.5261 -- 3.8940)  data: 0.0025 (0.0004 -- 0.0156)  max mem: 16413
[2023-08-30 17:36:22,458] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:36:22,459] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:36:22,460] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:36:22,460] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [105]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.7004 (1.8131)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4997 (8.8239)  time: 0.7606 (0.5258 -- 2.9270)  data: 0.0013 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [105]  [140/160]  eta: 0:00:17  lr: 0.000011  min_lr: 0.000000  loss: 1.7521 (1.8044)  loss_scale: 32768.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7871 (8.9287)  time: 0.8368 (0.5318 -- 2.5932)  data: 0.0024 (0.0002 -- 0.0205)  max mem: 16413
[2023-08-30 17:36:50,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16952
[2023-08-30 17:36:50,440] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:36:50,440] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 16952
[2023-08-30 17:36:50,441] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:36:50,441] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [105]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.8718 (1.7986)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7193 (9.0134)  time: 0.6919 (0.4997 -- 2.1851)  data: 0.0250 (0.0002 -- 0.4791)  max mem: 16413
Epoch: [105] Total time: 0:02:19 (0.8737 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.8718 (1.7971)  loss_scale: 32768.0000 (20070.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7193 (9.0134)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3414 (0.3414)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2567 (2.2567 -- 2.2567)  data: 2.0294 (2.0294 -- 2.0294)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4662 (0.7460)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4131 (0.1945 -- 2.2567)  data: 0.1950 (0.0005 -- 2.0294)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4873 (0.6825)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2256 (0.1696 -- 0.3627)  data: 0.0187 (0.0001 -- 0.1599)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6002 (0.7365)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2107 (0.1329 -- 0.3627)  data: 0.0181 (0.0001 -- 0.1599)  max mem: 16413
Val: Total time: 0:00:07 (0.2872 s / it)
* Acc@1 82.365 Acc@5 96.680 loss 0.687
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.57%
Epoch: [106]  [  0/160]  eta: 0:20:44  lr: 0.000011  min_lr: 0.000000  loss: 1.7034 (1.7034)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0168 (9.0168)  time: 7.7774 (7.7774 -- 7.7774)  data: 6.2279 (6.2279 -- 6.2279)  max mem: 16413
Epoch: [106]  [ 20/160]  eta: 0:02:36  lr: 0.000011  min_lr: 0.000000  loss: 1.9392 (1.8388)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9437 (8.9418)  time: 0.7812 (0.5262 -- 2.5219)  data: 0.0926 (0.0005 -- 1.2987)  max mem: 16413
[2023-08-30 17:37:41,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=99, lr=[2.622140921840345e-07, 2.622140921840345e-07, 3.4961878957871263e-07, 3.4961878957871263e-07, 4.661583861049502e-07, 4.661583861049502e-07, 6.215445148066003e-07, 6.215445148066003e-07, 8.287260197421337e-07, 8.287260197421337e-07, 1.104968026322845e-06, 1.104968026322845e-06, 1.4732907017637933e-06, 1.4732907017637933e-06, 1.9643876023517244e-06, 1.9643876023517244e-06, 2.619183469802299e-06, 2.619183469802299e-06, 3.4922446264030654e-06, 3.4922446264030654e-06, 4.656326168537421e-06, 4.656326168537421e-06, 6.2084348913832275e-06, 6.2084348913832275e-06, 8.27791318851097e-06, 8.27791318851097e-06, 1.1037217584681293e-05, 1.1037217584681293e-05], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 17:37:41,795] [INFO] [timer.py:260:stop] epoch=0/micro_step=17000/global_step=17000, RunningAvgSamplesPerSec=17.713317429458307, CurrSamplesPerSec=21.981842099286716, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [106]  [ 40/160]  eta: 0:02:03  lr: 0.000011  min_lr: 0.000000  loss: 1.9106 (1.8594)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5074 (8.9191)  time: 0.9412 (0.5285 -- 3.1474)  data: 0.3037 (0.0005 -- 2.4221)  max mem: 16413
Epoch: [106]  [ 60/160]  eta: 0:01:38  lr: 0.000011  min_lr: 0.000000  loss: 1.8319 (1.8492)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2232 (8.9434)  time: 0.8987 (0.5191 -- 4.0170)  data: 0.2596 (0.0005 -- 3.5046)  max mem: 16413
Epoch: [106]  [ 80/160]  eta: 0:01:14  lr: 0.000011  min_lr: 0.000000  loss: 1.6756 (1.8157)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9606 (8.9185)  time: 0.7567 (0.5317 -- 2.3530)  data: 0.0985 (0.0002 -- 1.1693)  max mem: 16413
Epoch: [106]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 1.7616 (1.8038)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2073 (8.9276)  time: 0.8629 (0.5312 -- 3.0652)  data: 0.0140 (0.0003 -- 0.2550)  max mem: 16413
[2023-08-30 17:38:40,331] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17064
[2023-08-30 17:38:40,331] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17064
[2023-08-30 17:38:40,332] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:38:40,331] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:38:40,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [106]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.7590 (1.7907)  loss_scale: 8192.0000 (15233.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0748 (8.8805)  time: 1.0087 (0.5127 -- 4.3880)  data: 0.0019 (0.0003 -- 0.0150)  max mem: 16413
Epoch: [106]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8551 (1.7988)  loss_scale: 8192.0000 (14234.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6205 (8.7445)  time: 0.7673 (0.5211 -- 4.4108)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
Epoch: [106]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.8433 (1.8019)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1027 (8.7792)  time: 0.7071 (0.4955 -- 2.7564)  data: 0.0006 (0.0002 -- 0.0016)  max mem: 16413
Epoch: [106] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.8433 (1.7942)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1027 (8.7792)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3332 (0.3332)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3994 (2.3994 -- 2.3994)  data: 2.1999 (2.1999 -- 2.1999)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4715 (0.7458)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4346 (0.1993 -- 2.3994)  data: 0.2207 (0.0006 -- 2.1999)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4715 (0.6782)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2169 (0.1704 -- 0.3567)  data: 0.0128 (0.0001 -- 0.1350)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5964 (0.7331)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2029 (0.1332 -- 0.3567)  data: 0.0125 (0.0001 -- 0.1350)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 81.950 Acc@5 96.888 loss 0.686
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.57%
Epoch: [107]  [  0/160]  eta: 0:18:34  lr: 0.000011  min_lr: 0.000000  loss: 1.8903 (1.8903)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7531 (4.7531)  time: 6.9631 (6.9631 -- 6.9631)  data: 6.2499 (6.2499 -- 6.2499)  max mem: 16413
Epoch: [107]  [ 20/160]  eta: 0:02:38  lr: 0.000011  min_lr: 0.000000  loss: 1.8366 (1.8062)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6712 (7.5613)  time: 0.8411 (0.5236 -- 3.1511)  data: 0.3023 (0.0006 -- 2.6308)  max mem: 16413
Epoch: [107]  [ 40/160]  eta: 0:02:08  lr: 0.000011  min_lr: 0.000000  loss: 1.9676 (1.8406)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0882 (8.4163)  time: 1.0027 (0.5171 -- 4.5613)  data: 0.4543 (0.0003 -- 3.9971)  max mem: 16413
Epoch: [107]  [ 60/160]  eta: 0:01:36  lr: 0.000011  min_lr: 0.000000  loss: 1.7907 (1.8359)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7442 (8.3139)  time: 0.7578 (0.5248 -- 3.5109)  data: 0.2075 (0.0005 -- 2.9611)  max mem: 16413
[2023-08-30 17:40:44,302] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:40:44,302] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:40:44,303] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:40:44,303] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [107]  [ 80/160]  eta: 0:01:18  lr: 0.000011  min_lr: 0.000000  loss: 1.7502 (1.8082)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6110 (8.3621)  time: 1.0458 (0.5228 -- 4.3797)  data: 0.4948 (0.0004 -- 3.8611)  max mem: 16413
Epoch: [107]  [100/160]  eta: 0:00:55  lr: 0.000011  min_lr: 0.000000  loss: 1.7959 (1.8134)  loss_scale: 16384.0000 (10463.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8914 (8.3345)  time: 0.6645 (0.5132 -- 1.9912)  data: 0.1162 (0.0003 -- 1.4695)  max mem: 16413
Epoch: [107]  [120/160]  eta: 0:00:37  lr: 0.000011  min_lr: 0.000000  loss: 1.6453 (1.7934)  loss_scale: 16384.0000 (11441.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9753 (8.4992)  time: 1.0345 (0.5323 -- 4.2503)  data: 0.4801 (0.0009 -- 3.7263)  max mem: 16413
Epoch: [107]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.9611 (1.8143)  loss_scale: 16384.0000 (12142.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9563 (8.5412)  time: 0.7773 (0.5171 -- 3.0695)  data: 0.2303 (0.0003 -- 2.5247)  max mem: 16413
Epoch: [107]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.9397 (1.8105)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1520 (8.5746)  time: 0.6082 (0.4972 -- 1.7471)  data: 0.0842 (0.0001 -- 1.2274)  max mem: 16413
Epoch: [107] Total time: 0:02:21 (0.8818 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.9397 (1.7793)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1520 (8.5746)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3333 (0.3333)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4720 (2.4720 -- 2.4720)  data: 2.2157 (2.2157 -- 2.2157)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4706 (0.7382)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4228 (0.2013 -- 2.4720)  data: 0.2025 (0.0007 -- 2.2157)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4811 (0.6782)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2201 (0.1697 -- 0.4052)  data: 0.0116 (0.0001 -- 0.2168)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6164 (0.7331)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2032 (0.1342 -- 0.4052)  data: 0.0113 (0.0001 -- 0.2168)  max mem: 16413
Val: Total time: 0:00:07 (0.2913 s / it)
* Acc@1 82.158 Acc@5 96.888 loss 0.687
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.57%
Epoch: [108]  [  0/160]  eta: 0:22:39  lr: 0.000011  min_lr: 0.000000  loss: 2.0200 (2.0200)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8933 (8.8933)  time: 8.4962 (8.4962 -- 8.4962)  data: 7.9733 (7.9733 -- 7.9733)  max mem: 16413
Epoch: [108]  [ 20/160]  eta: 0:02:54  lr: 0.000011  min_lr: 0.000000  loss: 1.7012 (1.8064)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5649 (9.9199)  time: 0.8803 (0.5294 -- 4.3455)  data: 0.2909 (0.0004 -- 3.8203)  max mem: 16413
Epoch: [108]  [ 40/160]  eta: 0:02:06  lr: 0.000011  min_lr: 0.000000  loss: 1.9087 (1.8281)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8442 (9.3447)  time: 0.8567 (0.5270 -- 2.6431)  data: 0.2603 (0.0002 -- 2.0638)  max mem: 16413
[2023-08-30 17:42:46,296] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:42:46,297] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:42:46,300] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:42:46,301] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:42:48,436] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17325
[2023-08-30 17:42:48,436] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17325
[2023-08-30 17:42:48,436] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:42:48,436] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:42:48,436] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [108]  [ 60/160]  eta: 0:01:39  lr: 0.000011  min_lr: 0.000000  loss: 1.9172 (1.8527)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9024 (9.2825)  time: 0.8797 (0.5190 -- 3.5652)  data: 0.3409 (0.0004 -- 3.0115)  max mem: 16413
Epoch: [108]  [ 80/160]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000000  loss: 1.7919 (1.8435)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1668 (9.1800)  time: 0.8596 (0.5222 -- 3.3993)  data: 0.3070 (0.0003 -- 2.8603)  max mem: 16413
Epoch: [108]  [100/160]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000000  loss: 1.8659 (1.8271)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8073 (8.9704)  time: 0.8808 (0.5066 -- 3.5570)  data: 0.3448 (0.0003 -- 3.0480)  max mem: 16413
Epoch: [108]  [120/160]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000000  loss: 1.8717 (1.8389)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4443 (8.8813)  time: 0.7546 (0.5337 -- 2.8456)  data: 0.2013 (0.0002 -- 2.3396)  max mem: 16413
Epoch: [108]  [140/160]  eta: 0:00:18  lr: 0.000011  min_lr: 0.000000  loss: 1.8571 (1.8456)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5484 (8.8384)  time: 0.9086 (0.5220 -- 3.2290)  data: 0.3467 (0.0003 -- 2.6956)  max mem: 16413
Epoch: [108]  [159/160]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000000  loss: 1.6675 (1.8302)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2968 (8.8183)  time: 0.6899 (0.4946 -- 3.5441)  data: 0.1669 (0.0002 -- 3.0352)  max mem: 16413
Epoch: [108] Total time: 0:02:22 (0.8886 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000000  loss: 1.6675 (1.8120)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2968 (8.8183)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3417 (0.3417)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2184 (2.2184 -- 2.2184)  data: 2.0234 (2.0234 -- 2.0234)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4473 (0.7437)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4085 (0.1970 -- 2.2184)  data: 0.1909 (0.0007 -- 2.0234)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4586 (0.6840)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2257 (0.1738 -- 0.3783)  data: 0.0188 (0.0001 -- 0.1688)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6026 (0.7342)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2115 (0.1330 -- 0.3783)  data: 0.0185 (0.0001 -- 0.1688)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 82.365 Acc@5 96.680 loss 0.687
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.57%
Epoch: [109]  [  0/160]  eta: 0:19:33  lr: 0.000011  min_lr: 0.000000  loss: 1.9487 (1.9487)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2039 (6.2039)  time: 7.3324 (7.3324 -- 7.3324)  data: 4.9073 (4.9073 -- 4.9073)  max mem: 16413
[2023-08-30 17:44:48,774] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:44:48,774] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:44:48,776] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:44:48,777] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [109]  [ 20/160]  eta: 0:02:44  lr: 0.000010  min_lr: 0.000000  loss: 1.8169 (1.7551)  loss_scale: 16384.0000 (21845.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5619 (9.1334)  time: 0.8671 (0.5222 -- 4.0130)  data: 0.0212 (0.0006 -- 0.3939)  max mem: 16413
Epoch: [109]  [ 40/160]  eta: 0:02:05  lr: 0.000010  min_lr: 0.000000  loss: 1.9510 (1.8373)  loss_scale: 32768.0000 (27173.4634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0575 (8.6607)  time: 0.9093 (0.5326 -- 3.3273)  data: 0.0280 (0.0003 -- 0.5320)  max mem: 16413
Epoch: [109]  [ 60/160]  eta: 0:01:39  lr: 0.000010  min_lr: 0.000000  loss: 1.7418 (1.8054)  loss_scale: 32768.0000 (29007.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1184 (8.6266)  time: 0.8791 (0.5214 -- 4.5668)  data: 0.0012 (0.0004 -- 0.0024)  max mem: 16413
[2023-08-30 17:45:37,310] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17506
[2023-08-30 17:45:37,310] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17506
[2023-08-30 17:45:37,310] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:45:37,310] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:45:37,310] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [109]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000000  loss: 1.9009 (1.8159)  loss_scale: 16384.0000 (26902.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6839 (8.5450)  time: 0.9239 (0.5234 -- 3.1554)  data: 0.0011 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [109]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 1.7868 (1.8105)  loss_scale: 16384.0000 (24819.3267)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3967 (8.4272)  time: 0.8034 (0.5139 -- 3.9577)  data: 0.0015 (0.0003 -- 0.0058)  max mem: 16413
Epoch: [109]  [120/160]  eta: 0:00:37  lr: 0.000010  min_lr: 0.000000  loss: 1.9381 (1.8326)  loss_scale: 16384.0000 (23425.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1096 (8.5037)  time: 0.9242 (0.5203 -- 4.6038)  data: 0.0013 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [109]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.9146 (1.8329)  loss_scale: 16384.0000 (22426.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1662 (8.5899)  time: 0.8085 (0.5205 -- 3.5193)  data: 0.0015 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [109]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8174 (1.8253)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7392 (8.5781)  time: 0.6861 (0.4955 -- 3.9479)  data: 0.0006 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [109] Total time: 0:02:22 (0.8928 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.8174 (1.8073)  loss_scale: 16384.0000 (21708.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7392 (8.5781)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.3218 (0.3218)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3870 (2.3870 -- 2.3870)  data: 2.1404 (2.1404 -- 2.1404)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4406 (0.7340)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4163 (0.2077 -- 2.3870)  data: 0.1956 (0.0008 -- 2.1404)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4447 (0.6699)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2170 (0.1701 -- 0.3223)  data: 0.0083 (0.0001 -- 0.1071)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5896 (0.7213)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.1998 (0.1333 -- 0.3223)  data: 0.0081 (0.0001 -- 0.1071)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.676
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.57%
Epoch: [110]  [  0/160]  eta: 0:21:40  lr: 0.000010  min_lr: 0.000000  loss: 2.2392 (2.2392)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1424 (8.1424)  time: 8.1307 (8.1307 -- 8.1307)  data: 7.6141 (7.6141 -- 7.6141)  max mem: 16413
Epoch: [110]  [ 20/160]  eta: 0:02:38  lr: 0.000010  min_lr: 0.000000  loss: 1.8439 (1.8835)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1491 (8.5169)  time: 0.7799 (0.5183 -- 2.0490)  data: 0.0783 (0.0003 -- 1.5257)  max mem: 16413
[2023-08-30 17:47:40,520] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:47:40,521] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:47:40,523] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:47:40,524] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:47:42,220] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17638
[2023-08-30 17:47:42,220] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17638
[2023-08-30 17:47:42,221] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:47:42,221] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:47:42,221] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [110]  [ 40/160]  eta: 0:02:03  lr: 0.000010  min_lr: 0.000000  loss: 1.7343 (1.8295)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2920 (8.6722)  time: 0.9210 (0.5265 -- 2.6785)  data: 0.1210 (0.0003 -- 2.0062)  max mem: 16413
Epoch: [110]  [ 60/160]  eta: 0:01:40  lr: 0.000010  min_lr: 0.000000  loss: 1.7018 (1.8035)  loss_scale: 16384.0000 (17189.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7798 (8.8132)  time: 0.9477 (0.5235 -- 3.5805)  data: 0.0017 (0.0002 -- 0.0068)  max mem: 16413
[2023-08-30 17:48:09,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17669
[2023-08-30 17:48:09,875] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17669
[2023-08-30 17:48:09,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:48:09,875] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:48:09,875] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [110]  [ 80/160]  eta: 0:01:15  lr: 0.000010  min_lr: 0.000000  loss: 1.8289 (1.8097)  loss_scale: 8192.0000 (15777.1852)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4530 (8.6056)  time: 0.7811 (0.5205 -- 2.8492)  data: 0.0019 (0.0001 -- 0.0084)  max mem: 16413
Epoch: [110]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.6536 (1.7876)  loss_scale: 8192.0000 (14275.1683)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7925 (8.7122)  time: 0.8492 (0.5204 -- 2.7074)  data: 0.1080 (0.0007 -- 2.1298)  max mem: 16413
Epoch: [110]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.5698 (1.7637)  loss_scale: 8192.0000 (13269.6860)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8823 (8.6273)  time: 0.7830 (0.5235 -- 2.1357)  data: 0.2342 (0.0004 -- 1.6193)  max mem: 16413
Epoch: [110]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.7781 (1.7599)  loss_scale: 8192.0000 (12549.4468)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7107 (8.5440)  time: 1.0102 (0.5330 -- 4.6887)  data: 0.4660 (0.0006 -- 4.1465)  max mem: 16413
Epoch: [110]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.9216 (1.7752)  loss_scale: 8192.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4540 (8.4911)  time: 0.6385 (0.4962 -- 2.0486)  data: 0.1124 (0.0001 -- 1.5439)  max mem: 16413
Epoch: [110] Total time: 0:02:21 (0.8865 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.9216 (1.7782)  loss_scale: 8192.0000 (12032.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4540 (8.4911)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3391 (0.3391)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4392 (2.4392 -- 2.4392)  data: 2.1952 (2.1952 -- 2.1952)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4664 (0.7305)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4181 (0.1934 -- 2.4392)  data: 0.2019 (0.0008 -- 2.1952)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4784 (0.6729)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2161 (0.1694 -- 0.3398)  data: 0.0098 (0.0001 -- 0.1341)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5825 (0.7257)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (96.2656)  time: 0.1996 (0.1328 -- 0.3398)  data: 0.0094 (0.0001 -- 0.1341)  max mem: 16413
Val: Total time: 0:00:07 (0.2869 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.685
Accuracy of the network on the 482 val images: 82.78%
[2023-08-30 17:49:31,181] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 17:49:31,183] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 17:49:31,183] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 17:49:31,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 17:49:32,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 17:49:32,718] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.78%
Epoch: [111]  [  0/160]  eta: 0:23:12  lr: 0.000010  min_lr: 0.000000  loss: 2.0927 (2.0927)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7506 (7.7506)  time: 8.7002 (8.7002 -- 8.7002)  data: 8.1700 (8.1700 -- 8.1700)  max mem: 16413
Epoch: [111]  [ 20/160]  eta: 0:02:38  lr: 0.000010  min_lr: 0.000000  loss: 1.8572 (1.8373)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1989 (9.3029)  time: 0.7536 (0.5336 -- 2.5805)  data: 0.1948 (0.0005 -- 2.0575)  max mem: 16413
[2023-08-30 17:50:12,236] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:50:12,237] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:50:12,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:50:12,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [111]  [ 40/160]  eta: 0:01:59  lr: 0.000010  min_lr: 0.000000  loss: 1.6931 (1.7450)  loss_scale: 8192.0000 (8791.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7518 (8.5546)  time: 0.8476 (0.5299 -- 2.9952)  data: 0.3017 (0.0004 -- 2.4802)  max mem: 16413
Epoch: [111]  [ 60/160]  eta: 0:01:36  lr: 0.000010  min_lr: 0.000000  loss: 1.7106 (1.7372)  loss_scale: 16384.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7388 (8.3114)  time: 0.9041 (0.5192 -- 2.8103)  data: 0.3181 (0.0003 -- 2.2793)  max mem: 16413
Epoch: [111]  [ 80/160]  eta: 0:01:14  lr: 0.000010  min_lr: 0.000000  loss: 1.7696 (1.7515)  loss_scale: 16384.0000 (12540.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6445 (8.3579)  time: 0.8450 (0.5252 -- 3.2775)  data: 0.2948 (0.0001 -- 2.7594)  max mem: 16413
Epoch: [111]  [100/160]  eta: 0:00:54  lr: 0.000010  min_lr: 0.000000  loss: 1.7695 (1.7431)  loss_scale: 16384.0000 (13301.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2778 (8.2371)  time: 0.8290 (0.5141 -- 2.8322)  data: 0.2797 (0.0009 -- 2.2872)  max mem: 16413
[2023-08-30 17:51:16,511] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17871
[2023-08-30 17:51:16,511] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 17:51:16,511] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 17871
[2023-08-30 17:51:16,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 17:51:16,512] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [111]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.7148 (1.7439)  loss_scale: 8192.0000 (13134.2810)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2038 (8.5388)  time: 0.9093 (0.5091 -- 2.6114)  data: 0.3563 (0.0005 -- 2.0661)  max mem: 16413
Epoch: [111]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.7681 (1.7600)  loss_scale: 8192.0000 (12433.2482)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8002 (8.6203)  time: 0.8512 (0.5288 -- 3.9266)  data: 0.3035 (0.0005 -- 3.4123)  max mem: 16413
Epoch: [111]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8677 (1.7659)  loss_scale: 8192.0000 (11929.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1092 (8.6349)  time: 0.7828 (0.4961 -- 4.2106)  data: 0.2716 (0.0002 -- 3.7037)  max mem: 16413
Epoch: [111] Total time: 0:02:22 (0.8913 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.8677 (1.7757)  loss_scale: 8192.0000 (11929.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1092 (8.6349)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.3299 (0.3299)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2210 (2.2210 -- 2.2210)  data: 2.0035 (2.0035 -- 2.0035)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4429 (0.7319)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4056 (0.2058 -- 2.2210)  data: 0.1885 (0.0010 -- 2.0035)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4602 (0.6709)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2212 (0.1705 -- 0.4060)  data: 0.0137 (0.0001 -- 0.1854)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6001 (0.7208)  acc1: 77.7778 (80.4979)  acc5: 100.0000 (95.8506)  time: 0.2038 (0.1329 -- 0.4060)  data: 0.0134 (0.0001 -- 0.1854)  max mem: 16413
Val: Total time: 0:00:07 (0.2830 s / it)
* Acc@1 82.780 Acc@5 96.888 loss 0.679
Accuracy of the network on the 482 val images: 82.78%
[2023-08-30 17:52:03,047] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 17:52:03,049] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 17:52:03,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 17:52:03,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 17:52:04,403] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 17:52:04,403] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.78%
Epoch: [112]  [  0/160]  eta: 0:18:39  lr: 0.000010  min_lr: 0.000000  loss: 0.9678 (0.9678)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5029 (6.5029)  time: 6.9979 (6.9979 -- 6.9979)  data: 6.4817 (6.4817 -- 6.4817)  max mem: 16413
Epoch: [112]  [ 20/160]  eta: 0:02:45  lr: 0.000010  min_lr: 0.000000  loss: 1.7298 (1.7271)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5105 (8.3466)  time: 0.8918 (0.5261 -- 3.5959)  data: 0.3039 (0.0008 -- 3.0433)  max mem: 16413
Epoch: [112]  [ 40/160]  eta: 0:01:57  lr: 0.000010  min_lr: 0.000000  loss: 1.8372 (1.7653)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2653 (8.5091)  time: 0.7647 (0.5334 -- 2.3952)  data: 0.1711 (0.0003 -- 1.8628)  max mem: 16413
Epoch: [112]  [ 60/160]  eta: 0:01:33  lr: 0.000010  min_lr: 0.000000  loss: 1.7329 (1.7803)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8604 (8.4499)  time: 0.8574 (0.5241 -- 3.8557)  data: 0.1253 (0.0004 -- 2.0250)  max mem: 16413
[2023-08-30 17:53:18,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=105, lr=[2.3441980955478927e-07, 2.3441980955478927e-07, 3.125597460730524e-07, 3.125597460730524e-07, 4.1674632809740317e-07, 4.1674632809740317e-07, 5.556617707965376e-07, 5.556617707965376e-07, 7.408823610620501e-07, 7.408823610620501e-07, 9.878431480827334e-07, 9.878431480827334e-07, 1.3171241974436446e-06, 1.3171241974436446e-06, 1.756165596591526e-06, 1.756165596591526e-06, 2.3415541287887014e-06, 2.3415541287887014e-06, 3.1220721717182684e-06, 3.1220721717182684e-06, 4.162762895624358e-06, 4.162762895624358e-06, 5.550350527499144e-06, 5.550350527499144e-06, 7.400467369998859e-06, 7.400467369998859e-06, 9.867289826665145e-06, 9.867289826665145e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 17:53:18,396] [INFO] [timer.py:260:stop] epoch=0/micro_step=18000/global_step=18000, RunningAvgSamplesPerSec=17.759649742122818, CurrSamplesPerSec=20.71969797078754, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
[2023-08-30 17:53:18,924] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:53:18,924] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 17:53:18,925] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:53:18,925] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [112]  [ 80/160]  eta: 0:01:13  lr: 0.000010  min_lr: 0.000000  loss: 1.8427 (1.7910)  loss_scale: 8192.0000 (8293.1358)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7963 (8.5329)  time: 0.8620 (0.5268 -- 2.1670)  data: 0.1300 (0.0002 -- 1.6437)  max mem: 16413
Epoch: [112]  [100/160]  eta: 0:00:55  lr: 0.000010  min_lr: 0.000000  loss: 1.7691 (1.7945)  loss_scale: 16384.0000 (9895.2871)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9794 (8.6473)  time: 0.9174 (0.5248 -- 2.6347)  data: 0.1588 (0.0002 -- 1.6250)  max mem: 16413
Epoch: [112]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.6843 (1.7766)  loss_scale: 16384.0000 (10967.8017)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1515 (8.7312)  time: 0.8982 (0.5233 -- 2.5502)  data: 0.1896 (0.0005 -- 2.0220)  max mem: 16413
Epoch: [112]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.7477 (1.7752)  loss_scale: 16384.0000 (11736.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7711 (8.7765)  time: 0.9285 (0.5209 -- 4.1610)  data: 0.3491 (0.0003 -- 3.6417)  max mem: 16413
Epoch: [112]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.8776 (1.7844)  loss_scale: 16384.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2590 (8.7011)  time: 0.6001 (0.4966 -- 1.7246)  data: 0.0811 (0.0002 -- 1.2042)  max mem: 16413
Epoch: [112] Total time: 0:02:20 (0.8807 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.8776 (1.7887)  loss_scale: 16384.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2590 (8.7011)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3318 (0.3318)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4367 (2.4367 -- 2.4367)  data: 2.1865 (2.1865 -- 2.1865)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4797 (0.7321)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4317 (0.1974 -- 2.4367)  data: 0.2187 (0.0006 -- 2.1865)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4797 (0.6704)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2225 (0.1691 -- 0.4306)  data: 0.0225 (0.0001 -- 0.2275)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6043 (0.7254)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2085 (0.1346 -- 0.4306)  data: 0.0219 (0.0001 -- 0.2275)  max mem: 16413
Val: Total time: 0:00:07 (0.2917 s / it)
* Acc@1 82.158 Acc@5 96.888 loss 0.681
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.78%
Epoch: [113]  [  0/160]  eta: 0:23:58  lr: 0.000010  min_lr: 0.000000  loss: 1.4886 (1.4886)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1815 (7.1815)  time: 8.9903 (8.9903 -- 8.9903)  data: 8.4538 (8.4538 -- 8.4538)  max mem: 16413
Epoch: [113]  [ 20/160]  eta: 0:02:37  lr: 0.000010  min_lr: 0.000000  loss: 1.9183 (1.8114)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1788 (9.6592)  time: 0.7330 (0.5340 -- 2.5650)  data: 0.1772 (0.0005 -- 2.0372)  max mem: 16413
Epoch: [113]  [ 40/160]  eta: 0:01:59  lr: 0.000010  min_lr: 0.000000  loss: 1.8184 (1.7963)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6211 (9.1447)  time: 0.8662 (0.5350 -- 2.4542)  data: 0.1904 (0.0008 -- 1.6347)  max mem: 16413
[2023-08-30 17:55:21,663] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:55:21,663] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:55:21,666] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:55:21,666] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:55:26,101] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18133
[2023-08-30 17:55:26,101] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18133
[2023-08-30 17:55:26,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:55:26,101] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:55:26,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [113]  [ 60/160]  eta: 0:01:36  lr: 0.000010  min_lr: 0.000000  loss: 1.7321 (1.8030)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1940 (9.0670)  time: 0.8930 (0.5132 -- 2.2937)  data: 0.1035 (0.0003 -- 0.9718)  max mem: 16413
Epoch: [113]  [ 80/160]  eta: 0:01:17  lr: 0.000010  min_lr: 0.000000  loss: 1.9515 (1.8455)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3607 (9.0553)  time: 0.9642 (0.5172 -- 4.4847)  data: 0.3920 (0.0004 -- 3.9252)  max mem: 16413
Epoch: [113]  [100/160]  eta: 0:00:56  lr: 0.000010  min_lr: 0.000000  loss: 1.9491 (1.8543)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2697 (8.9300)  time: 0.8608 (0.5179 -- 4.2662)  data: 0.3208 (0.0003 -- 3.7393)  max mem: 16413
Epoch: [113]  [120/160]  eta: 0:00:36  lr: 0.000010  min_lr: 0.000000  loss: 1.6834 (1.8341)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4214 (8.9121)  time: 0.8259 (0.5246 -- 2.8837)  data: 0.2860 (0.0003 -- 2.3683)  max mem: 16413
Epoch: [113]  [140/160]  eta: 0:00:18  lr: 0.000010  min_lr: 0.000000  loss: 1.9172 (1.8398)  loss_scale: 16384.0000 (16964.9929)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7414 (8.9418)  time: 0.9072 (0.5163 -- 4.1543)  data: 0.3606 (0.0004 -- 3.6306)  max mem: 16413
Epoch: [113]  [159/160]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000000  loss: 1.9560 (1.8515)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3846 (9.0803)  time: 0.5533 (0.4977 -- 1.0810)  data: 0.0300 (0.0002 -- 0.5867)  max mem: 16413
Epoch: [113] Total time: 0:02:20 (0.8784 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000000  loss: 1.9560 (1.8262)  loss_scale: 16384.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3846 (9.0803)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3408 (0.3408)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4210 (2.4210 -- 2.4210)  data: 2.1821 (2.1821 -- 2.1821)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4450 (0.7244)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4189 (0.1908 -- 2.4210)  data: 0.2006 (0.0007 -- 2.1821)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4751 (0.6729)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2122 (0.1745 -- 0.2458)  data: 0.0035 (0.0001 -- 0.0309)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.6025 (0.7206)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.1941 (0.1330 -- 0.2416)  data: 0.0032 (0.0001 -- 0.0309)  max mem: 16413
Val: Total time: 0:00:07 (0.2835 s / it)
* Acc@1 82.780 Acc@5 96.680 loss 0.679
Accuracy of the network on the 482 val images: 82.78%
[2023-08-30 17:57:01,459] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 17:57:01,461] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 17:57:01,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 17:57:01,461] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 17:57:02,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 17:57:02,781] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.78%
Epoch: [114]  [  0/160]  eta: 0:19:04  lr: 0.000010  min_lr: 0.000000  loss: 1.7576 (1.7576)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1787 (9.1787)  time: 7.1554 (7.1554 -- 7.1554)  data: 6.6327 (6.6327 -- 6.6327)  max mem: 16413
Epoch: [114]  [ 20/160]  eta: 0:02:47  lr: 0.000010  min_lr: 0.000000  loss: 1.7487 (1.8144)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5287 (8.5697)  time: 0.8972 (0.5282 -- 3.3025)  data: 0.3499 (0.0003 -- 2.7313)  max mem: 16413
[2023-08-30 17:57:28,924] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:57:28,924] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:57:28,926] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:57:28,926] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:57:40,096] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18272
[2023-08-30 17:57:40,096] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:57:40,096] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 17:57:40,096] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18272
[2023-08-30 17:57:40,097] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [114]  [ 40/160]  eta: 0:02:08  lr: 0.000010  min_lr: 0.000000  loss: 1.9392 (1.8634)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6803 (8.7824)  time: 0.9481 (0.5124 -- 3.9057)  data: 0.4044 (0.0001 -- 3.3657)  max mem: 16413
Epoch: [114]  [ 60/160]  eta: 0:01:37  lr: 0.000010  min_lr: 0.000000  loss: 1.8086 (1.8134)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2624 (8.7225)  time: 0.7753 (0.5284 -- 3.3320)  data: 0.2212 (0.0004 -- 2.7992)  max mem: 16413
Epoch: [114]  [ 80/160]  eta: 0:01:17  lr: 0.000009  min_lr: 0.000000  loss: 1.7066 (1.7957)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3930 (8.7800)  time: 0.9532 (0.5139 -- 4.9613)  data: 0.4072 (0.0004 -- 4.4104)  max mem: 16413
Epoch: [114]  [100/160]  eta: 0:00:56  lr: 0.000009  min_lr: 0.000000  loss: 1.9644 (1.8105)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7788 (8.7438)  time: 0.7832 (0.5173 -- 4.0585)  data: 0.2332 (0.0001 -- 3.5386)  max mem: 16413
Epoch: [114]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.5981 (1.7947)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4579 (8.6926)  time: 0.8657 (0.5332 -- 3.3735)  data: 0.3026 (0.0002 -- 2.7598)  max mem: 16413
Epoch: [114]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.7846 (1.7903)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8189 (8.6753)  time: 0.8508 (0.5138 -- 5.1692)  data: 0.2028 (0.0003 -- 2.7157)  max mem: 16413
Epoch: [114]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8390 (1.7992)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1264 (8.8132)  time: 0.6260 (0.4948 -- 1.6033)  data: 0.0912 (0.0002 -- 1.0319)  max mem: 16413
Epoch: [114] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.8390 (1.7921)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1264 (8.8132)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3179 (0.3179)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2583 (2.2583 -- 2.2583)  data: 2.0145 (2.0145 -- 2.0145)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4371 (0.7311)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4142 (0.1964 -- 2.2583)  data: 0.1963 (0.0006 -- 2.0145)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4371 (0.6670)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2290 (0.1698 -- 0.5519)  data: 0.0249 (0.0001 -- 0.3499)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5815 (0.7235)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2111 (0.1325 -- 0.5519)  data: 0.0246 (0.0001 -- 0.3499)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.677
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.78%
Epoch: [115]  [  0/160]  eta: 0:16:49  lr: 0.000009  min_lr: 0.000000  loss: 1.3271 (1.3271)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9242 (10.9242)  time: 6.3114 (6.3114 -- 6.3114)  data: 5.2016 (5.2016 -- 5.2016)  max mem: 16413
[2023-08-30 17:59:38,122] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:59:38,123] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:59:38,125] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 17:59:38,126] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 17:59:40,712] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18405
[2023-08-30 17:59:40,712] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 17:59:40,712] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18405
[2023-08-30 17:59:40,713] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 17:59:40,713] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [115]  [ 20/160]  eta: 0:02:39  lr: 0.000009  min_lr: 0.000000  loss: 1.8455 (1.7294)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2108 (8.2059)  time: 0.8838 (0.5364 -- 3.5058)  data: 0.0936 (0.0005 -- 0.8818)  max mem: 16413
Epoch: [115]  [ 40/160]  eta: 0:02:02  lr: 0.000009  min_lr: 0.000000  loss: 1.6184 (1.7224)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5888 (8.3790)  time: 0.8923 (0.5295 -- 3.0297)  data: 0.0018 (0.0007 -- 0.0059)  max mem: 16413
Epoch: [115]  [ 60/160]  eta: 0:01:33  lr: 0.000009  min_lr: 0.000000  loss: 1.5858 (1.7043)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4732 (8.6459)  time: 0.7676 (0.5236 -- 2.7663)  data: 0.0019 (0.0005 -- 0.0097)  max mem: 16413
Epoch: [115]  [ 80/160]  eta: 0:01:12  lr: 0.000009  min_lr: 0.000000  loss: 1.9956 (1.7521)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1802 (8.7661)  time: 0.8361 (0.5330 -- 2.5351)  data: 0.0016 (0.0006 -- 0.0033)  max mem: 16413
Epoch: [115]  [100/160]  eta: 0:00:54  lr: 0.000009  min_lr: 0.000000  loss: 1.7818 (1.7458)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1003 (8.7459)  time: 0.8957 (0.5224 -- 2.8695)  data: 0.0022 (0.0004 -- 0.0163)  max mem: 16413
Epoch: [115]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.8439 (1.7518)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7096 (8.6699)  time: 0.9470 (0.5251 -- 4.3661)  data: 0.0025 (0.0004 -- 0.0183)  max mem: 16413
[2023-08-30 18:01:32,582] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:01:32,582] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:01:32,582] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:01:32,582] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:01:38,669] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18539
[2023-08-30 18:01:38,670] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:01:38,670] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18539
[2023-08-30 18:01:38,670] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:01:38,670] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [115]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.8353 (1.7481)  loss_scale: 16384.0000 (17429.7872)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9592 (8.6026)  time: 0.8604 (0.5391 -- 2.8945)  data: 0.0018 (0.0006 -- 0.0066)  max mem: 16413
Epoch: [115]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7185 (1.7608)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9473 (8.6266)  time: 0.6198 (0.4942 -- 2.3606)  data: 0.0007 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [115] Total time: 0:02:19 (0.8739 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7185 (1.7835)  loss_scale: 16384.0000 (17305.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9473 (8.6266)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3063 (0.3063)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2766 (2.2766 -- 2.2766)  data: 2.0375 (2.0375 -- 2.0375)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4321 (0.7260)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4162 (0.1919 -- 2.2766)  data: 0.1994 (0.0004 -- 2.0375)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4608 (0.6664)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2184 (0.1698 -- 0.3811)  data: 0.0097 (0.0001 -- 0.1401)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5801 (0.7195)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2046 (0.1331 -- 0.3811)  data: 0.0096 (0.0001 -- 0.1401)  max mem: 16413
Val: Total time: 0:00:07 (0.2828 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.673
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.78%
Epoch: [116]  [  0/160]  eta: 0:18:42  lr: 0.000009  min_lr: 0.000000  loss: 2.0211 (2.0211)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8912 (7.8912)  time: 7.0152 (7.0152 -- 7.0152)  data: 6.3955 (6.3955 -- 6.3955)  max mem: 16413
Epoch: [116]  [ 20/160]  eta: 0:02:33  lr: 0.000009  min_lr: 0.000000  loss: 1.7598 (1.7333)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7954 (8.9525)  time: 0.8030 (0.5288 -- 3.3627)  data: 0.1063 (0.0005 -- 1.5352)  max mem: 16413
Epoch: [116]  [ 40/160]  eta: 0:02:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8730 (1.8206)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9500 (8.9022)  time: 0.9090 (0.5270 -- 2.9224)  data: 0.1786 (0.0004 -- 2.3008)  max mem: 16413
Epoch: [116]  [ 60/160]  eta: 0:01:35  lr: 0.000009  min_lr: 0.000000  loss: 1.7791 (1.8118)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9954 (8.7276)  time: 0.8539 (0.5215 -- 2.3038)  data: 0.1213 (0.0006 -- 1.6890)  max mem: 16413
Epoch: [116]  [ 80/160]  eta: 0:01:14  lr: 0.000009  min_lr: 0.000000  loss: 1.6980 (1.7932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9490 (8.6381)  time: 0.8521 (0.5246 -- 3.2642)  data: 0.0017 (0.0003 -- 0.0062)  max mem: 16413
Epoch: [116]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.7932 (1.7984)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7072 (8.7632)  time: 0.9270 (0.5245 -- 4.2360)  data: 0.0014 (0.0004 -- 0.0046)  max mem: 16413
[2023-08-30 18:03:38,397] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:03:38,398] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:03:38,400] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:03:38,400] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:03:39,467] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18670
[2023-08-30 18:03:39,467] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18670
[2023-08-30 18:03:39,468] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:03:39,468] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:03:39,468] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [116]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.7357 (1.7901)  loss_scale: 16384.0000 (16654.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6072 (8.8412)  time: 0.7624 (0.5080 -- 2.2650)  data: 0.0020 (0.0004 -- 0.0119)  max mem: 16413
Epoch: [116]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.7480 (1.7911)  loss_scale: 16384.0000 (16616.3972)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6518 (8.7636)  time: 0.9268 (0.5284 -- 3.9093)  data: 0.0013 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [116]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.7059 (1.7811)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4259 (8.8123)  time: 0.6795 (0.4962 -- 2.1710)  data: 0.0844 (0.0002 -- 1.6665)  max mem: 16413
Epoch: [116] Total time: 0:02:20 (0.8797 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.7059 (1.7792)  loss_scale: 16384.0000 (16588.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4259 (8.8123)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3042 (0.3042)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3032 (2.3032 -- 2.3032)  data: 2.0705 (2.0705 -- 2.0705)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4322 (0.7372)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4084 (0.2070 -- 2.3032)  data: 0.1894 (0.0009 -- 2.0705)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4644 (0.6695)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2212 (0.1711 -- 0.4049)  data: 0.0126 (0.0001 -- 0.2129)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5864 (0.7286)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.2045 (0.1331 -- 0.4049)  data: 0.0123 (0.0001 -- 0.2129)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 82.158 Acc@5 97.095 loss 0.676
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.78%
Epoch: [117]  [  0/160]  eta: 0:21:03  lr: 0.000009  min_lr: 0.000000  loss: 1.6562 (1.6562)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0608 (8.0608)  time: 7.8998 (7.8998 -- 7.8998)  data: 5.2679 (5.2679 -- 5.2679)  max mem: 16413
Epoch: [117]  [ 20/160]  eta: 0:02:42  lr: 0.000009  min_lr: 0.000000  loss: 1.8038 (1.7603)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8663 (9.9303)  time: 0.8275 (0.5181 -- 2.4708)  data: 0.0286 (0.0004 -- 0.5095)  max mem: 16413
[2023-08-30 18:04:58,345] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18745
[2023-08-30 18:04:58,345] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 18745
[2023-08-30 18:04:58,345] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:04:58,345] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:04:58,346] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [117]  [ 40/160]  eta: 0:02:12  lr: 0.000009  min_lr: 0.000000  loss: 1.7295 (1.7878)  loss_scale: 8192.0000 (13187.1220)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7087 (9.3560)  time: 1.0355 (0.5314 -- 4.2367)  data: 0.0017 (0.0005 -- 0.0073)  max mem: 16413
Epoch: [117]  [ 60/160]  eta: 0:01:37  lr: 0.000009  min_lr: 0.000000  loss: 1.8238 (1.8043)  loss_scale: 8192.0000 (11549.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1042 (8.9674)  time: 0.7209 (0.5269 -- 3.3599)  data: 0.0015 (0.0003 -- 0.0047)  max mem: 16413
Epoch: [117]  [ 80/160]  eta: 0:01:16  lr: 0.000009  min_lr: 0.000000  loss: 1.7872 (1.8192)  loss_scale: 8192.0000 (10720.3951)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6774 (9.0002)  time: 0.8764 (0.5261 -- 4.1353)  data: 0.0017 (0.0002 -- 0.0061)  max mem: 16413
Epoch: [117]  [100/160]  eta: 0:00:54  lr: 0.000009  min_lr: 0.000000  loss: 1.7474 (1.7933)  loss_scale: 8192.0000 (10219.7228)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1723 (8.7390)  time: 0.7623 (0.5308 -- 2.9267)  data: 0.0018 (0.0004 -- 0.0092)  max mem: 16413
Epoch: [117]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.7744 (1.7918)  loss_scale: 8192.0000 (9884.5620)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6810 (8.8348)  time: 0.8301 (0.5282 -- 2.5609)  data: 0.0013 (0.0005 -- 0.0027)  max mem: 16413
Epoch: [117]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.7410 (1.7861)  loss_scale: 8192.0000 (9644.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9998 (8.7750)  time: 0.9809 (0.5250 -- 4.1401)  data: 0.0011 (0.0002 -- 0.0031)  max mem: 16413
[2023-08-30 18:06:45,585] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:06:45,585] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:06:45,586] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:06:45,586] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [117]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.8052 (1.7815)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5944 (8.7931)  time: 0.6353 (0.4948 -- 2.8163)  data: 0.0008 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [117] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.8052 (1.7795)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5944 (8.7931)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.3259 (0.3259)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1801 (2.1801 -- 2.1801)  data: 1.9508 (1.9508 -- 1.9508)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4211 (0.7196)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.3992 (0.1988 -- 2.1801)  data: 0.1786 (0.0008 -- 1.9508)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4502 (0.6600)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2277 (0.1695 -- 0.4959)  data: 0.0196 (0.0001 -- 0.2943)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5807 (0.7162)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2097 (0.1335 -- 0.4959)  data: 0.0192 (0.0001 -- 0.2943)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 82.988 Acc@5 97.303 loss 0.670
Accuracy of the network on the 482 val images: 82.99%
[2023-08-30 18:06:55,874] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-best is about to be saved!
[2023-08-30 18:06:55,876] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt
[2023-08-30 18:06:55,876] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt...
[2023-08-30 18:06:55,876] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
[2023-08-30 18:06:57,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-best/mp_rank_00_model_states.pt.
[2023-08-30 18:06:57,266] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-best is ready now!
Max accuracy: 82.99%
Epoch: [118]  [  0/160]  eta: 0:21:22  lr: 0.000009  min_lr: 0.000000  loss: 1.2836 (1.2836)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6312 (7.6312)  time: 8.0180 (8.0180 -- 8.0180)  data: 5.5752 (5.5752 -- 5.5752)  max mem: 16413
Epoch: [118]  [ 20/160]  eta: 0:02:53  lr: 0.000009  min_lr: 0.000000  loss: 1.8375 (1.8764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3638 (8.1406)  time: 0.8979 (0.5311 -- 3.8123)  data: 0.0990 (0.0004 -- 1.9510)  max mem: 16413
Epoch: [118]  [ 40/160]  eta: 0:02:06  lr: 0.000009  min_lr: 0.000000  loss: 1.7990 (1.8469)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8589 (8.5236)  time: 0.8627 (0.5148 -- 2.9037)  data: 0.0843 (0.0003 -- 1.3093)  max mem: 16413
Epoch: [118]  [ 60/160]  eta: 0:01:39  lr: 0.000009  min_lr: 0.000000  loss: 1.8529 (1.8308)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1289 (8.6572)  time: 0.8782 (0.5212 -- 3.0981)  data: 0.0018 (0.0003 -- 0.0080)  max mem: 16413
Epoch: [118]  [ 80/160]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000000  loss: 1.9037 (1.8108)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9812 (8.8800)  time: 0.7775 (0.5193 -- 1.8259)  data: 0.0018 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [118]  [100/160]  eta: 0:00:55  lr: 0.000009  min_lr: 0.000000  loss: 1.8578 (1.8248)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3737 (8.8781)  time: 0.8215 (0.5290 -- 3.5210)  data: 0.0019 (0.0002 -- 0.0068)  max mem: 16413
[2023-08-30 18:08:49,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=111, lr=[2.0707675652636497e-07, 2.0707675652636497e-07, 2.761023420351533e-07, 2.761023420351533e-07, 3.6813645604687107e-07, 3.6813645604687107e-07, 4.908486080624948e-07, 4.908486080624948e-07, 6.54464810749993e-07, 6.54464810749993e-07, 8.726197476666574e-07, 8.726197476666574e-07, 1.1634929968888765e-06, 1.1634929968888765e-06, 1.5513239958518352e-06, 1.5513239958518352e-06, 2.0684319944691137e-06, 2.0684319944691137e-06, 2.757909325958818e-06, 2.757909325958818e-06, 3.677212434611758e-06, 3.677212434611758e-06, 4.9029499128156765e-06, 4.9029499128156765e-06, 6.537266550420903e-06, 6.537266550420903e-06, 8.716355400561203e-06, 8.716355400561203e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 18:08:49,274] [INFO] [timer.py:260:stop] epoch=0/micro_step=19000/global_step=19000, RunningAvgSamplesPerSec=17.702070993751185, CurrSamplesPerSec=22.686526906971608, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [118]  [120/160]  eta: 0:00:37  lr: 0.000009  min_lr: 0.000000  loss: 1.8500 (1.8165)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7936 (8.8130)  time: 1.0012 (0.5262 -- 3.2270)  data: 0.0023 (0.0002 -- 0.0119)  max mem: 16413
[2023-08-30 18:08:51,146] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:08:51,146] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:08:51,147] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:08:51,148] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [118]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.8687 (1.8223)  loss_scale: 32768.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0211 (8.6833)  time: 0.7572 (0.5230 -- 3.1833)  data: 0.0012 (0.0003 -- 0.0021)  max mem: 16413
Epoch: [118]  [159/160]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000000  loss: 1.6202 (1.8079)  loss_scale: 32768.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5547 (8.6574)  time: 0.8117 (0.4959 -- 5.0173)  data: 0.0006 (0.0002 -- 0.0018)  max mem: 16413
Epoch: [118] Total time: 0:02:23 (0.8979 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000000  loss: 1.6202 (1.8062)  loss_scale: 32768.0000 (20275.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5547 (8.6574)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.3179 (0.3179)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4568 (2.4568 -- 2.4568)  data: 2.1936 (2.1936 -- 2.1936)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4025 (0.7255)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4216 (0.2076 -- 2.4568)  data: 0.2006 (0.0007 -- 2.1936)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4581 (0.6686)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2140 (0.1693 -- 0.2705)  data: 0.0052 (0.0001 -- 0.0862)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5862 (0.7205)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.1972 (0.1331 -- 0.2705)  data: 0.0048 (0.0001 -- 0.0862)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.674
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [119]  [  0/160]  eta: 0:20:44  lr: 0.000009  min_lr: 0.000000  loss: 1.8222 (1.8222)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7227 (7.7227)  time: 7.7778 (7.7778 -- 7.7778)  data: 4.6075 (4.6075 -- 4.6075)  max mem: 16413
[2023-08-30 18:09:45,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19050
[2023-08-30 18:09:45,192] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19050
[2023-08-30 18:09:45,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:09:45,192] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:09:45,192] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [119]  [ 20/160]  eta: 0:02:46  lr: 0.000009  min_lr: 0.000000  loss: 1.8434 (1.7713)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0590 (8.4502)  time: 0.8606 (0.5350 -- 2.2238)  data: 0.1632 (0.0004 -- 1.6837)  max mem: 16413
Epoch: [119]  [ 40/160]  eta: 0:02:01  lr: 0.000009  min_lr: 0.000000  loss: 1.7985 (1.8181)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5105 (8.7875)  time: 0.8318 (0.5262 -- 2.4369)  data: 0.1843 (0.0004 -- 1.9022)  max mem: 16413
Epoch: [119]  [ 60/160]  eta: 0:01:36  lr: 0.000009  min_lr: 0.000000  loss: 1.8136 (1.8251)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5437 (8.9592)  time: 0.8719 (0.5264 -- 3.6494)  data: 0.1637 (0.0002 -- 2.9016)  max mem: 16413
Epoch: [119]  [ 80/160]  eta: 0:01:15  lr: 0.000009  min_lr: 0.000000  loss: 1.9463 (1.8368)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9971 (9.0005)  time: 0.8548 (0.5112 -- 4.1764)  data: 0.0015 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [119]  [100/160]  eta: 0:00:54  lr: 0.000009  min_lr: 0.000000  loss: 1.7099 (1.8096)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8211 (8.9017)  time: 0.7816 (0.5338 -- 2.9263)  data: 0.0017 (0.0003 -- 0.0056)  max mem: 16413
Epoch: [119]  [120/160]  eta: 0:00:36  lr: 0.000009  min_lr: 0.000000  loss: 1.7305 (1.7960)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7061 (8.8389)  time: 0.9798 (0.5326 -- 3.9493)  data: 0.0080 (0.0003 -- 0.1292)  max mem: 16413
[2023-08-30 18:11:36,233] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:11:36,234] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:11:36,235] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:11:36,235] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:11:36,800] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19180
[2023-08-30 18:11:36,800] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19180
[2023-08-30 18:11:36,800] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:11:36,800] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:11:36,800] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [119]  [140/160]  eta: 0:00:18  lr: 0.000009  min_lr: 0.000000  loss: 1.9682 (1.8188)  loss_scale: 16384.0000 (17662.1844)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4736 (8.9056)  time: 0.8359 (0.5243 -- 3.7305)  data: 0.0016 (0.0005 -- 0.0072)  max mem: 16413
Epoch: [119]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6399 (1.8023)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6472 (9.0443)  time: 0.6673 (0.4956 -- 2.2368)  data: 0.0008 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [119] Total time: 0:02:20 (0.8809 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6399 (1.7963)  loss_scale: 16384.0000 (17510.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6472 (9.0443)
[2023-08-30 18:11:49,653] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-119 is about to be saved!
[2023-08-30 18:11:49,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
[2023-08-30 18:11:49,657] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt
[2023-08-30 18:11:49,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt...
[2023-08-30 18:11:50,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-119/mp_rank_00_model_states.pt.
[2023-08-30 18:11:50,787] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-119 is ready now!
Val:  [ 0/27]  eta: 0:01:05  loss: 0.3272 (0.3272)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4323 (2.4323 -- 2.4323)  data: 2.1812 (2.1812 -- 2.1812)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4511 (0.7201)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4387 (0.2003 -- 2.4323)  data: 0.2140 (0.0007 -- 2.1812)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4511 (0.6622)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.8254)  time: 0.2245 (0.1687 -- 0.3624)  data: 0.0165 (0.0001 -- 0.1615)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5724 (0.7168)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2022 (0.1325 -- 0.3624)  data: 0.0161 (0.0001 -- 0.1615)  max mem: 16413
Val: Total time: 0:00:07 (0.2928 s / it)
* Acc@1 82.780 Acc@5 97.510 loss 0.676
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [120]  [  0/160]  eta: 0:20:34  lr: 0.000008  min_lr: 0.000000  loss: 2.1563 (2.1563)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3141 (7.3141)  time: 7.7142 (7.7142 -- 7.7142)  data: 5.4611 (5.4611 -- 5.4611)  max mem: 16413
Epoch: [120]  [ 20/160]  eta: 0:02:41  lr: 0.000008  min_lr: 0.000000  loss: 1.9028 (1.8361)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8441 (8.9813)  time: 0.8269 (0.5235 -- 3.3726)  data: 0.0857 (0.0002 -- 1.1256)  max mem: 16413
Epoch: [120]  [ 40/160]  eta: 0:02:07  lr: 0.000008  min_lr: 0.000000  loss: 1.7072 (1.8013)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8419 (8.7704)  time: 0.9727 (0.5304 -- 2.7592)  data: 0.0253 (0.0003 -- 0.4776)  max mem: 16413
Epoch: [120]  [ 60/160]  eta: 0:01:40  lr: 0.000008  min_lr: 0.000000  loss: 1.7922 (1.8148)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8112 (9.3540)  time: 0.8721 (0.5300 -- 3.0528)  data: 0.0615 (0.0005 -- 0.6313)  max mem: 16413
Epoch: [120]  [ 80/160]  eta: 0:01:17  lr: 0.000008  min_lr: 0.000000  loss: 1.4579 (1.7701)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9801 (9.0986)  time: 0.8810 (0.5283 -- 3.4804)  data: 0.0026 (0.0003 -- 0.0150)  max mem: 16413
Epoch: [120]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.9388 (1.8057)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7584 (9.1173)  time: 0.7959 (0.5199 -- 2.6947)  data: 0.0013 (0.0003 -- 0.0039)  max mem: 16413
[2023-08-30 18:13:40,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19309
[2023-08-30 18:13:40,799] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19309
[2023-08-30 18:13:40,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:13:40,799] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:13:40,799] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [120]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.7926 (1.8028)  loss_scale: 8192.0000 (15571.5702)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8528 (8.9393)  time: 0.8345 (0.5276 -- 2.9440)  data: 0.0014 (0.0001 -- 0.0028)  max mem: 16413
Epoch: [120]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.8176 (1.7931)  loss_scale: 8192.0000 (14524.8227)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1883 (8.8417)  time: 0.8331 (0.5240 -- 2.1120)  data: 0.1015 (0.0002 -- 1.5789)  max mem: 16413
Epoch: [120]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.8593 (1.7944)  loss_scale: 8192.0000 (13772.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6748 (8.7801)  time: 0.6392 (0.4969 -- 1.4946)  data: 0.0011 (0.0001 -- 0.0032)  max mem: 16413
Epoch: [120] Total time: 0:02:20 (0.8768 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.8593 (1.7730)  loss_scale: 8192.0000 (13772.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6748 (8.7801)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2878 (0.2878)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3212 (2.3212 -- 2.3212)  data: 2.0534 (2.0534 -- 2.0534)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4244 (0.7069)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4144 (0.2087 -- 2.3212)  data: 0.1903 (0.0005 -- 2.0534)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4308 (0.6493)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2210 (0.1708 -- 0.4066)  data: 0.0121 (0.0001 -- 0.1979)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5718 (0.7074)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.2656)  time: 0.2042 (0.1333 -- 0.4066)  data: 0.0116 (0.0001 -- 0.1979)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 82.158 Acc@5 97.303 loss 0.666
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [121]  [  0/160]  eta: 0:25:46  lr: 0.000008  min_lr: 0.000000  loss: 2.1503 (2.1503)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.5980 (13.5980)  time: 9.6681 (9.6681 -- 9.6681)  data: 5.4741 (5.4741 -- 5.4741)  max mem: 16413
Epoch: [121]  [ 20/160]  eta: 0:02:48  lr: 0.000008  min_lr: 0.000000  loss: 1.8993 (1.7984)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7830 (9.6262)  time: 0.7813 (0.5299 -- 4.9505)  data: 0.0016 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [121]  [ 40/160]  eta: 0:02:07  lr: 0.000008  min_lr: 0.000000  loss: 1.5486 (1.7183)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8960 (9.4891)  time: 0.9120 (0.5270 -- 3.4878)  data: 0.0276 (0.0004 -- 0.5039)  max mem: 16413
Epoch: [121]  [ 60/160]  eta: 0:01:36  lr: 0.000008  min_lr: 0.000000  loss: 1.9187 (1.7719)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2712 (9.2038)  time: 0.7674 (0.5215 -- 3.7417)  data: 0.0585 (0.0007 -- 1.1349)  max mem: 16413
[2023-08-30 18:15:40,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:15:40,793] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:15:40,793] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:15:40,793] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [121]  [ 80/160]  eta: 0:01:16  lr: 0.000008  min_lr: 0.000000  loss: 1.7367 (1.7775)  loss_scale: 8192.0000 (8495.4074)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3753 (9.1410)  time: 0.9455 (0.5236 -- 3.5554)  data: 0.2430 (0.0004 -- 2.3249)  max mem: 16413
Epoch: [121]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 1.7136 (1.7655)  loss_scale: 16384.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5923 (9.0958)  time: 0.7581 (0.5254 -- 3.9561)  data: 0.0687 (0.0003 -- 1.3431)  max mem: 16413
Epoch: [121]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.8882 (1.7788)  loss_scale: 16384.0000 (11103.2066)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6834 (9.0394)  time: 0.9046 (0.5297 -- 3.3635)  data: 0.0021 (0.0004 -- 0.0105)  max mem: 16413
Epoch: [121]  [140/160]  eta: 0:00:17  lr: 0.000008  min_lr: 0.000000  loss: 1.8265 (1.7849)  loss_scale: 16384.0000 (11852.2553)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5216 (9.1523)  time: 0.7603 (0.5227 -- 2.5042)  data: 0.0243 (0.0006 -- 0.4397)  max mem: 16413
Epoch: [121]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6359 (1.7759)  loss_scale: 16384.0000 (12390.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3409 (9.2052)  time: 0.7336 (0.4955 -- 2.7574)  data: 0.1492 (0.0002 -- 2.1893)  max mem: 16413
Epoch: [121] Total time: 0:02:20 (0.8775 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6359 (1.7792)  loss_scale: 16384.0000 (12390.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3409 (9.2052)
Val:  [ 0/27]  eta: 0:00:55  loss: 0.3004 (0.3004)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0737 (2.0737 -- 2.0737)  data: 1.7918 (1.7918 -- 1.7918)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4311 (0.7082)  acc1: 88.8889 (79.7980)  acc5: 100.0000 (95.9596)  time: 0.4064 (0.1975 -- 2.0737)  data: 0.1858 (0.0007 -- 1.7918)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4453 (0.6560)  acc1: 88.8889 (83.0688)  acc5: 100.0000 (96.2963)  time: 0.2331 (0.1699 -- 0.4495)  data: 0.0261 (0.0001 -- 0.2378)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5827 (0.7158)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2136 (0.1332 -- 0.4495)  data: 0.0222 (0.0001 -- 0.2378)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 82.158 Acc@5 97.095 loss 0.671
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [122]  [  0/160]  eta: 0:21:51  lr: 0.000008  min_lr: 0.000000  loss: 1.9414 (1.9414)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4601 (8.4601)  time: 8.1986 (8.1986 -- 8.1986)  data: 7.0063 (7.0063 -- 7.0063)  max mem: 16413
Epoch: [122]  [ 20/160]  eta: 0:02:34  lr: 0.000008  min_lr: 0.000000  loss: 1.7075 (1.7114)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6706 (8.9083)  time: 0.7496 (0.5245 -- 2.7910)  data: 0.1299 (0.0003 -- 2.2160)  max mem: 16413
Epoch: [122]  [ 40/160]  eta: 0:01:58  lr: 0.000008  min_lr: 0.000000  loss: 1.8638 (1.7947)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0361 (8.8744)  time: 0.8662 (0.5326 -- 3.2527)  data: 0.3132 (0.0004 -- 2.7299)  max mem: 16413
[2023-08-30 18:17:40,253] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19564
[2023-08-30 18:17:40,253] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19564
[2023-08-30 18:17:40,254] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:17:40,254] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:17:40,254] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [122]  [ 60/160]  eta: 0:01:39  lr: 0.000008  min_lr: 0.000000  loss: 1.8677 (1.8302)  loss_scale: 8192.0000 (14100.9836)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2638 (8.8963)  time: 1.0058 (0.5126 -- 3.2897)  data: 0.4652 (0.0003 -- 2.7391)  max mem: 16413
Epoch: [122]  [ 80/160]  eta: 0:01:15  lr: 0.000008  min_lr: 0.000000  loss: 1.7375 (1.8086)  loss_scale: 8192.0000 (12641.9753)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9722 (9.0103)  time: 0.7962 (0.5266 -- 3.3346)  data: 0.2482 (0.0001 -- 2.8063)  max mem: 16413
Epoch: [122]  [100/160]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000000  loss: 1.7525 (1.8048)  loss_scale: 8192.0000 (11760.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6568 (8.7785)  time: 0.9136 (0.5080 -- 3.9578)  data: 0.3766 (0.0003 -- 3.4399)  max mem: 16413
Epoch: [122]  [120/160]  eta: 0:00:35  lr: 0.000008  min_lr: 0.000000  loss: 1.8165 (1.8008)  loss_scale: 8192.0000 (11170.9091)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2244 (8.8327)  time: 0.6326 (0.5435 -- 1.7202)  data: 0.0702 (0.0002 -- 1.1526)  max mem: 16413
Epoch: [122]  [140/160]  eta: 0:00:17  lr: 0.000008  min_lr: 0.000000  loss: 1.7461 (1.7784)  loss_scale: 8192.0000 (10748.3688)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0413 (8.8229)  time: 0.9295 (0.5189 -- 2.7163)  data: 0.2476 (0.0006 -- 2.1798)  max mem: 16413
Epoch: [122]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.8049 (1.7737)  loss_scale: 8192.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6390 (8.9077)  time: 0.7845 (0.4966 -- 2.0694)  data: 0.1441 (0.0002 -- 1.5094)  max mem: 16413
Epoch: [122] Total time: 0:02:20 (0.8809 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.8049 (1.7633)  loss_scale: 8192.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6390 (8.9077)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.2952 (0.2952)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1321 (2.1321 -- 2.1321)  data: 1.9249 (1.9249 -- 1.9249)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4126 (0.7134)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.3940 (0.1949 -- 2.1321)  data: 0.1768 (0.0005 -- 1.9249)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4462 (0.6596)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2254 (0.1760 -- 0.4976)  data: 0.0151 (0.0001 -- 0.2774)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5732 (0.7160)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2074 (0.1332 -- 0.4976)  data: 0.0146 (0.0001 -- 0.2774)  max mem: 16413
Val: Total time: 0:00:07 (0.2828 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.667
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [123]  [  0/160]  eta: 0:17:42  lr: 0.000008  min_lr: 0.000000  loss: 1.4474 (1.4474)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6235 (12.6235)  time: 6.6413 (6.6413 -- 6.6413)  data: 6.0631 (6.0631 -- 6.0631)  max mem: 16413
[2023-08-30 18:19:41,149] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:19:41,149] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:19:41,149] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:19:41,149] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [123]  [ 20/160]  eta: 0:02:37  lr: 0.000008  min_lr: 0.000000  loss: 1.9117 (1.8433)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7956 (9.8329)  time: 0.8485 (0.5321 -- 2.2164)  data: 0.0720 (0.0007 -- 0.6913)  max mem: 16413
Epoch: [123]  [ 40/160]  eta: 0:01:58  lr: 0.000008  min_lr: 0.000000  loss: 1.9092 (1.8482)  loss_scale: 16384.0000 (13786.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4442 (9.3361)  time: 0.8368 (0.5194 -- 3.6025)  data: 0.1865 (0.0004 -- 1.1596)  max mem: 16413
Epoch: [123]  [ 60/160]  eta: 0:01:38  lr: 0.000008  min_lr: 0.000000  loss: 2.0420 (1.8794)  loss_scale: 16384.0000 (14638.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3268 (9.4438)  time: 0.9888 (0.5206 -- 3.9404)  data: 0.4461 (0.0003 -- 3.4152)  max mem: 16413
Epoch: [123]  [ 80/160]  eta: 0:01:12  lr: 0.000008  min_lr: 0.000000  loss: 1.8994 (1.8457)  loss_scale: 16384.0000 (15069.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1134 (9.2659)  time: 0.6833 (0.5234 -- 1.8160)  data: 0.1201 (0.0004 -- 1.2545)  max mem: 16413
Epoch: [123]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 1.8853 (1.8457)  loss_scale: 16384.0000 (15329.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7669 (8.9857)  time: 0.9621 (0.5341 -- 2.9948)  data: 0.3132 (0.0004 -- 2.4339)  max mem: 16413
Epoch: [123]  [120/160]  eta: 0:00:35  lr: 0.000008  min_lr: 0.000000  loss: 2.0473 (1.8451)  loss_scale: 16384.0000 (15503.8678)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5062 (8.9863)  time: 0.7186 (0.5319 -- 2.1146)  data: 0.1670 (0.0004 -- 1.5832)  max mem: 16413
Epoch: [123]  [140/160]  eta: 0:00:17  lr: 0.000008  min_lr: 0.000000  loss: 1.6807 (1.8222)  loss_scale: 16384.0000 (15628.7092)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0128 (8.9296)  time: 0.9193 (0.5323 -- 1.7603)  data: 0.3245 (0.0009 -- 1.2222)  max mem: 16413
[2023-08-30 18:21:29,954] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:21:29,954] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:21:29,955] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:21:29,955] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [123]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6289 (1.8028)  loss_scale: 32768.0000 (17664.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3726 (8.9213)  time: 0.6812 (0.4963 -- 1.5330)  data: 0.1410 (0.0002 -- 0.9642)  max mem: 16413
Epoch: [123] Total time: 0:02:18 (0.8682 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6289 (1.7875)  loss_scale: 32768.0000 (17664.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3726 (8.9213)
Val:  [ 0/27]  eta: 0:01:07  loss: 0.2825 (0.2825)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4956 (2.4956 -- 2.4956)  data: 2.2668 (2.2668 -- 2.2668)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4139 (0.7216)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4292 (0.2031 -- 2.4956)  data: 0.2071 (0.0006 -- 2.2668)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4313 (0.6605)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2153 (0.1698 -- 0.3198)  data: 0.0069 (0.0001 -- 0.1251)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5770 (0.7170)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.1964 (0.1347 -- 0.3198)  data: 0.0067 (0.0001 -- 0.1251)  max mem: 16413
Val: Total time: 0:00:07 (0.2885 s / it)
* Acc@1 81.950 Acc@5 97.303 loss 0.668
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.99%
Epoch: [124]  [  0/160]  eta: 0:20:20  lr: 0.000008  min_lr: 0.000000  loss: 2.3221 (2.3221)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1285 (8.1285)  time: 7.6261 (7.6261 -- 7.6261)  data: 7.0491 (7.0491 -- 7.0491)  max mem: 16413
Epoch: [124]  [ 20/160]  eta: 0:02:57  lr: 0.000008  min_lr: 0.000000  loss: 1.7757 (1.7226)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5404 (8.6552)  time: 0.9527 (0.5326 -- 4.7332)  data: 0.4067 (0.0004 -- 4.2039)  max mem: 16413
[2023-08-30 18:22:23,041] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19865
[2023-08-30 18:22:23,041] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:22:23,041] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 18:22:23,041] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 19865
[2023-08-30 18:22:23,041] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [124]  [ 40/160]  eta: 0:02:15  lr: 0.000008  min_lr: 0.000000  loss: 1.8883 (1.7698)  loss_scale: 16384.0000 (26374.2439)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9975 (8.7049)  time: 0.9758 (0.5272 -- 4.1720)  data: 0.3465 (0.0002 -- 3.6556)  max mem: 16413
Epoch: [124]  [ 60/160]  eta: 0:01:39  lr: 0.000008  min_lr: 0.000000  loss: 1.8026 (1.7693)  loss_scale: 16384.0000 (23098.7541)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9728 (8.6498)  time: 0.7246 (0.5174 -- 2.9941)  data: 0.1766 (0.0002 -- 2.4428)  max mem: 16413
Epoch: [124]  [ 80/160]  eta: 0:01:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7730 (1.7753)  loss_scale: 16384.0000 (21440.7901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9648 (8.5755)  time: 0.9379 (0.5130 -- 3.7786)  data: 0.3986 (0.0004 -- 3.2401)  max mem: 16413
Epoch: [124]  [100/160]  eta: 0:00:55  lr: 0.000008  min_lr: 0.000000  loss: 1.8800 (1.7935)  loss_scale: 16384.0000 (20439.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0866 (8.6182)  time: 0.6986 (0.5247 -- 3.0591)  data: 0.1499 (0.0004 -- 2.5228)  max mem: 16413
Epoch: [124]  [120/160]  eta: 0:00:36  lr: 0.000008  min_lr: 0.000000  loss: 1.6837 (1.7883)  loss_scale: 16384.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6892 (8.6691)  time: 0.8816 (0.5156 -- 3.7866)  data: 0.2045 (0.0003 -- 3.2582)  max mem: 16413
Epoch: [124]  [140/160]  eta: 0:00:18  lr: 0.000008  min_lr: 0.000000  loss: 1.7203 (1.7840)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8628 (8.7466)  time: 0.8813 (0.5263 -- 1.8739)  data: 0.1114 (0.0003 -- 1.3127)  max mem: 16413
[2023-08-30 18:24:09,903] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:24:09,903] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:24:09,903] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:24:09,903] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:24:12,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=116, lr=[1.8046192724572118e-07, 1.8046192724572118e-07, 2.406159029942949e-07, 2.406159029942949e-07, 3.2082120399239323e-07, 3.2082120399239323e-07, 4.2776160532319094e-07, 4.2776160532319094e-07, 5.703488070975879e-07, 5.703488070975879e-07, 7.604650761301172e-07, 7.604650761301172e-07, 1.0139534348401563e-06, 1.0139534348401563e-06, 1.3519379131202085e-06, 1.3519379131202085e-06, 1.8025838841602778e-06, 1.8025838841602778e-06, 2.4034451788803704e-06, 2.4034451788803704e-06, 3.204593571840494e-06, 3.204593571840494e-06, 4.272791429120658e-06, 4.272791429120658e-06, 5.697055238827545e-06, 5.697055238827545e-06, 7.59607365177006e-06, 7.59607365177006e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 18:24:12,403] [INFO] [timer.py:260:stop] epoch=0/micro_step=20000/global_step=20000, RunningAvgSamplesPerSec=17.691546415267705, CurrSamplesPerSec=24.69788551214416, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [124]  [159/160]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000000  loss: 1.6689 (1.7709)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7222 (8.6879)  time: 0.6954 (0.4944 -- 2.4013)  data: 0.0960 (0.0002 -- 1.8740)  max mem: 16413
Epoch: [124] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000000  loss: 1.6689 (1.7501)  loss_scale: 16384.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7222 (8.6879)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2919 (0.2919)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2097 (2.2097 -- 2.2097)  data: 2.0059 (2.0059 -- 2.0059)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4553 (0.7088)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4080 (0.1941 -- 2.2097)  data: 0.1966 (0.0007 -- 2.0059)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4583 (0.6569)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2236 (0.1691 -- 0.3837)  data: 0.0178 (0.0001 -- 0.1961)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5762 (0.7155)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2088 (0.1335 -- 0.3837)  data: 0.0175 (0.0001 -- 0.1961)  max mem: 16413
Val: Total time: 0:00:07 (0.2839 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.670
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [125]  [  0/160]  eta: 0:18:35  lr: 0.000008  min_lr: 0.000000  loss: 1.5631 (1.5631)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5321 (4.5321)  time: 6.9695 (6.9695 -- 6.9695)  data: 6.4403 (6.4403 -- 6.4403)  max mem: 16413
[2023-08-30 18:24:42,608] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20018
[2023-08-30 18:24:42,608] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:24:42,608] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 18:24:42,608] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20018
[2023-08-30 18:24:42,608] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [125]  [ 20/160]  eta: 0:02:37  lr: 0.000008  min_lr: 0.000000  loss: 1.8529 (1.8078)  loss_scale: 32768.0000 (30427.4286)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9531 (8.0330)  time: 0.8342 (0.5134 -- 3.6330)  data: 0.2740 (0.0003 -- 3.0971)  max mem: 16413
Epoch: [125]  [ 40/160]  eta: 0:02:07  lr: 0.000008  min_lr: 0.000000  loss: 1.6595 (1.7347)  loss_scale: 16384.0000 (23576.9756)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8918 (8.0151)  time: 1.0027 (0.5384 -- 3.9641)  data: 0.2055 (0.0003 -- 2.2616)  max mem: 16413
Epoch: [125]  [ 60/160]  eta: 0:01:36  lr: 0.000008  min_lr: 0.000000  loss: 1.7092 (1.7537)  loss_scale: 16384.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5929 (8.4381)  time: 0.7678 (0.5257 -- 3.4202)  data: 0.0020 (0.0004 -- 0.0095)  max mem: 16413
[2023-08-30 18:25:33,663] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20076
[2023-08-30 18:25:33,663] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20076
[2023-08-30 18:25:33,663] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:25:33,663] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:25:33,663] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [125]  [ 80/160]  eta: 0:01:15  lr: 0.000008  min_lr: 0.000000  loss: 1.8241 (1.7809)  loss_scale: 16384.0000 (19519.2099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9032 (8.4454)  time: 0.8903 (0.5153 -- 3.7225)  data: 0.0888 (0.0003 -- 1.2092)  max mem: 16413
Epoch: [125]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.7801 (1.7841)  loss_scale: 8192.0000 (17276.1980)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2946 (8.4784)  time: 0.9296 (0.5241 -- 4.4401)  data: 0.0021 (0.0004 -- 0.0131)  max mem: 16413
Epoch: [125]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.6489 (1.7736)  loss_scale: 8192.0000 (15774.6777)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7201 (8.5307)  time: 0.9710 (0.5238 -- 4.5056)  data: 0.0012 (0.0004 -- 0.0051)  max mem: 16413
Epoch: [125]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.9300 (1.7836)  loss_scale: 8192.0000 (14699.1206)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6805 (8.6202)  time: 0.6954 (0.5139 -- 2.4418)  data: 0.0013 (0.0004 -- 0.0032)  max mem: 16413
Epoch: [125]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7898 (1.7817)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5915 (8.6164)  time: 0.6819 (0.4949 -- 3.7258)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [125] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7898 (1.7845)  loss_scale: 8192.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5915 (8.6164)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2855 (0.2855)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4227 (2.4227 -- 2.4227)  data: 2.1503 (2.1503 -- 2.1503)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4389 (0.7214)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4251 (0.1969 -- 2.4227)  data: 0.1980 (0.0005 -- 2.1503)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4543 (0.6657)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2154 (0.1689 -- 0.2829)  data: 0.0064 (0.0001 -- 0.0971)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5956 (0.7245)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.1974 (0.1332 -- 0.2829)  data: 0.0059 (0.0001 -- 0.0971)  max mem: 16413
Val: Total time: 0:00:07 (0.2877 s / it)
* Acc@1 81.950 Acc@5 97.095 loss 0.675
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.99%
Epoch: [126]  [  0/160]  eta: 0:21:38  lr: 0.000007  min_lr: 0.000000  loss: 1.4665 (1.4665)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5396 (6.5396)  time: 8.1171 (8.1171 -- 8.1171)  data: 7.5747 (7.5747 -- 7.5747)  max mem: 16413
Epoch: [126]  [ 20/160]  eta: 0:02:38  lr: 0.000007  min_lr: 0.000000  loss: 1.7187 (1.7130)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9177 (8.3473)  time: 0.7831 (0.5227 -- 3.8073)  data: 0.2356 (0.0006 -- 3.2846)  max mem: 16413
Epoch: [126]  [ 40/160]  eta: 0:02:01  lr: 0.000007  min_lr: 0.000000  loss: 1.8189 (1.7457)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4017 (8.5190)  time: 0.8847 (0.5297 -- 2.8999)  data: 0.3338 (0.0003 -- 2.3703)  max mem: 16413
[2023-08-30 18:27:36,718] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:27:36,718] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:27:36,719] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:27:36,719] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [126]  [ 60/160]  eta: 0:01:37  lr: 0.000007  min_lr: 0.000000  loss: 1.6899 (1.7416)  loss_scale: 16384.0000 (10340.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1174 (8.6805)  time: 0.9117 (0.5261 -- 3.3391)  data: 0.3482 (0.0003 -- 2.7967)  max mem: 16413
[2023-08-30 18:28:03,448] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20239
[2023-08-30 18:28:03,448] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20239
[2023-08-30 18:28:03,448] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:28:03,448] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:28:03,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [126]  [ 80/160]  eta: 0:01:13  lr: 0.000007  min_lr: 0.000000  loss: 1.8494 (1.7675)  loss_scale: 16384.0000 (11630.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9925 (8.6986)  time: 0.7522 (0.5337 -- 2.1957)  data: 0.1575 (0.0005 -- 1.6524)  max mem: 16413
Epoch: [126]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.8274 (1.7663)  loss_scale: 8192.0000 (10949.7030)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2294 (8.6629)  time: 1.0039 (0.5386 -- 3.5823)  data: 0.1698 (0.0004 -- 2.2470)  max mem: 16413
Epoch: [126]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.8774 (1.7942)  loss_scale: 8192.0000 (10493.8843)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1351 (8.6279)  time: 0.7877 (0.5209 -- 2.8390)  data: 0.1690 (0.0002 -- 2.3116)  max mem: 16413
Epoch: [126]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.9401 (1.8006)  loss_scale: 8192.0000 (10167.3759)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9895 (8.7444)  time: 0.8824 (0.5288 -- 4.4415)  data: 0.0380 (0.0003 -- 0.7327)  max mem: 16413
Epoch: [126]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.6994 (1.7937)  loss_scale: 8192.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2373 (8.7135)  time: 0.6808 (0.4992 -- 1.8361)  data: 0.0009 (0.0002 -- 0.0050)  max mem: 16413
Epoch: [126] Total time: 0:02:21 (0.8836 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.6994 (1.7826)  loss_scale: 8192.0000 (9932.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2373 (8.7135)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2884 (0.2884)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2916 (2.2916 -- 2.2916)  data: 2.0712 (2.0712 -- 2.0712)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4169 (0.7206)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4196 (0.1973 -- 2.2916)  data: 0.2061 (0.0004 -- 2.0712)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4370 (0.6633)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2168 (0.1732 -- 0.4117)  data: 0.0100 (0.0002 -- 0.1875)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5829 (0.7200)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2029 (0.1332 -- 0.4117)  data: 0.0097 (0.0001 -- 0.1875)  max mem: 16413
Val: Total time: 0:00:07 (0.2820 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.668
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [127]  [  0/160]  eta: 0:17:28  lr: 0.000007  min_lr: 0.000000  loss: 2.0142 (2.0142)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2114 (8.2114)  time: 6.5523 (6.5523 -- 6.5523)  data: 6.0094 (6.0094 -- 6.0094)  max mem: 16413
Epoch: [127]  [ 20/160]  eta: 0:02:46  lr: 0.000007  min_lr: 0.000000  loss: 1.8654 (1.8112)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7619 (9.0464)  time: 0.9203 (0.5196 -- 2.8029)  data: 0.1160 (0.0003 -- 1.3707)  max mem: 16413
Epoch: [127]  [ 40/160]  eta: 0:02:05  lr: 0.000007  min_lr: 0.000000  loss: 1.5483 (1.7168)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8025 (8.8545)  time: 0.8895 (0.5251 -- 3.1453)  data: 0.2265 (0.0004 -- 2.6285)  max mem: 16413
[2023-08-30 18:30:08,253] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:30:08,254] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:30:08,255] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:30:08,256] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [127]  [ 60/160]  eta: 0:01:40  lr: 0.000007  min_lr: 0.000000  loss: 1.8775 (1.7784)  loss_scale: 16384.0000 (9937.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1659 (8.7546)  time: 0.9171 (0.5103 -- 4.6336)  data: 0.1453 (0.0004 -- 1.4641)  max mem: 16413
Epoch: [127]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000000  loss: 1.7351 (1.7805)  loss_scale: 16384.0000 (11529.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7799 (8.6377)  time: 0.7789 (0.5252 -- 2.8179)  data: 0.0015 (0.0002 -- 0.0034)  max mem: 16413
Epoch: [127]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.8685 (1.7891)  loss_scale: 16384.0000 (12490.7723)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1377 (8.5075)  time: 0.8951 (0.5208 -- 2.2324)  data: 0.1705 (0.0003 -- 1.7284)  max mem: 16413
Epoch: [127]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.6898 (1.7754)  loss_scale: 16384.0000 (13134.2810)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2517 (8.5112)  time: 0.7582 (0.5218 -- 2.4076)  data: 0.1787 (0.0001 -- 1.8558)  max mem: 16413
Epoch: [127]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8181 (1.7848)  loss_scale: 16384.0000 (13595.2340)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9883 (8.5459)  time: 1.0098 (0.5237 -- 3.8881)  data: 0.1530 (0.0004 -- 1.7797)  max mem: 16413
Epoch: [127]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.9000 (1.7891)  loss_scale: 16384.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1564 (8.6094)  time: 0.6399 (0.4966 -- 1.9364)  data: 0.1180 (0.0002 -- 1.4197)  max mem: 16413
Epoch: [127] Total time: 0:02:22 (0.8889 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.9000 (1.7893)  loss_scale: 16384.0000 (13926.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1564 (8.6094)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3014 (0.3014)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2734 (2.2734 -- 2.2734)  data: 2.0558 (2.0558 -- 2.0558)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4183 (0.7187)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4232 (0.1973 -- 2.2734)  data: 0.2090 (0.0008 -- 2.0558)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4436 (0.6623)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2228 (0.1694 -- 0.5063)  data: 0.0177 (0.0001 -- 0.2324)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5812 (0.7210)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.6805)  time: 0.2096 (0.1329 -- 0.5063)  data: 0.0173 (0.0001 -- 0.2324)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 82.158 Acc@5 97.510 loss 0.671
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [128]  [  0/160]  eta: 0:18:45  lr: 0.000007  min_lr: 0.000000  loss: 2.2971 (2.2971)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8036 (7.8036)  time: 7.0313 (7.0313 -- 7.0313)  data: 6.4819 (6.4819 -- 6.4819)  max mem: 16413
[2023-08-30 18:32:10,646] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:32:10,646] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:32:10,646] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:32:10,646] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [128]  [ 20/160]  eta: 0:02:43  lr: 0.000007  min_lr: 0.000000  loss: 1.6185 (1.6884)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9650 (8.7019)  time: 0.8782 (0.5228 -- 2.9880)  data: 0.1109 (0.0002 -- 1.6426)  max mem: 16413
[2023-08-30 18:32:18,178] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20504
[2023-08-30 18:32:18,178] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:32:18,178] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20504
[2023-08-30 18:32:18,178] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:32:18,179] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 18:32:21,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20510
[2023-08-30 18:32:21,525] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20510
[2023-08-30 18:32:21,525] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:32:21,525] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:32:21,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [128]  [ 40/160]  eta: 0:02:03  lr: 0.000007  min_lr: 0.000000  loss: 1.8753 (1.7781)  loss_scale: 8192.0000 (17383.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1313 (8.7720)  time: 0.8779 (0.5343 -- 3.0420)  data: 0.0468 (0.0002 -- 0.9080)  max mem: 16413
Epoch: [128]  [ 60/160]  eta: 0:01:41  lr: 0.000007  min_lr: 0.000000  loss: 1.7164 (1.7757)  loss_scale: 8192.0000 (14369.5738)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2534 (8.7586)  time: 0.9890 (0.5130 -- 3.9563)  data: 0.0041 (0.0002 -- 0.0491)  max mem: 16413
Epoch: [128]  [ 80/160]  eta: 0:01:15  lr: 0.000007  min_lr: 0.000000  loss: 1.8135 (1.7655)  loss_scale: 8192.0000 (12844.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1548 (8.7323)  time: 0.7230 (0.5124 -- 2.8222)  data: 0.0014 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [128]  [100/160]  eta: 0:00:56  lr: 0.000007  min_lr: 0.000000  loss: 1.8482 (1.7885)  loss_scale: 8192.0000 (11923.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3567 (8.8097)  time: 0.9171 (0.5273 -- 3.3969)  data: 0.0017 (0.0002 -- 0.0061)  max mem: 16413
Epoch: [128]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.7143 (1.7912)  loss_scale: 8192.0000 (11306.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7626 (8.7437)  time: 0.7579 (0.5237 -- 2.7659)  data: 0.0014 (0.0004 -- 0.0036)  max mem: 16413
Epoch: [128]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.8412 (1.8005)  loss_scale: 8192.0000 (10864.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5153 (8.9187)  time: 0.9140 (0.5380 -- 3.1556)  data: 0.0030 (0.0002 -- 0.0166)  max mem: 16413
[2023-08-30 18:34:10,900] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:34:10,900] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:34:10,901] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:34:10,901] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [128]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.6651 (1.7948)  loss_scale: 8192.0000 (10598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5099 (8.8556)  time: 0.7207 (0.4951 -- 2.9855)  data: 0.0009 (0.0002 -- 0.0037)  max mem: 16413
Epoch: [128] Total time: 0:02:22 (0.8879 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.6651 (1.7804)  loss_scale: 8192.0000 (10598.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5099 (8.8556)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.3179 (0.3179)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2991 (2.2991 -- 2.2991)  data: 2.0919 (2.0919 -- 2.0919)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4163 (0.7201)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4189 (0.2024 -- 2.2991)  data: 0.2013 (0.0007 -- 2.0919)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4454 (0.6584)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2242 (0.1696 -- 0.4172)  data: 0.0178 (0.0001 -- 0.2297)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5961 (0.7172)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2054 (0.1329 -- 0.4172)  data: 0.0174 (0.0001 -- 0.2297)  max mem: 16413
Val: Total time: 0:00:07 (0.2878 s / it)
* Acc@1 82.158 Acc@5 97.303 loss 0.669
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [129]  [  0/160]  eta: 0:22:12  lr: 0.000007  min_lr: 0.000000  loss: 1.5245 (1.5245)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5906 (6.5906)  time: 8.3309 (8.3309 -- 8.3309)  data: 7.5647 (7.5647 -- 7.5647)  max mem: 16413
Epoch: [129]  [ 20/160]  eta: 0:02:35  lr: 0.000007  min_lr: 0.000000  loss: 1.9318 (1.8778)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9043 (8.6318)  time: 0.7497 (0.5118 -- 2.5463)  data: 0.1560 (0.0003 -- 1.9953)  max mem: 16413
[2023-08-30 18:34:48,194] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20667
[2023-08-30 18:34:48,194] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:34:48,194] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20667
[2023-08-30 18:34:48,194] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:34:48,194] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [129]  [ 40/160]  eta: 0:02:00  lr: 0.000007  min_lr: 0.000000  loss: 1.6875 (1.8262)  loss_scale: 8192.0000 (13586.7317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3215 (8.7264)  time: 0.8883 (0.5205 -- 3.8482)  data: 0.1532 (0.0003 -- 1.5055)  max mem: 16413
Epoch: [129]  [ 60/160]  eta: 0:01:36  lr: 0.000007  min_lr: 0.000000  loss: 1.9029 (1.8217)  loss_scale: 8192.0000 (11817.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5763 (8.7501)  time: 0.8976 (0.5185 -- 2.8793)  data: 0.0021 (0.0002 -- 0.0156)  max mem: 16413
Epoch: [129]  [ 80/160]  eta: 0:01:16  lr: 0.000007  min_lr: 0.000000  loss: 1.8393 (1.8121)  loss_scale: 8192.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6333 (8.5958)  time: 0.8970 (0.5249 -- 3.0343)  data: 0.1186 (0.0004 -- 1.7555)  max mem: 16413
Epoch: [129]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.7245 (1.7930)  loss_scale: 8192.0000 (10381.9406)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6330 (8.7574)  time: 0.8568 (0.5232 -- 3.9761)  data: 0.0017 (0.0003 -- 0.0103)  max mem: 16413
Epoch: [129]  [120/160]  eta: 0:00:36  lr: 0.000007  min_lr: 0.000000  loss: 1.7289 (1.7946)  loss_scale: 8192.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8194 (8.6870)  time: 0.8338 (0.5293 -- 3.8748)  data: 0.0365 (0.0002 -- 0.6986)  max mem: 16413
Epoch: [129]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.9059 (1.8060)  loss_scale: 8192.0000 (9760.6809)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2779 (8.6609)  time: 0.8618 (0.5203 -- 3.7493)  data: 0.0016 (0.0003 -- 0.0046)  max mem: 16413
[2023-08-30 18:36:37,599] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:36:37,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:36:37,599] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:36:37,599] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [129]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.7998 (1.8018)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9995 (8.7207)  time: 0.6450 (0.4971 -- 2.8406)  data: 0.0008 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [129] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.7998 (1.7968)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9995 (8.7207)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2949 (0.2949)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4158 (2.4158 -- 2.4158)  data: 2.1998 (2.1998 -- 2.1998)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4357 (0.7118)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4156 (0.1989 -- 2.4158)  data: 0.2038 (0.0005 -- 2.1998)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4654 (0.6553)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2159 (0.1701 -- 0.4152)  data: 0.0139 (0.0001 -- 0.2342)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5782 (0.7109)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.2014 (0.1333 -- 0.4152)  data: 0.0136 (0.0001 -- 0.2342)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 81.950 Acc@5 97.510 loss 0.666
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.99%
Epoch: [130]  [  0/160]  eta: 0:18:15  lr: 0.000007  min_lr: 0.000000  loss: 1.5827 (1.5827)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4706 (11.4706)  time: 6.8476 (6.8476 -- 6.8476)  data: 6.3168 (6.3168 -- 6.3168)  max mem: 16413
Epoch: [130]  [ 20/160]  eta: 0:02:38  lr: 0.000007  min_lr: 0.000000  loss: 1.9905 (1.9014)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6219 (8.0768)  time: 0.8429 (0.5247 -- 2.2588)  data: 0.1562 (0.0003 -- 1.7051)  max mem: 16413
Epoch: [130]  [ 40/160]  eta: 0:01:58  lr: 0.000007  min_lr: 0.000000  loss: 2.0005 (1.9012)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4966 (8.6500)  time: 0.8468 (0.5272 -- 2.7155)  data: 0.0486 (0.0004 -- 0.9411)  max mem: 16413
Epoch: [130]  [ 60/160]  eta: 0:01:36  lr: 0.000007  min_lr: 0.000000  loss: 1.6988 (1.7952)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4327 (8.9953)  time: 0.9191 (0.5247 -- 3.1661)  data: 0.0571 (0.0004 -- 1.1158)  max mem: 16413
[2023-08-30 18:37:59,731] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20876
[2023-08-30 18:37:59,731] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:37:59,731] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 20876
[2023-08-30 18:37:59,731] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:37:59,731] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [130]  [ 80/160]  eta: 0:01:14  lr: 0.000007  min_lr: 0.000000  loss: 1.6547 (1.7834)  loss_scale: 16384.0000 (15878.3210)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4016 (8.9690)  time: 0.7951 (0.5213 -- 4.4601)  data: 0.0301 (0.0005 -- 0.5763)  max mem: 16413
Epoch: [130]  [100/160]  eta: 0:00:55  lr: 0.000007  min_lr: 0.000000  loss: 1.8740 (1.8002)  loss_scale: 8192.0000 (14356.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5742 (8.9932)  time: 0.9626 (0.5182 -- 3.8609)  data: 0.0018 (0.0002 -- 0.0043)  max mem: 16413
Epoch: [130]  [120/160]  eta: 0:00:37  lr: 0.000007  min_lr: 0.000000  loss: 1.7783 (1.8010)  loss_scale: 8192.0000 (13337.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8377 (9.0085)  time: 0.8920 (0.5292 -- 4.5130)  data: 0.0012 (0.0003 -- 0.0032)  max mem: 16413
Epoch: [130]  [140/160]  eta: 0:00:18  lr: 0.000007  min_lr: 0.000000  loss: 1.6535 (1.7807)  loss_scale: 8192.0000 (12607.5461)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3771 (9.0129)  time: 0.8990 (0.5237 -- 3.7398)  data: 0.0011 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [130]  [159/160]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000000  loss: 1.8099 (1.7802)  loss_scale: 8192.0000 (12083.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7759 (8.9968)  time: 0.6964 (0.4965 -- 2.5014)  data: 0.0005 (0.0002 -- 0.0009)  max mem: 16413
Epoch: [130] Total time: 0:02:23 (0.8963 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000000  loss: 1.8099 (1.7858)  loss_scale: 8192.0000 (12083.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7759 (8.9968)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.3124 (0.3124)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5281 (2.5281 -- 2.5281)  data: 2.1902 (2.1902 -- 2.1902)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4146 (0.7064)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4319 (0.1974 -- 2.5281)  data: 0.2001 (0.0008 -- 2.1902)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4649 (0.6553)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2153 (0.1707 -- 0.3268)  data: 0.0071 (0.0001 -- 0.1279)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5765 (0.7112)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.1966 (0.1360 -- 0.3268)  data: 0.0068 (0.0001 -- 0.1279)  max mem: 16413
Val: Total time: 0:00:07 (0.2899 s / it)
* Acc@1 82.158 Acc@5 97.510 loss 0.668
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [131]  [  0/160]  eta: 0:19:16  lr: 0.000007  min_lr: 0.000000  loss: 1.6724 (1.6724)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5298 (8.5298)  time: 7.2254 (7.2254 -- 7.2254)  data: 4.9917 (4.9917 -- 4.9917)  max mem: 16413
Epoch: [131]  [ 20/160]  eta: 0:02:43  lr: 0.000007  min_lr: 0.000000  loss: 1.8090 (1.7857)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9892 (8.9345)  time: 0.8625 (0.5245 -- 3.0001)  data: 0.0112 (0.0002 -- 0.1672)  max mem: 16413
[2023-08-30 18:39:58,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=123, lr=[1.5484493871302982e-07, 1.5484493871302982e-07, 2.0645991828403974e-07, 2.0645991828403974e-07, 2.7527989104538634e-07, 2.7527989104538634e-07, 3.670398547271818e-07, 3.670398547271818e-07, 4.893864729695757e-07, 4.893864729695757e-07, 6.525152972927676e-07, 6.525152972927676e-07, 8.700203963903568e-07, 8.700203963903568e-07, 1.1600271951871424e-06, 1.1600271951871424e-06, 1.5467029269161898e-06, 1.5467029269161898e-06, 2.0622705692215864e-06, 2.0622705692215864e-06, 2.7496940922954488e-06, 2.7496940922954488e-06, 3.666258789727265e-06, 3.666258789727265e-06, 4.8883450529696865e-06, 4.8883450529696865e-06, 6.517793403959582e-06, 6.517793403959582e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 18:39:58,329] [INFO] [timer.py:260:stop] epoch=0/micro_step=21000/global_step=21000, RunningAvgSamplesPerSec=17.601106257698277, CurrSamplesPerSec=23.314857465790862, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [131]  [ 40/160]  eta: 0:01:59  lr: 0.000007  min_lr: 0.000000  loss: 1.9552 (1.8322)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0829 (9.2001)  time: 0.8123 (0.5277 -- 3.4822)  data: 0.0058 (0.0003 -- 0.0862)  max mem: 16413
[2023-08-30 18:40:04,964] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:40:04,964] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:40:04,965] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:40:04,965] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [131]  [ 60/160]  eta: 0:01:36  lr: 0.000006  min_lr: 0.000000  loss: 1.6876 (1.8100)  loss_scale: 16384.0000 (10340.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2807 (8.9988)  time: 0.9149 (0.5262 -- 3.8889)  data: 0.0023 (0.0003 -- 0.0162)  max mem: 16413
Epoch: [131]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 1.8814 (1.8183)  loss_scale: 16384.0000 (11832.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3174 (9.2937)  time: 0.9428 (0.5231 -- 3.4951)  data: 0.0016 (0.0007 -- 0.0031)  max mem: 16413
Epoch: [131]  [100/160]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000000  loss: 1.8245 (1.8124)  loss_scale: 16384.0000 (12734.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2448 (9.2266)  time: 0.8625 (0.5325 -- 3.7792)  data: 0.0015 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [131]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.5828 (1.7847)  loss_scale: 16384.0000 (13337.3884)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4155 (9.3099)  time: 0.7214 (0.5208 -- 2.2877)  data: 0.0016 (0.0002 -- 0.0062)  max mem: 16413
Epoch: [131]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.5655 (1.7643)  loss_scale: 16384.0000 (13769.5319)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1686 (9.2063)  time: 0.8945 (0.5164 -- 5.4054)  data: 0.0014 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [131]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6638 (1.7538)  loss_scale: 16384.0000 (14080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6745 (9.2142)  time: 0.6898 (0.4981 -- 2.4792)  data: 0.0007 (0.0002 -- 0.0013)  max mem: 16413
Epoch: [131] Total time: 0:02:20 (0.8798 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6638 (1.7632)  loss_scale: 16384.0000 (14080.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6745 (9.2142)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2995 (0.2995)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3134 (2.3134 -- 2.3134)  data: 2.0817 (2.0817 -- 2.0817)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4146 (0.7083)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (97.9798)  time: 0.4073 (0.2099 -- 2.3134)  data: 0.1902 (0.0006 -- 2.0817)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4521 (0.6548)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (97.3545)  time: 0.2196 (0.1690 -- 0.3462)  data: 0.0135 (0.0001 -- 0.1412)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5786 (0.7115)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (97.0954)  time: 0.2036 (0.1333 -- 0.3462)  data: 0.0132 (0.0001 -- 0.1412)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 82.780 Acc@5 97.718 loss 0.664
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [132]  [  0/160]  eta: 0:22:24  lr: 0.000006  min_lr: 0.000000  loss: 1.7953 (1.7953)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9198 (10.9198)  time: 8.4030 (8.4030 -- 8.4030)  data: 6.5693 (6.5693 -- 6.5693)  max mem: 16413
[2023-08-30 18:42:05,937] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:42:05,937] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:42:05,938] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:42:05,938] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [132]  [ 20/160]  eta: 0:02:53  lr: 0.000006  min_lr: 0.000000  loss: 1.8021 (1.7766)  loss_scale: 16384.0000 (22625.5238)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4596 (8.8170)  time: 0.8796 (0.5235 -- 2.7986)  data: 0.0014 (0.0002 -- 0.0041)  max mem: 16413
[2023-08-30 18:42:14,290] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21143
[2023-08-30 18:42:14,290] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21143
[2023-08-30 18:42:14,291] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:42:14,291] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:42:14,291] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [132]  [ 40/160]  eta: 0:02:02  lr: 0.000006  min_lr: 0.000000  loss: 1.8529 (1.7881)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3135 (8.6988)  time: 0.7868 (0.5354 -- 3.1007)  data: 0.0081 (0.0006 -- 0.1178)  max mem: 16413
[2023-08-30 18:42:36,881] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21169
[2023-08-30 18:42:36,882] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:42:36,881] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21169
[2023-08-30 18:42:36,882] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:42:36,882] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [132]  [ 60/160]  eta: 0:01:35  lr: 0.000006  min_lr: 0.000000  loss: 1.7009 (1.7752)  loss_scale: 8192.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0704 (8.8675)  time: 0.8164 (0.5109 -- 4.1919)  data: 0.0578 (0.0003 -- 0.7265)  max mem: 16413
Epoch: [132]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 1.8947 (1.7865)  loss_scale: 8192.0000 (15170.3704)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4355 (8.9483)  time: 0.9577 (0.5306 -- 3.5966)  data: 0.4097 (0.0003 -- 3.0702)  max mem: 16413
Epoch: [132]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7500 (1.7851)  loss_scale: 8192.0000 (13788.5149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8984 (8.8683)  time: 0.8077 (0.5210 -- 2.5129)  data: 0.2565 (0.0002 -- 1.9912)  max mem: 16413
Epoch: [132]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.7432 (1.7710)  loss_scale: 8192.0000 (12863.4711)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5176 (8.8258)  time: 0.8631 (0.5217 -- 2.6374)  data: 0.3218 (0.0004 -- 2.0938)  max mem: 16413
Epoch: [132]  [140/160]  eta: 0:00:17  lr: 0.000006  min_lr: 0.000000  loss: 1.8909 (1.7879)  loss_scale: 8192.0000 (12200.8511)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9385 (8.8503)  time: 0.8039 (0.5258 -- 2.8153)  data: 0.2574 (0.0004 -- 2.2551)  max mem: 16413
Epoch: [132]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7999 (1.7908)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6235 (8.9357)  time: 0.7074 (0.4964 -- 1.9823)  data: 0.1866 (0.0003 -- 1.4554)  max mem: 16413
Epoch: [132] Total time: 0:02:20 (0.8770 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7999 (1.7881)  loss_scale: 8192.0000 (11724.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6235 (8.9357)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.3092 (0.3092)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3361 (2.3361 -- 2.3361)  data: 2.0956 (2.0956 -- 2.0956)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4116 (0.7118)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4231 (0.2049 -- 2.3361)  data: 0.1929 (0.0007 -- 2.0956)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4334 (0.6564)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2203 (0.1709 -- 0.2704)  data: 0.0032 (0.0001 -- 0.0323)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5812 (0.7140)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2007 (0.1330 -- 0.2704)  data: 0.0028 (0.0001 -- 0.0323)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 82.365 Acc@5 97.303 loss 0.666
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [133]  [  0/160]  eta: 0:18:09  lr: 0.000006  min_lr: 0.000000  loss: 1.9822 (1.9822)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6819 (8.6819)  time: 6.8085 (6.8085 -- 6.8085)  data: 5.0391 (5.0391 -- 5.0391)  max mem: 16413
[2023-08-30 18:44:37,253] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:44:37,253] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:44:37,253] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:44:37,254] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [133]  [ 20/160]  eta: 0:02:41  lr: 0.000006  min_lr: 0.000000  loss: 1.5806 (1.6251)  loss_scale: 8192.0000 (9362.2857)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2125 (7.3870)  time: 0.8710 (0.5295 -- 2.8626)  data: 0.0613 (0.0003 -- 1.0772)  max mem: 16413
Epoch: [133]  [ 40/160]  eta: 0:02:05  lr: 0.000006  min_lr: 0.000000  loss: 1.5695 (1.6203)  loss_scale: 16384.0000 (12787.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4663 (7.6636)  time: 0.9343 (0.5245 -- 4.5985)  data: 0.0460 (0.0003 -- 0.9000)  max mem: 16413
Epoch: [133]  [ 60/160]  eta: 0:01:38  lr: 0.000006  min_lr: 0.000000  loss: 1.9075 (1.7353)  loss_scale: 16384.0000 (13966.6885)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6988 (7.9959)  time: 0.8565 (0.5219 -- 4.5288)  data: 0.0014 (0.0003 -- 0.0041)  max mem: 16413
Epoch: [133]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000000  loss: 1.9111 (1.7914)  loss_scale: 16384.0000 (14563.5556)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8962 (8.2235)  time: 0.7936 (0.5327 -- 3.4609)  data: 0.0020 (0.0002 -- 0.0086)  max mem: 16413
Epoch: [133]  [100/160]  eta: 0:00:57  lr: 0.000006  min_lr: 0.000000  loss: 1.8949 (1.7933)  loss_scale: 16384.0000 (14924.0396)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0366 (8.2172)  time: 1.0743 (0.5049 -- 4.8167)  data: 0.1524 (0.0003 -- 1.8347)  max mem: 16413
Epoch: [133]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.8746 (1.8018)  loss_scale: 16384.0000 (15165.3554)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0336 (8.3259)  time: 0.7272 (0.5193 -- 2.8574)  data: 0.0020 (0.0003 -- 0.0117)  max mem: 16413
Epoch: [133]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.6145 (1.7848)  loss_scale: 16384.0000 (15338.2128)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7498 (8.2964)  time: 0.8013 (0.5164 -- 2.8972)  data: 0.0018 (0.0003 -- 0.0072)  max mem: 16413
[2023-08-30 18:46:28,672] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:46:28,672] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:46:28,675] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:46:28,675] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [133]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.7477 (1.7829)  loss_scale: 32768.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3255 (8.2516)  time: 0.7378 (0.4984 -- 2.5616)  data: 0.0007 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [133] Total time: 0:02:22 (0.8892 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.7477 (1.7899)  loss_scale: 32768.0000 (16896.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3255 (8.2516)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.3030 (0.3030)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2556 (2.2556 -- 2.2556)  data: 2.0327 (2.0327 -- 2.0327)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4262 (0.7147)  acc1: 88.8889 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4113 (0.1894 -- 2.2556)  data: 0.2004 (0.0007 -- 2.0327)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4311 (0.6557)  acc1: 88.8889 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2228 (0.1700 -- 0.3697)  data: 0.0216 (0.0001 -- 0.1586)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5670 (0.7143)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.6805)  time: 0.2093 (0.1334 -- 0.3697)  data: 0.0212 (0.0001 -- 0.1586)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 81.535 Acc@5 97.510 loss 0.666
Accuracy of the network on the 482 val images: 81.54%
Max accuracy: 82.99%
Epoch: [134]  [  0/160]  eta: 0:19:20  lr: 0.000006  min_lr: 0.000000  loss: 2.5712 (2.5712)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6265 (8.6265)  time: 7.2535 (7.2535 -- 7.2535)  data: 6.7377 (6.7377 -- 6.7377)  max mem: 16413
[2023-08-30 18:46:54,957] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21445
[2023-08-30 18:46:54,957] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21445
[2023-08-30 18:46:54,957] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:46:54,957] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:46:54,957] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [ 20/160]  eta: 0:02:46  lr: 0.000006  min_lr: 0.000000  loss: 1.7671 (1.8041)  loss_scale: 16384.0000 (20284.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8544 (9.1096)  time: 0.8827 (0.5091 -- 4.5036)  data: 0.3330 (0.0005 -- 3.9778)  max mem: 16413
Epoch: [134]  [ 40/160]  eta: 0:02:03  lr: 0.000006  min_lr: 0.000000  loss: 1.7561 (1.7811)  loss_scale: 16384.0000 (18382.0488)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8126 (9.1163)  time: 0.8664 (0.5334 -- 2.4622)  data: 0.3157 (0.0003 -- 1.8900)  max mem: 16413
Epoch: [134]  [ 60/160]  eta: 0:01:39  lr: 0.000006  min_lr: 0.000000  loss: 1.7356 (1.7799)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0096 (9.0023)  time: 0.9233 (0.5284 -- 3.4001)  data: 0.3724 (0.0005 -- 2.8548)  max mem: 16413
Epoch: [134]  [ 80/160]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000000  loss: 1.7759 (1.7442)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4598 (8.8362)  time: 0.8449 (0.5312 -- 3.1750)  data: 0.2965 (0.0007 -- 2.6695)  max mem: 16413
Epoch: [134]  [100/160]  eta: 0:00:57  lr: 0.000006  min_lr: 0.000000  loss: 1.7234 (1.7401)  loss_scale: 16384.0000 (17195.0891)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1964 (9.0276)  time: 0.9373 (0.5248 -- 3.3956)  data: 0.3931 (0.0003 -- 2.8552)  max mem: 16413
Epoch: [134]  [120/160]  eta: 0:00:37  lr: 0.000006  min_lr: 0.000000  loss: 1.5952 (1.7111)  loss_scale: 16384.0000 (17061.0248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3417 (8.9388)  time: 0.8435 (0.5234 -- 3.7488)  data: 0.2979 (0.0006 -- 3.2329)  max mem: 16413
[2023-08-30 18:48:48,350] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:48:48,350] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:48:48,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:48:48,350] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [134]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.8903 (1.7288)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6493 (8.9300)  time: 0.8135 (0.5310 -- 3.2471)  data: 0.2649 (0.0002 -- 2.7222)  max mem: 16413
[2023-08-30 18:49:05,616] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21597
[2023-08-30 18:49:05,616] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21597
[2023-08-30 18:49:05,616] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:49:05,616] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:49:05,616] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [134]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.6055 (1.7254)  loss_scale: 32768.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2650 (8.8396)  time: 0.6480 (0.4830 -- 2.3739)  data: 0.1243 (0.0002 -- 1.8364)  max mem: 16413
Epoch: [134] Total time: 0:02:21 (0.8870 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.6055 (1.7365)  loss_scale: 32768.0000 (19251.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2650 (8.8396)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2974 (0.2974)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4246 (2.4246 -- 2.4246)  data: 2.2098 (2.2098 -- 2.2098)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4212 (0.7101)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4344 (0.1916 -- 2.4246)  data: 0.2219 (0.0005 -- 2.2098)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4499 (0.6511)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2165 (0.1687 -- 0.4427)  data: 0.0124 (0.0001 -- 0.2218)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5683 (0.7070)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2040 (0.1325 -- 0.4427)  data: 0.0122 (0.0001 -- 0.2218)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 82.365 Acc@5 97.510 loss 0.661
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [135]  [  0/160]  eta: 0:22:22  lr: 0.000006  min_lr: 0.000000  loss: 1.5090 (1.5090)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8160 (7.8160)  time: 8.3923 (8.3923 -- 8.3923)  data: 7.8816 (7.8816 -- 7.8816)  max mem: 16413
Epoch: [135]  [ 20/160]  eta: 0:02:44  lr: 0.000006  min_lr: 0.000000  loss: 1.8435 (1.7518)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6590 (9.5472)  time: 0.8164 (0.5163 -- 3.4828)  data: 0.2700 (0.0005 -- 2.9651)  max mem: 16413
Epoch: [135]  [ 40/160]  eta: 0:02:01  lr: 0.000006  min_lr: 0.000000  loss: 1.7648 (1.7702)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8866 (9.5370)  time: 0.8457 (0.5250 -- 3.7856)  data: 0.2927 (0.0003 -- 3.2582)  max mem: 16413
Epoch: [135]  [ 60/160]  eta: 0:01:34  lr: 0.000006  min_lr: 0.000000  loss: 1.6261 (1.7387)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7042 (9.4197)  time: 0.7963 (0.5229 -- 3.2502)  data: 0.1457 (0.0003 -- 2.1012)  max mem: 16413
Epoch: [135]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000000  loss: 1.8451 (1.7564)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5536 (9.7153)  time: 0.8698 (0.5219 -- 3.2799)  data: 0.1921 (0.0004 -- 1.4095)  max mem: 16413
[2023-08-30 18:50:41,924] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21694
[2023-08-30 18:50:41,924] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21694
[2023-08-30 18:50:41,924] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:50:41,924] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:50:41,924] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [135]  [100/160]  eta: 0:00:54  lr: 0.000006  min_lr: 0.000000  loss: 1.9647 (1.7760)  loss_scale: 16384.0000 (15816.2376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6694 (9.3602)  time: 0.8643 (0.5258 -- 3.4087)  data: 0.2776 (0.0003 -- 2.8746)  max mem: 16413
Epoch: [135]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.8429 (1.7881)  loss_scale: 8192.0000 (14556.0331)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3312 (9.2371)  time: 0.9100 (0.5173 -- 3.8178)  data: 0.3695 (0.0002 -- 3.2830)  max mem: 16413
Epoch: [135]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.5973 (1.7636)  loss_scale: 8192.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3373 (9.1617)  time: 0.8270 (0.5286 -- 4.2809)  data: 0.2732 (0.0003 -- 3.7606)  max mem: 16413
Epoch: [135]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8157 (1.7786)  loss_scale: 8192.0000 (13004.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3180 (9.0799)  time: 0.7072 (0.4954 -- 4.1029)  data: 0.1941 (0.0002 -- 3.5829)  max mem: 16413
Epoch: [135] Total time: 0:02:20 (0.8787 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8157 (1.7782)  loss_scale: 8192.0000 (13004.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3180 (9.0799)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2943 (0.2943)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5536 (2.5536 -- 2.5536)  data: 2.3311 (2.3311 -- 2.3311)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4290 (0.7130)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4405 (0.2031 -- 2.5536)  data: 0.2248 (0.0008 -- 2.3311)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4493 (0.6516)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2136 (0.1695 -- 0.3521)  data: 0.0092 (0.0001 -- 0.1200)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5607 (0.7113)  acc1: 77.7778 (78.4232)  acc5: 100.0000 (96.6805)  time: 0.1975 (0.1336 -- 0.3521)  data: 0.0083 (0.0001 -- 0.1200)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 81.950 Acc@5 97.510 loss 0.663
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.99%
Epoch: [136]  [  0/160]  eta: 0:18:19  lr: 0.000006  min_lr: 0.000000  loss: 2.1431 (2.1431)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8081 (8.8081)  time: 6.8735 (6.8735 -- 6.8735)  data: 6.3331 (6.3331 -- 6.3331)  max mem: 16413
Epoch: [136]  [ 20/160]  eta: 0:02:38  lr: 0.000006  min_lr: 0.000000  loss: 1.7301 (1.7115)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5181 (8.7363)  time: 0.8447 (0.5258 -- 3.0769)  data: 0.0467 (0.0003 -- 0.8930)  max mem: 16413
Epoch: [136]  [ 40/160]  eta: 0:01:58  lr: 0.000006  min_lr: 0.000000  loss: 1.9101 (1.7983)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5249 (8.6516)  time: 0.8289 (0.5346 -- 3.1473)  data: 0.0015 (0.0002 -- 0.0039)  max mem: 16413
Epoch: [136]  [ 60/160]  eta: 0:01:33  lr: 0.000006  min_lr: 0.000000  loss: 1.9665 (1.8586)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4495 (8.5247)  time: 0.8226 (0.5290 -- 2.5684)  data: 0.0019 (0.0004 -- 0.0099)  max mem: 16413
[2023-08-30 18:52:42,696] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:52:42,696] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:52:42,696] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:52:42,696] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:52:47,530] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21829
[2023-08-30 18:52:47,530] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 21829
[2023-08-30 18:52:47,531] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:52:47,531] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:52:47,531] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [136]  [ 80/160]  eta: 0:01:14  lr: 0.000006  min_lr: 0.000000  loss: 1.7129 (1.8154)  loss_scale: 8192.0000 (8798.8148)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5325 (8.6115)  time: 0.9391 (0.5286 -- 3.0233)  data: 0.2621 (0.0001 -- 2.4819)  max mem: 16413
Epoch: [136]  [100/160]  eta: 0:00:55  lr: 0.000006  min_lr: 0.000000  loss: 1.7196 (1.8030)  loss_scale: 8192.0000 (8678.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9361 (8.8161)  time: 0.8888 (0.5210 -- 3.1068)  data: 0.2099 (0.0002 -- 2.5258)  max mem: 16413
Epoch: [136]  [120/160]  eta: 0:00:36  lr: 0.000006  min_lr: 0.000000  loss: 1.8326 (1.8100)  loss_scale: 8192.0000 (8598.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7333 (8.8589)  time: 0.8473 (0.5214 -- 3.0809)  data: 0.2181 (0.0006 -- 2.5549)  max mem: 16413
Epoch: [136]  [140/160]  eta: 0:00:18  lr: 0.000006  min_lr: 0.000000  loss: 1.7344 (1.8010)  loss_scale: 8192.0000 (8540.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1614 (8.8554)  time: 0.8609 (0.5368 -- 4.1506)  data: 0.2767 (0.0003 -- 3.6103)  max mem: 16413
Epoch: [136]  [159/160]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000000  loss: 1.8820 (1.8011)  loss_scale: 8192.0000 (8499.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3363 (8.8882)  time: 0.6891 (0.4972 -- 2.5603)  data: 0.1672 (0.0002 -- 2.0368)  max mem: 16413
Epoch: [136] Total time: 0:02:20 (0.8799 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000000  loss: 1.8820 (1.7921)  loss_scale: 8192.0000 (8499.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3363 (8.8882)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2997 (0.2997)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3499 (2.3499 -- 2.3499)  data: 2.0545 (2.0545 -- 2.0545)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4094 (0.7067)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4103 (0.2035 -- 2.3499)  data: 0.1884 (0.0003 -- 2.0545)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4501 (0.6500)  acc1: 88.8889 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2192 (0.1693 -- 0.3846)  data: 0.0136 (0.0001 -- 0.1654)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5625 (0.7059)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2035 (0.1329 -- 0.3846)  data: 0.0134 (0.0001 -- 0.1654)  max mem: 16413
Val: Total time: 0:00:07 (0.2861 s / it)
* Acc@1 82.365 Acc@5 97.510 loss 0.661
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [137]  [  0/160]  eta: 0:20:21  lr: 0.000006  min_lr: 0.000000  loss: 1.9254 (1.9254)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.5016 (14.5016)  time: 7.6371 (7.6371 -- 7.6371)  data: 6.9038 (6.9038 -- 6.9038)  max mem: 16413
Epoch: [137]  [ 20/160]  eta: 0:02:47  lr: 0.000006  min_lr: 0.000000  loss: 1.7858 (1.8337)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6042 (8.7585)  time: 0.8734 (0.5328 -- 3.9075)  data: 0.0895 (0.0005 -- 0.8482)  max mem: 16413
[2023-08-30 18:54:50,517] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:54:50,518] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:54:50,520] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:54:50,520] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [137]  [ 40/160]  eta: 0:02:05  lr: 0.000006  min_lr: 0.000000  loss: 1.7429 (1.7918)  loss_scale: 8192.0000 (8791.4146)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4014 (8.7425)  time: 0.8973 (0.5345 -- 3.2763)  data: 0.3493 (0.0003 -- 2.7325)  max mem: 16413
Epoch: [137]  [ 60/160]  eta: 0:01:34  lr: 0.000006  min_lr: 0.000000  loss: 1.8426 (1.8060)  loss_scale: 16384.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3004 (8.6404)  time: 0.7432 (0.5220 -- 2.7111)  data: 0.1888 (0.0004 -- 2.1506)  max mem: 16413
[2023-08-30 18:55:26,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=22000, skipped=129, lr=[1.304852994725238e-07, 1.304852994725238e-07, 1.7398039929669843e-07, 1.7398039929669843e-07, 2.3197386572893125e-07, 2.3197386572893125e-07, 3.09298487638575e-07, 3.09298487638575e-07, 4.123979835181e-07, 4.123979835181e-07, 5.498639780241332e-07, 5.498639780241332e-07, 7.331519706988444e-07, 7.331519706988444e-07, 9.775359609317925e-07, 9.775359609317925e-07, 1.30338128124239e-06, 1.30338128124239e-06, 1.7378417083231867e-06, 1.7378417083231867e-06, 2.317122277764249e-06, 2.317122277764249e-06, 3.089496370352332e-06, 3.089496370352332e-06, 4.11932849380311e-06, 4.11932849380311e-06, 5.492437991737479e-06, 5.492437991737479e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 18:55:26,022] [INFO] [timer.py:260:stop] epoch=0/micro_step=22000/global_step=22000, RunningAvgSamplesPerSec=17.62384839037953, CurrSamplesPerSec=19.890331315516796, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [137]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.8028 (1.8082)  loss_scale: 16384.0000 (12540.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8255 (8.7135)  time: 0.9330 (0.5323 -- 2.6618)  data: 0.3650 (0.0003 -- 2.1320)  max mem: 16413
Epoch: [137]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.8521 (1.8145)  loss_scale: 16384.0000 (13301.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3729 (8.8085)  time: 0.8694 (0.5297 -- 3.7431)  data: 0.3204 (0.0004 -- 3.1954)  max mem: 16413
Epoch: [137]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.8643 (1.8196)  loss_scale: 16384.0000 (13811.3058)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4211 (8.8241)  time: 0.9630 (0.5151 -- 3.7437)  data: 0.4221 (0.0003 -- 3.2141)  max mem: 16413
Epoch: [137]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.7829 (1.8155)  loss_scale: 16384.0000 (14176.2270)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4066 (8.9256)  time: 0.8012 (0.5279 -- 3.4049)  data: 0.2541 (0.0006 -- 2.8959)  max mem: 16413
Epoch: [137]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.9039 (1.8205)  loss_scale: 16384.0000 (14438.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7252 (8.8419)  time: 0.7411 (0.4955 -- 4.7893)  data: 0.2343 (0.0002 -- 4.2824)  max mem: 16413
Epoch: [137] Total time: 0:02:23 (0.8973 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.9039 (1.7959)  loss_scale: 16384.0000 (14438.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7252 (8.8419)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2944 (0.2944)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3703 (2.3703 -- 2.3703)  data: 2.1383 (2.1383 -- 2.1383)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4087 (0.7017)  acc1: 88.8889 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4173 (0.2038 -- 2.3703)  data: 0.1994 (0.0009 -- 2.1383)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4366 (0.6449)  acc1: 88.8889 (82.5397)  acc5: 100.0000 (96.2963)  time: 0.2147 (0.1696 -- 0.2900)  data: 0.0077 (0.0001 -- 0.0959)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5625 (0.7015)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.1992 (0.1326 -- 0.2900)  data: 0.0074 (0.0001 -- 0.0959)  max mem: 16413
Val: Total time: 0:00:07 (0.2836 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.657
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [138]  [  0/160]  eta: 0:23:22  lr: 0.000005  min_lr: 0.000000  loss: 1.9360 (1.9360)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4439 (11.4439)  time: 8.7633 (8.7633 -- 8.7633)  data: 8.2354 (8.2354 -- 8.2354)  max mem: 16413
[2023-08-30 18:56:54,751] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:56:54,751] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:56:54,752] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:56:54,752] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 18:56:59,050] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22089
[2023-08-30 18:56:59,050] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22089
[2023-08-30 18:56:59,050] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:56:59,050] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 18:56:59,051] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 18:57:06,809] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22100
[2023-08-30 18:57:06,809] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22100
[2023-08-30 18:57:06,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:57:06,809] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 18:57:06,810] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [138]  [ 20/160]  eta: 0:02:40  lr: 0.000005  min_lr: 0.000000  loss: 1.7345 (1.7982)  loss_scale: 16384.0000 (18334.4762)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6828 (10.1524)  time: 0.7691 (0.5075 -- 3.2160)  data: 0.2233 (0.0002 -- 2.7005)  max mem: 16413
Epoch: [138]  [ 40/160]  eta: 0:02:06  lr: 0.000005  min_lr: 0.000000  loss: 1.8394 (1.8066)  loss_scale: 8192.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5679 (9.8065)  time: 0.9519 (0.5222 -- 3.8738)  data: 0.4117 (0.0004 -- 3.3613)  max mem: 16413
Epoch: [138]  [ 60/160]  eta: 0:01:36  lr: 0.000005  min_lr: 0.000000  loss: 1.7157 (1.7846)  loss_scale: 8192.0000 (11683.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7049 (9.5054)  time: 0.7962 (0.5242 -- 2.3735)  data: 0.1291 (0.0003 -- 1.5551)  max mem: 16413
Epoch: [138]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.7589 (1.7488)  loss_scale: 8192.0000 (10821.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2508 (9.3977)  time: 0.8735 (0.5197 -- 4.5777)  data: 0.0012 (0.0004 -- 0.0026)  max mem: 16413
Epoch: [138]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.8281 (1.7607)  loss_scale: 8192.0000 (10300.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7563 (9.3301)  time: 0.8936 (0.5207 -- 4.4205)  data: 0.0017 (0.0002 -- 0.0111)  max mem: 16413
Epoch: [138]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.7708 (1.7469)  loss_scale: 8192.0000 (9952.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9433 (9.4285)  time: 0.8767 (0.5258 -- 4.7468)  data: 0.0011 (0.0004 -- 0.0021)  max mem: 16413
Epoch: [138]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6944 (1.7424)  loss_scale: 8192.0000 (9702.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8179 (9.3610)  time: 0.8813 (0.5163 -- 3.8525)  data: 0.0014 (0.0004 -- 0.0040)  max mem: 16413
[2023-08-30 18:58:59,300] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:58:59,300] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 18:58:59,300] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 18:58:59,300] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [138]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.6352 (1.7378)  loss_scale: 16384.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6325 (9.2893)  time: 0.6329 (0.4959 -- 2.6316)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [138] Total time: 0:02:21 (0.8861 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.6352 (1.7764)  loss_scale: 16384.0000 (10086.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6325 (9.2893)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2913 (0.2913)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3384 (2.3384 -- 2.3384)  data: 2.0969 (2.0969 -- 2.0969)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3962 (0.7044)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (97.9798)  time: 0.4172 (0.2066 -- 2.3384)  data: 0.1986 (0.0005 -- 2.0969)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4576 (0.6493)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (97.3545)  time: 0.2220 (0.1701 -- 0.4348)  data: 0.0162 (0.0001 -- 0.2339)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5726 (0.7061)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (97.0954)  time: 0.2055 (0.1326 -- 0.4348)  data: 0.0160 (0.0001 -- 0.2339)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 82.158 Acc@5 97.718 loss 0.657
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [139]  [  0/160]  eta: 0:20:13  lr: 0.000005  min_lr: 0.000000  loss: 1.6584 (1.6584)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3364 (11.3364)  time: 7.5861 (7.5861 -- 7.5861)  data: 7.0765 (7.0765 -- 7.0765)  max mem: 16413
Epoch: [139]  [ 20/160]  eta: 0:02:39  lr: 0.000005  min_lr: 0.000000  loss: 1.6556 (1.7340)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9540 (8.4213)  time: 0.8206 (0.5201 -- 2.1899)  data: 0.2483 (0.0003 -- 1.6515)  max mem: 16413
Epoch: [139]  [ 40/160]  eta: 0:02:10  lr: 0.000005  min_lr: 0.000000  loss: 1.9348 (1.8421)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6119 (9.0155)  time: 1.0230 (0.5258 -- 3.7566)  data: 0.4851 (0.0006 -- 3.2424)  max mem: 16413
Epoch: [139]  [ 60/160]  eta: 0:01:39  lr: 0.000005  min_lr: 0.000000  loss: 1.6161 (1.8136)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4807 (9.0747)  time: 0.8163 (0.5153 -- 3.3650)  data: 0.2722 (0.0003 -- 2.8315)  max mem: 16413
Epoch: [139]  [ 80/160]  eta: 0:01:21  lr: 0.000005  min_lr: 0.000000  loss: 1.7658 (1.7973)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7682 (8.9930)  time: 1.0882 (0.5091 -- 4.9627)  data: 0.5516 (0.0002 -- 4.4460)  max mem: 16413
Epoch: [139]  [100/160]  eta: 0:00:56  lr: 0.000005  min_lr: 0.000000  loss: 1.8853 (1.8217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0551 (8.9706)  time: 0.6466 (0.5110 -- 2.6609)  data: 0.1080 (0.0001 -- 2.1345)  max mem: 16413
[2023-08-30 19:01:01,603] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:01:01,603] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:01:01,603] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:01:01,603] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [139]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.6774 (1.8021)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1692 (8.9113)  time: 0.9212 (0.5252 -- 3.4235)  data: 0.3683 (0.0002 -- 2.8923)  max mem: 16413
Epoch: [139]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.7735 (1.8090)  loss_scale: 32768.0000 (19172.7660)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1347 (8.9559)  time: 0.8851 (0.5203 -- 4.8411)  data: 0.3459 (0.0004 -- 4.3274)  max mem: 16413
[2023-08-30 19:01:33,507] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22394
[2023-08-30 19:01:33,507] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22394
[2023-08-30 19:01:33,507] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:01:33,507] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:01:33,507] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [139]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.9001 (1.8099)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5581 (9.0167)  time: 0.6359 (0.4838 -- 2.5103)  data: 0.1210 (0.0002 -- 1.9942)  max mem: 16413
Epoch: [139] Total time: 0:02:23 (0.8987 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.9001 (1.8172)  loss_scale: 32768.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5581 (9.0167)
[2023-08-30 19:01:36,000] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-139 is about to be saved!
[2023-08-30 19:01:36,002] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt
[2023-08-30 19:01:36,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt...
[2023-08-30 19:01:36,002] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
[2023-08-30 19:01:36,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-139/mp_rank_00_model_states.pt.
[2023-08-30 19:01:36,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-139 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2891 (0.2891)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3359 (2.3359 -- 2.3359)  data: 2.1272 (2.1272 -- 2.1272)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4139 (0.7108)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4311 (0.1915 -- 2.3359)  data: 0.2132 (0.0005 -- 2.1272)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4598 (0.6553)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2310 (0.1692 -- 0.4565)  data: 0.0223 (0.0001 -- 0.2248)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5654 (0.7104)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.2149 (0.1328 -- 0.4565)  data: 0.0214 (0.0001 -- 0.2248)  max mem: 16413
Val: Total time: 0:00:07 (0.2942 s / it)
* Acc@1 82.158 Acc@5 97.510 loss 0.660
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [140]  [  0/160]  eta: 0:17:39  lr: 0.000005  min_lr: 0.000000  loss: 1.6077 (1.6077)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7076 (11.7076)  time: 6.6246 (6.6246 -- 6.6246)  data: 5.5662 (5.5662 -- 5.5662)  max mem: 16413
Epoch: [140]  [ 20/160]  eta: 0:02:48  lr: 0.000005  min_lr: 0.000000  loss: 1.9129 (1.8225)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4471 (9.9009)  time: 0.9350 (0.5144 -- 4.2501)  data: 0.3155 (0.0003 -- 3.7160)  max mem: 16413
Epoch: [140]  [ 40/160]  eta: 0:02:06  lr: 0.000005  min_lr: 0.000000  loss: 1.8162 (1.8111)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2265 (9.5693)  time: 0.8961 (0.5179 -- 4.1777)  data: 0.0066 (0.0002 -- 0.1038)  max mem: 16413
Epoch: [140]  [ 60/160]  eta: 0:01:39  lr: 0.000005  min_lr: 0.000000  loss: 1.7975 (1.8156)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6232 (9.2018)  time: 0.8593 (0.5293 -- 4.1467)  data: 0.0012 (0.0004 -- 0.0023)  max mem: 16413
Epoch: [140]  [ 80/160]  eta: 0:01:17  lr: 0.000005  min_lr: 0.000000  loss: 1.7337 (1.7862)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3086 (9.1747)  time: 0.9267 (0.5186 -- 3.2390)  data: 0.0019 (0.0002 -- 0.0152)  max mem: 16413
Epoch: [140]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.7218 (1.7825)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6188 (9.2325)  time: 0.7406 (0.5301 -- 2.6996)  data: 0.0017 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [140]  [120/160]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000000  loss: 1.8736 (1.8047)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9921 (9.1367)  time: 0.9791 (0.5260 -- 5.1006)  data: 0.0015 (0.0004 -- 0.0034)  max mem: 16413
[2023-08-30 19:03:39,841] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:03:39,841] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:03:39,848] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:03:39,849] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:03:41,460] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22526
[2023-08-30 19:03:41,460] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22526
[2023-08-30 19:03:41,461] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:03:41,461] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:03:41,461] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [140]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6912 (1.7996)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5019 (9.2646)  time: 0.8439 (0.5219 -- 3.8529)  data: 0.0016 (0.0002 -- 0.0133)  max mem: 16413
Epoch: [140]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.6023 (1.7972)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0913 (9.3065)  time: 0.6095 (0.4965 -- 2.3396)  data: 0.0008 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [140] Total time: 0:02:21 (0.8871 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.6023 (1.7760)  loss_scale: 16384.0000 (16691.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0913 (9.3065)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2948 (0.2948)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3891 (2.3891 -- 2.3891)  data: 2.1806 (2.1806 -- 2.1806)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4078 (0.7138)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4291 (0.2014 -- 2.3891)  data: 0.2107 (0.0009 -- 2.1806)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4637 (0.6544)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2188 (0.1695 -- 0.3581)  data: 0.0090 (0.0001 -- 0.1212)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5658 (0.7103)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.6805)  time: 0.2014 (0.1340 -- 0.3581)  data: 0.0084 (0.0001 -- 0.1212)  max mem: 16413
Val: Total time: 0:00:07 (0.2875 s / it)
* Acc@1 82.158 Acc@5 97.510 loss 0.661
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [141]  [  0/160]  eta: 0:18:45  lr: 0.000005  min_lr: 0.000000  loss: 1.1411 (1.1411)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0677 (7.0677)  time: 7.0343 (7.0343 -- 7.0343)  data: 6.5145 (6.5145 -- 6.5145)  max mem: 16413
Epoch: [141]  [ 20/160]  eta: 0:02:33  lr: 0.000005  min_lr: 0.000000  loss: 1.6833 (1.7284)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1153 (9.5166)  time: 0.8017 (0.5308 -- 2.6394)  data: 0.0786 (0.0004 -- 1.3594)  max mem: 16413
Epoch: [141]  [ 40/160]  eta: 0:02:01  lr: 0.000005  min_lr: 0.000000  loss: 1.6787 (1.7197)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7797 (9.1615)  time: 0.9148 (0.5153 -- 2.8177)  data: 0.2497 (0.0004 -- 1.4355)  max mem: 16413
Epoch: [141]  [ 60/160]  eta: 0:01:37  lr: 0.000005  min_lr: 0.000000  loss: 1.8001 (1.7348)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6262 (9.7124)  time: 0.8941 (0.5279 -- 2.6247)  data: 0.3379 (0.0005 -- 2.1088)  max mem: 16413
Epoch: [141]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.7917 (1.7571)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3024 (9.9394)  time: 0.8516 (0.5259 -- 3.0313)  data: 0.3068 (0.0003 -- 2.4601)  max mem: 16413
[2023-08-30 19:05:42,479] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:05:42,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:05:42,480] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:05:42,480] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [141]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.8662 (1.7776)  loss_scale: 16384.0000 (17357.3069)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2201 (9.6020)  time: 0.8484 (0.5258 -- 3.2271)  data: 0.2921 (0.0006 -- 2.7208)  max mem: 16413
[2023-08-30 19:06:06,490] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22680
[2023-08-30 19:06:06,491] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22680
[2023-08-30 19:06:06,491] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:06:06,532] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:06:06,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [141]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.6567 (1.7621)  loss_scale: 32768.0000 (19769.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1700 (9.4098)  time: 0.9317 (0.5102 -- 3.4172)  data: 0.3906 (0.0004 -- 2.8848)  max mem: 16413
Epoch: [141]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.6465 (1.7540)  loss_scale: 16384.0000 (19288.9645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4657 (9.2982)  time: 0.7712 (0.5217 -- 3.3246)  data: 0.2244 (0.0004 -- 2.7823)  max mem: 16413
Epoch: [141]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.7604 (1.7538)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8403 (9.1361)  time: 0.7115 (0.4953 -- 4.0121)  data: 0.2010 (0.0003 -- 3.5080)  max mem: 16413
Epoch: [141] Total time: 0:02:21 (0.8816 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.7604 (1.7637)  loss_scale: 16384.0000 (18944.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8403 (9.1361)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.3013 (0.3013)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2805 (2.2805 -- 2.2805)  data: 2.0528 (2.0528 -- 2.0528)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4047 (0.7085)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (96.9697)  time: 0.4118 (0.2075 -- 2.2805)  data: 0.1943 (0.0005 -- 2.0528)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4433 (0.6488)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.8254)  time: 0.2232 (0.1693 -- 0.4577)  data: 0.0194 (0.0001 -- 0.2708)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5681 (0.7076)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.6805)  time: 0.2081 (0.1322 -- 0.4577)  data: 0.0191 (0.0001 -- 0.2708)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 81.743 Acc@5 97.510 loss 0.660
Accuracy of the network on the 482 val images: 81.74%
Max accuracy: 82.99%
Epoch: [142]  [  0/160]  eta: 0:17:33  lr: 0.000005  min_lr: 0.000000  loss: 1.7526 (1.7526)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4955 (8.4955)  time: 6.5835 (6.5835 -- 6.5835)  data: 5.8967 (5.8967 -- 5.8967)  max mem: 16413
Epoch: [142]  [ 20/160]  eta: 0:02:47  lr: 0.000005  min_lr: 0.000000  loss: 1.9461 (1.8682)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5838 (8.4202)  time: 0.9273 (0.5223 -- 4.0140)  data: 0.2958 (0.0003 -- 2.6909)  max mem: 16413
[2023-08-30 19:07:13,789] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22746
[2023-08-30 19:07:13,789] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22746
[2023-08-30 19:07:13,830] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:07:13,830] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:07:13,830] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [142]  [ 40/160]  eta: 0:01:57  lr: 0.000005  min_lr: 0.000000  loss: 1.9549 (1.8930)  loss_scale: 8192.0000 (13386.9268)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0816 (8.6147)  time: 0.7580 (0.5149 -- 1.7201)  data: 0.1693 (0.0001 -- 1.1761)  max mem: 16413
Epoch: [142]  [ 60/160]  eta: 0:01:35  lr: 0.000005  min_lr: 0.000000  loss: 1.7760 (1.8450)  loss_scale: 8192.0000 (11683.6721)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1740 (8.9017)  time: 0.8892 (0.5291 -- 2.4920)  data: 0.0353 (0.0005 -- 0.3406)  max mem: 16413
Epoch: [142]  [ 80/160]  eta: 0:01:13  lr: 0.000005  min_lr: 0.000000  loss: 1.8401 (1.8581)  loss_scale: 8192.0000 (10821.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2268 (9.0223)  time: 0.8200 (0.5312 -- 2.5210)  data: 0.2390 (0.0003 -- 1.9829)  max mem: 16413
Epoch: [142]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.7219 (1.8352)  loss_scale: 8192.0000 (10300.8317)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9418 (8.9349)  time: 0.9261 (0.5271 -- 3.4016)  data: 0.2076 (0.0003 -- 2.4586)  max mem: 16413
Epoch: [142]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.6146 (1.8007)  loss_scale: 8192.0000 (9952.2645)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0456 (8.9938)  time: 0.9388 (0.5100 -- 3.3890)  data: 0.1640 (0.0002 -- 2.8466)  max mem: 16413
Epoch: [142]  [140/160]  eta: 0:00:18  lr: 0.000005  min_lr: 0.000000  loss: 1.8654 (1.8186)  loss_scale: 8192.0000 (9702.5816)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7111 (8.9747)  time: 0.8249 (0.5239 -- 3.7300)  data: 0.2025 (0.0001 -- 3.1987)  max mem: 16413
[2023-08-30 19:09:02,835] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:09:02,835] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 19:09:02,836] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:09:02,836] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [142]  [159/160]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000000  loss: 1.6254 (1.7986)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3422 (9.0251)  time: 0.6883 (0.4966 -- 2.8103)  data: 0.0973 (0.0002 -- 1.9315)  max mem: 16413
Epoch: [142] Total time: 0:02:21 (0.8843 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000000  loss: 1.6254 (1.7743)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3422 (9.0251)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2972 (0.2972)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4050 (2.4050 -- 2.4050)  data: 2.1908 (2.1908 -- 2.1908)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3933 (0.7016)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4144 (0.1944 -- 2.4050)  data: 0.2001 (0.0007 -- 2.1908)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4430 (0.6442)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2166 (0.1699 -- 0.3776)  data: 0.0103 (0.0001 -- 0.1924)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5807 (0.7026)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2023 (0.1346 -- 0.3776)  data: 0.0100 (0.0001 -- 0.1924)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 82.365 Acc@5 97.510 loss 0.655
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [143]  [  0/160]  eta: 0:22:39  lr: 0.000005  min_lr: 0.000000  loss: 1.6511 (1.6511)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7482 (11.7482)  time: 8.4940 (8.4940 -- 8.4940)  data: 7.9595 (7.9595 -- 7.9595)  max mem: 16413
Epoch: [143]  [ 20/160]  eta: 0:02:42  lr: 0.000005  min_lr: 0.000000  loss: 1.8066 (1.7988)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1682 (8.4172)  time: 0.7976 (0.5251 -- 3.2607)  data: 0.1791 (0.0004 -- 2.2250)  max mem: 16413
Epoch: [143]  [ 40/160]  eta: 0:02:03  lr: 0.000005  min_lr: 0.000000  loss: 1.8573 (1.8495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5044 (8.2550)  time: 0.8909 (0.5202 -- 3.0163)  data: 0.1038 (0.0003 -- 1.5284)  max mem: 16413
Epoch: [143]  [ 60/160]  eta: 0:01:37  lr: 0.000005  min_lr: 0.000000  loss: 1.7181 (1.7941)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4026 (8.6515)  time: 0.8462 (0.5318 -- 3.4949)  data: 0.2928 (0.0003 -- 2.9684)  max mem: 16413
[2023-08-30 19:10:16,694] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22947
[2023-08-30 19:10:16,694] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 22947
[2023-08-30 19:10:16,694] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:10:16,694] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:10:16,694] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [143]  [ 80/160]  eta: 0:01:15  lr: 0.000005  min_lr: 0.000000  loss: 1.8382 (1.8081)  loss_scale: 8192.0000 (14968.0988)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4439 (8.5898)  time: 0.8606 (0.5278 -- 4.5246)  data: 0.2063 (0.0004 -- 1.8874)  max mem: 16413
Epoch: [143]  [100/160]  eta: 0:00:55  lr: 0.000005  min_lr: 0.000000  loss: 1.7994 (1.8112)  loss_scale: 8192.0000 (13626.2970)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1229 (8.6456)  time: 0.8664 (0.5170 -- 3.9227)  data: 0.1298 (0.0005 -- 2.3192)  max mem: 16413
[2023-08-30 19:11:01,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=23000, skipped=136, lr=[1.0762978070533814e-07, 1.0762978070533814e-07, 1.4350637427378418e-07, 1.4350637427378418e-07, 1.9134183236504557e-07, 1.9134183236504557e-07, 2.551224431533941e-07, 2.551224431533941e-07, 3.401632575378588e-07, 3.401632575378588e-07, 4.535510100504784e-07, 4.535510100504784e-07, 6.047346800673045e-07, 6.047346800673045e-07, 8.063129067564061e-07, 8.063129067564061e-07, 1.075083875675208e-06, 1.075083875675208e-06, 1.433445167566944e-06, 1.433445167566944e-06, 1.911260223422592e-06, 1.911260223422592e-06, 2.548346964563456e-06, 2.548346964563456e-06, 3.3977959527512746e-06, 3.3977959527512746e-06, 4.5303946036683665e-06, 4.5303946036683665e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 19:11:01,606] [INFO] [timer.py:260:stop] epoch=0/micro_step=23000/global_step=23000, RunningAvgSamplesPerSec=17.665446547673355, CurrSamplesPerSec=20.757296055830473, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [143]  [120/160]  eta: 0:00:36  lr: 0.000005  min_lr: 0.000000  loss: 1.8733 (1.8058)  loss_scale: 8192.0000 (12728.0661)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1881 (8.6525)  time: 0.8924 (0.5277 -- 3.4992)  data: 0.3051 (0.0002 -- 2.1617)  max mem: 16413
Epoch: [143]  [140/160]  eta: 0:00:17  lr: 0.000005  min_lr: 0.000000  loss: 1.8778 (1.8170)  loss_scale: 8192.0000 (12084.6525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2415 (8.8131)  time: 0.7583 (0.5228 -- 3.0816)  data: 0.0753 (0.0002 -- 1.2779)  max mem: 16413
Epoch: [143]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7050 (1.8187)  loss_scale: 8192.0000 (11622.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0857 (8.8075)  time: 0.6928 (0.4962 -- 2.4382)  data: 0.0955 (0.0002 -- 0.7897)  max mem: 16413
Epoch: [143] Total time: 0:02:20 (0.8755 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7050 (1.7845)  loss_scale: 8192.0000 (11622.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0857 (8.8075)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2967 (0.2967)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1830 (2.1830 -- 2.1830)  data: 1.9904 (1.9904 -- 1.9904)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4116 (0.7055)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (95.9596)  time: 0.4228 (0.1990 -- 2.1830)  data: 0.2065 (0.0007 -- 1.9904)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4399 (0.6441)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2263 (0.1697 -- 0.5114)  data: 0.0153 (0.0001 -- 0.2694)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5674 (0.7043)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2099 (0.1331 -- 0.5114)  data: 0.0150 (0.0001 -- 0.2694)  max mem: 16413
Val: Total time: 0:00:07 (0.2850 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.657
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [144]  [  0/160]  eta: 0:25:11  lr: 0.000004  min_lr: 0.000000  loss: 1.0017 (1.0017)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3356 (9.3356)  time: 9.4469 (9.4469 -- 9.4469)  data: 5.6475 (5.6475 -- 5.6475)  max mem: 16413
Epoch: [144]  [ 20/160]  eta: 0:02:47  lr: 0.000004  min_lr: 0.000000  loss: 1.5750 (1.6743)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1876 (8.9127)  time: 0.7862 (0.5295 -- 2.4627)  data: 0.0842 (0.0006 -- 1.3005)  max mem: 16413
[2023-08-30 19:12:19,782] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:12:19,782] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 19:12:19,783] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:12:19,783] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [144]  [ 40/160]  eta: 0:02:07  lr: 0.000004  min_lr: 0.000000  loss: 1.8846 (1.6950)  loss_scale: 8192.0000 (9191.0244)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1909 (8.7044)  time: 0.9212 (0.5290 -- 4.7642)  data: 0.1281 (0.0002 -- 2.3838)  max mem: 16413
Epoch: [144]  [ 60/160]  eta: 0:01:37  lr: 0.000004  min_lr: 0.000000  loss: 1.8321 (1.7382)  loss_scale: 16384.0000 (11549.3770)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5014 (8.9570)  time: 0.8082 (0.5227 -- 4.1207)  data: 0.0886 (0.0003 -- 1.6211)  max mem: 16413
Epoch: [144]  [ 80/160]  eta: 0:01:20  lr: 0.000004  min_lr: 0.000000  loss: 1.7861 (1.7288)  loss_scale: 16384.0000 (12743.1111)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6555 (8.9548)  time: 1.0674 (0.5209 -- 5.7600)  data: 0.1314 (0.0003 -- 1.3573)  max mem: 16413
Epoch: [144]  [100/160]  eta: 0:00:57  lr: 0.000004  min_lr: 0.000000  loss: 1.7551 (1.7563)  loss_scale: 16384.0000 (13464.0792)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5778 (8.9428)  time: 0.7644 (0.5001 -- 3.7302)  data: 0.0127 (0.0001 -- 0.2298)  max mem: 16413
Epoch: [144]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.7077 (1.7578)  loss_scale: 16384.0000 (13946.7107)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7139 (8.9689)  time: 0.8197 (0.5254 -- 4.1075)  data: 0.1024 (0.0002 -- 1.3992)  max mem: 16413
Epoch: [144]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.7770 (1.7473)  loss_scale: 16384.0000 (14292.4255)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0466 (8.8809)  time: 0.8255 (0.5231 -- 3.6327)  data: 0.0753 (0.0003 -- 0.7425)  max mem: 16413
Epoch: [144]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7012 (1.7386)  loss_scale: 16384.0000 (14540.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8944 (9.0325)  time: 0.6713 (0.4978 -- 2.0828)  data: 0.0658 (0.0002 -- 1.0211)  max mem: 16413
Epoch: [144] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7012 (1.7439)  loss_scale: 16384.0000 (14540.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8944 (9.0325)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2783 (0.2783)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4373 (2.4373 -- 2.4373)  data: 2.1921 (2.1921 -- 2.1921)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4164 (0.7085)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4200 (0.2073 -- 2.4373)  data: 0.2023 (0.0005 -- 2.1921)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4359 (0.6453)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2149 (0.1702 -- 0.3236)  data: 0.0083 (0.0001 -- 0.1297)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5636 (0.7055)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.1988 (0.1325 -- 0.3236)  data: 0.0080 (0.0001 -- 0.1297)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 82.365 Acc@5 97.510 loss 0.655
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [145]  [  0/160]  eta: 0:17:19  lr: 0.000004  min_lr: 0.000000  loss: 1.5641 (1.5641)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6649 (12.6649)  time: 6.4968 (6.4968 -- 6.4968)  data: 5.4211 (5.4211 -- 5.4211)  max mem: 16413
[2023-08-30 19:14:21,717] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:14:21,717] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:14:21,717] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:14:21,717] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:14:30,170] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23213
[2023-08-30 19:14:30,170] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23213
[2023-08-30 19:14:30,170] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:14:30,170] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:14:30,171] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [145]  [ 20/160]  eta: 0:02:51  lr: 0.000004  min_lr: 0.000000  loss: 1.6986 (1.7239)  loss_scale: 16384.0000 (23405.7143)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7656 (8.4104)  time: 0.9583 (0.5259 -- 4.1263)  data: 0.1725 (0.0004 -- 1.7160)  max mem: 16413
Epoch: [145]  [ 40/160]  eta: 0:02:06  lr: 0.000004  min_lr: 0.000000  loss: 1.8358 (1.7308)  loss_scale: 16384.0000 (19980.4878)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4548 (8.9375)  time: 0.8811 (0.5253 -- 2.8930)  data: 0.0015 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [145]  [ 60/160]  eta: 0:01:41  lr: 0.000004  min_lr: 0.000000  loss: 1.8091 (1.7398)  loss_scale: 16384.0000 (18801.3115)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3918 (9.0171)  time: 0.9455 (0.5314 -- 4.2380)  data: 0.0016 (0.0004 -- 0.0038)  max mem: 16413
Epoch: [145]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.7640 (1.7573)  loss_scale: 16384.0000 (18204.4444)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3869 (9.3673)  time: 0.7333 (0.5369 -- 2.6845)  data: 0.0020 (0.0001 -- 0.0098)  max mem: 16413
Epoch: [145]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.5983 (1.7287)  loss_scale: 16384.0000 (17843.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6989 (9.1836)  time: 0.8574 (0.5332 -- 3.7157)  data: 0.0063 (0.0004 -- 0.0838)  max mem: 16413
Epoch: [145]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7458 (1.7389)  loss_scale: 16384.0000 (17602.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7611 (9.1309)  time: 0.8298 (0.5437 -- 3.8948)  data: 0.0019 (0.0004 -- 0.0071)  max mem: 16413
[2023-08-30 19:16:06,994] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23325
[2023-08-30 19:16:06,994] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:16:06,994] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23325
[2023-08-30 19:16:06,995] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:16:06,995] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [145]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.6787 (1.7349)  loss_scale: 8192.0000 (16500.1986)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1359 (9.1572)  time: 0.9727 (0.5232 -- 3.4712)  data: 0.0013 (0.0003 -- 0.0050)  max mem: 16413
Epoch: [145]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.6626 (1.7314)  loss_scale: 8192.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2294 (9.3299)  time: 0.6625 (0.4985 -- 2.7536)  data: 0.0006 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [145] Total time: 0:02:20 (0.8786 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.6626 (1.7681)  loss_scale: 8192.0000 (15513.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2294 (9.3299)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2804 (0.2804)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3050 (2.3050 -- 2.3050)  data: 2.0759 (2.0759 -- 2.0759)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4179 (0.7065)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4179 (0.1944 -- 2.3050)  data: 0.2022 (0.0006 -- 2.0759)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4491 (0.6458)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2263 (0.1701 -- 0.4767)  data: 0.0216 (0.0001 -- 0.2818)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5654 (0.7032)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.2110 (0.1329 -- 0.4767)  data: 0.0213 (0.0001 -- 0.2818)  max mem: 16413
Val: Total time: 0:00:07 (0.2894 s / it)
* Acc@1 82.365 Acc@5 97.510 loss 0.654
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [146]  [  0/160]  eta: 0:20:50  lr: 0.000004  min_lr: 0.000000  loss: 1.7793 (1.7793)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5858 (8.5858)  time: 7.8127 (7.8127 -- 7.8127)  data: 7.2781 (7.2781 -- 7.2781)  max mem: 16413
Epoch: [146]  [ 20/160]  eta: 0:02:46  lr: 0.000004  min_lr: 0.000000  loss: 1.5912 (1.6496)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1893 (8.3962)  time: 0.8581 (0.5227 -- 3.2374)  data: 0.2346 (0.0004 -- 2.7021)  max mem: 16413
Epoch: [146]  [ 40/160]  eta: 0:02:05  lr: 0.000004  min_lr: 0.000000  loss: 1.6952 (1.6446)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1973 (8.6557)  time: 0.8965 (0.5273 -- 2.5194)  data: 0.1655 (0.0006 -- 1.7648)  max mem: 16413
Epoch: [146]  [ 60/160]  eta: 0:01:40  lr: 0.000004  min_lr: 0.000000  loss: 1.7577 (1.6914)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1286 (8.9516)  time: 0.9157 (0.5147 -- 3.7916)  data: 0.0891 (0.0003 -- 1.4302)  max mem: 16413
Epoch: [146]  [ 80/160]  eta: 0:01:17  lr: 0.000004  min_lr: 0.000000  loss: 1.7790 (1.7166)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7701 (9.2283)  time: 0.8793 (0.5033 -- 3.0803)  data: 0.2446 (0.0001 -- 2.5575)  max mem: 16413
[2023-08-30 19:18:08,509] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:18:08,509] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:18:08,509] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 19:18:08,509] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [146]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.8552 (1.7496)  loss_scale: 8192.0000 (8759.7624)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3163 (9.0199)  time: 0.8471 (0.5235 -- 3.3719)  data: 0.1073 (0.0004 -- 2.1141)  max mem: 16413
Epoch: [146]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.9488 (1.7710)  loss_scale: 16384.0000 (10019.9669)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0097 (8.9917)  time: 0.7363 (0.5141 -- 3.4539)  data: 0.0017 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [146]  [140/160]  eta: 0:00:17  lr: 0.000004  min_lr: 0.000000  loss: 1.5110 (1.7542)  loss_scale: 16384.0000 (10922.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9076 (9.0338)  time: 0.8122 (0.5311 -- 3.8486)  data: 0.0019 (0.0003 -- 0.0112)  max mem: 16413
Epoch: [146]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.7534 (1.7600)  loss_scale: 16384.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0933 (9.0489)  time: 0.6820 (0.4966 -- 1.3783)  data: 0.0691 (0.0002 -- 0.7065)  max mem: 16413
Epoch: [146] Total time: 0:02:19 (0.8740 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.7534 (1.7584)  loss_scale: 16384.0000 (11571.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0933 (9.0489)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2776 (0.2776)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3481 (2.3481 -- 2.3481)  data: 2.0405 (2.0405 -- 2.0405)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4065 (0.6997)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4153 (0.1912 -- 2.3481)  data: 0.1947 (0.0007 -- 2.0405)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4605 (0.6451)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2187 (0.1712 -- 0.3218)  data: 0.0084 (0.0001 -- 0.0919)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5645 (0.7022)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (95.8506)  time: 0.2070 (0.1328 -- 0.3218)  data: 0.0081 (0.0001 -- 0.0919)  max mem: 16413
Val: Total time: 0:00:07 (0.2858 s / it)
* Acc@1 82.158 Acc@5 97.095 loss 0.652
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [147]  [  0/160]  eta: 0:24:31  lr: 0.000004  min_lr: 0.000000  loss: 1.2262 (1.2262)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5587 (9.5587)  time: 9.1943 (9.1943 -- 9.1943)  data: 4.5766 (4.5766 -- 4.5766)  max mem: 16413
Epoch: [147]  [ 20/160]  eta: 0:02:50  lr: 0.000004  min_lr: 0.000000  loss: 1.8566 (1.7398)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4019 (9.7490)  time: 0.8216 (0.5128 -- 3.5421)  data: 0.0016 (0.0004 -- 0.0051)  max mem: 16413
Epoch: [147]  [ 40/160]  eta: 0:02:14  lr: 0.000004  min_lr: 0.000000  loss: 1.9092 (1.8155)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1733 (9.6526)  time: 1.0174 (0.5186 -- 4.2319)  data: 0.0015 (0.0004 -- 0.0056)  max mem: 16413
Epoch: [147]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.6994 (1.8003)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1822 (9.6280)  time: 0.6953 (0.5150 -- 2.2271)  data: 0.0017 (0.0003 -- 0.0039)  max mem: 16413
[2023-08-30 19:20:07,428] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:20:07,428] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:20:07,429] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:20:07,429] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:20:14,685] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23589
[2023-08-30 19:20:14,685] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23589
[2023-08-30 19:20:14,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:20:14,685] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:20:14,685] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [147]  [ 80/160]  eta: 0:01:16  lr: 0.000004  min_lr: 0.000000  loss: 1.6930 (1.7928)  loss_scale: 16384.0000 (17799.9012)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0247 (9.2839)  time: 0.9000 (0.5139 -- 3.1534)  data: 0.0021 (0.0003 -- 0.0150)  max mem: 16413
Epoch: [147]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.7926 (1.8065)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4205 (9.3211)  time: 0.7465 (0.5172 -- 2.5263)  data: 0.0018 (0.0002 -- 0.0077)  max mem: 16413
Epoch: [147]  [120/160]  eta: 0:00:37  lr: 0.000004  min_lr: 0.000000  loss: 1.7820 (1.7982)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6494 (9.2595)  time: 0.9752 (0.5294 -- 4.0460)  data: 0.0013 (0.0002 -- 0.0029)  max mem: 16413
Epoch: [147]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.6276 (1.7745)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8745 (9.1517)  time: 0.7751 (0.5235 -- 2.5431)  data: 0.0018 (0.0003 -- 0.0062)  max mem: 16413
Epoch: [147]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.9832 (1.7890)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1013 (9.1518)  time: 0.6910 (0.4956 -- 1.5340)  data: 0.0237 (0.0003 -- 0.4627)  max mem: 16413
Epoch: [147] Total time: 0:02:21 (0.8820 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.9832 (1.7837)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1013 (9.1518)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2758 (0.2758)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2964 (2.2964 -- 2.2964)  data: 2.0547 (2.0547 -- 2.0547)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4017 (0.6929)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4137 (0.2103 -- 2.2964)  data: 0.1943 (0.0004 -- 2.0547)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4625 (0.6411)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2310 (0.1698 -- 0.6434)  data: 0.0274 (0.0001 -- 0.4627)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5635 (0.6988)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2138 (0.1329 -- 0.6434)  data: 0.0266 (0.0001 -- 0.4627)  max mem: 16413
Val: Total time: 0:00:07 (0.2930 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.651
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [148]  [  0/160]  eta: 0:24:34  lr: 0.000004  min_lr: 0.000000  loss: 1.8461 (1.8461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3816 (11.3816)  time: 9.2170 (9.2170 -- 9.2170)  data: 8.6907 (8.6907 -- 8.6907)  max mem: 16413
Epoch: [148]  [ 20/160]  eta: 0:02:54  lr: 0.000004  min_lr: 0.000000  loss: 1.7106 (1.7501)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2072 (9.0148)  time: 0.8483 (0.5212 -- 4.2503)  data: 0.0689 (0.0004 -- 1.2967)  max mem: 16413
[2023-08-30 19:22:16,620] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:22:16,621] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:22:16,625] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:22:16,625] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [148]  [ 40/160]  eta: 0:02:11  lr: 0.000004  min_lr: 0.000000  loss: 1.9603 (1.7800)  loss_scale: 16384.0000 (17582.8293)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0856 (8.7595)  time: 0.9319 (0.5313 -- 3.6349)  data: 0.0014 (0.0004 -- 0.0028)  max mem: 16413
[2023-08-30 19:22:21,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23722
[2023-08-30 19:22:21,353] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23722
[2023-08-30 19:22:21,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:22:21,354] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:22:21,354] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [148]  [ 60/160]  eta: 0:01:38  lr: 0.000004  min_lr: 0.000000  loss: 1.7484 (1.7608)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0987 (8.9774)  time: 0.7480 (0.5241 -- 2.5211)  data: 0.0018 (0.0004 -- 0.0045)  max mem: 16413
Epoch: [148]  [ 80/160]  eta: 0:01:14  lr: 0.000004  min_lr: 0.000000  loss: 1.6693 (1.7485)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8237 (9.0876)  time: 0.7674 (0.5180 -- 3.8763)  data: 0.0014 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [148]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.9308 (1.7590)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1875 (9.2590)  time: 0.8837 (0.5328 -- 2.7894)  data: 0.1096 (0.0004 -- 1.3316)  max mem: 16413
Epoch: [148]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.6696 (1.7454)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3577 (9.1443)  time: 0.8505 (0.5237 -- 2.5709)  data: 0.1561 (0.0009 -- 2.0314)  max mem: 16413
Epoch: [148]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8090 (1.7463)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6986 (9.1783)  time: 0.8922 (0.5275 -- 2.9221)  data: 0.3037 (0.0002 -- 2.3967)  max mem: 16413
Epoch: [148]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8232 (1.7585)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9453 (9.1726)  time: 0.6355 (0.4956 -- 1.5010)  data: 0.0608 (0.0001 -- 0.7290)  max mem: 16413
Epoch: [148] Total time: 0:02:19 (0.8741 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8232 (1.7738)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9453 (9.1726)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2747 (0.2747)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4008 (2.4008 -- 2.4008)  data: 2.1975 (2.1975 -- 2.1975)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4031 (0.6965)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4359 (0.1916 -- 2.4008)  data: 0.2220 (0.0006 -- 2.1975)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4586 (0.6433)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2219 (0.1684 -- 0.4645)  data: 0.0194 (0.0001 -- 0.2322)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5670 (0.7001)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2079 (0.1330 -- 0.4645)  data: 0.0190 (0.0001 -- 0.2322)  max mem: 16413
Val: Total time: 0:00:07 (0.2898 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.651
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [149]  [  0/160]  eta: 0:22:50  lr: 0.000004  min_lr: 0.000000  loss: 1.4611 (1.4611)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.7578 (14.7578)  time: 8.5628 (8.5628 -- 8.5628)  data: 8.0282 (8.0282 -- 8.0282)  max mem: 16413
[2023-08-30 19:24:20,017] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:24:20,017] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:24:20,020] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:24:20,020] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [149]  [ 20/160]  eta: 0:02:53  lr: 0.000004  min_lr: 0.000000  loss: 1.6192 (1.6756)  loss_scale: 16384.0000 (24185.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7550 (8.6825)  time: 0.8720 (0.5230 -- 4.7270)  data: 0.3217 (0.0005 -- 4.2190)  max mem: 16413
[2023-08-30 19:24:32,555] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23865
[2023-08-30 19:24:32,555] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23865
[2023-08-30 19:24:32,555] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:24:32,555] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:24:32,555] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 19:24:38,494] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23873
[2023-08-30 19:24:38,495] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 23873
[2023-08-30 19:24:38,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:24:38,495] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:24:38,496] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [149]  [ 40/160]  eta: 0:02:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8475 (1.7237)  loss_scale: 16384.0000 (20380.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5534 (8.6271)  time: 0.7524 (0.5282 -- 2.1165)  data: 0.2054 (0.0003 -- 1.5854)  max mem: 16413
Epoch: [149]  [ 60/160]  eta: 0:01:35  lr: 0.000004  min_lr: 0.000000  loss: 1.7102 (1.7151)  loss_scale: 8192.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0378 (8.3759)  time: 0.8575 (0.5284 -- 2.7931)  data: 0.3078 (0.0004 -- 2.2453)  max mem: 16413
Epoch: [149]  [ 80/160]  eta: 0:01:14  lr: 0.000004  min_lr: 0.000000  loss: 1.8629 (1.7287)  loss_scale: 8192.0000 (14361.2840)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6928 (8.4269)  time: 0.8466 (0.5385 -- 2.9757)  data: 0.2935 (0.0008 -- 2.4281)  max mem: 16413
Epoch: [149]  [100/160]  eta: 0:00:55  lr: 0.000004  min_lr: 0.000000  loss: 1.8817 (1.7499)  loss_scale: 8192.0000 (13139.6436)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9018 (8.5382)  time: 0.8730 (0.5263 -- 3.0554)  data: 0.3023 (0.0002 -- 2.5255)  max mem: 16413
Epoch: [149]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.8155 (1.7700)  loss_scale: 8192.0000 (12321.8512)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1911 (8.6824)  time: 0.8441 (0.5261 -- 2.8682)  data: 0.2167 (0.0004 -- 2.3407)  max mem: 16413
Epoch: [149]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8088 (1.7638)  loss_scale: 8192.0000 (11736.0567)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7579 (8.8323)  time: 0.9829 (0.5241 -- 4.2933)  data: 0.0673 (0.0004 -- 1.3239)  max mem: 16413
[2023-08-30 19:26:24,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=24000, skipped=142, lr=[8.650991635604222e-08, 8.650991635604222e-08, 1.1534655514138963e-07, 1.1534655514138963e-07, 1.5379540685518617e-07, 1.5379540685518617e-07, 2.0506054247358157e-07, 2.0506054247358157e-07, 2.734140566314421e-07, 2.734140566314421e-07, 3.6455207550858947e-07, 3.6455207550858947e-07, 4.860694340114526e-07, 4.860694340114526e-07, 6.480925786819368e-07, 6.480925786819368e-07, 8.641234382425824e-07, 8.641234382425824e-07, 1.1521645843234432e-06, 1.1521645843234432e-06, 1.536219445764591e-06, 1.536219445764591e-06, 2.048292594352788e-06, 2.048292594352788e-06, 2.731056792470384e-06, 2.731056792470384e-06, 3.6414090566271786e-06, 3.6414090566271786e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 19:26:24,398] [INFO] [timer.py:260:stop] epoch=0/micro_step=24000/global_step=24000, RunningAvgSamplesPerSec=17.607396571216658, CurrSamplesPerSec=24.801453446687983, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [149]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8092 (1.7759)  loss_scale: 8192.0000 (11315.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7974 (8.8045)  time: 0.6307 (0.4947 -- 2.5738)  data: 0.0005 (0.0001 -- 0.0012)  max mem: 16413
Epoch: [149] Total time: 0:02:21 (0.8828 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8092 (1.8009)  loss_scale: 8192.0000 (11315.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7974 (8.8045)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2726 (0.2726)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3217 (2.3217 -- 2.3217)  data: 2.0552 (2.0552 -- 2.0552)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3872 (0.6966)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4232 (0.2264 -- 2.3217)  data: 0.1895 (0.0006 -- 2.0552)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4622 (0.6458)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2195 (0.1698 -- 0.2684)  data: 0.0062 (0.0001 -- 0.0894)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5635 (0.7014)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.1977 (0.1327 -- 0.2684)  data: 0.0057 (0.0001 -- 0.0894)  max mem: 16413
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.651
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [150]  [  0/160]  eta: 0:20:43  lr: 0.000004  min_lr: 0.000000  loss: 2.0889 (2.0889)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8281 (9.8281)  time: 7.7716 (7.7716 -- 7.7716)  data: 7.2342 (7.2342 -- 7.2342)  max mem: 16413
[2023-08-30 19:26:41,033] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:26:41,033] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:26:41,033] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 19:26:41,033] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [150]  [ 20/160]  eta: 0:02:47  lr: 0.000004  min_lr: 0.000000  loss: 1.7143 (1.8241)  loss_scale: 16384.0000 (15603.8095)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6373 (8.7325)  time: 0.8699 (0.5238 -- 2.8612)  data: 0.2051 (0.0002 -- 2.3339)  max mem: 16413
Epoch: [150]  [ 40/160]  eta: 0:01:59  lr: 0.000004  min_lr: 0.000000  loss: 1.6678 (1.7705)  loss_scale: 16384.0000 (15984.3902)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0489 (9.0908)  time: 0.7766 (0.5363 -- 2.2830)  data: 0.0792 (0.0004 -- 1.1075)  max mem: 16413
Epoch: [150]  [ 60/160]  eta: 0:01:36  lr: 0.000004  min_lr: 0.000000  loss: 1.8532 (1.7743)  loss_scale: 16384.0000 (16115.4098)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4146 (9.0469)  time: 0.9046 (0.5380 -- 3.1314)  data: 0.1911 (0.0004 -- 2.5863)  max mem: 16413
Epoch: [150]  [ 80/160]  eta: 0:01:15  lr: 0.000004  min_lr: 0.000000  loss: 1.7819 (1.7756)  loss_scale: 16384.0000 (16181.7284)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0957 (8.9185)  time: 0.8755 (0.5156 -- 4.6087)  data: 0.0434 (0.0002 -- 0.8452)  max mem: 16413
Epoch: [150]  [100/160]  eta: 0:00:56  lr: 0.000004  min_lr: 0.000000  loss: 1.9082 (1.8060)  loss_scale: 16384.0000 (16221.7822)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5563 (9.0323)  time: 0.9459 (0.5096 -- 3.8452)  data: 0.0010 (0.0003 -- 0.0020)  max mem: 16413
Epoch: [150]  [120/160]  eta: 0:00:36  lr: 0.000004  min_lr: 0.000000  loss: 1.7386 (1.7872)  loss_scale: 16384.0000 (16248.5950)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3886 (9.1271)  time: 0.7739 (0.5185 -- 3.6167)  data: 0.0018 (0.0002 -- 0.0127)  max mem: 16413
[2023-08-30 19:28:33,413] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:28:33,413] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:28:33,414] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:28:33,414] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:28:33,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24131
[2023-08-30 19:28:33,930] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24131
[2023-08-30 19:28:33,930] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:28:33,930] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:28:33,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [150]  [140/160]  eta: 0:00:18  lr: 0.000004  min_lr: 0.000000  loss: 1.8204 (1.7906)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5382 (8.9253)  time: 0.9068 (0.5061 -- 3.7641)  data: 0.0017 (0.0005 -- 0.0065)  max mem: 16413
Epoch: [150]  [159/160]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000000  loss: 1.8029 (1.7930)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4264 (8.9104)  time: 0.6298 (0.4954 -- 2.6662)  data: 0.0007 (0.0002 -- 0.0017)  max mem: 16413
Epoch: [150] Total time: 0:02:20 (0.8808 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000000  loss: 1.8029 (1.7543)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4264 (8.9104)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2684 (0.2684)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2484 (2.2484 -- 2.2484)  data: 2.0379 (2.0379 -- 2.0379)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4037 (0.6966)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4185 (0.1868 -- 2.2484)  data: 0.2028 (0.0006 -- 2.0379)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4625 (0.6431)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2332 (0.1694 -- 0.5657)  data: 0.0286 (0.0001 -- 0.3771)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5557 (0.6992)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.6805)  time: 0.2165 (0.1333 -- 0.5657)  data: 0.0283 (0.0001 -- 0.3771)  max mem: 16413
Val: Total time: 0:00:07 (0.2924 s / it)
* Acc@1 82.365 Acc@5 97.510 loss 0.650
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [151]  [  0/160]  eta: 0:18:42  lr: 0.000004  min_lr: 0.000000  loss: 1.4908 (1.4908)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1910 (7.1910)  time: 7.0133 (7.0133 -- 7.0133)  data: 6.0342 (6.0342 -- 6.0342)  max mem: 16413
Epoch: [151]  [ 20/160]  eta: 0:02:40  lr: 0.000003  min_lr: 0.000000  loss: 1.7524 (1.7110)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5448 (8.8732)  time: 0.8508 (0.5347 -- 3.3124)  data: 0.1734 (0.0005 -- 2.5591)  max mem: 16413
Epoch: [151]  [ 40/160]  eta: 0:02:01  lr: 0.000003  min_lr: 0.000000  loss: 1.8419 (1.7456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7555 (8.5599)  time: 0.8787 (0.5361 -- 2.4483)  data: 0.1403 (0.0005 -- 1.3372)  max mem: 16413
Epoch: [151]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000000  loss: 1.7642 (1.7438)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2820 (8.5936)  time: 0.8964 (0.5246 -- 4.4963)  data: 0.0016 (0.0004 -- 0.0042)  max mem: 16413
Epoch: [151]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.7828 (1.7620)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8699 (8.6935)  time: 0.8893 (0.5161 -- 3.0831)  data: 0.2030 (0.0001 -- 1.6796)  max mem: 16413
[2023-08-30 19:30:34,096] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:30:34,096] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:30:34,097] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:30:34,097] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [151]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.8748 (1.7615)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7939 (8.7214)  time: 0.7887 (0.5263 -- 2.9318)  data: 0.2428 (0.0004 -- 2.3956)  max mem: 16413
Epoch: [151]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.9320 (1.7880)  loss_scale: 32768.0000 (19227.5041)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7926 (8.7473)  time: 0.8459 (0.5252 -- 3.4698)  data: 0.2139 (0.0002 -- 2.1144)  max mem: 16413
Epoch: [151]  [140/160]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 1.6583 (1.7753)  loss_scale: 32768.0000 (21148.1418)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7370 (8.7790)  time: 0.7890 (0.5154 -- 1.8816)  data: 0.2315 (0.0003 -- 1.3465)  max mem: 16413
[2023-08-30 19:31:11,524] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24305
[2023-08-30 19:31:11,524] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24305
[2023-08-30 19:31:11,525] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:31:11,525] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:31:11,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [151]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8543 (1.7854)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3101 (8.8513)  time: 0.7513 (0.4952 -- 2.5551)  data: 0.2176 (0.0002 -- 1.9125)  max mem: 16413
Epoch: [151] Total time: 0:02:19 (0.8733 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8543 (1.7826)  loss_scale: 16384.0000 (20992.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3101 (8.8513)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2764 (0.2764)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3023 (2.3023 -- 2.3023)  data: 2.0952 (2.0952 -- 2.0952)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4205 (0.6994)  acc1: 77.7778 (76.7677)  acc5: 100.0000 (95.9596)  time: 0.4104 (0.1938 -- 2.3023)  data: 0.2008 (0.0006 -- 2.0952)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4596 (0.6453)  acc1: 77.7778 (80.9524)  acc5: 100.0000 (96.2963)  time: 0.2244 (0.1691 -- 0.4393)  data: 0.0215 (0.0001 -- 0.2542)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5559 (0.7027)  acc1: 77.7778 (78.8382)  acc5: 100.0000 (96.2656)  time: 0.2091 (0.1330 -- 0.4393)  data: 0.0211 (0.0001 -- 0.2542)  max mem: 16413
Val: Total time: 0:00:07 (0.2882 s / it)
* Acc@1 81.950 Acc@5 97.303 loss 0.654
Accuracy of the network on the 482 val images: 81.95%
Max accuracy: 82.99%
Epoch: [152]  [  0/160]  eta: 0:22:32  lr: 0.000003  min_lr: 0.000000  loss: 1.8101 (1.8101)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0322 (10.0322)  time: 8.4523 (8.4523 -- 8.4523)  data: 7.9219 (7.9219 -- 7.9219)  max mem: 16413
Epoch: [152]  [ 20/160]  eta: 0:02:43  lr: 0.000003  min_lr: 0.000000  loss: 1.6764 (1.7217)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6315 (8.2365)  time: 0.8009 (0.5157 -- 2.6296)  data: 0.2551 (0.0004 -- 2.1030)  max mem: 16413
Epoch: [152]  [ 40/160]  eta: 0:02:11  lr: 0.000003  min_lr: 0.000000  loss: 1.9296 (1.8212)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2825 (8.6498)  time: 1.0218 (0.5292 -- 4.4895)  data: 0.4763 (0.0003 -- 3.9604)  max mem: 16413
Epoch: [152]  [ 60/160]  eta: 0:01:42  lr: 0.000003  min_lr: 0.000000  loss: 1.6535 (1.7698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2840 (8.7631)  time: 0.8936 (0.5063 -- 5.0705)  data: 0.3555 (0.0003 -- 4.5526)  max mem: 16413
Epoch: [152]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000000  loss: 1.7743 (1.7685)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0538 (9.3187)  time: 0.8039 (0.5128 -- 2.3782)  data: 0.2583 (0.0001 -- 1.8377)  max mem: 16413
[2023-08-30 19:32:50,723] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24406
[2023-08-30 19:32:50,723] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24406
[2023-08-30 19:32:50,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:32:50,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:32:50,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [152]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.8241 (1.7777)  loss_scale: 8192.0000 (15167.3663)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8392 (9.2971)  time: 0.7850 (0.5132 -- 2.8935)  data: 0.2282 (0.0002 -- 2.3333)  max mem: 16413
Epoch: [152]  [120/160]  eta: 0:00:37  lr: 0.000003  min_lr: 0.000000  loss: 1.6764 (1.7507)  loss_scale: 8192.0000 (14014.4132)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6685 (9.2169)  time: 0.9906 (0.5238 -- 3.5568)  data: 0.4422 (0.0002 -- 3.0154)  max mem: 16413
Epoch: [152]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.8762 (1.7548)  loss_scale: 8192.0000 (13188.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6796 (9.2525)  time: 0.8150 (0.5119 -- 4.5905)  data: 0.2787 (0.0002 -- 4.0784)  max mem: 16413
Epoch: [152]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7903 (1.7605)  loss_scale: 8192.0000 (12595.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8929 (9.2458)  time: 0.6914 (0.4951 -- 4.1893)  data: 0.1833 (0.0002 -- 3.6567)  max mem: 16413
Epoch: [152] Total time: 0:02:24 (0.9000 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7903 (1.7499)  loss_scale: 8192.0000 (12595.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8929 (9.2458)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2742 (0.2742)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2049 (2.2049 -- 2.2049)  data: 2.0142 (2.0142 -- 2.0142)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4139 (0.6966)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4107 (0.2021 -- 2.2049)  data: 0.1997 (0.0008 -- 2.0142)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4602 (0.6429)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2263 (0.1730 -- 0.4398)  data: 0.0216 (0.0001 -- 0.2453)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5629 (0.7003)  acc1: 77.7778 (79.2531)  acc5: 100.0000 (96.2656)  time: 0.2123 (0.1326 -- 0.4398)  data: 0.0207 (0.0001 -- 0.2453)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 82.365 Acc@5 97.303 loss 0.652
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [153]  [  0/160]  eta: 0:22:36  lr: 0.000003  min_lr: 0.000000  loss: 1.5540 (1.5540)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8488 (8.8488)  time: 8.4790 (8.4790 -- 8.4790)  data: 7.9250 (7.9250 -- 7.9250)  max mem: 16413
Epoch: [153]  [ 20/160]  eta: 0:02:40  lr: 0.000003  min_lr: 0.000000  loss: 1.6427 (1.6533)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8704 (8.7716)  time: 0.7802 (0.5221 -- 2.7050)  data: 0.1725 (0.0006 -- 2.1688)  max mem: 16413
Epoch: [153]  [ 40/160]  eta: 0:02:04  lr: 0.000003  min_lr: 0.000000  loss: 1.6126 (1.6923)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6862 (8.7349)  time: 0.9279 (0.5298 -- 3.2325)  data: 0.2731 (0.0003 -- 2.7164)  max mem: 16413
[2023-08-30 19:34:54,321] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:34:54,321] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:34:54,321] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 19:34:54,321] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [153]  [ 60/160]  eta: 0:01:39  lr: 0.000003  min_lr: 0.000000  loss: 1.9361 (1.7456)  loss_scale: 8192.0000 (8997.7705)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6435 (8.7631)  time: 0.9141 (0.5197 -- 4.8748)  data: 0.0215 (0.0003 -- 0.4076)  max mem: 16413
Epoch: [153]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.7566 (1.7575)  loss_scale: 16384.0000 (10821.5309)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3265 (8.9495)  time: 0.7679 (0.5132 -- 3.2260)  data: 0.0013 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [153]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.9448 (1.7788)  loss_scale: 16384.0000 (11923.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4450 (8.7682)  time: 0.9675 (0.5127 -- 4.2139)  data: 0.0011 (0.0002 -- 0.0044)  max mem: 16413
Epoch: [153]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7850 (1.7892)  loss_scale: 16384.0000 (12660.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5572 (8.7430)  time: 0.8128 (0.5222 -- 2.9461)  data: 0.1270 (0.0003 -- 2.4122)  max mem: 16413
Epoch: [153]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7500 (1.7797)  loss_scale: 16384.0000 (13188.5390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5489 (8.8103)  time: 0.8768 (0.5267 -- 3.5216)  data: 0.0273 (0.0002 -- 0.5084)  max mem: 16413
Epoch: [153]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.8784 (1.7851)  loss_scale: 16384.0000 (13568.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7658 (8.7576)  time: 0.6490 (0.4940 -- 2.6653)  data: 0.0014 (0.0002 -- 0.0162)  max mem: 16413
Epoch: [153] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.8784 (1.7956)  loss_scale: 16384.0000 (13568.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7658 (8.7576)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2828 (0.2828)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4595 (2.4595 -- 2.4595)  data: 2.2409 (2.2409 -- 2.2409)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4162 (0.6954)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4239 (0.2098 -- 2.4595)  data: 0.2058 (0.0008 -- 2.2409)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4576 (0.6418)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2198 (0.1692 -- 0.3771)  data: 0.0114 (0.0001 -- 0.2025)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5520 (0.6969)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2016 (0.1330 -- 0.3771)  data: 0.0105 (0.0001 -- 0.2025)  max mem: 16413
Val: Total time: 0:00:07 (0.2903 s / it)
* Acc@1 82.365 Acc@5 97.303 loss 0.652
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [154]  [  0/160]  eta: 0:19:42  lr: 0.000003  min_lr: 0.000000  loss: 1.5088 (1.5088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7542 (4.7542)  time: 7.3924 (7.3924 -- 7.3924)  data: 6.8163 (6.8163 -- 6.8163)  max mem: 16413
Epoch: [154]  [ 20/160]  eta: 0:02:38  lr: 0.000003  min_lr: 0.000000  loss: 1.7892 (1.7916)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4952 (9.7558)  time: 0.8180 (0.5210 -- 2.9192)  data: 0.1580 (0.0004 -- 1.6861)  max mem: 16413
[2023-08-30 19:36:55,956] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:36:55,956] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:36:55,956] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:36:55,956] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:37:03,117] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24670
[2023-08-30 19:37:03,117] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24670
[2023-08-30 19:37:03,117] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:37:03,117] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:37:03,117] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [154]  [ 40/160]  eta: 0:02:09  lr: 0.000003  min_lr: 0.000000  loss: 1.7890 (1.7719)  loss_scale: 16384.0000 (19181.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4089 (9.5546)  time: 1.0226 (0.5120 -- 4.2161)  data: 0.4615 (0.0003 -- 3.6786)  max mem: 16413
[2023-08-30 19:37:16,513] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24684
[2023-08-30 19:37:16,513] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24684
[2023-08-30 19:37:16,513] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:37:16,513] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:37:16,513] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [154]  [ 60/160]  eta: 0:01:37  lr: 0.000003  min_lr: 0.000000  loss: 1.7117 (1.7809)  loss_scale: 8192.0000 (15981.1148)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7070 (9.7253)  time: 0.7664 (0.5193 -- 3.1050)  data: 0.2252 (0.0003 -- 2.5853)  max mem: 16413
Epoch: [154]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.8673 (1.7902)  loss_scale: 8192.0000 (14057.8765)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4542 (9.6355)  time: 0.8740 (0.5240 -- 3.1071)  data: 0.3235 (0.0001 -- 2.5751)  max mem: 16413
Epoch: [154]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.9249 (1.7963)  loss_scale: 8192.0000 (12896.3168)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5714 (9.4491)  time: 0.8538 (0.5204 -- 3.5446)  data: 0.3187 (0.0002 -- 3.0390)  max mem: 16413
Epoch: [154]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.7314 (1.7966)  loss_scale: 8192.0000 (12118.7438)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5939 (9.1913)  time: 0.8839 (0.5290 -- 3.5141)  data: 0.3412 (0.0002 -- 2.9839)  max mem: 16413
Epoch: [154]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.8916 (1.8096)  loss_scale: 8192.0000 (11561.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9201 (9.1038)  time: 0.8319 (0.5216 -- 3.8246)  data: 0.2781 (0.0003 -- 3.2972)  max mem: 16413
Epoch: [154]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5072 (1.7839)  loss_scale: 8192.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5152 (9.0996)  time: 0.6590 (0.4947 -- 2.4814)  data: 0.0984 (0.0002 -- 1.9535)  max mem: 16413
Epoch: [154] Total time: 0:02:21 (0.8816 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5072 (1.7636)  loss_scale: 8192.0000 (11161.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5152 (9.0996)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2764 (0.2764)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2690 (2.2690 -- 2.2690)  data: 2.0366 (2.0366 -- 2.0366)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.4019 (0.6916)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4025 (0.1934 -- 2.2690)  data: 0.1870 (0.0009 -- 2.0366)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4502 (0.6394)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2197 (0.1699 -- 0.4147)  data: 0.0164 (0.0001 -- 0.2061)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5536 (0.6943)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2019 (0.1336 -- 0.4147)  data: 0.0156 (0.0001 -- 0.2061)  max mem: 16413
Val: Total time: 0:00:07 (0.2836 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.650
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [155]  [  0/160]  eta: 0:20:37  lr: 0.000003  min_lr: 0.000000  loss: 1.4473 (1.4473)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6772 (10.6772)  time: 7.7315 (7.7315 -- 7.7315)  data: 6.6793 (6.6793 -- 6.6793)  max mem: 16413
[2023-08-30 19:39:16,533] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:39:16,534] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 19:39:16,535] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:39:16,535] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [155]  [ 20/160]  eta: 0:02:46  lr: 0.000003  min_lr: 0.000000  loss: 1.6865 (1.7067)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8369 (8.9210)  time: 0.8588 (0.5274 -- 2.4752)  data: 0.2447 (0.0004 -- 1.9403)  max mem: 16413
Epoch: [155]  [ 40/160]  eta: 0:02:03  lr: 0.000003  min_lr: 0.000000  loss: 1.5695 (1.6504)  loss_scale: 16384.0000 (13786.5366)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9837 (8.2878)  time: 0.8636 (0.5348 -- 2.9755)  data: 0.1785 (0.0002 -- 2.4016)  max mem: 16413
Epoch: [155]  [ 60/160]  eta: 0:01:35  lr: 0.000003  min_lr: 0.000000  loss: 1.9061 (1.7457)  loss_scale: 16384.0000 (14638.1639)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4433 (8.5724)  time: 0.8104 (0.5319 -- 2.6279)  data: 0.1382 (0.0004 -- 1.3798)  max mem: 16413
Epoch: [155]  [ 80/160]  eta: 0:01:13  lr: 0.000003  min_lr: 0.000000  loss: 1.6192 (1.7249)  loss_scale: 16384.0000 (15069.2346)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7435 (8.7162)  time: 0.7908 (0.5350 -- 3.6143)  data: 0.1193 (0.0004 -- 1.4752)  max mem: 16413
Epoch: [155]  [100/160]  eta: 0:00:55  lr: 0.000003  min_lr: 0.000000  loss: 1.7756 (1.7296)  loss_scale: 16384.0000 (15329.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8908 (8.6313)  time: 1.0000 (0.5218 -- 2.9531)  data: 0.4628 (0.0003 -- 2.4332)  max mem: 16413
Epoch: [155]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.6163 (1.7165)  loss_scale: 16384.0000 (15503.8678)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8611 (8.6639)  time: 0.8541 (0.5128 -- 3.9130)  data: 0.3174 (0.0002 -- 3.4120)  max mem: 16413
Epoch: [155]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7890 (1.7251)  loss_scale: 16384.0000 (15628.7092)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8448 (8.6040)  time: 0.8863 (0.5249 -- 2.2009)  data: 0.3407 (0.0004 -- 1.6673)  max mem: 16413
[2023-08-30 19:41:08,501] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:41:08,502] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:41:08,503] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:41:08,503] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:41:16,250] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24951
[2023-08-30 19:41:16,250] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:41:16,250] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24951
[2023-08-30 19:41:16,250] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 19:41:16,250] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [155]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7559 (1.7407)  loss_scale: 16384.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1606 (8.5286)  time: 0.6571 (0.4860 -- 2.2497)  data: 0.1390 (0.0002 -- 1.7268)  max mem: 16413
Epoch: [155] Total time: 0:02:21 (0.8851 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7559 (1.7714)  loss_scale: 16384.0000 (16742.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1606 (8.5286)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2822 (0.2822)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2710 (2.2710 -- 2.2710)  data: 2.0434 (2.0434 -- 2.0434)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3884 (0.6867)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4040 (0.1974 -- 2.2710)  data: 0.1918 (0.0006 -- 2.0434)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4498 (0.6351)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2209 (0.1696 -- 0.5268)  data: 0.0212 (0.0001 -- 0.3335)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5520 (0.6913)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2064 (0.1331 -- 0.5268)  data: 0.0206 (0.0001 -- 0.3335)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 82.780 Acc@5 97.510 loss 0.650
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [156]  [  0/160]  eta: 0:15:35  lr: 0.000003  min_lr: 0.000000  loss: 2.3698 (2.3698)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8863 (7.8863)  time: 5.8482 (5.8482 -- 5.8482)  data: 5.2566 (5.2566 -- 5.2566)  max mem: 16413
Epoch: [156]  [ 20/160]  eta: 0:02:44  lr: 0.000003  min_lr: 0.000000  loss: 1.9333 (1.8932)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6304 (8.9058)  time: 0.9425 (0.5313 -- 2.6397)  data: 0.2423 (0.0007 -- 2.0951)  max mem: 16413
[2023-08-30 19:42:08,983] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24998
[2023-08-30 19:42:08,983] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 24998
[2023-08-30 19:42:08,983] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:42:08,983] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:42:08,983] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 19:42:09,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=25000, skipped=149, lr=[6.733965761740349e-08, 6.733965761740349e-08, 8.978621015653799e-08, 8.978621015653799e-08, 1.1971494687538399e-07, 1.1971494687538399e-07, 1.5961992916717863e-07, 1.5961992916717863e-07, 2.1282657222290486e-07, 2.1282657222290486e-07, 2.8376876296387316e-07, 2.8376876296387316e-07, 3.7835835061849753e-07, 3.7835835061849753e-07, 5.044778008246634e-07, 5.044778008246634e-07, 6.726370677662179e-07, 6.726370677662179e-07, 8.968494236882905e-07, 8.968494236882905e-07, 1.1957992315843872e-06, 1.1957992315843872e-06, 1.5943989754458497e-06, 1.5943989754458497e-06, 2.125865300594466e-06, 2.125865300594466e-06, 2.8344870674592883e-06, 2.8344870674592883e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 19:42:09,507] [INFO] [timer.py:260:stop] epoch=0/micro_step=25000/global_step=25000, RunningAvgSamplesPerSec=17.648707270327293, CurrSamplesPerSec=23.501100075081524, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [156]  [ 40/160]  eta: 0:02:08  lr: 0.000003  min_lr: 0.000000  loss: 1.7047 (1.7990)  loss_scale: 16384.0000 (15784.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9013 (9.0823)  time: 0.9658 (0.5206 -- 4.8958)  data: 0.0011 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [156]  [ 60/160]  eta: 0:01:39  lr: 0.000003  min_lr: 0.000000  loss: 1.7886 (1.8142)  loss_scale: 8192.0000 (13295.2131)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6429 (9.2258)  time: 0.8406 (0.5198 -- 2.2218)  data: 0.1318 (0.0003 -- 1.4898)  max mem: 16413
Epoch: [156]  [ 80/160]  eta: 0:01:15  lr: 0.000003  min_lr: 0.000000  loss: 1.7560 (1.7957)  loss_scale: 8192.0000 (12035.1605)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9052 (9.0983)  time: 0.7962 (0.5253 -- 2.1882)  data: 0.0738 (0.0004 -- 0.7963)  max mem: 16413
Epoch: [156]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.6821 (1.7740)  loss_scale: 8192.0000 (11274.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1669 (9.2695)  time: 0.9146 (0.5313 -- 3.3277)  data: 0.2888 (0.0005 -- 2.7994)  max mem: 16413
Epoch: [156]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.6926 (1.7668)  loss_scale: 8192.0000 (10764.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4640 (9.2185)  time: 0.8154 (0.5176 -- 3.4749)  data: 0.2656 (0.0004 -- 2.9566)  max mem: 16413
Epoch: [156]  [140/160]  eta: 0:00:18  lr: 0.000003  min_lr: 0.000000  loss: 1.7773 (1.7759)  loss_scale: 8192.0000 (10399.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5528 (9.2028)  time: 0.8945 (0.5130 -- 3.9422)  data: 0.3553 (0.0005 -- 3.4117)  max mem: 16413
Epoch: [156]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.6304 (1.7606)  loss_scale: 8192.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3456 (9.1558)  time: 0.6840 (0.4947 -- 3.4717)  data: 0.1701 (0.0002 -- 2.9418)  max mem: 16413
Epoch: [156] Total time: 0:02:22 (0.8901 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.6304 (1.7646)  loss_scale: 8192.0000 (10137.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3456 (9.1558)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2797 (0.2797)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2938 (2.2938 -- 2.2938)  data: 2.0758 (2.0758 -- 2.0758)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3881 (0.6884)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4109 (0.2013 -- 2.2938)  data: 0.1969 (0.0007 -- 2.0758)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4490 (0.6371)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2231 (0.1698 -- 0.4887)  data: 0.0218 (0.0001 -- 0.2981)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5527 (0.6935)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2079 (0.1331 -- 0.4887)  data: 0.0213 (0.0001 -- 0.2981)  max mem: 16413
Val: Total time: 0:00:07 (0.2866 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.650
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [157]  [  0/160]  eta: 0:18:46  lr: 0.000003  min_lr: 0.000000  loss: 1.6384 (1.6384)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6248 (9.6248)  time: 7.0436 (7.0436 -- 7.0436)  data: 5.6184 (5.6184 -- 5.6184)  max mem: 16413
[2023-08-30 19:44:09,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:44:09,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 19:44:09,238] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:44:09,238] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [157]  [ 20/160]  eta: 0:02:45  lr: 0.000003  min_lr: 0.000000  loss: 1.7776 (1.7934)  loss_scale: 16384.0000 (13653.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7106 (9.6105)  time: 0.8910 (0.5057 -- 4.3596)  data: 0.2121 (0.0007 -- 2.6931)  max mem: 16413
Epoch: [157]  [ 40/160]  eta: 0:02:09  lr: 0.000003  min_lr: 0.000000  loss: 1.6823 (1.7609)  loss_scale: 16384.0000 (14985.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5067 (9.0314)  time: 0.9677 (0.5238 -- 4.0719)  data: 0.0392 (0.0003 -- 0.7588)  max mem: 16413
Epoch: [157]  [ 60/160]  eta: 0:01:40  lr: 0.000003  min_lr: 0.000000  loss: 1.6890 (1.7669)  loss_scale: 16384.0000 (15443.9344)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8824 (9.0055)  time: 0.8544 (0.5213 -- 4.7486)  data: 0.1462 (0.0004 -- 1.9940)  max mem: 16413
Epoch: [157]  [ 80/160]  eta: 0:01:17  lr: 0.000003  min_lr: 0.000000  loss: 1.7035 (1.7732)  loss_scale: 16384.0000 (15676.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6742 (8.9000)  time: 0.8546 (0.5284 -- 3.2038)  data: 0.0826 (0.0002 -- 1.3980)  max mem: 16413
Epoch: [157]  [100/160]  eta: 0:00:56  lr: 0.000003  min_lr: 0.000000  loss: 1.8344 (1.7864)  loss_scale: 16384.0000 (15816.2376)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7605 (8.9765)  time: 0.8316 (0.5336 -- 4.6086)  data: 0.0690 (0.0004 -- 0.7230)  max mem: 16413
Epoch: [157]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.8005 (1.7880)  loss_scale: 16384.0000 (15910.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3692 (8.9613)  time: 0.8102 (0.5285 -- 2.4254)  data: 0.0440 (0.0005 -- 0.8537)  max mem: 16413
[2023-08-30 19:46:00,988] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:46:00,988] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:46:00,991] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:46:00,992] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [157]  [140/160]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 1.8653 (1.7820)  loss_scale: 16384.0000 (16674.4965)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8979 (9.1423)  time: 0.7714 (0.5162 -- 3.1628)  data: 0.1264 (0.0005 -- 1.7487)  max mem: 16413
Epoch: [157]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5737 (1.7646)  loss_scale: 32768.0000 (18585.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5171 (9.0217)  time: 0.7277 (0.4973 -- 1.7256)  data: 0.0579 (0.0002 -- 1.1420)  max mem: 16413
Epoch: [157] Total time: 0:02:20 (0.8793 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.5737 (1.7746)  loss_scale: 32768.0000 (18585.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5171 (9.0217)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2809 (0.2809)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2545 (2.2545 -- 2.2545)  data: 2.0255 (2.0255 -- 2.0255)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3889 (0.6905)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4232 (0.1979 -- 2.2545)  data: 0.2070 (0.0007 -- 2.0255)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4392 (0.6376)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2273 (0.1690 -- 0.4740)  data: 0.0220 (0.0001 -- 0.2425)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5545 (0.6942)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2126 (0.1333 -- 0.4740)  data: 0.0218 (0.0001 -- 0.2425)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.650
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [158]  [  0/160]  eta: 0:18:00  lr: 0.000003  min_lr: 0.000000  loss: 1.5810 (1.5810)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3720 (14.3720)  time: 6.7511 (6.7511 -- 6.7511)  data: 6.0589 (6.0589 -- 6.0589)  max mem: 16413
[2023-08-30 19:46:35,794] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25284
[2023-08-30 19:46:35,794] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25284
[2023-08-30 19:46:35,794] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:46:35,794] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:46:35,794] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [158]  [ 20/160]  eta: 0:02:36  lr: 0.000003  min_lr: 0.000000  loss: 1.8572 (1.8997)  loss_scale: 16384.0000 (19504.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9407 (9.4157)  time: 0.8398 (0.5280 -- 3.6121)  data: 0.1137 (0.0003 -- 1.4190)  max mem: 16413
Epoch: [158]  [ 40/160]  eta: 0:02:04  lr: 0.000003  min_lr: 0.000000  loss: 1.8930 (1.9189)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5773 (9.6575)  time: 0.9457 (0.5305 -- 3.1334)  data: 0.1364 (0.0003 -- 1.4940)  max mem: 16413
Epoch: [158]  [ 60/160]  eta: 0:01:33  lr: 0.000003  min_lr: 0.000000  loss: 1.9209 (1.9172)  loss_scale: 16384.0000 (17458.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2020 (9.2700)  time: 0.7347 (0.5374 -- 3.5674)  data: 0.0462 (0.0004 -- 0.8961)  max mem: 16413
Epoch: [158]  [ 80/160]  eta: 0:01:16  lr: 0.000003  min_lr: 0.000000  loss: 1.8414 (1.8932)  loss_scale: 16384.0000 (17193.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5480 (9.3612)  time: 1.0005 (0.5337 -- 3.9366)  data: 0.0966 (0.0006 -- 0.8656)  max mem: 16413
Epoch: [158]  [100/160]  eta: 0:00:54  lr: 0.000003  min_lr: 0.000000  loss: 1.6020 (1.8330)  loss_scale: 16384.0000 (17032.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7126 (9.2696)  time: 0.7626 (0.5281 -- 3.4500)  data: 0.0014 (0.0004 -- 0.0050)  max mem: 16413
Epoch: [158]  [120/160]  eta: 0:00:36  lr: 0.000003  min_lr: 0.000000  loss: 1.5623 (1.8061)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4876 (9.1788)  time: 0.9470 (0.5336 -- 3.7343)  data: 0.4005 (0.0003 -- 3.2127)  max mem: 16413
[2023-08-30 19:48:27,729] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:48:27,730] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:48:27,730] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:48:27,730] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [158]  [140/160]  eta: 0:00:17  lr: 0.000003  min_lr: 0.000000  loss: 1.7884 (1.8065)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1769 (9.1533)  time: 0.7351 (0.5272 -- 2.8108)  data: 0.1854 (0.0004 -- 2.2840)  max mem: 16413
Epoch: [158]  [159/160]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000000  loss: 1.7884 (1.8024)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8293 (9.1629)  time: 0.7554 (0.4964 -- 3.6534)  data: 0.2440 (0.0002 -- 3.1497)  max mem: 16413
Epoch: [158] Total time: 0:02:20 (0.8794 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000000  loss: 1.7884 (1.7767)  loss_scale: 32768.0000 (19558.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8293 (9.1629)
Val:  [ 0/27]  eta: 0:00:57  loss: 0.2782 (0.2782)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1477 (2.1477 -- 2.1477)  data: 1.9405 (1.9405 -- 1.9405)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3964 (0.6936)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4010 (0.1977 -- 2.1477)  data: 0.1787 (0.0005 -- 1.9405)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4370 (0.6388)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2255 (0.1700 -- 0.4368)  data: 0.0135 (0.0001 -- 0.1993)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5513 (0.6938)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2034 (0.1337 -- 0.4368)  data: 0.0124 (0.0001 -- 0.1993)  max mem: 16413
Val: Total time: 0:00:07 (0.2831 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.650
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [159]  [  0/160]  eta: 0:19:22  lr: 0.000003  min_lr: 0.000000  loss: 1.8809 (1.8809)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5925 (9.5925)  time: 7.2659 (7.2659 -- 7.2659)  data: 5.5958 (5.5958 -- 5.5958)  max mem: 16413
Epoch: [159]  [ 20/160]  eta: 0:02:45  lr: 0.000002  min_lr: 0.000000  loss: 1.6935 (1.7521)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0366 (9.8662)  time: 0.8765 (0.5246 -- 2.7009)  data: 0.0063 (0.0005 -- 0.0785)  max mem: 16413
[2023-08-30 19:49:33,853] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25477
[2023-08-30 19:49:33,853] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25477
[2023-08-30 19:49:33,853] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:49:33,853] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:49:33,853] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [159]  [ 40/160]  eta: 0:02:01  lr: 0.000002  min_lr: 0.000000  loss: 1.8138 (1.7501)  loss_scale: 32768.0000 (31169.5610)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1708 (9.4502)  time: 0.8302 (0.5279 -- 1.7735)  data: 0.0916 (0.0006 -- 0.5820)  max mem: 16413
Epoch: [159]  [ 60/160]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000000  loss: 1.8020 (1.7667)  loss_scale: 16384.0000 (26321.8361)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6460 (9.3342)  time: 0.9223 (0.5244 -- 3.7135)  data: 0.0677 (0.0005 -- 0.8573)  max mem: 16413
Epoch: [159]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.6743 (1.7252)  loss_scale: 16384.0000 (23868.0494)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8388 (9.2552)  time: 0.8058 (0.5311 -- 2.9875)  data: 0.1146 (0.0004 -- 1.2561)  max mem: 16413
Epoch: [159]  [100/160]  eta: 0:00:54  lr: 0.000002  min_lr: 0.000000  loss: 1.4063 (1.7103)  loss_scale: 16384.0000 (22386.0594)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6890 (9.0057)  time: 0.8111 (0.5222 -- 3.2916)  data: 0.2271 (0.0007 -- 2.7349)  max mem: 16413
Epoch: [159]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8172 (1.7207)  loss_scale: 16384.0000 (21393.9835)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0970 (8.8414)  time: 0.9157 (0.5261 -- 3.9894)  data: 0.3612 (0.0003 -- 3.4364)  max mem: 16413
Epoch: [159]  [140/160]  eta: 0:00:17  lr: 0.000002  min_lr: 0.000000  loss: 1.7281 (1.7206)  loss_scale: 16384.0000 (20683.3475)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3660 (8.9588)  time: 0.7550 (0.5220 -- 2.2509)  data: 0.1113 (0.0008 -- 1.7275)  max mem: 16413
Epoch: [159]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8019 (1.7295)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0324 (8.9615)  time: 0.7895 (0.4957 -- 4.5703)  data: 0.0033 (0.0003 -- 0.0451)  max mem: 16413
Epoch: [159] Total time: 0:02:20 (0.8804 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8019 (1.7584)  loss_scale: 16384.0000 (20172.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0324 (8.9615)
[2023-08-30 19:51:16,185] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-159 is about to be saved!
[2023-08-30 19:51:16,187] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
[2023-08-30 19:51:16,190] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt
[2023-08-30 19:51:16,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt...
[2023-08-30 19:51:17,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-159/mp_rank_00_model_states.pt.
[2023-08-30 19:51:17,241] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-159 is ready now!
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2818 (0.2818)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2149 (2.2149 -- 2.2149)  data: 2.0154 (2.0154 -- 2.0154)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3971 (0.6929)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4281 (0.2095 -- 2.2149)  data: 0.2069 (0.0006 -- 2.0154)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4334 (0.6373)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2277 (0.1712 -- 0.4775)  data: 0.0152 (0.0001 -- 0.2465)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5468 (0.6922)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2094 (0.1330 -- 0.4775)  data: 0.0149 (0.0001 -- 0.2465)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 82.365 Acc@5 97.303 loss 0.649
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [160]  [  0/160]  eta: 0:21:40  lr: 0.000002  min_lr: 0.000000  loss: 2.4007 (2.4007)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5392 (6.5392)  time: 8.1261 (8.1261 -- 8.1261)  data: 6.3576 (6.3576 -- 6.3576)  max mem: 16413
[2023-08-30 19:51:36,508] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:51:36,508] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:51:36,509] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:51:36,509] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [160]  [ 20/160]  eta: 0:02:46  lr: 0.000002  min_lr: 0.000000  loss: 1.7624 (1.8172)  loss_scale: 32768.0000 (28086.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8999 (9.7666)  time: 0.8410 (0.5262 -- 3.8485)  data: 0.2514 (0.0003 -- 3.2833)  max mem: 16413
Epoch: [160]  [ 40/160]  eta: 0:02:06  lr: 0.000002  min_lr: 0.000000  loss: 1.7307 (1.7577)  loss_scale: 32768.0000 (30370.3415)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1212 (9.3567)  time: 0.9181 (0.5324 -- 3.9603)  data: 0.3689 (0.0001 -- 3.4153)  max mem: 16413
[2023-08-30 19:52:16,090] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25650
[2023-08-30 19:52:16,090] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25650
[2023-08-30 19:52:16,090] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:52:16,090] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:52:16,090] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [160]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8123 (1.7493)  loss_scale: 16384.0000 (28201.9672)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8764 (9.1330)  time: 0.8158 (0.5111 -- 3.7140)  data: 0.1741 (0.0002 -- 2.1038)  max mem: 16413
Epoch: [160]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.6172 (1.7258)  loss_scale: 16384.0000 (25283.9506)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9828 (9.1521)  time: 0.9049 (0.5260 -- 3.6116)  data: 0.2606 (0.0002 -- 2.5605)  max mem: 16413
Epoch: [160]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.6234 (1.7252)  loss_scale: 16384.0000 (23521.5842)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5085 (9.1740)  time: 0.8680 (0.5073 -- 4.3504)  data: 0.1555 (0.0003 -- 2.2139)  max mem: 16413
[2023-08-30 19:53:13,963] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25717
[2023-08-30 19:53:13,963] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25717
[2023-08-30 19:53:13,963] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:53:13,963] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:53:13,963] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [160]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8259 (1.7496)  loss_scale: 16384.0000 (22071.0083)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9302 (9.2132)  time: 0.8713 (0.5137 -- 3.7742)  data: 0.0082 (0.0002 -- 0.1395)  max mem: 16413
Epoch: [160]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8097 (1.7642)  loss_scale: 8192.0000 (20102.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5596 (9.2559)  time: 0.7748 (0.5191 -- 3.2058)  data: 0.0015 (0.0001 -- 0.0074)  max mem: 16413
Epoch: [160]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8473 (1.7736)  loss_scale: 8192.0000 (18688.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5329 (9.2963)  time: 0.6745 (0.4947 -- 3.2114)  data: 0.0010 (0.0002 -- 0.0051)  max mem: 16413
Epoch: [160] Total time: 0:02:20 (0.8811 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8473 (1.7815)  loss_scale: 8192.0000 (18688.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5329 (9.2963)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2848 (0.2848)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2977 (2.2977 -- 2.2977)  data: 2.0721 (2.0721 -- 2.0721)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4038 (0.6996)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4133 (0.1929 -- 2.2977)  data: 0.1927 (0.0004 -- 2.0721)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4358 (0.6397)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2258 (0.1702 -- 0.5340)  data: 0.0199 (0.0001 -- 0.3459)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5460 (0.6958)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2102 (0.1337 -- 0.5340)  data: 0.0194 (0.0001 -- 0.3459)  max mem: 16413
Val: Total time: 0:00:07 (0.2891 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.651
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [161]  [  0/160]  eta: 0:19:21  lr: 0.000002  min_lr: 0.000000  loss: 2.1834 (2.1834)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8973 (8.8973)  time: 7.2586 (7.2586 -- 7.2586)  data: 5.3938 (5.3938 -- 5.3938)  max mem: 16413
Epoch: [161]  [ 20/160]  eta: 0:02:41  lr: 0.000002  min_lr: 0.000000  loss: 1.4826 (1.5321)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5171 (8.1872)  time: 0.8507 (0.5173 -- 4.1978)  data: 0.3059 (0.0004 -- 3.6822)  max mem: 16413
Epoch: [161]  [ 40/160]  eta: 0:02:10  lr: 0.000002  min_lr: 0.000000  loss: 1.7467 (1.6350)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7475 (8.7467)  time: 1.0161 (0.5287 -- 4.4790)  data: 0.4714 (0.0002 -- 3.9486)  max mem: 16413
Epoch: [161]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7631 (1.6805)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0448 (8.5384)  time: 0.7273 (0.5367 -- 2.9342)  data: 0.1708 (0.0002 -- 2.4150)  max mem: 16413
Epoch: [161]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.8230 (1.7064)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3519 (8.5923)  time: 0.8445 (0.5198 -- 3.5475)  data: 0.1643 (0.0004 -- 1.1566)  max mem: 16413
[2023-08-30 19:55:13,194] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:55:13,194] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 19:55:13,196] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:55:13,197] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [161]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8599 (1.7391)  loss_scale: 16384.0000 (9408.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2582 (8.5532)  time: 0.8649 (0.5242 -- 4.6201)  data: 0.0431 (0.0003 -- 0.8300)  max mem: 16413
Epoch: [161]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8440 (1.7421)  loss_scale: 16384.0000 (10561.5868)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2258 (8.5891)  time: 0.9607 (0.5122 -- 5.5450)  data: 0.0015 (0.0003 -- 0.0063)  max mem: 16413
Epoch: [161]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.6731 (1.7428)  loss_scale: 16384.0000 (11387.4610)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0581 (8.6380)  time: 0.7450 (0.5197 -- 2.5869)  data: 0.0014 (0.0004 -- 0.0025)  max mem: 16413
Epoch: [161]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6839 (1.7395)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6144 (8.6184)  time: 0.6871 (0.4944 -- 2.6375)  data: 0.0007 (0.0002 -- 0.0021)  max mem: 16413
Epoch: [161] Total time: 0:02:20 (0.8790 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6839 (1.7747)  loss_scale: 16384.0000 (11980.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6144 (8.6184)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2891 (0.2891)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4033 (2.4033 -- 2.4033)  data: 2.1486 (2.1486 -- 2.1486)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4013 (0.6978)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4172 (0.2040 -- 2.4033)  data: 0.1964 (0.0005 -- 2.1486)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4475 (0.6397)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2150 (0.1688 -- 0.3164)  data: 0.0088 (0.0001 -- 0.1087)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5473 (0.6946)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.1983 (0.1326 -- 0.3164)  data: 0.0084 (0.0001 -- 0.1087)  max mem: 16413
Val: Total time: 0:00:07 (0.2846 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.651
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [162]  [  0/160]  eta: 0:18:52  lr: 0.000002  min_lr: 0.000000  loss: 1.7550 (1.7550)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2432 (9.2432)  time: 7.0782 (7.0782 -- 7.0782)  data: 6.5536 (6.5536 -- 6.5536)  max mem: 16413
Epoch: [162]  [ 20/160]  eta: 0:02:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7501 (1.7560)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5263 (8.5042)  time: 0.8293 (0.5178 -- 2.2597)  data: 0.2720 (0.0004 -- 1.7275)  max mem: 16413
Epoch: [162]  [ 40/160]  eta: 0:02:03  lr: 0.000002  min_lr: 0.000000  loss: 1.6126 (1.7355)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9195 (8.5542)  time: 0.9250 (0.5231 -- 2.6146)  data: 0.3558 (0.0002 -- 2.0843)  max mem: 16413
[2023-08-30 19:57:14,443] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:57:14,443] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 19:57:14,444] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 19:57:14,445] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [162]  [ 60/160]  eta: 0:01:35  lr: 0.000002  min_lr: 0.000000  loss: 1.7162 (1.7269)  loss_scale: 16384.0000 (18264.1311)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0051 (8.3814)  time: 0.7961 (0.5294 -- 2.6539)  data: 0.2492 (0.0003 -- 2.1457)  max mem: 16413
[2023-08-30 19:57:35,545] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25997
[2023-08-30 19:57:35,545] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 25997
[2023-08-30 19:57:35,545] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:57:35,545] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 19:57:35,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 19:57:36,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=26000, skipped=154, lr=[5.031320553422787e-08, 5.031320553422787e-08, 6.708427404563716e-08, 6.708427404563716e-08, 8.944569872751621e-08, 8.944569872751621e-08, 1.192609316366883e-07, 1.192609316366883e-07, 1.590145755155844e-07, 1.590145755155844e-07, 2.1201943402077917e-07, 2.1201943402077917e-07, 2.8269257869437223e-07, 2.8269257869437223e-07, 3.7692343825916296e-07, 3.7692343825916296e-07, 5.025645843455506e-07, 5.025645843455506e-07, 6.700861124607342e-07, 6.700861124607342e-07, 8.934481499476456e-07, 8.934481499476456e-07, 1.1912641999301942e-06, 1.1912641999301942e-06, 1.5883522665735923e-06, 1.5883522665735923e-06, 2.117803022098123e-06, 2.117803022098123e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 19:57:36,643] [INFO] [timer.py:260:stop] epoch=0/micro_step=26000/global_step=26000, RunningAvgSamplesPerSec=17.654244061634884, CurrSamplesPerSec=22.82768676885649, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [162]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 1.6464 (1.7171)  loss_scale: 32768.0000 (21036.2469)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1480 (8.4053)  time: 0.8793 (0.5112 -- 3.1559)  data: 0.3337 (0.0008 -- 2.6174)  max mem: 16413
Epoch: [162]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.7661 (1.7206)  loss_scale: 16384.0000 (20115.0099)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6854 (8.4415)  time: 0.8607 (0.5276 -- 3.2931)  data: 0.3122 (0.0005 -- 2.7733)  max mem: 16413
Epoch: [162]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7523 (1.7326)  loss_scale: 16384.0000 (19498.3140)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6578 (8.6190)  time: 0.8389 (0.5258 -- 3.5557)  data: 0.2825 (0.0003 -- 3.0142)  max mem: 16413
Epoch: [162]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7566 (1.7368)  loss_scale: 16384.0000 (19056.5674)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7551 (9.0217)  time: 0.8682 (0.5351 -- 2.5307)  data: 0.2360 (0.0008 -- 2.0035)  max mem: 16413
[2023-08-30 19:58:39,607] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26072
[2023-08-30 19:58:39,607] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26072
[2023-08-30 19:58:39,607] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:58:39,607] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 19:58:39,607] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [162]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.6277 (1.7332)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0044 (9.0215)  time: 0.7316 (0.4911 -- 2.7875)  data: 0.0270 (0.0002 -- 0.5235)  max mem: 16413
Epoch: [162] Total time: 0:02:21 (0.8822 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.6277 (1.7634)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0044 (9.0215)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2820 (0.2820)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2364 (2.2364 -- 2.2364)  data: 2.0076 (2.0076 -- 2.0076)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3958 (0.6985)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4097 (0.1902 -- 2.2364)  data: 0.1951 (0.0007 -- 2.0076)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4425 (0.6385)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2251 (0.1706 -- 0.3888)  data: 0.0164 (0.0001 -- 0.1641)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5465 (0.6941)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2093 (0.1334 -- 0.3888)  data: 0.0161 (0.0001 -- 0.1641)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 82.988 Acc@5 97.303 loss 0.649
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [163]  [  0/160]  eta: 0:21:56  lr: 0.000002  min_lr: 0.000000  loss: 0.9638 (0.9638)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.7579 (14.7579)  time: 8.2298 (8.2298 -- 8.2298)  data: 5.5699 (5.5699 -- 5.5699)  max mem: 16413
Epoch: [163]  [ 20/160]  eta: 0:02:46  lr: 0.000002  min_lr: 0.000000  loss: 1.7177 (1.7204)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7364 (9.6892)  time: 0.8374 (0.5211 -- 3.5735)  data: 0.0930 (0.0003 -- 1.0396)  max mem: 16413
Epoch: [163]  [ 40/160]  eta: 0:02:04  lr: 0.000002  min_lr: 0.000000  loss: 1.7510 (1.7134)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6287 (9.7331)  time: 0.8722 (0.5260 -- 2.7750)  data: 0.2752 (0.0004 -- 2.1967)  max mem: 16413
Epoch: [163]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.5446 (1.7008)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0577 (9.5131)  time: 0.8082 (0.5241 -- 1.9730)  data: 0.1587 (0.0007 -- 1.4336)  max mem: 16413
Epoch: [163]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.6375 (1.6922)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1490 (9.1014)  time: 0.8724 (0.5228 -- 3.0776)  data: 0.3261 (0.0001 -- 2.5527)  max mem: 16413
Epoch: [163]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.8315 (1.7171)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8912 (8.9946)  time: 0.9409 (0.5118 -- 3.4307)  data: 0.2871 (0.0006 -- 2.9099)  max mem: 16413
Epoch: [163]  [120/160]  eta: 0:00:36  lr: 0.000002  min_lr: 0.000000  loss: 1.8499 (1.7446)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9460 (9.2944)  time: 0.8354 (0.5147 -- 3.5783)  data: 0.2923 (0.0002 -- 3.0566)  max mem: 16413
[2023-08-30 20:00:43,325] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:00:43,325] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:00:43,326] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:00:43,326] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [163]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7298 (1.7602)  loss_scale: 16384.0000 (9353.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4530 (9.2422)  time: 0.9039 (0.5175 -- 3.8146)  data: 0.3546 (0.0004 -- 3.2843)  max mem: 16413
Epoch: [163]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8907 (1.7639)  loss_scale: 16384.0000 (10188.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0452 (9.2952)  time: 0.7108 (0.4949 -- 2.0293)  data: 0.1947 (0.0002 -- 1.5338)  max mem: 16413
Epoch: [163] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8907 (1.7750)  loss_scale: 16384.0000 (10188.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0452 (9.2952)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2831 (0.2831)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3562 (2.3562 -- 2.3562)  data: 2.1082 (2.1082 -- 2.1082)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3948 (0.6970)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4193 (0.1952 -- 2.3562)  data: 0.2045 (0.0007 -- 2.1082)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4437 (0.6375)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2235 (0.1691 -- 0.3944)  data: 0.0176 (0.0001 -- 0.2072)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5456 (0.6938)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.2117 (0.1328 -- 0.3944)  data: 0.0172 (0.0001 -- 0.2072)  max mem: 16413
Val: Total time: 0:00:07 (0.2893 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.649
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [164]  [  0/160]  eta: 0:19:04  lr: 0.000002  min_lr: 0.000000  loss: 2.3401 (2.3401)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4600 (9.4600)  time: 7.1559 (7.1559 -- 7.1559)  data: 6.6219 (6.6219 -- 6.6219)  max mem: 16413
Epoch: [164]  [ 20/160]  eta: 0:02:38  lr: 0.000002  min_lr: 0.000000  loss: 1.7408 (1.8222)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0983 (8.7277)  time: 0.8309 (0.5313 -- 3.0181)  data: 0.2021 (0.0004 -- 1.5391)  max mem: 16413
Epoch: [164]  [ 40/160]  eta: 0:02:08  lr: 0.000002  min_lr: 0.000000  loss: 1.8498 (1.8129)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2578 (8.7534)  time: 1.0145 (0.5248 -- 4.1388)  data: 0.2082 (0.0004 -- 1.6216)  max mem: 16413
Epoch: [164]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8444 (1.7956)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5649 (8.7477)  time: 0.7753 (0.5152 -- 3.2048)  data: 0.0012 (0.0003 -- 0.0027)  max mem: 16413
[2023-08-30 20:02:40,412] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26320
[2023-08-30 20:02:40,412] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26320
[2023-08-30 20:02:40,412] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:02:40,412] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:02:40,412] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [164]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000000  loss: 1.7462 (1.7966)  loss_scale: 16384.0000 (16282.8642)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3395 (8.6520)  time: 0.9207 (0.5166 -- 3.0946)  data: 0.0769 (0.0003 -- 0.8844)  max mem: 16413
Epoch: [164]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.7621 (1.7863)  loss_scale: 8192.0000 (14680.7129)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6250 (8.8897)  time: 0.8471 (0.5227 -- 3.3826)  data: 0.2993 (0.0003 -- 2.8476)  max mem: 16413
Epoch: [164]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7282 (1.7837)  loss_scale: 8192.0000 (13608.1983)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3103 (8.8996)  time: 0.9287 (0.5300 -- 3.3139)  data: 0.0430 (0.0003 -- 0.8313)  max mem: 16413
Epoch: [164]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8860 (1.7936)  loss_scale: 8192.0000 (12839.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2819 (8.7399)  time: 0.8514 (0.5156 -- 4.8268)  data: 0.0011 (0.0003 -- 0.0045)  max mem: 16413
Epoch: [164]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7869 (1.7841)  loss_scale: 8192.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8024 (8.7596)  time: 0.6083 (0.4966 -- 2.1560)  data: 0.0006 (0.0002 -- 0.0014)  max mem: 16413
Epoch: [164] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7869 (1.7700)  loss_scale: 8192.0000 (12288.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8024 (8.7596)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2818 (0.2818)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2484 (2.2484 -- 2.2484)  data: 2.0287 (2.0287 -- 2.0287)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3945 (0.6929)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4073 (0.1987 -- 2.2484)  data: 0.1857 (0.0005 -- 2.0287)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4396 (0.6372)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2237 (0.1731 -- 0.4233)  data: 0.0148 (0.0001 -- 0.2269)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5508 (0.6929)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2034 (0.1328 -- 0.4233)  data: 0.0143 (0.0001 -- 0.2269)  max mem: 16413
Val: Total time: 0:00:07 (0.2857 s / it)
* Acc@1 82.573 Acc@5 97.303 loss 0.649
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [165]  [  0/160]  eta: 0:19:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9411 (1.9411)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0568 (10.0568)  time: 7.3569 (7.3569 -- 7.3569)  data: 6.7934 (6.7934 -- 6.7934)  max mem: 16413
Epoch: [165]  [ 20/160]  eta: 0:02:39  lr: 0.000002  min_lr: 0.000000  loss: 1.9176 (1.7846)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5081 (9.1853)  time: 0.8288 (0.5325 -- 3.4245)  data: 0.2741 (0.0007 -- 2.8791)  max mem: 16413
Epoch: [165]  [ 40/160]  eta: 0:02:07  lr: 0.000002  min_lr: 0.000000  loss: 1.7485 (1.7626)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1810 (9.1799)  time: 0.9831 (0.5247 -- 4.1605)  data: 0.3328 (0.0002 -- 3.6237)  max mem: 16413
[2023-08-30 20:04:44,137] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:04:44,137] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:04:44,137] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:04:44,137] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [165]  [ 60/160]  eta: 0:01:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8728 (1.7717)  loss_scale: 16384.0000 (9803.5410)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3377 (9.0914)  time: 0.7881 (0.5250 -- 3.7849)  data: 0.1039 (0.0004 -- 1.7015)  max mem: 16413
Epoch: [165]  [ 80/160]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000000  loss: 1.8286 (1.7793)  loss_scale: 16384.0000 (11428.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3701 (9.2568)  time: 0.9185 (0.5270 -- 3.4679)  data: 0.0026 (0.0002 -- 0.0154)  max mem: 16413
Epoch: [165]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8924 (1.7883)  loss_scale: 16384.0000 (12409.6634)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2497 (9.1798)  time: 0.7704 (0.5287 -- 3.5257)  data: 0.0783 (0.0002 -- 1.5274)  max mem: 16413
Epoch: [165]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.8112 (1.7909)  loss_scale: 16384.0000 (13066.5785)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0024 (9.0692)  time: 0.9576 (0.5261 -- 3.4771)  data: 0.3486 (0.0003 -- 2.9330)  max mem: 16413
Epoch: [165]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7856 (1.7928)  loss_scale: 16384.0000 (13537.1348)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9510 (9.1301)  time: 0.8302 (0.5221 -- 4.9453)  data: 0.2881 (0.0006 -- 4.4185)  max mem: 16413
[2023-08-30 20:06:08,167] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26547
[2023-08-30 20:06:08,167] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26547
[2023-08-30 20:06:08,167] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:06:08,167] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:06:08,167] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [165]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8855 (1.7840)  loss_scale: 8192.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1650 (9.2279)  time: 0.6756 (0.4965 -- 3.6927)  data: 0.1598 (0.0002 -- 3.1835)  max mem: 16413
Epoch: [165] Total time: 0:02:21 (0.8868 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8855 (1.8156)  loss_scale: 8192.0000 (13209.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1650 (9.2279)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2803 (0.2803)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2797 (2.2797 -- 2.2797)  data: 2.0505 (2.0505 -- 2.0505)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3986 (0.6959)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4260 (0.1953 -- 2.2797)  data: 0.2096 (0.0005 -- 2.0505)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4359 (0.6378)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2239 (0.1713 -- 0.4850)  data: 0.0209 (0.0001 -- 0.2459)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5453 (0.6935)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2092 (0.1325 -- 0.4850)  data: 0.0206 (0.0001 -- 0.2459)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 82.365 Acc@5 97.303 loss 0.650
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [166]  [  0/160]  eta: 0:22:05  lr: 0.000002  min_lr: 0.000000  loss: 1.7474 (1.7474)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4296 (8.4296)  time: 8.2832 (8.2832 -- 8.2832)  data: 7.7669 (7.7669 -- 7.7669)  max mem: 16413
Epoch: [166]  [ 20/160]  eta: 0:03:07  lr: 0.000002  min_lr: 0.000000  loss: 1.9436 (1.8916)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6075 (9.8375)  time: 0.9939 (0.5149 -- 5.7394)  data: 0.4529 (0.0003 -- 5.2261)  max mem: 16413
Epoch: [166]  [ 40/160]  eta: 0:02:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8545 (1.8253)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3287 (9.3560)  time: 0.9570 (0.5069 -- 4.7371)  data: 0.4122 (0.0001 -- 4.1948)  max mem: 16413
Epoch: [166]  [ 60/160]  eta: 0:01:44  lr: 0.000002  min_lr: 0.000000  loss: 1.6992 (1.7691)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2403 (9.1875)  time: 0.8170 (0.5160 -- 4.1714)  data: 0.2806 (0.0002 -- 3.6729)  max mem: 16413
Epoch: [166]  [ 80/160]  eta: 0:01:17  lr: 0.000002  min_lr: 0.000000  loss: 1.6972 (1.7538)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1386 (9.1976)  time: 0.7412 (0.5158 -- 2.3653)  data: 0.1915 (0.0002 -- 1.8232)  max mem: 16413
Epoch: [166]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.7214 (1.7674)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6791 (9.0641)  time: 0.8521 (0.5299 -- 3.9842)  data: 0.3007 (0.0004 -- 3.4556)  max mem: 16413
[2023-08-30 20:08:10,783] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:08:10,783] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:08:10,783] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:08:10,784] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [166]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.7146 (1.7649)  loss_scale: 8192.0000 (8530.5124)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2432 (9.0210)  time: 0.8500 (0.5287 -- 3.4111)  data: 0.2998 (0.0004 -- 2.8836)  max mem: 16413
Epoch: [166]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.7096 (1.7612)  loss_scale: 16384.0000 (9644.4823)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4153 (9.0865)  time: 0.8468 (0.5147 -- 4.5563)  data: 0.2989 (0.0001 -- 4.0455)  max mem: 16413
Epoch: [166]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.8611 (1.7791)  loss_scale: 16384.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4811 (9.0823)  time: 0.6305 (0.4979 -- 1.7376)  data: 0.0999 (0.0002 -- 1.1889)  max mem: 16413
Epoch: [166] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.8611 (1.7855)  loss_scale: 16384.0000 (10444.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4811 (9.0823)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2800 (0.2800)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3204 (2.3204 -- 2.3204)  data: 2.0880 (2.0880 -- 2.0880)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3982 (0.6983)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4099 (0.1975 -- 2.3204)  data: 0.1908 (0.0006 -- 2.0880)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4440 (0.6390)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2186 (0.1699 -- 0.4351)  data: 0.0114 (0.0001 -- 0.2132)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5450 (0.6948)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2011 (0.1332 -- 0.4351)  data: 0.0111 (0.0001 -- 0.2132)  max mem: 16413
Val: Total time: 0:00:07 (0.2851 s / it)
* Acc@1 82.365 Acc@5 97.303 loss 0.650
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [167]  [  0/160]  eta: 0:17:22  lr: 0.000002  min_lr: 0.000000  loss: 2.3417 (2.3417)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9180 (5.9180)  time: 6.5157 (6.5157 -- 6.5157)  data: 5.9782 (5.9782 -- 5.9782)  max mem: 16413
Epoch: [167]  [ 20/160]  eta: 0:02:34  lr: 0.000002  min_lr: 0.000000  loss: 1.7906 (1.7470)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5459 (9.4904)  time: 0.8353 (0.5225 -- 2.0868)  data: 0.0979 (0.0004 -- 1.5384)  max mem: 16413
Epoch: [167]  [ 40/160]  eta: 0:02:05  lr: 0.000002  min_lr: 0.000000  loss: 1.5658 (1.6748)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1328 (9.0307)  time: 0.9806 (0.5239 -- 3.3223)  data: 0.3805 (0.0004 -- 2.8079)  max mem: 16413
Epoch: [167]  [ 60/160]  eta: 0:01:36  lr: 0.000002  min_lr: 0.000000  loss: 1.7462 (1.7314)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0961 (9.0278)  time: 0.8070 (0.5163 -- 3.9933)  data: 0.1001 (0.0002 -- 1.3145)  max mem: 16413
Epoch: [167]  [ 80/160]  eta: 0:01:14  lr: 0.000002  min_lr: 0.000000  loss: 1.8821 (1.7556)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7859 (9.0814)  time: 0.8256 (0.5206 -- 2.7587)  data: 0.0325 (0.0003 -- 0.3226)  max mem: 16413
[2023-08-30 20:10:10,854] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:10:10,855] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:10:10,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:10:10,855] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:10:17,577] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26811
[2023-08-30 20:10:17,577] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26811
[2023-08-30 20:10:17,577] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:10:17,577] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:10:17,577] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [167]  [100/160]  eta: 0:00:56  lr: 0.000002  min_lr: 0.000000  loss: 1.8669 (1.7817)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1922 (9.1174)  time: 0.9714 (0.5053 -- 3.9573)  data: 0.0100 (0.0004 -- 0.1685)  max mem: 16413
Epoch: [167]  [120/160]  eta: 0:00:37  lr: 0.000002  min_lr: 0.000000  loss: 1.9322 (1.7907)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3648 (9.0437)  time: 0.8651 (0.5255 -- 3.1038)  data: 0.1536 (0.0006 -- 1.8819)  max mem: 16413
Epoch: [167]  [140/160]  eta: 0:00:18  lr: 0.000002  min_lr: 0.000000  loss: 1.8388 (1.7908)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0723 (8.9505)  time: 0.7658 (0.5227 -- 2.7488)  data: 0.1748 (0.0003 -- 2.1966)  max mem: 16413
Epoch: [167]  [159/160]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000000  loss: 1.7908 (1.7884)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5112 (8.8717)  time: 0.7296 (0.4941 -- 2.8876)  data: 0.1188 (0.0002 -- 2.3623)  max mem: 16413
Epoch: [167] Total time: 0:02:21 (0.8850 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000000  loss: 1.7908 (1.7791)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5112 (8.8717)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2804 (0.2804)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4691 (2.4691 -- 2.4691)  data: 2.2302 (2.2302 -- 2.2302)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3950 (0.7000)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4290 (0.2053 -- 2.4691)  data: 0.2057 (0.0008 -- 2.2302)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4409 (0.6400)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2185 (0.1695 -- 0.3502)  data: 0.0105 (0.0001 -- 0.1735)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5447 (0.6961)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.1980 (0.1327 -- 0.3502)  data: 0.0098 (0.0001 -- 0.1735)  max mem: 16413
Val: Total time: 0:00:07 (0.2897 s / it)
* Acc@1 82.573 Acc@5 97.510 loss 0.651
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [168]  [  0/160]  eta: 0:20:04  lr: 0.000002  min_lr: 0.000000  loss: 1.9870 (1.9870)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3724 (8.3724)  time: 7.5254 (7.5254 -- 7.5254)  data: 6.9749 (6.9749 -- 6.9749)  max mem: 16413
[2023-08-30 20:11:37,881] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26891
[2023-08-30 20:11:37,881] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:11:37,881] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26891
[2023-08-30 20:11:37,881] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:11:37,881] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [168]  [ 20/160]  eta: 0:02:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8111 (1.7811)  loss_scale: 8192.0000 (12483.0476)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6678 (8.8166)  time: 0.9436 (0.5144 -- 4.7732)  data: 0.3572 (0.0003 -- 4.2536)  max mem: 16413
Epoch: [168]  [ 40/160]  eta: 0:02:08  lr: 0.000002  min_lr: 0.000000  loss: 1.8369 (1.7886)  loss_scale: 8192.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1381 (8.6889)  time: 0.8782 (0.5224 -- 3.3466)  data: 0.3286 (0.0001 -- 2.8015)  max mem: 16413
[2023-08-30 20:12:06,820] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26924
[2023-08-30 20:12:06,820] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 26924
[2023-08-30 20:12:06,821] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 20:12:06,821] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 20:12:06,821] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [168]  [ 60/160]  eta: 0:01:39  lr: 0.000002  min_lr: 0.000000  loss: 1.8633 (1.8050)  loss_scale: 4096.0000 (8527.7377)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2289 (8.8292)  time: 0.8514 (0.5210 -- 4.5768)  data: 0.3101 (0.0003 -- 4.0439)  max mem: 16413
Epoch: [168]  [ 80/160]  eta: 0:01:15  lr: 0.000002  min_lr: 0.000000  loss: 1.8196 (1.8191)  loss_scale: 4096.0000 (7433.4815)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3689 (8.9363)  time: 0.7845 (0.5211 -- 2.0872)  data: 0.1227 (0.0004 -- 1.5482)  max mem: 16413
Epoch: [168]  [100/160]  eta: 0:00:55  lr: 0.000002  min_lr: 0.000000  loss: 1.8430 (1.8193)  loss_scale: 4096.0000 (6772.5941)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0048 (8.9643)  time: 0.8745 (0.5192 -- 2.8018)  data: 0.2111 (0.0003 -- 2.2634)  max mem: 16413
[2023-08-30 20:13:09,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=27000, skipped=160, lr=[3.560304368271369e-08, 3.560304368271369e-08, 4.747072491028493e-08, 4.747072491028493e-08, 6.32942998803799e-08, 6.32942998803799e-08, 8.439239984050654e-08, 8.439239984050654e-08, 1.1252319978734205e-07, 1.1252319978734205e-07, 1.500309330497894e-07, 1.500309330497894e-07, 2.0004124406638586e-07, 2.0004124406638586e-07, 2.6672165875518115e-07, 2.6672165875518115e-07, 3.5562887834024156e-07, 3.5562887834024156e-07, 4.741718377869887e-07, 4.741718377869887e-07, 6.322291170493183e-07, 6.322291170493183e-07, 8.429721560657577e-07, 8.429721560657577e-07, 1.1239628747543436e-06, 1.1239628747543436e-06, 1.4986171663391248e-06, 1.4986171663391248e-06], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 20:13:09,829] [INFO] [timer.py:260:stop] epoch=0/micro_step=27000/global_step=27000, RunningAvgSamplesPerSec=17.70341990215104, CurrSamplesPerSec=22.939353307342632, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [168]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.8270 (1.8125)  loss_scale: 4096.0000 (6330.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6669 (9.0497)  time: 0.9265 (0.5243 -- 3.6060)  data: 0.3775 (0.0005 -- 3.0650)  max mem: 16413
Epoch: [168]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7313 (1.8012)  loss_scale: 4096.0000 (6013.2766)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7372 (8.9428)  time: 0.8522 (0.5156 -- 4.0334)  data: 0.3006 (0.0004 -- 3.5294)  max mem: 16413
Epoch: [168]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8657 (1.8005)  loss_scale: 4096.0000 (5785.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1106 (8.9292)  time: 0.6475 (0.4946 -- 2.5986)  data: 0.1319 (0.0002 -- 2.0688)  max mem: 16413
Epoch: [168] Total time: 0:02:22 (0.8886 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8657 (1.7797)  loss_scale: 4096.0000 (5785.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1106 (8.9292)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2836 (0.2836)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4107 (2.4107 -- 2.4107)  data: 2.1566 (2.1566 -- 2.1566)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3989 (0.7004)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4212 (0.2027 -- 2.4107)  data: 0.1978 (0.0005 -- 2.1566)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4485 (0.6413)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.8254)  time: 0.2163 (0.1697 -- 0.2578)  data: 0.0051 (0.0001 -- 0.0789)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5457 (0.6966)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.6805)  time: 0.1991 (0.1323 -- 0.2578)  data: 0.0044 (0.0001 -- 0.0789)  max mem: 16413
Val: Total time: 0:00:07 (0.2860 s / it)
* Acc@1 82.988 Acc@5 97.510 loss 0.652
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [169]  [  0/160]  eta: 0:19:26  lr: 0.000001  min_lr: 0.000000  loss: 1.6618 (1.6618)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8121 (11.8121)  time: 7.2929 (7.2929 -- 7.2929)  data: 6.6266 (6.6266 -- 6.6266)  max mem: 16413
[2023-08-30 20:14:08,408] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:14:08,409] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-30 20:14:08,409] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:14:08,409] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [169]  [ 20/160]  eta: 0:02:52  lr: 0.000001  min_lr: 0.000000  loss: 1.9155 (1.9089)  loss_scale: 4096.0000 (5656.3810)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8104 (8.4516)  time: 0.9323 (0.5187 -- 4.8813)  data: 0.3907 (0.0002 -- 4.3495)  max mem: 16413
Epoch: [169]  [ 40/160]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000000  loss: 1.6814 (1.7888)  loss_scale: 8192.0000 (6893.2683)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5882 (9.2885)  time: 0.8800 (0.5246 -- 3.0661)  data: 0.3345 (0.0004 -- 2.5456)  max mem: 16413
Epoch: [169]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.9997 (1.8433)  loss_scale: 8192.0000 (7319.0820)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1828 (9.4047)  time: 0.8727 (0.5130 -- 3.0667)  data: 0.3362 (0.0002 -- 2.5452)  max mem: 16413
Epoch: [169]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.7590 (1.8340)  loss_scale: 8192.0000 (7534.6173)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9971 (9.0919)  time: 0.8384 (0.5290 -- 4.3113)  data: 0.2956 (0.0003 -- 3.8063)  max mem: 16413
Epoch: [169]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.7037 (1.8098)  loss_scale: 8192.0000 (7664.7921)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4257 (9.2214)  time: 0.7765 (0.5178 -- 3.1058)  data: 0.2292 (0.0001 -- 2.5776)  max mem: 16413
Epoch: [169]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6194 (1.8095)  loss_scale: 8192.0000 (7751.9339)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1081 (9.1702)  time: 0.8003 (0.5221 -- 3.1795)  data: 0.2510 (0.0004 -- 2.6348)  max mem: 16413
Epoch: [169]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.8197 (1.8187)  loss_scale: 8192.0000 (7814.3546)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4646 (9.2240)  time: 1.0146 (0.5169 -- 5.1017)  data: 0.4633 (0.0002 -- 4.5852)  max mem: 16413
[2023-08-30 20:16:00,730] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:16:00,730] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:16:00,737] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:16:00,737] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [169]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7582 (1.8224)  loss_scale: 16384.0000 (8832.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7590 (9.1651)  time: 0.5663 (0.4970 -- 1.3642)  data: 0.0465 (0.0001 -- 0.8681)  max mem: 16413
Epoch: [169] Total time: 0:02:20 (0.8776 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7582 (1.7861)  loss_scale: 16384.0000 (8832.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7590 (9.1651)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2860 (0.2860)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1641 (2.1641 -- 2.1641)  data: 1.9665 (1.9665 -- 1.9665)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4055 (0.7021)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4169 (0.2031 -- 2.1641)  data: 0.2021 (0.0008 -- 1.9665)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4479 (0.6422)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2256 (0.1695 -- 0.4824)  data: 0.0172 (0.0001 -- 0.2188)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5460 (0.6983)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2104 (0.1334 -- 0.4824)  data: 0.0162 (0.0001 -- 0.2188)  max mem: 16413
Val: Total time: 0:00:07 (0.2842 s / it)
* Acc@1 82.573 Acc@5 97.510 loss 0.652
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [170]  [  0/160]  eta: 0:20:06  lr: 0.000001  min_lr: 0.000000  loss: 1.9078 (1.9078)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6319 (8.6319)  time: 7.5430 (7.5430 -- 7.5430)  data: 5.3725 (5.3725 -- 5.3725)  max mem: 16413
Epoch: [170]  [ 20/160]  eta: 0:02:50  lr: 0.000001  min_lr: 0.000000  loss: 1.6145 (1.6322)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9315 (9.7578)  time: 0.9034 (0.5146 -- 5.4055)  data: 0.0836 (0.0003 -- 1.6473)  max mem: 16413
Epoch: [170]  [ 40/160]  eta: 0:02:10  lr: 0.000001  min_lr: 0.000000  loss: 1.6832 (1.5803)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3165 (9.0895)  time: 0.9532 (0.5165 -- 4.9872)  data: 0.1319 (0.0005 -- 1.2960)  max mem: 16413
Epoch: [170]  [ 60/160]  eta: 0:01:40  lr: 0.000001  min_lr: 0.000000  loss: 1.9349 (1.6951)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9892 (9.1597)  time: 0.8285 (0.5093 -- 3.6802)  data: 0.0011 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [170]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.8410 (1.7258)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5218 (9.1639)  time: 0.8523 (0.5274 -- 3.0434)  data: 0.0014 (0.0003 -- 0.0028)  max mem: 16413
Epoch: [170]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6939 (1.7298)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4045 (9.0364)  time: 0.8026 (0.5198 -- 3.9465)  data: 0.0014 (0.0002 -- 0.0031)  max mem: 16413
[2023-08-30 20:17:59,560] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:17:59,561] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:17:59,561] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:17:59,561] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:18:03,475] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27313
[2023-08-30 20:18:03,475] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27313
[2023-08-30 20:18:03,475] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:18:03,475] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:18:03,475] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [170]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6549 (1.7282)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3379 (8.8673)  time: 0.8201 (0.5204 -- 2.7299)  data: 0.0015 (0.0002 -- 0.0038)  max mem: 16413
Epoch: [170]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.8721 (1.7400)  loss_scale: 16384.0000 (16848.7943)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1688 (8.9338)  time: 0.8446 (0.5361 -- 3.4177)  data: 0.0021 (0.0009 -- 0.0052)  max mem: 16413
Epoch: [170]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7512 (1.7379)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3317 (8.9095)  time: 0.6565 (0.4971 -- 2.1234)  data: 0.0010 (0.0002 -- 0.0033)  max mem: 16413
Epoch: [170] Total time: 0:02:20 (0.8763 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7512 (1.7526)  loss_scale: 16384.0000 (16793.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3317 (8.9095)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2854 (0.2854)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2137 (2.2137 -- 2.2137)  data: 2.0052 (2.0052 -- 2.0052)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4028 (0.6999)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4267 (0.1981 -- 2.2137)  data: 0.2143 (0.0006 -- 2.0052)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4501 (0.6412)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.8254)  time: 0.2233 (0.1698 -- 0.5568)  data: 0.0178 (0.0001 -- 0.3385)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5486 (0.6973)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.6805)  time: 0.2105 (0.1368 -- 0.5568)  data: 0.0175 (0.0001 -- 0.3385)  max mem: 16413
Val: Total time: 0:00:07 (0.2841 s / it)
* Acc@1 82.573 Acc@5 97.510 loss 0.651
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [171]  [  0/160]  eta: 0:22:53  lr: 0.000001  min_lr: 0.000000  loss: 1.5553 (1.5553)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2214 (10.2214)  time: 8.5856 (8.5856 -- 8.5856)  data: 8.0196 (8.0196 -- 8.0196)  max mem: 16413
Epoch: [171]  [ 20/160]  eta: 0:02:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6078 (1.6756)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7999 (8.6675)  time: 0.8914 (0.5207 -- 4.0765)  data: 0.1173 (0.0003 -- 1.7826)  max mem: 16413
Epoch: [171]  [ 40/160]  eta: 0:02:04  lr: 0.000001  min_lr: 0.000000  loss: 1.6776 (1.7287)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4123 (8.7738)  time: 0.8051 (0.5228 -- 3.8519)  data: 0.0017 (0.0001 -- 0.0052)  max mem: 16413
Epoch: [171]  [ 60/160]  eta: 0:01:43  lr: 0.000001  min_lr: 0.000000  loss: 1.7491 (1.7382)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0579 (9.0482)  time: 1.0241 (0.5208 -- 3.9786)  data: 0.0012 (0.0004 -- 0.0027)  max mem: 16413
Epoch: [171]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.7425 (1.7426)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1400 (9.1749)  time: 0.7726 (0.5068 -- 2.8523)  data: 0.0011 (0.0003 -- 0.0030)  max mem: 16413
[2023-08-30 20:20:07,695] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:20:07,695] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:20:07,696] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:20:07,696] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [171]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6584 (1.7391)  loss_scale: 32768.0000 (19466.1386)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9210 (9.2174)  time: 0.7794 (0.5261 -- 2.0029)  data: 0.0019 (0.0002 -- 0.0042)  max mem: 16413
[2023-08-30 20:20:21,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27461
[2023-08-30 20:20:21,359] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27461
[2023-08-30 20:20:21,360] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:20:21,360] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:20:21,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [171]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7279 (1.7557)  loss_scale: 16384.0000 (18956.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7353 (9.1537)  time: 0.9012 (0.5319 -- 4.7719)  data: 0.0014 (0.0003 -- 0.0031)  max mem: 16413
Epoch: [171]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5332 (1.7416)  loss_scale: 16384.0000 (18591.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6888 (9.1272)  time: 0.8859 (0.5175 -- 3.4093)  data: 0.0018 (0.0003 -- 0.0052)  max mem: 16413
Epoch: [171]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7363 (1.7406)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7939 (9.1986)  time: 0.6345 (0.4959 -- 2.8414)  data: 0.0007 (0.0001 -- 0.0035)  max mem: 16413
Epoch: [171] Total time: 0:02:21 (0.8873 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7363 (1.7539)  loss_scale: 16384.0000 (18329.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7939 (9.1986)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2836 (0.2836)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4290 (2.4290 -- 2.4290)  data: 2.1927 (2.1927 -- 2.1927)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4027 (0.7003)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4662 (0.1997 -- 2.4290)  data: 0.2489 (0.0005 -- 2.1927)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4530 (0.6420)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2283 (0.1707 -- 0.7550)  data: 0.0274 (0.0001 -- 0.5307)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5486 (0.6984)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (96.2656)  time: 0.2133 (0.1327 -- 0.7550)  data: 0.0272 (0.0001 -- 0.5307)  max mem: 16413
Val: Total time: 0:00:07 (0.2956 s / it)
* Acc@1 82.365 Acc@5 97.303 loss 0.652
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [172]  [  0/160]  eta: 0:19:25  lr: 0.000001  min_lr: 0.000000  loss: 1.9596 (1.9596)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2468 (12.2468)  time: 7.2853 (7.2853 -- 7.2853)  data: 6.7570 (6.7570 -- 6.7570)  max mem: 16413
Epoch: [172]  [ 20/160]  eta: 0:02:43  lr: 0.000001  min_lr: 0.000000  loss: 1.8759 (1.8990)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7955 (9.1257)  time: 0.8586 (0.5308 -- 2.4023)  data: 0.3168 (0.0008 -- 1.8706)  max mem: 16413
Epoch: [172]  [ 40/160]  eta: 0:02:08  lr: 0.000001  min_lr: 0.000000  loss: 1.6643 (1.7939)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5519 (8.9566)  time: 0.9711 (0.5146 -- 3.3733)  data: 0.3821 (0.0003 -- 2.8538)  max mem: 16413
Epoch: [172]  [ 60/160]  eta: 0:01:39  lr: 0.000001  min_lr: 0.000000  loss: 1.6862 (1.7630)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6079 (8.9069)  time: 0.8270 (0.5070 -- 3.6249)  data: 0.1564 (0.0003 -- 3.0662)  max mem: 16413
[2023-08-30 20:22:27,390] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:22:27,390] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:22:27,391] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:22:27,391] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:22:28,443] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27592
[2023-08-30 20:22:28,443] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27592
[2023-08-30 20:22:28,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:22:28,443] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:22:28,444] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [172]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.6497 (1.7518)  loss_scale: 16384.0000 (16788.5432)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4718 (9.0143)  time: 0.8548 (0.5147 -- 3.1287)  data: 0.1309 (0.0002 -- 2.5745)  max mem: 16413
Epoch: [172]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.8218 (1.7668)  loss_scale: 16384.0000 (16708.4356)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8736 (8.8485)  time: 0.8655 (0.5230 -- 2.2591)  data: 0.0731 (0.0002 -- 0.7499)  max mem: 16413
[2023-08-30 20:23:03,419] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27634
[2023-08-30 20:23:03,420] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:23:03,420] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27634
[2023-08-30 20:23:03,420] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 20:23:03,420] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [172]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.9428 (1.7905)  loss_scale: 16384.0000 (16180.8926)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5623 (9.0456)  time: 0.9060 (0.5205 -- 3.5503)  data: 0.3211 (0.0002 -- 3.0339)  max mem: 16413
Epoch: [172]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6158 (1.7620)  loss_scale: 8192.0000 (15047.7163)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4582 (9.0548)  time: 0.8366 (0.5219 -- 4.1244)  data: 0.2867 (0.0004 -- 3.6031)  max mem: 16413
Epoch: [172]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7809 (1.7513)  loss_scale: 8192.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5082 (9.0061)  time: 0.7108 (0.4951 -- 4.4314)  data: 0.1967 (0.0002 -- 3.9237)  max mem: 16413
Epoch: [172] Total time: 0:02:23 (0.8959 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7809 (1.7430)  loss_scale: 8192.0000 (14233.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5082 (9.0061)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2827 (0.2827)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3230 (2.3230 -- 2.3230)  data: 2.0958 (2.0958 -- 2.0958)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.4028 (0.6996)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4168 (0.1973 -- 2.3230)  data: 0.1958 (0.0009 -- 2.0958)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4533 (0.6415)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2217 (0.1690 -- 0.3944)  data: 0.0130 (0.0001 -- 0.1967)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5453 (0.6983)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2053 (0.1361 -- 0.3944)  data: 0.0127 (0.0001 -- 0.1967)  max mem: 16413
Val: Total time: 0:00:07 (0.2873 s / it)
* Acc@1 82.158 Acc@5 97.095 loss 0.652
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [173]  [  0/160]  eta: 0:18:10  lr: 0.000001  min_lr: 0.000000  loss: 1.8477 (1.8477)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1080 (11.1080)  time: 6.8132 (6.8132 -- 6.8132)  data: 6.1086 (6.1086 -- 6.1086)  max mem: 16413
Epoch: [173]  [ 20/160]  eta: 0:02:51  lr: 0.000001  min_lr: 0.000000  loss: 1.7956 (1.8565)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3540 (8.5614)  time: 0.9493 (0.5252 -- 4.0908)  data: 0.2642 (0.0005 -- 1.4371)  max mem: 16413
Epoch: [173]  [ 40/160]  eta: 0:02:01  lr: 0.000001  min_lr: 0.000000  loss: 1.8029 (1.8134)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9116 (8.7922)  time: 0.7871 (0.5139 -- 2.9693)  data: 0.1064 (0.0002 -- 0.9353)  max mem: 16413
Epoch: [173]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.8193 (1.7985)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4588 (8.8232)  time: 0.9381 (0.5292 -- 4.2275)  data: 0.0985 (0.0003 -- 1.4508)  max mem: 16413
Epoch: [173]  [ 80/160]  eta: 0:01:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7674 (1.7911)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1012 (8.7065)  time: 0.9764 (0.5196 -- 4.3633)  data: 0.0011 (0.0003 -- 0.0028)  max mem: 16413
[2023-08-30 20:25:09,286] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:25:09,287] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:25:09,290] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:25:09,290] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [173]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.7930 (1.7757)  loss_scale: 16384.0000 (9651.9604)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0765 (8.6541)  time: 0.8790 (0.5247 -- 4.6560)  data: 0.0011 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [173]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7024 (1.7672)  loss_scale: 16384.0000 (10764.6942)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3963 (8.7625)  time: 0.7913 (0.5188 -- 2.5357)  data: 0.0015 (0.0004 -- 0.0035)  max mem: 16413
Epoch: [173]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.9195 (1.7890)  loss_scale: 16384.0000 (11561.7589)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9281 (8.8322)  time: 0.7777 (0.5183 -- 2.8212)  data: 0.0022 (0.0004 -- 0.0140)  max mem: 16413
Epoch: [173]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8201 (1.7850)  loss_scale: 16384.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5086 (8.8246)  time: 0.7056 (0.4962 -- 2.1003)  data: 0.0501 (0.0002 -- 0.5960)  max mem: 16413
Epoch: [173] Total time: 0:02:22 (0.8895 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8201 (1.7598)  loss_scale: 16384.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5086 (8.8246)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2816 (0.2816)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5302 (2.5302 -- 2.5302)  data: 2.3018 (2.3018 -- 2.3018)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3975 (0.6985)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4269 (0.2054 -- 2.5302)  data: 0.2118 (0.0006 -- 2.3018)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4520 (0.6411)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2091 (0.1685 -- 0.2321)  data: 0.0027 (0.0001 -- 0.0138)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5476 (0.6977)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.1930 (0.1331 -- 0.2321)  data: 0.0022 (0.0001 -- 0.0138)  max mem: 16413
Val: Total time: 0:00:07 (0.2852 s / it)
* Acc@1 82.158 Acc@5 97.095 loss 0.651
Accuracy of the network on the 482 val images: 82.16%
Max accuracy: 82.99%
Epoch: [174]  [  0/160]  eta: 0:18:39  lr: 0.000001  min_lr: 0.000000  loss: 1.7453 (1.7453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5825 (8.5825)  time: 6.9960 (6.9960 -- 6.9960)  data: 6.4176 (6.4176 -- 6.4176)  max mem: 16413
Epoch: [174]  [ 20/160]  eta: 0:02:35  lr: 0.000001  min_lr: 0.000000  loss: 1.8049 (1.7356)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3263 (9.6199)  time: 0.8148 (0.5326 -- 2.0727)  data: 0.1657 (0.0004 -- 1.3400)  max mem: 16413
Epoch: [174]  [ 40/160]  eta: 0:02:04  lr: 0.000001  min_lr: 0.000000  loss: 1.7624 (1.7540)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1183 (9.6081)  time: 0.9589 (0.5312 -- 3.3016)  data: 0.1267 (0.0005 -- 2.1146)  max mem: 16413
[2023-08-30 20:27:10,440] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:27:10,440] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:27:10,440] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:27:10,440] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [174]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6161 (1.7292)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2056 (9.3433)  time: 0.8259 (0.5163 -- 2.3775)  data: 0.1873 (0.0004 -- 1.4291)  max mem: 16413
Epoch: [174]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6226 (1.7136)  loss_scale: 32768.0000 (22452.1481)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0278 (9.3074)  time: 0.8504 (0.5242 -- 2.8386)  data: 0.2112 (0.0003 -- 2.3076)  max mem: 16413
[2023-08-30 20:27:38,981] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27924
[2023-08-30 20:27:38,981] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 27924
[2023-08-30 20:27:38,981] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:27:39,022] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:27:39,022] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [174]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.6426 (1.7009)  loss_scale: 16384.0000 (21737.1881)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8538 (9.2874)  time: 0.9751 (0.5184 -- 3.4969)  data: 0.4288 (0.0003 -- 2.9800)  max mem: 16413
Epoch: [174]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8220 (1.7314)  loss_scale: 16384.0000 (20852.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6139 (9.2302)  time: 0.7497 (0.5219 -- 2.7468)  data: 0.2054 (0.0002 -- 2.2060)  max mem: 16413
Epoch: [174]  [140/160]  eta: 0:00:17  lr: 0.000001  min_lr: 0.000000  loss: 1.8293 (1.7489)  loss_scale: 16384.0000 (20218.5532)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5551 (9.2321)  time: 0.7881 (0.5356 -- 2.7152)  data: 0.2325 (0.0010 -- 2.1557)  max mem: 16413
[2023-08-30 20:28:38,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=28000, skipped=165, lr=[2.3358190854927335e-08, 2.3358190854927335e-08, 3.1144254473236446e-08, 3.1144254473236446e-08, 4.1525672630981926e-08, 4.1525672630981926e-08, 5.53675635079759e-08, 5.53675635079759e-08, 7.382341801063453e-08, 7.382341801063453e-08, 9.843122401417938e-08, 9.843122401417938e-08, 1.3124163201890585e-07, 1.3124163201890585e-07, 1.7498884269187445e-07, 1.7498884269187445e-07, 2.3331845692249928e-07, 2.3331845692249928e-07, 3.110912758966657e-07, 3.110912758966657e-07, 4.147883678622209e-07, 4.147883678622209e-07, 5.53051157149628e-07, 5.53051157149628e-07, 7.374015428661706e-07, 7.374015428661706e-07, 9.83202057154894e-07, 9.83202057154894e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 20:28:38,141] [INFO] [timer.py:260:stop] epoch=0/micro_step=28000/global_step=28000, RunningAvgSamplesPerSec=17.700528156472505, CurrSamplesPerSec=24.444254064276617, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [174]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7109 (1.7474)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2881 (9.2059)  time: 0.7292 (0.4952 -- 2.4400)  data: 0.2008 (0.0002 -- 1.9453)  max mem: 16413
Epoch: [174] Total time: 0:02:20 (0.8768 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7109 (1.7470)  loss_scale: 16384.0000 (19763.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2881 (9.2059)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2805 (0.2805)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4404 (2.4404 -- 2.4404)  data: 2.1587 (2.1587 -- 2.1587)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3956 (0.6969)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4304 (0.1951 -- 2.4404)  data: 0.2063 (0.0005 -- 2.1587)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4503 (0.6398)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2154 (0.1698 -- 0.3661)  data: 0.0094 (0.0001 -- 0.1031)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5459 (0.6965)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.1994 (0.1330 -- 0.3661)  data: 0.0092 (0.0001 -- 0.1031)  max mem: 16413
Val: Total time: 0:00:07 (0.2865 s / it)
* Acc@1 82.365 Acc@5 97.095 loss 0.650
Accuracy of the network on the 482 val images: 82.37%
Max accuracy: 82.99%
Epoch: [175]  [  0/160]  eta: 0:25:03  lr: 0.000001  min_lr: 0.000000  loss: 1.6199 (1.6199)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5349 (4.5349)  time: 9.3975 (9.3975 -- 9.3975)  data: 6.5214 (6.5214 -- 6.5214)  max mem: 16413
[2023-08-30 20:29:02,273] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28008
[2023-08-30 20:29:02,273] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:29:02,273] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28008
[2023-08-30 20:29:02,274] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 20:29:02,274] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [175]  [ 20/160]  eta: 0:02:58  lr: 0.000001  min_lr: 0.000000  loss: 1.8642 (1.7705)  loss_scale: 8192.0000 (11312.7619)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7988 (9.2785)  time: 0.8674 (0.5346 -- 4.2856)  data: 0.0017 (0.0004 -- 0.0053)  max mem: 16413
Epoch: [175]  [ 40/160]  eta: 0:02:13  lr: 0.000001  min_lr: 0.000000  loss: 1.7722 (1.7655)  loss_scale: 8192.0000 (9790.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1461 (8.9654)  time: 0.9376 (0.5212 -- 3.7959)  data: 0.0017 (0.0003 -- 0.0059)  max mem: 16413
Epoch: [175]  [ 60/160]  eta: 0:01:42  lr: 0.000001  min_lr: 0.000000  loss: 1.6376 (1.7527)  loss_scale: 8192.0000 (9266.3607)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2625 (8.9270)  time: 0.8575 (0.5085 -- 5.3219)  data: 0.0018 (0.0002 -- 0.0122)  max mem: 16413
Epoch: [175]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.8288 (1.7615)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8032 (9.0734)  time: 0.7302 (0.5222 -- 2.9229)  data: 0.0013 (0.0001 -- 0.0034)  max mem: 16413
Epoch: [175]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.9182 (1.7723)  loss_scale: 8192.0000 (8840.8713)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5255 (9.1385)  time: 0.7690 (0.5286 -- 2.1486)  data: 0.0021 (0.0003 -- 0.0057)  max mem: 16413
Epoch: [175]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6286 (1.7656)  loss_scale: 8192.0000 (8733.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4645 (9.1876)  time: 0.8257 (0.5317 -- 2.4546)  data: 0.0020 (0.0003 -- 0.0092)  max mem: 16413
[2023-08-30 20:30:51,536] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:30:51,536] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:30:51,537] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:30:51,537] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [175]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.8133 (1.7561)  loss_scale: 8192.0000 (8889.1915)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9548 (9.2814)  time: 0.9005 (0.5245 -- 2.7276)  data: 0.1391 (0.0002 -- 2.2055)  max mem: 16413
Epoch: [175]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8578 (1.7513)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6079 (9.1877)  time: 0.7219 (0.4958 -- 3.6279)  data: 0.2029 (0.0002 -- 3.1040)  max mem: 16413
Epoch: [175] Total time: 0:02:21 (0.8819 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8578 (1.7588)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6079 (9.1877)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2820 (0.2820)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3383 (2.3383 -- 2.3383)  data: 2.0700 (2.0700 -- 2.0700)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3929 (0.6950)  acc1: 77.7778 (77.7778)  acc5: 100.0000 (96.9697)  time: 0.4105 (0.1981 -- 2.3383)  data: 0.1897 (0.0008 -- 2.0700)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4514 (0.6380)  acc1: 77.7778 (81.4815)  acc5: 100.0000 (96.2963)  time: 0.2227 (0.1702 -- 0.3617)  data: 0.0145 (0.0001 -- 0.1539)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5493 (0.6946)  acc1: 77.7778 (79.6681)  acc5: 100.0000 (95.8506)  time: 0.2054 (0.1381 -- 0.3617)  data: 0.0141 (0.0001 -- 0.1539)  max mem: 16413
Val: Total time: 0:00:07 (0.2883 s / it)
* Acc@1 82.573 Acc@5 97.095 loss 0.649
Accuracy of the network on the 482 val images: 82.57%
Max accuracy: 82.99%
Epoch: [176]  [  0/160]  eta: 0:17:35  lr: 0.000001  min_lr: 0.000000  loss: 1.8195 (1.8195)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6909 (7.6909)  time: 6.5945 (6.5945 -- 6.5945)  data: 6.0700 (6.0700 -- 6.0700)  max mem: 16413
Epoch: [176]  [ 20/160]  eta: 0:02:39  lr: 0.000001  min_lr: 0.000000  loss: 1.7975 (1.8522)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4473 (9.3448)  time: 0.8643 (0.5193 -- 2.5765)  data: 0.2675 (0.0004 -- 2.0519)  max mem: 16413
[2023-08-30 20:31:40,345] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28183
[2023-08-30 20:31:40,345] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28183
[2023-08-30 20:31:40,345] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:31:40,346] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:31:40,346] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [176]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.6569 (1.7834)  loss_scale: 8192.0000 (12787.5122)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7325 (9.4951)  time: 0.9020 (0.5122 -- 3.3140)  data: 0.3611 (0.0005 -- 2.7877)  max mem: 16413
Epoch: [176]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7356 (1.7594)  loss_scale: 8192.0000 (11280.7869)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1100 (9.1987)  time: 0.8698 (0.5257 -- 4.3217)  data: 0.3298 (0.0004 -- 3.7976)  max mem: 16413
Epoch: [176]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.7697 (1.7774)  loss_scale: 8192.0000 (10518.1235)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6530 (9.1379)  time: 0.8207 (0.5336 -- 4.4675)  data: 0.2737 (0.0004 -- 3.9450)  max mem: 16413
Epoch: [176]  [100/160]  eta: 0:00:56  lr: 0.000001  min_lr: 0.000000  loss: 1.9324 (1.7993)  loss_scale: 8192.0000 (10057.5050)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0783 (9.0547)  time: 0.9798 (0.5261 -- 3.5084)  data: 0.4341 (0.0009 -- 2.9612)  max mem: 16413
Epoch: [176]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8317 (1.8078)  loss_scale: 8192.0000 (9749.1570)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1616 (9.1402)  time: 0.7551 (0.5167 -- 3.7069)  data: 0.2103 (0.0002 -- 3.1729)  max mem: 16413
Epoch: [176]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7018 (1.7965)  loss_scale: 8192.0000 (9528.2837)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9331 (9.1638)  time: 0.8576 (0.5177 -- 3.1411)  data: 0.2921 (0.0006 -- 2.5432)  max mem: 16413
[2023-08-30 20:33:30,593] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:33:30,593] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:33:30,596] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:33:30,596] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [176]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7494 (1.7847)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1112 (9.0773)  time: 0.6760 (0.4969 -- 1.8153)  data: 0.0090 (0.0002 -- 0.1591)  max mem: 16413
Epoch: [176] Total time: 0:02:20 (0.8785 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7494 (1.7603)  loss_scale: 8192.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1112 (9.0773)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2820 (0.2820)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4150 (2.4150 -- 2.4150)  data: 2.1894 (2.1894 -- 2.1894)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3934 (0.6937)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4207 (0.1827 -- 2.4150)  data: 0.2024 (0.0004 -- 2.1894)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4495 (0.6379)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2216 (0.1707 -- 0.4129)  data: 0.0131 (0.0001 -- 0.2228)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5527 (0.6941)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2056 (0.1330 -- 0.4129)  data: 0.0129 (0.0001 -- 0.2228)  max mem: 16413
Val: Total time: 0:00:07 (0.2902 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.649
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [177]  [  0/160]  eta: 0:21:15  lr: 0.000001  min_lr: 0.000000  loss: 2.0476 (2.0476)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5821 (8.5821)  time: 7.9708 (7.9708 -- 7.9708)  data: 7.4476 (7.4476 -- 7.4476)  max mem: 16413
[2023-08-30 20:34:09,455] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28340
[2023-08-30 20:34:09,455] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28340
[2023-08-30 20:34:09,455] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:34:09,455] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:34:09,455] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [177]  [ 20/160]  eta: 0:02:54  lr: 0.000001  min_lr: 0.000000  loss: 1.7532 (1.7663)  loss_scale: 16384.0000 (15993.9048)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9447 (9.3406)  time: 0.9105 (0.5237 -- 4.3175)  data: 0.3604 (0.0003 -- 3.7709)  max mem: 16413
Epoch: [177]  [ 40/160]  eta: 0:02:09  lr: 0.000001  min_lr: 0.000000  loss: 1.7777 (1.7808)  loss_scale: 8192.0000 (12188.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4223 (9.0323)  time: 0.8951 (0.5217 -- 3.0722)  data: 0.3491 (0.0002 -- 2.5552)  max mem: 16413
Epoch: [177]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.9159 (1.8013)  loss_scale: 8192.0000 (10877.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1315 (9.2684)  time: 0.7551 (0.5277 -- 3.1563)  data: 0.2101 (0.0002 -- 2.6410)  max mem: 16413
Epoch: [177]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.6468 (1.7629)  loss_scale: 8192.0000 (10214.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5125 (9.4257)  time: 0.8682 (0.5293 -- 4.1306)  data: 0.3169 (0.0006 -- 3.5839)  max mem: 16413
Epoch: [177]  [100/160]  eta: 0:00:54  lr: 0.000001  min_lr: 0.000000  loss: 1.9467 (1.7897)  loss_scale: 8192.0000 (9814.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6881 (9.3266)  time: 0.7880 (0.5284 -- 1.9545)  data: 0.2392 (0.0003 -- 1.4372)  max mem: 16413
Epoch: [177]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5556 (1.7711)  loss_scale: 8192.0000 (9546.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3442 (9.1710)  time: 0.9063 (0.5270 -- 4.2254)  data: 0.3620 (0.0004 -- 3.7105)  max mem: 16413
Epoch: [177]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.7307 (1.7783)  loss_scale: 8192.0000 (9353.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4221 (9.1091)  time: 0.8750 (0.5261 -- 4.4212)  data: 0.3214 (0.0003 -- 3.9149)  max mem: 16413
[2023-08-30 20:35:58,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:35:58,600] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:35:58,600] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:35:58,600] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [177]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7113 (1.7679)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3298 (9.0651)  time: 0.6561 (0.4973 -- 2.0340)  data: 0.1266 (0.0002 -- 1.5217)  max mem: 16413
Epoch: [177] Total time: 0:02:20 (0.8782 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7113 (1.7585)  loss_scale: 16384.0000 (9779.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3298 (9.0651)
Val:  [ 0/27]  eta: 0:01:06  loss: 0.2826 (0.2826)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4655 (2.4655 -- 2.4655)  data: 2.2481 (2.2481 -- 2.2481)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3948 (0.6949)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4208 (0.1885 -- 2.4655)  data: 0.2072 (0.0004 -- 2.2481)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4545 (0.6385)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2154 (0.1703 -- 0.3773)  data: 0.0112 (0.0001 -- 0.1881)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5528 (0.6950)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.1998 (0.1331 -- 0.3773)  data: 0.0109 (0.0001 -- 0.1881)  max mem: 16413
Val: Total time: 0:00:07 (0.2874 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.649
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [178]  [  0/160]  eta: 0:21:50  lr: 0.000001  min_lr: 0.000000  loss: 2.3238 (2.3238)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1238 (7.1238)  time: 8.1936 (8.1936 -- 8.1936)  data: 6.8900 (6.8900 -- 6.8900)  max mem: 16413
Epoch: [178]  [ 20/160]  eta: 0:02:42  lr: 0.000001  min_lr: 0.000000  loss: 1.5615 (1.6647)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1270 (8.9449)  time: 0.8127 (0.5279 -- 2.8093)  data: 0.2205 (0.0005 -- 2.2780)  max mem: 16413
Epoch: [178]  [ 40/160]  eta: 0:02:02  lr: 0.000001  min_lr: 0.000000  loss: 1.8314 (1.7458)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9902 (8.8253)  time: 0.8622 (0.5167 -- 4.4346)  data: 0.3235 (0.0002 -- 3.9168)  max mem: 16413
Epoch: [178]  [ 60/160]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7104 (1.7380)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2856 (9.0915)  time: 0.8921 (0.5303 -- 3.5071)  data: 0.3438 (0.0005 -- 2.9661)  max mem: 16413
Epoch: [178]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.7319 (1.7440)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6326 (9.1177)  time: 0.8234 (0.5138 -- 3.7754)  data: 0.0066 (0.0002 -- 0.0863)  max mem: 16413
Epoch: [178]  [100/160]  eta: 0:00:54  lr: 0.000001  min_lr: 0.000000  loss: 1.8203 (1.7495)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6103 (9.0647)  time: 0.8278 (0.5346 -- 2.8121)  data: 0.0523 (0.0003 -- 1.0119)  max mem: 16413
[2023-08-30 20:37:59,879] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:37:59,879] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:37:59,881] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:37:59,881] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [178]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.9769 (1.7774)  loss_scale: 16384.0000 (16925.6198)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0167 (9.2069)  time: 1.0343 (0.5249 -- 3.9174)  data: 0.0024 (0.0003 -- 0.0159)  max mem: 16413
[2023-08-30 20:38:19,445] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28617
[2023-08-30 20:38:19,445] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28617
[2023-08-30 20:38:19,446] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:38:19,446] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:38:19,446] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [178]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.8390 (1.7950)  loss_scale: 32768.0000 (18707.9716)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0363 (9.3091)  time: 0.8088 (0.5228 -- 1.8779)  data: 0.0013 (0.0003 -- 0.0030)  max mem: 16413
Epoch: [178]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.8091 (1.7943)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1184 (9.2871)  time: 0.6591 (0.4958 -- 1.7853)  data: 0.0005 (0.0002 -- 0.0014)  max mem: 16413
Epoch: [178] Total time: 0:02:22 (0.8881 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.8091 (1.7828)  loss_scale: 16384.0000 (18432.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1184 (9.2871)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2839 (0.2839)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4024 (2.4024 -- 2.4024)  data: 2.1256 (2.1256 -- 2.1256)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3957 (0.6955)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4152 (0.2027 -- 2.4024)  data: 0.1943 (0.0008 -- 2.1256)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4551 (0.6389)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2235 (0.1696 -- 0.5538)  data: 0.0195 (0.0001 -- 0.3762)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5525 (0.6952)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2091 (0.1326 -- 0.5538)  data: 0.0192 (0.0001 -- 0.3762)  max mem: 16413
Val: Total time: 0:00:07 (0.2910 s / it)
* Acc@1 82.780 Acc@5 97.095 loss 0.650
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [179]  [  0/160]  eta: 0:20:57  lr: 0.000001  min_lr: 0.000000  loss: 1.1404 (1.1404)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5726 (11.5726)  time: 7.8585 (7.8585 -- 7.8585)  data: 5.8934 (5.8934 -- 5.8934)  max mem: 16413
Epoch: [179]  [ 20/160]  eta: 0:02:53  lr: 0.000001  min_lr: 0.000000  loss: 1.8685 (1.7845)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2780 (11.1461)  time: 0.9099 (0.5296 -- 4.7959)  data: 0.0016 (0.0003 -- 0.0091)  max mem: 16413
Epoch: [179]  [ 40/160]  eta: 0:02:01  lr: 0.000001  min_lr: 0.000000  loss: 1.6463 (1.7339)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4895 (10.1891)  time: 0.7772 (0.5168 -- 3.3872)  data: 0.0064 (0.0002 -- 0.0901)  max mem: 16413
Epoch: [179]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.7708 (1.7697)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8218 (9.9069)  time: 0.8779 (0.5316 -- 2.5812)  data: 0.3294 (0.0005 -- 2.0560)  max mem: 16413
Epoch: [179]  [ 80/160]  eta: 0:01:14  lr: 0.000001  min_lr: 0.000000  loss: 1.8176 (1.7681)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0976 (9.7037)  time: 0.8303 (0.5321 -- 2.6089)  data: 0.2790 (0.0003 -- 2.0687)  max mem: 16413
Epoch: [179]  [100/160]  eta: 0:00:57  lr: 0.000001  min_lr: 0.000000  loss: 1.9208 (1.7770)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1480 (9.7990)  time: 1.0574 (0.5150 -- 4.1332)  data: 0.5204 (0.0003 -- 3.6175)  max mem: 16413
[2023-08-30 20:40:20,623] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28744
[2023-08-30 20:40:20,624] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 28744
[2023-08-30 20:40:20,624] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:40:20,624] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:40:20,624] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [179]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.8288 (1.7788)  loss_scale: 8192.0000 (15233.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7486 (9.6221)  time: 0.7154 (0.5164 -- 3.6860)  data: 0.1728 (0.0002 -- 3.1553)  max mem: 16413
Epoch: [179]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5476 (1.7468)  loss_scale: 8192.0000 (14234.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6725 (9.5054)  time: 0.8528 (0.5247 -- 3.1768)  data: 0.3007 (0.0004 -- 2.6511)  max mem: 16413
Epoch: [179]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7543 (1.7510)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7977 (9.5062)  time: 0.6998 (0.4957 -- 4.1273)  data: 0.1817 (0.0001 -- 3.6211)  max mem: 16413
Epoch: [179] Total time: 0:02:21 (0.8860 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7543 (1.7686)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7977 (9.5062)
[2023-08-30 20:41:03,333] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-179 is about to be saved!
[2023-08-30 20:41:03,334] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt
[2023-08-30 20:41:03,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt...
[2023-08-30 20:41:03,335] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
[2023-08-30 20:41:04,268] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-179/mp_rank_00_model_states.pt.
[2023-08-30 20:41:04,268] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-179 is ready now!
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2829 (0.2829)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5328 (2.5328 -- 2.5328)  data: 2.2915 (2.2915 -- 2.2915)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3966 (0.6941)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4293 (0.1951 -- 2.5328)  data: 0.2106 (0.0009 -- 2.2915)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4563 (0.6376)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2142 (0.1688 -- 0.2969)  data: 0.0071 (0.0001 -- 0.1137)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5482 (0.6935)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.1980 (0.1344 -- 0.2969)  data: 0.0068 (0.0001 -- 0.1137)  max mem: 16413
Val: Total time: 0:00:07 (0.2892 s / it)
* Acc@1 82.988 Acc@5 97.303 loss 0.649
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [180]  [  0/160]  eta: 0:18:07  lr: 0.000001  min_lr: 0.000000  loss: 2.0735 (2.0735)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.2814 (15.2814)  time: 6.7984 (6.7984 -- 6.7984)  data: 6.1342 (6.1342 -- 6.1342)  max mem: 16413
Epoch: [180]  [ 20/160]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000000  loss: 1.7735 (1.8093)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7268 (9.3658)  time: 0.9024 (0.5266 -- 4.3176)  data: 0.3524 (0.0004 -- 3.7728)  max mem: 16413
Epoch: [180]  [ 40/160]  eta: 0:02:13  lr: 0.000001  min_lr: 0.000000  loss: 1.7824 (1.7838)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5959 (9.3759)  time: 1.0415 (0.5314 -- 5.0215)  data: 0.4959 (0.0006 -- 4.5149)  max mem: 16413
Epoch: [180]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.7278 (1.7660)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0745 (9.5097)  time: 0.7147 (0.5114 -- 3.0430)  data: 0.1683 (0.0001 -- 2.5252)  max mem: 16413
[2023-08-30 20:42:22,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:42:22,876] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:42:22,876] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:42:22,876] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [180]  [ 80/160]  eta: 0:01:16  lr: 0.000001  min_lr: 0.000000  loss: 1.7069 (1.7663)  loss_scale: 8192.0000 (9001.0864)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4012 (9.7656)  time: 0.8908 (0.5220 -- 3.8094)  data: 0.3431 (0.0003 -- 3.2695)  max mem: 16413
Epoch: [180]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.6709 (1.7401)  loss_scale: 16384.0000 (10463.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7111 (9.6405)  time: 0.7636 (0.5218 -- 2.7850)  data: 0.2197 (0.0006 -- 2.2497)  max mem: 16413
Epoch: [180]  [120/160]  eta: 0:00:36  lr: 0.000001  min_lr: 0.000000  loss: 1.5980 (1.7214)  loss_scale: 16384.0000 (11441.7190)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1497 (9.5750)  time: 0.8705 (0.5152 -- 2.8025)  data: 0.1901 (0.0004 -- 1.7370)  max mem: 16413
Epoch: [180]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.5227 (1.7114)  loss_scale: 16384.0000 (12142.7518)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9297 (9.5844)  time: 0.8823 (0.5325 -- 3.6087)  data: 0.0092 (0.0003 -- 0.1557)  max mem: 16413
Epoch: [180]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.4997 (1.7043)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6297 (9.3650)  time: 0.6452 (0.4986 -- 2.2463)  data: 0.1178 (0.0001 -- 1.7259)  max mem: 16413
Epoch: [180] Total time: 0:02:20 (0.8781 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.4997 (1.7314)  loss_scale: 16384.0000 (12646.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6297 (9.3650)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2815 (0.2815)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1736 (2.1736 -- 2.1736)  data: 1.9539 (1.9539 -- 1.9539)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3941 (0.6945)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.3956 (0.2050 -- 2.1736)  data: 0.1787 (0.0008 -- 1.9539)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4515 (0.6378)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2259 (0.1715 -- 0.3230)  data: 0.0139 (0.0001 -- 0.1130)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5496 (0.6937)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (96.2656)  time: 0.2094 (0.1373 -- 0.3230)  data: 0.0136 (0.0001 -- 0.1130)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 82.780 Acc@5 97.303 loss 0.648
Accuracy of the network on the 482 val images: 82.78%
Max accuracy: 82.99%
Epoch: [181]  [  0/160]  eta: 0:25:10  lr: 0.000001  min_lr: 0.000000  loss: 2.0376 (2.0376)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0635 (8.0635)  time: 9.4432 (9.4432 -- 9.4432)  data: 7.6225 (7.6225 -- 7.6225)  max mem: 16413
Epoch: [181]  [ 20/160]  eta: 0:02:49  lr: 0.000001  min_lr: 0.000000  loss: 1.7882 (1.8275)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6174 (8.6432)  time: 0.7978 (0.5261 -- 3.6575)  data: 0.1418 (0.0004 -- 2.1547)  max mem: 16413
[2023-08-30 20:44:21,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=29000, skipped=170, lr=[1.3702691449281511e-08, 1.3702691449281511e-08, 1.8270255265708683e-08, 1.8270255265708683e-08, 2.4360340354278244e-08, 2.4360340354278244e-08, 3.248045380570432e-08, 3.248045380570432e-08, 4.33072717409391e-08, 4.33072717409391e-08, 5.77430289879188e-08, 5.77430289879188e-08, 7.699070531722507e-08, 7.699070531722507e-08, 1.0265427375630009e-07, 1.0265427375630009e-07, 1.3687236500840012e-07, 1.3687236500840012e-07, 1.8249648667786682e-07, 1.8249648667786682e-07, 2.4332864890382244e-07, 2.4332864890382244e-07, 3.244381985384299e-07, 3.244381985384299e-07, 4.3258426471790657e-07, 4.3258426471790657e-07, 5.767790196238754e-07, 5.767790196238754e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 20:44:21,274] [INFO] [timer.py:260:stop] epoch=0/micro_step=29000/global_step=29000, RunningAvgSamplesPerSec=17.74261686850726, CurrSamplesPerSec=22.04950176679374, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [181]  [ 40/160]  eta: 0:02:08  lr: 0.000001  min_lr: 0.000000  loss: 1.6818 (1.7755)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0966 (8.5943)  time: 0.9318 (0.5193 -- 4.5198)  data: 0.3686 (0.0003 -- 3.9806)  max mem: 16413
[2023-08-30 20:44:24,937] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:44:24,937] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:44:24,937] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:44:24,937] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:44:39,861] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29019
[2023-08-30 20:44:39,861] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29019
[2023-08-30 20:44:39,861] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:44:39,861] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:44:39,861] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [181]  [ 60/160]  eta: 0:01:38  lr: 0.000001  min_lr: 0.000000  loss: 1.8733 (1.7939)  loss_scale: 32768.0000 (21218.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3423 (9.0509)  time: 0.7988 (0.5175 -- 3.2499)  data: 0.2555 (0.0002 -- 2.7390)  max mem: 16413
Epoch: [181]  [ 80/160]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000000  loss: 1.7762 (1.7993)  loss_scale: 16384.0000 (20024.8889)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7904 (8.9441)  time: 0.8062 (0.5246 -- 3.4969)  data: 0.2562 (0.0004 -- 2.9879)  max mem: 16413
Epoch: [181]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.7258 (1.7964)  loss_scale: 16384.0000 (19303.9208)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6327 (9.0105)  time: 0.8606 (0.5344 -- 3.5915)  data: 0.2299 (0.0004 -- 2.4503)  max mem: 16413
Epoch: [181]  [120/160]  eta: 0:00:37  lr: 0.000001  min_lr: 0.000000  loss: 1.7691 (1.7950)  loss_scale: 16384.0000 (18821.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3650 (8.9614)  time: 0.9506 (0.5179 -- 4.2845)  data: 0.0610 (0.0002 -- 1.1966)  max mem: 16413
Epoch: [181]  [140/160]  eta: 0:00:18  lr: 0.000001  min_lr: 0.000000  loss: 1.6746 (1.7801)  loss_scale: 16384.0000 (18475.5745)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6466 (8.8740)  time: 0.8569 (0.5203 -- 5.2182)  data: 0.0165 (0.0005 -- 0.2901)  max mem: 16413
Epoch: [181]  [159/160]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000000  loss: 1.7758 (1.7784)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4893 (8.9899)  time: 0.6564 (0.4948 -- 2.3169)  data: 0.0073 (0.0002 -- 0.1319)  max mem: 16413
Epoch: [181] Total time: 0:02:22 (0.8882 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000000  loss: 1.7758 (1.7829)  loss_scale: 16384.0000 (18227.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4893 (8.9899)
Val:  [ 0/27]  eta: 0:01:10  loss: 0.2819 (0.2819)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.6022 (2.6022 -- 2.6022)  data: 2.3439 (2.3439 -- 2.3439)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3947 (0.6934)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4269 (0.1996 -- 2.6022)  data: 0.2143 (0.0006 -- 2.3439)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4495 (0.6367)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2120 (0.1698 -- 0.4166)  data: 0.0125 (0.0001 -- 0.2327)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5477 (0.6928)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.1980 (0.1359 -- 0.4166)  data: 0.0122 (0.0001 -- 0.2327)  max mem: 16413
Val: Total time: 0:00:07 (0.2901 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [182]  [  0/160]  eta: 0:19:01  lr: 0.000001  min_lr: 0.000000  loss: 1.5290 (1.5290)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3380 (11.3380)  time: 7.1375 (7.1375 -- 7.1375)  data: 6.5871 (6.5871 -- 6.5871)  max mem: 16413
Epoch: [182]  [ 20/160]  eta: 0:02:39  lr: 0.000001  min_lr: 0.000000  loss: 1.8079 (1.7235)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7258 (9.3143)  time: 0.8400 (0.5170 -- 3.2822)  data: 0.1466 (0.0004 -- 1.7835)  max mem: 16413
[2023-08-30 20:46:41,866] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:46:41,866] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:46:41,867] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:46:41,867] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:46:42,417] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29149
[2023-08-30 20:46:42,417] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29149
[2023-08-30 20:46:42,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:46:42,418] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:46:42,418] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [182]  [ 40/160]  eta: 0:01:59  lr: 0.000001  min_lr: 0.000000  loss: 1.8092 (1.7555)  loss_scale: 16384.0000 (16783.6098)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3053 (8.8190)  time: 0.8407 (0.5410 -- 2.7008)  data: 0.0688 (0.0005 -- 1.1435)  max mem: 16413
Epoch: [182]  [ 60/160]  eta: 0:01:36  lr: 0.000001  min_lr: 0.000000  loss: 1.6265 (1.6928)  loss_scale: 16384.0000 (16652.5902)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2224 (9.0860)  time: 0.9077 (0.5341 -- 4.1157)  data: 0.0017 (0.0004 -- 0.0052)  max mem: 16413
Epoch: [182]  [ 80/160]  eta: 0:01:17  lr: 0.000001  min_lr: 0.000000  loss: 1.8399 (1.7052)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1959 (8.9161)  time: 0.9536 (0.5208 -- 2.9492)  data: 0.0013 (0.0004 -- 0.0046)  max mem: 16413
Epoch: [182]  [100/160]  eta: 0:00:55  lr: 0.000001  min_lr: 0.000000  loss: 1.7584 (1.6996)  loss_scale: 16384.0000 (16546.2178)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2709 (8.9661)  time: 0.7735 (0.5154 -- 2.0801)  data: 0.0014 (0.0004 -- 0.0038)  max mem: 16413
[2023-08-30 20:47:45,589] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29222
[2023-08-30 20:47:45,589] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29222
[2023-08-30 20:47:45,590] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:47:45,590] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:47:45,590] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [182]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7383 (1.7181)  loss_scale: 8192.0000 (15233.0579)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4680 (8.9721)  time: 0.9211 (0.5184 -- 3.5176)  data: 0.0013 (0.0004 -- 0.0031)  max mem: 16413
Epoch: [182]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7344 (1.7132)  loss_scale: 8192.0000 (14234.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1879 (8.8282)  time: 0.8251 (0.5362 -- 3.4881)  data: 0.0019 (0.0004 -- 0.0138)  max mem: 16413
Epoch: [182]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7717 (1.7259)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9889 (8.9026)  time: 0.7289 (0.4943 -- 2.9775)  data: 0.0007 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [182] Total time: 0:02:22 (0.8902 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7717 (1.7373)  loss_scale: 8192.0000 (13516.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9889 (8.9026)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2815 (0.2815)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3226 (2.3226 -- 2.3226)  data: 2.0672 (2.0672 -- 2.0672)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3949 (0.6933)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (95.9596)  time: 0.4142 (0.1932 -- 2.3226)  data: 0.1975 (0.0009 -- 2.0672)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4522 (0.6372)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (95.7672)  time: 0.2209 (0.1697 -- 0.3287)  data: 0.0165 (0.0001 -- 0.1150)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5485 (0.6931)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.4357)  time: 0.2054 (0.1332 -- 0.3287)  data: 0.0161 (0.0001 -- 0.1150)  max mem: 16413
Val: Total time: 0:00:07 (0.2867 s / it)
* Acc@1 82.988 Acc@5 96.888 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [183]  [  0/160]  eta: 0:22:30  lr: 0.000000  min_lr: 0.000000  loss: 1.9808 (1.9808)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8116 (8.8116)  time: 8.4408 (8.4408 -- 8.4408)  data: 5.0531 (5.0531 -- 5.0531)  max mem: 16413
Epoch: [183]  [ 20/160]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 1.8838 (1.8090)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3386 (8.5444)  time: 0.8000 (0.5178 -- 3.0503)  data: 0.0025 (0.0003 -- 0.0165)  max mem: 16413
Epoch: [183]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.7758 (1.7872)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4250 (8.7765)  time: 0.9118 (0.5227 -- 4.1986)  data: 0.0890 (0.0001 -- 1.2293)  max mem: 16413
Epoch: [183]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.7530 (1.7846)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7060 (8.6000)  time: 0.8783 (0.5228 -- 4.5004)  data: 0.0016 (0.0004 -- 0.0054)  max mem: 16413
[2023-08-30 20:49:50,486] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:49:50,486] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:49:50,489] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:49:50,490] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [183]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.6232 (1.7517)  loss_scale: 8192.0000 (9203.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9780 (8.8595)  time: 0.8833 (0.5263 -- 2.5458)  data: 0.0013 (0.0004 -- 0.0034)  max mem: 16413
Epoch: [183]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.6950 (1.7469)  loss_scale: 16384.0000 (10625.2673)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0510 (9.0405)  time: 0.7473 (0.5263 -- 2.6479)  data: 0.0233 (0.0002 -- 0.4392)  max mem: 16413
Epoch: [183]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7679 (1.7432)  loss_scale: 16384.0000 (11577.1240)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8029 (8.9548)  time: 0.9109 (0.5243 -- 2.4998)  data: 0.0876 (0.0006 -- 1.2479)  max mem: 16413
Epoch: [183]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7421 (1.7404)  loss_scale: 16384.0000 (12258.9504)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4276 (9.0310)  time: 0.8597 (0.5237 -- 3.6657)  data: 0.0098 (0.0004 -- 0.1721)  max mem: 16413
[2023-08-30 20:50:55,978] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29428
[2023-08-30 20:50:55,978] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:50:55,978] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29428
[2023-08-30 20:50:55,978] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-08-30 20:50:55,978] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
Epoch: [183]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7693 (1.7414)  loss_scale: 8192.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4196 (9.0276)  time: 0.6903 (0.4949 -- 3.4177)  data: 0.0008 (0.0002 -- 0.0025)  max mem: 16413
Epoch: [183] Total time: 0:02:21 (0.8847 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7693 (1.7371)  loss_scale: 8192.0000 (12134.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4196 (9.0276)
Val:  [ 0/27]  eta: 0:01:00  loss: 0.2819 (0.2819)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2499 (2.2499 -- 2.2499)  data: 2.0388 (2.0388 -- 2.0388)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3955 (0.6934)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4247 (0.2081 -- 2.2499)  data: 0.1989 (0.0006 -- 2.0388)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4513 (0.6369)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2224 (0.1701 -- 0.3562)  data: 0.0077 (0.0001 -- 0.1139)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5474 (0.6931)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2018 (0.1327 -- 0.3562)  data: 0.0073 (0.0001 -- 0.1139)  max mem: 16413
Val: Total time: 0:00:07 (0.2845 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [184]  [  0/160]  eta: 0:17:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5539 (1.5539)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0042 (8.0042)  time: 6.6071 (6.6071 -- 6.6071)  data: 5.6203 (5.6203 -- 5.6203)  max mem: 16413
Epoch: [184]  [ 20/160]  eta: 0:02:33  lr: 0.000000  min_lr: 0.000000  loss: 1.7983 (1.7652)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7748 (8.6944)  time: 0.8193 (0.5372 -- 2.3215)  data: 0.2532 (0.0007 -- 1.7910)  max mem: 16413
Epoch: [184]  [ 40/160]  eta: 0:01:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8176 (1.7828)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7328 (8.9368)  time: 0.8435 (0.5337 -- 2.5934)  data: 0.1863 (0.0003 -- 2.0300)  max mem: 16413
Epoch: [184]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7418 (1.7611)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1545 (9.1104)  time: 0.9534 (0.5262 -- 2.5858)  data: 0.2244 (0.0003 -- 2.0567)  max mem: 16413
Epoch: [184]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7504 (1.7470)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9025 (9.1802)  time: 0.8562 (0.5096 -- 4.3111)  data: 0.3045 (0.0003 -- 3.7946)  max mem: 16413
Epoch: [184]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.9107 (1.7542)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0272 (9.0200)  time: 0.8276 (0.5348 -- 2.6602)  data: 0.2244 (0.0006 -- 2.1406)  max mem: 16413
[2023-08-30 20:52:56,322] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:52:56,322] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 20:52:56,324] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:52:56,324] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [184]  [120/160]  eta: 0:00:35  lr: 0.000000  min_lr: 0.000000  loss: 1.6508 (1.7394)  loss_scale: 8192.0000 (8462.8099)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6245 (8.9689)  time: 0.7915 (0.5299 -- 2.2306)  data: 0.1014 (0.0003 -- 1.3564)  max mem: 16413
Epoch: [184]  [140/160]  eta: 0:00:17  lr: 0.000000  min_lr: 0.000000  loss: 1.7309 (1.7452)  loss_scale: 16384.0000 (9586.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4275 (8.9983)  time: 0.8525 (0.5309 -- 2.9344)  data: 0.1828 (0.0008 -- 2.0648)  max mem: 16413
Epoch: [184]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5488 (1.7425)  loss_scale: 16384.0000 (10393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0350 (9.0451)  time: 0.7414 (0.4961 -- 3.1688)  data: 0.0771 (0.0002 -- 1.3554)  max mem: 16413
Epoch: [184] Total time: 0:02:19 (0.8737 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5488 (1.7426)  loss_scale: 16384.0000 (10393.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0350 (9.0451)
Val:  [ 0/27]  eta: 0:01:04  loss: 0.2816 (0.2816)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3706 (2.3706 -- 2.3706)  data: 2.0833 (2.0833 -- 2.0833)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3915 (0.6933)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4307 (0.1914 -- 2.3706)  data: 0.2040 (0.0006 -- 2.0833)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4510 (0.6366)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2213 (0.1689 -- 0.3782)  data: 0.0142 (0.0001 -- 0.1274)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5467 (0.6929)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2053 (0.1332 -- 0.3782)  data: 0.0139 (0.0001 -- 0.1274)  max mem: 16413
Val: Total time: 0:00:07 (0.2884 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [185]  [  0/160]  eta: 0:21:27  lr: 0.000000  min_lr: 0.000000  loss: 1.8635 (1.8635)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2425 (11.2425)  time: 8.0459 (8.0459 -- 8.0459)  data: 7.2076 (7.2076 -- 7.2076)  max mem: 16413
Epoch: [185]  [ 20/160]  eta: 0:02:35  lr: 0.000000  min_lr: 0.000000  loss: 1.5292 (1.5860)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6181 (8.2904)  time: 0.7674 (0.5320 -- 3.2689)  data: 0.2124 (0.0005 -- 2.7187)  max mem: 16413
Epoch: [185]  [ 40/160]  eta: 0:02:13  lr: 0.000000  min_lr: 0.000000  loss: 1.8363 (1.6838)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9433 (8.4155)  time: 1.1129 (0.5170 -- 6.1507)  data: 0.5748 (0.0004 -- 5.6475)  max mem: 16413
Epoch: [185]  [ 60/160]  eta: 0:01:40  lr: 0.000000  min_lr: 0.000000  loss: 1.7120 (1.7089)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8077 (8.7490)  time: 0.7821 (0.5291 -- 5.2018)  data: 0.2353 (0.0004 -- 4.6808)  max mem: 16413
Epoch: [185]  [ 80/160]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 1.9084 (1.7447)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1823 (9.0894)  time: 0.9597 (0.5290 -- 4.6317)  data: 0.4091 (0.0004 -- 4.1081)  max mem: 16413
[2023-08-30 20:55:00,555] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:55:00,556] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:55:00,556] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:55:00,556] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 20:55:05,948] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29688
[2023-08-30 20:55:05,949] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29688
[2023-08-30 20:55:05,949] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:55:05,949] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 20:55:05,949] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [185]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8699 (1.7607)  loss_scale: 16384.0000 (16870.6535)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1482 (8.9701)  time: 0.7209 (0.5084 -- 4.2686)  data: 0.1893 (0.0001 -- 3.7707)  max mem: 16413
Epoch: [185]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7113 (1.7582)  loss_scale: 16384.0000 (16790.2149)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7029 (8.8440)  time: 0.8525 (0.5319 -- 3.1971)  data: 0.3035 (0.0004 -- 2.6666)  max mem: 16413
Epoch: [185]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6357 (1.7541)  loss_scale: 16384.0000 (16732.5957)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9768 (8.8691)  time: 0.9135 (0.5252 -- 3.5144)  data: 0.0876 (0.0002 -- 1.5230)  max mem: 16413
[2023-08-30 20:55:48,230] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29741
[2023-08-30 20:55:48,230] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29741
[2023-08-30 20:55:48,230] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:55:48,230] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 20:55:48,230] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [185]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8895 (1.7545)  loss_scale: 8192.0000 (15718.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7659 (8.8589)  time: 0.6615 (0.4970 -- 2.5748)  data: 0.0007 (0.0002 -- 0.0024)  max mem: 16413
Epoch: [185] Total time: 0:02:22 (0.8934 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8895 (1.7645)  loss_scale: 8192.0000 (15718.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7659 (8.8589)
Val:  [ 0/27]  eta: 0:01:08  loss: 0.2822 (0.2822)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.5353 (2.5353 -- 2.5353)  data: 2.2759 (2.2759 -- 2.2759)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3926 (0.6944)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4308 (0.2041 -- 2.5353)  data: 0.2080 (0.0007 -- 2.2759)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4502 (0.6370)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2137 (0.1707 -- 0.2551)  data: 0.0028 (0.0001 -- 0.0397)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5467 (0.6935)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.1979 (0.1326 -- 0.2551)  data: 0.0024 (0.0001 -- 0.0397)  max mem: 16413
Val: Total time: 0:00:07 (0.2889 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [186]  [  0/160]  eta: 0:24:09  lr: 0.000000  min_lr: 0.000000  loss: 1.4107 (1.4107)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4095 (11.4095)  time: 9.0612 (9.0612 -- 9.0612)  data: 6.6182 (6.6182 -- 6.6182)  max mem: 16413
Epoch: [186]  [ 20/160]  eta: 0:02:58  lr: 0.000000  min_lr: 0.000000  loss: 1.5754 (1.6310)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5327 (8.6543)  time: 0.8821 (0.5226 -- 3.6275)  data: 0.1634 (0.0004 -- 2.7513)  max mem: 16413
Epoch: [186]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.8881 (1.7844)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8691 (9.1520)  time: 0.7612 (0.5170 -- 3.5887)  data: 0.0021 (0.0002 -- 0.0151)  max mem: 16413
Epoch: [186]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.8024 (1.7872)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1910 (8.7856)  time: 0.8969 (0.5328 -- 3.2920)  data: 0.0023 (0.0004 -- 0.0208)  max mem: 16413
Epoch: [186]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.9481 (1.7985)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3744 (9.0847)  time: 0.8412 (0.5280 -- 2.2435)  data: 0.0015 (0.0003 -- 0.0038)  max mem: 16413
Epoch: [186]  [100/160]  eta: 0:00:54  lr: 0.000000  min_lr: 0.000000  loss: 1.7557 (1.7791)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3589 (9.1329)  time: 0.7890 (0.5256 -- 2.7781)  data: 0.0015 (0.0003 -- 0.0038)  max mem: 16413
[2023-08-30 20:57:47,055] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29867
[2023-08-30 20:57:47,055] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 20:57:47,055] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 29867
[2023-08-30 20:57:47,055] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 20:57:47,055] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [186]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6015 (1.7514)  loss_scale: 4096.0000 (7718.0826)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1528 (9.0420)  time: 0.8760 (0.5296 -- 3.0989)  data: 0.0048 (0.0004 -- 0.0586)  max mem: 16413
Epoch: [186]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7657 (1.7306)  loss_scale: 4096.0000 (7204.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1586 (9.0600)  time: 0.9598 (0.5140 -- 3.6357)  data: 0.2387 (0.0003 -- 3.1057)  max mem: 16413
Epoch: [186]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6971 (1.7267)  loss_scale: 4096.0000 (6835.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8785 (9.0220)  time: 0.5433 (0.4948 -- 0.8232)  data: 0.0008 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [186] Total time: 0:02:19 (0.8722 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6971 (1.7555)  loss_scale: 4096.0000 (6835.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8785 (9.0220)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2817 (0.2817)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2777 (2.2777 -- 2.2777)  data: 2.0610 (2.0610 -- 2.0610)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3926 (0.6938)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4082 (0.1945 -- 2.2777)  data: 0.1954 (0.0008 -- 2.0610)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4498 (0.6367)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2185 (0.1701 -- 0.4218)  data: 0.0152 (0.0001 -- 0.2127)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5468 (0.6932)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2051 (0.1328 -- 0.4218)  data: 0.0149 (0.0001 -- 0.2127)  max mem: 16413
Val: Total time: 0:00:07 (0.2832 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [187]  [  0/160]  eta: 0:21:21  lr: 0.000000  min_lr: 0.000000  loss: 2.3141 (2.3141)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8181 (10.8181)  time: 8.0067 (8.0067 -- 8.0067)  data: 6.7554 (6.7554 -- 6.7554)  max mem: 16413
Epoch: [187]  [ 20/160]  eta: 0:02:43  lr: 0.000000  min_lr: 0.000000  loss: 1.8812 (1.8164)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0995 (8.6064)  time: 0.8281 (0.5104 -- 5.9607)  data: 0.1016 (0.0003 -- 1.9950)  max mem: 16413
Epoch: [187]  [ 40/160]  eta: 0:02:11  lr: 0.000000  min_lr: 0.000000  loss: 1.9950 (1.8559)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1553 (9.3999)  time: 1.0244 (0.5237 -- 4.5137)  data: 0.0022 (0.0002 -- 0.0159)  max mem: 16413
Epoch: [187]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7033 (1.8087)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2685 (9.4574)  time: 0.7322 (0.5143 -- 2.5744)  data: 0.0018 (0.0004 -- 0.0100)  max mem: 16413
[2023-08-30 20:59:48,760] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:59:48,761] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-30 20:59:48,764] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 20:59:48,802] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-30 20:59:50,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=30000, skipped=177, lr=[6.73435885986037e-09, 6.73435885986037e-09, 8.979145146480493e-09, 8.979145146480493e-09, 1.1972193528640657e-08, 1.1972193528640657e-08, 1.596292470485421e-08, 1.596292470485421e-08, 2.128389960647228e-08, 2.128389960647228e-08, 2.8378532808629706e-08, 2.8378532808629706e-08, 3.783804374483961e-08, 3.783804374483961e-08, 5.045072499311948e-08, 5.045072499311948e-08, 6.72676333241593e-08, 6.72676333241593e-08, 8.969017776554574e-08, 8.969017776554574e-08, 1.1958690368739433e-07, 1.1958690368739433e-07, 1.5944920491652577e-07, 1.5944920491652577e-07, 2.12598939888701e-07, 2.12598939888701e-07, 2.834652531849347e-07, 2.834652531849347e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 20:59:50,503] [INFO] [timer.py:260:stop] epoch=0/micro_step=30000/global_step=30000, RunningAvgSamplesPerSec=17.705911517472, CurrSamplesPerSec=22.64861196858727, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [187]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.8295 (1.8071)  loss_scale: 4096.0000 (4348.8395)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8208 (9.6345)  time: 0.7950 (0.5283 -- 2.7676)  data: 0.1896 (0.0005 -- 2.2374)  max mem: 16413
Epoch: [187]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.6744 (1.7897)  loss_scale: 8192.0000 (5109.8614)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3418 (9.5170)  time: 0.9395 (0.5199 -- 4.4666)  data: 0.0015 (0.0002 -- 0.0048)  max mem: 16413
Epoch: [187]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7426 (1.7760)  loss_scale: 8192.0000 (5619.3058)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6074 (9.2517)  time: 0.8211 (0.5273 -- 2.3616)  data: 0.0219 (0.0004 -- 0.4061)  max mem: 16413
Epoch: [187]  [140/160]  eta: 0:00:17  lr: 0.000000  min_lr: 0.000000  loss: 1.7803 (1.7813)  loss_scale: 8192.0000 (5984.2270)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8104 (9.2788)  time: 0.7840 (0.5337 -- 1.8825)  data: 0.1797 (0.0005 -- 1.3573)  max mem: 16413
Epoch: [187]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6703 (1.7673)  loss_scale: 8192.0000 (6246.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5246 (9.2710)  time: 0.7622 (0.4952 -- 2.5854)  data: 0.0410 (0.0002 -- 0.7894)  max mem: 16413
Epoch: [187] Total time: 0:02:21 (0.8826 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6703 (1.7806)  loss_scale: 8192.0000 (6246.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5246 (9.2710)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2815 (0.2815)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1726 (2.1726 -- 2.1726)  data: 1.9699 (1.9699 -- 1.9699)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3917 (0.6932)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.3961 (0.1992 -- 2.1726)  data: 0.1802 (0.0005 -- 1.9699)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4481 (0.6367)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2243 (0.1713 -- 0.4943)  data: 0.0150 (0.0001 -- 0.2831)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5471 (0.6931)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2091 (0.1331 -- 0.4943)  data: 0.0147 (0.0001 -- 0.2831)  max mem: 16413
Val: Total time: 0:00:07 (0.2849 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [188]  [  0/160]  eta: 0:23:38  lr: 0.000000  min_lr: 0.000000  loss: 1.4710 (1.4710)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1634 (8.1634)  time: 8.8631 (8.8631 -- 8.8631)  data: 8.3419 (8.3419 -- 8.3419)  max mem: 16413
Epoch: [188]  [ 20/160]  eta: 0:03:06  lr: 0.000000  min_lr: 0.000000  loss: 1.8795 (1.8270)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0318 (9.0965)  time: 0.9538 (0.5059 -- 4.9578)  data: 0.1926 (0.0002 -- 2.7331)  max mem: 16413
[2023-08-30 21:01:42,492] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30112
[2023-08-30 21:01:42,492] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 21:01:42,492] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30112
[2023-08-30 21:01:42,492] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-08-30 21:01:42,493] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [188]  [ 40/160]  eta: 0:02:09  lr: 0.000000  min_lr: 0.000000  loss: 1.8654 (1.8098)  loss_scale: 8192.0000 (7292.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8323 (9.0342)  time: 0.8167 (0.5219 -- 3.6532)  data: 0.0020 (0.0003 -- 0.0078)  max mem: 16413
Epoch: [188]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.8076 (1.8109)  loss_scale: 4096.0000 (6244.7213)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0751 (8.9430)  time: 0.7786 (0.5239 -- 2.9091)  data: 0.0191 (0.0003 -- 0.3519)  max mem: 16413
Epoch: [188]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7299 (1.8079)  loss_scale: 4096.0000 (5714.1728)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3529 (9.3397)  time: 0.9907 (0.5140 -- 4.1148)  data: 0.0078 (0.0006 -- 0.1239)  max mem: 16413
Epoch: [188]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7500 (1.7934)  loss_scale: 4096.0000 (5393.7426)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9949 (9.2875)  time: 0.8001 (0.5167 -- 3.6828)  data: 0.0011 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [188]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6710 (1.7820)  loss_scale: 4096.0000 (5179.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1569 (9.2591)  time: 0.9695 (0.5267 -- 4.3638)  data: 0.0015 (0.0006 -- 0.0024)  max mem: 16413
Epoch: [188]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9132 (1.7930)  loss_scale: 4096.0000 (5025.5887)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5904 (9.1918)  time: 0.7502 (0.5188 -- 3.4705)  data: 0.0013 (0.0004 -- 0.0028)  max mem: 16413
Epoch: [188]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.5016 (1.7631)  loss_scale: 4096.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3600 (9.1666)  time: 0.6635 (0.4979 -- 2.0238)  data: 0.0244 (0.0002 -- 0.4736)  max mem: 16413
Epoch: [188] Total time: 0:02:22 (0.8926 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.5016 (1.7851)  loss_scale: 4096.0000 (4915.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3600 (9.1666)
Val:  [ 0/27]  eta: 0:00:58  loss: 0.2805 (0.2805)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.1787 (2.1787 -- 2.1787)  data: 1.9702 (1.9702 -- 1.9702)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3915 (0.6928)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4214 (0.1893 -- 2.1787)  data: 0.2051 (0.0006 -- 1.9702)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4475 (0.6364)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2277 (0.1708 -- 0.3761)  data: 0.0211 (0.0001 -- 0.1499)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5474 (0.6928)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2140 (0.1334 -- 0.3761)  data: 0.0209 (0.0001 -- 0.1499)  max mem: 16413
Val: Total time: 0:00:07 (0.2859 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [189]  [  0/160]  eta: 0:23:34  lr: 0.000000  min_lr: 0.000000  loss: 1.6955 (1.6955)  loss_scale: 4096.0000 (4096.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2059 (10.2059)  time: 8.8407 (8.8407 -- 8.8407)  data: 8.3071 (8.3071 -- 8.3071)  max mem: 16413
[2023-08-30 21:03:44,475] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:03:44,476] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
[2023-08-30 21:03:44,477] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:03:44,478] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [189]  [ 20/160]  eta: 0:02:50  lr: 0.000000  min_lr: 0.000000  loss: 1.8134 (1.8381)  loss_scale: 8192.0000 (7996.9524)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0873 (9.2878)  time: 0.8377 (0.5362 -- 3.1052)  data: 0.1810 (0.0003 -- 2.1005)  max mem: 16413
Epoch: [189]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 1.8072 (1.8243)  loss_scale: 8192.0000 (8092.0976)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2129 (9.4924)  time: 0.9131 (0.5242 -- 3.7285)  data: 0.1055 (0.0002 -- 0.9971)  max mem: 16413
Epoch: [189]  [ 60/160]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 1.7471 (1.8060)  loss_scale: 8192.0000 (8124.8525)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2433 (9.4479)  time: 0.7207 (0.5251 -- 2.4346)  data: 0.1730 (0.0002 -- 1.8857)  max mem: 16413
Epoch: [189]  [ 80/160]  eta: 0:01:16  lr: 0.000000  min_lr: 0.000000  loss: 1.7887 (1.7975)  loss_scale: 8192.0000 (8141.4321)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4686 (9.3880)  time: 0.9497 (0.5230 -- 3.4605)  data: 0.2943 (0.0003 -- 2.5190)  max mem: 16413
Epoch: [189]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.8004 (1.8021)  loss_scale: 8192.0000 (8151.4455)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5265 (9.2474)  time: 0.8258 (0.5271 -- 3.8256)  data: 0.2760 (0.0004 -- 3.2840)  max mem: 16413
Epoch: [189]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5673 (1.7857)  loss_scale: 8192.0000 (8158.1488)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3374 (9.3954)  time: 0.9191 (0.5199 -- 3.2866)  data: 0.3717 (0.0001 -- 2.7392)  max mem: 16413
[2023-08-30 21:05:34,885] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:05:34,885] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 21:05:34,886] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:05:34,886] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 21:05:43,978] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30379
[2023-08-30 21:05:43,978] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30379
[2023-08-30 21:05:43,979] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:05:43,979] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:05:43,979] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [189]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7329 (1.7770)  loss_scale: 8192.0000 (8743.9433)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8994 (9.3135)  time: 0.8629 (0.5256 -- 4.0515)  data: 0.3121 (0.0003 -- 3.5089)  max mem: 16413
Epoch: [189]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7594 (1.7708)  loss_scale: 8192.0000 (8678.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3035 (9.3841)  time: 0.6275 (0.4943 -- 1.6034)  data: 0.1007 (0.0002 -- 1.0463)  max mem: 16413
Epoch: [189] Total time: 0:02:21 (0.8840 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7594 (1.7699)  loss_scale: 8192.0000 (8678.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3035 (9.3841)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2808 (0.2808)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2137 (2.2137 -- 2.2137)  data: 1.9818 (1.9818 -- 1.9818)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3927 (0.6930)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4054 (0.2001 -- 2.2137)  data: 0.1900 (0.0005 -- 1.9818)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4464 (0.6363)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2265 (0.1686 -- 0.3838)  data: 0.0188 (0.0001 -- 0.1614)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5475 (0.6927)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2100 (0.1329 -- 0.3838)  data: 0.0184 (0.0001 -- 0.1614)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [190]  [  0/160]  eta: 0:17:40  lr: 0.000000  min_lr: 0.000000  loss: 1.1615 (1.1615)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0644 (5.0644)  time: 6.6306 (6.6306 -- 6.6306)  data: 5.7545 (5.7545 -- 5.7545)  max mem: 16413
Epoch: [190]  [ 20/160]  eta: 0:02:51  lr: 0.000000  min_lr: 0.000000  loss: 1.6912 (1.7211)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3209 (9.6160)  time: 0.9535 (0.5218 -- 3.6707)  data: 0.1841 (0.0006 -- 1.8367)  max mem: 16413
Epoch: [190]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.8201 (1.7515)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3445 (9.3333)  time: 0.8965 (0.5189 -- 3.9525)  data: 0.0550 (0.0001 -- 0.5850)  max mem: 16413
Epoch: [190]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.7798 (1.7670)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3190 (9.2034)  time: 0.7517 (0.5316 -- 2.1125)  data: 0.0602 (0.0007 -- 1.1731)  max mem: 16413
Epoch: [190]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7175 (1.7669)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4712 (9.0682)  time: 0.8909 (0.5325 -- 2.2026)  data: 0.3238 (0.0003 -- 1.6784)  max mem: 16413
Epoch: [190]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 2.0320 (1.7965)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0784 (9.1781)  time: 0.9076 (0.5120 -- 3.8511)  data: 0.3673 (0.0005 -- 3.3279)  max mem: 16413
[2023-08-30 21:07:46,355] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:07:46,356] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 21:07:46,357] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:07:46,357] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [190]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.6023 (1.7862)  loss_scale: 16384.0000 (9072.1322)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8510 (9.0303)  time: 0.8424 (0.5248 -- 2.0412)  data: 0.2918 (0.0005 -- 1.5097)  max mem: 16413
Epoch: [190]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6655 (1.7739)  loss_scale: 16384.0000 (10109.2766)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2748 (9.0889)  time: 0.8490 (0.5322 -- 2.9010)  data: 0.2592 (0.0003 -- 2.3624)  max mem: 16413
Epoch: [190]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8114 (1.7806)  loss_scale: 16384.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2895 (9.1428)  time: 0.6882 (0.4957 -- 2.9333)  data: 0.0011 (0.0002 -- 0.0054)  max mem: 16413
Epoch: [190] Total time: 0:02:21 (0.8855 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8114 (1.7738)  loss_scale: 16384.0000 (10854.4000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2895 (9.1428)
Val:  [ 0/27]  eta: 0:00:56  loss: 0.2810 (0.2810)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.0933 (2.0933 -- 2.0933)  data: 1.8842 (1.8842 -- 1.8842)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3920 (0.6932)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.3997 (0.2043 -- 2.0933)  data: 0.1874 (0.0007 -- 1.8842)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4465 (0.6365)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2265 (0.1699 -- 0.3313)  data: 0.0185 (0.0001 -- 0.1151)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5475 (0.6929)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2121 (0.1328 -- 0.3313)  data: 0.0182 (0.0001 -- 0.1151)  max mem: 16413
Val: Total time: 0:00:07 (0.2819 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [191]  [  0/160]  eta: 0:21:53  lr: 0.000000  min_lr: 0.000000  loss: 1.1343 (1.1343)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7937 (8.7937)  time: 8.2106 (8.2106 -- 8.2106)  data: 7.6776 (7.6776 -- 7.6776)  max mem: 16413
Epoch: [191]  [ 20/160]  eta: 0:02:52  lr: 0.000000  min_lr: 0.000000  loss: 1.6457 (1.6421)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4764 (8.9508)  time: 0.8819 (0.5258 -- 3.8709)  data: 0.2503 (0.0004 -- 2.7907)  max mem: 16413
Epoch: [191]  [ 40/160]  eta: 0:02:03  lr: 0.000000  min_lr: 0.000000  loss: 1.8668 (1.7687)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4928 (8.9107)  time: 0.8182 (0.5293 -- 2.5626)  data: 0.1681 (0.0006 -- 1.7635)  max mem: 16413
Epoch: [191]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.8054 (1.7455)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4926 (8.9604)  time: 0.8742 (0.5085 -- 3.1688)  data: 0.0702 (0.0004 -- 1.3671)  max mem: 16413
[2023-08-30 21:09:45,500] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:09:45,500] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 21:09:45,501] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:09:45,501] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [191]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7306 (1.7402)  loss_scale: 16384.0000 (17395.3580)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9716 (9.0418)  time: 0.8619 (0.5303 -- 3.4536)  data: 0.1511 (0.0004 -- 1.5015)  max mem: 16413
[2023-08-30 21:09:57,730] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30650
[2023-08-30 21:09:57,731] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:09:57,731] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-08-30 21:09:57,731] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30650
[2023-08-30 21:09:57,731] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
Epoch: [191]  [100/160]  eta: 0:00:53  lr: 0.000000  min_lr: 0.000000  loss: 1.8137 (1.7636)  loss_scale: 16384.0000 (18655.0495)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3877 (9.0226)  time: 0.6872 (0.5330 -- 1.6117)  data: 0.0219 (0.0005 -- 0.3974)  max mem: 16413
Epoch: [191]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.5306 (1.7406)  loss_scale: 16384.0000 (18279.6694)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3246 (8.9974)  time: 0.9558 (0.5289 -- 3.2137)  data: 0.2649 (0.0002 -- 2.6961)  max mem: 16413
Epoch: [191]  [140/160]  eta: 0:00:17  lr: 0.000000  min_lr: 0.000000  loss: 1.6440 (1.7301)  loss_scale: 16384.0000 (18010.7801)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9300 (9.0081)  time: 0.8255 (0.5249 -- 2.0946)  data: 0.1366 (0.0002 -- 1.0068)  max mem: 16413
Epoch: [191]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8458 (1.7355)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6579 (8.9391)  time: 0.7077 (0.4949 -- 2.6369)  data: 0.0258 (0.0002 -- 0.5037)  max mem: 16413
Epoch: [191] Total time: 0:02:19 (0.8749 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8458 (1.7406)  loss_scale: 16384.0000 (17817.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6579 (8.9391)
Val:  [ 0/27]  eta: 0:01:01  loss: 0.2810 (0.2810)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2894 (2.2894 -- 2.2894)  data: 2.0832 (2.0832 -- 2.0832)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3928 (0.6931)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4292 (0.1889 -- 2.2894)  data: 0.2208 (0.0005 -- 2.0832)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4461 (0.6362)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2258 (0.1701 -- 0.5481)  data: 0.0241 (0.0001 -- 0.3340)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5474 (0.6927)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2122 (0.1331 -- 0.5481)  data: 0.0237 (0.0001 -- 0.3340)  max mem: 16413
Val: Total time: 0:00:07 (0.2886 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [192]  [  0/160]  eta: 0:17:06  lr: 0.000000  min_lr: 0.000000  loss: 1.0945 (1.0945)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2644 (11.2644)  time: 6.4143 (6.4143 -- 6.4143)  data: 5.7144 (5.7144 -- 5.7144)  max mem: 16413
Epoch: [192]  [ 20/160]  eta: 0:02:48  lr: 0.000000  min_lr: 0.000000  loss: 1.7921 (1.7442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5072 (9.5407)  time: 0.9441 (0.5184 -- 4.3308)  data: 0.3757 (0.0004 -- 3.8101)  max mem: 16413
Epoch: [192]  [ 40/160]  eta: 0:02:04  lr: 0.000000  min_lr: 0.000000  loss: 1.8808 (1.7709)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3024 (9.9422)  time: 0.8595 (0.5357 -- 2.9604)  data: 0.0697 (0.0003 -- 1.2584)  max mem: 16413
[2023-08-30 21:11:58,201] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30776
[2023-08-30 21:11:58,201] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 30776
[2023-08-30 21:11:58,202] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:11:58,202] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:11:58,202] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [192]  [ 60/160]  eta: 0:01:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8276 (1.7931)  loss_scale: 16384.0000 (15712.5246)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5974 (9.4665)  time: 0.8250 (0.5186 -- 3.5647)  data: 0.0259 (0.0002 -- 0.3965)  max mem: 16413
Epoch: [192]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.7862 (1.7906)  loss_scale: 8192.0000 (13855.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6626 (9.2102)  time: 0.8234 (0.5353 -- 3.3820)  data: 0.0014 (0.0002 -- 0.0031)  max mem: 16413
Epoch: [192]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.8204 (1.8102)  loss_scale: 8192.0000 (12734.0990)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9962 (9.1930)  time: 0.8624 (0.5279 -- 3.3560)  data: 0.2628 (0.0004 -- 2.8247)  max mem: 16413
Epoch: [192]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8427 (1.8093)  loss_scale: 8192.0000 (11983.3388)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8081 (9.2402)  time: 0.9432 (0.5289 -- 3.2097)  data: 0.3978 (0.0004 -- 2.6965)  max mem: 16413
Epoch: [192]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8274 (1.8089)  loss_scale: 8192.0000 (11445.5603)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0585 (9.2987)  time: 0.7860 (0.5275 -- 3.1720)  data: 0.2397 (0.0002 -- 2.6585)  max mem: 16413
Epoch: [192]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8484 (1.8037)  loss_scale: 8192.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2117 (9.3009)  time: 0.7488 (0.4971 -- 2.3919)  data: 0.1150 (0.0002 -- 1.8635)  max mem: 16413
Epoch: [192] Total time: 0:02:21 (0.8858 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8484 (1.7907)  loss_scale: 8192.0000 (11059.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2117 (9.3009)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2810 (0.2810)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3310 (2.3310 -- 2.3310)  data: 2.0960 (2.0960 -- 2.0960)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3928 (0.6929)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4139 (0.2039 -- 2.3310)  data: 0.1983 (0.0008 -- 2.0960)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4463 (0.6362)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2263 (0.1701 -- 0.5659)  data: 0.0231 (0.0001 -- 0.3733)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5476 (0.6927)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2113 (0.1335 -- 0.5659)  data: 0.0227 (0.0001 -- 0.3733)  max mem: 16413
Val: Total time: 0:00:07 (0.2907 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [193]  [  0/160]  eta: 0:17:58  lr: 0.000000  min_lr: 0.000000  loss: 1.2043 (1.2043)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9390 (7.9390)  time: 6.7430 (6.7430 -- 6.7430)  data: 6.1950 (6.1950 -- 6.1950)  max mem: 16413
Epoch: [193]  [ 20/160]  eta: 0:02:42  lr: 0.000000  min_lr: 0.000000  loss: 1.6900 (1.7437)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3270 (8.5333)  time: 0.8839 (0.5333 -- 3.2185)  data: 0.0054 (0.0003 -- 0.0719)  max mem: 16413
[2023-08-30 21:14:00,080] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:14:00,080] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 21:14:00,080] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:14:00,080] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [193]  [ 40/160]  eta: 0:02:05  lr: 0.000000  min_lr: 0.000000  loss: 1.7010 (1.7677)  loss_scale: 16384.0000 (11388.8780)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4025 (8.9052)  time: 0.9202 (0.5227 -- 3.0182)  data: 0.0316 (0.0006 -- 0.6006)  max mem: 16413
Epoch: [193]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6788 (1.7418)  loss_scale: 16384.0000 (13026.6230)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1012 (8.9883)  time: 0.8487 (0.5289 -- 2.9408)  data: 0.0015 (0.0008 -- 0.0039)  max mem: 16413
Epoch: [193]  [ 80/160]  eta: 0:01:14  lr: 0.000000  min_lr: 0.000000  loss: 1.8898 (1.7679)  loss_scale: 16384.0000 (13855.6049)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0596 (9.1671)  time: 0.7720 (0.5109 -- 2.9638)  data: 0.0016 (0.0003 -- 0.0091)  max mem: 16413
Epoch: [193]  [100/160]  eta: 0:00:54  lr: 0.000000  min_lr: 0.000000  loss: 1.8274 (1.7834)  loss_scale: 16384.0000 (14356.2772)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0231 (9.0880)  time: 0.8523 (0.5206 -- 3.3868)  data: 0.0017 (0.0004 -- 0.0067)  max mem: 16413
[2023-08-30 21:15:18,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=31000, skipped=181, lr=[2.523784594471536e-09, 2.523784594471536e-09, 3.3650461259620475e-09, 3.3650461259620475e-09, 4.486728167949397e-09, 4.486728167949397e-09, 5.982304223932529e-09, 5.982304223932529e-09, 7.976405631910039e-09, 7.976405631910039e-09, 1.0635207509213385e-08, 1.0635207509213385e-08, 1.4180276678951179e-08, 1.4180276678951179e-08, 1.8907035571934907e-08, 1.8907035571934907e-08, 2.5209380762579874e-08, 2.5209380762579874e-08, 3.361250768343983e-08, 3.361250768343983e-08, 4.481667691125311e-08, 4.481667691125311e-08, 5.975556921500415e-08, 5.975556921500415e-08, 7.96740922866722e-08, 7.96740922866722e-08, 1.0623212304889626e-07, 1.0623212304889626e-07], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 21:15:18,701] [INFO] [timer.py:260:stop] epoch=0/micro_step=31000/global_step=31000, RunningAvgSamplesPerSec=17.69055858637376, CurrSamplesPerSec=21.695423980137246, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [193]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8397 (1.7793)  loss_scale: 16384.0000 (14691.4380)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7154 (9.1371)  time: 0.8604 (0.5209 -- 2.4187)  data: 0.0642 (0.0002 -- 0.9148)  max mem: 16413
[2023-08-30 21:15:36,185] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31016
[2023-08-30 21:15:36,185] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31016
[2023-08-30 21:15:36,185] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:15:36,185] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:15:36,186] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [193]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.7521 (1.7726)  loss_scale: 16384.0000 (14641.0213)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8885 (9.0658)  time: 0.9008 (0.5314 -- 3.3155)  data: 0.0255 (0.0002 -- 0.4717)  max mem: 16413
Epoch: [193]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8533 (1.7807)  loss_scale: 8192.0000 (13875.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8053 (8.9742)  time: 0.6379 (0.4981 -- 2.3911)  data: 0.0007 (0.0002 -- 0.0026)  max mem: 16413
Epoch: [193] Total time: 0:02:19 (0.8732 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8533 (1.7771)  loss_scale: 8192.0000 (13875.2000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8053 (8.9742)
Val:  [ 0/27]  eta: 0:00:59  loss: 0.2809 (0.2809)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.2076 (2.2076 -- 2.2076)  data: 1.9959 (1.9959 -- 1.9959)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3924 (0.6929)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4046 (0.1999 -- 2.2076)  data: 0.1935 (0.0004 -- 1.9959)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4469 (0.6363)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2227 (0.1701 -- 0.4310)  data: 0.0189 (0.0001 -- 0.2250)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5473 (0.6926)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2092 (0.1330 -- 0.4310)  data: 0.0186 (0.0001 -- 0.2250)  max mem: 16413
Val: Total time: 0:00:07 (0.2833 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [194]  [  0/160]  eta: 0:21:17  lr: 0.000000  min_lr: 0.000000  loss: 2.0429 (2.0429)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3602 (11.3602)  time: 7.9858 (7.9858 -- 7.9858)  data: 5.5251 (5.5251 -- 5.5251)  max mem: 16413
Epoch: [194]  [ 20/160]  eta: 0:02:51  lr: 0.000000  min_lr: 0.000000  loss: 1.8713 (1.8006)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3914 (8.9691)  time: 0.8852 (0.5217 -- 5.3839)  data: 0.0022 (0.0004 -- 0.0165)  max mem: 16413
Epoch: [194]  [ 40/160]  eta: 0:02:10  lr: 0.000000  min_lr: 0.000000  loss: 1.7908 (1.7982)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8273 (8.6295)  time: 0.9412 (0.5266 -- 3.9647)  data: 0.0013 (0.0002 -- 0.0030)  max mem: 16413
Epoch: [194]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6421 (1.7874)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6421 (8.7836)  time: 0.7744 (0.5265 -- 3.7138)  data: 0.0022 (0.0004 -- 0.0149)  max mem: 16413
Epoch: [194]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.7392 (1.7865)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8278 (8.7012)  time: 0.8280 (0.5351 -- 3.7354)  data: 0.0747 (0.0006 -- 1.1752)  max mem: 16413
Epoch: [194]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.8205 (1.8057)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9056 (8.7962)  time: 0.9116 (0.5256 -- 3.6916)  data: 0.0386 (0.0004 -- 0.7368)  max mem: 16413
[2023-08-30 21:17:39,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:17:39,130] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 21:17:39,130] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:17:39,130] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [194]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8353 (1.8085)  loss_scale: 16384.0000 (9275.2397)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9655 (8.9485)  time: 0.7768 (0.5427 -- 3.6397)  data: 0.0017 (0.0006 -- 0.0042)  max mem: 16413
[2023-08-30 21:18:02,103] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31174
[2023-08-30 21:18:02,103] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:18:02,103] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31174
[2023-08-30 21:18:02,103] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:18:02,103] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [194]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.6751 (1.7935)  loss_scale: 16384.0000 (9876.8794)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4087 (8.9614)  time: 0.9682 (0.5039 -- 4.0329)  data: 0.0165 (0.0003 -- 0.3091)  max mem: 16413
Epoch: [194]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7244 (1.7840)  loss_scale: 8192.0000 (9676.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9473 (8.9826)  time: 0.7210 (0.4941 -- 4.4650)  data: 0.0006 (0.0001 -- 0.0027)  max mem: 16413
Epoch: [194] Total time: 0:02:23 (0.8975 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7244 (1.7847)  loss_scale: 8192.0000 (9676.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9473 (8.9826)
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2806 (0.2806)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3538 (2.3538 -- 2.3538)  data: 2.1123 (2.1123 -- 2.1123)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3920 (0.6929)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4157 (0.1993 -- 2.3538)  data: 0.1930 (0.0007 -- 2.1123)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4470 (0.6364)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2266 (0.1717 -- 0.5598)  data: 0.0194 (0.0001 -- 0.3747)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5476 (0.6927)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2092 (0.1326 -- 0.5598)  data: 0.0191 (0.0001 -- 0.3747)  max mem: 16413
Val: Total time: 0:00:07 (0.2915 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [195]  [  0/160]  eta: 0:21:16  lr: 0.000000  min_lr: 0.000000  loss: 0.7785 (0.7785)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8379 (6.8379)  time: 7.9803 (7.9803 -- 7.9803)  data: 7.4591 (7.4591 -- 7.4591)  max mem: 16413
Epoch: [195]  [ 20/160]  eta: 0:02:51  lr: 0.000000  min_lr: 0.000000  loss: 1.7677 (1.6782)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4696 (8.3421)  time: 0.8900 (0.5197 -- 2.9047)  data: 0.3498 (0.0004 -- 2.3671)  max mem: 16413
Epoch: [195]  [ 40/160]  eta: 0:02:07  lr: 0.000000  min_lr: 0.000000  loss: 1.9394 (1.7976)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5502 (8.6646)  time: 0.8958 (0.5220 -- 3.5795)  data: 0.3469 (0.0005 -- 3.0555)  max mem: 16413
Epoch: [195]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.9167 (1.8125)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3858 (8.5081)  time: 0.8293 (0.5215 -- 3.7028)  data: 0.2770 (0.0003 -- 3.1567)  max mem: 16413
Epoch: [195]  [ 80/160]  eta: 0:01:19  lr: 0.000000  min_lr: 0.000000  loss: 1.8343 (1.8193)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6734 (8.6402)  time: 0.9921 (0.5168 -- 4.3492)  data: 0.4518 (0.0004 -- 3.8258)  max mem: 16413
Epoch: [195]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.6444 (1.7889)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9782 (8.7660)  time: 0.7518 (0.5112 -- 3.2188)  data: 0.2094 (0.0001 -- 2.6888)  max mem: 16413
[2023-08-30 21:20:06,735] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:20:06,735] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:20:06,735] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 21:20:06,735] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [195]  [120/160]  eta: 0:00:38  lr: 0.000000  min_lr: 0.000000  loss: 1.6596 (1.7708)  loss_scale: 16384.0000 (9410.6446)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7903 (8.7376)  time: 1.0448 (0.5126 -- 4.3924)  data: 0.5034 (0.0002 -- 3.8879)  max mem: 16413
Epoch: [195]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9608 (1.7953)  loss_scale: 16384.0000 (10399.7730)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1805 (8.8255)  time: 0.7707 (0.5238 -- 2.7982)  data: 0.2268 (0.0001 -- 2.2675)  max mem: 16413
[2023-08-30 21:20:43,016] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31343
[2023-08-30 21:20:43,016] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31343
[2023-08-30 21:20:43,016] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:20:43,016] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-08-30 21:20:43,016] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [195]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6481 (1.7769)  loss_scale: 8192.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9296 (8.8509)  time: 0.6561 (0.4965 -- 2.7672)  data: 0.1400 (0.0002 -- 2.2375)  max mem: 16413
Epoch: [195] Total time: 0:02:24 (0.9005 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6481 (1.7753)  loss_scale: 8192.0000 (10240.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9296 (8.8509)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2807 (0.2807)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3286 (2.3286 -- 2.3286)  data: 2.1216 (2.1216 -- 2.1216)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3920 (0.6932)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4250 (0.1978 -- 2.3286)  data: 0.2164 (0.0005 -- 2.1216)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4468 (0.6364)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2233 (0.1696 -- 0.4094)  data: 0.0211 (0.0001 -- 0.1802)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5473 (0.6927)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2102 (0.1323 -- 0.4094)  data: 0.0209 (0.0001 -- 0.1802)  max mem: 16413
Val: Total time: 0:00:07 (0.2880 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [196]  [  0/160]  eta: 0:22:58  lr: 0.000000  min_lr: 0.000000  loss: 1.4995 (1.4995)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1158 (10.1158)  time: 8.6130 (8.6130 -- 8.6130)  data: 8.0755 (8.0755 -- 8.0755)  max mem: 16413
Epoch: [196]  [ 20/160]  eta: 0:02:47  lr: 0.000000  min_lr: 0.000000  loss: 1.7921 (1.7395)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3807 (10.2522)  time: 0.8290 (0.5168 -- 3.4445)  data: 0.2875 (0.0003 -- 2.9133)  max mem: 16413
Epoch: [196]  [ 40/160]  eta: 0:02:08  lr: 0.000000  min_lr: 0.000000  loss: 1.8893 (1.7931)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1614 (9.6660)  time: 0.9336 (0.5207 -- 3.5559)  data: 0.2919 (0.0002 -- 3.0371)  max mem: 16413
Epoch: [196]  [ 60/160]  eta: 0:01:38  lr: 0.000000  min_lr: 0.000000  loss: 1.8296 (1.8043)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2107 (9.3732)  time: 0.8126 (0.5232 -- 3.5359)  data: 0.2649 (0.0003 -- 2.9863)  max mem: 16413
Epoch: [196]  [ 80/160]  eta: 0:01:18  lr: 0.000000  min_lr: 0.000000  loss: 1.9313 (1.8278)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3151 (9.5369)  time: 0.9860 (0.5225 -- 3.7566)  data: 0.4507 (0.0003 -- 3.2188)  max mem: 16413
Epoch: [196]  [100/160]  eta: 0:00:57  lr: 0.000000  min_lr: 0.000000  loss: 1.8057 (1.8186)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5983 (9.9461)  time: 0.8083 (0.5163 -- 3.7807)  data: 0.2669 (0.0003 -- 3.2637)  max mem: 16413
[2023-08-30 21:22:49,450] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:22:49,450] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[2023-08-30 21:22:49,450] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:22:49,450] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [196]  [120/160]  eta: 0:00:37  lr: 0.000000  min_lr: 0.000000  loss: 1.7257 (1.8058)  loss_scale: 8192.0000 (8801.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1201 (9.7035)  time: 0.8918 (0.5246 -- 3.9545)  data: 0.3452 (0.0002 -- 3.4336)  max mem: 16413
Epoch: [196]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8689 (1.8045)  loss_scale: 16384.0000 (9876.8794)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6327 (9.6580)  time: 0.8526 (0.5096 -- 4.6629)  data: 0.2179 (0.0005 -- 2.2345)  max mem: 16413
Epoch: [196]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7387 (1.7926)  loss_scale: 16384.0000 (10649.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3491 (9.5894)  time: 0.5910 (0.4965 -- 1.4936)  data: 0.0007 (0.0002 -- 0.0019)  max mem: 16413
Epoch: [196] Total time: 0:02:22 (0.8887 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7387 (1.7854)  loss_scale: 16384.0000 (10649.6000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3491 (9.5894)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2807 (0.2807)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3057 (2.3057 -- 2.3057)  data: 2.1122 (2.1122 -- 2.1122)  max mem: 16413
Val:  [10/27]  eta: 0:00:06  loss: 0.3924 (0.6931)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4059 (0.1998 -- 2.3057)  data: 0.1930 (0.0006 -- 2.1122)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4472 (0.6363)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2204 (0.1715 -- 0.3333)  data: 0.0100 (0.0001 -- 0.0987)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5475 (0.6926)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2045 (0.1329 -- 0.3333)  data: 0.0097 (0.0001 -- 0.0987)  max mem: 16413
Val: Total time: 0:00:07 (0.2854 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [197]  [  0/160]  eta: 0:18:01  lr: 0.000000  min_lr: 0.000000  loss: 2.0042 (2.0042)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4661 (9.4661)  time: 6.7581 (6.7581 -- 6.7581)  data: 5.6164 (5.6164 -- 5.6164)  max mem: 16413
Epoch: [197]  [ 20/160]  eta: 0:02:40  lr: 0.000000  min_lr: 0.000000  loss: 1.8254 (1.8033)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1284 (9.9372)  time: 0.8648 (0.5153 -- 3.3530)  data: 0.3021 (0.0006 -- 2.8341)  max mem: 16413
Epoch: [197]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.6993 (1.7601)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3349 (9.1665)  time: 0.8849 (0.5310 -- 3.2679)  data: 0.1687 (0.0003 -- 2.7328)  max mem: 16413
Epoch: [197]  [ 60/160]  eta: 0:01:35  lr: 0.000000  min_lr: 0.000000  loss: 1.8762 (1.7919)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5627 (9.1402)  time: 0.8394 (0.5246 -- 4.1315)  data: 0.0022 (0.0002 -- 0.0150)  max mem: 16413
[2023-08-30 21:24:47,631] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:24:47,631] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:24:47,632] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 21:24:47,632] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [197]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8849 (1.8033)  loss_scale: 16384.0000 (16586.2716)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.6333 (9.0684)  time: 0.8702 (0.5245 -- 4.3567)  data: 0.0014 (0.0002 -- 0.0036)  max mem: 16413
[2023-08-30 21:24:53,729] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31607
[2023-08-30 21:24:53,729] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31607
[2023-08-30 21:24:53,729] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:24:53,729] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:24:53,730] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [197]  [100/160]  eta: 0:00:55  lr: 0.000000  min_lr: 0.000000  loss: 1.6879 (1.7723)  loss_scale: 16384.0000 (17519.5248)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9891 (9.0945)  time: 0.8790 (0.5265 -- 3.1203)  data: 0.0016 (0.0003 -- 0.0039)  max mem: 16413
Epoch: [197]  [120/160]  eta: 0:00:35  lr: 0.000000  min_lr: 0.000000  loss: 1.8568 (1.7832)  loss_scale: 16384.0000 (17331.8347)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3291 (8.9399)  time: 0.7174 (0.5221 -- 2.3142)  data: 0.0015 (0.0003 -- 0.0046)  max mem: 16413
Epoch: [197]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.8305 (1.7876)  loss_scale: 16384.0000 (17197.3901)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9950 (8.9513)  time: 1.0218 (0.5353 -- 4.4956)  data: 0.0015 (0.0002 -- 0.0067)  max mem: 16413
Epoch: [197]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.8305 (1.7925)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2352 (8.8701)  time: 0.6849 (0.4972 -- 3.0267)  data: 0.0007 (0.0002 -- 0.0022)  max mem: 16413
Epoch: [197] Total time: 0:02:21 (0.8842 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.8305 (1.7570)  loss_scale: 16384.0000 (17100.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2352 (8.8701)
Val:  [ 0/27]  eta: 0:01:05  loss: 0.2807 (0.2807)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.4162 (2.4162 -- 2.4162)  data: 2.1429 (2.1429 -- 2.1429)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3926 (0.6929)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4282 (0.1995 -- 2.4162)  data: 0.2020 (0.0005 -- 2.1429)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4473 (0.6363)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2206 (0.1688 -- 0.3358)  data: 0.0107 (0.0001 -- 0.1261)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5473 (0.6927)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2048 (0.1332 -- 0.3358)  data: 0.0104 (0.0001 -- 0.1261)  max mem: 16413
Val: Total time: 0:00:07 (0.2900 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [198]  [  0/160]  eta: 0:22:05  lr: 0.000000  min_lr: 0.000000  loss: 2.0376 (2.0376)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6993 (9.6993)  time: 8.2867 (8.2867 -- 8.2867)  data: 7.7378 (7.7378 -- 7.7378)  max mem: 16413
Epoch: [198]  [ 20/160]  eta: 0:02:54  lr: 0.000000  min_lr: 0.000000  loss: 1.8740 (1.8415)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6179 (8.6532)  time: 0.8946 (0.5130 -- 3.3803)  data: 0.2368 (0.0003 -- 2.8511)  max mem: 16413
Epoch: [198]  [ 40/160]  eta: 0:02:01  lr: 0.000000  min_lr: 0.000000  loss: 1.8511 (1.8475)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8115 (8.8146)  time: 0.7605 (0.5233 -- 2.2534)  data: 0.0013 (0.0004 -- 0.0038)  max mem: 16413
[2023-08-30 21:26:55,832] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:26:55,832] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 21:26:55,834] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:26:55,834] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [198]  [ 60/160]  eta: 0:01:39  lr: 0.000000  min_lr: 0.000000  loss: 1.6439 (1.7968)  loss_scale: 16384.0000 (17726.9508)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2430 (8.6995)  time: 0.9627 (0.5218 -- 4.1712)  data: 0.0016 (0.0004 -- 0.0050)  max mem: 16413
[2023-08-30 21:27:09,826] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31748
[2023-08-30 21:27:09,826] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31748
[2023-08-30 21:27:09,826] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:27:09,826] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:27:09,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [198]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.9124 (1.8061)  loss_scale: 16384.0000 (18811.2593)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8223 (8.7783)  time: 0.8106 (0.5161 -- 4.3551)  data: 0.0012 (0.0003 -- 0.0023)  max mem: 16413
Epoch: [198]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.9341 (1.8198)  loss_scale: 16384.0000 (18330.6139)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3699 (8.9064)  time: 0.9026 (0.5249 -- 3.4355)  data: 0.0028 (0.0004 -- 0.0165)  max mem: 16413
Epoch: [198]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.8500 (1.8223)  loss_scale: 16384.0000 (18008.8595)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0180 (8.8027)  time: 0.7951 (0.5308 -- 4.3348)  data: 0.0013 (0.0002 -- 0.0032)  max mem: 16413
Epoch: [198]  [140/160]  eta: 0:00:17  lr: 0.000000  min_lr: 0.000000  loss: 1.8381 (1.8229)  loss_scale: 16384.0000 (17778.3830)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0417 (8.9522)  time: 0.7854 (0.5205 -- 2.9411)  data: 0.0019 (0.0002 -- 0.0046)  max mem: 16413
Epoch: [198]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.7058 (1.8055)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4838 (9.0422)  time: 0.6822 (0.4972 -- 2.3006)  data: 0.0322 (0.0002 -- 0.6272)  max mem: 16413
Epoch: [198] Total time: 0:02:19 (0.8728 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.7058 (1.7881)  loss_scale: 16384.0000 (17612.8000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4838 (9.0422)
Val:  [ 0/27]  eta: 0:01:02  loss: 0.2809 (0.2809)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3107 (2.3107 -- 2.3107)  data: 2.0998 (2.0998 -- 2.0998)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3922 (0.6929)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4171 (0.2118 -- 2.3107)  data: 0.1935 (0.0008 -- 2.0998)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4469 (0.6362)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2210 (0.1758 -- 0.3237)  data: 0.0091 (0.0001 -- 0.1184)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5473 (0.6925)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2035 (0.1336 -- 0.3237)  data: 0.0087 (0.0001 -- 0.1184)  max mem: 16413
Val: Total time: 0:00:07 (0.2862 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Epoch: [199]  [  0/160]  eta: 0:17:52  lr: 0.000000  min_lr: 0.000000  loss: 1.0460 (1.0460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5687 (6.5687)  time: 6.7013 (6.7013 -- 6.7013)  data: 5.9550 (5.9550 -- 5.9550)  max mem: 16413
Epoch: [199]  [ 20/160]  eta: 0:02:34  lr: 0.000000  min_lr: 0.000000  loss: 1.7906 (1.6950)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7289 (9.2786)  time: 0.8234 (0.5306 -- 1.9057)  data: 0.0017 (0.0003 -- 0.0033)  max mem: 16413
[2023-08-30 21:29:07,090] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:29:07,091] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-08-30 21:29:07,092] [INFO] [fused_optimizer.py:355:_update_scale] No Grad overflow for 128 iterations
[2023-08-30 21:29:07,093] [INFO] [fused_optimizer.py:356:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [199]  [ 40/160]  eta: 0:02:02  lr: 0.000000  min_lr: 0.000000  loss: 1.7567 (1.7324)  loss_scale: 16384.0000 (17982.4390)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0561 (9.3499)  time: 0.9331 (0.5336 -- 4.2990)  data: 0.0023 (0.0003 -- 0.0180)  max mem: 16413
[2023-08-30 21:29:15,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31887
[2023-08-30 21:29:15,724] [INFO] [fused_optimizer.py:347:_update_scale] 
Grad overflow on iteration 31887
[2023-08-30 21:29:15,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:29:15,724] [INFO] [fused_optimizer.py:348:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-08-30 21:29:15,724] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [199]  [ 60/160]  eta: 0:01:37  lr: 0.000000  min_lr: 0.000000  loss: 1.5629 (1.7168)  loss_scale: 16384.0000 (19069.9016)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4038 (9.1581)  time: 0.8818 (0.5221 -- 2.6280)  data: 0.0017 (0.0004 -- 0.0070)  max mem: 16413
Epoch: [199]  [ 80/160]  eta: 0:01:15  lr: 0.000000  min_lr: 0.000000  loss: 1.8839 (1.7601)  loss_scale: 16384.0000 (18406.7160)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7989 (9.0558)  time: 0.8283 (0.5313 -- 2.8641)  data: 0.0030 (0.0005 -- 0.0164)  max mem: 16413
Epoch: [199]  [100/160]  eta: 0:00:56  lr: 0.000000  min_lr: 0.000000  loss: 1.7000 (1.7476)  loss_scale: 16384.0000 (18006.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2465 (9.0916)  time: 0.9531 (0.5123 -- 3.5655)  data: 0.0023 (0.0002 -- 0.0127)  max mem: 16413
Epoch: [199]  [120/160]  eta: 0:00:36  lr: 0.000000  min_lr: 0.000000  loss: 1.9125 (1.7532)  loss_scale: 16384.0000 (17738.0496)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1246 (9.1717)  time: 0.7891 (0.5197 -- 3.1051)  data: 0.0019 (0.0007 -- 0.0091)  max mem: 16413
Epoch: [199]  [140/160]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.5483 (1.7311)  loss_scale: 16384.0000 (17545.9858)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2474 (9.0839)  time: 0.9075 (0.5213 -- 4.0432)  data: 0.0018 (0.0005 -- 0.0057)  max mem: 16413
[2023-08-30 21:30:51,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=32000, skipped=187, lr=[1.1136231593853239e-09, 1.1136231593853239e-09, 1.4848308791804318e-09, 1.4848308791804318e-09, 1.979774505573909e-09, 1.979774505573909e-09, 2.639699340765212e-09, 2.639699340765212e-09, 3.5195991210202825e-09, 3.5195991210202825e-09, 4.692798828027044e-09, 4.692798828027044e-09, 6.257065104036058e-09, 6.257065104036058e-09, 8.342753472048078e-09, 8.342753472048078e-09, 1.1123671296064103e-08, 1.1123671296064103e-08, 1.4831561728085471e-08, 1.4831561728085471e-08, 1.9775415637447294e-08, 1.9775415637447294e-08, 2.6367220849929725e-08, 2.6367220849929725e-08, 3.5156294466572967e-08, 3.5156294466572967e-08, 4.687505928876396e-08, 4.687505928876396e-08], mom=[[0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999], [0.9, 0.999]]
[2023-08-30 21:30:51,282] [INFO] [timer.py:260:stop] epoch=0/micro_step=32000/global_step=32000, RunningAvgSamplesPerSec=17.655579516683225, CurrSamplesPerSec=24.3437831312248, MemAllocated=1.21GB, MaxMemAllocated=16.03GB
Epoch: [199]  [159/160]  eta: 0:00:00  lr: 0.000000  min_lr: 0.000000  loss: 1.6017 (1.7237)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7152 (9.1867)  time: 0.7220 (0.4960 -- 2.9063)  data: 0.0009 (0.0002 -- 0.0057)  max mem: 16413
Epoch: [199] Total time: 0:02:22 (0.8931 s / it)
Averaged stats: lr: 0.000000  min_lr: 0.000000  loss: 1.6017 (1.7250)  loss_scale: 16384.0000 (17408.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7152 (9.1867)
[2023-08-30 21:30:51,288] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint checkpoint-199 is about to be saved!
[2023-08-30 21:30:51,291] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
[2023-08-30 21:30:51,293] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt
[2023-08-30 21:30:51,293] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt...
[2023-08-30 21:30:52,389] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/vislab-001/Jared/Envy_AI_City/VideoMAEv2/training_output/checkpoint-199/mp_rank_00_model_states.pt.
[2023-08-30 21:30:52,389] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint checkpoint-199 is ready now!
Val:  [ 0/27]  eta: 0:01:03  loss: 0.2804 (0.2804)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 2.3367 (2.3367 -- 2.3367)  data: 2.0867 (2.0867 -- 2.0867)  max mem: 16413
Val:  [10/27]  eta: 0:00:07  loss: 0.3925 (0.6929)  acc1: 77.7778 (78.7879)  acc5: 100.0000 (96.9697)  time: 0.4130 (0.2013 -- 2.3367)  data: 0.1910 (0.0006 -- 2.0867)  max mem: 16413
Val:  [20/27]  eta: 0:00:02  loss: 0.4471 (0.6363)  acc1: 77.7778 (82.0106)  acc5: 100.0000 (96.2963)  time: 0.2204 (0.1694 -- 0.4032)  data: 0.0140 (0.0001 -- 0.1942)  max mem: 16413
Val:  [26/27]  eta: 0:00:00  loss: 0.5474 (0.6926)  acc1: 77.7778 (80.0830)  acc5: 100.0000 (95.8506)  time: 0.2034 (0.1327 -- 0.4032)  data: 0.0135 (0.0001 -- 0.1942)  max mem: 16413
Val: Total time: 0:00:07 (0.2864 s / it)
* Acc@1 82.988 Acc@5 97.095 loss 0.648
Accuracy of the network on the 482 val images: 82.99%
Max accuracy: 82.99%
Test:  [  0/603]  eta: 1:06:58  loss: 0.4129 (0.4129)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 6.6645 (6.6645 -- 6.6645)  data: 6.4179 (6.4179 -- 6.4179)  max mem: 16413
Test:  [ 10/603]  eta: 0:09:46  loss: 0.4887 (0.6221)  acc1: 83.3333 (80.3030)  acc5: 100.0000 (98.4848)  time: 0.9884 (0.1293 -- 6.6645)  data: 0.8326 (0.0005 -- 6.4179)  max mem: 16413
Test:  [ 20/603]  eta: 0:11:51  loss: 0.7555 (0.9365)  acc1: 66.6667 (69.0476)  acc5: 100.0000 (94.4444)  time: 0.9480 (0.1263 -- 9.1244)  data: 0.8047 (0.0002 -- 8.9967)  max mem: 16413
configs/vit_b_k710.sh: line 43: 2128088 Killed                  OMP_NUM_THREADS=1 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node=${GPUS_PER_NODE} --master_port 12320 --nnodes=1 --node_rank=0 --master_addr=localhost run_class_finetuning.py --model vit_base_patch16_224 --data_set AI-City-Track-3 --nb_classes 16 --data_path ${DATA_PATH} --finetune ${MODEL_PATH} --log_dir ${OUTPUT_DIR} --output_dir ${OUTPUT_DIR} --batch_size 6 --input_size 224 --short_side_size 224 --save_ckpt_freq 20 --num_frames 16 --sampling_rate 4 --num_sample 2 --num_workers 8 --opt adamw --lr 5e-4 --drop_path 0.1 --head_drop_rate 0.0 --layer_decay 0.75 --opt_betas 0.9 0.999 --warmup_epochs 5 --epochs 200 --test_num_segment 5 --test_num_crop 3 --dist_eval --enable_deepspeed
